self.learning_rule.update_weights(self.model.current_state, self.reward)  # Update weights based on the state and received reward
class Receiver(LewisAgent):
def receive(self):
self.received_signals = [sender.message for sender in self.model.senders]
if self.received_signals:
self.action = self.learning_rule.choose_option()  # Choose an action based on received signals and learned weights
def calc_reward(self):
correct_action = self.model.states_actions[self.model.current_state]
self.reward = 1 if self.action == correct_action else 0
print(f"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}")
def update_learning(self):
for signal in self.received_signals:
self.learning_rule.update_weights(signal, self.reward)  # Update weights based on signals and rewards
class SignalingGame(Model):
def __init__(self, senders_count=1, receivers_count=1, k=3):
super().__init__()
self.k = k
self.current_state = None
# Initialize the states, signals, and actions mapping
self.states_signals = list(range(k))  # States are simply numbers
self.signals_actions = list(chr(65 + i) for i in range(k))  # Signals are characters
self.states_actions = {i: i for i in range(k)}  # Mapping states to correct actions
self.senders = [Sender(i, self, self.signals_actions) for i in range(senders_count)]
self.receivers = [Receiver(i + senders_count, self, self.signals_actions) for i in range(receivers_count)]
self.schedule = StagedActivation(self, agents = self.senders + self.receivers, stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])
def get_state(self):
return random.choice(self.states_signals)
def step(self):
self.current_state = self.get_state()
print(f"New state of the world: {self.current_state}")
self.schedule.step()
# Running the model
model = SignalingGame(senders_count=2, receivers_count=1, k=3)
for i in range(10):
print(f"--- Step {i+1} ---")
model.step()
from mesa import Agent, Model
from mesa.time import StagedActivation
import random
import numpy as np
class LearningRule:
def __init__(self, options, learning_rate=0.1):
print('lr_init()')
self.weights = {option: 1.0 for option in options}  # Start with equal weights for all options
self.learning_rate = learning_rate
def update_weights(self, option, reward):
print('lr_uw()')
# Update the weight of the chosen option by adding the reward scaled by the learning rate
old_weight = self.weights[option]
self.weights[option] += self.learning_rate * reward
print(f"Updated weight for option {option}: {old_weight} -> {self.weights[option]}")
def choose_option(self):
print('lr_co()')
# Select an option based on the weighted probabilities
total = sum(self.weights.values())
probabilities = [self.weights[opt] / total for opt in self.weights]
return np.random.choice(list(self.weights.keys()), p=probabilities)
class LewisAgent(Agent):
def __init__(self, unique_id, model, learning_options):
super().__init__(unique_id, model)
self.message = None
self.action = None
self.reward = 0
self.learning_rule = LearningRule(learning_options, learning_rate=0.1)  # Initialize learning with given options
def send(self):
return
def receive(self):
return
def calc_reward(self):
return
def set_reward(self):
print(f"Agent {self.unique_id} received reward: {self.reward}")
class Sender(LewisAgent):
def send(self):
state = self.model.get_state()
self.message = self.learning_rule.choose_option()  # Send a signal based on the learned weights
print(f"Sender {self.unique_id} sends signal for state {state}: {self.message}")
def update_learning(self):
self.learning_rule.update_weights(self.model.current_state, self.reward)  # Update weights based on the state and received reward
class Receiver(LewisAgent):
def receive(self):
self.received_signals = [sender.message for sender in self.model.senders]
if self.received_signals:
self.action = self.learning_rule.choose_option()  # Choose an action based on received signals and learned weights
def calc_reward(self):
correct_action = self.model.states_actions[self.model.current_state]
self.reward = 1 if self.action == correct_action else 0
print(f"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}")
def update_learning(self):
for signal in self.received_signals:
self.learning_rule.update_weights(signal, self.reward)  # Update weights based on signals and rewards
class SignalingGame(Model):
def __init__(self, senders_count=1, receivers_count=1, k=3):
super().__init__()
self.k = k
self.current_state = None
# Initialize the states, signals, and actions mapping
self.states_signals = list(range(k))  # States are simply numbers
self.signals_actions = list(chr(65 + i) for i in range(k))  # Signals are characters
self.states_actions = {i: i for i in range(k)}  # Mapping states to correct actions
self.senders = [Sender(i, self, self.signals_actions) for i in range(senders_count)]
self.receivers = [Receiver(i + senders_count, self, self.signals_actions) for i in range(receivers_count)]
self.schedule = StagedActivation(self, agents = self.senders + self.receivers, stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])
def get_state(self):
return random.choice(self.states_signals)
def step(self):
self.current_state = self.get_state()
print(f"New state of the world: {self.current_state}")
self.schedule.step()
# Running the model
model = SignalingGame(senders_count=1, receivers_count=1, k=3)
for i in range(10):
print(f"--- Step {i+1} ---")
model.step()
from mesa import Agent, Model
from mesa.time import StagedActivation
import random
import numpy as np
class LearningRule:
def __init__(self, options, learning_rate=0.1):
print('lr_init()')
self.weights = {option: 1.0 for option in options}  # Start with equal weights for all options
self.learning_rate = learning_rate
def update_weights(self, option, reward):
print('lr_uw()')
# Update the weight of the chosen option by adding the reward scaled by the learning rate
old_weight = self.weights[option]
self.weights[option] += self.learning_rate * reward
print(f"Updated weight for option {option}: {old_weight} -> {self.weights[option]}")
def choose_option(self):
print('lr_co()')
# Select an option based on the weighted probabilities
total = sum(self.weights.values())
probabilities = [self.weights[opt] / total for opt in self.weights]
return np.random.choice(list(self.weights.keys()), p=probabilities)
class LewisAgent(Agent):
def __init__(self, unique_id, model, learning_options):
super().__init__(unique_id, model)
self.message = None
self.action = None
self.reward = 0
self.learning_rule = LearningRule(learning_options, learning_rate=0.1)  # Initialize learning with given options
def send(self):
return
def receive(self):
return
def calc_reward(self):
return
def set_reward(self):
print(f"Agent {self.unique_id} received reward: {self.reward}")
class Sender(LewisAgent):
def send(self):
state = self.model.get_state()
self.message = self.learning_rule.choose_option()  # Send a signal based on the learned weights
print(f"Sender {self.unique_id} sends signal for state {state}: {self.message}")
def update_learning(self):
self.learning_rule.update_weights(self.model.current_state, self.reward)  # Update weights based on the state and received reward
class Receiver(LewisAgent):
def receive(self):
self.received_signals = [sender.message for sender in self.model.senders]
if self.received_signals:
self.action = self.learning_rule.choose_option()  # Choose an action based on received signals and learned weights
def calc_reward(self):
correct_action = self.model.states_actions[self.model.current_state]
self.reward = 1 if self.action == correct_action else 0
print(f"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}")
def update_learning(self):
for signal in self.received_signals:
self.learning_rule.update_weights(signal, self.reward)  # Update weights based on signals and rewards
class SignalingGame(Model):
def __init__(self, senders_count=1, receivers_count=1, k=3):
super().__init__()
self.k = k
self.current_state = None
# Initialize the states, signals, and actions mapping
self.states_signals = list(range(k))  # States are simply numbers
self.signals_actions = list(chr(65 + i) for i in range(k))  # Signals are characters
self.states_actions = {i: i for i in range(k)}  # Mapping states to correct actions
self.senders = [Sender(i, self, self.signals_actions) for i in range(senders_count)]
self.receivers = [Receiver(i + senders_count, self, self.signals_actions) for i in range(receivers_count)]
self.schedule = StagedActivation(self, agents = self.senders + self.receivers, stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])
def get_state(self):
return random.choice(self.states_signals)
def step(self):
self.current_state = self.get_state()
print(f"New state of the world: {self.current_state}")
self.schedule.step()
# Running the model
model = SignalingGame(senders_count=1, receivers_count=1, k=3)
for i in range(10):
print(f"--- Step {i+1} ---")
model.step()
from mesa import Agent, Model
from mesa.time import StagedActivation
import random
import numpy as np
class LearningRule:
def __init__(self, options, learning_rate=0.1):
print('lr_init()')
self.weights = {option: 1.0 for option in options}  # Start with equal weights for all options
self.learning_rate = learning_rate
def update_weights(self, option, reward):
print('lr_uw()')
old_weight = self.weights[option]
self.weights[option] += self.learning_rate * reward
print(f"Updated weight for option {option}: {old_weight} -> {self.weights[option]}")
def choose_option(self):
print('lr_co()')
total = sum(self.weights.values())
probabilities = [self.weights[opt] / total for opt in self.weights]
return np.random.choice(list(self.weights.keys()), p=probabilities)
class LewisAgent(Agent):
def __init__(self, unique_id, model, learning_options):
super().__init__(unique_id, model)
self.message = None
self.action = None
self.reward = 0
self.learning_rule = LearningRule(learning_options, learning_rate=0.1)  # Initialize learning with given options
def send(self):
return
def receive(self):
return
def calc_reward(self):
return
def set_reward(self):
print(f"Agent {self.unique_id} received reward: {self.reward}")
class Sender(LewisAgent):
def send(self):
state = self.model.get_state()
self.message = self.learning_rule.choose_option()  # Send a signal based on the learned weights
print(f"Sender {self.unique_id} sends signal for state {state}: {self.message}")
def update_learning(self):
self.learning_rule.update_weights(self.model.current_state, self.reward)  # Update weights based on the state and received reward
class Receiver(LewisAgent):
def receive(self):
self.received_signals = [sender.message for sender in self.model.senders]
if self.received_signals:
self.action = self.learning_rule.choose_option()  # Choose an action based on received signals and learned weights
def calc_reward(self):
correct_action = self.model.states_actions[self.model.current_state]
self.reward = 1 if self.action == correct_action else 0
print(f"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}")
def update_learning(self):
for signal in self.received_signals:
self.learning_rule.update_weights(signal, self.reward)  # Update weights based on signals and rewards
class SignalingGame(Model):
def __init__(self, senders_count=1, receivers_count=1, k=3):
super().__init__()
self.k = k
self.current_state = None
# Initialize the states, signals, and actions mapping
self.states_signals = list(range(k))  # States are simply numbers
self.signals_actions = list(chr(65 + i) for i in range(k))  # Signals are characters
self.states_actions = {i: i for i in range(k)}  # Mapping states to correct actions
self.senders = [Sender(i, self, self.signals_actions) for i in range(senders_count)]
self.receivers = [Receiver(i + senders_count, self, self.signals_actions) for i in range(receivers_count)]
self.schedule = StagedActivation(self, agents = self.senders + self.receivers, stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])
def get_state(self):
return random.choice(self.states_signals)
def step(self):
self.current_state = self.get_state()
print(f"New state of the world: {self.current_state}")
self.schedule.step()
# Running the model
model = SignalingGame(senders_count=1, receivers_count=1, k=3)
for i in range(10):
print(f"--- Step {i+1} ---")
model.step()
from mesa import Agent, Model
from mesa.time import StagedActivation
import random
import numpy as np
class LearningRule:
def __init__(self, options, learning_rate=0.1):
print('lr_init()')
self.weights = {option: 1.0 for option in options}  # Start with equal weights for all options
self.learning_rate = learning_rate
def update_weights(self, option, reward):
print('lr_uw()')
old_weight = self.weights[option]
self.weights[option] += self.learning_rate * reward
print(f"Updated weight for option {option}: {old_weight} -> {self.weights[option]}")
def choose_option(self):
print('lr_co()')
total = sum(self.weights.values())
probabilities = [self.weights[opt] / total for opt in self.weights]
return np.random.choice(list(self.weights.keys()), p=probabilities)
class LewisAgent(Agent):
def __init__(self, unique_id, model, learning_options):
super().__init__(unique_id, model)
self.message = None
self.action = None
self.reward = 0
self.learning_rule = LearningRule(learning_options, learning_rate=0.1)  # Initialize learning with given options
def send(self):
return
def receive(self):
return
def calc_reward(self):
return
def set_reward(self):
print(f"Agent {self.unique_id} received reward: {self.reward}")
class Sender(LewisAgent):
def send(self):
state = self.model.get_state()
self.message = self.learning_rule.choose_option()  # Send a signal based on the learned weights
print(f"Sender {self.unique_id} sends signal for state {state}: {self.message}")
def update_learning(self):
self.learning_rule.update_weights(self.model.current_state, self.reward)  # Update weights based on the state and received reward
class Receiver(LewisAgent):
def receive(self):
self.received_signals = [sender.message for sender in self.model.senders]
if self.received_signals:
self.action = self.learning_rule.choose_option()  # Choose an action based on received signals and learned weights
def calc_reward(self):
correct_action = self.model.states_actions[self.model.current_state]
self.reward = 1 if self.action == correct_action else 0
print(f"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}")
def update_learning(self):
for signal in self.received_signals:
self.learning_rule.update_weights(signal, self.reward)  # Update weights based on signals and rewards
class SignalingGame(Model):
def __init__(self, senders_count=1, receivers_count=1, k=3):
super().__init__()
self.k = k
self.current_state = None
# Initialize the states, signals, and actions mapping
self.states_signals = list(range(k))  # States are simply numbers
self.signals_actions = list(chr(65 + i) for i in range(k))  # Signals are characters
self.states_actions = {i: i for i in range(k)}  # Mapping states to correct actions
self.senders = [Sender(i, self, self.signals_actions) for i in range(senders_count)]
self.receivers = [Receiver(i + senders_count, self, self.signals_actions) for i in range(receivers_count)]
self.schedule = StagedActivation(self, agents = self.senders + self.receivers, stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])
def get_state(self):
return random.choice(self.states_signals)
def step(self):
self.current_state = self.get_state()
print(f"New state of the world: {self.current_state}")
self.schedule.step()
# Running the model
model = SignalingGame(senders_count=1, receivers_count=1, k=3)
for i in range(10):
print(f"--- Step {i+1} ---")
model.step()
from mesa import Agent, Model
from mesa.time import StagedActivation
import random
import numpy as np
class LearningRule:
def __init__(self, options, learning_rate=0.1):
print('lr_init()')
self.weights = {option: 1.0 for option in options}  # Start with equal weights for all options
self.learning_rate = learning_rate
def update_weights(self, option, reward):
print(f'LearningRule.update_weights({option=}{reward=})')
print(f"Current weights: {self.weights}")
old_weight = self.weights[option]
self.weights[option] += self.learning_rate * reward
print(f"Updated weight for option {option}: {old_weight} -> {self.weights[option]}")
def choose_option(self):
print('lr_co()')
total = sum(self.weights.values())
probabilities = [self.weights[opt] / total for opt in self.weights]
return np.random.choice(list(self.weights.keys()), p=probabilities)
class LewisAgent(Agent):
def __init__(self, unique_id, model, learning_options):
super().__init__(unique_id, model)
self.message = None
self.action = None
self.reward = 0
self.learning_rule = LearningRule(learning_options, learning_rate=0.1)  # Initialize learning with given options
def send(self):
return
def receive(self):
return
def calc_reward(self):
return
def set_reward(self):
print(f"Agent {self.unique_id} received reward: {self.reward}")
class Sender(LewisAgent):
def send(self):
state = self.model.get_state()
self.message = self.learning_rule.choose_option()  # Send a signal based on the learned weights
print(f"Sender {self.unique_id} sends signal for state {state}: {self.message}")
def update_learning(self):
self.learning_rule.update_weights(self.model.current_state, self.reward)  # Update weights based on the state and received reward
class Receiver(LewisAgent):
def receive(self):
self.received_signals = [sender.message for sender in self.model.senders]
if self.received_signals:
self.action = self.learning_rule.choose_option()  # Choose an action based on received signals and learned weights
def calc_reward(self):
correct_action = self.model.states_actions[self.model.current_state]
self.reward = 1 if self.action == correct_action else 0
print(f"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}")
def update_learning(self):
for signal in self.received_signals:
self.learning_rule.update_weights(signal, self.reward)  # Update weights based on signals and rewards
class SignalingGame(Model):
def __init__(self, senders_count=1, receivers_count=1, k=3):
super().__init__()
self.k = k
self.current_state = None
# Initialize the states, signals, and actions mapping
self.states_signals = list(range(k))  # States are simply numbers
self.signals_actions = list(chr(65 + i) for i in range(k))  # Signals are characters
self.states_actions = {i: i for i in range(k)}  # Mapping states to correct actions
self.senders = [Sender(i, self, options=self.signals_actions) for i in range(senders_count)]
self.receivers = [Receiver(i + senders_count, self, options=self.states_actions) for i in range(receivers_count)]
self.schedule = StagedActivation(self, agents = self.senders + self.receivers, stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])
def get_state(self):
return random.choice(self.states_signals)
def step(self):
self.current_state = self.get_state()
print(f"New state of the world: {self.current_state}")
self.schedule.step()
# Running the model
model = SignalingGame(senders_count=1, receivers_count=1, k=3)
for i in range(10):
print(f"--- Step {i+1} ---")
model.step()
from mesa import Agent, Model
from mesa.time import StagedActivation
import random
import numpy as np
class LearningRule:
def __init__(self, options, learning_rate=0.1,verbose=False):
self.verbose = verbose
if self.verbose:
print(f'LearningRule.__init__(Options: {options})')
self.weights = {option: 1.0 for option in options}  # Start with equal weights for all options
self.learning_rate = learning_rate
def update_signal_weights(self, option, reward):
if self.verbose:
print(f'LearningRule.update_weights({option=}{reward=})')
print(f"Current weights: {self.weights}")
old_weight = self.weights[option]
self.weights[option] += self.learning_rate * reward
print(f"Updated weight for option {option}: {old_weight} -> {self.weights[option]}")
def update_action_weights(self, option, reward):
if self.verbose:
print(f'LearningRule.update_weights({option=}{reward=})')
print(f"Current weights: {self.weights}")
old_weight = self.weights[option]
self.weights[option] += self.learning_rate * reward
print(f"Updated weight for option {option}: {old_weight} -> {self.weights[option]}")
def choose_option(self):
if self.verbose:
print('lr_co()')
total = sum(self.weights.values())
probabilities = [self.weights[opt] / total for opt in self.weights]
return np.random.choice(list(self.weights.keys()), p=probabilities)
class LewisAgent(Agent):
def __init__(self, unique_id, model, learning_options):
super().__init__(unique_id, model)
self.message = None
self.action = None
self.reward = 0
self.learning_rule = LearningRule(learning_options, learning_rate=0.1)  # Initialize learning with given options
def send(self):
return
def receive(self):
return
def calc_reward(self):
return
def set_reward(self):
self.reward = self.model.reward
print(f"Agent {self.unique_id} received reward: {self.reward}")
class Sender(LewisAgent):
def send(self):
state = self.model.get_state()
self.message = self.learning_rule.choose_option()  # Send a signal based on the learned weights
print(f"Sender {self.unique_id} sends signal for state {state}: {self.message}")
def update_learning(self):
self.learning_rule.update_signal_weights(self.message, self.reward)  # Update weights based on the state and received reward
class Receiver(LewisAgent):
def receive(self):
self.received_signals = [sender.message for sender in self.model.senders]
if self.received_signals:
self.action = self.learning_rule.choose_option()  # Choose an action based on received signals and learned weights
def calc_reward(self):
correct_action = self.model.states_actions[self.model.current_state]
self.model.reward = 1 if self.action == correct_action else 0
print(f"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}")
def update_learning(self):
self.learning_rule.update_action_weights(self.action, self.reward)  # Update weights based on signals and rewards
class SignalingGame(Model):
def __init__(self, senders_count=1, receivers_count=1, k=3):
super().__init__()
self.k = k
self.current_state = None
# Initialize the states, signals, and actions mapping
self.states_signals = list(range(k))  # States are simply numbers
self.signals_actions = list(chr(65 + i) for i in range(k))  # Signals are characters
self.states_actions = {i: i for i in range(k)}  # Mapping states to correct actions
self.senders = [Sender(i, self, learning_options=self.signals_actions) for i in range(senders_count)]
self.receivers = [Receiver(i + senders_count, self, learning_options=self.states_signals) for i in range(receivers_count)]
self.schedule = StagedActivation(self, agents = self.senders + self.receivers, stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])
def get_state(self):
return random.choice(self.states_signals)
def step(self):
self.current_state = self.get_state()
print(f"New state of the world: {self.current_state}")
self.schedule.step()
# Running the model
model = SignalingGame(senders_count=1, receivers_count=1, k=3)
for i in range(10):
print(f"--- Step {i+1} ---")
model.step()
reticulate::repl_python()
reticulate::repl_python()
reticulate::repl_python()
