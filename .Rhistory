fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=False
)
plt.show()
#| label: fig-decision-tree
#| fig-cap: "A simple decision tree for the Salary DataSet"
from sklearn import tree
plot=tree.plot_tree(dt_clf_model,filled=True,
fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=True
)
plt.show()
#| label: fig-decision-tree
#| fig-cap: "A simple decision tree for the Salary DataSet"
from sklearn import tree
plot=tree.plot_tree(dt_clf_model,filled=True,
fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=True,impurity=True
)
plt.show()
dt_clf_model = DecisionTreeRegressor(
#max_depth=3,
random_state=123)
dt_clf_model.fit(X_train, y_train)
#Predict the response for test dataset
y_pred = dt_clf_model.predict(X_test)
# Model Accuracy, how often is the classifier correct?
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
dt_clf_model = DecisionTreeRegressor(
#max_depth=3,
random_state=123)
dt_clf_model.fit(X_train, y_train)
#Predict the response for test dataset
y_pred = dt_clf_model.predict(X_test)
# Model Accuracy, how often is the classifier correct?
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
#| label: fig-decision-tree
#| fig-cap: "A simple decision tree for the Salary DataSet"
from sklearn import tree
plot=tree.plot_tree(dt_clf_model,filled=True,
fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=True)
plt.show()
#| label: fig-decision-tree
#| fig-cap: "A simple decision tree for the Salary DataSet"
from sklearn import tree
plot=tree.plot_tree(dt_clf_model,filled=True,
#fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=True)
plt.show()
import graphviz
dot_data = tree.export_graphviz(dt_clf_model, out_file=None,
feature_names=X_train.columns,
class_names=y,
filled=True, rounded=True,
special_characters=True)
graph = graphviz.Source(dot_data)
_=plt.show()
import graphviz
dot_data = tree.export_graphviz(dt_clf_model, out_file=None,
feature_names=X_train.columns,
class_names=y,
filled=True, rounded=True,
special_characters=True)
graph = graphviz.Source(dot_data)
plt.show()
import shap
explainer = shap.TreeExplainer(dt_clf_model,X_test)
shap_values = explainer.shap_values(X)
#| label: code-load
#| warning: false
import numpy as np                                          # <1>
import pandas as pd                                         # <1>
from itables import show
import matplotlib.pyplot as plt                             # <1>
import seaborn as sns                                       # <1>
from sklearn.model_selection import train_test_split        # <1>
import xgboost as xgb                                       # <1>
df = pd.read_csv('./data/Salary Data.csv')                  # <2>
show(df)                                                    # <3>
from sklearn.preprocessing import LabelEncoder,  OneHotEncoder # <1>
df = (  df.dropna()                   # <2>
.drop_duplicates()          # <3>
.assign(is_male=lambda x: x['Gender'].apply(lambda y: 1 if y == 'Male' else 0),               # <4>
is_PhD=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'PhD' else 0),        # <5>
is_BA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Bachelor\'s' else 0), # <5>
is_MA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Master\'s' else 0),   # <5>
)
.rename(columns={'Years of Experience':'xp'}) #<6>
.drop(['Gender','Education Level','Job Title'],axis=1) #<7>
)
#df['Education Level'] = edu_label_encoder.fit_transform(df['Education Level'])
#job_title_encoder = LabelEncoder()
#df['Job Title']=job_title_encoder.fit_transform(df['Job Title'])
show(df)                                                    # <8>
df.info()
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor       # <1>
from sklearn.model_selection import train_test_split  # <1>
from sklearn import metrics # <1>
y = df['Salary'] # <2>
X = df.drop(['Salary'], axis=1) # <3>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
dt_clf_model = DecisionTreeRegressor(
#max_depth=3,
random_state=123)
dt_clf_model.fit(X_train, y_train)
#Predict the response for test dataset
y_pred = dt_clf_model.predict(X_test)
# Model Accuracy, how often is the classifier correct?
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
#| label: fig-decision-tree
#| fig-cap: "A simple decision tree for the Salary DataSet"
from sklearn import tree
plot=tree.plot_tree(dt_clf_model,filled=True,
#fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=True)
plt.show()
import graphviz
dot_data = tree.export_graphviz(dt_clf_model, out_file=None,
feature_names=X_train.columns,
class_names=y,
filled=True, rounded=True,
special_characters=True)
graph = graphviz.Source(dot_data)
graph
plt.show()
import shap
explainer = shap.TreeExplainer(dt_clf_model,X_test)
shap_values = explainer.shap_values(X)
import shap
explainer = shap.TreeExplainer(dt_clf_model,X_test)
shap_values = explainer.shap_values(X)
shap_values
import lime
import lime.lime_tabular
lime_explainer = lime_tabular.LimeTabularExplainer(dt_clf_model,feature_names=X_train.columns,discretize_continuous=True)
i = np.random.randint(0, test.shape[0])
exp = lime_explainer.explain_instance(test[i], rf.predict_proba, num_features=2, top_labels=1)
#| label: code-load
#| warning: false
import numpy as np                                          # <1>
import pandas as pd                                         # <1>
from itables import show
import matplotlib.pyplot as plt                             # <1>
import seaborn as sns                                       # <1>
from sklearn.model_selection import train_test_split        # <1>
import xgboost as xgb                                       # <1>
df = pd.read_csv('./data/Salary Data.csv')                  # <2>
show(df)                                                    # <3>
from sklearn.preprocessing import LabelEncoder,  OneHotEncoder # <1>
df = (  df.dropna()                   # <2>
.drop_duplicates()          # <3>
.assign(is_male=lambda x: x['Gender'].apply(lambda y: 1 if y == 'Male' else 0),               # <4>
is_PhD=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'PhD' else 0),        # <5>
is_BA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Bachelor\'s' else 0), # <5>
is_MA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Master\'s' else 0),   # <5>
)
.rename(columns={'Years of Experience':'xp'}) #<6>
.drop(['Gender','Education Level','Job Title'],axis=1) #<7>
)
#df['Education Level'] = edu_label_encoder.fit_transform(df['Education Level'])
#job_title_encoder = LabelEncoder()
#df['Job Title']=job_title_encoder.fit_transform(df['Job Title'])
show(df)                                                    # <8>
df.info()
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor       # <1>
from sklearn.model_selection import train_test_split  # <1>
from sklearn import metrics # <1>
y = df['Salary'] # <2>
X = df.drop(['Salary'], axis=1) # <3>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
dt_clf_model = DecisionTreeRegressor(
#max_depth=3,
random_state=123)
dt_clf_model.fit(X_train, y_train)
#Predict the response for test dataset
y_pred = dt_clf_model.predict(X_test)
# Model Accuracy, how often is the classifier correct?
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
#| label: fig-decision-tree
#| fig-cap: "A simple decision tree for the Salary DataSet"
from sklearn import tree
plot=tree.plot_tree(dt_clf_model,filled=True,
#fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=True)
plt.show()
import graphviz
dot_data = tree.export_graphviz(dt_clf_model, out_file=None,
feature_names=X_train.columns,
class_names=y,
filled=True, rounded=True,
special_characters=True)
graph = graphviz.Source(dot_data)
graph
plt.show()
import lime
import lime.lime_tabular
lime_explainer = lime_tabular.LimeTabularExplainer(dt_clf_model,feature_names=X_train.columns,discretize_continuous=True)
i = np.random.randint(0, test.shape[0])
exp = lime_explainer.explain_instance(test[i], rf.predict_proba, num_features=2, top_labels=1)
import lime
import lime.lime_tabular
lime_explainer = lime_tabular.LimeTabularExplainer(dt_clf_model,feature_names=X_train.columns,discretize_continuous=True)
i = np.random.randint(0, X_test.shape[0])
exp = lime_explainer.explain_instance(X_test[i], dt_clf_model, num_features=2, top_labels=1)
import lime
import lime.lime_tabular
lime_explainer = lime_tabular.LimeTabularExplainer(dt_clf_model,feature_names=X_train.columns,discretize_continuous=True)
i = np.random.randint(0, X_test.shape[0])
exp = lime_explainer.explain_instance(X_test[i], dt_clf_model, num_features=2, top_labels=1)
#| label: code-load
#| warning: false
import numpy as np                                          # <1>
import pandas as pd                                         # <1>
from itables import show
import matplotlib.pyplot as plt                             # <1>
import seaborn as sns                                       # <1>
from sklearn.model_selection import train_test_split        # <1>
import xgboost as xgb                                       # <1>
df = pd.read_csv('./data/Salary Data.csv')                  # <2>
show(df)                                                    # <3>
from sklearn.preprocessing import LabelEncoder,  OneHotEncoder # <1>
df = (  df.dropna()                   # <2>
.drop_duplicates()          # <3>
.assign(is_male=lambda x: x['Gender'].apply(lambda y: 1 if y == 'Male' else 0),               # <4>
is_PhD=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'PhD' else 0),        # <5>
is_BA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Bachelor\'s' else 0), # <5>
is_MA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Master\'s' else 0),   # <5>
)
.rename(columns={'Years of Experience':'xp'}) #<6>
.drop(['Gender','Education Level','Job Title'],axis=1) #<7>
)
#df['Education Level'] = edu_label_encoder.fit_transform(df['Education Level'])
#job_title_encoder = LabelEncoder()
#df['Job Title']=job_title_encoder.fit_transform(df['Job Title'])
show(df)                                                    # <8>
df.info()
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor       # <1>
from sklearn.model_selection import train_test_split  # <1>
from sklearn import metrics # <1>
y = df['Salary'] # <2>
X = df.drop(['Salary'], axis=1) # <3>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
dt_clf_model = DecisionTreeRegressor(
#max_depth=3,
random_state=123)
dt_clf_model.fit(X_train, y_train)
#Predict the response for test dataset
y_pred = dt_clf_model.predict(X_test)
# Model Accuracy, how often is the classifier correct?
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
#| label: fig-decision-tree
#| fig-cap: "A simple decision tree for the Salary DataSet"
from sklearn import tree
plot=tree.plot_tree(dt_clf_model,filled=True,
#fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=True)
plt.show()
import graphviz
dot_data = tree.export_graphviz(dt_clf_model, out_file=None,
feature_names=X_train.columns,
class_names=y,
filled=True, rounded=True,
special_characters=True)
graph = graphviz.Source(dot_data)
graph
plt.show()
import lime
import lime.lime_tabular
lime_explainer = lime_tabular.LimeTabularExplainer(dt_clf_model,feature_names=X_train.columns,discretize_continuous=True)
i = np.random.randint(0, X_test.shape[0])
exp = lime_explainer.explain_instance(X_test[i], dt_clf_model, num_features=2, top_labels=1)
from  lime import lime_tabular
lime_explainer = lime_tabular.LimeTabularExplainer(dt_clf_model,feature_names=X_train.columns)
i = np.random.randint(0, X_test.shape[0])
exp = lime_explainer.explain_instance(X_test[i], dt_clf_model, num_features=2, top_labels=1)
from  lime import lime_tabular
y_pred = dt_clf_model.predict(X_test)
explainer = lime_tabular.LimeTabularExplainer(dt_clf_model,feature_names=X_train.columns)
i = np.random.randint(0, X_test.shape[0])
exp = explainer.explain_instance(X_test[i], y_pred, num_features=2, top_labels=1)
#exp = explainer.explain_instance(X_test[i], y_pred, num_features=4)
from  lime import lime_tabular
y_pred = dt_clf_model.predict(X_test)
feature_names=X_train.columns
explainer = lime_tabular.LimeTabularExplainer(X_train.values,feature_names=feature_names,class_names=['Salary'],categorical_features=['is_male','is_BA','is_PHD'],verbose=True,mode='regression'])
i = np.random.randint(0, X_test.shape[0])
#exp = explainer.explain_instance(X_test[i], y_pred, num_features=4)
from  lime import lime_tabular
y_pred = dt_clf_model.predict(X_test)
feature_names=X_train.columns
explainer = lime_tabular.LimeTabularExplainer(X_train.values,feature_names=feature_names,class_names=['Salary'],categorical_features=['is_male','is_BA','is_PHD'],verbose=True,mode='regression')
i = np.random.randint(0, X_test.shape[0])
#exp = explainer.explain_instance(X_test[i], y_pred, num_features=4)
from  lime import lime_tabular
y_pred = dt_clf_model.predict(X_test)
feature_names=X_train.columns
lime_explainer = lime_tabular.LimeTabularExplainer(X_train.values,feature_names=feature_names,class_names=['Salary'],categorical_features=['is_male','is_BA','is_PHD'],verbose=True,mode='regression')
i = np.random.randint(0, X_test.shape[0])
exp = lime_explainer.explain_instance(X_test.values[i,:], dt_clf_model.predict,
num_features=5,num_samples=100)
exp.as_list()
dt_clf_model = DecisionTreeRegressor(
max_depth=3,
random_state=123)
dt_clf_model.fit(X_train, y_train)
#Predict the response for test dataset
y_pred = dt_clf_model.predict(X_test)
# Model Accuracy, how often is the classifier correct?
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor       # <1>
from sklearn.model_selection import train_test_split  # <1>
from sklearn import metrics      # <1>
y = df['Salary']                 # <2>
X = df.drop(['Salary'], axis=1)  # <3>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
df.info()
#| label: fig-decision-tree
#| fig-cap: "A simple decision tree for the Salary DataSet"
from sklearn import tree
plot=tree.plot_tree(dt_clf_model,filled=True,
#fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=True)
plt.show()
import graphviz
dot_data = tree.export_graphviz(dt_clf_model, out_file=None,
feature_names=X_train.columns,
class_names=y,
filled=True, rounded=True,
special_characters=True)
graph = graphviz.Source(dot_data)
graph
plt.show()
import shap
explainer = shap.TreeExplainer(dt_clf_model,X_test)
shap_values = explainer.shap_values(X)
shap_values[i]
#| label: code-load
#| warning: false
import numpy as np                                          # <1>
import pandas as pd                                         # <1>
from itables import show
import matplotlib.pyplot as plt                             # <1>
import seaborn as sns                                       # <1>
from sklearn.model_selection import train_test_split        # <1>
import xgboost as xgb                                       # <1>
df = pd.read_csv('./data/Salary Data.csv')                  # <2>
show(df)                                                    # <3>
from sklearn.preprocessing import LabelEncoder,  OneHotEncoder # <1>
df = (  df.dropna()                   # <2>
.drop_duplicates()          # <3>
.assign(is_male=lambda x: x['Gender'].apply(lambda y: 1 if y == 'Male' else 0),               # <4>
is_PhD=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'PhD' else 0),        # <5>
is_BA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Bachelor\'s' else 0), # <5>
#is_MA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Master\'s' else 0),   # <5>
)
.rename(columns={'Years of Experience':'xp'}) #<6>
.drop(['Gender','Education Level','Job Title'],axis=1) #<7>
)
#df['Education Level'] = edu_label_encoder.fit_transform(df['Education Level'])
#job_title_encoder = LabelEncoder()
#df['Job Title']=job_title_encoder.fit_transform(df['Job Title'])
show(df)                                                    # <8>
df.info()
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor       # <1>
from sklearn.model_selection import train_test_split  # <1>
from sklearn import metrics      # <1>
y = df['Salary']                 # <2>
X = df.drop(['Salary'], axis=1)  # <3>
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
dt_clf_model = DecisionTreeRegressor(
max_depth=3,
random_state=123)
dt_clf_model.fit(X_train, y_train)
#Predict the response for test dataset
y_pred = dt_clf_model.predict(X_test)
# Model Accuracy, how often is the classifier correct?
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
#| label: fig-decision-tree
#| fig-cap: "A simple decision tree for the Salary DataSet"
from sklearn import tree
plot=tree.plot_tree(dt_clf_model,filled=True,
#fontsize=4,proportion=True,
feature_names=X_train.columns,rounded=True)
plt.show()
import graphviz
dot_data = tree.export_graphviz(dt_clf_model, out_file=None,
feature_names=X_train.columns,
class_names=y,
filled=True, rounded=True,
special_characters=True)
graph = graphviz.Source(dot_data)
graph
plt.show()
from  lime import lime_tabular
y_pred = dt_clf_model.predict(X_test)
feature_names=X_train.columns
lime_explainer = lime_tabular.LimeTabularExplainer(X_train.to_numpy(),feature_names=feature_names,class_names=['Salary'],
categorical_features=['is_male','is_BA','is_PhD'],verbose=True,mode='regression')
i = np.random.randint(0, X_test.shape[0])
exp = lime_explainer.explain_instance(X_test.values[i,:], dt_clf_model.predict,
num_features=5,num_samples=100)
exp.as_list()
from  lime import lime_tabular
y_pred = dt_clf_model.predict(X_test)
feature_names=X_train.columns
lime_explainer = lime_tabular.LimeTabularExplainer(
training_data=X_train.to_numpy(),
feature_names=feature_names,
class_names=['Salary'],
categorical_features=['is_male','is_BA','is_PhD'],
#verbose=True,
mode='regression')
i = np.random.randint(0, X_test.shape[0])
exp = lime_explainer.explain_instance(X_test.values[i,:], dt_clf_model.predict,
num_features=5,num_samples=100)
exp.as_list()
from  lime import lime_tabular
y_pred = dt_clf_model.predict(X_test)
feature_names=X_train.columns
lime_explainer = lime_tabular.LimeTabularExplainer(
training_data=X_train.to_numpy(),
feature_names=feature_names,
class_names=['Salary'],
categorical_features=['is_male','is_BA','is_PhD'],
verbose=True,
mode='regression')
i = np.random.randint(0, X_test.shape[0])
exp = lime_explainer.explain_instance(X_test.values[i,:],
dt_clf_model.predict,
num_features=5,
num_samples=100)
exp.as_list()
import graphviz
dot_data = tree.export_graphviz(dt_clf_model, out_file=None,
feature_names=X_train.columns,
class_names=y,
filled=True, rounded=True,
special_characters=True)
graph = graphviz.Source(dot_data)
graph
#plt.show()
version
git status
version()
ver
ver()
help version
help(version)
R.Version()
install.packages(c("corrplot", "rjags", "summarytools", "tree", "treeheatr"))
install.packages(c("rethinker", "StanHeaders", "coda", "mvtnorm", "devtools", "loo", "dagitty", "shape"))
install.packages(c("StanHeaders", "coda", "mvtnorm", "loo", "dagitty", "shape"))
install.packages(c("StanHeaders", "coda", "mvtnorm", "loo", "shape"))
install.packages("devtools")
install.packages("devtools")
install.packages(c("coda","mvtnorm","devtools","loo","dagitty","shape"))
install.packages(c("dagitty"))
install.packages(c("dagitty"))
devtools::install_github("rmcelreath/rethinking")
# we recommend running this is a fresh R session or restarting your current session
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
cmdstanr::install_cmdstan()
install.packages(c("ggplot2", "GGally", "BART"))
install.packages("rjags")
install.packages("rjags")
LS *.TXT
install.packages("nimble")
install.packages("igraph")
install.packages("igraph")
options(
repos = c(
igraph = 'https://igraph.r-universe.dev',
CRAN = 'https://cloud.r-project.org'
)
)
install.packages('igraph')
options(
repos = c(
igraph = 'https://igraph.r-universe.dev',
CRAN = 'https://cloud.r-project.org'
)
)
install.packages('igraph')
options(
repos = c(
igraph = 'https://igraph.r-universe.dev',
CRAN = 'https://cloud.r-project.org'
)
)
install.packages('igraph')
install.packages("nimble")
install.packages("car")
install.packages(c("MASS", "COUNT"))
install.packages(c("ggm", "parsnip", "BART", "umap", "tidyverse", "dbscan"))
install.packages("car")
install.packages("car")
install.packages(c("ggm", "parsnip", "BART", "umap", "tidyverse", "dbscan"))
install.packages(c("DBI", "XLConnect", "xlsx", "lubridate", "htmlwidgets", "caret", "zoo", "data.table", "httr", "jsonlite"))
install.packages(c("wikibooks", "WikidataR", "WikidataQueryServiceR", "wikiTools"))
install.packages("WikidataQueryServiceR")
install.packages("pageviews")
install.packages("MFDFA")
install.packages("GGally")
install.packages(c("bain", "BayesFactor", "BMA", "BMS", "RoBMA", "BAS", "dbarts", "bartcs", "bartCause"))
install.packages("SuperLearner")
R.version()
R.version
reticulate::repl_python()
quit
quit()
reticulate::repl_python()
