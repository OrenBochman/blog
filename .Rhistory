from mesa import Model, Agent
from mesa.time import RandomActivation
import numpy as np
class EpsilonGreedyAgent(Agent):
"""
This agent implements the epsilon-greedy
"""
def __init__(self, unique_id, model, num_arms, epsilon=0.1):
super().__init__(unique_id,model)
self.num_arms = num_arms
self.epsilon = epsilon
self.q_values = np.zeros(num_arms)  # Initialize Q-value estimates
self.action_counts = np.zeros(num_arms)  # Track action counts
def choose_action(self):
if np.random.rand() < self.epsilon:
# Exploration: Choose random arm
return np.random.randint(0, self.num_arms)
else:
# Exploitation: Choose arm with highest Q-value
return np.argmax(self.q_values)
def step(self, model):
chosen_arm = self.choose_action()
reward = model.get_reward(chosen_arm)
assert reward is not None, "Reward is not provided by the model"
self.action_counts[chosen_arm] += 1
self.q_values[chosen_arm] = (self.q_values[chosen_arm] * self.action_counts[chosen_arm] + reward) / (self.action_counts[chosen_arm] + 1)
class TestbedModel(Model):
"""
This model represents the 10-armed bandit testbed environment.
"""
def __init__(self, num_arms, mean_reward, std_dev,num_agents=1):
super().__init__()
self.num_agents = num_agents
self.num_arms = num_arms
self.mean_reward = mean_reward
self.std_dev = std_dev
self.arms = [None] * num_arms  # List to store arm rewards
self.schedule = RandomActivation(self)
for i in range(self.num_agents):
self.create_agent(EpsilonGreedyAgent, i, 0.1)
def env_init(self,env_info={}):
self.arms = np.random.randn(self.num_arms)  # Initialize arm rewards
def create_agent(self, agent_class, agent_id, epsilon):
"""
Create an RL agent instance with the specified class and parameters.
"""
agent = agent_class(agent_id, self, self.num_arms, epsilon)
self.schedule.add(agent)
return agent
def step(self):
for agent in self.schedule.agents:
chosen_arm = agent.choose_action()
reward = np.random.normal(self.mean_reward, self.std_dev)
self.arms[chosen_arm] = reward  # Update arm reward in the model
agent.step(self)  # Pass the model instance to the agent for reward access
def get_reward(self, arm_id):
# Access reward from the stored list
return self.arms[arm_id]
# Example usage
model = TestbedModel(10, 0, 1)  # Create model with 10 arms
# Run simulation for multiple steps
for _ in range(100):
model.step()
from tqdm import tqdm
from mesa import Model, Agent
from mesa.time import RandomActivation
import numpy as np
class EpsilonGreedyAgent(Agent):
"""
This agent implements the epsilon-greedy
"""
def __init__(self, unique_id, model, num_arms, epsilon=0.1):
super().__init__(unique_id,model)
self.num_arms = num_arms
self.epsilon = epsilon
self.q_values = np.zeros(num_arms)  # Initialize Q-value estimates
self.action_counts = np.zeros(num_arms)  # Track action counts
def choose_action(self):
if np.random.rand() < self.epsilon:
# Exploration: Choose random arm
return np.random.randint(0, self.num_arms)
else:
# Exploitation: Choose arm with highest Q-value
return np.argmax(self.q_values)
def step(self, model):
chosen_arm = self.choose_action()
reward = model.get_reward(chosen_arm)
assert reward is not None, "Reward is not provided by the model"
self.action_counts[chosen_arm] += 1
self.q_values[chosen_arm] = (self.q_values[chosen_arm] * self.action_counts[chosen_arm] + reward) / (self.action_counts[chosen_arm] + 1)
class TestbedModel(Model):
"""
This model represents the 10-armed bandit testbed environment.
"""
def __init__(self, num_arms, mean_reward, std_dev,num_agents=1):
super().__init__()
self.num_agents = num_agents
self.num_arms = num_arms
self.mean_reward = mean_reward
self.std_dev = std_dev
self.env_init()
self.arms = [None] * num_arms  # List to store arm rewards
self.schedule = RandomActivation(self)
for i in range(self.num_agents):
self.create_agent(EpsilonGreedyAgent, i, 0.1)
def env_init(self,env_info={}):
self.arms = np.random.randn(self.num_arms)  # Initialize arm rewards
def create_agent(self, agent_class, agent_id, epsilon):
"""
Create an RL agent instance with the specified class and parameters.
"""
agent = agent_class(agent_id, self, self.num_arms, epsilon)
self.schedule.add(agent)
return agent
def step(self):
for agent in self.schedule.agents:
chosen_arm = agent.choose_action()
reward = np.random.normal(self.mean_reward, self.std_dev)
self.arms[chosen_arm] = reward  # Update arm reward in the model
agent.step(self)  # Pass the model instance to the agent for reward access
def get_reward(self, arm_id):
# Access reward from the stored list
return self.arms[arm_id]
# Example usage
model = TestbedModel(10, 0, 1)  # Create model with 10 arms
num_runs = 200                  # The number of times we run the experiment
num_steps = 1000                # The number of pulls of each arm the agent takes
# Run simulation for multiple steps
for _ in tqdm(range(num_runs)):
for _ in range(num_steps):
model.step()
model.step()
install.packages("rmarkdown")
renv::status()
?renv::status()
renv::restore()
install.packages(c("downlit", "xml2"))
reticulate::repl_python()
rnev::status()
renv::status()
?renv::status()
renv::install()
renv::snapshot()
renv::status()
ls
renv::status()
renv::status()
renv::update()
renv::snapshot()
renv::snapshot()
renv::snapshot()
renv::snapshot()
renv::init()
renv::status()
?renv::status()
renv::snapshot()
set.seed(2021)
r=0.95
lambda=12
phi=numeric(2)
phi[1]=2*r*cos(2*pi/lambda)
phi[2]=-r^2
sd=1 # innovation standard deviation
T=300 # number of time points
# generate stationary AR(2) process
yt=arima.sim(n = T, model = list(ar = phi), sd = sd)
par(mfrow=c(1,1))
plot(yt)
## Case 1: Conditional likelihood
p=2
y=rev(yt[(p+1):T]) # response
X=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));
XtX=t(X)%*%X
XtX_inv=solve(XtX)
phi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi
s2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v
cat("\n MLE of conditional likelihood for phi: ", phi_MLE, "\n",
"Estimate for v: ", s2, "\n")
#####################################################################################
##  AR(2) case
### Posterior inference, conditional likelihood + reference prior via
### direct sampling
#####################################################################################
n_sample=1000 # posterior sample size
library(MASS)
## step 1: sample v from inverse gamma distribution
v_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)
## step 2: sample phi conditional on v from normal distribution
phi_sample=matrix(0, nrow = n_sample, ncol = p)
for(i in 1:n_sample){
phi_sample[i, ]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)
}
## plot histogram of posterior samples of phi and nu
par(mfrow = c(1, 3), cex.lab = 1.3)
for(i in 1:2){
hist(phi_sample[, i], xlab = bquote(phi),
main = bquote("Histogram of "~phi[.(i)]))
abline(v = phi[i], col = 'red')
}
hist(v_sample, xlab = bquote(nu), main = bquote("Histogram of "~v))
abline(v = sd, col = 'red')
#| label: ar-spectral-density
### Simulate 300 observations from an AR(2) prcess with a pair of complex-valued roots
set.seed(2021)
r=0.95
lambda=12
phi=numeric(2)
phi[1]<- 2*r*cos(2*pi/lambda)
phi[2] <- -r^2
sd=1 # innovation standard deviation
T=300 # number of time points
# sample from the AR(2) process
yt=arima.sim(n = T, model = list(ar = phi), sd = sd)
# Compute the MLE of phi and the unbiased estimator of v using the conditional likelihood
p=2
y=rev(yt[(p+1):T])
X=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));
XtX=t(X)%*%X
XtX_inv=solve(XtX)
phi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi
s2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v
# Obtain 200 samples from the posterior distribution under the conditional likelihood and the reference prior
n_sample=200 # posterior sample size
library(MASS)
## step 1: sample v from inverse gamma distribution
v_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)
## step 2: sample phi conditional on v from normal distribution
phi_sample=matrix(0, nrow = n_sample, ncol = p)
for(i in 1:n_sample){
phi_sample[i,]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)
}
### using spec.ar to draw spectral density based on the data assuming an AR(2)
spec.ar(yt, order = 2, main = "yt")
### using arma.spec from astsa package to draw spectral density
library("astsa")
## plot spectral density of simulated data with posterior sampled
## ar coefficients and innvovation variance
par(mfrow = c(1, 1))
result_MLE=arma.spec(ar=phi_MLE, var.noise = s2, log='yes',main = '')
#| label: ar-spectral-density
### Simulate 300 observations from an AR(2) prcess with a pair of complex-valued roots
set.seed(2021)
r=0.95
lambda=12
phi=numeric(2)
phi[1]<- 2*r*cos(2*pi/lambda)
phi[2] <- -r^2
sd=1 # innovation standard deviation
T=300 # number of time points
# sample from the AR(2) process
yt=arima.sim(n = T, model = list(ar = phi), sd = sd)
# Compute the MLE of phi and the unbiased estimator of v using the conditional likelihood
p=2
y=rev(yt[(p+1):T])
X=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));
XtX=t(X)%*%X
XtX_inv=solve(XtX)
phi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi
s2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v
# Obtain 200 samples from the posterior distribution under the conditional likelihood and the reference prior
n_sample=200 # posterior sample size
library(MASS)
## step 1: sample v from inverse gamma distribution
v_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)
## step 2: sample phi conditional on v from normal distribution
phi_sample=matrix(0, nrow = n_sample, ncol = p)
for(i in 1:n_sample){
phi_sample[i,]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)
}
### using spec.ar to draw spectral density based on the data assuming an AR(2)
spec.ar(yt, order = 2, main = "yt")
### using arma.spec from astsa package to draw spectral density
library("astsa")
## plot spectral density of simulated data with posterior sampled
## ar coefficients and innovation variance
par(mfrow = c(1, 1))
result_MLE = arma.spec(ar=phi_MLE, var.noise = s2, log='yes',main = '')
result_MLE = arma.spec(ar=phi_MLE, var.noise = s2, log='yes',main = '')
freq=result_MLE$freq
## plot spectral density of simulated data with posterior sampled
## ar coefficients and innovation variance
par(mfrow = c(1, 1))
result_MLE = arma.spec(ar=phi_MLE, var.noise = s2, log='yes',main = '')
