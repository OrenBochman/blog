from mesa import Model, Agent
from mesa.time import RandomActivation
import numpy as np
class EpsilonGreedyAgent(Agent):
"""
This agent implements the epsilon-greedy
"""
def __init__(self, unique_id, model, num_arms, epsilon=0.1):
super().__init__(unique_id,model)
self.num_arms = num_arms
self.epsilon = epsilon
self.q_values = np.zeros(num_arms)  # Initialize Q-value estimates
self.action_counts = np.zeros(num_arms)  # Track action counts
def choose_action(self):
if np.random.rand() < self.epsilon:
# Exploration: Choose random arm
return np.random.randint(0, self.num_arms)
else:
# Exploitation: Choose arm with highest Q-value
return np.argmax(self.q_values)
def step(self, model):
chosen_arm = self.choose_action()
reward = model.get_reward(chosen_arm)
assert reward is not None, "Reward is not provided by the model"
self.action_counts[chosen_arm] += 1
self.q_values[chosen_arm] = (self.q_values[chosen_arm] * self.action_counts[chosen_arm] + reward) / (self.action_counts[chosen_arm] + 1)
class TestbedModel(Model):
"""
This model represents the 10-armed bandit testbed environment.
"""
def __init__(self, num_arms, mean_reward, std_dev,num_agents=1):
super().__init__()
self.num_agents = num_agents
self.num_arms = num_arms
self.mean_reward = mean_reward
self.std_dev = std_dev
self.arms = [None] * num_arms  # List to store arm rewards
self.schedule = RandomActivation(self)
for i in range(self.num_agents):
self.create_agent(EpsilonGreedyAgent, i, 0.1)
def env_init(self,env_info={}):
self.arms = np.random.randn(self.num_arms)  # Initialize arm rewards
def create_agent(self, agent_class, agent_id, epsilon):
"""
Create an RL agent instance with the specified class and parameters.
"""
agent = agent_class(agent_id, self, self.num_arms, epsilon)
self.schedule.add(agent)
return agent
def step(self):
for agent in self.schedule.agents:
chosen_arm = agent.choose_action()
reward = np.random.normal(self.mean_reward, self.std_dev)
self.arms[chosen_arm] = reward  # Update arm reward in the model
agent.step(self)  # Pass the model instance to the agent for reward access
def get_reward(self, arm_id):
# Access reward from the stored list
return self.arms[arm_id]
# Example usage
model = TestbedModel(10, 0, 1)  # Create model with 10 arms
# Run simulation for multiple steps
for _ in range(100):
model.step()
from tqdm import tqdm
from mesa import Model, Agent
from mesa.time import RandomActivation
import numpy as np
class EpsilonGreedyAgent(Agent):
"""
This agent implements the epsilon-greedy
"""
def __init__(self, unique_id, model, num_arms, epsilon=0.1):
super().__init__(unique_id,model)
self.num_arms = num_arms
self.epsilon = epsilon
self.q_values = np.zeros(num_arms)  # Initialize Q-value estimates
self.action_counts = np.zeros(num_arms)  # Track action counts
def choose_action(self):
if np.random.rand() < self.epsilon:
# Exploration: Choose random arm
return np.random.randint(0, self.num_arms)
else:
# Exploitation: Choose arm with highest Q-value
return np.argmax(self.q_values)
def step(self, model):
chosen_arm = self.choose_action()
reward = model.get_reward(chosen_arm)
assert reward is not None, "Reward is not provided by the model"
self.action_counts[chosen_arm] += 1
self.q_values[chosen_arm] = (self.q_values[chosen_arm] * self.action_counts[chosen_arm] + reward) / (self.action_counts[chosen_arm] + 1)
class TestbedModel(Model):
"""
This model represents the 10-armed bandit testbed environment.
"""
def __init__(self, num_arms, mean_reward, std_dev,num_agents=1):
super().__init__()
self.num_agents = num_agents
self.num_arms = num_arms
self.mean_reward = mean_reward
self.std_dev = std_dev
self.env_init()
self.arms = [None] * num_arms  # List to store arm rewards
self.schedule = RandomActivation(self)
for i in range(self.num_agents):
self.create_agent(EpsilonGreedyAgent, i, 0.1)
def env_init(self,env_info={}):
self.arms = np.random.randn(self.num_arms)  # Initialize arm rewards
def create_agent(self, agent_class, agent_id, epsilon):
"""
Create an RL agent instance with the specified class and parameters.
"""
agent = agent_class(agent_id, self, self.num_arms, epsilon)
self.schedule.add(agent)
return agent
def step(self):
for agent in self.schedule.agents:
chosen_arm = agent.choose_action()
reward = np.random.normal(self.mean_reward, self.std_dev)
self.arms[chosen_arm] = reward  # Update arm reward in the model
agent.step(self)  # Pass the model instance to the agent for reward access
def get_reward(self, arm_id):
# Access reward from the stored list
return self.arms[arm_id]
# Example usage
model = TestbedModel(10, 0, 1)  # Create model with 10 arms
num_runs = 200                  # The number of times we run the experiment
num_steps = 1000                # The number of pulls of each arm the agent takes
# Run simulation for multiple steps
for _ in tqdm(range(num_runs)):
for _ in range(num_steps):
model.step()
model.step()
install.packages("rmarkdown")
renv::status()
?renv::status()
renv::restore()
install.packages(c("downlit", "xml2"))
reticulate::repl_python()
rnev::status()
renv::status()
?renv::status()
renv::install()
renv::snapshot()
