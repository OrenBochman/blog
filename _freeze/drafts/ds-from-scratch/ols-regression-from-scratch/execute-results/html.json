{
  "hash": "efc3f0adf6208b28c643c78fe28a5cb7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: OLS regression From Scratch\ndate: 2023-02-01\ncategories:\n    - data science\n    - ml\n    - algorithms\n---\n\n# OLS regression\n\nOLS regression is a method for estimating the parameters of a linear regression model. The goal is to find the line that best fits a set of data points. The line is represented by an equation of the form\n$$\ny = mx + b\n$$\n\nwhere :\n\n- $y$ is the **dependent variable**, \n- $x$ is the **independent variable**, \n- $m$ is the **slope** of the line, and \n- $b$ is the **y-intercept**.\n\n## Generate random data\n\n::: {#6a93d2a3 .cell execution_count=1}\n``` {.python .cell-code}\n# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\n\n# Generate random data\nn = 100\nx = np.random.rand(n)\ny = 2*x + np.random.normal(size=n)\n\n# Create a DataFrame and save to CSV\ndf = pd.DataFrame({'x': x, 'y': y})\ndf.to_csv('your_dataset.csv', index=False)\n```\n:::\n\n\n## \n\n::: {#6513e4a7 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom numpy import ndarray\nimport pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the data and split into independent and dependent variables\ndata = pd.read_csv('your_dataset.csv')\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\n\n# add a column of 1s to the X matrix for the intercept term\nX = np.append(arr=np.ones((len(X), 1)), values=X, axis=1)\n\n# calculate the coefficients using the OLS formula\nbeta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n\ny_pred = X.dot(beta)\n```\n:::\n\n\n::: {#f7a9f0ff .cell execution_count=3}\n``` {.python .cell-code}\ndef mean_squared_error(y_true:ndarray, y_pred:ndarray):\n    n = len(y_true)\n    mse = sum([(y_true[i] - y_pred[i])**2 for i in range(n)]) / n\n    return mse\n\ndef r2_score(y_true:ndarray, y_pred:ndarray):\n    ssr = sum([(y_true[i] - y_pred[i])**2 for i in range(len(y_true))])\n    sst = sum([(y_true[i] - np.mean(y_true))**2 for i in range(len(y_true))])\n    r2 = 1 - (ssr / sst)\n    return r2\n\nrmse = np.sqrt(mean_squared_error(y, y_pred))\nr2 = r2_score(y, y_pred)\n\nprint(\"RMSE: \", rmse)\nprint(\"R-squared: \", r2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE:  1.0241344405437345\nR-squared:  0.1878834966882431\n```\n:::\n:::\n\n\n::: {#b3465b22 .cell execution_count=4}\n``` {.python .cell-code}\nplt.scatter(X[:, 1], y, color='blue')\nplt.plot(X[:, 1], y_pred, color='red')\nplt.title('OLS Regression')\nplt.xlabel('Independent variable')\nplt.ylabel('Dependent variable')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ols-regression-from-scratch_files/figure-html/cell-5-output-1.png){width=587 height=449}\n:::\n:::\n\n\n",
    "supporting": [
      "ols-regression-from-scratch_files"
    ],
    "filters": [],
    "includes": {}
  }
}