{
  "hash": "0effcaf115b83579b151d3caf34f144f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-03-01\ntitle: Sample-based Learning Methods\nsubtitle: Monte Carlo Methods for Prediction & Control\ndescription: In these unit we define some key terms like rewards, states, action, value functions, action values functions. Then we consider at the the multi-armed bandit problem leading to exploration explotation dillema, the epsilon greedy algorithm.\nauthor: Oren Bochman\ndraft: false\ncategories:\n  - Coursera\n  - notes\n  - rl\n  - reinforcement learning\n  - Monte-Carlo methods\n  - exploring starts\n  - Off-policy learning\n  - importance sampling\n  - TD(0) algorithm\nreference-location: margin\n\n---\n\n```{html}\n<style>\n.callout-tip ul {\n    list-style-type: none; /* Remove bullets */\n    padding: 0; /* Remove padding */\n    margin: 0; /* Remove margins */\n}\n</style>\n```\n\n\n![RL algorithms](img/alg_selector.jpeg){.column-margin group=\"slides\"}\n\n::: callout-note\n## Reading {.unnumbered}\n\n-   [x] [RL BookÂ§5.0-5.5 (pp.91-104)](http://incompleteideas.net/book/RLbook2020.pdf#page=91)\n:::\n\n::: callout-note\n## Definitions {.unnumbered}\n\n::: {#dfn-action-value}\n\nAction Value Function\n\n:   $q_\\pi(a) \\doteq \\mathbb{E}[G_t \\vert A_t=a] \\space \\forall a \\in \\{a_1 ... a_k\\}$\n:::\n\n::: {#dfn-bootstrap}\n\nBootstrapping\n\n:   \"learning by guessing from a guess\" or more formally\n\n    the process of updating an estimate of the value or action-value function based on other estimated values. It involves using the current estimate of the value function to update and improve the estimate itself.\n:::\n\n::: {#dfn-control}\n\nControl\n\n:   to approximate optimal policies using the DP approach of GPI\n:::\n\n::: {#dfn-epsilon-soft}\n\nEpsilon Soft Policy\n\n:   A policy in which each possible action is assigned at least $\\epsilon / |A|$ probability.\n:::\n\n::: {#dfn-exploring-starts}\n\nExploring Starts\n\n:   Learning the value or action values of a policy by starting in each action value state at least once.\n:::\n\n::: {#dfn-mc}\n\nMonte-Carlo Methods\n\n:   Estimation methods which relies on repeated random sampling. Also see [Monte-Carlo methods <i class=\"bi bi-wikipedia\"></i>](https://en.wikipedia.org/wiki/Monte_Carlo_method)\n:::\n\n::: {#dfn-on-policy-learning}\n\nOn-policy learning\n\n:   learning a policy $\\pi$ by sampling from $\\pi$\n:::\n\n::: {#dfn-off-policy-learning}\n\nOff-policy learning\n\n:   learning a policy $\\pi$ by sampling from some other policy $\\pi'$\n:::\n\n::: {#dfn-prediction}\n\nPrediction\n\n:   Estimating $v_\\pi(s)$ is called policy evaluation in the DP literature.\n\n    We also refer to it as the Prediction problem [^1]\n:::\n\n::: {#dfn-return}\n\nReturn ($G_t$)\n\n:   $G_0 \\doteq R_1+ \\gamma^1 R_2 + \\cdots+ \\gamma^n R_n$\n\n    i.e. the discounted sum of future rewards\n:::\n\n::: {#dfn-tqabular}\n\nTabular methods\n\n:   RL methods for which the action-values can be represented by a table\n:::\n\n::: {#dfn-value}\n\nValue Function $v_\\pi(s)$\n\n:   $v_\\pi(s) \\doteq \\mathbb{E}[G_t|S_t=s]$\n\n    i.e. a state's value is its expected return\n:::\n:::\n\n[^1]: **Prediction** in the sense that we want to predict for $\\pi$ how well it will preforms i.e. its expected returns for a state\n\n-   Sample based methods learning from experience, without having prior knowledge of the underlying MDP model.\n-   We will cover tabular methods in which the action-values can be represented by a table.\n\n## Lesson 1: Introduction to Monte-Carlo Methods\n\n\n::: callout-note\n## Lesson Learning Goals {.unnumbered}\n\n-   [x] Understand how [Monte-Carlo](#dfn-mc) can be used to estimate $v(s)$ value functions from sampled interaction\n-   [x] Identify problems that can be solved using [Monte-Carlo](#dfn-mc) methods\n-   [x] Use [Monte-Carlo](#dfn-mc) prediction to estimate the value function for a given policy.\n:::\n\n\n\n-   After completing the introduction we all think that MDPs and DP are the best?\n-   Alas, Martha burst this bubble, introducing some shortcomings of DP, namely they require us to know a model of the dynamics $p(s,a|s',r)$ and rewards $r$ of the MDP to estimate $v(s)$ or $q(a)$.\n\n\n![MC methods for Policy evaluation](img/rl-mc-methods.png){.column-margin group=\"slides\"}\n\nlet us now try to understand how [Monte-Carlo](#dfn-mc) can be used to estimate $v(s)$ value functions from sampled interaction.\n\n\n![12 dice](img/rl-mc-12-dice.png){.column-margin group=\"slides\"}\n\n:::{#exm-dp-dice}\n### Rolling 12 Dice {.unnumbered}\n\n\n- Say our MDP requires rolling 12 dice.\n  - this is probably intractable to estimate theoretically using DP.\n  - this is likely to be error prone (particularly and constitutionally).\n  - this will be easy to estimate using MC methods\n\n:::\n\n-   For most MDPs knowing the dynamics and rewards is an unreasonably strong requirement.\n-   If we can treat this like a bandit problem we can try to use the long term avarages rewards to estimate value of a state\n\n![MC bandits](img/rl-mc-bandit.png){.column-margin group=\"slides\"}\n\nmore formally we can use the MC value prediction algorithm.\n\n\n![MC value prediction algorithm any visit](img/rl-mc-alg.png){.column-margin group=\"slides\"}\n\n::: {.callout-tip .alg}\n### MC prediction, for estimating $V \\approx v_\\pi$ {#alg-mc-prediction}\n\n- Input: a policy $\\pi$ to be evaluated\n- Initialize:\n  - $V(s) \\in \\mathbb{R}$, arbitrarily, $\\forall s \\in S$\n  - Returns(s) an empty list, for all $s \\in S$\n- Loop forever (for each episode):\n  - Generate an episode following $\\pi: S_0, A_0, R_1, S_1, A_1, R_2,\\ldots, S_{T-1}, A_{T-1}, R_T$\n  - $G  \\leftarrow 0$\n  - Loop for each step of episode, $t = T-1, T-2,..., 0$:\n    - $G \\leftarrow \\gamma G + R_{t+1}$\n    - ~~Unless St appears in S~0~, S~1~,...,S~t-1~:~~\n      - Append G to Returns(St)\n      - $V(S_t) \\leftarrow average(Returns(St))$\n\n:::\n\n![MC value prediction algorithm first visits](img/rl-mc-first-visit-prediction.png){.column-margin group=\"slides\"}\n\n::: {.callout-caution}\n### Any visit / First-visit\nThe book uses presents a small variation called the *first visit MC method*, We considered the any-visit case. This estimates $v_\\pi(s)$ using the average of the returns following an episode's first visit to $s$, whereas this the every-visit MC alg averages the returns following all visits to $s$ \n\nAlthough it not so clear I added the line excluding the later visits in struckout\n\n:::\n\nWhat lies behind the algorithm is the following technique for efficently computing returns.\n\n![Efficient returns calculations](img/rl-mc-calc.png){.column-margin group=\"slides\"}\n\n![incremental-update rule](img/rl-mc-incremental-updates.png){.column-margin group=\"slides\"}\n\nthis bring us to our second example:\n\n\n:::{#exm-black-jack-mdp}\n\n### Blackjack MDP\n\n![Blackjack example](img/rl-bj-example.png){.column-margin group=\"slides\"}\n\n- **Undiscounted** MDP where each game of blackjack corresponds to an episode with\n  - Rewards: \n    - r= -1 for a loss\n    - r=  0 for a draw\n    - r=  1 for a win\n  - Actions : $a\\in \\{\\text{Hit}, \\text{Stick}\\}$\n  - States S:\n    - player has a usable ace (Yes/No) ^[worth either 1 or 11]\n    - sum of cards (12-21)^[face card are worth 10]\n    - The card the dealer's card shows (Ace-10)\n  - Cards are dealt with replacement^[this is a big simplifying assumption]\n  - Policy $\\pi$: \n      - if sum < 20, stick \n      - otherwise, hit\n\n:::\n\nIn the programming assignment we will produce the following graphs\n\n\n\n![Blackjack outcomes](img/rl-bj-outcomes.png){.column-margin group=\"slides\"}\n\n![MC implications](img/rl-bj-overview.png){.column-margin group=\"slides\"}\n\n\n-   In real world settings we typical don't know theoretical functions like values, action values or rewards. Out best option is to sample reality in trial and error experiment of testing different interventions.\n-   However under certain conditions such samples may be enough to perform the [prediction task](#dfn-prediction) learn a [value function](#dfn-value) or the [action value function](#dfn-action-value) .\n-   We can these function to learn better policies from this experience.\n-   A second scenario involves historical samples collected from past interactions. We can use probabilistic methods like MCMC to estimate $q(a)$.\n\nwe can use the MC prediction alg to estimate the expected returns for a state given a policy $\\pi$\n\n::: callout-note\nThe key limitations of *MC value estimation algorithm* is its requirement for episodic tasks and for completing such an episode before it starts. In some games an episode can be very long.\n:::\n\n::: callout-note\n## :bulb: Is this really so? :thinking:\n\n-   If we work in the Bayesian paradigm with some prior and use Bayesian updating.\n-   At every step we should have well defined means.\n-   So it seems one can perhaps do sample based on non-episodic tasks\n-   One more idea is to treat n_steps as an episode.\n-   Without episodic we most likely lose the efficient updating. :thinking:\n-   Perhaps we can use the online update rule for the mean.\n:::\n\n-   [ ] TODO - try to implement this as an algorithm.\n\n-   To ensure well-defined average sample returns, we define Monte Carlo methods only on episodic tasks that all eventually terminate - only on termination are value estimates and policies updated.\n\n## Lesson 2: Monte Carlo for Control\n\n![action values ](img/rl-mc-action-values.png){.column-margin group=\"slides\"}\n\n![back off](img/rl-mc-backoff.png){.column-margin group=\"slides\"}\n\n::: callout-note\n## Lesson Learning Goals\n\n-   [ ] Estimate action-value functions using [Monte Carlo](#dfn-mc)\n-   [ ] Understand the importance of maintaining exploration in Monte Carlo algorithms\n-   [ ] Understand how to use [Monte Carlo](#dfn-mc) methods to implement a GPI algorithm.\n-   [ ] Apply [Monte Carlo](#dfn-mc) with exploring starts to solve an MDP\n:::\n\n-   Recall that [control](#def-contol) is simply improving a policy using our action values estimate.\n-   Policy improvement is done by **Greedifing** a policy $\\pi$ at a state $s$ by selecting the action $a$ with the highest action value. If we are missing some action values we can make the policy worse.\n-   We need to ensure that our RL algorithm engages the different actions of a state. There are two startegies:\n    -   Exploring starts\n    -   $\\epsilon$-Soft strategies\n\n## Lesson 3: Exploration Methods for Monte Carlo\n\n::: callout-note\n## Lesson Learning Goals {.unnumbered}\n\n-   [ ] Understand why Exploring Starts can be problematic in real problems\n-   [ ] Describe an alternative exploration method for Monte Carlo control\n:::\n\n## Lesson 4: Off-policy learning for prediction\n\n::: callout-note\n## Lesson Learning Goals {.unnumbered}\n\n-   [ ] Understand how off-policy learning can help deal with the exploration problem\n-   [ ] Produce examples of target policies and examples of behavior policies.\n-   [ ] Understand importance sampling\n-   [ ] Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution.\n-   [ ] Understand how to use importance sampling to correct returns\n-   [ ] Understand how to modify the Monte Carlo prediction algorithm for off-policy learning.\n:::\n\n$$\n\\begin{align*}\n  P(A_t, S_{t+1}, & A_{t+1}, ... ,S_T | S_t, A_{t:T-1} \\sim \\pi) \\\\\n  & = \\pi(A_t|S_t)p(S_{t+1}|S_t, A_t)\\pi(A_{t+1}, S_{t+1}) \\cdot\\cdot\\cdot p(S_T|S_{T-1}, A_{T-1}) \\\\\n  & = \\prod_{k=t}^{T-1} \\pi(A_k|S_k)p(S_{k+1}|S_k, A_k)\n\\end{align*}\n$$ {#eq-trajectory-probability}\n\n**Definition:** The importance sampling ratio (rho, $\\rho$) is the relative probability of the trajectory under the target vs behavior policy:\n\n$$\n\\begin{align}\n\\rho_{t:T-1} & \\doteq \\frac{\\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k)p(S_{k+1} \\mid S_k, A_k)}{\\prod_{k=t}^{T-1} b(A_k \\mid S_k)p(S_{k+1} \\mid S_k, A_k)} \\\\\n             & = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k \\mid S_k)}{b(A_k \\mid S_k)}\n\\end{align}\n$$ {#eq-importance-sampling}\n\n$$\nv_\\pi(s) = \\mathbb{E}_b[\\rho_{t:T-1} \\cdot G_t \\mid S_t = s] \\qquad\n$$ {#eq-value1}\n\n$$\nV(s) \\doteq \\frac{\\displaystyle \\sum_{t\\in \\mathscr T(s)}\\rho_{t:T(t) - 1} \\cdot G_t}{|\\mathscr T (s)|} \\qquad\n$$ {#eq-value2}\n\n$$\nV(s) \\doteq \\frac{\\displaystyle \\sum_{t\\in \\mathscr T(s)} \\Big(\\rho_{t:T(t) - 1} \\cdot G_t\\Big)}{\\displaystyle \\sum_{t\\in \\mathscr T(s)}\\rho_{t:T(t) - 1}} \\qquad\n$$ {#eq-value3}\n\n",
    "supporting": [
      "c2-w1_files"
    ],
    "filters": [],
    "includes": {}
  }
}