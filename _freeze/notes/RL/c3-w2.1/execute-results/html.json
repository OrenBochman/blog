{
  "hash": "2fefa2e280fc8edf8da85572132f9569",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-04-02\nlastmod: 2024-04-02\ntitle: Constructing Features for Prediction\nsubtitle: Prediction and Control with Function Approximation\nauthor: Oren Bochman\ndraft: false\ncategories:\n  - Coursera\n  - notes\n  - rl\n  - reinforcement learning\nkeywords:\n  - reinforcement learning\n  - neural networks\n  - feature construction\n  - tile coding\n  - coarse coding\n  - feed-forward architecture\n  - activation functions\n  - deep networks\n  - gradient\n  - online setting\n  - offline setting\n  - representation\nimage: /images/nlp-brain-wordcloud.jpg\ntitle-block-banner: /images/banner_black_3.jpg\n---\n\n\n![RL algorithms](img/alg_selector.jpeg){.column-margin}\n\n# Introduction\n\n::: {.callout-tip collapse=\"true\"}\n### Readings {.unnumbered}\n\n-   [x] [@sutton2018reinforcement§9.4-9.5.0, pp. 204-210] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n-   [x] [@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n-   [x] [@sutton2018reinforcement§9.7, pp. 223-228] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n\n:::\n\n\nIn this module I wish to cover some material from the chapter 9 of the book by Sutton and Barto. The bulk of the material is covered in  the notes. \n\nIn this module we consider environments that allow to consider simple state aggregationI'd like to cover the following topics:\n\n1. create gymnasium compatible environment for the 1000 step Random Walk environment. This will allow to use it with many RL agents built using many libraries. I'd also want to extend this environment to include a neighborhood size parameter - this will set the bound how far a left or right step will move us. A second extension is to add a dimension parameter to control dimensions of the state space. This can allow us to consider state aggregation in one two and three dimensions.\n2. implement Gradient MC agent\n3. implement Semi Gradient TD(0) agent\n4. use these agents with and without state aggregation to learn the value function of the Random Walk 1000 environment.\n5. implement coarse coding via tile coding to create features for the Random Walk 1000 environment.\n6. implement use of **polynomial features** to create features for the Random Walk 1000 environment.\n7. implement use of **radial basis functions** to create features for the Random Walk 1000 environment.\n8. implement use of **Fourier basis functions** to create features for the Random Walk 1000 environment.\n9. implement use of \n\nIn this chapter we use a simple environment called the 1000 state Random walk. I implemented this independently in Python. \n\nWe also learned  the MC prediction algorithm and the TD(0) algorithms for function approximation.\n We will use these algorithms to learn the value function of the Random Walk 1000 environment. We will also use tile coding and coarse coding to create features for the Random Walk 1000 environment.\n\n# Feature Construction & linear Function approximation\n\n::: callout-note\n### Learning Objectives {.unnumbered}\n\n:::\n\nIn this lesson we use the Random Walk 1000 environment to demonstrate how to construct features for linear methods. We will use tile coding and coarse coding to create features for the Random Walk 1000 environment.\n\n::: {#9dbc8e53 .cell execution_count=2}\n``` {.python .cell-code}\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass RandomWalk1000(gym.Env):\n    def __init__(self, num_states=1000, neighborhood_size=100, seed=None):\n        super().__init__()\n        self.num_states = num_states\n        self.neighborhood_size = neighborhood_size\n        self.observation_space = spaces.Discrete(num_states + 2) # add two states 0 and num_states + 1 as terminal states\n        self.action_space = spaces.Discrete(2)  # 0 for left, 1 for right\n        self.current_state = 500 # start in the middle\n        self.np_random, seed = gym.utils.seeding.np_random(seed)\n        self.trajectory = [500]\n\n    def reset(self, *, seed=None, options=None):\n        super().reset(seed=seed)\n        self.current_state = 500\n        self.trajectory = [500]\n        return self.current_state, {}\n\n    def step(self, action):\n\n        if action == 0: # move left\n             # left neighbours\n            left_start = max(1, self.current_state - self.neighborhood_size)\n            left_end = self.current_state\n            num_left = left_end - left_start\n\n            if left_start == 1:\n                prob_terminate_left = (self.neighborhood_size - num_left) / self.neighborhood_size\n            else:\n                prob_terminate_left = 0\n            \n            if self.np_random.random() < prob_terminate_left:\n               \n                return 0, -1, True, False, {} # terminate left\n\n            next_state = self.np_random.integers(low=left_start, high=left_end)\n\n\n        elif action == 1: # move right\n             # right neighbours\n            right_start = self.current_state + 1\n            right_end = min(self.num_states + 1, self.current_state + self.neighborhood_size + 1)\n            num_right = right_end - right_start\n            if right_end == self.num_states + 1:\n                 prob_terminate_right = (self.neighborhood_size - num_right) / self.neighborhood_size\n            else:\n                prob_terminate_right = 0\n            \n            if self.np_random.random() < prob_terminate_right:\n\n                return self.num_states + 1, 1, True, False, {} # terminate right\n\n            next_state = self.np_random.integers(low=right_start, high=right_end)\n        else:\n            raise ValueError(\"Invalid action\")\n\n        self.current_state = next_state\n\n        self.trajectory.append(self.current_state)\n        return self.current_state, 0, False, False, {} # not terminated or truncated\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_trajectory(trajectory, num_states):\n    \"\"\"Plots the trajectory of the random walk.\"\"\"\n    x = np.arange(len(trajectory))\n    y = np.array(trajectory)\n    \n    plt.figure(figsize=(12, 4))\n    plt.plot(x, y, marker='o', linestyle='-', markersize=3)\n    plt.xlabel('Time Step')\n    plt.ylabel('State')\n    plt.title('Random Walk Trajectory')\n    plt.yticks(np.arange(0, num_states+2, 100))\n    plt.grid(axis='y')\n\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\n::: {#6ea74f94 .cell execution_count=3}\n``` {.python .cell-code}\n#import gymnasium as gym\n#from random_walk_gym import RandomWalk1000\n\nenv = RandomWalk1000()\n\n# Reset the env\nobs, info = env.reset()\nterminated = False\n\nwhile not terminated:\n    # For this environment, an action is not needed.\n    # Here we pass in a dummy value\n    obs, reward, terminated, truncated, info = env.step(0)\n    print(f\"State: {obs + 1}, Reward: {reward}, Terminated: {terminated}\")\n\nenv.close()\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nState: 444, Reward: 0, Terminated: False\nState: 379, Reward: 0, Terminated: False\nState: 287, Reward: 0, Terminated: False\nState: 265, Reward: 0, Terminated: False\nState: 251, Reward: 0, Terminated: False\nState: 187, Reward: 0, Terminated: False\nState: 95, Reward: 0, Terminated: False\nState: 62, Reward: 0, Terminated: False\nState: 1, Reward: -1, Terminated: True\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](c3-w2.1_files/figure-html/cell-3-output-2.png){width=1142 height=374}\n:::\n:::\n\n\n::: {#b7645717 .cell execution_count=4}\n``` {.python .cell-code}\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\ntrajectory = []\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n    print(f'{obs=}, {action=}, {reward=}, {terminated=}')\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nobs=440, action=0, reward=0, terminated=False\nobs=503, action=1, reward=0, terminated=False\nobs=521, action=1, reward=0, terminated=False\nobs=448, action=0, reward=0, terminated=False\nobs=435, action=0, reward=0, terminated=False\nobs=531, action=1, reward=0, terminated=False\nobs=512, action=0, reward=0, terminated=False\nobs=474, action=0, reward=0, terminated=False\nobs=556, action=1, reward=0, terminated=False\nobs=507, action=0, reward=0, terminated=False\nobs=433, action=0, reward=0, terminated=False\nobs=469, action=1, reward=0, terminated=False\nobs=410, action=0, reward=0, terminated=False\nobs=311, action=0, reward=0, terminated=False\nobs=299, action=0, reward=0, terminated=False\nobs=358, action=1, reward=0, terminated=False\nobs=385, action=1, reward=0, terminated=False\nobs=390, action=1, reward=0, terminated=False\nobs=485, action=1, reward=0, terminated=False\nobs=492, action=1, reward=0, terminated=False\nobs=556, action=1, reward=0, terminated=False\nobs=631, action=1, reward=0, terminated=False\nobs=627, action=0, reward=0, terminated=False\nobs=652, action=1, reward=0, terminated=False\nobs=696, action=1, reward=0, terminated=False\nobs=756, action=1, reward=0, terminated=False\nobs=828, action=1, reward=0, terminated=False\nobs=846, action=1, reward=0, terminated=False\nobs=772, action=0, reward=0, terminated=False\nobs=693, action=0, reward=0, terminated=False\nobs=680, action=0, reward=0, terminated=False\nobs=748, action=1, reward=0, terminated=False\nobs=756, action=1, reward=0, terminated=False\nobs=755, action=0, reward=0, terminated=False\nobs=793, action=1, reward=0, terminated=False\nobs=798, action=1, reward=0, terminated=False\nobs=871, action=1, reward=0, terminated=False\nobs=945, action=1, reward=0, terminated=False\nobs=849, action=0, reward=0, terminated=False\nobs=891, action=1, reward=0, terminated=False\nobs=850, action=0, reward=0, terminated=False\nobs=778, action=0, reward=0, terminated=False\nobs=720, action=0, reward=0, terminated=False\nobs=773, action=1, reward=0, terminated=False\nobs=856, action=1, reward=0, terminated=False\nobs=937, action=1, reward=0, terminated=False\nobs=984, action=1, reward=0, terminated=False\nobs=947, action=0, reward=0, terminated=False\nobs=983, action=1, reward=0, terminated=False\nobs=971, action=0, reward=0, terminated=False\nobs=946, action=0, reward=0, terminated=False\nobs=1001, action=1, reward=1, terminated=True\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](c3-w2.1_files/figure-html/cell-4-output-2.png){width=1142 height=374}\n:::\n:::\n\n\nLets simulate the random walk till success and plot its trajectory.\n\n::: {#3a7f8348 .cell execution_count=5}\n``` {.python .cell-code}\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n```\n\n::: {.cell-output .cell-output-display}\n![](c3-w2.1_files/figure-html/cell-5-output-1.png){width=1142 height=374}\n:::\n:::\n\n\n",
    "supporting": [
      "c3-w2.1_files"
    ],
    "filters": [],
    "includes": {}
  }
}