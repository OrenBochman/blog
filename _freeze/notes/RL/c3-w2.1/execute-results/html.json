{
  "hash": "08f19749ae8dd91bebb00b2fa9cd8a1b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-04-02\nlastmod: 2024-04-02\ntitle: Constructing Features for Prediction\nsubtitle: Prediction and Control with Function Approximation\nauthor: Oren Bochman\ndraft: false\ncategories:\n  - Coursera\n  - notes\n  - rl\n  - reinforcement learning\nkeywords:\n  - reinforcement learning\n  - neural networks\n  - feature construction\n  - tile coding\n  - coarse coding\n  - feed-forward architecture\n  - activation functions\n  - deep networks\n  - gradient\n  - online setting\n  - offline setting\n  - representation\nimage: /images/nlp-brain-wordcloud.jpg\ntitle-block-banner: /images/banner_black_3.jpg\n---\n\n\n![RL algorithms](img/alg_selector.jpeg){.column-margin}\n\n# Introduction\n\n::: {.callout-tip collapse=\"true\"}\n### Readings {.unnumbered}\n\n-   [x] [@sutton2018reinforcement§9.4-9.5.0, pp. 204-210] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n-   [x] [@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n-   [x] [@sutton2018reinforcement§9.7, pp. 223-228] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n\n:::\n\nThis is not a video lecture or notes for a learning goal. This is however my attempts to cover some material from the readings from chapter 9 of [@sutton2018reinforcement] mentioned above.\n\nI've added this material about a year after completing the specialization as I have been taking a course on deep reinforcement learning. I had felt that the material in this course had been both  challenging on occasion and rather basic on others. \n\nWhat I felt was that I was not happy about the basics in this chapter. This includes the parameterization of the value function, the convergence results regarding linear function approximation. The ideas about why TD is a semi-gradient method etc. \n\nI am also having many idea about both creating algorithms for creating features for RL environments. As I get familiar with gridwolds, atari games, sokoban etc I had many good ideas for making progress in both these environments and for improving algorithms for more general cases. \n\nFor example it appears that in many papers it turns out the agents are not learning very basic abilities. My DQN agent for space invaders was very poor at shooting bullets. I had an few ideas that should make a big difference. Like adding features for bullets,  the invaders and so on. This are kind of challenging to implement in the current gymnasium environments. However I soon had a much more interesting idea that seems to be good for many of the atari environments and quite possibly even more broadly to most cnn based agents. \n\n- In brief this would combine \n    - a multiframe YOLO, \n    - a generelised value function to replace YOLO's supervision \n    - a 'robust' causal attention mechanism to decide which pixels are more or less important \n        - dropping them would not impact performance. e.g. bullets\n        - which affect survival e.g. certain bullets\n        - for scoring e.g. mother ship \n        - which ones we can influence e.g. standing under an invader gets it to shoot at us.\n    \nNote this is not the causal attention mechanism from NLP where one censored the future inputs but rather a mechanism that decides which pixels represent features that are potentialy the cause of the future states.\n\nClearly this Algorithm and its parts need to be worked out in a very simple environment. The YOLO part is all about modeling features using bounding boxes via a single pass. The GVFs are to replace yolo supervision loss with a RL compatible loss and the causal attention to reduce the overfitting and speed up learning.\n\nI decided that converting some of these simple environments to gymnasium environments would be a good way to kick start some of these ideas, more so as reviewing papers and talks by Adam and Martha White shows that most experiments in RL environments turn out to be too complicated and access to simple environments turns out to be the way to get the experiments started.\n\n\nIn this chapter we use a simple environment called the 1000 state Random walk. I implemented this independently in Python. \n\nWe also learned  the MC prediction algorithm and the TD(0) algorithms for function approximation.\n\nWe will use these algorithms to learn the value function of the Random Walk 1000 environment. We will also use tile coding and coarse coding to create features for the Random Walk 1000 environment.\n\n# Feature Construction & linear Function approximation\n\n::: callout-note\n### Learning Objectives {.unnumbered}\n\n\n\nIn this module we consider environments that allow to consider simple state aggregationI'd like to cover the following topics:\n\n1. [x] create gymnasium compatible environment for the 1000 step Random Walk environment. \n    - This will allow to use it with many RL agents built using many libraries. \n    - [x] I'd also want to extend this environment to include a neighborhood size parameter - this will set the bound how far a left or right step will move us. \n    - [x] A dimension parameter to control dimensions of the state space. This can allow us to consider state aggregation in one two and three dimensions.\n2. [x] Plot the trajectory of the random walk.\n3. [ ] Implement the function approximation environment for gymnasium.\n    - [ ] extend to looking at interpolation and regression problems in the context of RL. (How - using a different dataset loaded by pandas?) Time series, Tabular data, Clustering, Classification, Regression, Pricing with elasticity. Multi-dimensional data.\n    - This environment can be a basis for looking at how Temporal Abstraction aggregation play with function approximation in a highly simplified form. \n    - This is a fundamental issue that [Doina Precup](https://www.youtube.com/watch?v=GntIVgNKkCI) has raised in her talks as an ongoing area of research. \n    - So such an environment might be useful in testing how different approaches can handle these issues in an environment that is very close to supervised learning. \n2. implement Gradient MC agent\n3. implement Semi Gradient TD(0) agent\n4. use these agents with and without state aggregation to learn the value function of the Random Walk 1000 environment.\n5. implement coarse coding via tile coding to create features for the Random Walk 1000 environment.\n6. implement use of **polynomial features** to create features for the Random Walk 1000 environment.\n7. implement use of **radial basis functions** to create features for the Random Walk 1000 environment.\n8. implement use of **Fourier basis functions** to create features for the Random Walk 1000 environment.\n\n:::\n\n## The 1000 Step Random Walk Environment \n\nIn this lesson we implement the 1000 Random Walk example as an environment. This is good to demonstrate how to construct features for linear methods. We will use tile coding and coarse coding to create features for the Random Walk 1000 environment.\n\n::: {#9dbc8e53 .cell execution_count=2}\n``` {.python .cell-code}\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass RandomWalk1000(gym.Env):\n    def __init__(self, num_states=1000, neighborhood_size=100, seed=None):\n        super().__init__()\n        self.num_states = num_states\n        self.neighborhood_size = neighborhood_size\n        self.observation_space = spaces.Discrete(num_states + 2) # add two states 0 and num_states + 1 as terminal states\n        self.action_space = spaces.Discrete(2)  # 0 for left, 1 for right\n        self.current_state = 500 # start in the middle\n        self.np_random, seed = gym.utils.seeding.np_random(seed)\n        self.trajectory = [500]\n\n    def reset(self, *, seed=None, options=None):\n        super().reset(seed=seed)\n        self.current_state = 500\n        self.trajectory = [500]\n        return self.current_state, {}\n\n    def step(self, action):\n\n        if action == 0: # move left\n             # left neighbours\n            left_start = max(1, self.current_state - self.neighborhood_size)\n            left_end = self.current_state\n            num_left = left_end - left_start\n\n            if left_start == 1:\n                prob_terminate_left = (self.neighborhood_size - num_left) / self.neighborhood_size\n            else:\n                prob_terminate_left = 0\n            \n            if self.np_random.random() < prob_terminate_left:\n               \n                return 0, -1, True, False, {} # terminate left\n\n            next_state = self.np_random.integers(low=left_start, high=left_end)\n\n\n        elif action == 1: # move right\n             # right neighbours\n            right_start = self.current_state + 1\n            right_end = min(self.num_states + 1, self.current_state + self.neighborhood_size + 1)\n            num_right = right_end - right_start\n            if right_end == self.num_states + 1:\n                 prob_terminate_right = (self.neighborhood_size - num_right) / self.neighborhood_size\n            else:\n                prob_terminate_right = 0\n            \n            if self.np_random.random() < prob_terminate_right:\n\n                return self.num_states + 1, 1, True, False, {} # terminate right\n\n            next_state = self.np_random.integers(low=right_start, high=right_end)\n        else:\n            raise ValueError(\"Invalid action\")\n\n        self.current_state = next_state\n\n        self.trajectory.append(self.current_state)\n        return self.current_state, 0, False, False, {} # not terminated or truncated\n\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_trajectory(trajectory, num_states):\n    \"\"\"Plots the trajectory of the random walk.\"\"\"\n    x = np.arange(len(trajectory))\n    y = np.array(trajectory)\n    \n    plt.figure(figsize=(12, 4))\n    plt.plot(x, y, marker='o', linestyle='-', markersize=3)\n    plt.xlabel('Time Step')\n    plt.ylabel('State')\n    plt.title('Random Walk Trajectory')\n    plt.yticks(np.arange(0, num_states+2, 100))\n    plt.grid(axis='y')\n\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\n::: {#6ea74f94 .cell execution_count=3}\n``` {.python .cell-code}\n#import gymnasium as gym\n#from random_walk_gym import RandomWalk1000\n\nenv = RandomWalk1000()\n\n# Reset the env\nobs, info = env.reset()\nterminated = False\n\nwhile not terminated:\n    # For this environment, an action is not needed.\n    # Here we pass in a dummy value\n    obs, reward, terminated, truncated, info = env.step(0)\n    print(f\"State: {obs + 1}, Reward: {reward}, Terminated: {terminated}\")\n\nenv.close()\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nState: 444, Reward: 0, Terminated: False\nState: 379, Reward: 0, Terminated: False\nState: 287, Reward: 0, Terminated: False\nState: 265, Reward: 0, Terminated: False\nState: 251, Reward: 0, Terminated: False\nState: 187, Reward: 0, Terminated: False\nState: 95, Reward: 0, Terminated: False\nState: 62, Reward: 0, Terminated: False\nState: 1, Reward: -1, Terminated: True\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](c3-w2.1_files/figure-html/cell-3-output-2.png){width=1142 height=374}\n:::\n:::\n\n\n::: {#b7645717 .cell execution_count=4}\n``` {.python .cell-code}\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\ntrajectory = []\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n    print(f'{obs=}, {action=}, {reward=}, {terminated=}')\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nobs=440, action=0, reward=0, terminated=False\nobs=503, action=1, reward=0, terminated=False\nobs=521, action=1, reward=0, terminated=False\nobs=448, action=0, reward=0, terminated=False\nobs=435, action=0, reward=0, terminated=False\nobs=531, action=1, reward=0, terminated=False\nobs=512, action=0, reward=0, terminated=False\nobs=474, action=0, reward=0, terminated=False\nobs=556, action=1, reward=0, terminated=False\nobs=507, action=0, reward=0, terminated=False\nobs=433, action=0, reward=0, terminated=False\nobs=469, action=1, reward=0, terminated=False\nobs=410, action=0, reward=0, terminated=False\nobs=311, action=0, reward=0, terminated=False\nobs=299, action=0, reward=0, terminated=False\nobs=358, action=1, reward=0, terminated=False\nobs=385, action=1, reward=0, terminated=False\nobs=390, action=1, reward=0, terminated=False\nobs=485, action=1, reward=0, terminated=False\nobs=492, action=1, reward=0, terminated=False\nobs=556, action=1, reward=0, terminated=False\nobs=631, action=1, reward=0, terminated=False\nobs=627, action=0, reward=0, terminated=False\nobs=652, action=1, reward=0, terminated=False\nobs=696, action=1, reward=0, terminated=False\nobs=756, action=1, reward=0, terminated=False\nobs=828, action=1, reward=0, terminated=False\nobs=846, action=1, reward=0, terminated=False\nobs=772, action=0, reward=0, terminated=False\nobs=693, action=0, reward=0, terminated=False\nobs=680, action=0, reward=0, terminated=False\nobs=748, action=1, reward=0, terminated=False\nobs=756, action=1, reward=0, terminated=False\nobs=755, action=0, reward=0, terminated=False\nobs=793, action=1, reward=0, terminated=False\nobs=798, action=1, reward=0, terminated=False\nobs=871, action=1, reward=0, terminated=False\nobs=945, action=1, reward=0, terminated=False\nobs=849, action=0, reward=0, terminated=False\nobs=891, action=1, reward=0, terminated=False\nobs=850, action=0, reward=0, terminated=False\nobs=778, action=0, reward=0, terminated=False\nobs=720, action=0, reward=0, terminated=False\nobs=773, action=1, reward=0, terminated=False\nobs=856, action=1, reward=0, terminated=False\nobs=937, action=1, reward=0, terminated=False\nobs=984, action=1, reward=0, terminated=False\nobs=947, action=0, reward=0, terminated=False\nobs=983, action=1, reward=0, terminated=False\nobs=971, action=0, reward=0, terminated=False\nobs=946, action=0, reward=0, terminated=False\nobs=1001, action=1, reward=1, terminated=True\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](c3-w2.1_files/figure-html/cell-4-output-2.png){width=1142 height=374}\n:::\n:::\n\n\nLets simulate the random walk till success and plot its trajectory.\n\n::: {#3a7f8348 .cell execution_count=5}\n``` {.python .cell-code}\nenv = RandomWalk1000(num_states=1000, neighborhood_size=100)\nobs, info = env.reset()\nterminated = False\ntruncated = False\nwhile not terminated and not truncated:\n    action = env.action_space.sample()  # Replace with your policy\n    obs, reward, terminated, truncated, info = env.step(action)\n\n\nplot_trajectory(env.trajectory, num_states=env.num_states)\n```\n\n::: {.cell-output .cell-output-display}\n![](c3-w2.1_files/figure-html/cell-5-output-1.png){width=1142 height=374}\n:::\n:::\n\n\n",
    "supporting": [
      "c3-w2.1_files"
    ],
    "filters": [],
    "includes": {}
  }
}