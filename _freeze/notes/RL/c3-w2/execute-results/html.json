{
  "hash": "0caab56157eefc844186a166c8a32b6b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-04-02\nlastmod: 2024-04-02\ntitle: Constructing Features for Prediction\nsubtitle: Prediction and Control with Function Approximation\nauthor: Oren Bochman\ndraft: false\ncategories:\n  - Coursera\n  - notes\n  - rl\n  - reinforcement learning\nkeywords:\n  - reinforcement learning\n  - neural networks\n  - feature construction\n  - tile coding\n  - coarse coding\n  - feed-forward architecture\n  - activation functions\n  - deep networks\n  - gradient\n  - online setting\n  - offline setting\n  - representation\nimage: /images/nlp-brain-wordcloud.jpg\ntitle-block-banner: /images/banner_black_3.jpg\n---\n\n\n![RL algorithms](img/alg_selector.jpeg){.column-margin}\n\n# Introduction\n\n::: {.callout-tip collapse=\"true\"}\n### Readings {.unnumbered}\n\n-   [x] [@sutton2018reinforcement§9.4-9.5.0, pp. 204-210] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n-   [x] [@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n-   [x] [@sutton2018reinforcement§9.7, pp. 223-228] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n\n:::\n\n# Lesson 1: Feature Construction for Linear Methods \n\n::: callout-note\n### Learning Objectives {.unnumbered}\n\n-   [x] *Define* the difference between **coarse coding** and tabular representations [\\#](#sec-l1g1)\n-   [x] *Explain* the trade-off when designing representations between discrimination and generalization [\\#](#sec-l1g2)\n-   [x] *Understand* how different coarse coding schemes affect the functions that can be represented [\\#](#sec-l1g3)\n-   [x] *Explain* how tile coding is a (computationally?) convenient case of coarse coding [\\#](#sec-l1g4)\n-   [x] *Describe* how designing the tilings affects the resultant representation [\\#](#sec-l1g5)\n-   [x] *Understand* that tile coding is a computationally efficient implementation of coarse coding [\\#](#sec-l1g6)\n\n:::\n\n\n## Coarse Coding (Video)\n\nIn this video, Adam White introduces the concept of **coarse coding**, covering the first learning objective of this lesson.\n\nCoarse coding are a way to represent states in a more general way than tabular representations. This allows for generalization across states. The trade-off is that the representation is less discriminative.\n\n\n## The difference between **coarse coding** and tabular representations {#sec-l1g1}\n\n\n![approximation](img/rl-coding_states.png){.column-margin}\n\nRecall that linear function approximation are paramertized by a weight vector $\\mathbf{w}$ and a feature vector $\\mathbf{x}(s)$. \n\nAs we saw in the previous unit tabular representations associates one feature per state, this is called a one-hot encoding of the state space.\n\n![one hot coding](img/rl-tabular_coding.png){.column-margin}\n\n We associate one hot encoding with an indicator function $\\delta_{ij}(s)$. This is a very discriminative representation but it does generalize.\n\n![state aggregation](img/rl-state-aggregation.png){.column-margin}\n\n\nWe also discussed using **state aggregation** for the 1000 state random walk example.\nIn state aggregation we break the continuous state space into discrete regions and associate a feature with each region. This is a more general representation than tabular representations but less discriminative. \n\n![coarse coding](img/rl-coarse-coding.png){.column-margin}\n\n**Coarse coding** uses multiple overlapping shapes to represent states. This is a more General representation than state aggregation but less discriminative. Features are the circles they are in. If the circles overlap, we can have items that are in multiple circles. I.e. they are characterized by multiple features. In the example shown there can be from one to three active features. \n\nSo the difference is that tabular representations are one hot encodings while coarse coding uses membership in multiple overlapping shapes to represent states.\n\nHow does coarse coding relates to state aggregation?\n\nCoarse coding is also a generalization of state aggregation. In state aggregation we break the state space into discrete regions and associate a feature with each region. But we don't let these regions overlap. In coarse coding we allow the regions to overlap which can give greater generalization as regions can share features.\n\nIn this video the term Reception Field is used to describe the region of the state space that a feature is associated with. This is an idea that comes from CNNs.\n\n\n## Generalization Properties of Coarse Coding (Video)\n\nIn this video Martha White discusses the generalization properties of coarse coding.\n\nShe looks at using small overlapping 1-d intervals to represent a 1-d function.\n\nWe see that changing shape size and number of effects the generalization properties of the representation.\n\n![scale](rl-scale-generalization.png){.column-margin}\n\n![shape](rl-shape-generalization.png){.column-margin}\n\n![discrimination](rl-shape-discrimination.png){.column-margin}\n\n\nNext we looked at using short interval vs longer intervals to approximate a 1-d function. We see that the longer intervals give a smoother approximation.\n\n\n## The trade-off between discrimination and generalization {#sec-l1g2}\n\n## Tile Coding  (Video)\n\nIn this video, Martha White introduces the concept of **tile coding**. This is simply a implementation of coarse coding using multiple overlapping grids.\n\n## Explain how tile coding is a (computationally?) convenient case of coarse coding  {#sec-l1g4}\n\nTile coding is a computationally efficient implementation of coarse coding. We use multiple overlapping tilings to represent states. Each tiling is a grid of tiles. Each tile is a feature.\n\nIf we use one tiling we get state aggregation. If we use multiple tilings we get coarse coding. One tiling means we don't discriminate between states that are in the same tile. Multiple tilings means we can discriminate between states that are in the same tile in one tiling but not in another.\n\n## Describe how designing the tilings affects the resultant representation {#sec-l1g5}\n\nThe textbook goes into some more details about how we can generelize using tile coding - using regular tilings generates in a diagonal pattern. Using random tilings generates more spherical regions.\n\nHowever we also saw that the number size and shape of the tiles affects the generalization properties of the representation. And that increasing the overlap between the tiles an increase the discrimination properties of the representation.\n\n## *Understand* that tile coding is a computationally efficient implementation of coarse coding {#sec-l1g6}\n\n\nTile coding is a computationally efficient implementation of coarse coding. Since grids are uniform it is easy to compute which cells a state is in. A second  reason is that end up with a sparse representations thus the dot product is just the sum of the weights of the active features for each state.\n\nOne caveat is that in high dimensional spaces we end up an exponential number of features. This is called the curse of dimensionality.\n\n\n## Using Tile Coding in TD (Video)\n\nIn this video, Adam White shows how to use tile coding in TD learning.\nHe goes back to the 1000 state random walk example and shows how to use tile coding to approximate the value function. We end up needing  six tiles. \n\n![tile coding v.s. state aggregation](img/rl-tile-coding-performance.png){.column-margin}\n\n\n## Other Forms of Coarse Coding\n\n\nIn the textbook we see that there are other forms of coarse coding. \n\n\nFor example in section 9.5.5 we see using radial basis functions. \n\nAn RBF\n: is a real-valued function whose value depends only on the distance between the input and a fixed point (called the center).   \n\nVisualizing - Imagine a hill or bump centered at a specific point. The height of the hill at any other point depends solely on its distance from the center. The hill gets flatter as you move away from the cente\n\n![one dimensional radial basis functions](rl-radial-basis-functions.png){.column-margin}\n$$\nx_i(s) = \\exp\\left(-\\frac{\\|s-c_i\\|^2}{2\\sigma_i^2}\\right)\n$$\n\n- Where \n- $c_i$ is the center of the radial basis function and \n- $\\sigma_i$ is the width.\n\nThis is a form of coarse coding where the features are the distance from a set of centers. This is a more general representation than tile coding but less discriminative. The advantage of RBFs over tiles is that they are approximate functions that vary smoothly and are differentiable. However it appears there is both a computational cost and no real advantage in having continuous/differential features according to the book.\n\n::: {#cell-fig-radial-basis-functions .cell .column-margin fig.height='3' fig.width='3' execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![One-dimensional radial basis functions with centers at -2, 0, and 2.](c3-w2_files/figure-html/fig-radial-basis-functions-output-1.png){#fig-radial-basis-functions width=662 height=470}\n:::\n:::\n\n\nI find this a bit disappointing as it seems like a nice intermediate step between linear function approximation with its convergence guarantees and neural networks which have no such guarantees.\n\n# Lesson 2: Neural Networks \n\n::: callout-note\n### Learning Objectives {.unnumbered}\n\n-   [x] *Define* a neural network [\\#](#sec-l2g1)\n-   [x] *Define* activation functions [\\#](#sec-l2g2)\n-   [x] *Define* a feed-forward architecture [\\#](#sec-l2g3)\n-   [x] *Understand* how neural networks are doing feature construction [\\#](#sec-l2g4)\n-   [x] *Understand* how neural networks are a non-linear function of state [\\#](#sec-l2g5)\n-   [x] *Understand* how deep networks are a composition of layers [\\#](#sec-l2g6)\n-   [x] *Understand* the tradeoff between learning capacity and challenges presented by deeper networks [\\#](#sec-l2g7)\n\n:::\n\n\n## What is a Neural Network? (Video)\n\nIn this video, Martha White introduces the concept of a neural network. We look at a simple one layer feed forward  neural network. Where the $output=f(sW)$ is a non-linear function of the input. \n\n\n## Define a neural network {#sec-l2g1}\n\n![](img/rl-feedforward-nn.png){.column-margin}\n\nA Neural network consists of a network of nodes which process and pass on information. \n\n- The circles are the noes\n- The lines are the connections\n- The nodes are organized in layers\n\nData starts at the input layer. It is passed through the connections to the hidden layer. The hidden layer is preforms some computation on the data and passes it to the output layer. This process repeats until the last layer produces the output of the network.\n\n\n## Neural Networks Mechanics\n\nA node in the network is a function\n\n$$\noutput = f[(w_1 \\times input_1) + (w_2 \\times input_2) + \\ldots + (w_n \\times input_n) + b]\n$$\n\n- where: \n  - $w_i$ are the weights, \n  - $input_i$ are the inputs, and \n  - $b$ is the bias.\n  - $f$ is the activation function.\n\nThe sum of the product of the weights and inputs is a linear operation. The activation function $f$ is where a non-linearity is introduced into the network.\n\n\n## Define activation functions {#sec-l2g2}\n\nActivation functions are non-linear functions that are applied to the output of a node. They introduce non-linearity into the network.\n\n![tanh activation](img/rl-activation-functions-tanh.png){.column-margin}\n\n![rectified linear activation function](img/rl-activation-functions-relu.png){.column-margin}\n\nMartha White also mentions threshold activation functions. However these are not used in practice as they are not differentiable. There is some work since this course came out on compressing neural networks to use threshold activation functions which are easy to compute on a CPU as matrix multiplication becomes a series of comparisons. However these are trained with a differentiable approximation of the threshold function and then quantized to the threshold function.\n\n## The Neural Network Implementation\n\n![Neural Network Implementation](img/rl-nn-implementation.png){.column-margin}\n\nA neural network is a parameterized function that is a composition of linear and non-linear functions. It is a function of the state. The linear functions are the weights and the non-linear functions are the activation functions. The weights are learned from data.\n\n## Define a feed-forward architecture {#sec-l2g3}\n\nA feed forward architecture is a neural network where the connections between nodes do not form a cycle. The data flows from the input layer to the output layer.\n\nAn example of a non-feed forward architecture is a recurrent neural network where the connections between nodes form cycles.\n\n\n\n\n## How neural networks are doing feature construction {#sec-l2g4}\n\n## Non-linear Approximation with Neural Networks (Video)\n\n## How neural networks are a non-linear function of state {#sec-l2g5}\n\n## Deep Neural Networks (Video)\n\n## How deep networks are a composition of layers {#sec-l2g6}\n\n## The tradeoff between learning capacity and challenges presented by deeper networks {#sec-l2g7}\n\n\n# Lesson 3: Training Neural Networks \n\n::: callout-note\n### Learning Objectives {.unnumbered}\n\n-   [x] *Compute* the gradient for a single hidden layer neural network [\\#](#sec-l3g1)\n-   [x] *Understand* how to compute the gradient for arbitrarily deep networks [\\#](#sec-l3g2)\n-   [x] *Understand* the importance of initialization for neural networks [\\#](#sec-l3g3)\n-   [x] *Describe* strategies for initializing neural networks [\\#](#sec-l3g4)\n-   [x] *Describe* optimization techniques for training neural networks [\\#](#sec-l3g5)\n\n:::\n\n\n::: callout-note\n\n### Discussion prompt {.unnumbered}\n\n> What properties of the representation are important for our online setting? This contrasts the offline, batch setting. \n\n:::\n\n",
    "supporting": [
      "c3-w2_files"
    ],
    "filters": [],
    "includes": {}
  }
}