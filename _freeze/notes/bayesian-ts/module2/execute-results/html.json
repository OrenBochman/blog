{
  "hash": "2bff6725fbeb9516fd3bc4e379b21cbc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 2: The AR(p) process\"\nsubtitle: Time Series Analysis\ndescription: \"The AR(1) process, Stationarity, ACF, PACF, Differencing, and Smoothing\"\ndate: 2024-10-24\ncategories: \n  - Coursera \n  - notes\n  - Bayesian Statistics\n  - Autoregressive Models\n  - Time Series\nkeywords: \n  - time series\n  - stability\n  - order of an AR process \n  - characteristic lag polynomial\n  - autocorrelation function\n  - ACF\n  - partial autocorrelation function\n  - PACF\n  - smoothing\n  - State Space Model\n  - ARMA process\n  - ARIMA\n  - moving average\n  - AR(p) process  \n  - R code\nauthor: Oren Bochman\nimage: course-banner.png\nfig-caption: Notes about ... Bayesian Statistics\ntitle-block-banner: banner_deep.jpg\nbibliography: bibliography.bib\nformat: \n    html: \n        code-fold: true\nlightbox:\n  match: auto\n  effect: fade\n  desc-position: left\n  loop: true\n#  css-class: \"my-css-class\"\n\n---\n\n\n\n## Learning Objectives\n\n-   Define the [autoregressive process of order p]{.mark} or AR(p) and use R to obtain samples from such process\n-   Define ARIMA (autoregressive moving average) models (honors)\n-   Perform posterior inference for the AR(p) under the conditional likelihood and the reference prior\n-   Perform a full data analysis in R using an AR(p) including\n  - likelihood estimation and \n  - Bayesian inference, \n  - model order selection, and \n  - forecasting\n-   Explain the relationship between the AR characteristic polynomial, the ACF, the forecast function and the spectral density in the case of an AR(p)\n\n# The general AR(p) process (video)\n\n## Definition and state-space representation (video)\n\n![AR(p) process, characteristic polynomial, stability, stationarity and MA representation ](m2_0001.png){.column-margin width=\"250px\" group=\"slides\"}\n\n\n[**AR(P)** is shorthand for *autoregressive process of order p* which generalizes the **AR(1)** process]{.mark} that we studied in the previous module. It is essentially a mapping that allows us to specify the current value of the time series in terms its past p-values and some noise. The number of parameter $p$, required is the **order** [**order**]{.column-margin} of the autoregressive process. It tells us how many *lags* we will be considering. \n\n[**AR(P)**]{.column-margin}\n\nWe will assume AR(P) has the following structure:\n\n$$\n\\textcolor{red}{y_t} = \\textcolor{blue}{\\phi_1} \\textcolor{red}{y_{t-1}} + \\textcolor{blue}{\\phi_2} \\textcolor{red}{y_{t-2}} + \\ldots + \\textcolor{blue}{\\phi_p} \\textcolor{red}{y_{t-p}} + \\textcolor{grey}{\\epsilon_t} \\qquad\n$$ {#eq-ar-p-annotated}\n\nwhere:\n\n-   $\\textcolor{red}{y_t}$ is the value of the time series at time t\n-   $\\textcolor{blue}{\\phi_{1:p}}$ are the AR coefficients\n-   $\\textcolor{grey}{\\epsilon_t} \\overset{\\text{iid}}{\\sim} \\text{N}(0,v) \\quad \\forall t$  is a **white noise process**.\n\n\n- The number of parameters has increased from one coefficient in AR(1) to p coefficients for AR(P).\n\nA central outcome of the autoregressive nature of the  **AR(p)** is due to the properties the AR characteristic polynomial $\\Phi$. [$\\Phi$ AR characteristic polynomial]{.column-margin} This is defined as : \n\nrecall the backshift operator $B$ is defined as $B y_t = y_{t-1}$, so that $B^j y_t = y_{t-j}$.\n\n$$\n\\begin{aligned}\n       y_t &= \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t  && \\text{(Ar(p) defn.)} \\newline\n       y_t &= \\phi_1 By_{t} + \\phi_2 B^2y_{t} + \\ldots + \\phi_p B^p y_{t} + \\epsilon_t && \\text{(B defn.)} \\newline\n\\epsilon_t &= y_t - \\phi_1 B y_t + \\phi_2 B^2 y_t + \\ldots + \\phi_p B^p y_t    && \\text{(rearranging)} \\newline \n\\epsilon_t  &= (1- \\phi_1 B + \\phi_2 B^2 + \\ldots + \\phi_p B^p) y_t            && \\text{(factoring out $y_t$)}\n\\end{aligned}\n$$\n$$\n\\Phi(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\ldots - \\phi_p z^p \\qquad \\text{(Characteristic polynomial)}\n$$ {#eq-ar-poly}\n\nwhere:\n\n- $z \\in \\mathbb{C}$ i.e. complex-valued.\n\nwe can also rewrite the characteristic polynomial in terms of the reciprocal roots of the polynomial.\n\n\nThe zeros of the characteristic polynomial are the roots of the **AR(p)** process.\n\n$$\n\\Phi(z) = \\prod_{j=1}^{p} (1 - \\alpha_j z) = 0  \\implies z = \\frac{1}{ \\alpha_j} \\qquad \\text{(reciprocal roots)}\n$$\n\nwhere: \n\n- $\\alpha_j$ are the reciprocal roots of the characteristic polynomial.\n\n> Why are we interested in this autoregresive lag polynomial? \n\n- [This polynomial and its roots informs us a lot about the process and its properties.]{.mark}\n- One of the main characteristics is it allows us to think about things like **quasi-periodic behavior**, whether it's present or not in a particular **AR(p)** process.\n- It allows us to think about whether a process is **stationary or not**, depending on some properties related to this polynomial. \n- In particular, we are going to say that the process is **stable** if all the roots of the characteristic polynomial have a modulus greater than one. [stability condition]{.column-margin}\n$$\n\\Phi(z) = 0 \\iff |z| > 1  \\qquad \\text{(stability condition)}\n$$ {#eq-ar(p)-stability}\n- For any of the roots, it has to be the case that the modulus of that root, they have to be all outside the unit circle.\n- If a process is stable, it will also be stationary.\n\nWe can show this as follows:\n\n- Once the process is stationary, and if all the roots of the characteristic polynomial are outside the unit circle, then we will be able to write this process in terms of an infinite order moving average process. In this case, if the process is stable, then we are going to be able to write it like this.\n\n$$\ny_t = \\Psi(B) \\epsilon_t = \\sum_{j=0}^{\\infty} \\psi_j \\epsilon_{t-j} \\ \\text {with} \\ \\psi_0 = 1 \\text{ and } \\sum_{j=0}^{\\infty} |\\psi_j| < \\infty\n$$ {#eq-ar-stationary}\n\nwhere:\n\n- $\\epsilon_t$ is a white noise process with zero mean and constant variance $v$.\n- $B$ is the lag operator AKA the backshift operator defined by $B \\varepsilon_t = \\varepsilon_{t-1}$. This need to be applied to a time series $\\epsilon_t$ to get the lagged values. \n- $\\Psi(B)$ is the infinite order polynomial in $B$ that representing a linear filter applied to the noise process.â€‹\n- $\\psi_t = 1$ is the weight for the white noise at time $t$.\n- the constraint $\\psi_0 = 1$ ensures that the current shock contributes directly to $y_t$\n- the constraint on the weights $\\sum_{j=0}^{\\infty} |\\psi_j| < \\infty$ ensures that the weights decay sufficiently fast, so that the process does not explode i.e. it is stable and thus stationary.\n\nthe notation with $\\psi$ a functional of operator $B$ and $\\psi_i$ as constants is confusing in both the reuse if the symbol and the complexity.\n\nHere, U is any complex valued number. \n\n> I am going to have an infinite order polynomial here on B, the backshift operator that I can write down just as the sum, j goes from zero to infinity.\n\n> Here $\\psi_0=1$. Then there is another condition on the Psi's for this to happen. We have to have finite sum of these on these coefficients. Once again, if the process is stable, then it would be stationary and we will be able to write down the AR as an infinite order moving average process here. If you recall, B is the backshift operator. Again, if I apply this to $y_t$, I'm just going to get $y_t-j$. I can write down Psi of B, as $1 + \\psi_1$ B, B squared, and so on. It's an infinite order process.\n\n>The AR characteristic polynomial can also be written in terms of the reciprocal roots of the polynomial. So instead of considering the roots, we can consider the reciprocal roots. In that case, let's say the \\$phi$ of u\n>for Alpha 1, Alpha 2, and so on. The reciprocal roots.\n>[Why do we care about all these roots? Why do we care about this structure? Again, we will be able to understand some properties of the process based on these roots]{.mark} as we will see. \n\n![A state space representation of Ar(p)](m2_0002.png){.column-margin width=\"250px\" group=\"slides\"}\n\n\n[We will now discuss another important representation of the AR(P) process, one that is based on a state-space representation of the process.]{.mark} Again, we care about this type of representations because they allow us to study some important properties of the process. In this case, our state-space or dynamic linear model representation, we will make some connections with these representations later when we talk about dynamic linear models, is given as follows for an AR(P). I have my y_t. I can write it as F transpose and then another vector x_t here. Then we're going to have x_t is going to be a function of x_t minus 1. That vector there is going to be an F and a G. I will describe what those are in a second. Then I'm going to have another vector here with some distribution. In our case, we are going to have a normal distribution also for that one. In the case of the AR(P), we're going to have x_t to be y_t, y_t minus 1.\n>It's a vector that has all these values of the y_t process. Then F is going to be a vector. It has to match the dimension of this vector. The first entry is going to be a one, and then I'm going to have zeros everywhere else. The w here is going to be a vector as well.\n>\n>The first component is going to be the Epsilon t. That we defined for the ARP process. Then every other entry is going to be a zero here. Again, the dimensions are going to match so that I get the right equations here. Then finally, my G matrix in this representation is going to be a very important matrix, the first row is going to contain the AR parameters, the AR coefficients.\n>We have p of those. That's my first row. In this block, I'm going to have an identity matrix. It's going to have ones in the diagonal and zeros everywhere else. I'm going to have a one here, and then I want to have zeros everywhere else. In this portion, I'm going to have column vector here of zeros. This is my G matrix. Why is this G matrix important? This G matrix is going to be related to the characteristic polynomial, in particular, is going to be related to the reciprocal roots of the characteristic polynomial that we discussed before. The eigenvalues of this matrix correspond precisely to the reciprocal roots of the characteristic polynomial. We will think about that and write down another representation related to this process. But before we go there, I just want you to look at this equation and see that if you do the matrix operations that are described these two equations, you get back the form of your autoregressive process. The other thing is, again, this is called a state-space representation because you have two equations here. One, you can call it the observational level equation where you are relating your observed y's with some other model information here. Then there is another equation that has a Markovian structure here, where x_t is a function of x_t minus 1. This is why this is a state-space representation. One of the nice things about working with this representation is we can use some definitions that apply to dynamic linear models or state-space models, and one of those definitions is the so-called forecast function. The forecast function, we can define it in terms of, I'm going to use here the notation f_t h to denote that is a function f that depends on the time t that you're considering, and then you're looking at forecasting h steps ahead in your time series. If you have observations up to today and you want to look at what is the forecast function five days later, you will have h equals 5 there. It's just the expected value. We are going to think of this as the expected value of y_t plus h. Conditional on all the observations or all the information you have received up to time t. I'm going to write it just like this. Using the state-space representation, you can see that if I use the first equation and I think about the expected value of y_t plus h is going to be F transpose, and then I have the expected value of the vector x_t plus h in that case. I can think of just applying this, then I would have expected value of x_t plus h given y_1 up to t. But now when I look at the structure of x_t plus h, if I go to my second equation here, I can see that x_t plus h is going to be dependent on x_t plus h minus 1, and there is a G matrix here. I can write this in terms of the expected value of x_t plus h, which is just G, expected value of x_t plus h minus 1, and then I also have plus expected value of the w_t's. But because of the structure of the AR process that we defined, we said that all the Epsilon T's are independent normally distributed random variables center at zero. In this case, those are going to be all zero. I can write down this as F transpose G, and then I have the expected value of x_t plus h minus 1 given y_1 up to t. If I continue with this process all the way until I get to time t, I'm going to get a product of all these G matrices here, and because we are starting with this lag h, I'm going to have the product of that G matrix h times. I can write this down as F transpose G to the power of h, and then I'm going to have the expected value of, finally, I get up to here.\n>\n>This is simply is going to be just my x_t vector. I can write this down as F transpose G^h, and then I have just my x_t. Again, why do we care? Now we are going to make that connection with this matrix and the eigenstructure of this matrix. I said before, one of the features of this matrix is that the eigenstructure is related to the reciprocal roots of the characteristic polynomial. In particular, the eigenvalues of this matrix correspond to the reciprocal roots of the characteristic polynomial. If we are working with the case in which we have exactly p different roots. We have as many different roots as the order of the AR process. Let's say, p distinct.\n>We can write down then G in terms of its eigendecomposition. I can write this down as E, a matrix Lambda here, E inverse. \n>\n>Here, Lambda is going to be a diagonal matrix,\n>you just put the reciprocal roots, I'm going to call those Alpha 1 up to Alpha p. They are all different. You just put them in the diagonal and you can use any order you want. But the eigendecomposition, the eigenvectors, have to follow the order that you choose for the eigenvalues. Then what happens is, regardless of that, you're going to have a unique G. But here, the E is a matrix of eigenvectors.\n\n\n\n>Again, why do we care? Well, if you look at what we have here, we have the power G to the power of h. Using that eigendecomposition, we can get to write this in this form. Whatever elements you have in the matrix of eigenvectors, they are now going to be functions of the reciprocal roots. The power that appears here, which is the number of steps ahead that you want to forecast in your time series for prediction, I'm just going to have the Alphas to the power of h. When I do this calculation, I can end up writing the forecast function just by doing that calculation as a sum from j equals 1 up to p of some constants. Those constants are going to be related to those E matrices but the important point is that what appears here is my Alpha to the power of h. What this means is I'm breaking this expected value of what I'm going to see in the future in terms of a function of the reciprocal roots of the characteristic polynomial. You can see that if the process is stable, is going to be stationary, all the moduli of my reciprocal roots are going to be below one. This is going to decay exponentially as a function of h. You're going to have something that decays exponentially. Depending on whether those reciprocal roots are real-valued or complex-valued, you're going to have behavior here that may be quasiperiodic for complex-valued roots or just non-quasiperiodic for the real valued roots. The other thing that matters is, if you're working with a stable process, are going to have moduli smaller than one. The contribution of each of the roots to these forecasts function is going to be dependent on how close that modulus of that reciprocal root is to one or minus one. For roots that have relatively large values of the modulus, then they are going to have more contribution in terms of what's going to happen in the future. This provides a way to interpret the AR process.\n\n## Examples (video)\n\n![AR(1)](m2_0011.png){.column-margin width=\"250px\" group=\"slides\"}\n\n![AR(2) two positive roots](m2_0012.png){.column-margin width=\"250px\" group=\"slides\"}\n\n![AR(2) complex roots](m2_0013.png){.column-margin width=\"250px\" group=\"slides\"}\n\n\n\n{{< lipsum 3 >}}\n\n\n\n\n## ACF of the AR(p) (video)\n\n![ACF of the AR(p)](m2_0021.png){.column-margin width=\"250px\" group=\"slides\"}\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n## Simulating data from an AR(p) (video)\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n## Computing the roots of the AR polynomial (reading)\n\nCompute AR reciprocal roots given the AR coefficients\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assume the folloing AR coefficients for an AR(8)\nphi=c(0.27, 0.07, -0.13, -0.15, -0.11, -0.15, -0.23, -0.14)\nroots=1/polyroot(c(1, -phi)) # compute reciprocal characteristic roots\nr=Mod(roots) # compute moduli of reciprocal roots\nlambda=2*pi/Arg(roots) # compute periods of reciprocal roots\n\n# print results modulus and frequency by decreasing order\nprint(cbind(r, abs(lambda))[order(r, decreasing=TRUE), ][c(2,4,6,8),]) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             r          \n[1,] 0.9722428 12.731401\n[2,] 0.8094950  5.103178\n[3,] 0.7196221  2.987712\n[4,] 0.6606487  2.232193\n```\n\n\n:::\n:::\n\n\n\n## Simulating data from an AR(p) (reading)\n\n1.  Rcode to simulate data from an AR(2) with one pair of complex-valued reciprocal roots and plot the corresponding sample ACF and sample PACF\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## simulate data from an AR(2)\nset.seed(2021)\n## AR(2) with a pair of complex-valued roots with modulus 0.95 and period 12 \nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]<- 2*r*cos(2*pi/lambda) \nphi[2] <- -r^2\nphi\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  1.645448 -0.902500\n```\n\n\n:::\n\n```{.r .cell-code}\nT=300 # number of time points\nsd=1 # innovation standard deviation\nyt=arima.sim(n=T, model = list(ar = phi), sd=sd)\n\npar(mfrow = c(3, 1), cex.lab = 1.5)\n## plot simulated data \nts.plot(yt)\n## draw sample autocorrelation function\nacf(yt, lag.max = 50,\n    type = \"correlation\", ylab = \"sample ACF\", \n    lty = 1, ylim = c(-1, 1), main = \" \")\n\n## draw sample partial autocorrelation function\npacf(yt, lag.ma = 50, main = \"sample PACF\")\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-sim-complex-valued-roots-1.png){width=672}\n:::\n:::\n\n\n\n2.  Rcode to simulate data from an AR(2) with two different real-valued reciprocal roots and plot the corresponding sample ACF and sample PACF\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Simulate from AR(2) with two real reciprocal roots (e.g., 0.95 and 0.5)\nset.seed(2021)\nrecip_roots=c(0.95, 0.5) ## two different real reciprocal roots\nphi=c(sum(recip_roots), -prod(recip_roots)) ## compute ar coefficients\nphi\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  1.450 -0.475\n```\n\n\n:::\n\n```{.r .cell-code}\nT=300 ## set up number of time points\nsd=1 ## set up standard deviation\nyt=arima.sim(n=T,model = list(ar=phi),sd=sd) # generate ar(2)\n\npar(mfrow = c(3, 1), cex.lab = 1.5, cex.main = 1.5)\n### plot simulated data \nts.plot(yt)\n### plot sample ACF\nacf(yt, lag.max = 50, type = \"correlation\",  main = \"sample ACF\")\n### plot sample PACF\npacf(yt, lag.max = 50, main = \"sample PACF\")\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-sim-real-valued-roots-1.png){width=672}\n:::\n:::\n\n\n\n3.  Rcode to simulate data from an AR(3) with one real reciprocal root and a pair of complex-valued reciprocal roots and plot the corresponding sample ACF and sample PACF\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Simulate from AR(3) with one real root \n### and a pair of complex roots (e.g., r=0.95 and lambda = 12 and real root with\n### 0.8 modulus)\nset.seed(2021)\nr= c(0.95, 0.95, 0.8) ## modulus\nlambda=c(-12, 12) ## lambda\nrecip_roots=c(r[1:2]*exp(2*pi/lambda*1i), r[3]) ## reciprocal roots\nphi <- numeric(3) # placeholder for phi\nphi[1]=Re(sum(recip_roots)) # ar coefficients at lag 1\nphi[2]=-Re(recip_roots[1]*recip_roots[2] + recip_roots[1]*recip_roots[3] + recip_roots[2]*recip_roots[3]) # ar coefficients at lag 2\nphi[3]=Re(prod(recip_roots))\nphi\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  2.445448 -2.218859  0.722000\n```\n\n\n:::\n\n```{.r .cell-code}\nT=300 # number of time points\nsd=1 # standard deviation\nyt=arima.sim(n=T,model = list(ar=phi), sd = sd) # generate ar(3)\n\npar(mfrow = c(3,1), cex.lab = 1.5, cex.main = 1.5)\n### plot simulated data \nts.plot(yt)\n### plot sample ACF\nacf(yt, lag.max = 50, type = \"correlation\",  main = \"sample ACF\")\n### plot sample PACF\npacf(yt, lag.max = 50, main = \"sample PACF\")\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-sim-ar3-1.png){width=672}\n:::\n:::\n\n\n\n## The AR(p): Review (Reading)\n\n### AR(p): Definition, stability, and stationarity\n\n::: callout-info\n\n### AR(p)\n\nA time series follows a zero-mean autoregressive process of order $p$, of AR(p), if:\n\n$$\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + \\epsilon_t \\qquad\n$$ {#eq-ar-p}\n\nwhere $\\phi_1, \\ldots, \\phi_p$ are the AR coefficients and $\\epsilon_t$ is a white noise process \n\nwith $\\epsilon_t \\sim \\text{i.i.d. } N(0, v)$, for all $t$.\n\n:::\n\nThe AR characteristic polynomial is given by\n\n$$\n\\Phi(u) = 1 - \\phi_1 u - \\phi_2 u^2 - \\ldots - \\phi_p u^p,\n$$\n\nwith $u$ complex-valued.\n\nThe AR(p) process is stable if $\\Phi(u) = 0$ only when $|u| > 1$. In this case, the process is also stationary and can be written as\n\n$$\ny_t = \\psi(B) \\epsilon_t = \\sum_{j=0}^{\\infty} \\psi_j \\epsilon_{t-j},\n$$\n\nwith $\\psi_0 = 1$ and $\\sum_{j=0}^{\\infty} |\\psi_j| < \\infty$. Here $B$ denotes the backshift operator, so $B^j \\epsilon_t = \\epsilon_{t-j}$ and\n\n$$\n\\psi(B) = 1 + \\psi_1 B + \\psi_2 B^2 + \\ldots + \\psi_j B^j + \\ldots\n$$\n\nThe AR polynomial can also be written as\n\n$$\n\\Phi(u) = \\prod_{j=1}^{p} (1 - \\alpha_j u),\n$$\n\nwith $\\alpha_j$ being the reciprocal roots of the characteristic polynomial. For the process to be stable (and consequently stationary), $|\\alpha_j| < 1$ for all $j = 1, \\ldots, p$.\n\n#### AR(p): State-space representation\n\nAn AR(p) can also be represented using the following state-space or dynamic linear (DLM) model representation:\n\n$$\ny_t = F' x_t,\n$$\n\n$$\nx_t = G x_{t-1} + \\omega_t,\n$$\n\nwith $x_t = (y_t, y_{t-1}, \\dots, y_{t-p+1})'$, $F = (1, 0, \\dots, 0)'$, $\\omega_t = (\\epsilon_t, 0, \\dots, 0)'$, and\n\n$$\nG = \\begin{pmatrix}\n\\phi_1 & \\phi_2 & \\phi_3 & \\dots & \\phi_{p-1} & \\phi_p \\\\\n1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 1 & 0 & \\dots & 0 & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n0 & 0 & 0 & \\dots & 1 & 0\n\\end{pmatrix}.\n$$\n\nUsing this representation, the expected behavior of the process in the future can be exhibited via the forecast function:\n\n$$\nf_t(h) = E(y_{t+h} | y_{1:t}) = F' G^h x_t, \\quad h > 0,\n$$\n\nfor any $t \\ge p$. The eigenvalues of the matrix $G$ are the reciprocal roots of the characteristic polynomial.\n\n::: callout-note\n##### Eigenvalues\n\n-   The eigenvalues can be real-valued or complex-valued.\n-   If they are Complex-valued the eigenvalues/reciprocal roots appear in conjugate pairs.\n:::\n\nAssuming the matrix $G$ has $p$ distinct eigenvalues, we can decompose $G$ into $G = E \\Lambda E^{-1}$, with\n\n$$\n\\Lambda = \\text{diag}(\\alpha_1, \\dots, \\alpha_p),\n$$\n\nfor a matrix of corresponding eigenvectors $E$. Then, $G^h = E \\Lambda^h E^{-1}$ and we have:\n\n$$\nf_t(h) = \\sum_{j=1}^{p} c_{tj} \\alpha_j^h.\n$$\n\n#### ACF of AR(p)\n\nFor a general AR(p), the ACF is given in terms of the homogeneous difference equation:\n\n$$\n\\rho(h) - \\phi_1 \\rho(h-1) - \\ldots - \\phi_p \\rho(h-p) = 0, \\quad h > 0.\n$$\n\nAssuming that $\\alpha_1, \\dots, \\alpha_r$ denotes the characteristic reciprocal roots each with multiplicity $m_1, \\ldots, m_r$, respectively, with $\\sum_{i=1}^{r} m_i = p$. Then, the general solution is\n\n$$\n\\rho(h) = \\alpha_1^h p_1(h) + \\ldots + \\alpha_r^h p_r(h),\n$$\n\nwith $p_j(h)$ being a polynomial of degree $m_j - 1$.\n\n##### Example: AR(1)\n\nWe already know that for $h \\ge 0$, $\\rho(h) = \\phi^h$. Using the result above, we have\n\n$$\n\\rho(h) = a \\phi^h,\n$$\n\nand so to find $a$, we take $\\rho(0) = 1 = a \\phi^0$, hence $a = 1$.\n\n##### Example: AR(2)\n\nSimilarly, using the result above in the case of two complex-valued reciprocal roots, we have\n\n$$\n\\rho(h) = a \\alpha_1^h + b \\alpha_2^h = c r^h \\cos(\\omega h + d).\n$$\n\n#### PACF of AR(p)\n\nWe can use the Durbin-Levinson recursion to obtain the PACF of an AR(p). \n\nUsing the same representation but substituting the true autocovariances and autocorrelations with their sampled versions, we can also obtain the sample PACF.\n\nIt is possible to show that the PACF of an AR(p) is equal to zero for $h > p$.\n\n### Quiz: The AR(p) process (Quiz)\n\nOmitted due to Coursera's Honor Code\n\n# Bayesian Inference in the AR(p)\n\n## Bayesian inference in the AR(p): Reference prior, conditional likelihood (video)\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n## Rcode: Maximum likelihood estimation, AR(p), conditional likelihood (Reading)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  set.seed(2021)\n# Simulate 300 observations from an AR(2) with one pair of complex-valued reciprocal roots \nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Compute the MLE for phi and the unbiased estimator for v using the conditional likelihood\np=2\ny=rev(yt[(p+1):T]) # response\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"Estimate for v: \", s2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n MLE of conditional likelihood for phi:  1.65272 -0.9189823 \n Estimate for v:  0.9901292 \n```\n\n\n:::\n:::\n\n\n\n## Model order selection (video)\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n## Example: Bayesian inference in the AR(p), conditional likelihood (Video)\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n## Rcode: Bayesian inference, AR(p), conditional likelihood (Reading)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate 300 observations from an AR(2) with one pair of complex-valued roots \nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \npar(mfrow=c(1,1))\nplot(yt)\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-bayesian-inference-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## Compute the MLE of phi and the unbiased estimator of v using the conditional likelihood\np=2\ny=rev(yt[(p+1):T]) # response\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\n#####################################################################################\n### Posterior inference, conditional likelihood + reference prior via \n### direct sampling                 \n#####################################################################################\n\nn_sample=1000 # posterior sample size\nlibrary(MASS)\n\n## step 1: sample v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)\n\n## step 2: sample phi conditional on v from normal distribution\nphi_sample=matrix(0, nrow = n_sample, ncol = p)\nfor(i in 1:n_sample){\n  phi_sample[i, ]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)\n}\n\npar(mfrow = c(2, 3), cex.lab = 1.3)\n## plot histogram of posterior samples of phi and v\n\nfor(i in 1:2){\n  hist(phi_sample[, i], xlab = bquote(phi), \n       main = bquote(\"Histogram of \"~phi[.(i)]),col='lightblue')\n  abline(v = phi[i], col = 'red')\n}\n\nhist(v_sample, xlab = bquote(nu), main = bquote(\"Histogram of \"~v),col='lightblue')\nabline(v = sd, col = 'red')\n\n#####################################################\n# Graph posterior for modulus and period \n#####################################################\nr_sample=sqrt(-phi_sample[,2])\nlambda_sample=2*pi/acos(phi_sample[,1]/(2*r_sample))\nhist(r_sample,xlab=\"modulus\",main=\"\",col='lightblue')\nabline(v=0.95,col='red')\nhist(lambda_sample,xlab=\"period\",main=\"\",col='lightblue')\nabline(v=12,col='red')\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-bayesian-inference-2.png){width=672}\n:::\n:::\n\n\n\n### Rcode: Model order selection (Reading)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###################################################\n# Simulate data from an AR(2)\n###################################################\nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n#############################################################################\n######   compute AIC and BIC for different AR(p)s based on simulated data ###\n#############################################################################\npmax=10 # the maximum of model order\nXall=t(matrix(yt[rev(rep((1:pmax),T-pmax)+rep((0:(T-pmax-1)),\n              rep(pmax,T-pmax)))], pmax, T-pmax));\ny=rev(yt[(pmax+1):T])\nn_cond=length(y) # (number of total time points - the maximum of model order)\n\n## compute MLE\nmy_MLE <- function(y, Xall, p){\n  n=length(y)\n  x=Xall[,1:p]\n  a=solve(t(x) %*%x)\n  a=(a + t(a))/2 # for numerical stability \n  b=a%*%t(x)%*%y # mle for ar coefficients\n  r=y - x%*%b # residuals \n  nu=n - p # degrees freedom\n  R=sum(r*r) # SSE\n  s=R/nu #MSE\n  return(list(b = b, s = s, R = R, nu = nu))\n}\n\n\n## function for AIC and BIC computation \nAIC_BIC <- function(y, Xall, p){\n  ## number of time points\n  n <- length(y)\n  \n  ## compute MLE\n  tmp=my_MLE(y, Xall, p)\n  \n  ## retrieve results\n  R=tmp$R\n  \n  ## compute likelihood\n  likl= n*log(R)\n  \n  ## compute AIC and BIC\n  aic =likl + 2*(p)\n  bic =likl + log(n)*(p)\n  return(list(aic = aic, bic = bic))\n}\n# Compute AIC, BIC \naic =numeric(pmax)\nbic =numeric(pmax)\n\nfor(p in 1:pmax){\n  tmp =AIC_BIC(y,Xall, p)\n  aic[p] =tmp$aic\n  bic[p] =tmp$bic\n  print(c(p, aic[p], bic[p])) # print AIC and BIC by model order\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]    1.000 2166.793 2170.463\n[1]    2.000 1635.816 1643.156\n[1]    3.000 1637.527 1648.536\n[1]    4.000 1639.059 1653.738\n[1]    5.000 1640.743 1659.093\n[1]    6.000 1641.472 1663.491\n[1]    7.000 1643.457 1669.147\n[1]    8.000 1645.370 1674.729\n[1]    9.000 1646.261 1679.290\n[1]   10.000 1647.915 1684.614\n```\n\n\n:::\n\n```{.r .cell-code}\n## compute difference between the value and its minimum\naic =aic-min(aic) \nbic =bic-min(bic) \n\n## draw plot of AIC, BIC, and the marginal likelihood\npar(mfrow = c(1, 1))\nmatplot(1:pmax,matrix(c(aic,bic),pmax,2),ylab='value',\n        xlab='AR order p',pch=\"ab\", col = 'black', main = \"AIC and BIC\")\n# highlight the model order selected by AIC\ntext(which.min(aic), aic[which.min(aic)], \"a\", col = 'red') \n# highlight the model order selected by BIC\ntext(which.min(bic), bic[which.min(bic)], \"b\", col = 'red') \n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n########################################################\np <- which.min(bic) # We set up the moder order\nprint(paste0(\"The chosen model order by BIC: \", p))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The chosen model order by BIC: 2\"\n```\n\n\n:::\n:::\n\n\n\n\n### Spectral representation of the AR(p) (video)\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n### Spectral representation of the AR(p): Example (video)\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n### Rcode: Spectral density of AR(p) (Reading)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Simulate 300 observations from an AR(2) prcess with a pair of complex-valued roots \nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]<- 2*r*cos(2*pi/lambda) \nphi[2] <- -r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# sample from the AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n# Compute the MLE of phi and the unbiased estimator of v using the conditional likelihood \np=2\ny=rev(yt[(p+1):T])\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\n# Obtain 200 samples from the posterior distribution under the conditional likelihood and the reference prior \nn_sample=200 # posterior sample size\nlibrary(MASS)\n\n## step 1: sample v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)\n\n## step 2: sample phi conditional on v from normal distribution\nphi_sample=matrix(0, nrow = n_sample, ncol = p)\nfor(i in 1:n_sample){\n  phi_sample[i,]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)\n}\n\n\n### using spec.ar to draw spectral density based on the data assuming an AR(2)\nspec.ar(yt, order = 2, main = \"yt\")\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-1.png){width=672}\n:::\n\n```{.r .cell-code}\n### using arma.spec from astsa package to draw spectral density\nlibrary(\"astsa\")\n\n## plot spectral density of simulated data with posterior sampled \n## ar coefficients and innvovation variance\npar(mfrow = c(1, 1))\n#result_MLE=arma.spec(ar=phi_MLE, var.noise = s2, log='yes',main = '')\nresult_MLE=arma.spec(ar=phi_MLE, var.noise = s2, main = '')\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-2.png){width=672}\n:::\n\n```{.r .cell-code}\nfreq=result_MLE$freq\n  \nspec=matrix(0,nrow=n_sample,ncol=length(freq))\n\nfor (i in 1:n_sample){\nresult=arma.spec(ar=phi_sample[i,], var.noise = v_sample[i],# log='yes',\n                 main = '')\nspec[i,]=result$spec\n}\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-15.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-16.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-17.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-18.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-19.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-20.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-21.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-22.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-23.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-24.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-25.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-26.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-27.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-28.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-29.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-30.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-31.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-32.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-33.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-34.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-35.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-36.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-37.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-38.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-39.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-40.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-41.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-42.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-43.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-44.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-45.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-46.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-47.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-48.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-49.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-50.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-51.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-52.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-53.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-54.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-55.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-56.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-57.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-58.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-59.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-60.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-61.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-62.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-63.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-64.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-65.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-66.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-67.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-68.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-69.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-70.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-71.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-72.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-73.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-74.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-75.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-76.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-77.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-78.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-79.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-80.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-81.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-82.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-83.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-84.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-85.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-86.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-87.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-88.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-89.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-90.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-91.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-92.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-93.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-94.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-95.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-96.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-97.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-98.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-99.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-100.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-101.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-102.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-103.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-104.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-105.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-106.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-107.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-108.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-109.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-110.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-111.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-112.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-113.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-114.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-115.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-116.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-117.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-118.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-119.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-120.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-121.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-122.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-123.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-124.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-125.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-126.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-127.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-128.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-129.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-130.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-131.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-132.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-133.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-134.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-135.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-136.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-137.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-138.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-139.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-140.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-141.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-142.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-143.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-144.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-145.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-146.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-147.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-148.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-149.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-150.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-151.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-152.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-153.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-154.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-155.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-156.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-157.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-158.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-159.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-160.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-161.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-162.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-163.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-164.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-165.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-166.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-167.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-168.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-169.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-170.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-171.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-172.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-173.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-174.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-175.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-176.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-177.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-178.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-179.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-180.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-181.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-182.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-183.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-184.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-185.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-186.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-187.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-188.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-189.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-190.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-191.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-192.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-193.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-194.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-195.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-196.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-197.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-198.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-199.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-200.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-201.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-202.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(2*pi*freq,log(spec[1,]),type='l',ylim=c(-3,12),ylab=\"log spectra\",\n     xlab=\"frequency\",col=0)\n#for (i in 1:n_sample){\nfor (i in 1:2){\nlines(2*pi*freq,log(spec[i,]),col='darkgray')\n}\nlines(2*pi*freq,log(result_MLE$spec))\nabline(v=2*pi/12,lty=2,col='red')\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/ar-spectral-density-203.png){width=672}\n:::\n:::\n\n\n\n### Quiz: Spectral representation of the AR(p)\n\nOmitted due to Coursera's Honor Code\n\n### Graded Assignment: Bayesian analysis of an EEG dataset using an AR(p)\n\nThe dataset below corresponds to a portion of an electroencephalogram (EEG) recorded in a particular location on the scalp of an individual. The original EEG dataset was originally recorded at 256Hz but was then subsampled every sixth observations, so the resulting sampling rate is about 42.7 observations per second. The dataset below has 400 observations corresponding approximately to 9.36 seconds.\n\nYou will use an AR(8) to model this dataset and obtain maximum likelihood estimation and Bayesian inference for the parameters of the model. For this you will need to do the following:\n\n1.  Download the dataset, and plot it in R. Upload a picture of your graph displaying the data and comment on the features of the data. Does it present any trends or quasi-periodic behavior?\n\n2.  Modify the code below to obtain the maximum likelihood estimators (MLEs) for the AR coefficients under the conditional likelihood. For this you will assume an autoregressive model of order p=8. The parameters of the model are $\\phi=(\\phi_1, \\ldots \\phi_8)'$ snf $v$. You will compute the MLE of $\\phi$ denoted as $\\hat\\phi$. â€‹\n\n3.  Obtain an unbiased estimator for the observational variance of the AR(8). You will compute the unbiased estimator for $v$ denoted as $s^2$.\n\n4.  Modify the code below to obtain 500 samples from the posterior distribution of the parameters $\\phi=(\\phi_1, \\ldots \\phi_8)'$ and $v$ under the conditional likelihood and the reference prior. You will assume an autoregressive model of order v. Once you obtain samples from the posterior distribution you will compute the posterior means of $\\phi$ and $v$, denoted as $\\hat\\phi$. and $\\hat v$, respectively.\n\nModify the code below to use the function polyroot and obtain the moduli and periods of the reciprocal roots of the AR polynomial evaluated at the posterior mean $\\hat\\phi$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2021)\nr=0.95\nlambda=12 \nphi=numeric(2) \nphi[1]=2*r*cos(2*pi/lambda) \nphi[2]=-r^2\nsd=1 # innovation standard deviation\nT=300 # number of time points\n# generate stationary AR(2) process\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \npar(mfrow=c(1,1))\nplot(yt)\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## Case 1: Conditional likelihood\np=2\ny=rev(yt[(p+1):T]) # response\nX=t(matrix(yt[rev(rep((1:p),T-p)+rep((0:(T-p-1)),rep(p,T-p)))],p,T-p));\nXtX=t(X)%*%X\nXtX_inv=solve(XtX)\nphi_MLE=XtX_inv%*%t(X)%*%y # MLE for phi\ns2=sum((y - X%*%phi_MLE)^2)/(length(y) - p) #unbiased estimate for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"Estimate for v: \", s2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n MLE of conditional likelihood for phi:  1.65272 -0.9189823 \n Estimate for v:  0.9901292 \n```\n\n\n:::\n\n```{.r .cell-code}\n#####################################################################################\n##  AR(2) case \n### Posterior inference, conditional likelihood + reference prior via \n### direct sampling                 \n#####################################################################################\n\nn_sample=1000 # posterior sample size\nlibrary(MASS)\n\n## step 1: sample v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2*p)/2, sum((y-X%*%phi_MLE)^2)/2)\n\n## step 2: sample phi conditional on v from normal distribution\nphi_sample=matrix(0, nrow = n_sample, ncol = p)\nfor(i in 1:n_sample){\n  phi_sample[i, ]=mvrnorm(1,phi_MLE,Sigma=v_sample[i]*XtX_inv)\n}\n\n## plot histogram of posterior samples of phi and nu\npar(mfrow = c(1, 3), cex.lab = 1.3)\nfor(i in 1:2){\n  hist(phi_sample[, i], xlab = bquote(phi), \n       main = bquote(\"Histogram of \"~phi[.(i)]))\n  abline(v = phi[i], col = 'red')\n}\n\nhist(v_sample, xlab = bquote(nu), main = bquote(\"Histogram of \"~v))\nabline(v = sd, col = 'red')\n```\n\n::: {.cell-output-display}\n![](module2_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\n\n## ARIMA processes (Reading)\n\n::: callout-info\n\n#### ARMA Model Definition\n\nA time series process is a zero-mean autoregressive moving average process if it is given by\n\n$$\ny_t = \\textcolor{red}\n                {\\underbrace{\\sum_{i=1}^{p} \\phi_i y_{t-i}}_{AR(P)}}\n      + \n      \\textcolor{blue}{\\underbrace{\\sum_{j=1}^{q} \\theta_j \\epsilon_{t-j}}_{MA(Q)}} + \\epsilon_t \\qquad \\text{(ARMA(p, q))}\n$$ {#eq-arma-definition}\n\nwith $\\epsilon_t \\sim N(0, v)$.\n\n- For $q = 0$, we get an AR(p) process.\n- For $p = 0$, we get a MA(q) i.e. moving average process of order $q$.\n\n:::\n\nNext we will define the notions of stability and invertibility of an ARMA process.\n\n::: callout-info\n\n#### Stability Definition\n\nAn ARMA process is **stable** if the roots of the AR characteristic polynomial [stable]{.column-margin}\n\n$$\n\\Phi(u) = 1 - \\phi_1 u - \\phi_2 u^2 - \\ldots - \\phi_p u^p\n$$\n\nlie outside the unit circle, i.e., for all $u$ such that $\\Phi(u) = 0$, $|u| > 1$.\n\nEquivalently, this happens when the reciprocal roots of the AR polynomial have moduli smaller than 1.\n\nThis condition implies stationarity.\n\n:::\n\n\n::: callout-info\n\n#### Invertible ARMA Definition\n\nAn ARMA process is **invertible** if the roots of the MA **characteristic polynomial** given by [invertible]{.column-margin}\n\n$$\n\\Theta(u) = 1 + \\theta_1 u + \\ldots + \\theta_q u^q,\n$$\n\nlie outside the unit circle.\n\n:::\n\nNote that $\\Phi(B) y_t = \\Theta(B) \\epsilon_t$.\n\n- When an ARMA process is **stable**, it can be written as an infinite order moving average process.\n\n- When an ARMA process is **invertible**, it can be written as an infinite order autoregressive process.\n\n::: callout-info\n\n#### ARIMA Processes\nAn autoregressive integrated moving average process with orders $p$, $d$, and $q$ is a process that can be written as\n\n$$\n(1 - B)^d y_t = \\sum_{i=1}^{p} \\phi_i y_{t-i} + \\sum_{j=1}^{q} \\theta_j \\epsilon_{t-j} + \\epsilon_t,\n$$\n\nin other words, $y_t$ follows an ARIMA(p, d, q) if the $d$ difference of $y_t$ follows an ARMA(p, q).\n\n:::\n\nEstimation in ARIMA processes can be done via *least squares*, *maximum likelihood*, and also *in a Bayesian way*. We will not discuss Bayesian estimation of ARIMA processes in this course.\n\n### Spectral Density of ARMA Processes\n\nFor a given AR(p) process with AR coefficients $\\phi_1, \\dots, \\phi_p$ and variance $v$, we can obtain its **spectral density** as\n\n$$\nf(\\omega) = \\frac{v}{2\\pi |\\Phi(e^{-i\\omega})|^2} = \\frac{v}{2\\pi |1 - \\phi_1 e^{-i\\omega} - \\ldots - \\phi_p e^{-ip\\omega}|^2},\n$$\n\nwith $\\omega$ a frequency in $(0, \\pi)$.\n\nThe spectral density provides a frequency-domain representation of the process that is appealing because of its interpretability.\n\nFor instance, an AR(2) process that has one pair of complex-valued reciprocal roots with modulus 0.7 and a period of $\\lambda = 12$, will show a mode in the spectral density located at a frequency of $2\\pi/12$. If we keep the period of the process at the same value of 12 but increase its modulus to 0.95, the spectral density will continue to show a mode at $2\\pi/12$, but the value of $f(2\\pi/12)$ will be higher, indicating a more persistent *quasi-periodic* behavior.\n\nSimilarly, we can obtain the spectral density of an ARMA process with AR characteristic polynomial $\\Phi(u) = 1 - \\phi_1 u - \\ldots - \\phi_p u^p$ and MA characteristic polynomial $\\Theta(u) = 1 + \\theta_1 u + \\ldots + \\theta_q u^q$, and variance $v$ as\n\n$$\nf(\\omega) = \\frac{v}{2\\pi} \\frac{|\\Theta(e^{-i\\omega})|^2}{|\\Phi(e^{-i\\omega})|^2}.\n$$\n\nNote that if we have posterior estimates or posterior samples of the AR/ARMA coefficients and the variance $v$, we can obtain samples from the spectral density of AR/ARMA processes using the equations above.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}