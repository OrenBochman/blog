{
  "hash": "bd12a297486803b6f2e48b0facdbc189",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 3: Normal Dynamic Linear Models, Part 1\"\nsubtitle: Time Series Analysis\ndescription: \"Normal Dynamic Linear Models, Part 1\"\ndate: 2024-10-25\ncategories: \n  - Coursera \n  - notes\n  - Bayesian Statistics\n  - Normal Dynamic Linear Models\n  - Time Series\nkeywords: \n  - time series\n  - filtering\n  - smoothing\n  - NLDM\n  - Polynomial Trend Models\n  - Regression Models\n  - Superposition Principle\n  - R code\nauthor: Oren Bochman\nimage: course-banner.png\nfig-caption: Notes about ... Bayesian Statistics\ntitle-block-banner: banner_deep.jpg\nbibliography: bibliography.bib\nformat: \n    html: \n        code-fold: true\n---\n\n\n\n\n\n\nNormal Dynamic Linear Models (NDLMs) are defined and illustrated in this module using several examples. Model building based on the forecast function via the superposition principle is explained. Methods for Bayesian filtering, smoothing and forecasting for NDLMs in the case of known observational variances and known system covariance matrices are discussed and illustrated.\n\n# Learning Objectives\n\n- [ ] Use R for analysis and forecasting of time series using NDLM (case of known observational and system variances)\n- [ ] Derive the equations to obtain posterior inference and forecasting in the NDLM with known observational and system variances, including the filtering, smoothing and forecasting equations\n- [ ] Apply the NDLM superposition principle and explain the role of the forecast function\n- [ ] Define trend and regression normal DLMs\n- [ ] Explain the general normal dynamic linear model (NDLM) representation\n\n\n# The Normal Dynamic Linear Model: Definition, model classes and the superposition principle\n\n## NDLM Definition (Video)\n\n![slide 1](m3_0001.png){.column-margin width=\"250px\"}\n\n> In this part of the course, [We will discuss the class of **normal dynamic linear models** for analyzing and forecasting non-stationary time series. We will talk about Bayesian inference and forecasting within this class of models and describe model building as well]{.mark}. \n\n### White Noise - A motivating example\n\n- We begin with a very simple model that has no temporal structure, just a model that is as follows: \n  - We have your time series $y_t$. \n  - We are interested in thinking about the mean level of a time series which we will denote as $\\mu$ and \n  - we will also have some noise $\\nu$ which is normally distributed. \n\n$$\ny_t = \\mu + v_t \\qquad v_t \\overset{\\text{iid}}{\\sim} N(0, V) \\qquad \n$$ {#eq-model-with-no-temporal-structure}\n\nwhere:\n\n- $\\mu$ is the expected value of $y_t$ and \n- $\\nu_t$ is the noise \n\nIf we plot this model we might see the graph in the slide.\n\n> In this model the mean of the time series is  $\\mu$ will be the the expected value of $y_t$, which is $\\mu$. \n\n$$\nE[y_t] = \\mu\n$$\n\n> The variance here of $y_t$ is $\\nu$ under this model.\n\n$$  \nVar[y_t] = \\nu\n$$\n\n> What may happen in practice again, this model has no temporal structure. \n\n> [We may want to incorporate some temporal structure e.g. the level of the expected value of this time series, should be changing over time.]{.mark} To do that, we should write down a new model where the $\\mu$ changes over time, so it's indexed in time. We will still have the same noise here. Let's again assume it is $N(0,V)$. We should have the following:\n\n\n$$\ny_t = \\mu_t + \\nu_t \\qquad \\nu_t \\overset{\\text{iid}}{\\sim} N(0, V) \\qquad \\text{observation equation}\n$$ {#eq-nldm-model-observation}\n\n> We should make a decision on how to incorporate temporal structure by modeling the changes over time in this parameter $\\mu_t$. We could consider different options. The simplest possible, probably that you can consider is something that looks like this:\n\n$$\n\\mu_t = \\mu_{t-1} + \\omega_t \\qquad \\omega_t \\overset{\\text{iid}}{\\sim} N(0, W) \\qquad \\text{system evolution equation}\n$$ {#eq-nldm-model-system-evolution}\n\n> We have that random walk type of structure where $\\mu_t$ will to be written as $\\mu(t-1)$. The expected value of $\\mu_t$, you'll think of it as $\\mu_{t-1} + \\text{some noise}$. That error is once again, assumed to be normally distributed random variable centered at zero and with variance $W$. \n\n> There is another assumption that we can make here and is that the $\\nu_t$ and $\\omega_t$ here, are also independent of each other. When I have this model, what am assuming here is that the mean level of the series is changing over time. \n\n> These type of models have a few characteristics. This is an example of a **normal dynamic linear model**, as we will see later. \n\nIn this models, we usually have a few things: \n\n1. We have two equations.\n  1. The **observation equation** that is relating your $y_t$, your observed process to some parameters in the model that are changing over time. \n  2. The **system level equation or evolution equation** that tells us how that time varying parameter is going to be changing over time. \n  \n2. We have a linear structure both in the observational level and in the system level. The linear structure, in the sense of the expected value of $y_t$ is just a linear function of that $\\mu_t$. It happens to be $\\mu_t$ in this particular case. In the second level, I can think of the expected value of $\\mu_t$ as a linear function given $\\mu(t-1)$, so it's a function that is linear in $\\mu(t-1)$. \n\n3. The other thing that we have here is at both levels, we have the **assumption of normality for the noise terms** in those equations. \n\nThis is an example of a *Gaussian or Normal dynamic*. These are time-varying parameters linear model. We will be discussing the general class of models. This is just an example. We will also discuss how to build different structures into the model, as well as how to perform Bayesian inference and forecasting.\n\n\n![slide 2](m3_0002.png){.column-margin width=\"250px\"}\n\n\n> The general class of dynamic linear models can be written as follows. Again, we are going to have two equations. One is the so-called observation equation that relates the observations to the parameters in the model, and the notation we are going to use is as follows.\n\n$$\ny_t = F_t' \\theta_t + \\nu_t \\qquad \\nu_t \\overset{\\text{iid}}{\\sim} N(0, \\nu_t) \\qquad \\text{observation}\n$$\n\nwhere:\n\n- $y_t$ is the univariate observation at time $t$\n- $F_t$ is a vector of known values of dimension $k$\n- $\\theta_t$ is a vector of parameters of dimension $k$\n- $\\nu_t$ is the variance (noise) at the observation level.\n- The noise is assumed to be IID normal random variables $N(0, V_t)$.\n\n$$\n\\theta_t = G_t \\theta_{t-1} + \\omega_t \\qquad \\omega_t \\overset{\\text{iid}}{\\sim} N(0, W_t) \\qquad \\text{system}\n$$\n\n- $\\theta_t$ is the vector of parameters.\n- $G_t$ is a known matrix of dimension $k \\times k$.\n- $W_t$ is the variance-covariance matrix at the system level.\n\n\n> The W_t we are going to assume at the beginning that these two quantities are also known for all the values t. This is the variance-covariance matrix at the system level.\n\n> Again, if we think about these two equations, we have the model defined in this way. \n\n> There is a next piece that we need to consider if we are going to perform based in inference for the model parameters. The next piece that we need to consider to just fully specify the model is what is the prior distribution. In a normal dynamic linear model, the prior distribution is assumed to be conjugate here. In the case again in which V_t and W_t are known, we are going to be assuming that, say that zero, the parameter vector before observing any data is going to be normally distributed. Multivariate normal with M_0 and C_0.\n\n$$\n\\theta_0 \\sim N(M_0, C_0)\n$$\n\n\n> The mean is a vector, again of the same dimension as Theta 0. Then I have k by k covariance matrix there as well. These are assumed to be also given to move forward with the model. In terms of the inference, there are different kinds of densities and quantities that we are interested in. One of the distributions that we are interested in finding is the so-called filtering distribution. We may be interested here in finding what is the density of Theta t given all the observations that we have up to time t. I'm going to call and all the information that I have up to time t.\n$$\nD_t= \\{D_0, y_{1:T}\\}\n$$\n\n> I'm going to call that D_t. It can also be, in some cases, I will just write down. So D_t, you can view with all the info up to time t. Usually, it is all the information I have at time zero. Then coupled, if there is no additional information that's going to be coupled with all the data points I have up to that time. Here I'm conditioning on all the observed quantities and the prior information up to time t, and I may be interested in just finding what is the distribution for Theta t. This is called filtering.\n\n$$\np(\\theta_t \\mid D_t) \\qquad \\text{filtering}\n$$\n\n> Another quantity that is very important in time series analysis is forecasting. I may be interested in just what is the density, the distribution of yt plus h? Again, the number of steps ahead here, here I'm thinking of h, given that I have all this information up to time t. I'm interested in predictions here. We will be talking about forecasting.\n\n$$\np(y_{t+h} \\mid D_t) \\qquad \\text{forecasting}\n$$\n\n> Then another important quantity or an important set of distributions is what we call the smoothing distribution. Usually, you have a time series, when you get your data, you observe, I don't know, 300 data points. As you go with the filtering, you are going to start from zero all the way to 300 and you're going to update these filtering distributions as you go and move forward. But then you may want to revisit your parameter at time 10, for example, given that you now have observed all these 300 observations. In that case, you're interested in densities that are of the form. Let's say that you observe capital T in your process and now you are going to revisit that density for Theta t. This is now in the past. Here we assume that t is smaller than capital T. This is called smoothing. So you have more observation once you have seen the data. We will talk about how to perform Bayesian inference to obtain all these distributions under this model setting. \n\n$$\np(\\theta_t \\mid D_T)  \\qquad t < T \\qquad \\text{smoothing}\n$$\n\n![slide 3](m3_0003.png){.column-margin width=\"250px\"}\n\n> In addition to all the structure that we described before and all the densities that we are interested in finding, we also have as usual, the so-called forecast function, which is just instead of being the density is just expected value of y(t+h) given all the information I have up to time t. In the case of a general normal dynamic linear model, we have the structure for these just using the equations, the observation and the system of equations. We're going to have here G_(t+h_. We multiply all these all the way to G_(t+1), and then we have the expected value of Theta_t given Dt. This is the form of the forecast function. There are particular cases and particular models that we will be discussing in which the Ft is equal to F, so is constant for all t and G_t is also constant for all t. In those cases, the forecast function can be simplified and written as F transpose G to the power of h expected value.\n\n$$\nf_t(h) = E(y_{t+h} \\mid D_t) = F'_{t+h} G_{t+h}\\ldots G_{t+1} E(\\theta_t \\mid D_t)\n$$\n\n> One thing that we will learn is that the eigenstructure of this matrix is very important to define the form of the forecast function, and it's very important for model building and for adding components into your model. Finally, just in terms of short notation, we can always write down when we're working with normal dynamic linear models, we may be referring to the model instead of writing the two equations, the system and the observation equation. I can just write all the components that define my model.\n\nIf $F_t=F$ and $G_t=G$ for all t, then the forecast function simplifies to:\n\n$$\nf_t(h) = E(y_{t+h} \\mid D_t) = F' G^h E(\\theta_t \\mid D_t)\n$$\n\n> This fully specifies the model in terms of the two equations. If I know what Ft is, what Gt is, what Vt is, and the covariance at the system level. \n\n> I sometimes will be just talking about just a short notation like this for defining the model.\n\n$$ \n\\{F_t, G_t, v_t, W_t\\} \n$$ {#eq-nldm-model-shothand-notation}\n\n## Polynomial Trend NDLM (Video)\n\n![slide 1](m3_0011.png){.column-margin width=\"250px\"}\n\n\n\n\n\n{{< lipsum 1 >}}\n\n\n\n\n\n\n\n\n![slide 2](m3_0012.png){.column-margin width=\"250px\"}\n\n\n\n\n\n{{< lipsum 1 >}}\n\n\n\n\n\n\n\n\n## Regression models (Video)\n\n![slide 1](m3_0021.png){.column-margin width=\"250px\"}\n\n\n\n\n\n{{< lipsum 1 >}}\n\n\n\n\n\n\n\n\n\n## Summary of polynomial trend and regression models (Reading)\n\n### Polynomial Trend Models\n\n#### First-Order Polynomial\n\n$$\n\\begin{aligned}\ny_t &= \\mu_t + \\nu_t, \\qquad & \\nu_t &\\sim N(0, v_t) \\\\\n\\mu_t &= \\mu_{t-1} + \\omega_t, \\qquad & \\omega_t &\\sim N(0, w_t)\n\\end{aligned}\n$$\n\nIn this case, we have:\n\n$\\theta_t = \\mu_t \\quad \\forall t$\n\n$F_t = 1 \\quad \\forall t$, \n\n$G_t = 1 \\quad \\forall t$, \n\nresulting in:\n\n$$\n\\{1, 1, v_t, w_t\\} \\qquad \\text{(short notation)}\n$$\n\nThe forecast function is:\n\n$$\nf_t(h) = E(\\mu_t \\mid \\mathcal{D}_t) = k_t, \\quad \\forall h > 0.\n$$\n\n#### Second-Order Polynomial\n\n$$\\begin{aligned}\n  y_t &= \\theta_{t,1} + \\nu_t, \\quad &\\nu_t &\\sim N(0, v_t) \\\\\n  \\theta_{t,1} &= \\theta_{t-1,1} + \\theta_{t-1,2} + \\omega_{t,1}, \\qquad &\\omega_{t,1} &\\sim N(0, w_{t,11}) \\\\\n  \\theta_{t,2} &= \\theta_{t-1,2} + \\omega_{t,2}, \\qquad &\\omega_{t,2} &\\sim N(0, w_{t,22}),\n\\end{aligned}\n$$\n\nwhere we can also have:\n\n$$\n\\text{Cov}(\\theta_{t,1}, \\theta_{t,2} ) = w_{t,12} = w_{t,21}\n$$\n\nThis can be written as a DLM with the state-space vector $\\theta_t = (\\theta_{t,1}, \\theta_{t,2})'$, and \n\n$$\n\\{\\mathbf{F}, \\mathbf{G}, v_t, \\mathbf{W}_t\\}  \\qquad \\text{(short notation)}\n$$ \n\nwith $\\mathbf{F} = (1, 0)'$ and \n\n$$\n\\mathbf{G} = \n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}, \\quad \\mathbf{W}_t = \n\\begin{pmatrix}\nw_{t,11} & w_{t,12} \\\\\nw_{t,21} & w_{t,22}\n\\end{pmatrix}.\n$$\n\nNote that \n\n$$\n\\mathbf{G}^2 = \n\\begin{pmatrix}\n1 & 2 \\\\\n0 & 1\n\\end{pmatrix}, \\quad \\mathbf{G}^h = \n\\begin{pmatrix}\n1 & h \\\\\n0 & 1\n\\end{pmatrix},\n$$\n\nand so:\n\n$$\nf_t(h) = (1, h) E(\\mathbf{\\theta}_t \\mid \\mathcal{D}_t) = (1, h) (k_{t,0}, k_{t,1})' = (k_{t,0} + h k_{t,1}).\n$$\n\nHere $\\mathbf{G} = \\mathbf{J}_2(1)$ (see below). \n\nAlso, we denote $\\mathbf{E}_2 = (1, 0)'$, and so the short notation for this model is \n\n$$\n\\{E_2, J_2(1), \\cdot, \\cdot\\}\n$$\n\n#### General $p$-th Order Polynomial Model\n\nWe can consider a $p$-th order polynomial model. This model will have a state-space vector of dimension $p$ and a polynomial of order $p-1$ forecast function on $h$. The model can be written as \n\n$$\\{E_p, J_p(1), v_t, W_t\\}  \\qquad \\text{(short notation)}\n$$\n\nwith $\\mathbf{F}_t = \\mathbf{E}_p = (1, 0, \\dots, 0)'$ and $\\mathbf{G}_t = \\mathbf{J}_p(1)$, with\n\n$$\n\\mathbf{J}_p(1) =\n\\begin{pmatrix}\n1 & 1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n0 & 1 & 1 & \\cdots & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 0 & 1\n\\end{pmatrix}.\n$$\n\nThe forecast function is given by\n\n$$\nf_t(h) = k_{t,0} + k_{t,1} h + \\dots + k_{t,p-1} h^{p-1}.\n$$\n\nThere is also an alternative parameterization of this model that leads to the same algebraic form of the forecast function, given by $\\{E_p, L_p, v_t, W_t\\}$, with\n\n$$\nL_p =\n\\begin{pmatrix}\n1 & 1 & 1 & \\cdots & 1 \\\\\n0 & 1 & 1 & \\cdots & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{pmatrix}.\n$$\n\n#### Dynamic Regression Models\n\n##### Simple Dynamic Regression\n\n$$\n\\begin{aligned}\n  y_t &= \\beta_{t,0} + \\beta_{t,1} x_t + \\nu_t \\\\\n  \\beta_{t,0} &= \\beta_{t-1,0} + \\omega_{t,0} \\\\\n  \\beta_{t,1} &= \\beta_{t-1,1} + \\omega_{t,1} \n\\end{aligned}\n$$\n\nThus:\n\n$$\n\\theta_t = (\\beta_{t,0}, \\beta_{t,1})'\n$$\n\n$$\nF_t = (1, x_t)'\n$$ \n\nand \n\n$$\nG = I_2\n$$\n\nThis results in a forecast function of the form\n\n$$\nf_t(h) = k_{t,0} + k_{t,1} x_{t+h}.\n$$\n\n#### General Dynamic Regression\n\n$$\n\\begin{aligned}\ny_t &= \\beta_{t,0} + \\beta_{t,1} x_{t,1} + \\dots + \\beta_{t,M} x_{t,M} + \\nu_t \\\\\n\\beta_{t,m} &= \\beta_{t-1,m} + \\omega_{t,m}, \\quad &m = 0:M.\n\\end{aligned}\n$$\n\nThen, \n\n$\\theta_t = (\\beta_{t,0}, \\dots, \\beta_{t,M})'$, \n\n$\\mathbf{F}_t = (1, x_{t,1}, \\dots, x_{t,M})'$ and \n\n$\\mathbf{G} = \\mathbf{I}_M$. \n\nThe forecast function is given by:\n\n$$\nf_t(h) = k_{t,0} + k_{t,1} x_{t+h,1} + \\dots + k_{t,M} x_{t+h,M}.\n$$\n\nA particular case of dynamic regressions is the case of **time-varying autoregressions (TVAR)** with\n[time-varying autoregressions (TVAR)]{.column-margin width=\"250px\"}\n\n$$\n\\begin{aligned}\n  y_t &= \\phi_{t,1} y_{t-1} + \\phi_{t,2} y_{t-2} + \\dots + \\phi_{t,p} y_{t-p} + \\nu_t \\\\\n  \\phi_{t,m} &= \\phi_{t-1,m} + \\omega_{t,m}, \\quad m = 1:p.\n\\end{aligned}\n$$\n\n## The superposition principle (Video)\n\n![slide 1](m3_0031.png){.column-margin width=\"250px\"}\n\n> [We can use the superposition principle to build models that have different kinds of components. The main idea is to think about what is the general structure we want for the forecast function and then isolate the different components of the forecast function and think about the classes of dynamic linear models that are represented in each of those components.]{.mark} Each of those components has a class and then we can build the general dynamic linear model with all those pieces together using this principle.\n\n> I will illustrate how to do that with an example:\n> Let's say that you want to create a model here with a forecast function that has a linear trend component. Let's say we have a linear function as a function of the number of steps ahead that you want to consider. Then suppose you also have a covariate here that you want to include in your model as a regression component.\n$$\nf_t(h) = \\underbrace{(k_{t,0} + k_{t,1}h)}_{\\text{linear trend component}} + \\underbrace{(k_{t,2}x_{t+h})}_{\\text{regression component}}\n$$ {#eq-superposition-principle-example}\n\nwhere:\n\n- $f_t(h)$ is the forecast function.\n- $k_{t,0}$, $k_{t,1}$ and $k_{t,2}$ are constants that depend on time $t$.\n- $x_{t+h}$ is the covariate.\n\n Let's say we have a $K_{t2}$ and then we have $x_{t+h}$, this is my covariate. Again, the k's here are just constants, as of constants in terms of h, they are dependent on time. This is the general structure we want to have for the forecast function. \n \n > [When I look at the forecast function, I can isolate and separate these two components. I have a component that looks like a linear trend and then I have a component that is a regression component. Each of this can be set in terms of two forecast functions]{.mark}. I'm going to call the forecast function $f_1t$ h, this is just the first piece.\n\n\n$$\nf_t(h) = f_{1,t}(h) + f_{2,t}(h)\n$$\n\nwhere:\n\n$$\nf_{1,t}(h) = k_{t,0} + k_{t,1} \\qquad \\text{(linear trend component)}\n$$ {#eq-superposition-principle-example-linear-trend-component}\nand\n$$\nf_{2,t}(h) = k_{t,2}x_{t+h} \\qquad \\text{(regression component)}\n$$ {#eq-superposition-principle-example-regression-component}\n\n\n>Then I have my second piece here. I'm going to call it $f_{2t}$, is just this piece here with the regression component. We know how to represent this forecast function in terms of a dynamic linear model. \n\nFor the linear trend component, we have a 2-dimensional state vector, $\\theta_t = (\\theta_{t,1}, \\theta_{t,2})'$, with\n\n\nfor the linear trend component $f_{1,t}(h)$ we have the following DLM representation:\n\n$$\\{F_1, G_1, \\cdot, \\cdot\\}  \\qquad \\text{(short notation)}\n$$\n\nwhere we don't explicitly specify the observational and system variances, V and W - The important bit are F,and G. The forecast function is given by:\n\n$$\nF_{1} = E_2 = (1, 0)'\n$$\n\n$$\nG_{1} =\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n$$\n\nfor the regression component $f_{2,t}(h)$ we have the following DLM representation:\n\n$$\\{F_2,t, G_2, \\cdot, \\cdot\\}  \\qquad \\text{(short notation)}\n$$\n\nwhere \n\n$$\nF_{2,t} = x_{t+h}\n$$\n\n$$\nG_{2} = 1\n$$\n\n\n> I can write down a model that has an F, G, and some V, and some W that I'm going to just leave here and not specify them explicitly because the important components for the structure of the model are the F and the G. If you'll recall the F in the case of a forecast function with a linear trend like this, is just my E_2 vector, which is a two-dimensional vector. The first entry is one, and the second one is a zero. Then the G in this case is just this upper triangular matrix that has 1, 1 in the first row and 0, 1 in the second one. Remember, in this case we have a two-dimensional state vector where one of the components in the vector is telling me information about the level of the time series, the other component is telling me about the rate of change in that level. This is a representation that corresponds to this forecast function. For this other forecast function, we have a single covariate, it's just a regression and I can represent these in terms of an F_2, G_2, and then some observational variance and some system variance here in the case of a single covariate and this one depends on t. We have F_2t is X_t and my G here is simply going to be one. This is a one-dimensional vector in terms of the state parameter vector. We have a single state vector and it's just going to tell me about the changes, the coefficient that goes with the X_t covariate. Once I have these, I can create my final model and I'm going to just say that my final model is F, G, and then I have some observational variance and some covariance also for the system where the F is going to be an F that has, you just concatenate the two Fs. You're going to get 1, 0 and then you're going to put the next component here. Again, this one is dependent on time because this component is time dependent and then the G, you can create it just taking a block diagonal structure,\n\n> $G_1$ and $G_2$. You just put together, the first one is 1, 1, 0, 1 and then I concatenate this one as a block diagonal. This should be one.\n\n> This gives me the full G function for the model. Now a model with this $F_t$ and this $G$ that is constant over time will give me this particular forecast function. I'm using the superposition principle to build this model. If you want additional components, we will learn how to incorporate seasonal components, regression components, trend components. You can build a fairly sophisticated model with different structures into this particular model using the superposition principle.\n\n\n## Superposition principle: General case (Reading)\n\nYou can build dynamic models with different components, for example, a trend component plus a regression component, by using the principle of superposition. The idea is to think about the general form of the forecast function you want to have for prediction. You then write that forecast function as a sum of different components where each component corresponds to a class of DLM with its own state-space representation. The final DLM can then be written by combining the pieces of the different components.\n\nFor example, suppose you are interested in a model with a forecast function that includes a linear polynomial trend and a single covariate $x_t$, i.e.,\n\n$$\nf_t(h) = k_{t,0} + k_{t,1}h + k_{t,3}x_{t+h}.\n$$\n\nThis forecast function can be written as $f_t(h) = f_{1,t}(h) + f_{2,t}(h)$, with\n\n$$\nf_{1,t}(h) = (k_{t,0} + k_{t,1}h), \\quad f_{2,t}(h) = k_{t,3}x_{t+h}.\n$$\n\nThe first component in the forecast function corresponds to a model with a 2-dimensional state vector, $F_{1,t} = F_1 = (1, 0)'$,\n\n$$\nG_{1,t} = G_1 = \n\\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\n\nThe second component corresponds to a model with a 1-dimensional state vector, $F_{2,t} = x_t$, $G_{2,t} = G_2 = 1$.\n\nThe model with forecast function $f_t(h)$ above is a model with a 3-dimensional state vector with $F_t = (F_1', F_{2,t})' = (1, 0, x_t)'$ and\n\n$$\nG_t = \\text{blockdiag}[G_1, G_2] = \n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\n\n### General Case\n\nAssume that you have a time series process $y_t$ with a forecast function\n\n$$\nf_t(h) = \\sum_{i=1}^{m} f_{i,t}(h),\n$$\n\nwhere each $f_{i,t}(h)$ is the forecast function of a DLM with representation $\\{F_{i,t}, G_{i,t}, v_{i,t}, W_{i,t}\\}$.\n\nThen, $f_t(h)$ has a DLM representation $\\{F_t, G_t, v_t, W_t\\}$ with\n\n$$\nF_t = (F_{1,t}', F_{2,t}', \\dots, F_{m,t}')',\n$$\n\n$$\nG_t = \\text{blockdiag}[G_{1,t}, \\dots, G_{m,t}],\n$$\n\n$$\nv_t = \\sum_{i=1}^{m} v_{i,t},\n$$\n\nand\n\n$$\nW_t = \\text{blockdiag}[W_{1,t}, \\dots, W_{m,t}].\n$$\n \n\n## Quiz: The Normal Dynamic Linear Model\n\nOmitted due to Coursera honor code\n\n# Bayesian Inference in the NDLM: Part 1\n\n## Filtering (Video)\n\n![slide 1](m3_0041.png){.column-margin width=\"250px\"}\n\n![slide 1](m3_0042.png){.column-margin width=\"250px\"}\n\n\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n\n\n\n\n## Summary of filtering distributions (Reading)\n\n### Bayesian Inference in NDLM: Known Variances\n\nConsider an NDLM given by:\n\n$$\ny_t = F_t' \\theta_t + \\nu_t, \\quad \\nu_t \\sim N(0, v_t), \\tag{1}\n$$\n\n$$\n\\theta_t = G_t \\theta_{t-1} + \\omega_t, \\quad \\omega_t \\sim N(0, W_t), \\tag{2}\n$$\n\nwith $F_t$, $G_t$, $v_t$, and $W_t$ known. We also assume a prior distribution of the form $(\\theta_0 \\mid D_0) \\sim N(m_0, C_0)$, with $m_0$, $C_0$ known.\n\n#### Filtering\n\nWe are interested in finding $p(\\theta_t \\mid D_t)$ for all $t$. Assume that the posterior at $t-1$ is such that:\n\n$$\n(\\theta_{t-1} \\mid D_{t-1}) \\sim N(m_{t-1}, C_{t-1}).\n$$\n\nThen, we can obtain the following:\n\n1. Prior at Time $t$\n\n$$\n(\\theta_t \\mid D_{t-1}) \\sim N(a_t, R_t),\n$$\n\nwith\n\n$$\na_t = G_t m_{t-1},\n$$\n\nand\n\n$$\nR_t = G_t C_{t-1} G_t' + W_t.\n$$\n\n2. One-Step Forecast\n\n$$\n(y_t \\mid D_{t-1}) \\sim N(f_t, q_t),\n$$\n\nwith\n\n$$\nf_t = F_t' a_t, \\quad q_t = F_t' R_t F_t + v_t.\n$$\n\n3. Posterior at Time $t:$ $(\\theta_t \\mid \\mathcal{D}_t) \\sim N(m_t, C_t)$ with\n\n$$\n\\begin{aligned}\nm_t &= a_t + R_t F_t q_t^{-1} (y_t - f_t), \\\\\nC_t &= R_t - R_t F_t q_t^{-1} F_t' R_t.\n\\end{aligned}\n$$\n\nNow, denoting $e_t = (y_t - f_t)$ and $A_t = R_t F_t q_t^{-1}$, we can rewrite the equations above as:\n\n$$\n\\begin{aligned}\nm_t &= a_t + A_t e_t, \\\\\nC_t &= R_t - A_t q_t A_t'\n\\end{aligned}\n$$\n\n## Rcode Filtering in the NDLM: Example (Reading)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#################################################\n##### Univariate DLM: Known, constant variances\n#################################################\nset_up_dlm_matrices <- function(FF, GG, VV, WW){\n  return(list(FF=FF, GG=GG, VV=VV, WW=WW))\n}\n\nset_up_initial_states <- function(m0, C0){\n  return(list(m0=m0, C0=C0))\n}\n\n### forward update equations ###\nforward_filter <- function(data, matrices, initial_states){\n  ## retrieve dataset\n  yt <- data$yt\n  T <- length(yt)\n  \n  ## retrieve a set of quadruples \n  # FF, GG, VV, WW are scalar\n  FF <- matrices$FF  \n  GG <- matrices$GG\n  VV <- matrices$VV\n  WW <- matrices$WW\n  \n  ## retrieve initial states\n  m0 <- initial_states$m0\n  C0 <- initial_states$C0\n  \n  ## create placeholder for results\n  d <- dim(GG)[1]\n  at <- matrix(NA, nrow=T, ncol=d)\n  Rt <- array(NA, dim=c(d, d, T))\n  ft <- numeric(T)\n  Qt <- numeric(T)\n  mt <- matrix(NA, nrow=T, ncol=d)\n  Ct <- array(NA, dim=c(d, d, T))\n  et <- numeric(T)\n  \n  for(i in 1:T){\n    # moments of priors at t\n    if(i == 1){\n      at[i, ] <- GG %*% t(m0)\n      Rt[, , i] <- GG %*% C0 %*% t(GG) + WW\n      Rt[, , i] <- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }else{\n      at[i, ] <- GG %*% t(mt[i-1, , drop=FALSE])\n      Rt[, , i] <- GG %*% Ct[, , i-1] %*% t(GG) + WW\n      Rt[, , i] <- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }\n    \n    # moments of one-step forecast:\n    ft[i] <- t(FF) %*% (at[i, ]) \n    Qt[i] <- t(FF) %*% Rt[, , i] %*% FF + VV\n    \n    # moments of posterior at t:\n    At <- Rt[, , i] %*% FF / Qt[i]\n    et[i] <- yt[i] - ft[i]\n    mt[i, ] <- at[i, ] + t(At) * et[i]\n    Ct[, , i] <- Rt[, , i] - Qt[i] * At %*% t(At)\n    Ct[, , i] <- 0.5*Ct[, , i]+ 0.5*t(Ct[, , i])\n  }\n  cat(\"Forward filtering is completed!\") # indicator of completion\n  return(list(mt = mt, Ct = Ct, at = at, Rt = \n                Rt, ft = ft, Qt = Qt))\n}\n\nforecast_function <- function(posterior_states, k, matrices){\n  \n  ## retrieve matrices\n  FF <- matrices$FF\n  GG <- matrices$GG\n  WW <- matrices$WW\n  VV <- matrices$VV\n  mt <- posterior_states$mt\n  Ct <- posterior_states$Ct\n  \n  ## set up matrices\n  T <- dim(mt)[1] # time points\n  d <- dim(mt)[2] # dimension of state-space parameter vector\n  \n  ## placeholder for results\n  at <- matrix(NA, nrow = k, ncol = d)\n  Rt <- array(NA, dim=c(d, d, k))\n  ft <- numeric(k)\n  Qt <- numeric(k)\n  \n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] <- GG %*% t(mt[T, , drop=FALSE])\n      Rt[, , i] <- GG %*% Ct[, , T] %*% t(GG) + WW\n      Rt[, , i] <- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }else{\n      at[i, ] <- GG %*% t(at[i-1, , drop=FALSE])\n      Rt[, , i] <- GG %*% Rt[, , i-1] %*% t(GG) + WW\n      Rt[, , i] <- 0.5*Rt[, , i]+0.5*t(Rt[, , i])\n    }\n    \n    ## moments of forecast distribution\n    ft[i] <- t(FF) %*% t(at[i, , drop=FALSE])\n    Qt[i] <- t(FF) %*% Rt[, , i] %*% FF + VV\n  }\n  cat(\"Forecasting is completed!\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval <- function(mu, sigma2, \n                          quantile = c(0.025, 0.975)){\n  z_quantile <- qnorm(quantile)\n  bound <- matrix(0, nrow=length(mu), ncol=2)\n  bound[, 1] <- mu + \n    z_quantile[1]*sqrt(as.numeric(sigma2)) # lower bound\n  bound[, 2] <- mu + \n    z_quantile[2]*sqrt(as.numeric(sigma2)) # upper bound\n  return(bound)\n}\n\n####################### Example: Lake Huron Data ######################\nplot(LakeHuron,main=\"Lake Huron Data\",\n     ylab=\"level in feet\") # Total of 98 observations \n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/Filtering in the NDLM-1.png){width=672}\n:::\n\n```{.r .cell-code}\nk=4\nT=length(LakeHuron)-k # We take the first 94 observations \n                      # only as our data\nts_data=LakeHuron[1:T]\nts_validation_data <- LakeHuron[(T+1):98]\n\ndata <- list(yt = ts_data)\n\n# First order polynomial model \n\n## set up the DLM matrices \nFF <- as.matrix(1)\nGG <- as.matrix(1)\nVV <- as.matrix(1)\nWW <- as.matrix(1)\nm0 <- as.matrix(570)\nC0 <- as.matrix(1e4)\n\n## wrap up all matrices and initial values\nmatrices <- set_up_dlm_matrices(FF, GG, VV, WW)\ninitial_states <- set_up_initial_states(m0, C0)\n\n## filtering\nresults_filtered <- forward_filter(data, matrices, \n                                   initial_states)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nForward filtering is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\nnames(results_filtered)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"mt\" \"Ct\" \"at\" \"Rt\" \"ft\" \"Qt\"\n```\n\n\n:::\n\n```{.r .cell-code}\nci_filtered <- get_credible_interval(results_filtered$mt, \n                                     results_filtered$Ct)\n\n## forecasting \nresults_forecast <- forecast_function(results_filtered,k, \n                                      matrices)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nForecasting is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\nci_forecast <- get_credible_interval(results_forecast$ft, \n                                     results_forecast$Qt)\n\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\nindex_forecast=index[(T+1):98]\n\nplot(index, LakeHuron, ylab = \"level\", \n     main = \"Lake Huron Level\",type='l',\n     xlab=\"time\",lty=3,ylim=c(574,584))\npoints(index,LakeHuron,pch=20)\n\nlines(index_filt, results_filtered$mt, type='l',\n      col='red',lwd=2)\nlines(index_filt, ci_filtered[, 1], type='l', \n      col='red', lty=2)\nlines(index_filt, ci_filtered[, 2], type='l', col='red', lty=2)\n\n\nlines(index_forecast, results_forecast$ft, type='l',\n      col='green',lwd=2)\nlines(index_forecast, ci_forecast[, 1], type='l',\n      col='green', lty=2)\nlines(index_forecast, ci_forecast[, 2], type='l',\n      col='green', lty=2)\n\nlegend('bottomleft', legend=c(\"filtered\",\"forecast\"),\n       col = c(\"red\", \"green\"), lty=c(1, 1))\n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/Filtering in the NDLM-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#Now consider a 100 times smaller signal to noise ratio \nVV <- as.matrix(1)\nWW <- as.matrix(0.01)\nmatrices_2 <- set_up_dlm_matrices(FF,GG, VV, WW)\n\n## filtering\nresults_filtered_2 <- forward_filter(data, matrices_2, \n                                     initial_states)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nForward filtering is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\nci_filtered_2 <- get_credible_interval(results_filtered_2$mt, \n                                       results_filtered_2$Ct)\n\nresults_forecast_2 <- forecast_function(results_filtered_2, \n                             length(ts_validation_data), \n                             matrices_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nForecasting is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\nci_forecast_2 <- get_credible_interval(results_forecast_2$ft, \n                                       results_forecast_2$Qt)\n\n\nplot(index, LakeHuron, ylab = \"level\", \n     main = \"Lake Huron Level\",type='l',\n     xlab=\"time\",lty=3,ylim=c(574,584))\npoints(index,LakeHuron,pch=20)\n\nlines(index_filt, results_filtered_2$mt, type='l', \n      col='magenta',lwd=2)\nlines(index_filt, ci_filtered_2[, 1], type='l', \n      col='magenta', lty=2)\nlines(index_filt, ci_filtered_2[, 2], type='l', \n      col='magenta', lty=2)\n\nlines(index_forecast, results_forecast_2$ft, type='l', \n      col='green',lwd=2)\nlines(index_forecast, ci_forecast_2[, 1], type='l', \n      col='green', lty=2)\nlines(index_forecast, ci_forecast_2[, 2], type='l', \n      col='green', lty=2)\n\nlegend('bottomleft', legend=c(\"filtered\",\"forecast\"),\n       col = c(\"magenta\", \"green\"), lty=c(1, 1))\n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/Filtering in the NDLM-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(index_filt,results_filtered$mt,type='l',col='red',lwd=2,\n     ylim=c(574,584),ylab=\"level\")\nlines(index_filt,results_filtered_2$mt,col='magenta',lwd=2)\npoints(index,LakeHuron,pch=20)\nlines(index,LakeHuron,lty=2)\n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/Filtering in the NDLM-4.png){width=672}\n:::\n:::\n\n\n\n\n\n## Smoothing and forecasting (Video)\n\n\n![Smoothing](m3_0051.png){.column-margin width=\"250px\"}\n\n> We know, we now discuss the smoothing equations for the case of the normal dynamic linear model. When we are assuming that both the variance at the observation level is known and the covariance matrix at the system level is also known. Recall we have two equations here, we have the observation equation, where yt is modeled as Ft'θt + noise the noise is N(0,vt). And we're assuming that the vt is given. We are also assuming that we know Ft for all t. And then in the evolution equation we have θt= Gtθ(t-1)+noise. And then again, the assumption for the wt is here is that they are normally distributed with mean zero, and these variants co variance matrix, capital Wt. So we can summarize the model in terms of Ft, Gt, Vt and Wt, that are given for all t. We have discussed the filtering equations. So the process for obtaining the distributions of θt given Dt, as we collect observations over time is called filtering. Now we will discuss what happens when we do smoothing, meaning when we revisit the distributions of θt, given now that we have received a set of observations. So Just to illustrate the process, we have here, θ0,θ1 all way up to θ4. And we can assume just for the sake of the example, that we are going to receive three observations. So we are going to proceed with the filtering, and then once we receive the last observation at time three, we're going to go backwards and we're going to revisit the distributions for the state parameters. So just to remind you how the filtering works, we move forward, before we receive any observations. In the normal dynamic linear model, when we have all the variances known. The conjugate prior distribution is a normal, with mean m0 and variance C0. So this is specified by the user, before collecting any observations. We can then use the structure of the model, meaning the system equation and the observation equation to obtain the distribution of θt, given D0. Before observing the first y. This gives us first the distribution of θt, θ1 given D0, which is normal a1 R1. And then we can also get the one step ahead forecast distribution for y1 given D0, which is a normal f1 and q1. And we have discussed how to obtain these moments using the filtering equations. Then we received the first observation, and the first observation can allows us to update the distribution of θ1. So we obtain now the distribution of θ1 given y1, and whatever information we have at D0. So this gives us m1 C1. And using again the structure of the model, we can get the prior distribution for θ2 given the one and that's a normal a2 R2. And then the one step ahead forecast distribution now for y2 given D1 and that's a normal f2 q2. So we can receive y2 update the distribution of θ2 and we can continue this process, now get the priors at T=3. And then once we get the observation at T=3, we update the distribution. And we can continue like this with the prior for θ4 and so on. Let's say that we stop here, at T=3. And now we are interested in answering the question. Well, what is the distribution for example of θ2 given that, now, I obtain not only y1 and y2, but also y3. I want to revisit that distribution using all that information. Same thing for say, the distribution of θ0, given the D0, y1, y2 and y3. So that's what it's called smoothing. So the smoothing equations, allow us to obtain those distributions. So just to talk a little bit about the notation again, in the normal dynamic linear model where vt and wt are known for all t's. We have that this is a normal, so the notation here, the T, is larger than t, here. So we're looking at the distribution of θt, now in the past and that one follows a normal distribution with mean aT(t-T). So the notation here for the subscript T means that I'm conditioning on all the information I have to T. And then the variance covariance matrix is given by this, RT(t-T). So this is just going to indicate how many steps I'm going to go backwards as you will see in the example.\n\n> So we have some recursions in the same way that we have the filtering equations. Now we have the smoothing equations. And for these smoothing equations we have that the mean. You can see here, that whenever you're computing a particular step t- T, you're going to need a quantity that you computed in the previous step, t-T+1. So you're going to need that, is a recursion, but you're also going to need mt and and at+1. So those are quantities that you computed using the filtering equations. So in order to get the smoothing equations, you first have to proceed with the filtering. Similarly for RT(t-T), you have also that depends on something you previously obtained. And then you also have the Ct, the Rt+1 and so on. So those quantities you computed when you were updating the filtering equations. The recursion begins with aT(0) meaning that you are not going to go backwards any points in time. So that is precisely the mean is going to be whatever you computed with the filtering equations of up to T, that's mT. And then RT(0) is going to be CT. So just to again illustrate how this would work in the example, if we start here right? If we condition, so the first step would be to compute again to initialize using the distribution of θ3 given D3. And that is a normal with mean a3(0) and variance covariance matrix R3(0), But those are precisely m3 and C3 respectively. Then we go backwards one step. And if we want to look at what is the distribution of θ2, now conditional on D3. That's a normal with mean a3(-1) and variance covariance matrix R3(-1). So if you look at the equations down here, you will see that, in order to compute a3 (-1), and R3(-1). You're going to need m2,C2, a3,R3 and then what you computed here these moments in the previous step, a3(0) and R3(0). Then you obtain that distribution and you can now look at the distribution of θ1 given D3, that's the normal a3(-2), R3(-2). And once again, to compute these moments, you're going to need m1,C1,a2,R2 and then you're going to need a3(-1),R3(-1). And you can continue all the way down to θ0 given D3 using these recursions. So the smoothing equations allow us to, just compute all these distributions. And the important equations work basically because of the linear and Gaussian structure in the normal dynamic linear model. \n\n\n![Forecasting](m3_0051.png){.column-margin width=\"250px\"}\n\n> In a similar way, we can compute the forecasting distributions. Now we are going to be looking forward, and in the case of forecasting, we are interested in the distribution of θ(t+h) given Dt. And now h is a positive lag. So here we assume that is h≥0. So we are going to have the recursion is a N(at(h), Rt(h)). The mean is at(h) and we are going to use the structure of the model to obtain these recursions, again. So here we are using the system equation, and the moment at(h) depends on what you computed at at(h-1) the previous lag, times Gt+h. And then, would you initialize the recursion with at(0)=mt. \n\n> Similarly, for the covariance matrix h steps ahead, you're going to have a recursion that depends on Rt(h-1). And then you're going to need to input also Gt+h and Wt+h. To initialize, the recursion with Rt(0)= Ct. So you can see that in order to compute these moments, you're going to need mt and Ct to start with. And then you're also going to have to input all the G's and the W's for the number of steps ahead that you require.\n\n> Similarly, you can compute the distribution, the h steps ahead distribution of yt+h given Dt. And that one also follows a normal, with mean ft(h), qt(h). And now we also have a recursion here, ft(h) depends on at(h) and as we said, at(h) depends on at(h-1) and so on. And qt(h) is just given by these equations. So once again, you have to have access to Ft+h for all the h, a number of steps ahead that you are trying to compute this distribution. And then you also have to provide the observational variance for every h value. So that you get vt+h. So this is specified in the modeling framework as well. If you want proceed with the forecasting distributions.\n\n## Summary of the smoothing and forecasting distributions (reading)\n\n<!-- start -->\n\n### Bayesian Inference in NDLM: Known Variances\n\nConsider the NDLM given by:\n\n$$\ny_t = F_t' \\theta_t + \\nu_t, \\quad \\nu_t \\sim N(0, v_t), \\tag{1}\n$$\n\n$$\n\\theta_t = G_t \\theta_{t-1} + \\omega_t, \\quad \\omega_t \\sim N(0, W_t), \\tag{2}\n$$\n\nwith $F_t$, $G_t$, $v_t$, and $W_t$ known. We also assume a prior distribution of the form $(\\theta_0 \\mid D_0) \\sim N(m_0, C_0)$, with $m_0$ and $C_0$ known.\n\n#### Smoothing\n\nFor $t < T$, we have that:\n\n$$\n(\\theta_t \\mid D_T) \\sim N(a_T(t - T), R_T(t - T)),\n$$\n\nwhere\n\n$$\na_T(t - T) = m_t - B_t [a_{t+1} - a_T(t - T + 1)],\n$$\n\n$$\nR_T(t - T) = C_t - B_t [R_{t+1} - R_T(t - T + 1)] B_t',\n$$\n\nfor $t = (T - 1), (T - 2), \\dots, 0$, with $B_t = C_t G_t' R_{t+1}^{-1}$, and $a_T(0) = m_T$, $R_T(0) = C_T$. Here $a_t$, $m_t$, $R_t$, and $C_t$ are obtained using the filtering equations as explained before.\n\n#### Forecasting\n\nFor $h \\geq 0$, it is possible to show that:\n\n$$\n(\\theta_{t+h} \\mid D_t) \\sim N(a_t(h), R_t(h)),\n$$\n\n$$\n(y_{t+h} \\mid D_t) \\sim N(f_t(h), q_t(h)),\n$$\n\nwith\n\n$$\na_t(h) = G_{t+h} a_t(h - 1),\n$$\n\n$$\nR_t(h) = G_{t+h} R_t(h - 1) G_{t+h}' + W_{t+h},\n$$\n\n$$\nf_t(h) = F_{t+h}' a_t(h),\n$$\n\n$$\nq_t(h) = F_{t+h}' R_t(h) F_{t+h} + v_{t+h},\n$$\n\nand\n\n$$\na_t(0) = m_t, \\quad R_t(0) = C_t.\n$$\n\n<!-- end -->\n\n## Smoothing in the NDLM, Example (Video)\n\n\n\n## R-code: Smoothing in the NDLM, Example (Reading)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#################################################\n##### Univariate DLM: Known, constant variances\n#################################################\nset_up_dlm_matrices <- function(FF, GG, VV, WW){\n  return(list(FF=FF, GG=GG, VV=VV, WW=WW))\n}\n\nset_up_initial_states <- function(m0, C0){\n  return(list(m0=m0, C0=C0))\n}\n\n### forward update equations ###\nforward_filter <- function(data, matrices, initial_states){\n  ## retrieve dataset\n  yt <- data$yt\n  T <- length(yt)\n  \n  ## retrieve a set of quadruples \n  # FF, GG, VV, WW are scalar\n  FF <- matrices$FF  \n  GG <- matrices$GG\n  VV <- matrices$VV\n  WW <- matrices$WW\n  \n  ## retrieve initial states\n  m0 <- initial_states$m0\n  C0 <- initial_states$C0\n  \n  ## create placeholder for results\n  d <- dim(GG)[1]\n  at <- matrix(NA, nrow=T, ncol=d)\n  Rt <- array(NA, dim=c(d, d, T))\n  ft <- numeric(T)\n  Qt <- numeric(T)\n  mt <- matrix(NA, nrow=T, ncol=d)\n  Ct <- array(NA, dim=c(d, d, T))\n  et <- numeric(T)\n  \n  \n  for(i in 1:T){\n    # moments of priors at t\n    if(i == 1){\n      at[i, ] <- GG %*% t(m0)\n      Rt[, , i] <- GG %*% C0 %*% t(GG) + WW\n      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] <- GG %*% t(mt[i-1, , drop=FALSE])\n      Rt[, , i] <- GG %*% Ct[, , i-1] %*% t(GG) + WW\n      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    # moments of one-step forecast:\n    ft[i] <- t(FF) %*% (at[i, ]) \n    Qt[i] <- t(FF) %*% Rt[, , i] %*% FF + VV\n    \n    # moments of posterior at t:\n    At <- Rt[, , i] %*% FF / Qt[i]\n    et[i] <- yt[i] - ft[i]\n    mt[i, ] <- at[i, ] + t(At) * et[i]\n    Ct[, , i] <- Rt[, , i] - Qt[i] * At %*% t(At)\n    Ct[,,i] <- 0.5*Ct[,,i] + 0.5*t(Ct[,,i]) \n  }\n  cat(\"Forward filtering is completed!\") # indicator of completion\n  return(list(mt = mt, Ct = Ct, at = at, Rt = Rt, \n              ft = ft, Qt = Qt))\n}\n\n\nforecast_function <- function(posterior_states, k, matrices){\n  \n  ## retrieve matrices\n  FF <- matrices$FF\n  GG <- matrices$GG\n  WW <- matrices$WW\n  VV <- matrices$VV\n  mt <- posterior_states$mt\n  Ct <- posterior_states$Ct\n  \n  ## set up matrices\n  T <- dim(mt)[1] # time points\n  d <- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at <- matrix(NA, nrow = k, ncol = d)\n  Rt <- array(NA, dim=c(d, d, k))\n  ft <- numeric(k)\n  Qt <- numeric(k)\n  \n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] <- GG %*% t(mt[T, , drop=FALSE])\n      Rt[, , i] <- GG %*% Ct[, , T] %*% t(GG) + WW\n      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] <- GG %*% t(at[i-1, , drop=FALSE])\n      Rt[, , i] <- GG %*% Rt[, , i-1] %*% t(GG) + WW\n      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    ## moments of forecast distribution\n    ft[i] <- t(FF) %*% t(at[i, , drop=FALSE])\n    Qt[i] <- t(FF) %*% Rt[, , i] %*% FF + VV\n  }\n  cat(\"Forecasting is completed!\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval <- function(mu, sigma2, \n                          quantile = c(0.025, 0.975)){\n  z_quantile <- qnorm(quantile)\n  bound <- matrix(0, nrow=length(mu), ncol=2)\n  bound[, 1] <- mu + z_quantile[1]*sqrt(as.numeric(sigma2)) # lower bound\n  bound[, 2] <- mu + z_quantile[2]*sqrt(as.numeric(sigma2)) # upper bound\n  return(bound)\n}\n\n### smoothing equations ###\nbackward_smoothing <- function(data, matrices, \n                               posterior_states){\n  ## retrieve data \n  yt <- data$yt\n  T <- length(yt) \n  \n  ## retrieve matrices\n  FF <- matrices$FF\n  GG <- matrices$GG\n  \n  ## retrieve matrices\n  mt <- posterior_states$mt\n  Ct <- posterior_states$Ct\n  at <- posterior_states$at\n  Rt <- posterior_states$Rt\n  \n  ## create placeholder for posterior moments \n  mnt <- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt <- array(NA, dim = dim(Ct))\n  fnt <- numeric(T)\n  Qnt <- numeric(T)\n  for(i in T:1){\n    # moments for the distributions of the state vector given D_T\n    if(i == T){\n      mnt[i, ] <- mt[i, ]\n      Cnt[, , i] <- Ct[, , i]\n      Cnt[, , i] <- 0.5*Cnt[, , i] + 0.5*t(Cnt[, , i]) \n    }else{\n      inv_Rtp1<-solve(Rt[,,i+1])\n      Bt <- Ct[, , i] %*% t(GG) %*% inv_Rtp1\n      mnt[i, ] <- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n      Cnt[, , i] <- Ct[, , i] + Bt %*% (Cnt[, , i + 1] - Rt[, , i+1]) %*% t(Bt)\n      Cnt[,,i] <- 0.5*Cnt[,,i] + 0.5*t(Cnt[,,i]) \n    }\n    # moments for the smoothed distribution of the mean response of the series\n    fnt[i] <- t(FF) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] <- t(FF) %*% t(Cnt[, , i]) %*% FF\n  }\n  cat(\"Backward smoothing is completed!\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n####################### Example: Lake Huron Data ######################\nplot(LakeHuron,main=\"Lake Huron Data\",ylab=\"level in feet\") \n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# 98 observations total \nk=4\nT=length(LakeHuron)-k # We take the first 94 observations \n                     #  as our data\nts_data=LakeHuron[1:T]\nts_validation_data <- LakeHuron[(T+1):98]\n\ndata <- list(yt = ts_data)\n\n## set up matrices\nFF <- as.matrix(1)\nGG <- as.matrix(1)\nVV <- as.matrix(1)\nWW <- as.matrix(1)\nm0 <- as.matrix(570)\nC0 <- as.matrix(1e4)\n\n## wrap up all matrices and initial values\nmatrices <- set_up_dlm_matrices(FF,GG,VV,WW)\ninitial_states <- set_up_initial_states(m0, C0)\n\n## filtering\nresults_filtered <- forward_filter(data, matrices, \n                                   initial_states)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nForward filtering is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\nci_filtered<-get_credible_interval(results_filtered$mt,\n                                   results_filtered$Ct)\n## smoothing\nresults_smoothed <- backward_smoothing(data, matrices, \n                                       results_filtered)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBackward smoothing is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\nci_smoothed <- get_credible_interval(results_smoothed$mnt, \n                                     results_smoothed$Cnt)\n\n\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\n\nplot(index, LakeHuron, main = \"Lake Huron Level \",type='l',\n     xlab=\"time\",ylab=\"level in feet\",lty=3,ylim=c(575,583))\npoints(index,LakeHuron,pch=20)\n\nlines(index_filt, results_filtered$mt, type='l', \n      col='red',lwd=2)\nlines(index_filt, ci_filtered[,1], type='l', col='red',lty=2)\nlines(index_filt, ci_filtered[,2], type='l', col='red',lty=2)\n\nlines(index_filt, results_smoothed$mnt, type='l', \n      col='blue',lwd=2)\nlines(index_filt, ci_smoothed[,1], type='l', col='blue',lty=2)\nlines(index_filt, ci_smoothed[,2], type='l', col='blue',lty=2)\n\nlegend('bottomleft', legend=c(\"filtered\",\"smoothed\"),\n       col = c(\"red\", \"blue\"), lty=c(1, 1))\n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n\n\n## Second order polynomial: Filtering and smoothing example (Video)\n\nIn this video walk through the code provided in the section below the comment\n\n> **Similarly, for the second order polynomial and the co2 data:**\n\n## Using the dlm package in R (Video)\n\nThe `dlm` package in R is a powerful tool for working with dynamic linear models. The package provides a wide range of functions for filtering, smoothing, forecasting, and parameter estimation in DLMs. In this video, we walk through the code provided in the section below.\n\nSince the code below is too long, I will try and split it into smaller bits later. [TODO]{.cn}\n\n\n\n## R-code: Using the `dlm` package in R (Reading)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#################################################\n##### Univariate DLM: Known, constant variances\n#################################################\nset_up_dlm_matrices <- function(FF, GG, VV, WW){\n  return(list(FF=FF, GG=GG, VV=VV, WW=WW))\n}\n\nset_up_initial_states <- function(m0, C0){\n  return(list(m0=m0, C0=C0))\n}\n\n### forward update equations ###\nforward_filter <- function(data, matrices, initial_states){\n  ## retrieve dataset\n  yt <- data$yt\n  T <- length(yt)\n  \n  ## retrieve a set of quadruples \n  # FF, GG, VV, WW are scalar\n  FF <- matrices$FF  \n  GG <- matrices$GG\n  VV <- matrices$VV\n  WW <- matrices$WW\n  \n  ## retrieve initial states\n  m0 <- initial_states$m0\n  C0 <- initial_states$C0\n  \n  ## create placeholder for results\n  d <- dim(GG)[1]\n  at <- matrix(NA, nrow=T, ncol=d)\n  Rt <- array(NA, dim=c(d, d, T))\n  ft <- numeric(T)\n  Qt <- numeric(T)\n  mt <- matrix(NA, nrow=T, ncol=d)\n  Ct <- array(NA, dim=c(d, d, T))\n  et <- numeric(T)\n  \n  \n  for(i in 1:T){\n    # moments of priors at t\n    if(i == 1){\n      at[i, ] <- GG %*% t(m0)\n      Rt[, , i] <- GG %*% C0 %*% t(GG) + WW\n      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] <- GG %*% t(mt[i-1, , drop=FALSE])\n      Rt[, , i] <- GG %*% Ct[, , i-1] %*% t(GG) + WW\n      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    # moments of one-step forecast:\n    ft[i] <- t(FF) %*% (at[i, ]) \n    Qt[i] <- t(FF) %*% Rt[, , i] %*% FF + VV\n    \n    # moments of posterior at t:\n    At <- Rt[, , i] %*% FF / Qt[i]\n    et[i] <- yt[i] - ft[i]\n    mt[i, ] <- at[i, ] + t(At) * et[i]\n    Ct[, , i] <- Rt[, , i] - Qt[i] * At %*% t(At)\n    Ct[,,i] <- 0.5*Ct[,,i] + 0.5*t(Ct[,,i]) \n  }\n  cat(\"Forward filtering is completed!\") # indicator of completion\n  return(list(mt = mt, Ct = Ct, at = at, Rt = Rt, \n              ft = ft, Qt = Qt))\n}\n\nforecast_function <- function(posterior_states, k, matrices){\n  \n  ## retrieve matrices\n  FF <- matrices$FF\n  GG <- matrices$GG\n  WW <- matrices$WW\n  VV <- matrices$VV\n  mt <- posterior_states$mt\n  Ct <- posterior_states$Ct\n  \n  ## set up matrices\n  T <- dim(mt)[1] # time points\n  d <- dim(mt)[2] # dimension of state parameter vector\n  \n  ## placeholder for results\n  at <- matrix(NA, nrow = k, ncol = d)\n  Rt <- array(NA, dim=c(d, d, k))\n  ft <- numeric(k)\n  Qt <- numeric(k)\n  \n  \n  for(i in 1:k){\n    ## moments of state distribution\n    if(i == 1){\n      at[i, ] <- GG %*% t(mt[T, , drop=FALSE])\n      Rt[, , i] <- GG %*% Ct[, , T] %*% t(GG) + WW\n      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }else{\n      at[i, ] <- GG %*% t(at[i-1, , drop=FALSE])\n      Rt[, , i] <- GG %*% Rt[, , i-1] %*% t(GG) + WW\n      Rt[,,i] <- 0.5*Rt[,,i]+0.5*t(Rt[,,i]) \n    }\n    \n    ## moments of forecast distribution\n    ft[i] <- t(FF) %*% t(at[i, , drop=FALSE])\n    Qt[i] <- t(FF) %*% Rt[, , i] %*% FF + VV\n  }\n  cat(\"Forecasting is completed!\") # indicator of completion\n  return(list(at=at, Rt=Rt, ft=ft, Qt=Qt))\n}\n\n## obtain 95% credible interval\nget_credible_interval <- function(mu, sigma2, \n                          quantile = c(0.025, 0.975)){\n  z_quantile <- qnorm(quantile)\n  bound <- matrix(0, nrow=length(mu), ncol=2)\n  bound[, 1] <- mu + z_quantile[1]*sqrt(as.numeric(sigma2)) # lower bound\n  bound[, 2] <- mu + z_quantile[2]*sqrt(as.numeric(sigma2)) # upper bound\n  return(bound)\n}\n\n### smoothing equations ###\nbackward_smoothing <- function(data, matrices, \n                               posterior_states){\n  ## retrieve data \n  yt <- data$yt\n  T <- length(yt) \n  \n  ## retrieve matrices\n  FF <- matrices$FF\n  GG <- matrices$GG\n  \n  ## retrieve matrices\n  mt <- posterior_states$mt\n  Ct <- posterior_states$Ct\n  at <- posterior_states$at\n  Rt <- posterior_states$Rt\n  \n  ## create placeholder for posterior moments \n  mnt <- matrix(NA, nrow = dim(mt)[1], ncol = dim(mt)[2])\n  Cnt <- array(NA, dim = dim(Ct))\n  fnt <- numeric(T)\n  Qnt <- numeric(T)\n  for(i in T:1){\n    # moments for the distributions of the state vector given D_T\n    if(i == T){\n      mnt[i, ] <- mt[i, ]\n      Cnt[, , i] <- Ct[, , i]\n      Cnt[, , i] <- 0.5*Cnt[, , i] + 0.5*t(Cnt[, , i]) \n    }else{\n      inv_Rtp1<-solve(Rt[,,i+1])\n      Bt <- Ct[, , i] %*% t(GG) %*% inv_Rtp1\n      mnt[i, ] <- mt[i, ] + Bt %*% (mnt[i+1, ] - at[i+1, ])\n      Cnt[, , i] <- Ct[, , i] + Bt %*% (Cnt[, , i + 1] - Rt[, , i+1]) %*% t(Bt)\n      Cnt[,,i] <- 0.5*Cnt[,,i] + 0.5*t(Cnt[,,i]) \n    }\n    # moments for the smoothed distribution of the mean response of the series\n    fnt[i] <- t(FF) %*% t(mnt[i, , drop=FALSE])\n    Qnt[i] <- t(FF) %*% t(Cnt[, , i]) %*% FF\n  }\n  cat(\"Backward smoothing is completed!\")\n  return(list(mnt = mnt, Cnt = Cnt, fnt=fnt, Qnt=Qnt))\n}\n\n\n####################### Example: Lake Huron Data ######################\nplot(LakeHuron) # 98 observations total \n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nk=4\nT=length(LakeHuron)-k # We take the first \n                      # 94 observations only as our data\nts_data=LakeHuron[1:T]\nts_validation_data <- LakeHuron[(T+1):98]\n\ndata <- list(yt = ts_data)\n\n## set up dlm matrices\nGG <- as.matrix(1)\nFF <- as.matrix(1)\nVV <- as.matrix(1)\nWW <- as.matrix(1)\nm0 <- as.matrix(570)\nC0 <- as.matrix(1e4)\n\n## wrap up all matrices and initial values\nmatrices <- set_up_dlm_matrices(FF, GG, VV, WW)\ninitial_states <- set_up_initial_states(m0, C0)\n\n## filtering and smoothing \nresults_filtered <- forward_filter(data, matrices, \n                                   initial_states)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nForward filtering is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\nresults_smoothed <- backward_smoothing(data, matrices, \n                                       results_filtered)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBackward smoothing is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\n\n\npar(mfrow=c(2,1))\nplot(index, LakeHuron, main = \"Lake Huron Level \",type='l',\n     xlab=\"time\",ylab=\"feet\",lty=3,ylim=c(575,583))\npoints(index,LakeHuron,pch=20)\nlines(index_filt, results_filtered$mt, type='l', \n      col='red',lwd=2)\nlines(index_filt, results_smoothed$mnt, type='l', \n      col='blue',lwd=2)\n\n\n# Now let's look at the DLM package \nlibrary(dlm)\nmodel=dlmModPoly(order=1,dV=1,dW=1,m0=570,C0=1e4)\nresults_filtered_dlm=dlmFilter(LakeHuron[1:T],model)\nresults_smoothed_dlm=dlmSmooth(results_filtered_dlm)\n\nplot(index_filt, LakeHuron[1:T], ylab = \"level\", \n     main = \"Lake Huron Level\",\n     type='l', xlab=\"time\",lty=3,ylim=c(575,583))\npoints(index_filt,LakeHuron[1:T],pch=20)\nlines(index_filt,results_filtered_dlm$m[-1],col='red',lwd=2)\nlines(index_filt,results_smoothed_dlm$s[-1],col='blue',lwd=2)\n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Similarly, for the second order polynomial and the co2 data:\nT=length(co2)\ndata=list(yt = co2)\n\nFF <- (as.matrix(c(1,0)))\nGG <- matrix(c(1,1,0,1),ncol=2,byrow=T)\nVV <- as.matrix(200)\nWW <- 0.01*diag(2)\nm0 <- t(as.matrix(c(320,0)))\nC0 <- 10*diag(2)\n\n## wrap up all matrices and initial values\nmatrices <- set_up_dlm_matrices(FF,GG, VV, WW)\ninitial_states <- set_up_initial_states(m0, C0)\n\n## filtering and smoothing \nresults_filtered <- forward_filter(data, matrices, \n                                   initial_states)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nForward filtering is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\nresults_smoothed <- backward_smoothing(data, matrices, \n                                       results_filtered)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBackward smoothing is completed!\n```\n\n\n:::\n\n```{.r .cell-code}\n#### Now, using the DLM package: \nmodel=dlmModPoly(order=2,dV=200,dW=0.01*rep(1,2),\n                 m0=c(320,0),C0=10*diag(2))\n# filtering and smoothing \nresults_filtered_dlm=dlmFilter(data$yt,model)\nresults_smoothed_dlm=dlmSmooth(results_filtered_dlm)\n\npar(mfrow=c(2,1))\nplot(as.vector(time(co2)),co2,type='l',xlab=\"time\",\n     ylim=c(300,380))\nlines(as.vector(time(co2)),results_filtered$mt[,1],\n      col='red',lwd=2)\nlines(as.vector(time(co2)),results_smoothed$mnt[,1],\n      col='blue',lwd=2)\n\nplot(as.vector(time(co2)),co2,type='l',xlab=\"time\",\n     ylim=c(300,380))\nlines(as.vector(time(co2)),results_filtered_dlm$m[-1,1],\n      col='red',lwd=2)\nlines(as.vector(time(co2)),results_smoothed_dlm$s[-1,1],\n      col='blue',lwd=2)\n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n:::\n\n\n\n\n\n## Practice Graded Assignment: NDLM: sensitivity to the model parameters\n\nThis is a peer reviewed assignment. I may drop in the instructions but the solution will not be provided here due to the Coursera honor code.\n\nThis peer-reviewed activity is highly recommended. It does not figure into your grade for this course, but it does provide you with the opportunity to apply what you've learned in R and prepare you for your data analysis project in week 5. \n\nConsider the following R code: \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#######################\n##### DLM package #####\n#######################\n\nlibrary(dlm)\nk=4\nT=length(LakeHuron)-k # We take the first \n                      # 94 observations only as our data\nindex=seq(1875, 1972, length.out = length(LakeHuron))\nindex_filt=index[1:T]\n\nmodel=dlmModPoly(order=1,dV=1,dW=1,m0=570,C0=1e4)\nresults_filtered_dlm=dlmFilter(LakeHuron[1:T],model)\nresults_smoothed_dlm=dlmSmooth(results_filtered_dlm)\n\nplot(index_filt, LakeHuron[1:T], ylab = \"level\", \n     main = \"Lake Huron Level\",\n     type='l', xlab=\"time\",lty=3,ylim=c(575,583))\npoints(index_filt,LakeHuron[1:T],pch=20)\nlines(index_filt,results_filtered_dlm$m[-1],col='red',lwd=2)\nlines(index_filt,results_smoothed_dlm$s[-1],col='blue',lwd=2)\nlegend(1880,577, legend=c(\"filtered\", \"smoothed\"),\n       col=c(\"red\", \"blue\"), lty=1, cex=0.8)\n```\n\n::: {.cell-output-display}\n![](module3_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNote that you will need to install the `dlm` package in R if you don't have it  installed in order to run the code above. After installing the package and running the code above you will be asked to change some of the model specifications, upload some graphs and and answer some questions. In particular, you will be asked to: \n\n1. Modify the above code to change the variance of the prior distribution from $C_0=10^4$ to $C_0=10$ and plot and upload the traces of $E(\\theta_t \\mid \\matchcal D_T)$ (mean of the filtered distribution) and $E(\\theta_t \\mid \\matchcal D_T)$ for $T\\geq t$ and all $t=1:T$ (mean of the smoothed distribution). Are these new results different from the results with the model with $C_0=10^4$?\n\n2. Keep the variance of the prior distribution at $C_0=10^4$. Now change the evolution variance from \n$W=1$ to $W=0.01$ . Plot and upload the new means of the filtered and smoothed results. Are they different from the results when evolution variance is $W=1$ ?\n\n::: {.callout-info}\n\n\n### Grading Criteria\n\nPeer reviewers will be asked to check whether \n\n1. the plots are correct, especially the shape of red and blue lines. \n2. the responses provided to the questions are correct. \n\nTo receive full credit for this assignment you will have to grade the assignments of 2 students taking the course.\n\n:::\n\n# Quiz - NDLM, Part I: Review\n\nThis is omitted due to the Coursera honor code.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}