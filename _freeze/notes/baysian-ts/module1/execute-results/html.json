{
  "hash": "a4275551bab89a1f719af9c4b0bca9ec",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Time Series Analysis\"\nsubtitle: Course 4 of Bayesian Statistics Specialization\ndescription: \"The AR(1) process, Stationarity, ACF, PACF, Differencing, and Smoothing\"\ndate: 2024-10-23\ncategories: \n  - Coursera \n  - notes\n  - Bayesian Statistics\n  - Autoregressive Models\n  - Time Series\nkeywords: \n  - time series\n  - strong stationarity\n  - weak stationarity\n  - autocorrelation function\n  - ACF\n  - partial autocorrelation function\n  - PACF\n  - smoothing\n  - trend\n  - seasonality\n  - Durbin-Levinson recursion\n  - Yule-Walker equations\n  - differencing operator\n  - back shift operator\n  - moving average\n  - AR(1) process\n  - R code\nauthor: Oren Bochman\nimage: course-banner.png\nfig-caption: Notes about ... Bayesian Statistics\ntitle-block-banner: banner_deep.jpg\nbibliography: bibliography.bib\nformat: \n    html: \n        code-fold: true\n---\n\n\n\n\n\n# Week 1: Introduction to Time Series and the AR(1) process\n\n## Learning Objectives\n\n-   [x] List the goals of the course \n-   [x] identify the basics of the R environment.\n-   [x] Explain stationary time series processes\n-   [ ] Define auto-correlation function (ACF) and partial auto-correlation function (PACF) and use R to plot the sample ACF and sample PACF of a time series\n-   [x] Explain the concepts of differencing and smoothing via moving averages to remove/highlight trends and seasonal components in a time series\n-   [ ] Define the zero-mean autoregressive process of order one or AR(1) and use R to obtain samples from this type of process\n-   [ ] Perform maximum likelihood estimation for the full and conditional likelihood in an AR(1)\n-   [ ] Perform Bayesian inference for the AR(1) under the conditional likelihood and the reference prior\n\n## Introduction\n\n### Welcome to Bayesian Statistics: Time Series\n\n-   [x] Obligatory introduction to the course and the instructors.\n- [Raquel Prado]() is a professor of statistics in the [Baskin School of Engineering](https://engineering.ucsc.edu/) at the University of California, Santa Cruz.  She was the reciepient 2022 Zellner Medal, see @BibEntry2024Sep\n\n### Introduction to R\n\n- [x]  [Introduction to R](https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf)\n\n### List of References\n\n1.  [@prado2023time]\n2.  [@west2013bayesian]\n3.  [@theodoridis2015ML]\n4.  [@von2002statistical]\n\n-   The first two are the course textbooks.\n\n-   The third is a classic text on machine learning which covers Durban-Levinson recursion and the Yule-Walker equations mentioned in the course.\n\n-   The fourth is a classic text on statistical learning theory which covers the basics of time series analysis.\n\n## Stationarity the ACF and the PACF\n\n### Stationarity (video)\n\nStationarity is a key concept in time series analysis. A time series is said to be stationary if its statistical properties such as mean, variance, and auto-correlation do not change over time.\n\n::: {#tip-notation .callout-tip}\n\n#### Notation\n\n-   $\\{y_t\\}$ - the time series process, where each $y_t$ is a univariate random variable.\n-   $y_{1:T}$ or $y_1, y_2, \\ldots, y_T$ - the observed data.\n:::\n\nStrong Stationarity\n\n:   [Strong Stationarity]{.column-margin}\n\n:   given $\\{y_t\\}$ for any $n>0$ and any $h>0$ and any subsequence the distribution of $y_t, y_{t+1}, \\ldots, y_{t+n}$ is the same as the distribution of $y_{t+h}, y_{t+h+1}, \\ldots, y_{t+h+n}$.\n\nsince it is difficult to verify strong stationarity in practice, we often work with weak stationarity.\n\nWeak Stationarity AKA Second-order Stationarity\n\n:   [Weak Stationarity]{.column-margin}\n\n:   the mean, variance, and auto-covariance are constant over time.\n\n-   strong stationarity implies weak stationarity, but\n-   the converse is not true.\n-   for a Gaussian process, our typical use case, they are equivalent!\n\nLet $y_t$ be a time series. We say that $y_t$ is stationary if the following conditions hold:\n\n### The auto-correlation function ACF (video)\n\n[auto-correlation AFC]{.column-margin}\n\n### The partial auto-correlation function PACF (Reading)\n\nLet ${y_t}$ be a zero-mean stationary process.\n\nLet\n\n$$\n\\hat{y}_t^{h-1} = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\ldots + \\beta_{h-1} y_{t-(h-1)}\n$$\n\nbe the best linear predictor of $y_t$ based on the previous $h − 1$ values $\\{y_{t−1}, \\ldots , y_{t−h+1}\\}$. The best linear predictor of $y_t$ based on the previous $h − 1$ values of the process is the linear predictor that minimizes\n\n$$\nE[(y_t − \\hat{y}_y^{h-1})^2]\n$$\n\nThe partial autocorrelation of this process at lag h, denoted by $\\phi(h, h)$ is defined as: [partial auto-correlation PAFC]{.column-margin}\n\n$$\n\\phi(h, h) = Corr(y_{t+h} − \\hat{y}_{t+h}^{h-1}, y_t − \\hat{y}_t^{h-1})\n$$\n\nfor $h ≥ 2$ and $\\phi(1, 1) = Corr(y_{t+1}, y_{t}) = \\rho(1)$.\n\nThe partial autocorrelation function can also be computed via the Durbin-Levinson recursion for stationary processes as $\\phi(0, 0) = 0$,\n\n$$\n\\phi(n, n) = \\frac{\\rho(n) − \\sum_{h=1}^{n-1} \\phi(n − 1, h)\\rho(n − h)}{1- \\sum_{h=1}^{n-1}\\phi(n − 1, h)\\rho(h)}\n$$\n\nfor $n ≥ 1$, and\n\n$$\n\\phi(n, h) = \\phi(n − 1, h) − \\phi(n, n)\\phi(n − 1, n − h),\n$$\n\nfor $n ≥ 2$, and $h = 1, \\ldots , (n − 1)$.\n\nNote that the sample PACF can be obtained by substituting the sample autocorrelations and the sample auto-covariances in the Durbin-Levinson recursion.\n\n### Durbin-Levinson recursion (Off-Course Reading)\n\nLike me, you might be curious about the Durbin-Levinson recursion mentioned above. This is not covered in the course, and turned out to be an enigma wrapped in a mystery.\n\nI present my finding in the note below - much of it is due to [@enwiki-LevinsonRecursion] and [@enwiki-YuleWalkerEquations]\n\nIn [@yule1927periodicities] and [@walker1931periodicity], Yule and Walker proposed a method for estimating the parameters of an autoregressive model. The method is based on the Yule-Walker equations which are a set of linear equations that can be used to estimate the parameters of an autoregressive model.\n\nDue to the autoregressive nature of the model, the equations are take a special form called a Toeplitz matrix. However at the time they probably had to use the numerically unstable Gauss-Jordan elimination to solve these equations which is $O(n^3)$ in time complexity.\n\nA decade or two later in [@Levinson1947Wiener] and [@Durbin1960Fitting] the authors came up for with a weakly stable yet more efficient algorithm for solving these autocorrelated system of equations which requires only $O(n^2)$ in time complexity. Later their work was further refined in [@Trench1964ToeplitzMI] and [@Zohar1969ToeplitzMI] to just $3\\times n^2$ multiplication. A cursory search reveals that Toeplitz matrix inversion is still an area of active research with papers covering parallel algorithms and stability studies. Not surprising as man of the more interesting deep learning models, including LLMs are autoregressive.\n\nSo the [Durbin-Levinson recursion](https://en.wikipedia.org/wiki/Levinson_recursion) is just an elegant bit of linear algebra for solving the [Yule-Walker equations](https://w.wiki/9gVB#Estimation_of_AR_parameters) more efficiently.\n\nHere is what I dug up:\n\n### Durbin-Levinson and the Yule-Walker equations (Off-Course Reading)\n\nThe Durbin-Levinson recursion is a method in linear algebra for computing the solution to an equation involving a *Toeplitz matrix* AKA a *diagonal-constant matrix* where descending diagonals are constant. The recursion runs in $O(n^2)$ time rather then $O(n^3)$ time required by Gauss-Jordan elimination.\n\nThe recursion can be used to compute the coefficients of the autoregressive model of a stationary time series. It is based on the [Yule-Walker equations](https://w.wiki/9gVB#Estimation_of_AR_parameters) and is used to compute the PACF of a time series.\n\nThe Yule-Walker equations can be stated as follows for an AR(p) process:\n\n$$\n\\gamma_m = \\sum_{k=1}^p \\phi_k \\gamma_{m-k} + \\sigma_\\epsilon^2\\delta_{m,0} \\qquad \\text{(Yule-Walker equations)}\n$$ {#eq-yule-walker}\n\nwhere:\n\n-   $\\gamma_m$ is the autocovariance function of the time series,\n-   $\\phi_k$ are the AR coefficients,\n-   $\\sigma_\\epsilon^2$ is the variance of the white noise process, and\n-   $\\delta_{m,0}$ is the Kronecker delta function.\n\nwhen $m=0$ the equation simplifies to:\n\n$$\n\\gamma_0 = \\sum_{k=1}^p \\phi_k \\gamma_{-k} + \\sigma_\\epsilon^2 \\qquad \\text{(Yule-Walker equations for m=0)}\n$$ {#eq-yule-walker-m-0}\n\nfor $m > 0$ the equation simplifies to:\n\n$$ \\begin{bmatrix}\n    \\gamma_1 \\newline\n    \\gamma_2 \\newline\n    \\gamma_3 \\newline\n    \\vdots \\newline\n    \\gamma_p \\newline\n \\end{bmatrix} =  \\begin{bmatrix}\n    \\gamma_0     & \\gamma_{-1}  & \\gamma_{-2}  & \\cdots \\newline\n    \\gamma_1     & \\gamma_0     & \\gamma_{-1}  & \\cdots \\newline\n    \\gamma_2     & \\gamma_1     & \\gamma_0     & \\cdots \\newline\n    \\vdots       & \\vdots       & \\vdots       & \\ddots \\newline\n    \\gamma_{p-1} & \\gamma_{p-2} & \\gamma_{p-3} & \\cdots \\newline\n \\end{bmatrix}  \\begin{bmatrix}\n    \\phi_{1} \\newline\n    \\phi_{2} \\newline\n    \\phi_{3} \\newline\n    \\vdots \\newline\n    \\phi_{p} \\newline\n \\end{bmatrix}\n$$\n\nand since this matrix is Toeplitz, we can use Durbin-Levinson recursion to efficiently solve the system for $\\phi_k \\forall k$.\n\nOnce $\\{\\phi_m ; m=1,2, \\dots ,p \\}$ are known, we can consider m=0 and solved for $\\sigma_\\epsilon^2$ by substituting the $\\phi_k$ into @eq-yule-walker-m-0 Yule-Walker equations.\n\nOf course the Durbin-Levinson recursion is not the last word on solving this system of equations. There are today numerous improvements which are both faster and more numerically stable.\n\n### Differencing and smoothing (Reading)\n\nMany time series models are built under the assumption of stationarity. However, time series data often present non-stationary features such as trends or seasonality. Practitioners may consider techniques for detrending, deseasonalizing and smoothing that can be applied to the\nobserved data to obtain a new time series that is consistent with the stationarity assumption.\n\nWe briefly discuss two methods that are commonly used in practice for detrending and smoothing.\n\n#### Differencing\n\nThe first method is differencing, which is generally used to remove trends in time series data. The first difference of a time series is defined in terms of the so called difference operator denoted as $D$, that produces the transformation [differencing operator]{.column-margin}\n\n$$\nDy_t = y_t - y_{t-1}.\n$$\n\nHigher order differences are obtained by successively applying the operator $D$. For example, \n\n$$\nD^2y_t = D(Dy_t) = D(y_t - y_{t-1}) = y_t - 2y_{t-1} + y_{t-2}.\n$$\n\nDifferencing can also be written in terms of the so called backshift operator $B$, with [backshift operator]{.column-margin} \n\n$$\nBy_t = y_{t-1},\n$$\n\nso that \n$$\nDy_t = (1 - B)y_t\n$$ \n\nand \n\n$$\nD^dy_t = (1 - B)d y_t.\n$$\n\n#### Smoothing\n\nThe second method we discuss is [moving averages, which is commonly used to \"smooth\" a time series by removing certain features]{.mark} (e.g., seasonality) [to highlight other features]{.mark} (e.g., trends). [A moving average is a weighted average of the time series around a particular time]{.mark} $t$. In general, if we have data $y1:T$ , we could obtain a new time series such that [moving average]{.column-margin}\n\n$$\nz_t = \\sum_{j=-q}^{p} w_j y_{t+j},\n$$\n\nfor $t = (q + 1) : (T − p)$, with weights $w_j ≥ 0$ and $\\sum^p_{j=−q} w_j = 1$\n\nWe will frequently work with *moving averages* for which \n$$\np = q \\qquad \\text{(centered)}\n$$\n\nand\n\n$$\nw_j = w_{−j} \\forall j  \\text{(symmetric)}\n$$\n\nAssume we have periodic data with period $d$. Then, symmetric and centered moving averages can be used to remove such periodicity as follows:\n\n-   If $d = 2q$ : \n\n$$\nz_t = \\frac{1}{2} (y_{t−q} + y_{t−q+1} + \\ldots + y_{t+q−1} + y_{t+q}) \n$$\n\n-   if $d = 2q + 1$ :\n\n$$\nz_t = \\frac{1}{d} (y_{t−q} + y_{t−q+1} + \\ldots + y_{t+q−1} + y_{t+q})\n$$\n\nExample: To remove seasonality in monthly data (i.e., seasonality with a period of d = 12 months), one can use a moving average with $p = q = 6$, $a_6 = a_{−6} = 1/24$, and $a_j = a_{−j} = 1/12$ for $j = 0, \\ldots , 5$ , resulting in:\n\n$$\nz_t = \\frac{1}{12} (y_{t−6} + y_{t−5} + \\ldots + y_{t+5} + y_{t+6})\n$$\n\n### ACF PACF Differencing and Smoothing Examples (Video)\n\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n\n\n### ACF PACF Differencing and Smoothing Examples (Video)\n\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n\n\n### R code for Differencing and filtering via moving averages (reading)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the CO2 dataset in R\ndata(co2) \n\n# Take first differences to remove the trend \nco2_1stdiff=diff(co2,differences=1)\n\n# Filter via moving averages to remove the seasonality \nco2_ma=filter(co2,filter=c(1/24,rep(1/12,11),1/24),sides=2)\n\npar(mfrow=c(3,1), cex.lab=1.2,cex.main=1.2)\nplot(co2) # plot the original data \nplot(co2_1stdiff) # plot the first differences (removes trend, highlights seasonality)\nplot(co2_ma) # plot the filtered series via moving averages (removes the seasonality, highlights the trend)\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/moving averages and differencing-1.png){width=672}\n:::\n:::\n\n\n\n\n### R Code: Simulate data from a white noise process (reading)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#\n# Simulate data with no temporal structure (white noise)\n#\nset.seed(2021)\nT=200\nt =1:T\ny_white_noise=rnorm(T, mean=0, sd=1)\n#\n# Define a time series object in R: \n# Assume the data correspond to annual observations starting in January 1960 \n#\nyt=ts(y_white_noise, start=c(1960), frequency=1)\n#\n# plot the simulated time series, their sample ACF and their sample PACF\n#\npar(mfrow = c(1, 3), cex.lab = 1.3, cex.main = 1.3)\nyt=ts(y_white_noise, start=c(1960), frequency=1)\nplot(yt, type = 'l', col='red', xlab = 'time (t)', ylab = \"Y(t)\")\nacf(yt, lag.max = 20, xlab = \"lag\",\n    ylab = \"Sample ACF\",ylim=c(-1,1),main=\"\")\npacf(yt, lag.max = 20,xlab = \"lag\",\n     ylab = \"Sample PACF\",ylim=c(-1,1),main=\"\")\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/white noise simulation-1.png){width=672}\n:::\n:::\n\n\n\n\n### Quiz 1: Stationarity, ACF, PACF, Differencing, and Smoothing\n\nomitted per coursera requirements\n\n## The AR(1) process: Definition and properties\n\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n\n\n### The AR(1) process (video)\n\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n\n\n### The PACF of the AR(1) process (reading)\n\nIt is possible to show that the PACF of an autoregressive process of order one is zero after the first lag. We can use the Durbin-Levinson recursion to show this.\n\nFor lag $n = 0$ we have $\\phi(0, 0) = 0$\n\nFor lag $n = 1$ we have: \n\n$$\n\\phi(1, 1) =  \\rho(1) = \\phi\n$$\n\nFor lag $n = 2$ we compute $\\phi(2, 2)$ as:\n\n$$\n\\phi(2, 2) = \\frac{(\\rho(2) − \\phi(1, 1)\\rho(1))}{ (1 − \\phi(1, 1)\\rho(1))} = \\frac{\\phi^2-\\phi^2}{1- \\phi^2}=0\n$$\n\nand we also obtain\n\n\n$$\n\\phi(2, 1) = \\phi(1, 1) − \\phi(2, 2)\\phi(1, 1) = \\phi.\n$$\n\nFor lag $n = 3$ we compute $\\phi(3, 3)$ as\n\n$$\n\\begin{align*}\n\\phi(3, 3) &= \\frac{(\\rho(3) − \\sum_{h=1}^2 \\phi(2, h)\\rho(3 − h))}{1 − \\sum_{h=1}^2 \\phi(2, h)\\rho(h)} \\newline\n&= \\frac{\\phi^3 - \\phi(2,1) \\rho(2) - \\phi(2,2) \\rho(1)}{1 - \\phi(2,1)\\rho(1) - \\phi(2,2)\\rho(2)} \\newline\n&= \\frac{\\phi^3 - \\phi^3 - 0}{1 - \\phi^2 } \\newline\n&= 0\n\\end{align*}\n$$\n\nand we also obtain\n\n$$\n\\phi(3, 1) = \\phi(2, 1) − \\phi(3, 3)\\phi(2, 2) = \\phi\n$$\n\n$$\n\\phi(3, 2) = \\phi(2, 2) − \\phi(3, 3)\\phi(2, 1) = 0\n$$\n\nWe can prove by induction that in the case of an AR(1), for any lag $n$,\n\n$\\phi(n, h) = 0, \\phi(n, 1) = \\phi$ and $\\phi(n, h) = 0$ for $h ≥ 2$ and $n ≥ 2$.\n\nThen, the PACF of an AR(1) is zero for any lag above 1 and the PACF coefficient at lag 1 is equal to the AR coefficient $\\phi$\n\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n\n\n### Sample data from AR(1) processes (Reading)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample data from 2 ar(1) processes and plot their ACF and PACF functions\n#\nset.seed(2021)\nT=500 # number of time points\n#\n# sample data from an ar(1) with ar coefficient phi = 0.9 and variance 1\n#\nv=1.0 # innovation variance\nsd=sqrt(v) #innovation stantard deviation\nphi1=0.9 # ar coefficient\nyt1=arima.sim(n = T, model = list(ar = phi1), sd = sd)\n#\n# sample data from an ar(1) with ar coefficient phi = -0.9 and variance 1\n#\nphi2=-0.9 # ar coefficient\nyt2=arima.sim(n = T, model = list(ar = phi2), sd = sd)\n\npar(mfrow = c(2, 1), cex.lab = 1.3)\nplot(yt1,main=expression(phi==0.9))\nplot(yt2,main=expression(phi==-0.9))\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/ar(1) sampling-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(3, 2), cex.lab = 1.3)\nlag.max=50 # max lag\n#\n## plot true ACFs for both processes\n#\ncov_0=sd^2/(1-phi1^2) # compute auto-covariance at h=0\ncov_h=phi1^(0:lag.max)*cov_0 # compute auto-covariance at h\nplot(0:lag.max, cov_h/cov_0, pch = 1, type = 'h', col = 'red',\n     ylab = \"true ACF\", xlab = \"Lag\",ylim=c(-1,1), main=expression(phi==0.9))\n\ncov_0=sd^2/(1-phi2^2) # compute auto-covariance at h=0\ncov_h=phi2^(0:lag.max)*cov_0 # compute auto-covariance at h\n# Plot autocorrelation function (ACF)\nplot(0:lag.max, cov_h/cov_0, pch = 1, type = 'h', col = 'red',\n     ylab = \"true ACF\", xlab = \"Lag\",ylim=c(-1,1),main=expression(phi==-0.9))\n\n## plot sample ACFs for both processes\n#\nacf(yt1, lag.max = lag.max, type = \"correlation\", ylab = \"sample ACF\",\n    lty = 1, ylim = c(-1, 1), main = \" \")\nacf(yt2, lag.max = lag.max, type = \"correlation\", ylab = \"sample ACF\",\n    lty = 1, ylim = c(-1, 1), main = \" \")\n## plot sample PACFs for both processes\n#\npacf(yt1, lag.ma = lag.max, ylab = \"sample PACF\", ylim=c(-1,1),main=\"\")\npacf(yt2, lag.ma = lag.max, ylab = \"sample PACF\", ylim=c(-1,1),main=\"\")\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/ar(1) sampling-2.png){width=672}\n:::\n:::\n\n\n\n\n### Quiz 2: The AR(1) definition and properties\n\nOmitted per Coursera requirements\n\n## The AR(1) process:Maximum likelihood estimation and Bayesian inference\n\n\n\n\n{{< lipsum 2 >}}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}