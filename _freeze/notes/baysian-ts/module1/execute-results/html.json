{
  "hash": "e5d12e94fe503188819a7d669e10491d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introductions to Time Series analysis & the AR(1) process\"\nsubtitle: Time Series Analysis\ndescription: \"In the first week of the fourth course of  Coursera's 'Bayesian  Statistics Specialization' we will define the AR(1) process, Stationarity, ACF, PACF, differencing, smoothing\"\ndate: 2024-10-23\ncategories: \n  - coursera \n  - notes\n  - bayesian statistics\n  - autoregressive models\n  - time series\nkeywords: \n  - time series\n  - stationarity\n  - strong stationarity\n  - weak stationarity\n  - autocorrelation function (ACF)\n  - partial autocorrelation function (PACF)\n  - smoothing\n  - trend\n  - seasonality\n  - Durbin-Levinson recursion\n  - Yule-Walker equations\n  - differencing operator\n  - back shift operator\n  - moving average\n  - AR(1) process\n  - R code\nauthor: Oren Bochman\nimage: course-banner.png\nfig-caption: Notes about ... Bayesian Statistics\ntitle-block-banner: banner_deep.jpg\nbibliography: bibliography.bib\nformat: \n    html: \n        code-fold: true\n        css: styles.css\n\n---\n\n\n\n\n\n# Week 1: Introduction to Time Series and the AR(1) process\n\n\n\n\n## Learning Objectives\n\n-   [x] List the goals of the course\n-   [x] identify the basics of the R environment.\n-   [x] Explain stationary time series processes\n-   [ ] Define auto-correlation function (ACF) and partial auto-correlation function (PACF) and use R to plot the sample ACF and sample PACF of a time series\n-   [x] Explain the concepts of differencing and smoothing via moving averages to remove/highlight trends and seasonal components in a time series\n-   [ ] Define the zero-mean autoregressive process of order one or AR(1) and use R to obtain samples from this type of process\n-   [ ] Perform maximum likelihood estimation for the full and conditional likelihood in an AR(1)\n-   [ ] Perform Bayesian inference for the AR(1) under the conditional likelihood and the reference prior\n\n## Introduction\n\n### Welcome to Bayesian Statistics: Time Series\n\n-   [x] Obligatory introduction to the course and the instructors.\n-   [Raquel Prado]() is a professor of statistics in the [Baskin School of Engineering](https://engineering.ucsc.edu/) at the University of California, Santa Cruz. She was the recipient 2022 [Zellner Medal](https://bayesian.org/project/zellner-medal/), see @BibEntry2024Sep.\n\n### Introduction to R\n\n-   [x] [Introduction to R](https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf)\n\n\n\n## Stationarity the ACF and the PACF\n\nBefore diving into the material here is a brief overview of the notations for timer series.\n\n::: {#tip-notation .callout-tip}\n#### Notation\n\n-   $\\{y_t\\}$ - the time series process, where each $y_t$ is a univariate random variable and t are the time points that are equally spaced. \n-   $y_{1:T}$ or $y_1, y_2, \\ldots, y_T$ - the observed data.\n\nalso see [@prado2023time pp. 2-3]\n:::\n\n\n### Stationarity (video)\n\n![~~slide 1~~](m1_0001.png){.column-margin width=\"250px\"}\n\nStationarity see [@prado2023time §1.2] is a fundamental concept in time series analysis. \n\n::: callout-important\n\n## TL;DR\n\nStationarity\n:   [Stationarity]{.column-margin}\n\n: A time series is said to be stationary if its statistical properties such as mean, variance, and auto-correlation do not change over time.\n\n- We make this definition more formal in the definitions of strong and weak stationarity below.\n:::\n\n\nStationarity is a key concept in time series analysis. A time series is said to be stationary if its statistical properties such as mean, variance, and auto-correlation do not change over time.\n\nLet $y_t$ be a time series. We say that $y_t$ is stationary if the following conditions hold:\n\nStrong Stationarity\n\n:   [Strong Stationarity]{.column-margin}\n\n:   Let $\\{y_t\\} \\forall n>0$ be a time series and $h > 0$ be a lag. If for any subsequence the distribution of $y_t, y_{t+1}, \\ldots, y_{t+n}$ is the same as the distribution of $y_{t+h}, y_{t+h+1}, \\ldots, y_{t+h+n}$ we call the series strongly stationary.\n\nSince it is difficult to verify strong stationarity in practice, we will often use the following weaker notion of stationarity.\n\nWeak Stationarity AKA Second-order Stationarity\n\n:   [Weak Stationarity]{.column-margin}\n\n:   the mean, variance, and auto-covariance are constant over time.\n\n-  $E(y_t) = \\mu \\quad \\forall t$\n-  $Var(y_t) = \\nu =\\sigma^2 \\quad \\forall t$\n-  $Cov(y_t , y_s ) = γ(t − s)$\n\n\n-   Strong stationarity $\\implies$ Weak stationarity, but\n-   The converse is not true.\n-   In this course when we deal with a Gaussian process, our typical use case, they are equivalent!\n\n::: callout-caution\n\n## Check your understanding\n\nQ. Can you explain with an example when a time series is weakly stationary but not strongly stationarity.\n\n:::\n\n\n### The auto-correlation function ACF (video)\n\n\n![slide 1](m1_0011.png){.column-margin width=\"250px\"}\n\nThe autocorrelation is simply how correlated a time series is with itself at different lags. Correlation in general is defined in terms of covariance of two variables. The covariance is a measure of the joint variability of two random variables. \n\n\n::: callout-important\n\nRecall that the covariance between two random variables $y_t$ and $y_s$ is defined as:\n\n$$\n\\begin{aligned}\nCov(y_t, y_s) &= \\mathbb{E}[(y_t-\\mathbb{E}[y_t])(y_s-\\mathbb{E}[y_s])] \\\\\n              &= \\mathbb{E}[(y_t-\\mu_t)(y_s-\\mu_s)] \\\\\n              &= E[y_t y_s] - \\mu_t \\times \\mu_s\n\\end{aligned} \\qquad\n$$ {#eq-covariance}\n\nWe will often use the notation $\\gamma(h)$ to denote the auto covariance at lig $h$ i.e. between $y_t$ and $y_{t+h}$\n\n$$\n\\gamma(h) = Cov(y_t, y_s) \\qquad\n$$ {#eq-autocovariance}\n\n:::\n\nwhere $\\mu_t = \\mathbb{E}(y_t)$ and $\\mu_s = \\mathbb{E}(y_s)$ are the means of $y_t$ and $y_s$ respectively.\n\nIf the time series is stationary, then the covariance only depends on the lag $h = |t-s|$ and we can write the covariance as $\\gamma(h)$.\n\n\nLet $\\{y_t\\}$ be a time series. Recall that the covariance between two random variables $y_t$ and $y_s$ is defined as:\n\n$$\n\\gamma(t,s)=Cov(y_t, y_s) = \\mathbb{E}[(y_t-\\mu_t)(y_s-\\mu_s)] \\qquad\n$$ {#eq-covariance}\n\nwhere $\\mu_t = \\mathbb{E}(y_t)$ and $\\mu_s = \\mathbb{E}(y_s)$ are the means of $y_t$ and $y_s$ respectively.\n\n$$\n\\mu_t = \\mathbb{E}(y_t) \\qquad \\mu_s = \\mathbb{E}(y_s)\n$$\n\nStationarity $\\implies \\mathbb{E}[y_t] = \\mu \\quad \\forall t \\qquad \\gamma(t,s)=\\gamma(|t-s|)$\n\nIf $h>0 \\qquad \\gamma(h)=Cov(y_t,y_{t-h})$\n\n::: callout-important\n\n###  Autocorrelation Function (AFC)\n[auto-correlation AFC]{.column-margin}\n\n\n$$\n\\rho(t,s) = \\frac{\\gamma(t,s)}{\\sqrt{\\gamma(t,t)\\gamma(s,s)}}\n$$\n\n:::\n\n$$\n\\text{Stationarity} \\implies \\rho(h)=\\frac{\\gamma(h)}{\\gamma(o)} \\qquad \\gamma(0)=Var(y_t)\n$$\n\n![slide 2](m1_0012.png){.column-margin alt=\"$$$$\" width=\"250px\"}\n\n$$\ny_{1:T}\n$$\n\n::: callout-important\n\n### The  sample AFC\n\n$$\n\\hat\\gamma(h)= \\frac{1}{T} \\sum_{t=1}^{T-h}(y_{t+h}-\\bar y )(y_t-\\hat y)\n$$\n\n:::\n\n$$\n\\bar y = \\frac{1}{T} \\sum_{t=1}^{T}y_t\n$$\n\n$$\n\\hat \\rho = \\frac{\\hat\\gamma(h)}{\\hat\\gamma(o)}\n$$\n\n### The partial auto-correlation function PACF (Reading)\n\nLet ${y_t}$ be a zero-mean stationary process.\n\nLet\n\n$$\n\\hat{y}_t^{h-1} = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\ldots + \\beta_{h-1} y_{t-(h-1)}\n$$\n\nbe the best linear predictor of $y_t$ based on the previous $h − 1$ values $\\{y_{t−1}, \\ldots , y_{t−h+1}\\}$. The best linear predictor of $y_t$ based on the previous $h − 1$ values of the process is the linear predictor that minimizes\n\n$$\nE[(y_t − \\hat{y}_y^{h-1})^2]\n$$\n\nThe partial autocorrelation of this process at lag h, denoted by $\\phi(h, h)$ is defined as: [partial auto-correlation PAFC]{.column-margin}\n\n$$\n\\phi(h, h) = Corr(y_{t+h} − \\hat{y}_{t+h}^{h-1}, y_t − \\hat{y}_t^{h-1})\n$$\n\nfor $h \\ge 2$ and $\\phi(1, 1) = Corr(y_{t+1}, y_{t}) = \\rho(1)$.\n\nThe partial autocorrelation function can also be computed via the Durbin-Levinson recursion for stationary processes as $\\phi(0, 0) = 0$,\n\n$$\n\\phi(n, n) = \\frac{\\rho(n) − \\sum_{h=1}^{n-1} \\phi(n − 1, h)\\rho(n − h)}{1- \\sum_{h=1}^{n-1}\\phi(n − 1, h)\\rho(h)}\n$$\n\nfor $n \\ge 1$, and\n\n$$\n\\phi(n, h) = \\phi(n − 1, h) − \\phi(n, n)\\phi(n − 1, n − h),\n$$\n\nfor $n \\ge 2$, and $h = 1, \\ldots , (n − 1)$.\n\nNote that the sample PACF can be obtained by substituting the sample autocorrelations and the sample auto-covariances in the Durbin-Levinson recursion.\n\n### Durbin-Levinson recursion (Off-Course Reading)\n\nLike me, you might be curious about the Durbin-Levinson recursion mentioned above. This is not covered in the course, and turned out to be an enigma wrapped in a mystery.\n\nI present my finding in the note below - much of it is due to [@enwiki-LevinsonRecursion] and [@enwiki-YuleWalkerEquations]\n\nIn [@yule1927periodicities] and [@walker1931periodicity], Yule and Walker proposed a method for estimating the parameters of an autoregressive model. The method is based on the Yule-Walker equations which are a set of linear equations that can be used to estimate the parameters of an autoregressive model.\n\nDue to the autoregressive nature of the model, the equations are take a special form called a Toeplitz matrix. However at the time they probably had to use the numerically unstable Gauss-Jordan elimination to solve these equations which is $O(n^3)$ in time complexity.\n\nA decade or two later in [@Levinson1947Wiener] and [@Durbin1960Fitting] the authors came up for with a weakly stable yet more efficient algorithm for solving these autocorrelated system of equations which requires only $O(n^2)$ in time complexity. Later their work was further refined in [@Trench1964ToeplitzMI] and [@Zohar1969ToeplitzMI] to just $3\\times n^2$ multiplication. A cursory search reveals that Toeplitz matrix inversion is still an area of active research with papers covering parallel algorithms and stability studies. Not surprising as man of the more interesting deep learning models, including LLMs are autoregressive.\n\nSo the [Durbin-Levinson recursion](https://en.wikipedia.org/wiki/Levinson_recursion) is just an elegant bit of linear algebra for solving the [Yule-Walker equations](https://w.wiki/9gVB#Estimation_of_AR_parameters) more efficiently.\n\nHere is what I dug up:\n\n### Durbin-Levinson and the Yule-Walker equations (Off-Course Reading)\n\nThe Durbin-Levinson recursion is a method in linear algebra for computing the solution to an equation involving a *Toeplitz matrix* AKA a *diagonal-constant matrix* where descending diagonals are constant. The recursion runs in $O(n^2)$ time rather then $O(n^3)$ time required by Gauss-Jordan elimination.\n\nThe recursion can be used to compute the coefficients of the autoregressive model of a stationary time series. It is based on the [Yule-Walker equations](https://w.wiki/9gVB#Estimation_of_AR_parameters) and is used to compute the PACF of a time series.\n\nThe Yule-Walker equations can be stated as follows for an AR(p) process:\n\n$$\n\\gamma_m = \\sum_{k=1}^p \\phi_k \\gamma_{m-k} + \\sigma_\\epsilon^2\\delta_{m,0} \\qquad \\text{(Yule-Walker equations)}\n$$ {#eq-yule-walker}\n\nwhere:\n\n-   $\\gamma_m$ is the autocovariance function of the time series,\n-   $\\phi_k$ are the AR coefficients,\n-   $\\sigma_\\epsilon^2$ is the variance of the white noise process, and\n-   $\\delta_{m,0}$ is the Kronecker delta function.\n\nwhen $m=0$ the equation simplifies to:\n\n$$\n\\gamma_0 = \\sum_{k=1}^p \\phi_k \\gamma_{-k} + \\sigma_\\epsilon^2 \\qquad \\text{(Yule-Walker equations for m=0)}\n$$ {#eq-yule-walker-m-0}\n\nfor $m > 0$ the equation simplifies to:\n\n$$ \\begin{bmatrix}\n    \\gamma_1 \\newline\n    \\gamma_2 \\newline\n    \\gamma_3 \\newline\n    \\vdots \\newline\n    \\gamma_p \\newline\n \\end{bmatrix} =  \\begin{bmatrix}\n    \\gamma_0     & \\gamma_{-1}  & \\gamma_{-2}  & \\cdots \\newline\n    \\gamma_1     & \\gamma_0     & \\gamma_{-1}  & \\cdots \\newline\n    \\gamma_2     & \\gamma_1     & \\gamma_0     & \\cdots \\newline\n    \\vdots       & \\vdots       & \\vdots       & \\ddots \\newline\n    \\gamma_{p-1} & \\gamma_{p-2} & \\gamma_{p-3} & \\cdots \\newline\n \\end{bmatrix}  \\begin{bmatrix}\n    \\phi_{1} \\newline\n    \\phi_{2} \\newline\n    \\phi_{3} \\newline\n    \\vdots \\newline\n    \\phi_{p} \\newline\n \\end{bmatrix}\n$$\n\nand since this matrix is Toeplitz, we can use Durbin-Levinson recursion to efficiently solve the system for $\\phi_k \\forall k$.\n\nOnce $\\{\\phi_m ; m=1,2, \\dots ,p \\}$ are known, we can consider m=0 and solved for $\\sigma_\\epsilon^2$ by substituting the $\\phi_k$ into @eq-yule-walker-m-0 Yule-Walker equations.\n\nOf course the Durbin-Levinson recursion is not the last word on solving this system of equations. There are today numerous improvements which are both faster and more numerically stable.\n\n### Differencing and smoothing (Reading)\n\nDifferencing and smoothing are covered in the [@prado2023time §1.4] and are techniques used to remove trends and seasonality in time series data.\n\nMany time series models are built under the assumption of stationarity. However, time series data often present non-stationary features such as trends or seasonality. Practitioners may consider techniques for detrending, deseasonalizing and smoothing that can be applied to the observed data to obtain a new time series that is consistent with the stationarity assumption.\n\nWe briefly discuss two methods that are commonly used in practice for detrending and smoothing.\n\n#### Differencing\n\nThe first method is differencing, which is generally used to remove trends in time series data. The first difference of a time series is defined in terms of the so called difference operator denoted as $D$, that produces the transformation [differencing operator]{.column-margin}\n\n$$\nDy_t = y_t - y_{t-1}.\n$$\n\nHigher order differences are obtained by successively applying the operator $D$. For example,\n\n$$\nD^2y_t = D(Dy_t) = D(y_t - y_{t-1}) = y_t - 2y_{t-1} + y_{t-2}.\n$$\n\nDifferencing can also be written in terms of the so called backshift operator $B$, with [backshift operator]{.column-margin}\n\n$$\nBy_t = y_{t-1},\n$$\n\nso that \n\n$$\nDy_t = (1 - B) y_t\n$$\n\nand\n\n$$\nD^dy_t = (1 - B) d y_t.\n$$\n\n#### Smoothing\n\nThe second method we discuss is [moving averages, which is commonly used to \"smooth\" a time series by removing certain features]{.mark} (e.g., seasonality) [to highlight other features]{.mark} (e.g., trends). [A moving average is a weighted average of the time series around a particular time]{.mark} $t$. In general, if we have data $y1:T$ , we could obtain a new time series such that [moving average]{.column-margin}\n\n$$\nz_t = \\sum_{j=-q}^{p} w_j y_{t+j} \\qquad\n$$ {#eq-moving-average}\n\nfor $t = (q + 1) : (T − p)$, with weights $w_j \\ge 0$ and $\\sum^p_{j=−q} w_j = 1$\n\nWe will frequently work with *moving averages* for which \n\n$$\np = q \\qquad \\text{(centered)}\n$$\n\nand\n\n$$\nw_j = w_{−j} \\forall j  \\text{(symmetric)}\n$$\n\nAssume we have periodic data with period $d$. Then, symmetric and centered moving averages can be used to remove such periodicity as follows:\n\n-   If $d = 2q$ :\n\n$$\nz_t = \\frac{1}{2} (y_{t−q} + y_{t−q+1} + \\ldots + y_{t+q−1} + y_{t+q}) \n$$\n\n-   if $d = 2q + 1$ :\n\n$$\nz_t = \\frac{1}{d} (y_{t−q} + y_{t−q+1} + \\ldots + y_{t+q−1} + y_{t+q})\n$$\n\nExample: To remove seasonality in monthly data (i.e., seasonality with a period of d = 12 months), one can use a moving average with $p = q = 6$, $a_6 = a_{−6} = 1/24$, and $a_j = a_{−j} = 1/12$ for $j = 0, \\ldots , 5$ , resulting in:\n\n$$\nz_t = \\frac{1}{12} (y_{t−6} + y_{t−5} + \\ldots + y_{t+5} + y_{t+6})\n$$\n\n### ACF PACF Differencing and Smoothing Examples (Video)\n\nThis video walks through the two code snippets bellow and provides examples of how to compute the ACF and PACF of a time series, how to use differencing to remove trends, and how to use moving averages to remove seasonality. \n\n\n- Outline:\n  - We begin by simulating data using the code in @sec-white-noise-simulation\n  - We simulates white noise data using the `rnorm(1:2000,mean=0,sd=1)` function in R\n  - We plot the white noise data which we can see lacks a temporal structure.\n  - We plot the ACF using the `acf` function in R:\n    - we specify the number of lags using the `lag.max=20`\n    - we shows a confidence interval for the ACF values\n  - We plot the PACF using the `pacf` function in R\n  - Next we define some time series objects in R using the `ts` function\n    - we define and plot monthly data starting in January 1960\n    - we define and plot yearly data with one observation per year starting in 1960\n    - we define and plot yearly data with four observations per year starting in 1960\n  - We move on to smoothing and differencing  in @sec-differencing-and-smoothing\n  - We load the CO2 dataset in R and plot it\n  - we plot the ACF and PACF of the CO2 dataset\n  - we use the `filter` function in R to remove the seasonal component of the CO2 dataset we plot the resulting time series highlighting the trend.\n  - To remove the trend we use the `diff` function in R to take the first and second differences of the CO2 dataset\n    - the `diff` function takes a parameter `differences` which specifies the number of differences to take\n  - we plot the resulting time series after taking the first and second differences\n  - the ACF and PACF of the resulting time series are plotted, they look different, in that they no longer have the slow decay characteristic of time series with a trend.\n\n\n\nThe r-code for the examples is provided below.\n\n\n### R code for Differencing and filtering via moving averages (reading) {#sec-differencing-and-smoothing}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the CO2 dataset in R\ndata(co2) \n\n# Take first differences to remove the trend \nco2_1stdiff=diff(co2,differences=1)\n\n# Filter via moving averages to remove the seasonality \nco2_ma=filter(co2,filter=c(1/24,rep(1/12,11),1/24),sides=2)\n\npar(mfrow=c(3,1), cex.lab=1.2,cex.main=1.2)\nplot(co2) # plot the original data \nplot(co2_1stdiff) # plot the first differences (removes trend, highlights seasonality)\nplot(co2_ma) # plot the filtered series via moving averages (removes the seasonality, highlights the trend)\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/moving-averages-and-differencing-1.png){width=672}\n:::\n:::\n\n\n\n### R Code: Simulate data from a white noise process (reading) {#sec-white-noise-simulation}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#\n# Simulate data with no temporal structure (white noise)\n#\nset.seed(2021)\nT=200\nt =1:T\ny_white_noise=rnorm(T, mean=0, sd=1)\n#\n# Define a time series object in R: \n# Assume the data correspond to annual observations starting in January 1960 \n#\nyt=ts(y_white_noise, start=c(1960), frequency=1)\n#\n# plot the simulated time series, their sample ACF and their sample PACF\n#\npar(mfrow = c(1, 3), cex.lab = 1.3, cex.main = 1.3)\nyt=ts(y_white_noise, start=c(1960), frequency=1)\nplot(yt, type = 'l', col='red', xlab = 'time (t)', ylab = \"Y(t)\")\nacf(yt, lag.max = 20, xlab = \"lag\",\n    ylab = \"Sample ACF\",ylim=c(-1,1),main=\"\")\npacf(yt, lag.max = 20,xlab = \"lag\",\n     ylab = \"Sample PACF\",ylim=c(-1,1),main=\"\")\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/white noise simulation-1.png){width=672}\n:::\n:::\n\n\n\n### Quiz 1: Stationarity, ACF, PACF, Differencing, and Smoothing\n\nomitted per coursera requirements\n\n## The AR(1) process: Definition and properties\n\nWe will next introduce the autoregressive process of order one, or AR(1) process, which is a fundamental model in time series analysis. We will discuss the definition of the AR(1) process, its properties, and how to simulate data from an AR(1) process.\n\n### The AR(1) process (video)\n\n\n![slide 1](m1_0031.png){.column-margin width=\"250px\"}\n\n![slide 2](m1_0032.png){.column-margin width=\"250px\"}\n\n\n\n{{< lipsum 2 >}}\n\n\n\n\n\n### The PACF of the AR(1) process (reading)\n\nIt is possible to show that the PACF of an autoregressive process of order one is zero after the first lag. We can use the Durbin-Levinson recursion to show this.\n\nFor lag $n = 0$ we have $\\phi(0, 0) = 0$\n\nFor lag $n = 1$ we have:\n\n$$\n\\phi(1, 1) =  \\rho(1) = \\phi\n$$\n\nFor lag $n = 2$ we compute $\\phi(2, 2)$ as:\n\n$$\n\\phi(2, 2) = \\frac{(\\rho(2) − \\phi(1, 1)\\rho(1))}{ (1 − \\phi(1, 1)\\rho(1))} = \\frac{\\phi^2-\\phi^2}{1- \\phi^2}=0\n$$\n\nand we also obtain\n\n$$\n\\phi(2, 1) = \\phi(1, 1) − \\phi(2, 2)\\phi(1, 1) = \\phi.\n$$\n\nFor lag $n = 3$ we compute $\\phi(3, 3)$ as\n\n$$\n\\begin{align*}\n\\phi(3, 3) &= \\frac{(\\rho(3) − \\sum_{h=1}^2 \\phi(2, h)\\rho(3 − h))}{1 − \\sum_{h=1}^2 \\phi(2, h)\\rho(h)} \\newline\n&= \\frac{\\phi^3 - \\phi(2,1) \\rho(2) - \\phi(2,2) \\rho(1)}{1 - \\phi(2,1)\\rho(1) - \\phi(2,2)\\rho(2)} \\newline\n&= \\frac{\\phi^3 - \\phi^3 - 0}{1 - \\phi^2 } \\newline\n&= 0\n\\end{align*}\n$$\n\nand we also obtain\n\n$$\n\\phi(3, 1) = \\phi(2, 1) − \\phi(3, 3)\\phi(2, 2) = \\phi\n$$\n\n$$\n\\phi(3, 2) = \\phi(2, 2) − \\phi(3, 3)\\phi(2, 1) = 0\n$$\n\nWe can prove by induction that in the case of an AR(1), for any lag $n$,\n\n$\\phi(n, h) = 0, \\phi(n, 1) = \\phi$ and $\\phi(n, h) = 0$ for $h \\ge 2$ and $n \\ge 2$.\n\nThen, the PACF of an AR(1) is zero for any lag above 1 and the PACF coefficient at lag 1 is equal to the AR coefficient $\\phi$\n\n### Simulate data from an AR(1) process (video)\n\nThis video walks through the code snippet below and provides examples of how to sample data from an AR(1) process and plot the ACF and PACF functions of the resulting time series.\n\n\n###  R code: Sample data from AR(1) processes (Reading)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample data from 2 ar(1) processes and plot their ACF and PACF functions\n#\nset.seed(2021)\nT=500 # number of time points\n#\n# sample data from an ar(1) with ar coefficient phi = 0.9 and variance 1\n#\nv=1.0 # innovation variance\nsd=sqrt(v) #innovation stantard deviation\nphi1=0.9 # ar coefficient\nyt1=arima.sim(n = T, model = list(ar = phi1), sd = sd)\n#\n# sample data from an ar(1) with ar coefficient phi = -0.9 and variance 1\n#\nphi2=-0.9 # ar coefficient\nyt2=arima.sim(n = T, model = list(ar = phi2), sd = sd)\n\npar(mfrow = c(2, 1), cex.lab = 1.3)\nplot(yt1,main=expression(phi==0.9))\nplot(yt2,main=expression(phi==-0.9))\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/ar(1) sampling-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(3, 2), cex.lab = 1.3)\nlag.max=50 # max lag\n#\n## plot true ACFs for both processes\n#\ncov_0=sd^2/(1-phi1^2) # compute auto-covariance at h=0\ncov_h=phi1^(0:lag.max)*cov_0 # compute auto-covariance at h\nplot(0:lag.max, cov_h/cov_0, pch = 1, type = 'h', col = 'red',\n     ylab = \"true ACF\", xlab = \"Lag\",ylim=c(-1,1), main=expression(phi==0.9))\n\ncov_0=sd^2/(1-phi2^2) # compute auto-covariance at h=0\ncov_h=phi2^(0:lag.max)*cov_0 # compute auto-covariance at h\n# Plot autocorrelation function (ACF)\nplot(0:lag.max, cov_h/cov_0, pch = 1, type = 'h', col = 'red',\n     ylab = \"true ACF\", xlab = \"Lag\",ylim=c(-1,1),main=expression(phi==-0.9))\n\n## plot sample ACFs for both processes\n#\nacf(yt1, lag.max = lag.max, type = \"correlation\", ylab = \"sample ACF\",\n    lty = 1, ylim = c(-1, 1), main = \" \")\nacf(yt2, lag.max = lag.max, type = \"correlation\", ylab = \"sample ACF\",\n    lty = 1, ylim = c(-1, 1), main = \" \")\n## plot sample PACFs for both processes\n#\npacf(yt1, lag.ma = lag.max, ylab = \"sample PACF\", ylim=c(-1,1),main=\"\")\npacf(yt2, lag.ma = lag.max, ylab = \"sample PACF\", ylim=c(-1,1),main=\"\")\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/ar(1) sampling-2.png){width=672}\n:::\n:::\n\n\n\n### Quiz 2: The AR(1) definition and properties\n\nOmitted per Coursera honor code requirements.\n\n## The AR(1) process:Maximum likelihood estimation and Bayesian inference\n\n\n### Review of maximum likelihood and Bayesian inference in regression\n\n#### Regression Models: Maximum Likelihood Estimation\n\nAssume a regression model with the following structure: \n$$\ny_i = \\beta_1x_{i,1} + \\ldots + \\beta_kx_{i,k} + \\epsilon_i,\n$$\n\nfor $i = 1, \\ldots, n$ and $\\epsilon_i$ independent random variables with $\\epsilon_i \\sim N (0, v) \\forall i$. This model can be written in matrix form as: \n\n$$\ny = X \\beta + \\epsilon, \\epsilon \\sim N (0, vI), \\qquad\n$$\n\nwhere:\n\n-   $y = (y_1, \\ldots, y_n)′$ is an n-dimensional vector of responses,\n-   $X$ is an n × k matrix containing the explanatory variables,\n-   $\\beta = (\\beta_1, \\ldots, \\beta_k)′$ is the k-dimensional vector of regression coefficients,\n-   $\\epsilon = (\\epsilon_1, \\ldots, \\epsilon_n)′$ is the n-dimensional vector of errors,\n-   $I$ is an n × n identity matrix.\n\nIf $X$ is a full rank matrix with rank $k$ the maximum likelihood estimator for β, denoted as $\\hat\\beta_{MLE}$ is given by:\n\n$$\n\\hat\\beta_{MLE} = (X′X)^{−1}X′y,\n$$\n\nand the MLE for v is given by\n\n$$\n\\hat v_{MLE} = \\frac{1}{n} (y − X \\hat\\beta_{MLE})′(y − X \\hat\\beta_{MLE})\n$$\n\n$\\hat v_{MLE}$ is not an unbiased estimator of v, therefore, the following unbiased estimator of v is typically used:\n\n$$\ns^2 = \\frac{1}{n-k}(y − X \\hat\\beta_{MLE} )′(y − X \\hat\\beta_{MLE} )\n$$\n\n#### Regression Models: Bayesian Inference\n\nAssume once again we have a model with the structure in (1), which results in a likelihood of the form\n\n$$\np(y \\mid \\beta , v) = \\frac{1}{(2\\pi v)^{n/2}}\\exp \\left\\{ -\\frac{1}{2} (y − X\\beta)′(y − X\\beta) \\right\\}\n$$\n\nIf a prior of the form \n\n$$ \np(\\beta, v) \\propto \\frac{1}{v}\n$$\n\nis used, we obtain that the posterior distribution is given by\n\n$$\np(\\beta,v \\mid y) \\propto \\frac{1}{v^{n/2+1}}\\exp \\left\\{ -\\frac{1}{2v} (y − X\\beta)′(y − X\\beta) \\right\\}\n$$\n\nIn addition it can be shown that\n\n-   $(β\\mid v, y) \\sim N (\\hat \\beta_{MLE} , v(X′X)−1)$\n-   $(v\\mid y) \\sim IG((n − k)/2, d/2)$ with\n\n$$\nd = (y − X \\hat \\beta_{MLE} )′(y − \\hat \\beta_{MLE} )\n$$\n\nwith $k = dim(\\beta)$.\n\nGiven that $p(\\beta, v \\mid y) = p(\\beta \\mid v, y)p(v \\mid y)$ the equations above provide a way to directly sample from the posterior distribution of β and v by first sampling v from the inverse-gamma distribution above and then conditioning on this sampled value of v, sampling β from the normal distribution above.\n\n### Maximum likelihood estimation in the AR(1) (video)\n\n![slide 1](m1_0041.png){.column-margin width=\"250px\"}\n\n![slide 2](m1_0042.png){.column-margin width=\"250px\"}\n\n![slide 3](m1_0043.png){.column-margin width=\"250px\"}\n\n\n\n{{< lipsum 3 >}}\n\n\n\n\n### R code: MLE for the AR(1), examples (reading)\n\nThe following code allows you to compute the MLE of the AR coefficient $\\psi$, the unbiased estimator of $v$, $s^2$ , and the MLE of v based on a dataset simulated from an AR(1) process and using the conditional likelihood.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2021)\nphi=0.9 # ar coefficient\nv=1\nsd=sqrt(v) # innovation standard deviation\nT=500 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Case 1: Conditional likelihood\ny=as.matrix(yt[2:T]) # response\nX=as.matrix(yt[1:(T-1)]) # design matrix\nphi_MLE=as.numeric((t(X)%*%y)/sum(X^2)) # MLE for phi\ns2=sum((y - phi_MLE*X)^2)/(length(y) - 1) # Unbiased estimate for v \nv_MLE=s2*(length(y)-1)/(length(y)) # MLE for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"MLE for the variance v: \", v_MLE, \"\\n\", \n    \"Estimate s2 for the variance v: \", s2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n MLE of conditional likelihood for phi:  0.9261423 \n MLE for the variance v:  1.048 \n Estimate s2 for the variance v:  1.050104 \n```\n\n\n:::\n:::\n\n\n\nThis code allows you to compute estimates of the AR(1) coefficient and the variance using the `arima` function in R. The first case uses the conditional sum of squares, the second and third cases use the full likelihood with different starting points for the numerical optimization required to compute the MLE with the full likelihood.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtaining parameter estimates using the arima function in R\nset.seed(2021)\nphi=0.9 # ar coefficient\nv=1\nsd=sqrt(v) # innovation standard deviation\nT=500 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n#Using conditional sum of squares, equivalent to conditional likelihood \narima_CSS=arima(yt,order=c(1,0,0),method=\"CSS\",n.cond=1,include.mean=FALSE)\ncat(\"AR estimates with conditional sum of squares (CSS) for phi and v:\", arima_CSS$coef,arima_CSS$sigma2,\n\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAR estimates with conditional sum of squares (CSS) for phi and v: 0.9261423 1.048 \n```\n\n\n:::\n\n```{.r .cell-code}\n#Uses ML with full likelihood \narima_ML=arima(yt,order=c(1,0,0),method=\"ML\",include.mean=FALSE)\ncat(\"AR estimates with full likelihood for phi and v:\", arima_ML$coef,arima_ML$sigma2,\n\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAR estimates with full likelihood for phi and v: 0.9265251 1.048434 \n```\n\n\n:::\n\n```{.r .cell-code}\n#Default: uses conditional sum of squares to find the starting point for ML and \n#         then uses ML \narima_CSS_ML=arima(yt,order=c(1,0,0),method=\"CSS-ML\",n.cond=1,include.mean=FALSE)\ncat(\"AR estimates with CSS to find starting point for ML for phi and v:\", \narima_CSS_ML$coef,arima_CSS_ML$sigma2,\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAR estimates with CSS to find starting point for ML for phi and v: 0.9265252 1.048434 \n```\n\n\n:::\n:::\n\n\n\nThis code shows you how to compute the MLE for $\\psi$ using the full likelihood and the function optimize in R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2021)\nphi=0.9 # ar coefficient\nv=1\nsd=sqrt(v) # innovation standard deviation\nT=500 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## MLE, full likelihood AR(1) with v=1 assumed known \n# log likelihood function\nlog_p <- function(phi, yt){\n  0.5*(log(1-phi^2) - sum((yt[2:T] - phi*yt[1:(T-1)])^2) - yt[1]^2*(1-phi^2))\n}\n\n# Use a built-in optimization method to obtain maximum likelihood estimates\nresult =optimize(log_p, c(-1, 1), tol = 0.0001, maximum = TRUE, yt = yt)\ncat(\"\\n MLE of full likelihood for phi: \", result$maximum)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n MLE of full likelihood for phi:  0.9265928\n```\n\n\n:::\n:::\n\n\n\n### Bayesian inference in the AR(1)\n\n![slide 1](m1_0051.png){.column-margin width=\"250px\"}\n\n\n\n{{< lipsum 1 >}}\n\n\n\n\n### Bayesian inference in the AR(1): Conditional likelihood example (video)\n\nThis video walks through the code snippet below and provides examples of how to sample from the posterior distribution of the AR coefficient $\\psi$ and the variance $v$ using the conditional likelihood and a reference prior.\n\n\n### R Code: AR(1) Bayesian inference, conditional likelihood example (reading)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n####################################################\n#####             MLE for AR(1)               ######\n####################################################\nset.seed(2021)\nphi=0.9 # ar coefficient\nsd=1 # innovation standard deviation\nT=200 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) # sample stationary AR(1) process\n\ny=as.matrix(yt[2:T]) # response\nX=as.matrix(yt[1:(T-1)]) # design matrix\nphi_MLE=as.numeric((t(X)%*%y)/sum(X^2)) # MLE for phi\ns2=sum((y - phi_MLE*X)^2)/(length(y) - 1) # Unbiased estimate for v\nv_MLE=s2*(length(y)-1)/(length(y)) # MLE for v \n\nprint(c(phi_MLE,s2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9178472 1.0491054\n```\n\n\n:::\n\n```{.r .cell-code}\n#######################################################\n######     Posterior inference, AR(1)               ###\n######     Conditional Likelihood + Reference Prior ###\n######     Direct sampling                          ###\n#######################################################\n\nn_sample=3000   # posterior sample size\n\n## step 1: sample posterior distribution of v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2)/2, sum((yt[2:T] - phi_MLE*yt[1:(T-1)])^2)/2)\n\n## step 2: sample posterior distribution of phi from normal distribution\nphi_sample=rep(0,n_sample)\nfor (i in 1:n_sample){\nphi_sample[i]=rnorm(1, mean = phi_MLE, sd=sqrt(v_sample[i]/sum(yt[1:(T-1)]^2)))}\n\n## plot histogram of posterior samples of phi and v\npar(mfrow = c(1, 2), cex.lab = 1.3)\nhist(phi_sample, xlab = bquote(phi), \n     main = bquote(\"Posterior for \"~phi),xlim=c(0.75,1.05), col='lightblue')\nabline(v = phi, col = 'red')\nhist(v_sample, xlab = bquote(v), col='lightblue', main = bquote(\"Posterior for \"~v))\nabline(v = sd, col = 'red')\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/AR(1) inference, conditional likelihood example-1.png){width=672}\n:::\n:::\n\n\n\n### Quizz - MLE and Bayesian inference in the AR(1)\n\nOmitted per Coursera honor code\n\n### Practice Graded Assignment: MLE and Bayesian inference in the AR(1)\n\nThis peer-reviewed activity is highly recommended. It does not figure into your grade for this course, but it does provide you with the opportunity to apply what you've learned in R and prepare you for your data analysis project in week 5.\n\n1.  Consider the R code below: MLE for the AR(1)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n####################################################\n#####             MLE for AR(1)               ######\n####################################################\nphi=0.9 # ar coefficient\nv=1\nsd=sqrt(v) # innovation standard deviation\nT=500 # number of time points\nyt=arima.sim(n = T, model = list(ar = phi), sd = sd) \n\n## Case 1: Conditional likelihood\ny=as.matrix(yt[2:T]) # response\nX=as.matrix(yt[1:(T-1)]) # design matrix\nphi_MLE=as.numeric((t(X)%*%y)/sum(X^2)) # MLE for phi\ns2=sum((y - phi_MLE*X)^2)/(length(y) - 1) # Unbiased estimate for v \nv_MLE=s2*(length(y)-1)/(length(y)) # MLE for v\n\ncat(\"\\n MLE of conditional likelihood for phi: \", phi_MLE, \"\\n\",\n    \"MLE for the variance v: \", v_MLE, \"\\n\", \n    \"Estimate s2 for the variance v: \", s2, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n MLE of conditional likelihood for phi:  0.9048951 \n MLE for the variance v:  1.084559 \n Estimate s2 for the variance v:  1.086737 \n```\n\n\n:::\n:::\n\n\n\nModify the code above to sample 800 observations from an AR(1) with AR coefficient $\\psi = -0.8$ and variance $v = 2$. Plot your simulated data. Obtain the MLE for $\\psi$ based on the conditional likelihood and the unbiased estimate $s^2$ for the variance $v$.\n\n2.  Consider the R code below: AR(1) Bayesian inference, conditional likelihood\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#######################################################\n######     Posterior inference, AR(1)               ###\n######     Conditional Likelihood + Reference Prior ###\n######     Direct sampling                          ###\n#######################################################\n\nn_sample=3000   # posterior sample size\n\n## step 1: sample posterior distribution of v from inverse gamma distribution\nv_sample=1/rgamma(n_sample, (T-2)/2, sum((yt[2:T] - phi_MLE*yt[1:(T-1)])^2)/2)\n\n## step 2: sample posterior distribution of phi from normal distribution\nphi_sample=rep(0,n_sample)\nfor (i in 1:n_sample){\nphi_sample[i]=rnorm(1, mean = phi_MLE, sd=sqrt(v_sample[i]/sum(yt[1:(T-1)]^2)))}\n\n## plot histogram of posterior samples of phi and v\npar(mfrow = c(1, 2), cex.lab = 1.3)\nhist(phi_sample, xlab = bquote(phi), \n     main = bquote(\"Posterior for \"~phi),xlim=c(0.75,1.05), col='lightblue')\nabline(v = phi, col = 'red')\nhist(v_sample, xlab = bquote(v), col='lightblue', main = bquote(\"Posterior for \"~v))\nabline(v = sd, col = 'red')\n```\n\n::: {.cell-output-display}\n![](module1_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nUsing your simulated data from part 1 modify the code above to summarize your posterior inference for $\\psi$ and $v$ based on 5000 samples from the joint posterior distribution of $\\psi$ and $v$.\n\n::: callout-tip\n\n#### Grading Criteria\n\nThe responses should follow the same template as the sample code provided above but you will submit your code lines in plain text. Peer reviewers will be asked to check whether the different pieces of code have been adequately modified to reflect that :\n\n1.  you generate 800 time points from the AR(1) rather than 500 and plot your simulated data.\n2.  your simulated data is from an AR(1) with AR cofficient $\\psi = -0.8$ and variance $v = 2$ rather than AR(1) with AR coefficient $\\psi = 0.9$ and variance $v = 1$ and\n3.  you obtain 5000 rather than 3000 samples from the posterior distribution from the new simulated process.\n:::\n\n### Bayesian Inference in the AR(1), : full likelihood example (reading)\n\nWe consider a prior distribution that assumes that $\\phi$ and $v$ are independent:\n\n$$\np(v) \\propto \\frac{1}{v},\n$$\n\n$$\np(\\phi) = \\frac{1}{2}, \\quad \\text{for } \\phi \\in (-1, 1),\n$$\n\ni.e., we assume a Uniform prior for $\\phi \\in (-1, 1)$. Combining this prior with the full likelihood in the AR(1) case, we obtain the following posterior density:\n\n$$\np(\\phi, v \\mid y_{1:T}) \\propto \\frac{(1 - \\phi^2)^{1/2} }{v^{T/2 + 1}} \\exp\\left(-\\frac{Q^*(\\phi)}{2v}\\right), \\quad -1 < \\phi < 1,\n$$\n\nwith\n\n$$\nQ^*(\\phi) = y_1^2(1 - \\phi^2) + \\sum_{t=2}^{T} (y_t - \\phi y_{t-1})^2.\n$$\n\nIt is not possible to get a closed-form expression for this posterior or to perform direct simulation. Therefore, we use simulation-based Markov Chain Monte Carlo (MCMC) methods to obtain samples from the posterior distribution.\n\n#### Transformation of $\\phi$\n\nWe first consider the following transformation on $\\phi$:\n\n$$\n\\eta = \\log\\left(\\frac{1 - \\phi}{\\phi + 1}\\right),\n$$\n\nso that $\\eta \\in (-\\infty, \\infty)$. The inverse transformation on $\\eta$ is:\n\n$$\n\\phi = \\frac{1 - \\exp(\\eta)}{1 + \\exp(\\eta)}.\n$$\n\nWriting down the posterior density for $\\eta$ and $v$, we obtain\n\n$$\np(\\eta, v \\mid y_{1:T}) \\propto\\frac{ (1 - \\phi^2)^{1/2} }{v^{T/2 + 1}} \\exp\\left(-\\frac{Q^*(\\phi)}{2v}\\right) \\cdot \\frac{2 \\exp(\\eta)}{(1 + \\exp(\\eta))^2},\n$$\n\nwith $\\phi$ written as a function of $\\eta$. We proceed to obtain samples from this posterior distribution using the MCMC algorithm outlined below. Once we have obtained $M$ samples from $\\eta$ and $v$ after convergence, we can use the inverse transformation above to obtain posterior samples for $\\phi$.\n\n#### MCMC Algorithm: Bayesian Inference for AR(1), Full Likelihood\n\n**Algorithm**:\n\n1.  Initialize $\\eta^{(0)}$ and $\\beta^{(0)}$.\n2.  For $m$ in $1:M$ do:\n    -   Sample $v^{(m)} \\sim \\text{IG}\\left(\\frac{T}{2}, \\frac{Q^*(\\phi^{(m-1)})}{2}\\right)$.\n    -   Sample $\\eta^{(m)}$ using Metropolis-Hastings:\n        1.  Sample $\\eta^* \\sim N(\\eta^{(m-1)}, c)$, where $c$ is a tuning parameter.\n        2.  Compute the importance ratio:\n\n$$\n        r = \\frac{p(\\eta^*, v^{(m)} \\mid y_{1:T})}{p(\\eta^{(m-1)}, v^{(m)} \\mid y_{1:T})}.\n$$\n\n 3. Set: \n\n$$\n        \\eta^{(m)} =\n        \\begin{cases}\n        \\eta^* & \\text{with probability } \\min(r, 1), \\\\\n        \\eta^{(m-1)} & \\text{otherwise}.\n        \\end{cases}\n$$\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}