{
  "hash": "f7a90e121855b521cda8c460d7da7039",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Primitive Actions Ontology\nsubtitle: Knowlede-Based AI --- Cognitive Systems\ndate: 2021-04-12\ndraft: true\ncategories: [notes, KB-AI, frames, modelling, AI, Primitive Actions] \nimage: nlp-brain-wordcloud.jpg\ntitle-block-banner: banner_black_3.jpg\neditor: \n  markdown: \n    wrap: sentence\n---\n\n# Primitive Actions\n\n**Primitive Actions** were proposed by *Roger Shank*  in [@Schank1977Scripts] circa 1972-5. These were supposed to represent statements in everyday discourse. Shank decided these on these by noting similarities of sentence placed into an actor-action-object framework.\nfor example: \n\n> \"Alice took the book from Bob\"\n\n\n```{mermaid}\nerDiagram    \n    Thematic-Role-Frame {\n    verb move-possession\n    agent Alice\n    source Bob\n    thematic-object book\n    }\n```\n\n::: {#2d278849 .cell execution_count=1}\n``` {.python .cell-code}\nthematic_role_frame_1 = {\n        'verb': 'move-possession',\n        'agent': 'Alice',\n        'source': 'Bob',\n        'thematic-object': 'book',\n}\n    \nprint(thematic_role_frame_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'verb': 'move-possession', 'agent': 'Alice', 'source': 'Bob', 'thematic-object': 'book'}\n```\n:::\n:::\n\n\n## The Primitive Actions \n\n::: {#cb1c7b7a .cell execution_count=2}\n``` {.python .cell-code}\nprimitive_actions = [     \n    'move-body-part' , 'move-possession', 'move-concept'   , 'move-object' ,  \n    'expel', 'ingest','propel', 'see', 'hear', 'speak','smell', 'feel',\n    'think-about','conclude'      \n]\nfor idx,action in  enumerate(primitive_actions):\n    print(f'{idx:2}. {action}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 0. move-body-part\n 1. move-possession\n 2. move-concept\n 3. move-object\n 4. expel\n 5. ingest\n 6. propel\n 7. see\n 8. hear\n 9. speak\n10. smell\n11. feel\n12. think-about\n13. conclude\n```\n:::\n:::\n\n\nThe main advantage of primitive verbs is that we reduce all action into these primitives which simplifies the class of verb from a large and open class to just 14 verbs. Also if one views the verb as the main component of most senteces we have an good entry point for representing senteces. As usual having so few verbs is a very weak represenatation. So the next step is to extend the primitive verbs by adding the ability to match particles. This greatly extends the expressive power of the system.\n\n## Details\n\n> The :smiling_imp: is in the details\nMaking use of primitive verbs requires an algorithm to parse a verb in. But in the course did not make a serious attempt at an algorithm. They presented the idea of primitive verbs and then showed that it requires the agent to be able to process $\\theta$-roles. Again this is a very big challenge both in terms of data and algorithm.\n\n## Processing\n\nSo how should we go about using primitive verbs. We could try to use of the \n\nThe following is the algorithm for processing a sentence \nfor each verb we find generate the different frames ordered by similarity. \nthen we can test the frame.\ntesting may be:\n\n1. while generator.has_next():\n1.  if all(map(frame.get_slots(),lambda:slot != [] ) # no empty slots?\n1.  $\\implies$ all slots are filled without violating hard (syntax) constraints.\n1.  if current solution has a lower score for soft constraints (semantics)\n    replace best match.\n\n::: {#be951ee1 .cell execution_count=3}\n``` {.python .cell-code}\n# unfortunately much time has passed since I wrote this snipet \n# and now that I can flesh it out, it is not very clear what I inteded.\n# I'll try to review and get back to this.\n#  \n\nimport functools\ndef is_verb(concept):\n    return False\ndef is_solution(concept,slot):\n    return False\ndef all_slot_are_full():\n    return True\ndef parse_primitive(words):\n    pre  = []\n    post = [ ]\n    for concept in words:\n        if not is_verb(concept) :\n            pre += concept\n            continue\n        else:\n            verb = concept\n            post = words [len(pre)+1:-1]\n            sort(primitives,  functools.partial(verb_similarity,verb) )\n            for primitive in primitives:\n                frame = frames[primitive]\n                for slot in frame:\n                    for concept in words:\n                        if is_solution(concept,slot):\n                            frame[slot] = concept\n                        if all_slot_are_full():\n                            return frame\n                        else:\n                            continue\n    return -1\n```\n:::\n\n\nwhere verb_similarity is a similarity function e.g. cosine distance.\nissues: we want to find the optimal match between the primitive frame slots and the available concepts\nsubject to \n\n2. for each open slot in the frame\n    assign the closest concept that fits according to:\n        - $\\theta$-roles$\n        - subcategorization  constraints. (verb complement syntax.)\n        - selection constraints. (semantic feature preference.)\n            \nexample:\n\n> \"Alice pushed the cart\"\n\n\n```{mermaid}\nerDiagram    \n    Thematic-Role-Frame {\n    verb propel\n    agent Alice\n    source Bob\n    thematic-object cart\n    }\n    agent {\n    constraint-1 animate\n    constraint-2 left-of-verb\n    }\n    thematic-object { \n    constraint-1 inanimate\n    constraint-2 right-of-verb\n    }\n```\n\n\n> \"Alice shot Bill\"\nrewritten as \n> Alice propelled a bullet into Bill.\n\n\n\n```{mermaid}\nerDiagram    \n    Thematic-Role-Frame {\n    verb propel\n    agent Alice\n    destination Bill\n    object bullet\n    }\n    Thematic-Role-Frame ||--|{ agent-constraints : satisfies\n    Thematic-Role-Frame ||--|{ object-constraints : satisfies\n    Thematic-Role-Frame ||--|{ destination-constraints : satisfies\n    agent-constraints {    \n    lexical-feature animate\n    position before-verb\n    }\n    object-constraints { \n    lexical-feature inanimate\n    position after-verb\n    } \n    destination-constraints { \n    proposition into\n    }\n```\n\n\nin this case:\n\nActions often imply an implicit state change and a cause and effect relations. We can also capture the outcome in a state change frame:\n\n> \"Alice enjoyed putting the wedge on the red block\n\n\n```{mermaid}\nerDiagram    \n    Thematic-Role-Frame {\n    verb move-object\n    agent Alice\n    object wedge\n    destination block\n    }    \n    Thematic-Role-Frame ||--|| State-Change-Frame : induces     \n    State-Change-Frame {\n    object alice\n    destination happy\n    }\n```\n\n\nThis looks like this might be a great job for an attention mechanism. We can try to learn using say 14 heads to specialize so as to match different parts of the sentence into each slot in the frame, then we would pick the best match as the frame. This may be even more powerful when looking at complex sentences where getting a full set of constraints would be more difficulty to encode manually.\n\n> \"John fertilized the field\"\nis difficult to encode but the equivalent:\n> \"John put fertilizer on the field\"\n\n| Action Frame  |   Value        | Constraints            |\n|:-------------:|:--------------:|:-----------------------|\n| **primitive** | put            |                        |\n| **agent**     | John           | +animate, left of verb |\n| **object**    | the cart       | +inanimate, right of the verb  |\n\n## Advantages \n\nThe main advantage of using this ontology is that it greatly reduces the complexity of expressing actions in normal language:\n\n- The unstructured sentence becomes a structured frame and if needed a frame graph.\n- The open category of verb is mapped to a smaller quotient space of 14 verbs equivalency groups.\n- It is easier to reason about the *primitive verb*.\n- Can be viewed as an **embedding**.\n- The *primitive verbs* are naturally extended using a context slots which allow many more meaning.\n- Can bed used to construct meaning of complex verb but allow analogy and common-sense reasoning.\n\n## Disadvantages \n\n- A number of these seem to fall under sense, could we use less and retain the power of this method.\n- Other verb groups might be interesting\n- How do we encode I love some one or I make something?\n- Non English verbs - would the choice of primitive actions change if we change language. I hazarded a guess that It should not.\n\n\n## Ontologies\n\nPrimitive action can be viewed as a precursor to more modern *Top Level Ontologies* or TLOs. A TLO is classified as generic if it avoid a choice of modeling concepts as entities or attributes. More generally generic mean avoiding commitments to a specific data model.[UML](), [Topic Maps](https://en.wikipedia.org/wiki/Topic_map) and [Schema.org](Schema.org)  are examples of generic ontologies. A consequence of an ontology being generic is a weaker mapping from the domain to the real world. I think there is much to be said on TLO's for deep learning applications but this will have to wait to a future post. For now I point you to a link a survey in the references section.\nMy view of an ontology like primitive actions is that it induces a *topology* on the information. \nA *rough topology* has fewer opportunity for concepts to be separated then a finer one but may be better at clustering. Being able to cluster related concepts is key when it comes to being able to generalize, particularly for NLP.\nWhile Primitive actions is a top level (rough) ontology it can be further clustered as follows:\n\n- primitive action\n  - move\n    - body part\n        - kick\n        - punch\n        - slap\n        - wink\n        - inhale\n        - exhale\n    - whole (transfer)\n        - drive \n        - push \n        - pull\n        - drop\n        - pick-up\n        - mail\n        - fly\n    - possession \n        - give\n        - gift\n        - offer\n    - concept \n        - communicate\n    - propel (move away)\n    - ingest (move in)\n        - eat\n        - drink\n    - expel (move out)\n        - cough\n        - spit\n        - eject\n        - exile\n  - sense\n    - see\n    - hear\n    - feel\n    - smell\n    - speak\n  - think\n    - think-about\n    - conclude\n    \n    \nWhere the sense of move may be inferred from the object \nWhen encoding sentences using using primitive action some cases are very intuitive while others require transforming the sentence  to fit within the 14 primitive action frame works.\n\n## Further work.\n\n1. Is there an annotated corpus using this ontology?\nUsing DNN a embeddings on many sentences what embeddings would the net pick. Would the representatives be like ours or different. \nWould dimension reduction on verbs results in this ontology arising - if not would such an ontology be more useful?\n\nie. would it use these primitives or go for quite different ones.\nFor a continuous learning paradigm one may try an algorithm which balances between exploration and exploitation in building an ontology of learned relation with dual goal of \n\n1. covering and separating many different concepts\n1. understanding how these generalize to a few abstract concepts.\n1. model a pooling posterior encoding shared knowledge.\n\n<hr>\n\n![Thematic Roles](KBAI-thematic-roles.png)\n\n# References\n\n- these Notes are based in part on \"Common Sense reasoning\", lesson 15 of the \"Cognitive Agents\" Udacity course.\n- [The Primitive ACTs of Conceptual Dependency] (Roger Shank)retrieved 13th of July 2017.\n- [Primitive action](https://computersciencewiki.org/index.php/Primitive_action)\n- [The Primitive ACTs of Conceptual Dependency](https://www.aclweb.org/anthology/T75-2008) (retrieved 13th of July.)\n- [Crowdsourcing a Parallel Corpus for Conceptual Analysis of Natural Language](https://www.aaai.org/ocs/index.php/HCOMP/HCOMP17/paper/viewFile/15924/15270) (Jamie C. Macbeth, Sandra Grandic 2017)\n- [A survey of Top-Level Ontologies](https://www.cdbb.cam.ac.uk/files/a_survey_of_top-level_ontologies_lowres.pdf)\n- [Understanding the Natural and Artificial Worlds](http://courses.washington.edu/thesisd/documents/Kun_Herbert%20Simon_Sciences_of_the_Artificial.pdf)\n- Stefik, M. (1995). Knowledge Systems. Morgan Kauffman: San Fransisco.\n- Rich, E., & Knight, K. (1991). Artificial intelligence. McGraw-Hill, New York.\n- Russell, S. & Norvig, P. (1995). Artificial Intelligence: A modern approach. Prentice-Hall: Englewood Cliffs.\n- Winston, P. (1993). Artificial Intelligence (3rd ed.). Addision-Wesley.\n\n",
    "supporting": [
      "2021-04-12-primitive-actions_files"
    ],
    "filters": [],
    "includes": {}
  }
}