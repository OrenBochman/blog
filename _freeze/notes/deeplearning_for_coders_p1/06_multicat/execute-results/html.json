{
  "hash": "a2f16d62fb7d1e96560459ed78e65da4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Other Computer Vision Problems\njupyter:\n  jupytext:\n    split_at_heading: true\n  kernelspec:\n    display_name: Python 3\n    language: python\n    name: python3\n---\n\n::: {#83798bee .cell execution_count=1}\n``` {.python .cell-code}\n#hide\n!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n```\n:::\n\n\n::: {#c7c1722a .cell execution_count=2}\n``` {.python .cell-code}\n#hide\nfrom fastbook import *\n```\n:::\n\n\n[[chapter_multicat]]\n\n\n\nIn the previous chapter you learned some important practical techniques for training models in practice. Considerations like selecting learning rates and the number of epochs are very important to getting good results.\n\nIn this chapter we are going to look at two other types of computer vision problems: multi-label classification and regression. The first one is when you want to predict more than one label per image (or sometimes none at all), and the second is when your labels are one or several numbers—a quantity instead of a category.\n\nIn the process will study more deeply the output activations, targets, and loss functions in deep learning models.\n\n## Multi-Label Classification\n\nMulti-label classification refers to the problem of identifying the categories of objects in images that may not contain exactly one type of object. There may be more than one kind of object, or there may be no objects at all in the classes that you are looking for.\n\nFor instance, this would have been a great approach for our bear classifier. One problem with the bear classifier that we rolled out in <<chapter_production>> was that if a user uploaded something that wasn't any kind of bear, the model would still say it was either a grizzly, black, or teddy bear—it had no ability to predict \"not a bear at all.\" In fact, after we have completed this chapter, it would be a great exercise for you to go back to your image classifier application, and try to retrain it using the multi-label technique, then test it by passing in an image that is not of any of your recognized classes.\n\nIn practice, we have not seen many examples of people training multi-label classifiers for this purpose—but we very often see both users and developers complaining about this problem. It appears that this simple solution is not at all widely understood or appreciated! Because in practice it is probably more common to have some images with zero matches or more than one match, we should probably expect in practice that multi-label classifiers are more widely applicable than single-label classifiers.\n\nFirst, let's see what a multi-label dataset looks like, then we'll explain how to get it ready for our model. You'll see that the architecture of the model does not change from the last chapter; only the loss function does. Let's start with the data.\n\n### The Data\n\nFor our example we are going to use the PASCAL dataset, which can have more than one kind of classified object per image.\n\nWe begin by downloading and extracting the dataset as per usual:\n\n::: {#11408018 .cell execution_count=3}\n``` {.python .cell-code}\nfrom fastai.vision.all import *\npath = untar_data(URLs.PASCAL_2007)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n    <div>\n      <progress value='1637801984' class='' max='1637796771' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [1637801984/1637796771 10:01&lt;00:00]\n    </div>\n    \n```\n:::\n:::\n\n\nThis dataset is different from the ones we have seen before, in that it is not structured by filename or folder but instead comes with a CSV (comma-separated values) file telling us what labels to use for each image. We can inspect the CSV file by reading it into a Pandas DataFrame:\n\n::: {#af4b57a3 .cell execution_count=4}\n``` {.python .cell-code}\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fname</th>\n      <th>labels</th>\n      <th>is_valid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000005.jpg</td>\n      <td>chair</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000007.jpg</td>\n      <td>car</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000009.jpg</td>\n      <td>horse person</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000012.jpg</td>\n      <td>car</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000016.jpg</td>\n      <td>bicycle</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAs you can see, the list of categories in each image is shown as a space-delimited string.\n\n### Sidebar: Pandas and DataFrames\n\nNo, it’s not actually a panda! *Pandas* is a Python library that is used to manipulate and analyze tabular and time series data. The main class is `DataFrame`, which represents a table of rows and columns. You can get a DataFrame from a CSV file, a database table, Python dictionaries, and many other sources. In Jupyter, a DataFrame is output as a formatted table, as shown here.\n\nYou can access rows and columns of a DataFrame with the `iloc` property, as if it were a matrix:\n\n::: {#080baf44 .cell execution_count=5}\n``` {.python .cell-code}\ndf.iloc[:,0]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n```\n:::\n:::\n\n\n::: {#f058d2d6 .cell execution_count=6}\n``` {.python .cell-code}\ndf.iloc[0,:]\n# Trailing :s are always optional (in numpy, pytorch, pandas, etc.),\n#   so this is equivalent:\ndf.iloc[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nfname       000005.jpg\nlabels           chair\nis_valid          True\nName: 0, dtype: object\n```\n:::\n:::\n\n\nYou can also grab a column by name by indexing into a DataFrame directly:\n\n::: {#a0b15188 .cell execution_count=7}\n``` {.python .cell-code}\ndf['fname']\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0       000005.jpg\n1       000007.jpg\n2       000009.jpg\n3       000012.jpg\n4       000016.jpg\n           ...    \n5006    009954.jpg\n5007    009955.jpg\n5008    009958.jpg\n5009    009959.jpg\n5010    009961.jpg\nName: fname, Length: 5011, dtype: object\n```\n:::\n:::\n\n\nYou can create new columns and do calculations using columns:\n\n::: {#acfed54e .cell execution_count=8}\n``` {.python .cell-code}\ntmp_df = pd.DataFrame({'a':[1,2], 'b':[3,4]})\ntmp_df\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#6bb96d5d .cell execution_count=9}\n``` {.python .cell-code}\ntmp_df['c'] = tmp_df['a']+tmp_df['b']\ntmp_df\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nPandas is a fast and flexible library, and an important part of every data scientist’s Python toolbox. Unfortunately, its API can be rather confusing and surprising, so it takes a while to get familiar with it. If you haven’t used Pandas before, we’d suggest going through a tutorial; we are particularly fond of the book [*Python for Data Analysis*](http://shop.oreilly.com/product/0636920023784.do) by Wes McKinney, the creator of Pandas (O'Reilly). It also covers other important libraries like `matplotlib` and `numpy`. We will try to briefly describe Pandas functionality we use as we come across it, but will not go into the level of detail of McKinney’s book.\n\n### End sidebar\n\nNow that we have seen what the data looks like, let's make it ready for model training.\n\n### Constructing a DataBlock\n\nHow do we convert from a `DataFrame` object to a `DataLoaders` object? We generally suggest using the data block API for creating a `DataLoaders` object, where possible, since it provides a good mix of flexibility and simplicity. Here we will show you the steps that we take to use the data blocks API to construct a `DataLoaders` object in practice, using this dataset as an example.\n\nAs we have seen, PyTorch and fastai have two main classes for representing and accessing a training set or validation set:\n\n- `Dataset`:: A collection that returns a tuple of your independent and dependent variable for a single item\n- `DataLoader`:: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables\n\nOn top of these, fastai provides two classes for bringing your training and validation sets together:\n\n- `Datasets`:: An object that contains a training `Dataset` and a validation `Dataset`\n- `DataLoaders`:: An object that contains a training `DataLoader` and a validation `DataLoader`\n\nSince a `DataLoader` builds on top of a `Dataset` and adds additional functionality to it (collating multiple items into a mini-batch), it’s often easiest to start by creating and testing `Datasets`, and then look at `DataLoaders` after that’s working.\n\nWhen we create a `DataBlock`, we build up gradually, step by step, and use the notebook to check our data along the way. This is a great way to make sure that you maintain momentum as you are coding, and that you keep an eye out for any problems. It’s easy to debug, because you know that if a problem arises, it is in the line of code you just typed!\n\nLet’s start with the simplest case, which is a data block created with no parameters:\n\n::: {#f5ec4fd1 .cell execution_count=10}\n``` {.python .cell-code}\ndblock = DataBlock()\n```\n:::\n\n\nWe can create a `Datasets` object from this. The only thing needed is a source—in this case, our DataFrame:\n\n::: {#b4d9bfd7 .cell execution_count=11}\n``` {.python .cell-code}\ndsets = dblock.datasets(df)\n```\n:::\n\n\nThis contains a `train` and a `valid` dataset, which we can index into:\n\n::: {#1dabcd48 .cell execution_count=12}\n``` {.python .cell-code}\nlen(dsets.train),len(dsets.valid)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n(4009, 1002)\n```\n:::\n:::\n\n\n::: {#1122f86e .cell execution_count=13}\n``` {.python .cell-code}\nx,y = dsets.train[0]\nx,y\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n(fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object,\n fname       008663.jpg\n labels      car person\n is_valid         False\n Name: 4346, dtype: object)\n```\n:::\n:::\n\n\nAs you can see, this simply returns a row of the DataFrame, twice. This is because by default, the data block assumes we have two things: input and target. We are going to need to grab the appropriate fields from the DataFrame, which we can do by passing `get_x` and `get_y` functions:\n\n::: {#508af334 .cell execution_count=14}\n``` {.python .cell-code}\nx['fname']\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n'008663.jpg'\n```\n:::\n:::\n\n\n::: {#77e54a95 .cell execution_count=15}\n``` {.python .cell-code}\ndblock = DataBlock(get_x = lambda r: r['fname'], get_y = lambda r: r['labels'])\ndsets = dblock.datasets(df)\ndsets.train[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n('005620.jpg', 'aeroplane')\n```\n:::\n:::\n\n\nAs you can see, rather than defining a function in the usual way, we are using Python’s `lambda` keyword. This is just a shortcut for defining and then referring to a function. The following more verbose approach is identical:\n\n::: {#6a25b5d3 .cell execution_count=16}\n``` {.python .cell-code}\ndef get_x(r): return r['fname']\ndef get_y(r): return r['labels']\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n('002549.jpg', 'tvmonitor')\n```\n:::\n:::\n\n\nLambda functions are great for quickly iterating, but they are not compatible with serialization, so we advise you to use the more verbose approach if you want to export your `Learner` after training (lambdas are fine if you are just experimenting).\n\nWe can see that the independent variable will need to be converted into a complete path, so that we can open it as an image, and the dependent variable will need to be split on the space character (which is the default for Python’s `split` function) so that it becomes a list:\n\n::: {#afdf3d88 .cell execution_count=17}\n``` {.python .cell-code}\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\ndblock = DataBlock(get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n(Path('/home/oren/.fastai/data/pascal_2007/train/002844.jpg'), ['train'])\n```\n:::\n:::\n\n\nTo actually open the image and do the conversion to tensors, we will need to use a set of transforms; block types will provide us with those. We can use the same block types that we have used previously, with one exception: the `ImageBlock` will work fine again, because we have a path that points to a valid image, but the `CategoryBlock` is not going to work. The problem is that block returns a single integer, but we need to be able to have multiple labels for each item. To solve this, we use a `MultiCategoryBlock`. This type of block expects to receive a list of strings, as we have in this case, so let’s test it out:\n\n::: {#d68531ec .cell execution_count=18}\n``` {.python .cell-code}\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   get_x = get_x, get_y = get_y)\ndsets = dblock.datasets(df)\ndsets.train[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n(PILImage mode=RGB size=500x375,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]))\n```\n:::\n:::\n\n\nAs you can see, our list of categories is not encoded in the same way that it was for the regular `CategoryBlock`. In that case, we had a single integer representing which category was present, based on its location in our vocab. In this case, however, we instead have a list of zeros, with a one in any position where that category is present. For example, if there is a one in the second and fourth positions, then that means that vocab items two and four are present in this image. This is known as *one-hot encoding*. The reason we can’t easily just use a list of category indices is that each list would be a different length, and PyTorch requires tensors, where everything has to be the same length.\n\n> jargon: One-hot encoding: Using a vector of zeros, with a one in each location that is represented in the data, to encode a list of integers.\n\nLet’s check what the categories represent for this example (we are using the convenient `torch.where` function, which tells us all of the indices where our condition is true or false):\n\n::: {#9b0c8cf4 .cell execution_count=19}\n``` {.python .cell-code}\nidxs = torch.where(dsets.train[0][1]==1.)[0]\ndsets.train.vocab[idxs]\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n(#1) ['dog']\n```\n:::\n:::\n\n\nWith NumPy arrays, PyTorch tensors, and fastai’s `L` class, we can index directly using a list or vector, which makes a lot of code (such as this example) much clearer and more concise.\n\nWe have ignored the column `is_valid` up until now, which means that `DataBlock` has been using a random split by default. To explicitly choose the elements of our validation set, we need to write a function and pass it to `splitter` (or use one of fastai's predefined functions or classes). It will take the items (here our whole DataFrame) and must return two (or more) lists of integers:\n\n::: {#5a3f1cbb .cell execution_count=20}\n``` {.python .cell-code}\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y)\n\ndsets = dblock.datasets(df)\ndsets.train[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n```\n:::\n:::\n\n\nAs we have discussed, a `DataLoader` collates the items from a `Dataset` into a mini-batch. This is a tuple of tensors, where each tensor simply stacks the items from that location in the `Dataset` item. \n\nNow that we have confirmed that the individual items look okay, there's one more step we need to ensure we can create our `DataLoaders`, which is to ensure that every item is of the same size. To do this, we can use `RandomResizedCrop`:\n\n::: {#c1a498b8 .cell execution_count=21}\n``` {.python .cell-code}\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n```\n:::\n\n\nAnd now we can display a sample of our data:\n\n::: {#97a0131d .cell execution_count=22}\n``` {.python .cell-code}\ndls.show_batch(nrows=1, ncols=3)\n```\n\n::: {.cell-output .cell-output-display}\n![](06_multicat_files/figure-html/cell-23-output-1.png){width=689 height=238}\n:::\n:::\n\n\nRemember that if anything goes wrong when you create your `DataLoaders` from your `DataBlock`, or if you want to view exactly what happens with your `DataBlock`, you can use the `summary` method we presented in the last chapter.\n\nOur data is now ready for training a model. As we will see, nothing is going to change when we create our `Learner`, but behind the scenes, the fastai library will pick a new loss function for us: binary cross-entropy.\n\n### Binary Cross-Entropy\n\nNow we'll create our `Learner`. We saw in <<chapter_mnist_basics>> that a `Learner` object contains four main things: the model, a `DataLoaders` object, an `Optimizer`, and the loss function to use. We already have our `DataLoaders`, we can leverage fastai's `resnet` models (which we'll learn how to create from scratch later), and we know how to create an `SGD` optimizer. So let's focus on ensuring we have a suitable loss function. To do this, let's use `cnn_learner` to create a `Learner`, so we can look at its activations:\n\n::: {#aea90133 .cell execution_count=23}\n``` {.python .cell-code}\nlearn = cnn_learner(dls, resnet18)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/oren/work/blog/.venv/lib/python3.10/site-packages/fastai/vision/learner.py:303: UserWarning:\n\n`cnn_learner` has been renamed to `vision_learner` -- please update your code\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/oren/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\r  0%|          | 0.00/44.7M [00:00<?, ?B/s]\r  0%|          | 128k/44.7M [00:00<01:21, 570kB/s]\r  1%|          | 256k/44.7M [00:00<01:12, 644kB/s]\r  1%|          | 384k/44.7M [00:00<01:20, 578kB/s]\r  1%|          | 512k/44.7M [00:00<01:10, 658kB/s]\r  1%|▏         | 640k/44.7M [00:01<01:09, 668kB/s]\r  2%|▏         | 768k/44.7M [00:01<01:11, 640kB/s]\r  2%|▏         | 896k/44.7M [00:01<01:15, 604kB/s]\r  2%|▏         | 1.00M/44.7M [00:01<01:15, 610kB/s]\r  3%|▎         | 1.12M/44.7M [00:01<01:19, 571kB/s]\r  3%|▎         | 1.25M/44.7M [00:02<01:15, 600kB/s]\r  3%|▎         | 1.38M/44.7M [00:02<01:18, 576kB/s]\r  3%|▎         | 1.50M/44.7M [00:02<01:17, 582kB/s]\r  4%|▎         | 1.62M/44.7M [00:02<01:20, 563kB/s]\r  4%|▍         | 1.75M/44.7M [00:03<01:34, 476kB/s]\r  4%|▍         | 1.88M/44.7M [00:03<01:34, 477kB/s]\r  4%|▍         | 2.00M/44.7M [00:03<01:25, 523kB/s]\r  5%|▍         | 2.12M/44.7M [00:03<01:18, 571kB/s]\r  5%|▌         | 2.25M/44.7M [00:04<01:14, 596kB/s]\r  5%|▌         | 2.38M/44.7M [00:04<01:05, 680kB/s]\r  6%|▌         | 2.50M/44.7M [00:04<01:04, 688kB/s]\r  6%|▌         | 2.62M/44.7M [00:04<00:59, 737kB/s]\r  6%|▌         | 2.75M/44.7M [00:04<00:55, 793kB/s]\r  6%|▋         | 2.88M/44.7M [00:04<00:52, 840kB/s]\r  7%|▋         | 3.00M/44.7M [00:04<00:48, 900kB/s]\r  7%|▋         | 3.12M/44.7M [00:05<00:47, 926kB/s]\r  7%|▋         | 3.25M/44.7M [00:05<00:43, 999kB/s]\r  8%|▊         | 3.38M/44.7M [00:05<00:41, 1.05MB/s]\r  8%|▊         | 3.50M/44.7M [00:05<00:39, 1.09MB/s]\r  8%|▊         | 3.62M/44.7M [00:05<00:40, 1.07MB/s]\r  8%|▊         | 3.75M/44.7M [00:05<00:39, 1.09MB/s]\r  9%|▊         | 3.88M/44.7M [00:05<00:39, 1.07MB/s]\r  9%|▉         | 4.00M/44.7M [00:05<00:38, 1.12MB/s]\r 10%|▉         | 4.25M/44.7M [00:06<00:33, 1.26MB/s]\r 10%|▉         | 4.38M/44.7M [00:06<00:36, 1.15MB/s]\r 10%|█         | 4.62M/44.7M [00:06<00:32, 1.31MB/s]\r 11%|█         | 4.88M/44.7M [00:06<00:30, 1.36MB/s]\r 11%|█▏        | 5.12M/44.7M [00:06<00:29, 1.40MB/s]\r 12%|█▏        | 5.38M/44.7M [00:06<00:29, 1.42MB/s]\r 13%|█▎        | 5.62M/44.7M [00:07<00:27, 1.49MB/s]\r 13%|█▎        | 5.88M/44.7M [00:07<00:27, 1.46MB/s]\r 14%|█▎        | 6.12M/44.7M [00:07<00:29, 1.36MB/s]\r 14%|█▍        | 6.38M/44.7M [00:07<00:28, 1.40MB/s]\r 15%|█▍        | 6.62M/44.7M [00:07<00:27, 1.46MB/s]\r 15%|█▌        | 6.88M/44.7M [00:07<00:26, 1.49MB/s]\r 16%|█▌        | 7.12M/44.7M [00:08<00:26, 1.50MB/s]\r 17%|█▋        | 7.38M/44.7M [00:08<00:26, 1.46MB/s]\r 17%|█▋        | 7.62M/44.7M [00:08<00:25, 1.53MB/s]\r 18%|█▊        | 7.88M/44.7M [00:08<00:24, 1.60MB/s]\r 18%|█▊        | 8.12M/44.7M [00:08<00:24, 1.58MB/s]\r 19%|█▉        | 8.38M/44.7M [00:09<00:27, 1.39MB/s]\r 19%|█▉        | 8.62M/44.7M [00:09<00:26, 1.45MB/s]\r 20%|█▉        | 8.88M/44.7M [00:09<00:28, 1.33MB/s]\r 20%|██        | 9.12M/44.7M [00:09<00:24, 1.53MB/s]\r 21%|██        | 9.38M/44.7M [00:09<00:27, 1.37MB/s]\r 22%|██▏       | 9.62M/44.7M [00:09<00:23, 1.58MB/s]\r 22%|██▏       | 9.88M/44.7M [00:10<00:21, 1.66MB/s]\r 23%|██▎       | 10.1M/44.7M [00:10<00:21, 1.70MB/s]\r 23%|██▎       | 10.4M/44.7M [00:10<00:20, 1.75MB/s]\r 24%|██▍       | 10.6M/44.7M [00:10<00:23, 1.54MB/s]\r 24%|██▍       | 10.9M/44.7M [00:10<00:23, 1.52MB/s]\r 25%|██▍       | 11.1M/44.7M [00:10<00:21, 1.62MB/s]\r 25%|██▌       | 11.4M/44.7M [00:10<00:20, 1.73MB/s]\r 26%|██▌       | 11.6M/44.7M [00:11<00:21, 1.62MB/s]\r 27%|██▋       | 11.9M/44.7M [00:11<00:21, 1.60MB/s]\r 27%|██▋       | 12.1M/44.7M [00:11<00:21, 1.62MB/s]\r 28%|██▊       | 12.4M/44.7M [00:11<00:21, 1.54MB/s]\r 28%|██▊       | 12.6M/44.7M [00:11<00:20, 1.61MB/s]\r 29%|██▉       | 12.9M/44.7M [00:11<00:19, 1.68MB/s]\r 29%|██▉       | 13.1M/44.7M [00:12<00:21, 1.53MB/s]\r 30%|██▉       | 13.4M/44.7M [00:12<00:20, 1.58MB/s]\r 31%|███       | 13.6M/44.7M [00:12<00:19, 1.64MB/s]\r 31%|███       | 13.9M/44.7M [00:12<00:18, 1.76MB/s]\r 32%|███▏      | 14.1M/44.7M [00:12<00:21, 1.50MB/s]\r 32%|███▏      | 14.4M/44.7M [00:12<00:19, 1.65MB/s]\r 33%|███▎      | 14.6M/44.7M [00:13<00:18, 1.69MB/s]\r 33%|███▎      | 14.9M/44.7M [00:13<00:19, 1.58MB/s]\r 34%|███▍      | 15.1M/44.7M [00:13<00:17, 1.76MB/s]\r 34%|███▍      | 15.4M/44.7M [00:13<00:17, 1.73MB/s]\r 35%|███▍      | 15.6M/44.7M [00:13<00:16, 1.81MB/s]\r 36%|███▌      | 15.9M/44.7M [00:13<00:17, 1.72MB/s]\r 36%|███▌      | 16.1M/44.7M [00:14<00:18, 1.58MB/s]\r 37%|███▋      | 16.4M/44.7M [00:14<00:18, 1.60MB/s]\r 37%|███▋      | 16.6M/44.7M [00:14<00:17, 1.68MB/s]\r 38%|███▊      | 16.9M/44.7M [00:14<00:16, 1.73MB/s]\r 38%|███▊      | 17.1M/44.7M [00:14<00:18, 1.56MB/s]\r 39%|███▉      | 17.4M/44.7M [00:14<00:20, 1.38MB/s]\r 40%|███▉      | 17.8M/44.7M [00:15<00:15, 1.81MB/s]\r 40%|████      | 18.0M/44.7M [00:15<00:16, 1.65MB/s]\r 41%|████      | 18.4M/44.7M [00:15<00:14, 1.92MB/s]\r 42%|████▏     | 18.6M/44.7M [00:15<00:14, 1.87MB/s]\r 43%|████▎     | 19.0M/44.7M [00:15<00:15, 1.78MB/s]\r 43%|████▎     | 19.2M/44.7M [00:15<00:14, 1.80MB/s]\r 44%|████▎     | 19.5M/44.7M [00:16<00:15, 1.69MB/s]\r 44%|████▍     | 19.8M/44.7M [00:16<00:14, 1.77MB/s]\r 45%|████▍     | 20.0M/44.7M [00:16<00:13, 1.86MB/s]\r 45%|████▌     | 20.2M/44.7M [00:16<00:13, 1.94MB/s]\r 46%|████▌     | 20.5M/44.7M [00:16<00:12, 1.95MB/s]\r 46%|████▋     | 20.8M/44.7M [00:16<00:12, 2.00MB/s]\r 47%|████▋     | 21.0M/44.7M [00:16<00:12, 2.00MB/s]\r 48%|████▊     | 21.2M/44.7M [00:17<00:20, 1.22MB/s]\r 48%|████▊     | 21.5M/44.7M [00:17<00:18, 1.29MB/s]\r 49%|████▊     | 21.8M/44.7M [00:17<00:18, 1.29MB/s]\r 49%|████▉     | 22.0M/44.7M [00:17<00:16, 1.43MB/s]\r 50%|████▉     | 22.2M/44.7M [00:17<00:15, 1.53MB/s]\r 50%|█████     | 22.5M/44.7M [00:18<00:16, 1.39MB/s]\r 51%|█████     | 22.9M/44.7M [00:18<00:12, 1.81MB/s]\r 52%|█████▏    | 23.1M/44.7M [00:18<00:12, 1.81MB/s]\r 52%|█████▏    | 23.4M/44.7M [00:18<00:11, 1.95MB/s]\r 53%|█████▎    | 23.6M/44.7M [00:18<00:12, 1.80MB/s]\r 53%|█████▎    | 23.9M/44.7M [00:18<00:11, 1.94MB/s]\r 54%|█████▍    | 24.1M/44.7M [00:18<00:11, 1.84MB/s]\r 55%|█████▍    | 24.5M/44.7M [00:19<00:10, 2.10MB/s]\r 55%|█████▌    | 24.8M/44.7M [00:19<00:10, 2.07MB/s]\r 56%|█████▋    | 25.1M/44.7M [00:19<00:08, 2.30MB/s]\r 57%|█████▋    | 25.4M/44.7M [00:19<00:08, 2.37MB/s]\r 57%|█████▋    | 25.6M/44.7M [00:19<00:08, 2.29MB/s]\r 58%|█████▊    | 25.9M/44.7M [00:19<00:09, 2.18MB/s]\r 58%|█████▊    | 26.1M/44.7M [00:19<00:08, 2.27MB/s]\r 59%|█████▉    | 26.5M/44.7M [00:19<00:07, 2.53MB/s]\r 60%|██████    | 26.9M/44.7M [00:20<00:06, 2.76MB/s]\r 61%|██████    | 27.2M/44.7M [00:20<00:06, 2.86MB/s]\r 62%|██████▏   | 27.6M/44.7M [00:20<00:05, 3.01MB/s]\r 63%|██████▎   | 28.0M/44.7M [00:20<00:05, 3.21MB/s]\r 64%|██████▎   | 28.4M/44.7M [00:20<00:05, 3.15MB/s]\r 64%|██████▍   | 28.8M/44.7M [00:20<00:05, 2.93MB/s]\r 65%|██████▌   | 29.1M/44.7M [00:20<00:05, 2.78MB/s]\r 66%|██████▌   | 29.5M/44.7M [00:21<00:06, 2.65MB/s]\r 67%|██████▋   | 29.9M/44.7M [00:21<00:06, 2.30MB/s]\r 68%|██████▊   | 30.2M/44.7M [00:21<00:06, 2.49MB/s]\r 69%|██████▊   | 30.6M/44.7M [00:21<00:05, 2.69MB/s]\r 69%|██████▉   | 31.0M/44.7M [00:21<00:05, 2.70MB/s]\r 70%|███████   | 31.4M/44.7M [00:21<00:04, 2.84MB/s]\r 71%|███████   | 31.8M/44.7M [00:22<00:06, 1.97MB/s]\r 73%|███████▎  | 32.6M/44.7M [00:22<00:03, 3.22MB/s]\r 74%|███████▍  | 33.1M/44.7M [00:22<00:04, 2.83MB/s]\r 75%|███████▌  | 33.6M/44.7M [00:22<00:03, 3.13MB/s]\r 76%|███████▌  | 34.0M/44.7M [00:22<00:03, 3.10MB/s]\r 77%|███████▋  | 34.4M/44.7M [00:22<00:03, 3.11MB/s]\r 78%|███████▊  | 34.8M/44.7M [00:22<00:03, 3.11MB/s]\r 79%|███████▊  | 35.1M/44.7M [00:23<00:03, 2.88MB/s]\r 79%|███████▉  | 35.5M/44.7M [00:23<00:03, 2.87MB/s]\r 80%|████████  | 35.9M/44.7M [00:23<00:03, 2.45MB/s]\r 82%|████████▏ | 36.5M/44.7M [00:23<00:03, 2.66MB/s]\r 83%|████████▎ | 37.1M/44.7M [00:23<00:02, 3.23MB/s]\r 84%|████████▍ | 37.6M/44.7M [00:23<00:02, 3.48MB/s]\r 85%|████████▌ | 38.1M/44.7M [00:24<00:01, 3.58MB/s]\r 86%|████████▌ | 38.5M/44.7M [00:24<00:01, 3.56MB/s]\r 88%|████████▊ | 39.1M/44.7M [00:24<00:01, 3.99MB/s]\r 89%|████████▊ | 39.6M/44.7M [00:24<00:01, 4.18MB/s]\r 90%|█████████ | 40.2M/44.7M [00:24<00:01, 4.51MB/s]\r 91%|█████████ | 40.8M/44.7M [00:24<00:00, 4.45MB/s]\r 92%|█████████▏| 41.2M/44.7M [00:24<00:00, 3.59MB/s]\r 93%|█████████▎| 41.6M/44.7M [00:25<00:01, 2.92MB/s]\r 94%|█████████▍| 42.0M/44.7M [00:25<00:00, 2.89MB/s]\r 95%|█████████▍| 42.4M/44.7M [00:25<00:00, 2.58MB/s]\r 96%|█████████▌| 42.8M/44.7M [00:25<00:00, 2.34MB/s]\r 96%|█████████▋| 43.0M/44.7M [00:25<00:00, 2.25MB/s]\r 97%|█████████▋| 43.2M/44.7M [00:25<00:00, 2.12MB/s]\r 97%|█████████▋| 43.5M/44.7M [00:26<00:00, 2.19MB/s]\r 98%|█████████▊| 43.9M/44.7M [00:26<00:00, 2.09MB/s]\r 99%|█████████▉| 44.2M/44.7M [00:26<00:00, 2.21MB/s]\r100%|█████████▉| 44.6M/44.7M [00:26<00:00, 2.56MB/s]\r100%|██████████| 44.7M/44.7M [00:26<00:00, 1.76MB/s]\n```\n:::\n:::\n\n\nWe also saw that the model in a `Learner` is generally an object of a class inheriting from `nn.Module`, and that we can call it using parentheses and it will return the activations of a model. You should pass it your independent variable, as a mini-batch. We can try it out by grabbing a mini batch from our `DataLoader` and then passing it to the model:\n\n::: {#a61f1a4e .cell execution_count=24}\n``` {.python .cell-code}\nx,y = to_cpu(dls.train.one_batch())\nactivs = learn.model(x)\nactivs.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\ntorch.Size([64, 20])\n```\n:::\n:::\n\n\nThink about why `activs` has this shape—we have a batch size of 64, and we need to calculate the probability of each of 20 categories. Here’s what one of those activations looks like:\n\n::: {#6d92db8a .cell execution_count=25}\n``` {.python .cell-code}\nactivs[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\nTensorImage([ 0.5674, -1.2013,  4.5409, -1.5284, -0.6600,  0.0999, -2.4757, -0.8773, -0.2934, -1.4746, -0.1738,  2.1763, -3.4473, -1.1407,  0.1783, -1.6922, -2.3396,  0.7602, -1.4213, -0.4334],\n            grad_fn=<AliasBackward0>)\n```\n:::\n:::\n\n\n> note: Getting Model Activations: Knowing how to manually get a mini-batch and pass it into a model, and look at the activations and loss, is really important for debugging your model. It is also very helpful for learning, so that you can see exactly what is going on.\n\nThey aren’t yet scaled to between 0 and 1, but we learned how to do that in <<chapter_mnist_basics>>, using the `sigmoid` function. We also saw how to calculate a loss based on this—this is our loss function from <<chapter_mnist_basics>>, with the addition of `log` as discussed in the last chapter:\n\n::: {#91335256 .cell execution_count=26}\n``` {.python .cell-code}\ndef binary_cross_entropy(inputs, targets):\n    inputs = inputs.sigmoid()\n    return -torch.where(targets==1, inputs, 1-inputs).log().mean()\n```\n:::\n\n\nNote that because we have a one-hot-encoded dependent variable, we can't directly use `nll_loss` or `softmax` (and therefore we can't use `cross_entropy`):\n\n- `softmax`, as we saw, requires that all predictions sum to 1, and tends to push one activation to be much larger than the others (due to the use of `exp`); however, we may well have multiple objects that we're confident appear in an image, so restricting the maximum sum of activations to 1 is not a good idea. By the same reasoning, we may want the sum to be *less* than 1, if we don't think *any* of the categories appear in an image.\n- `nll_loss`, as we saw, returns the value of just one activation: the single activation corresponding with the single label for an item. This doesn't make sense when we have multiple labels.\n\nOn the other hand, the `binary_cross_entropy` function, which is just `mnist_loss` along with `log`, provides just what we need, thanks to the magic of PyTorch's elementwise operations. Each activation will be compared to each target for each column, so we don't have to do anything to make this function work for multiple columns.\n\n> j: One of the things I really like about working with libraries like PyTorch, with broadcasting and elementwise operations, is that quite frequently I find I can write code that works equally well for a single item or a batch of items, without changes. `binary_cross_entropy` is a great example of this. By using these operations, we don't have to write loops ourselves, and can rely on PyTorch to do the looping we need as appropriate for the rank of the tensors we're working with.\n\nPyTorch already provides this function for us. In fact, it provides a number of versions, with rather confusing names!\n\n`F.binary_cross_entropy` and its module equivalent `nn.BCELoss` calculate cross-entropy on a one-hot-encoded target, but do not include the initial `sigmoid`. Normally for one-hot-encoded targets you'll want `F.binary_cross_entropy_with_logits` (or `nn.BCEWithLogitsLoss`), which do both sigmoid and binary cross-entropy in a single function, as in the preceding example.\n\nThe equivalent for single-label datasets (like MNIST or the Pet dataset), where the target is encoded as a single integer, is `F.nll_loss` or `nn.NLLLoss` for the version without the initial softmax, and `F.cross_entropy` or `nn.CrossEntropyLoss` for the version with the initial softmax.\n\nSince we have a one-hot-encoded target, we will use `BCEWithLogitsLoss`:\n\n::: {#2ee54dd4 .cell execution_count=27}\n``` {.python .cell-code}\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\nloss\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[27], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> loss_func <span style=\"color:rgb(98,98,98)\">=</span> nn<span style=\"color:rgb(98,98,98)\">.</span>BCEWithLogitsLoss()\n<span class=\"ansi-green-fg\">----&gt; 2</span> loss <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">loss_func</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">activs</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">y</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> loss\n\nFile <span class=\"ansi-green-fg\">~/work/blog/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532</span>, in <span class=\"ansi-cyan-fg\">Module._wrapped_call_impl</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1530</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_compiled_call_impl(<span style=\"color:rgb(98,98,98)\">*</span>args, <span style=\"color:rgb(98,98,98)\">*</span><span style=\"color:rgb(98,98,98)\">*</span>kwargs)  <span style=\"font-style:italic;color:rgb(95,135,135)\"># type: ignore[misc]</span>\n<span class=\"ansi-green-fg ansi-bold\">   1531</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg\">-&gt; 1532</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_call_impl</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">~/work/blog/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541</span>, in <span class=\"ansi-cyan-fg\">Module._call_impl</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1536</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># If we don't have any hooks, we want to skip the rest of the logic in</span>\n<span class=\"ansi-green-fg ansi-bold\">   1537</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># this function, and just call forward.</span>\n<span class=\"ansi-green-fg ansi-bold\">   1538</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> (<span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_backward_hooks <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_backward_pre_hooks <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_forward_hooks <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_forward_pre_hooks\n<span class=\"ansi-green-fg ansi-bold\">   1539</span>         <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> _global_backward_pre_hooks <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> _global_backward_hooks\n<span class=\"ansi-green-fg ansi-bold\">   1540</span>         <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> _global_forward_hooks <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> _global_forward_pre_hooks):\n<span class=\"ansi-green-fg\">-&gt; 1541</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">forward_call</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1543</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">try</span>:\n<span class=\"ansi-green-fg ansi-bold\">   1544</span>     result <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\nFile <span class=\"ansi-green-fg\">~/work/blog/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:731</span>, in <span class=\"ansi-cyan-fg\">BCEWithLogitsLoss.forward</span><span class=\"ansi-blue-fg\">(self, input, target)</span>\n<span class=\"ansi-green-fg ansi-bold\">    730</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span> <span style=\"color:rgb(0,0,255)\">forward</span>(<span style=\"color:rgb(0,135,0)\">self</span>, <span style=\"color:rgb(0,135,0)\">input</span>: Tensor, target: Tensor) <span style=\"color:rgb(98,98,98)\">-</span><span style=\"color:rgb(98,98,98)\">&gt;</span> Tensor:\n<span class=\"ansi-green-fg\">--&gt; 731</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">F</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">binary_cross_entropy_with_logits</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">input</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">target</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    732</span> <span class=\"ansi-yellow-bg\">                                              </span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">weight</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    733</span> <span class=\"ansi-yellow-bg\">                                              </span><span class=\"ansi-yellow-bg\">pos_weight</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">pos_weight</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    734</span> <span class=\"ansi-yellow-bg\">                                              </span><span class=\"ansi-yellow-bg\">reduction</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">reduction</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">~/work/blog/.venv/lib/python3.10/site-packages/torch/nn/functional.py:3207</span>, in <span class=\"ansi-cyan-fg\">binary_cross_entropy_with_logits</span><span class=\"ansi-blue-fg\">(input, target, weight, size_average, reduce, reduction, pos_weight)</span>\n<span class=\"ansi-green-fg ansi-bold\">   3166</span> <span style=\"color:rgb(175,0,0)\">r</span><span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Calculate Binary Cross Entropy between target and input logits.</span>\n<span class=\"ansi-green-fg ansi-bold\">   3167</span> \n<span class=\"ansi-green-fg ansi-bold\">   3168</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">See :class:`~torch.nn.BCEWithLogitsLoss` for details.</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">   3204</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">     &gt;&gt;&gt; loss.backward()</span>\n<span class=\"ansi-green-fg ansi-bold\">   3205</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"</span>\n<span class=\"ansi-green-fg ansi-bold\">   3206</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> has_torch_function_variadic(<span style=\"color:rgb(0,135,0)\">input</span>, target, weight, pos_weight):\n<span class=\"ansi-green-fg\">-&gt; 3207</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">handle_torch_function</span><span class=\"ansi-yellow-bg\">(</span>\n<span class=\"ansi-green-fg ansi-bold\">   3208</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">binary_cross_entropy_with_logits</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   3209</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">input</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">target</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">weight</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">pos_weight</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   3210</span> <span class=\"ansi-yellow-bg\">        </span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">input</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   3211</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">target</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   3212</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">weight</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">weight</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   3213</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">size_average</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">size_average</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   3214</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">reduce</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">reduce</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   3215</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">reduction</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">reduction</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   3216</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">pos_weight</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">pos_weight</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   3217</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   3218</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> size_average <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> reduce <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>:\n<span class=\"ansi-green-fg ansi-bold\">   3219</span>     reduction_enum <span style=\"color:rgb(98,98,98)\">=</span> _Reduction<span style=\"color:rgb(98,98,98)\">.</span>legacy_get_enum(size_average, reduce)\n\nFile <span class=\"ansi-green-fg\">~/work/blog/.venv/lib/python3.10/site-packages/torch/overrides.py:1648</span>, in <span class=\"ansi-cyan-fg\">handle_torch_function</span><span class=\"ansi-blue-fg\">(public_api, relevant_args, *args, **kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1646</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> _is_torch_function_mode_enabled():\n<span class=\"ansi-green-fg ansi-bold\">   1647</span>     msg <span style=\"color:rgb(98,98,98)\">+</span><span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\"> nor in mode </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>_get_current_function_mode()<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\">\"</span>\n<span class=\"ansi-green-fg\">-&gt; 1648</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">TypeError</span>(msg)\n\n<span class=\"ansi-red-fg\">TypeError</span>: no implementation found for 'torch.nn.functional.binary_cross_entropy_with_logits' on types that implement __torch_function__: [&lt;class 'fastai.torch_core.TensorImage'&gt;, &lt;class 'fastai.torch_core.TensorMultiCategory'&gt;]</pre>\n```\n:::\n\n:::\n:::\n\n\nWe don't actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the `DataLoaders` has multiple category labels, so it will use `nn.BCEWithLogitsLoss` by default.\n\nOne change compared to the last chapter is the metric we use: because this is a multilabel problem, we can't use the accuracy function. Why is that? Well, accuracy was comparing our outputs to our targets like so:\n\n```python\ndef accuracy(inp, targ, axis=-1):\n    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n    pred = inp.argmax(dim=axis)\n    return (pred == targ).float().mean()\n```\n\nThe class predicted was the one with the highest activation (this is what `argmax` does). Here it doesn't work because we could have more than one prediction on a single image. After applying the sigmoid to our activations (to make them between 0 and 1), we need to decide which ones are 0s and which ones are 1s by picking a *threshold*. Each value above the threshold will be considered as a 1, and each value lower than the threshold will be considered a 0:\n\n```python\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()\n```\n\nIf we pass `accuracy_multi` directly as a metric, it will use the default value for `threshold`, which is 0.5. We might want to adjust that default and create a new version of `accuracy_multi` that has a different default. To help with this, there is a function in Python called `partial`. It allows us to *bind* a function with some arguments or keyword arguments, making a new version of that function that, whenever it is called, always includes those arguments. For instance, here is a simple function taking two arguments:\n\n::: {#de3e049c .cell execution_count=28}\n``` {.python .cell-code}\ndef say_hello(name, say_what=\"Hello\"): return f\"{say_what} {name}.\"\nsay_hello('Jeremy'),say_hello('Jeremy', 'Ahoy!')\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n('Hello Jeremy.', 'Ahoy! Jeremy.')\n```\n:::\n:::\n\n\nWe can switch to a French version of that function by using `partial`:\n\n::: {#ea9a1d22 .cell execution_count=29}\n``` {.python .cell-code}\nf = partial(say_hello, say_what=\"Bonjour\")\nf(\"Jeremy\"),f(\"Sylvain\")\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n('Bonjour Jeremy.', 'Bonjour Sylvain.')\n```\n:::\n:::\n\n\nWe can now train our model. Let's try setting the accuracy threshold to 0.2 for our metric:\n\n::: {#e3f6d2ac .cell execution_count=30}\n``` {.python .cell-code}\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy_multi</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.956891</td>\n      <td>0.712695</td>\n      <td>0.224263</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.831191</td>\n      <td>0.542630</td>\n      <td>0.285458</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.602212</td>\n      <td>0.192332</td>\n      <td>0.831315</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.355315</td>\n      <td>0.114507</td>\n      <td>0.946374</td>\n      <td>00:08</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy_multi</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.128022</td>\n      <td>0.102201</td>\n      <td>0.953247</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.114871</td>\n      <td>0.095215</td>\n      <td>0.957769</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.098530</td>\n      <td>0.092200</td>\n      <td>0.957928</td>\n      <td>00:09</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nPicking a threshold is important. If you pick a threshold that's too low, you'll often be failing to select correctly labeled objects. We can see this by changing our metric, and then calling `validate`, which returns the validation loss and metrics:\n\n::: {#df41658d .cell execution_count=31}\n``` {.python .cell-code}\nlearn.metrics = partial(accuracy_multi, thresh=0.1)\nlearn.validate()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n(#2) [0.09220042824745178,0.9372310638427734]\n```\n:::\n:::\n\n\nIf you pick a threshold that's too high, you'll only be selecting the objects for which your model is very confident:\n\n::: {#35e534c3 .cell execution_count=32}\n``` {.python .cell-code}\nlearn.metrics = partial(accuracy_multi, thresh=0.99)\nlearn.validate()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n(#2) [0.09220042824745178,0.9422311782836914]\n```\n:::\n:::\n\n\nWe can find the best threshold by trying a few levels and seeing what works best. This is much faster if we just grab the predictions once:\n\n::: {#365653c4 .cell execution_count=33}\n``` {.python .cell-code}\npreds,targs = learn.get_preds()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n:::\n\n\nThen we can call the metric directly. Note that by default `get_preds` applies the output activation function (sigmoid, in this case) for us, so we'll need to tell `accuracy_multi` to not apply it:\n\n::: {#dac411f2 .cell execution_count=34}\n``` {.python .cell-code}\naccuracy_multi(preds, targs, thresh=0.9, sigmoid=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\nTensorBase(0.9587)\n```\n:::\n:::\n\n\nWe can now use this approach to find the best threshold level:\n\n::: {#dda6888a .cell execution_count=35}\n``` {.python .cell-code}\nxs = torch.linspace(0.05,0.95,29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs,accs);\n```\n\n::: {.cell-output .cell-output-display}\n![](06_multicat_files/figure-html/cell-36-output-1.png){width=585 height=414}\n:::\n:::\n\n\nIn this case, we're using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. Sometimes students have expressed their concern that we might be *overfitting* to the validation set, since we're trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we're clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don't try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it's fine to do this).\n\nThis concludes the part of this chapter dedicated to multi-label classification. Next, we'll take a look at a regression problem.\n\n## Regression\n\nIt's easy to think of deep learning models as being classified into domains, like *computer vision*, *NLP*, and so forth. And indeed, that's how fastai classifies its applications—largely because that's how most people are used to thinking of things.\n\nBut really, that's hiding a more interesting and deeper perspective. A model is defined by its independent and dependent variables, along with its loss function. That means that there's really a far wider array of models than just the simple domain-based split. Perhaps we have an independent variable that's an image, and a dependent that's text (e.g., generating a caption from an image); or perhaps we have an independent variable that's text and dependent that's an image (e.g., generating an image from a caption—which is actually possible for deep learning to do!); or perhaps we've got images, texts, and tabular data as independent variables, and we're trying to predict product purchases... the possibilities really are endless.\n\nTo be able to move beyond fixed applications, to crafting your own novel solutions to novel problems, it helps to really understand the data block API (and maybe also the mid-tier API, which we'll see later in the book). As an example, let's consider the problem of *image regression*. This refers to learning from a dataset where the independent variable is an image, and the dependent variable is one or more floats. Often we see people treat image regression as a whole separate application—but as you'll see here, we can treat it as just another CNN on top of the data block API.\n\nWe're going to jump straight to a somewhat tricky variant of image regression, because we know you're ready for it! We're going to do a key point model. A *key point* refers to a specific location represented in an image—in this case, we'll use images of people and we'll be looking for the center of the person's face in each image. That means we'll actually be predicting *two* values for each image: the row and column of the face center. \n\n### Assemble the Data\n\nWe will use the [Biwi Kinect Head Pose dataset](https://icu.ee.ethz.ch/research/datsets.html) for this section. We'll begin by downloading the dataset as usual:\n\n::: {#11ee318d .cell execution_count=36}\n``` {.python .cell-code}\npath = untar_data(URLs.BIWI_HEAD_POSE)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n    <div>\n      <progress value='452321280' class='' max='452316199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [452321280/452316199 03:18&lt;00:00]\n    </div>\n    \n```\n:::\n:::\n\n\n::: {#fe0bea68 .cell execution_count=37}\n``` {.python .cell-code}\n#hide\nPath.BASE_PATH = path\n```\n:::\n\n\nLet's see what we've got!\n\n::: {#c41ada1e .cell execution_count=38}\n``` {.python .cell-code}\npath.ls().sorted()\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\n(#50) [Path('01'),Path('01.obj'),Path('02'),Path('02.obj'),Path('03'),Path('03.obj'),Path('04'),Path('04.obj'),Path('05'),Path('05.obj'),Path('06'),Path('06.obj'),Path('07'),Path('07.obj'),Path('08'),Path('08.obj'),Path('09'),Path('09.obj'),Path('10'),Path('10.obj')...]\n```\n:::\n:::\n\n\nThere are 24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding *.obj* file for each (we won't need them here). Let's take a look inside one of these directories:\n\n::: {#cdbdb750 .cell execution_count=39}\n``` {.python .cell-code}\n(path/'01').ls().sorted()\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\n(#1000) [Path('01/depth.cal'),Path('01/frame_00003_pose.txt'),Path('01/frame_00003_rgb.jpg'),Path('01/frame_00004_pose.txt'),Path('01/frame_00004_rgb.jpg'),Path('01/frame_00005_pose.txt'),Path('01/frame_00005_rgb.jpg'),Path('01/frame_00006_pose.txt'),Path('01/frame_00006_rgb.jpg'),Path('01/frame_00007_pose.txt'),Path('01/frame_00007_rgb.jpg'),Path('01/frame_00008_pose.txt'),Path('01/frame_00008_rgb.jpg'),Path('01/frame_00009_pose.txt'),Path('01/frame_00009_rgb.jpg'),Path('01/frame_00010_pose.txt'),Path('01/frame_00010_rgb.jpg'),Path('01/frame_00011_pose.txt'),Path('01/frame_00011_rgb.jpg'),Path('01/frame_00012_pose.txt')...]\n```\n:::\n:::\n\n\nInside the subdirectories, we have different frames, each of them come with an image (*\\_rgb.jpg*) and a pose file (*\\_pose.txt*). We can easily get all the image files recursively with `get_image_files`, then write a function that converts an image filename to its associated pose file:\n\n::: {#096bffde .cell execution_count=40}\n``` {.python .cell-code}\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\nPath('06/frame_00233_pose.txt')\n```\n:::\n:::\n\n\nLet's take a look at our first image:\n\n::: {#97718650 .cell execution_count=41}\n``` {.python .cell-code}\nim = PILImage.create(img_files[0])\nim.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\n(480, 640)\n```\n:::\n:::\n\n\n::: {#f8aefbfb .cell execution_count=42}\n``` {.python .cell-code}\nim.to_thumb(160)\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n![](06_multicat_files/figure-html/cell-43-output-1.png){}\n:::\n:::\n\n\nThe Biwi dataset website used to explain the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren't important for our purposes, so we'll just show the function we use to extract the head center point:\n\n::: {#d948b360 .cell execution_count=43}\n``` {.python .cell-code}\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n```\n:::\n\n\nThis function returns the coordinates as a tensor of two items:\n\n::: {#e206d908 .cell execution_count=44}\n``` {.python .cell-code}\nget_ctr(img_files[0])\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\ntensor([336.7871, 296.3146])\n```\n:::\n:::\n\n\nWe can pass this function to `DataBlock` as `get_y`, since it is responsible for labeling each item. We'll resize the images to half their input size, just to speed up training a bit.\n\nOne important point to note is that we should not just use a random splitter. The reason for this is that the same people appears in multiple images in this dataset, but we want to ensure that our model can generalize to people that it hasn't seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person's images.\n\nThe only other difference tfrom the previous data block examples is that the second block is a `PointBlock`. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images:\n\n::: {#8b68ffab .cell execution_count=45}\n``` {.python .cell-code}\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=[*aug_transforms(size=(240,320)), \n                Normalize.from_stats(*imagenet_stats)]\n)\n```\n:::\n\n\n> important: Points and Data Augmentation: We're not aware of other libraries (except for fastai) that automatically and correctly apply data augmentation to coordinates. So, if you're working with another library, you may need to disable data augmentation for these kinds of problems.\n\nBefore doing any modeling, we should look at our data to confirm it seems okay:\n\n::: {#26902d5c .cell execution_count=46}\n``` {.python .cell-code}\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\n```\n\n::: {.cell-output .cell-output-display}\n![](06_multicat_files/figure-html/cell-47-output-1.png){width=613 height=463}\n:::\n:::\n\n\nThat's looking good! As well as looking at the batch visually, it's a good idea to also look at the underlying tensors (especially as a student; it will help clarify your understanding of what your model is really seeing):\n\n::: {#30d63582 .cell execution_count=47}\n``` {.python .cell-code}\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\n(torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))\n```\n:::\n:::\n\n\nMake sure that you understand *why* these are the shapes for our mini-batches.\n\nHere's an example of one row from the dependent variable:\n\n::: {#919a5c4e .cell execution_count=48}\n``` {.python .cell-code}\nyb[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\nTensorPoint([[-0.0807,  0.0236]], device='cuda:0')\n```\n:::\n:::\n\n\nAs you can see, we haven't had to use a separate *image regression* application; all we've had to do is label the data, and tell fastai what kinds of data the independent and dependent variables represent.\n\nIt's the same for creating our `Learner`. We will use the same function as before, with one new parameter, and we will be ready to train our model.\n\n### Training a Model\n\nAs usual, we can use `cnn_learner` to create our `Learner`. Remember way back in <<chapter_intro>> how we used `y_range` to tell fastai the range of our targets? We'll do the same here (coordinates in fastai and PyTorch are always rescaled between -1 and +1):\n\n::: {#8cc4879c .cell execution_count=49}\n``` {.python .cell-code}\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\n```\n:::\n\n\n`y_range` is implemented in fastai using `sigmoid_range`, which is defined as:\n\n::: {#83071fb3 .cell execution_count=50}\n``` {.python .cell-code}\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\n```\n:::\n\n\nThis is set as the final layer of the model, if `y_range` is defined. Take a moment to think about what this function does, and why it forces the model to output activations in the range `(lo,hi)`.\n\nHere's what it looks like:\n\n::: {#714b4823 .cell execution_count=51}\n``` {.python .cell-code}\nplot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)\n```\n\n::: {.cell-output .cell-output-display}\n![](06_multicat_files/figure-html/cell-52-output-1.png){width=524 height=340}\n:::\n:::\n\n\nWe didn't specify a loss function, which means we're getting whatever fastai chooses as the default. Let's see what it picked for us:\n\n::: {#1754ec63 .cell execution_count=52}\n``` {.python .cell-code}\ndls.loss_func\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```\nFlattenedLoss of MSELoss()\n```\n:::\n:::\n\n\nThis makes sense, since when coordinates are used as the dependent variable, most of the time we're likely to be trying to predict something as close as possible; that's basically what `MSELoss` (mean squared error loss) does. If you want to use a different loss function, you can pass it to `cnn_learner` using the `loss_func` parameter.\n\nNote also that we didn't specify any metrics. That's because the MSE is already a useful metric for this task (although it's probably more interpretable after we take the square root). \n\nWe can pick a good learning rate with the learning rate finder:\n\n::: {#1c45d2ed .cell execution_count=53}\n``` {.python .cell-code}\nlearn.lr_find()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\nSuggestedLRs(valley=0.001737800776027143)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](06_multicat_files/figure-html/cell-54-output-4.png){width=616 height=435}\n:::\n:::\n\n\nWe'll try an LR of 2e-2:\n\n::: {#e84777a8 .cell execution_count=54}\n``` {.python .cell-code}\nlr = 1e-2\nlearn.fine_tune(3, lr)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.048314</td>\n      <td>0.017151</td>\n      <td>00:40</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.007288</td>\n      <td>0.003974</td>\n      <td>00:51</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.003084</td>\n      <td>0.002280</td>\n      <td>00:55</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.001432</td>\n      <td>0.000088</td>\n      <td>00:56</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nGenerally when we run this we get a loss of around 0.0001, which corresponds to an average coordinate prediction error of:\n\n::: {#eb5b1754 .cell execution_count=55}\n``` {.python .cell-code}\nmath.sqrt(0.0001)\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```\n0.01\n```\n:::\n:::\n\n\nThis sounds very accurate! But it's important to take a look at our results with `Learner.show_results`. The left side are the actual (*ground truth*) coordinates and the right side are our model's predictions:\n\n::: {#95309970 .cell execution_count=56}\n``` {.python .cell-code}\nlearn.show_results(ds_idx=1, nrows=3, figsize=(6,8))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](06_multicat_files/figure-html/cell-57-output-3.png){width=466 height=676}\n:::\n:::\n\n\nIt's quite amazing that with just a few minutes of computation we've created such an accurate key points model, and without any special domain-specific application. This is the power of building on flexible APIs, and using transfer learning! It's particularly striking that we've been able to use transfer learning so effectively even between totally different tasks; our pretrained model was trained to do image classification, and we fine-tuned for image regression.\n\n## Conclusion\n\nIn problems that are at first glance completely different (single-label classification, multi-label classification, and regression), we end up using the same model with just different numbers of outputs. The loss function is the one thing that changes, which is why it's important to double-check that you are using the right loss function for your problem.\n\nfastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your `DataLoader`s, make sure you think hard when you have to decide on your about your choice of loss function, and remember that you most probably want:\n\n- `nn.CrossEntropyLoss` for single-label classification\n- `nn.BCEWithLogitsLoss` for multi-label classification\n- `nn.MSELoss` for regression\n\n## Questionnaire\n\n1. How could multi-label classification improve the usability of the bear classifier?\n1. How do we encode the dependent variable in a multi-label classification problem?\n1. How do you access the rows and columns of a DataFrame as if it was a matrix?\n1. How do you get a column by name from a DataFrame?\n1. What is the difference between a `Dataset` and `DataLoader`?\n1. What does a `Datasets` object normally contain?\n1. What does a `DataLoaders` object normally contain?\n1. What does `lambda` do in Python?\n1. What are the methods to customize how the independent and dependent variables are created with the data block API?\n1. Why is softmax not an appropriate output activation function when using a one hot encoded target?\n1. Why is `nll_loss` not an appropriate loss function when using a one-hot-encoded target?\n1. What is the difference between `nn.BCELoss` and `nn.BCEWithLogitsLoss`?\n1. Why can't we use regular accuracy in a multi-label problem?\n1. When is it okay to tune a hyperparameter on the validation set?\n1. How is `y_range` implemented in fastai? (See if you can implement it yourself and test it without peeking!)\n1. What is a regression problem? What loss function should you use for such a problem?\n1. What do you need to do to make sure the fastai library applies the same data augmentation to your inputs images and your target point coordinates?\n\n### Further Research\n\n1. Read a tutorial about Pandas DataFrames and experiment with a few methods that look interesting to you. See the book's website for recommended tutorials.\n1. Retrain the bear classifier using multi-label classification. See if you can make it work effectively with images that don't contain any bears, including showing that information in the web application. Try an image with two different kinds of bears. Check whether the accuracy on the single-label dataset is impacted using multi-label classification.\n\n",
    "supporting": [
      "06_multicat_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}