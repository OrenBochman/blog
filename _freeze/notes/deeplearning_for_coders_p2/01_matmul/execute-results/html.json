{
  "hash": "af2e1586bfca76aa0e77961dbc12c945",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Matrix multiplication from foundations\njupyter: python3\n---\n\n\n\n\n\n\nThe *foundations* we'll assume throughout this course are:\n\n- Python\n- matplotlib\n- The Python standard library\n- Jupyter notebooks and nbdev\n\n::: {#1069e129 .cell execution_count=1}\n``` {.python .cell-code}\nfrom pathlib import Path\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt\n```\n:::\n\n\n## Get data\n\n::: {#906e91c6 .cell execution_count=2}\n``` {.python .cell-code}\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n```\n:::\n\n\n[urlretrieve](https://docs.python.org/3/library/urllib.request.html#urllib.request.urlretrieve) - (read the docs!)\n\n::: {#c1eef375 .cell execution_count=3}\n``` {.python .cell-code}\nfrom urllib.request import urlretrieve\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(PosixPath('data/mnist.pkl.gz'), <http.client.HTTPMessage at 0x7395703ed900>)\n```\n:::\n:::\n\n\n::: {#e37eec60 .cell execution_count=4}\n``` {.python .cell-code}\n!ls -l data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntotal 16656\r\n-rw-rw-r-- 1 oren oren 17051982 Jan 14 15:14 mnist.pkl.gz\r\n```\n:::\n:::\n\n\n::: {#7a180cec .cell execution_count=5}\n``` {.python .cell-code}\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n```\n:::\n\n\n::: {#acdbe872 .cell execution_count=6}\n``` {.python .cell-code}\nlst1 = list(x_train[0])\nvals = lst1[200:210]\nvals\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n```\n:::\n:::\n\n\n::: {#443e3caa .cell execution_count=7}\n``` {.python .cell-code}\ndef chunks(x, sz):\n    for i in range(0, len(x), sz): yield x[i:i+sz]\n```\n:::\n\n\n::: {#c5782da5 .cell execution_count=8}\n``` {.python .cell-code}\nlist(chunks(vals, 5))\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n```\n:::\n:::\n\n\n::: {#7910574d .cell execution_count=9}\n``` {.python .cell-code}\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n```\n\n::: {.cell-output .cell-output-display}\n![](01_matmul_files/figure-html/cell-10-output-1.png){width=415 height=411}\n:::\n:::\n\n\n[islice](https://docs.python.org/3/library/itertools.html#itertools.islice)\n\n::: {#5d17a8e6 .cell execution_count=10}\n``` {.python .cell-code}\nfrom itertools import islice\n```\n:::\n\n\n::: {#86729852 .cell execution_count=11}\n``` {.python .cell-code}\nit = iter(vals)\nislice(it, 5)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<itertools.islice at 0x7395263ca2a0>\n```\n:::\n:::\n\n\n::: {#29574dd7 .cell execution_count=12}\n``` {.python .cell-code}\nlist(islice(it, 5))\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n```\n:::\n:::\n\n\n::: {#b6aeb253 .cell execution_count=13}\n``` {.python .cell-code}\nlist(islice(it, 5))\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n[0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]\n```\n:::\n:::\n\n\n::: {#dbcf5490 .cell execution_count=14}\n``` {.python .cell-code}\nlist(islice(it, 5))\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n[]\n```\n:::\n:::\n\n\n::: {#ed09e629 .cell execution_count=15}\n``` {.python .cell-code}\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n```\n:::\n\n\n::: {#fea96a05 .cell execution_count=16}\n``` {.python .cell-code}\nplt.imshow(img);\n```\n\n::: {.cell-output .cell-output-display}\n![](01_matmul_files/figure-html/cell-17-output-1.png){width=415 height=411}\n:::\n:::\n\n\n## Matrix and tensor\n\n::: {#7e558d5e .cell execution_count=17}\n``` {.python .cell-code}\nimg[20][15]\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n0.98828125\n```\n:::\n:::\n\n\n::: {#3a4c9ed2 .cell execution_count=18}\n``` {.python .cell-code}\nclass Matrix:\n    def __init__(self, xs): self.xs = xs\n    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]\n```\n:::\n\n\n::: {#1cbc9c9b .cell execution_count=19}\n``` {.python .cell-code}\nm = Matrix(img)\nm[20,15]\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n0.98828125\n```\n:::\n:::\n\n\n::: {#46971d99 .cell execution_count=20}\n``` {.python .cell-code}\nimport torch\nfrom torch import tensor\n```\n:::\n\n\n::: {#eec52aa9 .cell execution_count=21}\n``` {.python .cell-code}\ntensor([1,2,3])\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\ntensor([1, 2, 3])\n```\n:::\n:::\n\n\n::: {#ce7ff24b .cell execution_count=22}\n``` {.python .cell-code}\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\ntorch.Size([50000, 784])\n```\n:::\n:::\n\n\n::: {#1ba75648 .cell execution_count=23}\n``` {.python .cell-code}\nx_train.type()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n'torch.FloatTensor'\n```\n:::\n:::\n\n\n[Tensor](https://pytorch.org/docs/stable/tensors.html)\n\n::: {#4dc41f5c .cell execution_count=24}\n``` {.python .cell-code}\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\ntorch.Size([50000, 28, 28])\n```\n:::\n:::\n\n\n::: {#ee6691f3 .cell execution_count=25}\n``` {.python .cell-code}\nplt.imshow(imgs[0]);\n```\n\n::: {.cell-output .cell-output-display}\n![](01_matmul_files/figure-html/cell-26-output-1.png){width=415 height=411}\n:::\n:::\n\n\n::: {#a4301327 .cell execution_count=26}\n``` {.python .cell-code}\nimgs[0,20,15]\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\ntensor(0.9883)\n```\n:::\n:::\n\n\n::: {#01f32b2c .cell execution_count=27}\n``` {.python .cell-code}\nn,c = x_train.shape\ny_train, y_train.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n```\n:::\n:::\n\n\n::: {#61800376 .cell execution_count=28}\n``` {.python .cell-code}\nmin(y_train),max(y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n(tensor(0), tensor(9))\n```\n:::\n:::\n\n\n::: {#9bd3576d .cell execution_count=29}\n``` {.python .cell-code}\ny_train.min(), y_train.max()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n(tensor(0), tensor(9))\n```\n:::\n:::\n\n\n## Random numbers\n\nBased on the Wichmann Hill algorithm used before Python 2.3.\n\n::: {#287f2c4e .cell execution_count=30}\n``` {.python .cell-code}\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n```\n:::\n\n\n::: {#407a7f04 .cell execution_count=31}\n``` {.python .cell-code}\nseed(457428938475)\nrnd_state\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n(4976, 20238, 499)\n```\n:::\n:::\n\n\n::: {#915d52ed .cell execution_count=32}\n``` {.python .cell-code}\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n```\n:::\n\n\n::: {#ddf939bf .cell execution_count=33}\n``` {.python .cell-code}\nrand(),rand(),rand()\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n```\n:::\n:::\n\n\n::: {#61603f2c .cell execution_count=34}\n``` {.python .cell-code}\nif os.fork(): print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIn parent: 0.9559050644103264\n```\n:::\n:::\n\n\n::: {#224dc0ea .cell execution_count=35}\n``` {.python .cell-code}\nif os.fork(): print(f'In parent: {torch.rand(1)}')\nelse:\n    print(f'In child: {torch.rand(1)}')\n    os._exit(os.EX_OK)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIn parent: tensor([0.3593])\n```\n:::\n:::\n\n\n::: {#350cd3a6 .cell execution_count=36}\n``` {.python .cell-code}\nplt.plot([rand() for _ in range(50)]);\n```\n\n::: {.cell-output .cell-output-display}\n![](01_matmul_files/figure-html/cell-37-output-1.png){width=571 height=411}\n:::\n:::\n\n\n::: {#2969b42d .cell execution_count=37}\n``` {.python .cell-code}\nplt.hist([rand() for _ in range(10000)]);\n```\n\n::: {.cell-output .cell-output-display}\n![](01_matmul_files/figure-html/cell-38-output-1.png){width=583 height=411}\n:::\n:::\n\n\n::: {#32a08731 .cell execution_count=38}\n``` {.python .cell-code}\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.63 ms ± 75 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n```\n:::\n:::\n\n\n::: {#0b4e8995 .cell execution_count=39}\n``` {.python .cell-code}\n%timeit -n 10 torch.randn(784,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n39.7 µs ± 22.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n```\n:::\n:::\n\n\n## Matrix multiplication\n\n::: {#82007b20 .cell execution_count=40}\n``` {.python .cell-code}\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\n<torch._C.Generator at 0x739556970fd0>\n```\n:::\n:::\n\n\n::: {#3b459f87 .cell execution_count=41}\n``` {.python .cell-code}\nm1 = x_valid[:5]\nm2 = weights\n```\n:::\n\n\n::: {#7f898668 .cell execution_count=42}\n``` {.python .cell-code}\nm1.shape,m2.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n(torch.Size([5, 784]), torch.Size([784, 10]))\n```\n:::\n:::\n\n\n::: {#4ab84bd8 .cell execution_count=43}\n``` {.python .cell-code}\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\n((5, 784), (784, 10))\n```\n:::\n:::\n\n\n::: {#30faaf4e .cell execution_count=44}\n``` {.python .cell-code}\nt1 = torch.zeros(ar, bc)\nt1.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\ntorch.Size([5, 10])\n```\n:::\n:::\n\n\n::: {#6b8a7887 .cell execution_count=45}\n``` {.python .cell-code}\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n```\n:::\n\n\n::: {#368b71ab .cell execution_count=46}\n``` {.python .cell-code}\nt1\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n```\n:::\n:::\n\n\n::: {#4ac78674 .cell execution_count=47}\n``` {.python .cell-code}\nt1.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\ntorch.Size([5, 10])\n```\n:::\n:::\n\n\n::: {#e1800a33 .cell execution_count=48}\n``` {.python .cell-code}\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nt1\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n```\n:::\n:::\n\n\n::: {#ae5fed42 .cell execution_count=49}\n``` {.python .cell-code}\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\n```\n:::\n\n\n::: {#d6547eb3 .cell execution_count=50}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n```\n:::\n\n\n::: {#27eb6622 .cell execution_count=51}\n``` {.python .cell-code}\n%time _=matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 520 ms, sys: 0 ns, total: 520 ms\nWall time: 520 ms\n```\n:::\n:::\n\n\n::: {#45ec2984 .cell execution_count=52}\n``` {.python .cell-code}\nar*bc*ac\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```\n39200\n```\n:::\n:::\n\n\n## Numba\n\n::: {#83363210 .cell execution_count=53}\n``` {.python .cell-code}\nfrom numba import njit\n```\n:::\n\n\n::: {#f891fb14 .cell execution_count=54}\n``` {.python .cell-code}\n@njit\ndef dot(a,b):\n    res = 0.\n    for i in range(len(a)): res+=a[i]*b[i]\n    return res\n```\n:::\n\n\n::: {#d6a0bf1e .cell execution_count=55}\n``` {.python .cell-code}\nfrom numpy import array\n```\n:::\n\n\n::: {#70169f03 .cell execution_count=56}\n``` {.python .cell-code}\n%time dot(array([1.,2,3]),array([2.,3,4]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 601 ms, sys: 1.46 s, total: 2.06 s\nWall time: 356 ms\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\n20.0\n```\n:::\n:::\n\n\n::: {#074205c7 .cell execution_count=57}\n``` {.python .cell-code}\n%time dot(array([1.,2,3]),array([2.,3,4]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 17 µs, sys: 18 µs, total: 35 µs\nWall time: 39.1 µs\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=57}\n```\n20.0\n```\n:::\n:::\n\n\nNow only two of our loops are running in Python, not three:\n\n::: {#3d09e798 .cell execution_count=58}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n```\n:::\n\n\n::: {#94f9b3e5 .cell execution_count=59}\n``` {.python .cell-code}\nm1a,m2a = m1.numpy(),m2.numpy()\n```\n:::\n\n\n::: {#183c7a2f .cell execution_count=60}\n``` {.python .cell-code}\nfrom fastcore.test import *\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[60], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">from</span> <span style=\"font-weight:bold;color:rgb(0,0,255)\">fastcore</span><span style=\"font-weight:bold;color:rgb(0,0,255)\">.</span><span style=\"font-weight:bold;color:rgb(0,0,255)\">test</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">import</span> <span style=\"color:rgb(98,98,98)\">*</span>\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named 'fastcore'</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#c8c2d456 .cell execution_count=61}\n``` {.python .cell-code}\ntest_close(t1,matmul(m1a, m2a))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[61], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> <span class=\"ansi-yellow-bg\">test_close</span>(t1,matmul(m1a, m2a))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'test_close' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#32703829 .cell execution_count=62}\n``` {.python .cell-code}\n%timeit -n 50 matmul(m1a,m2a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe slowest run took 5.43 times longer than the fastest. This could mean that an intermediate result is being cached.\n448 µs ± 414 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n```\n:::\n:::\n\n\n## Elementwise ops\n\n[TryAPL](https://tryapl.org/)\n\n::: {#06a51d73 .cell execution_count=63}\n``` {.python .cell-code}\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n```\n:::\n:::\n\n\n::: {#a14786e2 .cell execution_count=64}\n``` {.python .cell-code}\na + b\n```\n\n::: {.cell-output .cell-output-display execution_count=64}\n```\ntensor([12., 14.,  3.])\n```\n:::\n:::\n\n\n::: {#63edc736 .cell execution_count=65}\n``` {.python .cell-code}\n(a < b).float().mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=65}\n```\ntensor(0.67)\n```\n:::\n:::\n\n\n::: {#225e0edc .cell execution_count=66}\n``` {.python .cell-code}\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n```\n\n::: {.cell-output .cell-output-display execution_count=66}\n```\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n```\n:::\n:::\n\n\nFrobenius norm:\n\n$$\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}$$\n\n*Hint*: you don't normally need to write equations in LaTeX yourself, instead, you can click 'edit' in Wikipedia and copy the LaTeX from there (which is what I did for the above equation). Or on arxiv.org, click \"Download: Other formats\" in the top right, then \"Download source\"; rename the downloaded file to end in `.tgz` if it doesn't already, and you should find the source there, including the equations to copy and paste. This is the source LaTeX that I pasted to render the equation above:\n\n```latex\n$$\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1/2}$$\n```\n\n::: {#d904aade .cell execution_count=67}\n``` {.python .cell-code}\nsf = (m*m).sum()\nsf\n```\n\n::: {.cell-output .cell-output-display execution_count=67}\n```\ntensor(285.)\n```\n:::\n:::\n\n\n::: {#80ebb5f4 .cell execution_count=68}\n``` {.python .cell-code}\nsf.sqrt()\n```\n\n::: {.cell-output .cell-output-display execution_count=68}\n```\ntensor(16.88)\n```\n:::\n:::\n\n\n::: {#b866e8f8 .cell execution_count=69}\n``` {.python .cell-code}\nm[2,:],m[:,2]\n```\n\n::: {.cell-output .cell-output-display execution_count=69}\n```\n(tensor([7., 8., 9.]), tensor([3., 6., 9.]))\n```\n:::\n:::\n\n\n::: {#6e26149b .cell execution_count=70}\n``` {.python .cell-code}\nm[2]\n```\n\n::: {.cell-output .cell-output-display execution_count=70}\n```\ntensor([7., 8., 9.])\n```\n:::\n:::\n\n\n::: {#ded8c133 .cell execution_count=71}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n    return c\n```\n:::\n\n\n::: {#9d342384 .cell execution_count=72}\n``` {.python .cell-code}\ntest_close(t1,matmul(m1, m2))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[72], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> <span class=\"ansi-yellow-bg\">test_close</span>(t1,matmul(m1, m2))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'test_close' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#91cbb7c5 .cell execution_count=73}\n``` {.python .cell-code}\n%timeit -n 50 _=matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n715 µs ± 2.51 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n```\n:::\n:::\n\n\n::: {#31f3c1c3 .cell execution_count=74}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = torch.dot(a[i,:], b[:,j])\n    return c\n```\n:::\n\n\n::: {#73266b86 .cell execution_count=75}\n``` {.python .cell-code}\ntest_close(t1,matmul(m1, m2))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[75], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> <span class=\"ansi-yellow-bg\">test_close</span>(t1,matmul(m1, m2))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'test_close' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#dbfdeebc .cell execution_count=76}\n``` {.python .cell-code}\n%timeit -n 50 _=matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n597 µs ± 5.52 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n```\n:::\n:::\n\n\n## Broadcasting\n\nThe term **broadcasting** describes how arrays with different shapes are treated during arithmetic operations.\n\nFrom the [Numpy Documentation](https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html):\n\n    The term broadcasting describes how numpy treats arrays with \n    different shapes during arithmetic operations. Subject to certain \n    constraints, the smaller array is “broadcast” across the larger \n    array so that they have compatible shapes. Broadcasting provides a \n    means of vectorizing array operations so that looping occurs in C\n    instead of Python. It does this without making needless copies of \n    data and usually leads to efficient algorithm implementations.\n    \nIn addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.\n\n*This section was adapted from [Chapter 4](http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/4.%20Compressed%20Sensing%20of%20CT%20Scans%20with%20Robust%20Regression.ipynb#4.-Compressed-Sensing-of-CT-Scans-with-Robust-Regression) of the fast.ai [Computational Linear Algebra](https://github.com/fastai/numerical-linear-algebra) course.*\n\n### Broadcasting with a scalar\n\n::: {#5818c0db .cell execution_count=77}\n``` {.python .cell-code}\na\n```\n\n::: {.cell-output .cell-output-display execution_count=77}\n```\ntensor([10.,  6., -4.])\n```\n:::\n:::\n\n\n::: {#8e2a12fa .cell execution_count=78}\n``` {.python .cell-code}\na > 0\n```\n\n::: {.cell-output .cell-output-display execution_count=78}\n```\ntensor([ True,  True, False])\n```\n:::\n:::\n\n\nHow are we able to do `a > 0`?  0 is being **broadcast** to have the same dimensions as a.\n\nFor instance you can normalize our dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar), using broadcasting.\n\nOther examples of broadcasting with a scalar:\n\n::: {#aba36631 .cell execution_count=79}\n``` {.python .cell-code}\na + 1\n```\n\n::: {.cell-output .cell-output-display execution_count=79}\n```\ntensor([11.,  7., -3.])\n```\n:::\n:::\n\n\n::: {#d196e677 .cell execution_count=80}\n``` {.python .cell-code}\nm\n```\n\n::: {.cell-output .cell-output-display execution_count=80}\n```\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n```\n:::\n:::\n\n\n::: {#3ff1a43e .cell execution_count=81}\n``` {.python .cell-code}\n2*m\n```\n\n::: {.cell-output .cell-output-display execution_count=81}\n```\ntensor([[ 2.,  4.,  6.],\n        [ 8., 10., 12.],\n        [14., 16., 18.]])\n```\n:::\n:::\n\n\n### Broadcasting a vector to a matrix\n\nAlthough broadcasting a scalar is an idea that dates back to APL, the more powerful idea of broadcasting across higher rank tensors [comes from](https://mail.python.org/pipermail/matrix-sig/1995-November/000143.html) a little known language called [Yorick](https://software.llnl.gov/yorick-doc/manual/yorick_50.html).\n\nWe can also broadcast a vector to a matrix:\n\n::: {#19b63098 .cell execution_count=82}\n``` {.python .cell-code}\nc = tensor([10.,20,30]); c\n```\n\n::: {.cell-output .cell-output-display execution_count=82}\n```\ntensor([10., 20., 30.])\n```\n:::\n:::\n\n\n::: {#91465799 .cell execution_count=83}\n``` {.python .cell-code}\nm\n```\n\n::: {.cell-output .cell-output-display execution_count=83}\n```\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n```\n:::\n:::\n\n\n::: {#327887d3 .cell execution_count=84}\n``` {.python .cell-code}\nm.shape,c.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n```\n(torch.Size([3, 3]), torch.Size([3]))\n```\n:::\n:::\n\n\n::: {#a49a9c78 .cell execution_count=85}\n``` {.python .cell-code}\nm + c\n```\n\n::: {.cell-output .cell-output-display execution_count=85}\n```\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n```\n:::\n:::\n\n\n::: {#6f930955 .cell execution_count=86}\n``` {.python .cell-code}\nc + m\n```\n\n::: {.cell-output .cell-output-display execution_count=86}\n```\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n```\n:::\n:::\n\n\n::: {#1347e336 .cell execution_count=87}\n``` {.python .cell-code}\nt = c.expand_as(m)\n```\n:::\n\n\n::: {#c42e8f6c .cell execution_count=88}\n``` {.python .cell-code}\nt\n```\n\n::: {.cell-output .cell-output-display execution_count=88}\n```\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n```\n:::\n:::\n\n\n::: {#f8363398 .cell execution_count=89}\n``` {.python .cell-code}\nm + t\n```\n\n::: {.cell-output .cell-output-display execution_count=89}\n```\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n```\n:::\n:::\n\n\nWe don't really copy the rows, but it looks as if we did. In fact, the rows are given a *stride* of 0.\n\n::: {#e076d654 .cell execution_count=90}\n``` {.python .cell-code}\nt.storage()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_42092/1911556638.py:1: UserWarning:\n\nTypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=90}\n```\n 10.0\n 20.0\n 30.0\n[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 3]\n```\n:::\n:::\n\n\n::: {#372379c5 .cell execution_count=91}\n``` {.python .cell-code}\nt.stride(), t.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=91}\n```\n((0, 1), torch.Size([3, 3]))\n```\n:::\n:::\n\n\nYou can index with the special value [None] or use `unsqueeze()` to convert a 1-dimensional array into a 2-dimensional array (although one of those dimensions has value 1).\n\n::: {#90ad1624 .cell execution_count=92}\n``` {.python .cell-code}\nc.unsqueeze(0), c[None, :]\n```\n\n::: {.cell-output .cell-output-display execution_count=92}\n```\n(tensor([[10., 20., 30.]]), tensor([[10., 20., 30.]]))\n```\n:::\n:::\n\n\n::: {#ec5e1ccf .cell execution_count=93}\n``` {.python .cell-code}\nc.shape, c.unsqueeze(0).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=93}\n```\n(torch.Size([3]), torch.Size([1, 3]))\n```\n:::\n:::\n\n\n::: {#2c227204 .cell execution_count=94}\n``` {.python .cell-code}\nc.unsqueeze(1), c[:, None]\n```\n\n::: {.cell-output .cell-output-display execution_count=94}\n```\n(tensor([[10.],\n         [20.],\n         [30.]]),\n tensor([[10.],\n         [20.],\n         [30.]]))\n```\n:::\n:::\n\n\n::: {#14f330e6 .cell execution_count=95}\n``` {.python .cell-code}\nc.shape, c.unsqueeze(1).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=95}\n```\n(torch.Size([3]), torch.Size([3, 1]))\n```\n:::\n:::\n\n\nYou can always skip trailling ':'s. And '...' means '*all preceding dimensions*'\n\n::: {#30081670 .cell execution_count=96}\n``` {.python .cell-code}\nc[None].shape,c[...,None].shape\n```\n\n::: {.cell-output .cell-output-display execution_count=96}\n```\n(torch.Size([1, 3]), torch.Size([3, 1]))\n```\n:::\n:::\n\n\n::: {#55258539 .cell execution_count=97}\n``` {.python .cell-code}\nc[:,None].expand_as(m)\n```\n\n::: {.cell-output .cell-output-display execution_count=97}\n```\ntensor([[10., 10., 10.],\n        [20., 20., 20.],\n        [30., 30., 30.]])\n```\n:::\n:::\n\n\n::: {#9cc596a0 .cell execution_count=98}\n``` {.python .cell-code}\nm + c[:,None]\n```\n\n::: {.cell-output .cell-output-display execution_count=98}\n```\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n```\n:::\n:::\n\n\n::: {#37ec5e16 .cell execution_count=99}\n``` {.python .cell-code}\nm + c[None,:]\n```\n\n::: {.cell-output .cell-output-display execution_count=99}\n```\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n```\n:::\n:::\n\n\n### Broadcasting Rules\n\n::: {#dd43e792 .cell execution_count=100}\n``` {.python .cell-code}\nc[None,:]\n```\n\n::: {.cell-output .cell-output-display execution_count=100}\n```\ntensor([[10., 20., 30.]])\n```\n:::\n:::\n\n\n::: {#e9f68af1 .cell execution_count=101}\n``` {.python .cell-code}\nc[None,:].shape\n```\n\n::: {.cell-output .cell-output-display execution_count=101}\n```\ntorch.Size([1, 3])\n```\n:::\n:::\n\n\n::: {#13e12f10 .cell execution_count=102}\n``` {.python .cell-code}\nc[:,None]\n```\n\n::: {.cell-output .cell-output-display execution_count=102}\n```\ntensor([[10.],\n        [20.],\n        [30.]])\n```\n:::\n:::\n\n\n::: {#a20ede08 .cell execution_count=103}\n``` {.python .cell-code}\nc[:,None].shape\n```\n\n::: {.cell-output .cell-output-display execution_count=103}\n```\ntorch.Size([3, 1])\n```\n:::\n:::\n\n\n::: {#64661300 .cell execution_count=104}\n``` {.python .cell-code}\nc[None,:] * c[:,None]\n```\n\n::: {.cell-output .cell-output-display execution_count=104}\n```\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n```\n:::\n:::\n\n\n::: {#b24d1cdf .cell execution_count=105}\n``` {.python .cell-code}\nc[None] > c[:,None]\n```\n\n::: {.cell-output .cell-output-display execution_count=105}\n```\ntensor([[False,  True,  True],\n        [False, False,  True],\n        [False, False, False]])\n```\n:::\n:::\n\n\n::: {#ff74b9fd .cell execution_count=106}\n``` {.python .cell-code}\nm*m\n```\n\n::: {.cell-output .cell-output-display execution_count=106}\n```\ntensor([[ 1.,  4.,  9.],\n        [16., 25., 36.],\n        [49., 64., 81.]])\n```\n:::\n:::\n\n\nWhen operating on two arrays/tensors, Numpy/PyTorch compares their shapes element-wise. It starts with the **trailing dimensions**, and works its way forward. Two dimensions are **compatible** when\n\n- they are equal, or\n- one of them is 1, in which case that dimension is broadcasted to make it the same size\n\nArrays do not need to have the same number of dimensions. For example, if you have a `256*256*3` array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n\n    Image  (3d array): 256 x 256 x 3\n    Scale  (1d array):             3\n    Result (3d array): 256 x 256 x 3\n\nThe [numpy documentation](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html#general-broadcasting-rules) includes several examples of what dimensions can and can not be broadcast together.\n\n## Matmul with broadcasting\n\n::: {#0c57c59e .cell execution_count=107}\n``` {.python .cell-code}\ndigit = m1[0]\ndigit.shape,m2.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=107}\n```\n(torch.Size([784]), torch.Size([784, 10]))\n```\n:::\n:::\n\n\n::: {#69326b0d .cell execution_count=108}\n``` {.python .cell-code}\ndigit[:,None].shape\n```\n\n::: {.cell-output .cell-output-display execution_count=108}\n```\ntorch.Size([784, 1])\n```\n:::\n:::\n\n\n::: {#0143cde2 .cell execution_count=109}\n``` {.python .cell-code}\ndigit[:,None].expand_as(m2).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=109}\n```\ntorch.Size([784, 10])\n```\n:::\n:::\n\n\n::: {#cf3fb539 .cell execution_count=110}\n``` {.python .cell-code}\n(digit[:,None]*m2).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=110}\n```\ntorch.Size([784, 10])\n```\n:::\n:::\n\n\n::: {#908af53d .cell execution_count=111}\n``` {.python .cell-code}\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:] * b[:,j]).sum()      # previous version\n        c[i]   = (a[i,:,None] * b).sum(dim=0) # broadcast version\n    return c\n```\n:::\n\n\n::: {#5917e261 .cell execution_count=112}\n``` {.python .cell-code}\ntest_close(t1,matmul(m1, m2))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[112], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> <span class=\"ansi-yellow-bg\">test_close</span>(t1,matmul(m1, m2))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'test_close' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#8bad392e .cell execution_count=113}\n``` {.python .cell-code}\n%timeit -n 50 _=matmul(m1, m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n79.4 µs ± 4.16 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n```\n:::\n:::\n\n\nOur time has gone from ~500ms to <0.1ms, an over 5000x improvement! We can run on the whole dataset now.\n\n::: {#bdad5112 .cell execution_count=114}\n``` {.python .cell-code}\ntr = matmul(x_train, weights)\ntr\n```\n\n::: {.cell-output .cell-output-display execution_count=114}\n```\ntensor([[  0.96,  -2.96,  -2.11,  ..., -15.09, -17.69,   0.60],\n        [  6.89,  -0.34,   0.79,  ..., -17.13, -25.36,  16.23],\n        [-10.18,   7.38,   4.13,  ...,  -6.73,  -6.79,  -1.58],\n        ...,\n        [  7.40,   7.64,  -3.50,  ...,  -1.02, -16.22,   2.07],\n        [  3.25,   9.52,  -9.37,  ...,   2.98, -19.58,  -1.96],\n        [ 15.70,   4.12,  -5.62,  ...,   8.08, -12.21,   0.42]])\n```\n:::\n:::\n\n\n::: {#be687d20 .cell execution_count=115}\n``` {.python .cell-code}\ntr.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=115}\n```\ntorch.Size([50000, 10])\n```\n:::\n:::\n\n\n::: {#f1e02815 .cell execution_count=116}\n``` {.python .cell-code}\n%time _=matmul(x_train, weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCPU times: user 862 ms, sys: 0 ns, total: 862 ms\nWall time: 785 ms\n```\n:::\n:::\n\n\n## Einstein summation\n\n[Einstein summation](https://ajcr.net/Basic-guide-to-einsum/) ([`einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)) is a compact representation for combining products and sums in a general way. The key rules are:\n\n- Repeating letters between input arrays means that values along those axes will be multiplied together.\n- Omitting a letter from the output means that values along that axis will be summed.\n\n::: {#532bc30d .cell execution_count=117}\n``` {.python .cell-code}\nm1.shape,m2.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=117}\n```\n(torch.Size([5, 784]), torch.Size([784, 10]))\n```\n:::\n:::\n\n\n::: {#f967a38c .cell execution_count=118}\n``` {.python .cell-code}\n# c[i,j] += a[i,k] * b[k,j]\n# c[i,j] = (a[i,:] * b[:,j]).sum()\nmr = torch.einsum('ik,kj->ikj', m1, m2)\nmr.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=118}\n```\ntorch.Size([5, 784, 10])\n```\n:::\n:::\n\n\n::: {#aacfd6f2 .cell execution_count=119}\n``` {.python .cell-code}\nmr.sum(1)\n```\n\n::: {.cell-output .cell-output-display execution_count=119}\n```\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n```\n:::\n:::\n\n\n::: {#0f02cfa6 .cell execution_count=120}\n``` {.python .cell-code}\ntorch.einsum('ik,kj->ij', m1, m2)\n```\n\n::: {.cell-output .cell-output-display execution_count=120}\n```\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n```\n:::\n:::\n\n\n::: {#10eefc31 .cell execution_count=121}\n``` {.python .cell-code}\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n```\n:::\n\n\n::: {#e959f550 .cell execution_count=122}\n``` {.python .cell-code}\ntest_close(tr, matmul(x_train, weights), eps=1e-3)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[122], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> <span class=\"ansi-yellow-bg\">test_close</span>(tr, matmul(x_train, weights), eps<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">1e-3</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'test_close' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#7c6ae73b .cell execution_count=123}\n``` {.python .cell-code}\n%timeit -n 5 _=matmul(x_train, weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10.9 ms ± 1.27 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n## pytorch op\n\nWe can use pytorch's function or operator directly for matrix multiplication.\n\n::: {#b252d513 .cell execution_count=124}\n``` {.python .cell-code}\ntest_close(tr, x_train@weights, eps=1e-3)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[124], line 1</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span> <span class=\"ansi-yellow-bg\">test_close</span>(tr, x_train<span style=\"color:rgb(175,0,255)\">@weights</span>, eps<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">1e-3</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'test_close' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#e02accc4 .cell execution_count=125}\n``` {.python .cell-code}\n%timeit -n 5 _=torch.matmul(x_train, weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10.2 ms ± 722 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n```\n:::\n:::\n\n\n## CUDA\n\n::: {#34bbc1ca .cell execution_count=126}\n``` {.python .cell-code}\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n```\n:::\n\n\n::: {#751d4380 .cell execution_count=127}\n``` {.python .cell-code}\nres = torch.zeros(ar, bc)\nmatmul((0,0), m1, m2, res)\nres\n```\n\n::: {.cell-output .cell-output-display execution_count=127}\n```\ntensor([[-10.94,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00],\n        [  0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00,   0.00]])\n```\n:::\n:::\n\n\n::: {#18b61217 .cell execution_count=128}\n``` {.python .cell-code}\ndef launch_kernel(kernel, grid_x, grid_y, *args, **kwargs):\n    for i in range(grid_x):\n        for j in range(grid_y): kernel((i,j), *args, **kwargs)\n```\n:::\n\n\n::: {#a1447aae .cell execution_count=129}\n``` {.python .cell-code}\nres = torch.zeros(ar, bc)\nlaunch_kernel(matmul, ar, bc, m1, m2, res)\nres\n```\n\n::: {.cell-output .cell-output-display execution_count=129}\n```\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n```\n:::\n:::\n\n\n::: {#aac25786 .cell execution_count=130}\n``` {.python .cell-code}\nfrom numba import cuda\n```\n:::\n\n\n::: {#3e492dd8 .cell execution_count=131}\n``` {.python .cell-code}\ndef matmul(grid, a,b,c):\n    i,j = grid\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n```\n:::\n\n\n::: {#f56e1ad8 .cell execution_count=132}\n``` {.python .cell-code}\n@cuda.jit\ndef matmul(a,b,c):\n    i, j = cuda.grid(2)\n    if i < c.shape[0] and j < c.shape[1]:\n        tmp = 0.\n        for k in range(a.shape[1]): tmp += a[i, k] * b[k, j]\n        c[i,j] = tmp\n```\n:::\n\n\n::: {#eb3de405 .cell execution_count=133}\n``` {.python .cell-code}\nr = np.zeros(tr.shape)\nm1g,m2g,rg = map(cuda.to_device, (x_train,weights,r))\n```\n:::\n\n\n::: {#6fe5b9cc .cell execution_count=134}\n``` {.python .cell-code}\nr.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=134}\n```\n(50000, 10)\n```\n:::\n:::\n\n\n::: {#4a2a959e .cell execution_count=135}\n``` {.python .cell-code}\nTPB = 16\nrr,rc = r.shape\nblockspergrid = (math.ceil(rr / TPB), math.ceil(rc / TPB))\nblockspergrid\n```\n\n::: {.cell-output .cell-output-display execution_count=135}\n```\n(3125, 1)\n```\n:::\n:::\n\n\n::: {#30ffb5b4 .cell execution_count=136}\n``` {.python .cell-code}\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\ntest_close(tr, r, eps=1e-3)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[136], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> matmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\n<span class=\"ansi-green-fg ansi-bold\">      2</span> r <span style=\"color:rgb(98,98,98)\">=</span> rg<span style=\"color:rgb(98,98,98)\">.</span>copy_to_host()\n<span class=\"ansi-green-fg\">----&gt; 3</span> <span class=\"ansi-yellow-bg\">test_close</span>(tr, r, eps<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">1e-3</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'test_close' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#3418192a .cell execution_count=137}\n``` {.python .cell-code}\n%%timeit -n 10\nmatmul[blockspergrid, (TPB,TPB)](m1g,m2g,rg)\nr = rg.copy_to_host()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n7.94 ms ± 709 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n```\n:::\n:::\n\n\n::: {#445cf5dc .cell execution_count=138}\n``` {.python .cell-code}\nm1c,m2c = x_train.cuda(),weights.cuda()\n```\n:::\n\n\n::: {#ad169e98 .cell execution_count=139}\n``` {.python .cell-code}\nr=(m1c@m2c).cpu()\n```\n:::\n\n\n::: {#5b95f591 .cell execution_count=140}\n``` {.python .cell-code}\n%timeit -n 10 r=(m1c@m2c).cpu()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.17 ms ± 196 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n```\n:::\n:::\n\n\nOur broadcasting version was >500ms, and our CUDA version is around 0.5ms, which is another 1000x improvement compared to broadcasting. So our total speedup is around 5 million times!\n\n",
    "supporting": [
      "01_matmul_files"
    ],
    "filters": [],
    "includes": {}
  }
}