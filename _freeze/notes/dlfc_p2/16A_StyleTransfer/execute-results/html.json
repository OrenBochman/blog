{
  "hash": "ad3ce89d2af1307e4cca3ce9ddccf606",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Setup\njupyter: python3\nimage: cover.png\n---\n\n::: {#c05ba460 .cell execution_count=1}\n``` {.python .cell-code}\nimport pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\nfrom fastcore.foundation import L, store_attr\n\nfrom miniai.datasets import *\nfrom miniai.conv import *\nfrom miniai.learner import *\nfrom miniai.activations import *\nfrom miniai.init import *\nfrom miniai.sgd import *\nfrom miniai.resnet import *\n```\n:::\n\n\n::: {#e956016b .cell execution_count=2}\n``` {.python .cell-code}\n# Image URLs for demos. Change as desired.\nface_url = \"https://images.pexels.com/photos/2690323/pexels-photo-2690323.jpeg?w=256\"\nspiderweb_url = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"\n```\n:::\n\n\n# Loading Images\n\n::: {#98e25ce5 .cell execution_count=3}\n``` {.python .cell-code}\ndef download_image(url):\n    imgb = fc.urlread(url, decode=False) \n    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255.\n```\n:::\n\n\n::: {#5d5a8bfb .cell execution_count=4}\n``` {.python .cell-code}\ncontent_im = download_image(face_url).to(def_device)\nprint('content_im.shape:', content_im.shape)\nshow_image(content_im);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncontent_im.shape: torch.Size([3, 256, 256])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-5-output-2.png){width=389 height=389}\n:::\n:::\n\n\n::: {#970da107 .cell execution_count=5}\n``` {.python .cell-code}\ncontent_im.min(),content_im.max() # Check bounds\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n(tensor(0., device='cuda:0'), tensor(1., device='cuda:0'))\n```\n:::\n:::\n\n\n# Optimizing Images\n\n::: {#4f8541d9 .cell execution_count=6}\n``` {.python .cell-code}\nclass LengthDataset():\n    def __init__(self, length=1): self.length=length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0,0\n\ndef get_dummy_dls(length=100):\n    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1), # Train\n                       DataLoader(LengthDataset(1), batch_size=1))      # Valid (length 1)\n```\n:::\n\n\n::: {#1c85c992 .cell execution_count=7}\n``` {.python .cell-code}\nclass TensorModel(nn.Module):\n    def __init__(self, t):\n        super().__init__()\n        self.t = nn.Parameter(t.clone())\n    def forward(self, x=0): return self.t\n```\n:::\n\n\n::: {#bff2daef .cell execution_count=8}\n``` {.python .cell-code}\nmodel = TensorModel(torch.rand_like(content_im))\nshow_image(model());\n```\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-9-output-1.png){width=389 height=389}\n:::\n:::\n\n\n::: {#d038de2a .cell execution_count=9}\n``` {.python .cell-code}\n[p.shape for p in model.parameters()]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n[torch.Size([3, 256, 256])]\n```\n:::\n:::\n\n\n::: {#a85c7e19 .cell execution_count=10}\n``` {.python .cell-code}\nclass ImageOptCB(TrainCB):\n    def predict(self, learn): learn.preds = learn.model()\n    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds)\n```\n:::\n\n\n::: {#d4e55992 .cell execution_count=11}\n``` {.python .cell-code}\ndef loss_fn_mse(im):\n    return F.mse_loss(im, content_im)\n\nmodel = TensorModel(torch.rand_like(content_im))\ncbs = [ImageOptCB(), ProgressCB(), MetricsCB(), DeviceCB()]\nlearn = Learner(model, get_dummy_dls(100), loss_fn_mse, \n                lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>loss</th>\n      <th>epoch</th>\n      <th>train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0.041</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <td>0.001</td>\n      <td>0</td>\n      <td>eval</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n::: {#64e79e3f .cell execution_count=12}\n``` {.python .cell-code}\n# Result (left) vs target image (right):\nshow_images([learn.model().clip(0, 1), content_im]);\n```\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-13-output-1.png){width=466 height=222}\n:::\n:::\n\n\n## Viewing progress\n\nIt would be great if we could see what is happening over time. You could save individual images and turn them into a video, but for quick feedback we can also log images every few iterations and display them in a grid in `after_fit`:\n\n::: {#140051cd .cell execution_count=13}\n``` {.python .cell-code}\nclass ImageLogCB(Callback):\n    order = ProgressCB.order + 1\n    def __init__(self, log_every=10): store_attr(); self.images=[]; self.i=0\n    def after_batch(self, learn): \n        if self.i%self.log_every == 0: self.images.append(to_cpu(learn.preds.clip(0, 1)))\n        self.i += 1\n    def after_fit(self, learn): show_images(self.images)\n```\n:::\n\n\n::: {#ecef7bb3 .cell execution_count=14}\n``` {.python .cell-code}\nmodel = TensorModel(torch.rand_like(content_im))\nlearn = Learner(model, get_dummy_dls(150), loss_fn_mse, \n                lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(30)])\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>loss</th>\n      <th>epoch</th>\n      <th>train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0.028</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <td>0.000</td>\n      <td>0</td>\n      <td>eval</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-15-output-3.png){width=689 height=458}\n:::\n:::\n\n\n## Getting Features from VGG16\n\nWe're going to peek inside a small CNN and extract the outputs of different layers.\n\n### Load VGG network\n\n![vgg diag](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)\n\n::: {#1c5e3df9 .cell execution_count=15}\n``` {.python .cell-code}\nprint(timm.list_models('*vgg*'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['repvgg_a0', 'repvgg_a1', 'repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'repvgg_d2se', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn']\n```\n:::\n:::\n\n\n::: {#f88f919d .cell execution_count=16}\n``` {.python .cell-code}\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\n```\n:::\n\n\n::: {#4401c738 .cell execution_count=17}\n``` {.python .cell-code}\n# vgg16\n```\n:::\n\n\n### Normalize Images\n\nThis model expacts images normalized with the same stats as those used during training, which in this case requires the stats of the ImageNet dataset. Previously we were working with single-channel images, and so normalizing was pretty straightforward. With three channels, we need to think a bit more about shapes and boradcasting rules:\n\n::: {#729aef9a .cell execution_count=18}\n``` {.python .cell-code}\nimagenet_mean = tensor([0.485, 0.456, 0.406])\nimagenet_std = tensor([0.229, 0.224, 0.225])\n```\n:::\n\n\n::: {#487d2aa1 .cell execution_count=19}\n``` {.python .cell-code}\n# Try 1 (won't work):\n# (content_im - imagenet_mean) / imagenet_std\n```\n:::\n\n\n::: {#ab081d61 .cell execution_count=20}\n``` {.python .cell-code}\nimagenet_mean.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\ntorch.Size([3])\n```\n:::\n:::\n\n\n::: {#b0d4d549 .cell execution_count=21}\n``` {.python .cell-code}\ncontent_im.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\ntorch.Size([3, 256, 256])\n```\n:::\n:::\n\n\n::: {#bbd595d5 .cell execution_count=22}\n``` {.python .cell-code}\nimagenet_mean[:,None,None].shape\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\ntorch.Size([3, 1, 1])\n```\n:::\n:::\n\n\n::: {#6cdffd94 .cell execution_count=23}\n``` {.python .cell-code}\n# Try 2:\ndef normalize(im):\n    imagenet_mean = tensor([0.485, 0.456, 0.406])[:,None,None].to(im.device)\n    imagenet_std = tensor([0.229, 0.224, 0.225])[:,None,None].to(im.device)\n    return (im - imagenet_mean) / imagenet_std\n```\n:::\n\n\n::: {#cec09d78 .cell execution_count=24}\n``` {.python .cell-code}\nnormalize(content_im).min(), normalize(content_im).max()\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n(tensor(-2.1179, device='cuda:0'), tensor(2.6400, device='cuda:0'))\n```\n:::\n:::\n\n\n::: {#e721ee0b .cell execution_count=25}\n``` {.python .cell-code}\nnormalize(content_im).mean(dim=(1, 2))\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\ntensor([-0.9724, -0.9594, -0.4191], device='cuda:0')\n```\n:::\n:::\n\n\n::: {#c7738674 .cell execution_count=26}\n``` {.python .cell-code}\n# And with torchvision transforms:\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n```\n:::\n\n\n::: {#6c47bb36 .cell execution_count=27}\n``` {.python .cell-code}\nnormalize(content_im).min(), normalize(content_im).max()\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n(tensor(-2.1179, device='cuda:0'), tensor(2.6400, device='cuda:0'))\n```\n:::\n:::\n\n\n### Get intermediate representations, take 1:\n\nWe want to feed some data through the network, storing thr outputs of different layers. Here's one way to do this:\n\n::: {#a1599192 .cell execution_count=28}\n``` {.python .cell-code}\ndef calc_features(imgs, target_layers=(18, 25)): \n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n```\n:::\n\n\n::: {#fe65ad3d .cell execution_count=29}\n``` {.python .cell-code}\n# Testing it out to see the shapes of the resulting feature maps:\nfeats = calc_features(content_im)\n[f.shape for f in feats]\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n[torch.Size([512, 32, 32]), torch.Size([512, 16, 16])]\n```\n:::\n:::\n\n\n::: {#0a68d8a6 .cell execution_count=30}\n``` {.python .cell-code}\n# Homework: Can you do this using hooks?\n```\n:::\n\n\n### What's the point?\n\nYou may remember us looking at https://distill.pub/2017/feature-visualization/ and talking about how deep CNNs 'learn' to classify images. Early layers tend to capture gradients and textures, while later layers tend towards more complex types of feature. We're going to exploit this hierarchy for artistic purposes, but being able to choose what kind of feature you'd like to use when comparing images has a number of other useful applications. \n\n# Optimizing an Image with Content Loss\n\nTo start with, let's try optimizing an image by comparing it's features (from two later layers) with those from the target image. If our theory is right, we should see the structure of the target emerge from the noise without necessarily seeing a perfect re-production of the target like we did in the previous MSE loss example.\n\n::: {#a56a6915 .cell execution_count=31}\n``` {.python .cell-code}\nclass ContentLossToTarget():\n    def __init__(self, target_im, target_layers=(18, 25)):\n        fc.store_attr()\n        with torch.no_grad():\n            self.target_features = calc_features(target_im, target_layers)\n    def __call__(self, input_im): \n        return sum((f1-f2).pow(2).mean() for f1, f2 in \n               zip(calc_features(input_im, self.target_layers), self.target_features))\n```\n:::\n\n\n::: {#13e8ad99 .cell execution_count=32}\n``` {.python .cell-code}\nloss_function_perceptual = ContentLossToTarget(content_im)\nmodel = TensorModel(torch.rand_like(content_im))\nlearn = Learner(model, get_dummy_dls(150), loss_function_perceptual, \n                lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(log_every=30)])\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>loss</th>\n      <th>epoch</th>\n      <th>train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>4.862</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <td>1.494</td>\n      <td>0</td>\n      <td>eval</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-33-output-3.png){width=689 height=458}\n:::\n:::\n\n\n**Choosing the layers determines the kind of features that are important:**\n\n::: {#3b76eba4 .cell execution_count=33}\n``` {.python .cell-code}\nloss_function_perceptual = ContentLossToTarget(content_im, target_layers=(1, 6))\nmodel = TensorModel(torch.rand_like(content_im))\nlearn = Learner(model, get_dummy_dls(150), loss_function_perceptual, \n                lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(log_every=30)])\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>loss</th>\n      <th>epoch</th>\n      <th>train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2.358</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <td>0.362</td>\n      <td>0</td>\n      <td>eval</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-34-output-3.png){width=689 height=458}\n:::\n:::\n\n\n# Style Loss with Gram Matrix\n\nSo, we know how to extract feature maps. The next thing we'd like to do is find a way to capture the **style** of an input image, based on those early layers and the kinds of textural feature that they learn. Unfortunately, we can't just compare the feature maps from some early layers since these 'maps' encode information spatially - which we don't want!\n\n![feature_map_extraction.png](attachment:feature_map_extraction.png)\n\nSo, we need a way to measure what **kinds** of style features are present, and ideally which kinds occur together, without worrying about **where** these features occur in the image. \n\nEnter something called the Gram Matrix. The idea here is that we'll measure the **correlation** between features. Given a feature map with `f` features in an `h` x `w` grid, we'll flatten out the spatial component and then for every feature we'll take the dot product of that row with itself, giving an `f` x `f` matrix as the result. Each entry in this matrix quantifies how correlated the relevant pair of features are and how frequently they occur - exactly what we want. In this diagram each feature is represented as a colored dot.\n\n![gram_calculation.png](attachment:gram_calculation.png)\n\nRe-creating the diagram operations in code:\n\n::: {#c32b2522 .cell execution_count=34}\n``` {.python .cell-code}\nt = tensor([[0, 1, 0, 1, 1, 0, 0, 1, 1],\n            [0, 1, 0, 1, 0, 0, 0, 0, 1],\n            [1, 0, 1, 1, 1, 1, 1, 1, 0],\n            [1, 0, 1, 1, 0, 1, 1, 0, 0]])\n```\n:::\n\n\n::: {#f437ff2e .cell execution_count=35}\n``` {.python .cell-code}\ntorch.einsum('fs, gs -> fg', t, t)\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\ntensor([[5, 3, 3, 1],\n        [3, 3, 1, 1],\n        [3, 1, 7, 5],\n        [1, 1, 5, 5]])\n```\n:::\n:::\n\n\n::: {#4caf3c72 .cell execution_count=36}\n``` {.python .cell-code}\nt.matmul(t.T) # Alternate approach\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\ntensor([[5, 3, 3, 1],\n        [3, 3, 1, 1],\n        [3, 1, 7, 5],\n        [1, 1, 5, 5]])\n```\n:::\n:::\n\n\n### Trying it out\n\n::: {#8c502a98 .cell execution_count=37}\n``` {.python .cell-code}\nstyle_im = download_image(spiderweb_url).to(def_device)\nshow_image(style_im);\n```\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-38-output-1.png){width=540 height=367}\n:::\n:::\n\n\n::: {#e4553239 .cell execution_count=38}\n``` {.python .cell-code}\ndef calc_grams(img, target_layers=(1, 6, 11, 18, 25)):\n    return L(torch.einsum('chw, dhw -> cd', x, x) / (x.shape[-2]*x.shape[-1]) # 'bchw, bdhw -> bcd' if batched\n            for x in calc_features(img, target_layers))\n```\n:::\n\n\n::: {#f4a6541a .cell execution_count=39}\n``` {.python .cell-code}\nstyle_grams = calc_grams(style_im)\n```\n:::\n\n\n::: {#5f6b93af .cell execution_count=40}\n``` {.python .cell-code}\n[g.shape for g in style_grams] # The gram matrices for features from different layers\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\n[torch.Size([64, 64]),\n torch.Size([128, 128]),\n torch.Size([256, 256]),\n torch.Size([512, 512]),\n torch.Size([512, 512])]\n```\n:::\n:::\n\n\n::: {#7efb285b .cell execution_count=41}\n``` {.python .cell-code}\nstyle_grams.attrgot('shape') # The magic of fastcore's L\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\n(#5) [torch.Size([64, 64]),torch.Size([128, 128]),torch.Size([256, 256]),torch.Size([512, 512]),torch.Size([512, 512])]\n```\n:::\n:::\n\n\n::: {#09529051 .cell execution_count=42}\n``` {.python .cell-code}\nclass StyleLossToTarget():\n    def __init__(self, target_im, target_layers=(1, 6, 11, 18, 25)):\n        fc.store_attr()\n        with torch.no_grad(): self.target_grams = calc_grams(target_im, target_layers)\n    def __call__(self, input_im): \n        return sum((f1-f2).pow(2).mean() for f1, f2 in \n               zip(calc_grams(input_im, self.target_layers), self.target_grams))\n```\n:::\n\n\n::: {#54aa9926 .cell execution_count=43}\n``` {.python .cell-code}\nstyle_loss = StyleLossToTarget(style_im)\n```\n:::\n\n\n::: {#eb629d45 .cell execution_count=44}\n``` {.python .cell-code}\nstyle_loss(content_im)\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\ntensor(429.7724, device='cuda:0', grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\n## Style Transfer\n\n::: {#6d559fce .cell execution_count=45}\n``` {.python .cell-code}\nmodel = TensorModel(content_im) # Start from content image\nstyle_loss = StyleLossToTarget(style_im)\ncontent_loss = ContentLossToTarget(content_im)\ndef combined_loss(x):\n    return style_loss(x) + content_loss(x)\nlearn = Learner(model, get_dummy_dls(150), combined_loss, lr=1e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(30)])\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>loss</th>\n      <th>epoch</th>\n      <th>train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>75.769</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <td>45.183</td>\n      <td>0</td>\n      <td>eval</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-46-output-3.png){width=689 height=458}\n:::\n:::\n\n\n::: {#ac7d8d29 .cell execution_count=46}\n``` {.python .cell-code}\nshow_image(learn.model().clip(0, 1)); # View the final result\n```\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-47-output-1.png){width=389 height=389}\n:::\n:::\n\n\nAnd trying with random starting image, weighting the style loss lower, using different layers:\n\n::: {#30b5b295 .cell execution_count=47}\n``` {.python .cell-code}\nmodel = TensorModel(torch.rand_like(content_im))\nstyle_loss = StyleLossToTarget(style_im)\ncontent_loss = ContentLossToTarget(content_im, target_layers=(6, 18, 25))\ndef combined_loss(x):\n    return style_loss(x) * 0.2 + content_loss(x)\nlearn = Learner(model, get_dummy_dls(300), combined_loss, lr=5e-2, cbs=cbs, opt_func=torch.optim.Adam)\nlearn.fit(1, cbs=[ImageLogCB(60)])\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>loss</th>\n      <th>epoch</th>\n      <th>train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>44.635</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <td>35.668</td>\n      <td>0</td>\n      <td>eval</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-48-output-3.png){width=689 height=458}\n:::\n:::\n\n\n## For Comparison: non-miniai version\n\n::: {#232e4332 .cell execution_count=48}\n``` {.python .cell-code}\n# The image to be optimized\nim = torch.rand(3, 256, 256).to(def_device)\nim.requires_grad = True\n\n# Set up the optimizer\nopt = torch.optim.Adam([im], lr=5e-2)\n\n# Define the loss function\nstyle_loss = StyleLossToTarget(style_im)\ncontent_loss = ContentLossToTarget(content_im, target_layers=[6, 18, 25])\ndef combined_loss(x):\n    return style_loss(x) * 0.2 + content_loss(x)\n\n# Optimization loop\nfor i in range(300):\n    loss = combined_loss(im)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    \n# Show the result\nshow_image(im.clip(0, 1));\n```\n\n::: {.cell-output .cell-output-display}\n![](16A_StyleTransfer_files/figure-html/cell-49-output-1.png){width=389 height=389}\n:::\n:::\n\n\nWhat do you think are some pros and cons? How would this look once we start displaying progress, testing different configurations and so on?\n\n",
    "supporting": [
      "16A_StyleTransfer_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}