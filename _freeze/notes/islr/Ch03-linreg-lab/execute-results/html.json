{
  "hash": "305306f8e764cb5fafd275f7acde3342",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Linear Regression\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/intro-stat-learning/ISLP_labs/blob/v2.2/Ch03-linreg-lab.ipynb\">\n<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/intro-stat-learning/ISLP_labs/v2.2?labpath=Ch03-linreg-lab.ipynb)\n\n\n\n## Importing packages\nWe import our standard libraries at this top\nlevel.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\n```\n:::\n\n\n\n \n\n### New imports\nThroughout this lab we will introduce new functions and libraries. However,\nwe will import them here to emphasize these are the new\ncode objects in this lab. Keeping imports near the top\nof a notebook makes the code more readable, since scanning the first few\nlines tells us what libraries are used.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.api as sm\n```\n:::\n\n\n\n We will provide relevant details about the\nfunctions below as they are needed.\n\nBesides importing whole modules, it is also possible\nto import only a few items from a given module. This\nwill help keep the  *namespace* clean.\nWe will use a few specific objects from the `statsmodels` package\nwhich we import here.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom statsmodels.stats.outliers_influence \\\n     import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n```\n:::\n\n\n\n\nAs one of the import statements above is quite a long line, we inserted a line break `\\` to\nease readability.\n\nWe will also use some functions written for the labs in this book in the `ISLP`\npackage.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,\n                         summarize,\n                         poly)\n```\n:::\n\n\n\n\n### Inspecting Objects and Namespaces\nThe\nfunction  `dir()`\nprovides a list of\nobjects in a namespace.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndir()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['MS', 'VIF', '__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'anova_lm', 'load_data', 'np', 'pd', 'poly', 'r', 'sm', 'subplots', 'summarize']\n```\n\n\n:::\n:::\n\n\n\n This shows you everything that `Python` can find at the top level.\nThere are certain objects like `__builtins__` that contain references to built-in\nfunctions like `print()`.\n\nEvery python object has its own notion of\nnamespace, also accessible with `dir()`. This will include\nboth the attributes of the object\nas well as any methods associated with it. For instance, we see `'sum'` in the listing for an\narray.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nA = np.array([3,5,11])\ndir(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['T', '__abs__', '__add__', '__and__', '__array__', '__array_finalize__', '__array_function__', '__array_interface__', '__array_prepare__', '__array_priority__', '__array_struct__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__class_getitem__', '__complex__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__divmod__', '__dlpack__', '__dlpack_device__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__xor__', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n```\n\n\n:::\n:::\n\n\n\n This indicates that the object `A.sum` exists. In this case it is a method\nthat can be used to compute the sum of the array `A` as can be seen by typing `A.sum?`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nA.sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n19\n```\n\n\n:::\n:::\n\n\n\n    \n \n\n## Simple Linear Regression\nIn this section we will  construct model \nmatrices (also called design matrices) using the `ModelSpec()`  transform from `ISLP.models`.\n\nWe  will use the `Boston` housing data set, which is contained in the `ISLP` package.  The `Boston` dataset records  `medv`  (median house value) for $506$ neighborhoods\naround Boston.  We will build a regression model to predict  `medv`  using $13$\npredictors such as  `rmvar`  (average number of rooms per house),\n `age`  (proportion of owner-occupied units built prior to 1940), and  `lstat`  (percent of\nhouseholds with low socioeconomic status).  We will use `statsmodels` for this\ntask, a `Python` package that implements several commonly used\nregression methods.\n\nWe have included a simple loading function `load_data()` in the\n`ISLP` package:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nBoston = load_data(\"Boston\")\nBoston.columns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIndex(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n       'ptratio', 'lstat', 'medv'],\n      dtype='object')\n```\n\n\n:::\n:::\n\n\n\n    \nType `Boston?` to find out more about these data.\n\nWe start by using the `sm.OLS()`  function to fit a\nsimple linear regression model.  Our response will be\n `medv`  and  `lstat`  will be the single predictor.\nFor this model, we can create the model matrix by hand.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX = pd.DataFrame({'intercept': np.ones(Boston.shape[0]),\n                  'lstat': Boston['lstat']})\nX[:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   intercept  lstat\n0        1.0   4.98\n1        1.0   9.14\n2        1.0   4.03\n3        1.0   2.94\n```\n\n\n:::\n:::\n\n\n\n    \nWe extract the response, and fit the model.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ny = Boston['medv']\nmodel = sm.OLS(y, X)\nresults = model.fit()\n```\n:::\n\n\n\nNote that `sm.OLS()` does\nnot fit the model; it specifies the model, and then `model.fit()` does the actual fitting.  \n\nOur `ISLP` function `summarize()` produces a simple table of the parameter estimates,\ntheir standard errors, t-statistics and p-values.\nThe function takes a single argument, such as the object `results` \nreturned here by the `fit`\nmethod, and returns such a summary.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsummarize(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              coef  std err       t  P>|t|\nintercept  34.5538    0.563  61.415    0.0\nlstat      -0.9500    0.039 -24.528    0.0\n```\n\n\n:::\n:::\n\n\n\n    \n\nBefore we describe other methods for working with fitted models, we outline a more useful and general framework for constructing a model matrix~`X`.\n### Using Transformations: Fit and Transform\nOur model above has a single predictor, and constructing `X` was straightforward. \nIn practice  we often fit models with more than one predictor, typically selected from an array or data frame.\nWe may wish to introduce transformations to the variables before fitting the model, specify interactions between variables, and expand some particular variables into sets of variables (e.g. polynomials).\nThe `sklearn`  package has a particular notion\nfor this type of task: a *transform*. A transform is an object\nthat is created with some parameters as arguments. The\nobject has two main methods: `fit()` and `transform()`.\n\nWe provide a general approach for specifying models and constructing\nthe model matrix through the transform `ModelSpec()` in the `ISLP` library.\n`ModelSpec()`\n(renamed `MS()` in the preamble) creates a\ntransform object, and then a pair of methods\n`transform()` and `fit()` are used to construct a\ncorresponding model matrix.\n\nWe first describe this process for our simple regression model  using a single predictor `lstat` in\nthe `Boston` data frame, but will use it repeatedly in more\ncomplex tasks in this and other labs in this book.\nIn our case the transform is created by the expression\n`design = MS(['lstat'])`.\n\nThe `fit()`  method takes the original array and may do some\ninitial computations on it, as specified in the transform object.\nFor example, it may compute means and standard deviations for centering and scaling.\nThe `transform()` \nmethod applies the fitted transformation to the array of data, and produces the model matrix.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndesign = MS(['lstat'])\ndesign = design.fit(Boston)\nX = design.transform(Boston)\nX[:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   intercept  lstat\n0        1.0   4.98\n1        1.0   9.14\n2        1.0   4.03\n3        1.0   2.94\n```\n\n\n:::\n:::\n\n\n\nIn this simple case, the `fit()`  method does very little; it simply checks that the variable `'lstat'` specified in `design` exists in `Boston`. Then `transform()` constructs the model matrix with two columns: an `intercept` and the variable `lstat`.\n\nThese two operations can be  combined with the\n`fit_transform()`  method.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndesign = MS(['lstat'])\nX = design.fit_transform(Boston)\nX[:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   intercept  lstat\n0        1.0   4.98\n1        1.0   9.14\n2        1.0   4.03\n3        1.0   2.94\n```\n\n\n:::\n:::\n\n\n\nNote that, as in the previous code chunk when the two steps were done separately, the `design` object is changed as a result of the `fit()` operation. The power of this pipeline will become clearer when we fit more complex models that involve interactions and transformations.\n\n\nLet's return to our fitted regression model.\nThe object\n`results` has several methods that can be used for inference.\nWe already presented a function `summarize()` for showing the essentials of the fit.\nFor a full and somewhat exhaustive summary of the fit, we can use the `summary()` \nmethod.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresults.summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.544</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.543</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   601.6</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Tue, 14 Jan 2025</td> <th>  Prob (F-statistic):</th> <td>5.08e-88</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:56:20</td>     <th>  Log-Likelihood:    </th> <td> -1641.5</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3287.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   504</td>      <th>  BIC:               </th> <td>   3295.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>intercept</th> <td>   34.5538</td> <td>    0.563</td> <td>   61.415</td> <td> 0.000</td> <td>   33.448</td> <td>   35.659</td>\n</tr>\n<tr>\n  <th>lstat</th>     <td>   -0.9500</td> <td>    0.039</td> <td>  -24.528</td> <td> 0.000</td> <td>   -1.026</td> <td>   -0.874</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>137.043</td> <th>  Durbin-Watson:     </th> <td>   0.892</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 291.373</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 1.453</td>  <th>  Prob(JB):          </th> <td>5.36e-64</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.319</td>  <th>  Cond. No.          </th> <td>    29.7</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n:::\n:::\n\n\n\n\nThe fitted coefficients can also be retrieved as the\n`params` attribute of `results`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nresults.params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nintercept    34.553841\nlstat        -0.950049\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n    \n \nThe `get_prediction()`  method can be used to obtain predictions, and produce confidence intervals and\nprediction intervals for the prediction of  `medv`  for  given values of  `lstat`.\n\nWe first create a new data frame, in this case containing only the variable `lstat`, with the values for this variable at which we wish to make predictions.\nWe then use the `transform()` method of `design` to create the corresponding model matrix.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})\nnewX = design.transform(new_df)\nnewX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   intercept  lstat\n0        1.0      5\n1        1.0     10\n2        1.0     15\n```\n\n\n:::\n:::\n\n\n\n\nNext we compute the predictions at `newX`, and view them by extracting the `predicted_mean` attribute.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnew_predictions = results.get_prediction(newX);\nnew_predictions.predicted_mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([29.80359411, 25.05334734, 20.30310057])\n```\n\n\n:::\n:::\n\n\n\nWe can produce confidence intervals for the predicted values.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnew_predictions.conf_int(alpha=0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[29.00741194, 30.59977628],\n       [24.47413202, 25.63256267],\n       [19.73158815, 20.87461299]])\n```\n\n\n:::\n:::\n\n\n\nPrediction intervals are computing by setting `obs=True`:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnew_predictions.conf_int(obs=True, alpha=0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[17.56567478, 42.04151344],\n       [12.82762635, 37.27906833],\n       [ 8.0777421 , 32.52845905]])\n```\n\n\n:::\n:::\n\n\n\n For instance, the 95% confidence interval associated with an\n `lstat`  value of 10 is (24.47, 25.63), and the 95% prediction\ninterval is (12.82, 37.28).  As expected, the confidence and\nprediction intervals are centered around the same point (a predicted\nvalue of 25.05 for  `medv`  when  `lstat`  equals\n10), but the latter are substantially wider.\n\nNext we will plot  `medv`  and  `lstat` \nusing `DataFrame.plot.scatter()`, \\definelongblankMR{plot.scatter()}{plot.slashslashscatter()}\nand wish to\nadd the regression line to the resulting plot.\n\n\n### Defining Functions\nWhile there is a function\nwithin the `ISLP` package that adds a line to an existing plot, we take this opportunity\nto define our first function to do so.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef abline(ax, b, m):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim)\n```\n:::\n\n\n\n A few things are illustrated above. First we see the syntax for defining a function:\n`def funcname(...)`. The function has arguments `ax, b, m`\nwhere `ax` is an axis object for an exisiting plot, `b` is the intercept and\n`m` is the slope of the desired line. Other plotting  options can be passed on to\n`ax.plot` by including additional optional arguments as follows:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef abline(ax, b, m, *args, **kwargs):\n    \"Add a line with slope m and intercept b to ax\"\n    xlim = ax.get_xlim()\n    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n    ax.plot(xlim, ylim, *args, **kwargs)\n```\n:::\n\n\n\nThe addition of `*args` allows any number of\nnon-named arguments to `abline`, while `*kwargs` allows any\nnumber of named arguments (such as `linewidth=3`) to `abline`.\nIn our function, we pass\nthese arguments verbatim to `ax.plot` above. Readers\ninterested in learning more about\nfunctions are referred to the section on\ndefining functions in [docs.python.org/tutorial](https://docs.python.org/3/tutorial/controlflow.html#defining-functions).\n\nLet’s use our new function to add this regression line to a plot of\n`medv` vs. `lstat`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nax = Boston.plot.scatter('lstat', 'medv')\nabline(ax,\n       results.params[0],\n       results.params[1],\n       'r--',\n       linewidth=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<string>:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n<string>:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Ch03-linreg-lab_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\nThus, the final call to `ax.plot()` is `ax.plot(xlim, ylim, 'r--', linewidth=3)`.\nWe have used the argument `'r--'` to produce a red dashed line, and added\nan argument to make it of width 3.\nThere is some evidence for non-linearity in the relationship between  `lstat`  and  `medv`. We will explore this issue later in this lab.\n\nAs mentioned above, there is an existing function to add a line to a plot --- `ax.axline()` --- but knowing how to write such functions empowers us to create more expressive displays.\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed\nin Section~\\ref{Ch3:problems.sec}.\nWe can find the fitted values and residuals\nof the fit as attributes of the `results` object.\nVarious influence measures describing the regression model\nare computed with the `get_influence()` method.\nAs we will not use the `fig` component returned\nas the first value from `subplots()`, we simply\ncapture the second returned value in `ax` below.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nax = subplots(figsize=(8,8))[1]\nax.scatter(results.fittedvalues, results.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n```\n\n::: {.cell-output-display}\n![](Ch03-linreg-lab_files/figure-html/unnamed-chunk-23-3.png){width=768}\n:::\n:::\n\n\n\n We add a horizontal line at 0 for reference using the\n `ax.axhline()`   method, indicating\nit should be black (`c='k'`) and have a dashed linestyle (`ls='--'`).\n\nOn the basis of the residual plot, there is some evidence of non-linearity.\nLeverage statistics can be computed for any number of predictors using the\n`hat_matrix_diag` attribute of the value returned by the\n`get_influence()`  method.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ninfl = results.get_influence()\nax = subplots(figsize=(8,8))[1]\nax.scatter(np.arange(X.shape[0]), infl.hat_matrix_diag)\nax.set_xlabel('Index')\nax.set_ylabel('Leverage')\nnp.argmax(infl.hat_matrix_diag)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n374\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Ch03-linreg-lab_files/figure-html/unnamed-chunk-24-5.png){width=768}\n:::\n:::\n\n\n\n The `np.argmax()`  function identifies the index of the largest element of an array, optionally computed over an axis of the array.\nIn this case, we maximized over the entire array\nto determine which observation has the largest leverage statistic.\n\n\n## Multiple Linear Regression\nIn order to fit a multiple linear regression model using least squares, we again use\nthe `ModelSpec()`  transform to construct the required\nmodel matrix and response. The arguments\nto `ModelSpec()` can be quite general, but in this case\na list of column names suffice. We consider a fit here with\nthe two variables `lstat` and `age`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX = MS(['lstat', 'age']).fit_transform(Boston)\nmodel1 = sm.OLS(y, X)\nresults1 = model1.fit()\nsummarize(results1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              coef  std err       t  P>|t|\nintercept  33.2228    0.731  45.458  0.000\nlstat      -1.0321    0.048 -21.416  0.000\nage         0.0345    0.012   2.826  0.005\n```\n\n\n:::\n:::\n\n\n\nNotice how we have compacted the first line into a succinct expression describing the construction of `X`.\n\nThe  `Boston`   data set contains 12 variables, and so it would be cumbersome\nto have to type all of these in order to perform a regression using all of the predictors.\nInstead, we can use the following short-hand:\\definelongblankMR{columns.drop()}{columns.slashslashdrop()}\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nterms = Boston.columns.drop('medv')\nterms\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIndex(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n       'ptratio', 'lstat'],\n      dtype='object')\n```\n\n\n:::\n:::\n\n\n\n \nWe can now fit the model with all the variables in `terms` using\nthe same model matrix builder.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX = MS(terms).fit_transform(Boston)\nmodel = sm.OLS(y, X)\nresults = model.fit()\nsummarize(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              coef  std err       t  P>|t|\nintercept  41.6173    4.936   8.431  0.000\ncrim       -0.1214    0.033  -3.678  0.000\nzn          0.0470    0.014   3.384  0.001\nindus       0.0135    0.062   0.217  0.829\nchas        2.8400    0.870   3.264  0.001\nnox       -18.7580    3.851  -4.870  0.000\nrm          3.6581    0.420   8.705  0.000\nage         0.0036    0.013   0.271  0.787\ndis        -1.4908    0.202  -7.394  0.000\nrad         0.2894    0.067   4.325  0.000\ntax        -0.0127    0.004  -3.337  0.001\nptratio    -0.9375    0.132  -7.091  0.000\nlstat      -0.5520    0.051 -10.897  0.000\n```\n\n\n:::\n:::\n\n\n\n \nWhat if we would like to perform a regression using all of the variables but one?  For\nexample, in the above regression output,   `age`  has a high $p$-value.\nSo we may wish to run a regression excluding this predictor.\nThe following syntax results in a regression using all predictors except  `age`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nminus_age = Boston.columns.drop(['medv', 'age']) \nXma = MS(minus_age).fit_transform(Boston)\nmodel1 = sm.OLS(y, Xma)\nsummarize(model1.fit())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              coef  std err       t  P>|t|\nintercept  41.5251    4.920   8.441  0.000\ncrim       -0.1214    0.033  -3.683  0.000\nzn          0.0465    0.014   3.379  0.001\nindus       0.0135    0.062   0.217  0.829\nchas        2.8528    0.868   3.287  0.001\nnox       -18.4851    3.714  -4.978  0.000\nrm          3.6811    0.411   8.951  0.000\ndis        -1.5068    0.193  -7.825  0.000\nrad         0.2879    0.067   4.322  0.000\ntax        -0.0127    0.004  -3.333  0.001\nptratio    -0.9346    0.132  -7.099  0.000\nlstat      -0.5474    0.048 -11.483  0.000\n```\n\n\n:::\n:::\n\n\n\n\n## Multivariate Goodness of Fit\nWe can access the individual components of `results` by name\n(`dir(results)` shows us what is available). Hence\n`results.rsquared` gives us the $R^2$,\nand\n`np.sqrt(results.scale)` gives us the RSE.\n\nVariance inflation factors (section~\\ref{Ch3:problems.sec}) are sometimes useful\nto assess the effect of collinearity in the model matrix of a regression model.\nWe will compute the VIFs in our multiple regression fit, and use the opportunity to introduce the idea of *list comprehension*.\n\n### List Comprehension\nOften we encounter a sequence of objects which we would like to transform\nfor some other task. Below, we compute the VIF for each\nfeature in our `X` matrix and produce a data frame\nwhose index agrees with the columns of `X`.\nThe notion of list comprehension can often make such\na task easier.\n\nList comprehensions are simple and powerful ways to form\nlists of `Python` objects. The language also supports\ndictionary and *generator* comprehension, though these are\nbeyond our scope here. Let's look at an example. We compute the VIF for each of the variables\nin the model matrix `X`, using the function `variance_inflation_factor()`.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nvals = [VIF(X, i)\n        for i in range(1, X.shape[1])]\nvif = pd.DataFrame({'vif':vals},\n                   index=X.columns[1:])\nvif\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              vif\ncrim     1.767486\nzn       2.298459\nindus    3.987181\nchas     1.071168\nnox      4.369093\nrm       1.912532\nage      3.088232\ndis      3.954037\nrad      7.445301\ntax      9.002158\nptratio  1.797060\nlstat    2.870777\n```\n\n\n:::\n:::\n\n\n\nThe function `VIF()` takes two arguments: a dataframe or array,\nand a variable column index. In the code above we call `VIF()` on the fly for all columns in `X`.  \nWe have excluded column 0 above (the intercept), which is not of interest. In this case the VIFs are not that exciting.\n\nThe object `vals` above could have been constructed with the following for loop:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nvals = []\nfor i in range(1, X.values.shape[1]):\n    vals.append(VIF(X.values, i))\n```\n:::\n\n\n\nList comprehension allows us to perform such repetitive operations in a more straightforward way.\n## Interaction Terms\nIt is easy to include interaction terms in a linear model using `ModelSpec()`.\nIncluding a tuple `(\"lstat\",\"age\")` tells the model\nmatrix builder to include an interaction term between\n `lstat`  and  `age`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX = MS(['lstat',\n        'age',\n        ('lstat', 'age')]).fit_transform(Boston)\nmodel2 = sm.OLS(y, X)\nsummarize(model2.fit())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              coef  std err       t  P>|t|\nintercept  36.0885    1.470  24.553  0.000\nlstat      -1.3921    0.167  -8.313  0.000\nage        -0.0007    0.020  -0.036  0.971\nlstat:age   0.0042    0.002   2.244  0.025\n```\n\n\n:::\n:::\n\n\n\n \n\n## Non-linear Transformations of the Predictors\nThe model matrix builder can include terms beyond\njust column names and interactions. For instance,\nthe `poly()` function supplied in `ISLP` specifies that\ncolumns representing polynomial functions\nof its first argument are added to the model matrix.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston)\nmodel3 = sm.OLS(y, X)\nresults3 = model3.fit()\nsummarize(results3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                              coef  std err       t  P>|t|\nintercept                  17.7151    0.781  22.681    0.0\npoly(lstat, degree=2)[0] -179.2279    6.733 -26.620    0.0\npoly(lstat, degree=2)[1]   72.9908    5.482  13.315    0.0\nage                         0.0703    0.011   6.471    0.0\n```\n\n\n:::\n:::\n\n\n\nThe effectively zero *p*-value associated with the quadratic term\n(i.e. the third row above) suggests that it leads to an improved model.\n\nBy default, `poly()` creates a basis matrix for inclusion in the\nmodel matrix whose\ncolumns are *orthogonal polynomials*, which are designed for stable\nleast squares computations. {Actually, `poly()` is a  wrapper for the workhorse and standalone  function `Poly()` that does the  work in building the model matrix.}\nAlternatively, had we included an argument\n`raw=True` in the above call to `poly()`, the basis matrix would consist simply of\n`lstat` and `lstat**2`. Since either of these bases\nrepresent quadratic polynomials, the fitted values  would not\nchange in this case, just the polynomial coefficients.  Also by default, the columns\ncreated by `poly()` do not include an intercept column as\nthat is automatically added by `MS()`.\n\nWe use the `anova_lm()` function to further quantify the extent to which the quadratic fit is\nsuperior to the linear fit.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nanova_lm(results1, results3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   df_resid           ssr  df_diff      ss_diff           F        Pr(>F)\n0     503.0  19168.128609      0.0          NaN         NaN           NaN\n1     502.0  14165.613251      1.0  5002.515357  177.278785  7.468491e-35\n```\n\n\n:::\n:::\n\n\n\nHere `results1` represents the linear submodel containing\npredictors `lstat` and `age`,\nwhile `results3` corresponds to the larger model above  with a quadratic\nterm in `lstat`.\nThe `anova_lm()` function performs a hypothesis test\ncomparing the two models. The null hypothesis is that the quadratic\nterm in the bigger model is not needed, and the alternative hypothesis is that the\nbigger model is superior. Here the *F*-statistic is 177.28 and\nthe associated *p*-value is zero.\nIn this case the *F*-statistic is the square of the\n*t*-statistic for the quadratic term in the linear model summary\nfor `results3` --- a consequence of the fact that these nested\nmodels differ by one degree of freedom.\nThis provides very clear evidence that the quadratic polynomial in\n`lstat` improves the linear model.\nThis is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv`\nand  `lstat`.\n\nThe function `anova_lm()` can take more than two nested models\nas input, in which case it compares every successive pair of models.\nThat also explains why their are `NaN`s in the first row above, since\nthere is no previous model with which to compare the first.\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nax = subplots(figsize=(8,8))[1]\nax.scatter(results3.fittedvalues, results3.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n```\n\n::: {.cell-output-display}\n![](Ch03-linreg-lab_files/figure-html/unnamed-chunk-34-7.png){width=768}\n:::\n:::\n\n\n\nWe see that when the quadratic term is included in the model,\nthere is little discernible pattern in the residuals.\nIn order to create a cubic or higher-degree polynomial fit, we can simply change the degree argument\nto `poly()`.\n\n \n\n## Qualitative Predictors\nHere we use the  `Carseats`  data, which is included in the\n`ISLP` package. We will  attempt to predict `Sales`\n(child car seat sales) in 400 locations based on a number of\npredictors.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nCarseats = load_data('Carseats')\nCarseats.columns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIndex(['Sales', 'CompPrice', 'Income', 'Advertising', 'Population', 'Price',\n       'ShelveLoc', 'Age', 'Education', 'Urban', 'US'],\n      dtype='object')\n```\n\n\n:::\n:::\n\n\n\nThe `Carseats`  \n data includes qualitative predictors such as\n `ShelveLoc`, an indicator of the quality of the shelving\n location --- that is,\nthe  space within a store in which the car seat is displayed. The predictor\n `ShelveLoc`  takes on three possible values, `Bad`, `Medium`, and `Good`.\nGiven a qualitative variable such as  `ShelveLoc`, `ModelSpec()` generates dummy\nvariables automatically.\nThese variables are often referred to as a *one-hot encoding* of the categorical\nfeature. Their columns sum to one, so to avoid collinearity with an intercept, the first column is dropped. Below we see\nthe column `ShelveLoc[Bad]` has been dropped, since `Bad` is the first level of `ShelveLoc`.\nBelow we fit a multiple regression model that includes some interaction terms.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nallvars = list(Carseats.columns.drop('Sales'))\ny = Carseats['Sales']\nfinal = allvars + [('Income', 'Advertising'),\n                   ('Price', 'Age')]\nX = MS(final).fit_transform(Carseats)\nmodel = sm.OLS(y, X)\nsummarize(model.fit())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      coef  std err       t  P>|t|\nintercept           6.5756    1.009   6.519  0.000\nCompPrice           0.0929    0.004  22.567  0.000\nIncome              0.0109    0.003   4.183  0.000\nAdvertising         0.0702    0.023   3.107  0.002\nPopulation          0.0002    0.000   0.433  0.665\nPrice              -0.1008    0.007 -13.549  0.000\nShelveLoc[Good]     4.8487    0.153  31.724  0.000\nShelveLoc[Medium]   1.9533    0.126  15.531  0.000\nAge                -0.0579    0.016  -3.633  0.000\nEducation          -0.0209    0.020  -1.063  0.288\nUrban[Yes]          0.1402    0.112   1.247  0.213\nUS[Yes]            -0.1576    0.149  -1.058  0.291\nIncome:Advertising  0.0008    0.000   2.698  0.007\nPrice:Age           0.0001    0.000   0.801  0.424\n```\n\n\n:::\n:::\n\n\n\nIn the first line above, we made `allvars` a list, so that we\ncould add the interaction terms two lines down. \nOur model-matrix builder has created a `ShelveLoc[Good]`\ndummy variable that takes on a value of 1 if the\nshelving location is good, and 0 otherwise. It has also created a `ShelveLoc[Medium]`\ndummy variable that equals 1 if the shelving location is medium, and 0 otherwise.\nA bad shelving location corresponds to a zero for each of the two dummy variables.\nThe fact that the coefficient for `ShelveLoc[Good]` in the regression output is\npositive indicates that a good shelving location is associated with high sales (relative to a bad location).\nAnd `ShelveLoc[Medium]` has a smaller positive coefficient,\nindicating that a medium shelving location leads to higher sales than a bad\nshelving location, but lower sales than a good shelving location.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}