{
  "hash": "3cc8bf42885d63bdba22bda9de866f85",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n# Cross-Validation and the Bootstrap\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/intro-stat-learning/ISLP_labs/blob/v2.2/Ch05-resample-lab.ipynb\">\n<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/intro-stat-learning/ISLP_labs/v2.2?labpath=Ch05-resample-lab.ipynb)\n\n\nIn this lab, we explore the resampling techniques covered in this\nchapter. Some of the commands in this lab may take a while to run on\nyour computer.\n\nWe again begin by placing most of our imports at this top level.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport statsmodels.api as sm\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,\n                         summarize,\n                         poly)\nfrom sklearn.model_selection import train_test_split\n```\n:::\n\n\n\n\n\nThere are several new imports needed for this lab.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom functools import partial\nfrom sklearn.model_selection import \\\n     (cross_validate,\n      KFold,\n      ShuffleSplit)\nfrom sklearn.base import clone\nfrom ISLP.models import sklearn_sm\n```\n:::\n\n\n\n\n\n## The Validation Set Approach\nWe explore the use of the validation set approach in order to estimate\nthe test error rates that result from fitting various linear models on\nthe  `Auto`  data set.\n\nWe use the function `train_test_split()` to split\nthe data into training and validation sets. As there are 392 observations,\nwe split into two equal sets of size 196 using the\nargument `test_size=196`. It is generally a good idea to set a random seed\nwhen performing operations like this that contain an\nelement of randomness, so that the results obtained can be reproduced\nprecisely at a later time. We set the random seed of the splitter\nwith the argument `random_state=0`. \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nAuto = load_data('Auto')\nAuto_train, Auto_valid = train_test_split(Auto,\n                                         test_size=196,\n                                         random_state=0)\n```\n:::\n\n\n\n\nNow we can fit a linear regression using only the observations corresponding to the training set `Auto_train`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nhp_mm = MS(['horsepower'])\nX_train = hp_mm.fit_transform(Auto_train)\ny_train = Auto_train['mpg']\nmodel = sm.OLS(y_train, X_train)\nresults = model.fit()\n```\n:::\n\n\n\n\nWe now use the `predict()` method of `results` evaluated on the model matrix for this model\ncreated using the validation data set. We also calculate the validation MSE of our model.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_valid = hp_mm.transform(Auto_valid)\ny_valid = Auto_valid['mpg']\nvalid_pred = results.predict(X_valid)\nnp.mean((y_valid - valid_pred)**2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n23.61661706966988\n```\n\n\n:::\n:::\n\n\n\n\nHence our estimate for the validation MSE of  the linear regression\nfit is $23.62$.\n\nWe can also estimate the validation error for\nhigher-degree polynomial regressions. We first provide a function `evalMSE()` that takes a model string as well\nas a training and test set and returns the MSE on the test set.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef evalMSE(terms,\n            response,\n            train,\n            test):\n\n   mm = MS(terms)\n   X_train = mm.fit_transform(train)\n   y_train = train[response]\n\n   X_test = mm.transform(test)\n   y_test = test[response]\n\n   results = sm.OLS(y_train, X_train).fit()\n   test_pred = results.predict(X_test)\n\n   return np.mean((y_test - test_pred)**2)\n```\n:::\n\n\n\n\nLet’s use this function to estimate the validation MSE\nusing linear, quadratic and cubic fits. We use the `enumerate()`  function\nhere, which gives both the values and indices of objects as one iterates\nover a for loop.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nMSE = np.zeros(3)\nfor idx, degree in enumerate(range(1, 4)):\n    MSE[idx] = evalMSE([poly('horsepower', degree)],\n                       'mpg',\n                       Auto_train,\n                       Auto_valid)\nMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([23.61661707, 18.76303135, 18.79694163])\n```\n\n\n:::\n:::\n\n\n\n\nThese error rates are $23.62, 18.76$, and $18.80$, respectively. If we\nchoose a different training/validation split instead, then we\ncan expect somewhat different errors on the validation set.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nAuto_train, Auto_valid = train_test_split(Auto,\n                                          test_size=196,\n                                          random_state=3)\nMSE = np.zeros(3)\nfor idx, degree in enumerate(range(1, 4)):\n    MSE[idx] = evalMSE([poly('horsepower', degree)],\n                       'mpg',\n                       Auto_train,\n                       Auto_valid)\nMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([20.75540796, 16.94510676, 16.97437833])\n```\n\n\n:::\n:::\n\n\n\n\nUsing this split of the observations into a training set and a validation set,\nwe find that the validation set error rates for the models with linear, quadratic, and cubic terms are $20.76$, $16.95$, and $16.97$, respectively.\n\nThese results are consistent with our previous findings: a model that\npredicts `mpg` using a quadratic function of `horsepower`\nperforms better than a model that involves only a linear function of\n`horsepower`, and there is no evidence of an improvement in using a cubic function of `horsepower`.\n\n\n## Cross-Validation\nIn theory, the cross-validation estimate can be computed for any generalized\nlinear model.  {}\nIn practice, however, the simplest way to cross-validate in\nPython is to use `sklearn`, which has a different interface or API\nthan `statsmodels`, the code we have been using to fit GLMs.\n\nThis is a problem which often confronts data scientists: \"I have a function to do task $A$, and need to feed it into something that performs task $B$, so that I can compute $B(A(D))$, where $D$ is my data.\" When $A$ and $B$ don’t naturally speak to each other, this\nrequires the use of a *wrapper*.\nIn the `ISLP` package,\nwe provide \na wrapper, `sklearn_sm()`, that enables us to easily use the cross-validation tools of `sklearn` with\nmodels fit by `statsmodels`.\n\nThe class `sklearn_sm()` \nhas  as its first argument\na model from `statsmodels`. It can take two additional\noptional arguments: `model_str` which can be\nused to specify a formula, and `model_args` which should\nbe a dictionary of additional arguments used when fitting\nthe model. For example, to fit a logistic regression model\nwe have to specify a `family` argument. This\nis passed as `model_args={'family':sm.families.Binomial()}`.\n\nHere is our wrapper in action:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nhp_model = sklearn_sm(sm.OLS,\n                      MS(['horsepower']))\nX, Y = Auto.drop(columns=['mpg']), Auto['mpg']\ncv_results = cross_validate(hp_model,\n                            X,\n                            Y,\n                            cv=Auto.shape[0])\ncv_err = np.mean(cv_results['test_score'])\ncv_err\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n24.23151351792922\n```\n\n\n:::\n:::\n\n\n\nThe arguments to `cross_validate()` are as follows: an\nobject with the appropriate `fit()`, `predict()`,\nand `score()` methods,  an\narray of features `X` and a response `Y`. \nWe also included an additional argument `cv` to `cross_validate()`; specifying an integer\n$K$ results in $K$-fold cross-validation. We have provided a value \ncorresponding to the total number of observations, which results in\nleave-one-out cross-validation (LOOCV). The `cross_validate()`  function produces a dictionary with several components;\nwe simply want the cross-validated test score here (MSE), which is estimated to be 24.23.\n\n\nWe can repeat this procedure for increasingly complex polynomial fits.\nTo automate the process, we again\nuse a for loop which iteratively fits polynomial\nregressions of degree 1 to 5, computes the\nassociated cross-validation error, and stores it in the $i$th element\nof the vector `cv_error`. The variable `d` in the for loop\ncorresponds to the degree of the polynomial. We begin by initializing the\nvector. This command may take a couple of seconds to run.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncv_error = np.zeros(5)\nH = np.array(Auto['horsepower'])\nM = sklearn_sm(sm.OLS)\nfor i, d in enumerate(range(1,6)):\n    X = np.power.outer(H, np.arange(d+1))\n    M_CV = cross_validate(M,\n                          X,\n                          Y,\n                          cv=Auto.shape[0])\n    cv_error[i] = np.mean(M_CV['test_score'])\ncv_error\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([24.23151352, 19.24821312, 19.33498406, 19.42443031, 19.03321178])\n```\n\n\n:::\n:::\n\n\n\nAs in Figure~\\ref{Ch5:cvplot}, we see a sharp drop in the estimated test MSE between the linear and\nquadratic fits, but then no clear improvement from using higher-degree polynomials.\n\nAbove we introduced the `outer()`  method of the `np.power()`\nfunction.  The `outer()` method is applied to an operation\nthat has two arguments, such as `add()`, `min()`, or\n`power()`.\nIt has two arrays as\narguments, and then forms a larger\narray where the operation is applied to each pair of elements of the\ntwo arrays. \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nA = np.array([3, 5, 9])\nB = np.array([2, 4])\nnp.add.outer(A, B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[ 5,  7],\n       [ 7,  9],\n       [11, 13]])\n```\n\n\n:::\n:::\n\n\n\n\nIn the CV example above, we used $K=n$, but of course we can also use $K<n$. The code is very similar\nto the above (and is significantly faster). Here we use `KFold()` to partition the data into $K=10$ random groups. We use `random_state` to set a random seed and initialize a vector `cv_error` in which we will store the CV errors corresponding to the\npolynomial fits of degrees one to five.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncv_error = np.zeros(5)\ncv = KFold(n_splits=10,\n           shuffle=True,\n           random_state=0) # use same splits for each degree\nfor i, d in enumerate(range(1,6)):\n    X = np.power.outer(H, np.arange(d+1))\n    M_CV = cross_validate(M,\n                          X,\n                          Y,\n                          cv=cv)\n    cv_error[i] = np.mean(M_CV['test_score'])\ncv_error\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([24.20766449, 19.18533142, 19.27626666, 19.47848402, 19.13720959])\n```\n\n\n:::\n:::\n\n\n\nNotice that the computation time is much shorter than that of LOOCV.\n(In principle, the computation time for LOOCV for a least squares\nlinear model should be faster than for $K$-fold CV, due to the\navailability of the formula~(\\ref{Ch5:eq:LOOCVform})  for LOOCV;\nhowever, the generic `cross_validate()`  function does not make\nuse of this formula.)  We still see little evidence that using cubic\nor higher-degree polynomial terms leads to a lower test error than simply\nusing a quadratic fit.\n\n\nThe `cross_validate()` function is flexible and can take\ndifferent splitting mechanisms as an argument. For instance, one can use the `ShuffleSplit()` funtion to implement\nthe validation set approach just as easily as K-fold cross-validation.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nvalidation = ShuffleSplit(n_splits=1,\n                          test_size=196,\n                          random_state=0)\nresults = cross_validate(hp_model,\n                         Auto.drop(['mpg'], axis=1),\n                         Auto['mpg'],\n                         cv=validation);\nresults['test_score']\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([23.61661707])\n```\n\n\n:::\n:::\n\n\n\n\n\nOne can estimate the variability in the test error by running the following:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nvalidation = ShuffleSplit(n_splits=10,\n                          test_size=196,\n                          random_state=0)\nresults = cross_validate(hp_model,\n                         Auto.drop(['mpg'], axis=1),\n                         Auto['mpg'],\n                         cv=validation)\nresults['test_score'].mean(), results['test_score'].std()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(23.802232661034168, 1.4218450941091842)\n```\n\n\n:::\n:::\n\n\n\n\nNote that this standard deviation is not a valid estimate of the\nsampling variability of the mean test score or the individual scores, since the randomly-selected training\nsamples overlap and hence introduce correlations. But it does give an\nidea of the Monte Carlo variation\nincurred by picking different random folds.\n\n## The Bootstrap\nWe illustrate the use of the bootstrap in the simple example\n {of Section~\\ref{Ch5:sec:bootstrap},}  as well as on an example involving\nestimating the accuracy of the linear regression model on the  `Auto`\ndata set.\n### Estimating the Accuracy of a Statistic of Interest\nOne of the great advantages of the bootstrap approach is that it can\nbe applied in almost all situations. No complicated mathematical\ncalculations are required. While there are several implementations\nof the bootstrap in Python, its use for estimating\nstandard error is simple enough that we write our own function\nbelow for the case when our data is stored\nin a dataframe.\n\nTo illustrate the bootstrap, we\nstart with a simple example.\nThe  `Portfolio`  data set in the `ISLP` package is described\nin Section~\\ref{Ch5:sec:bootstrap}. The goal is to estimate the\nsampling variance of the parameter $\\alpha$ given in formula~(\\ref{Ch5:min.var}).  We will\ncreate a function\n`alpha_func()`, which takes as input a dataframe `D` assumed\nto have columns `X` and `Y`, as well as a\nvector `idx` indicating which observations should be used to\nestimate \n$\\alpha$. The function then outputs the estimate for $\\alpha$ based on\nthe selected observations.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nPortfolio = load_data('Portfolio')\ndef alpha_func(D, idx):\n   cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n   return ((cov_[1,1] - cov_[0,1]) /\n           (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))\n```\n:::\n\n\n\nThis function returns an estimate for $\\alpha$\nbased on applying the minimum\n    variance formula (\\ref{Ch5:min.var}) to the observations indexed by\nthe argument `idx`.  For instance, the following command\nestimates $\\alpha$ using all 100 observations.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nalpha_func(Portfolio, range(100))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.57583207459283\n```\n\n\n:::\n:::\n\n\n\n\nNext we randomly select\n100 observations from `range(100)`, with replacement. This is equivalent\nto constructing a new bootstrap data set and recomputing $\\hat{\\alpha}$\nbased on the new data set.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrng = np.random.default_rng(0)\nalpha_func(Portfolio,\n           rng.choice(100,\n                      100,\n                      replace=True))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.6074452469619004\n```\n\n\n:::\n:::\n\n\n\n\n\nThis process can be generalized to create a simple function `boot_SE()` for\ncomputing the bootstrap standard error for arbitrary\nfunctions that take only a data frame as an argument.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef boot_SE(func,\n            D,\n            n=None,\n            B=1000,\n            seed=0):\n    rng = np.random.default_rng(seed)\n    first_, second_ = 0, 0\n    n = n or D.shape[0]\n    for _ in range(B):\n        idx = rng.choice(D.index,\n                         n,\n                         replace=True)\n        value = func(D, idx)\n        first_ += value\n        second_ += value**2\n    return np.sqrt(second_ / B - (first_ / B)**2)\n```\n:::\n\n\n\nNotice the use of `_` as a loop variable in `for _ in range(B)`. This is often used if the value of the counter is\nunimportant and simply makes sure  the loop is executed `B` times.\n\nLet’s use our function to evaluate the accuracy of our\nestimate of $\\alpha$ using $B=1{,}000$ bootstrap replications. \n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nalpha_SE = boot_SE(alpha_func,\n                   Portfolio,\n                   B=1000,\n                   seed=0)\nalpha_SE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.09118176521277699\n```\n\n\n:::\n:::\n\n\n\n\nThe final output shows that the bootstrap estimate for ${\\rm SE}(\\hat{\\alpha})$ is $0.0912$.\n\n### Estimating the Accuracy of a Linear Regression Model\nThe bootstrap approach can be used to assess the variability of the\ncoefficient estimates and predictions from a statistical learning\nmethod. Here we use the bootstrap approach in order to assess the\nvariability of the estimates for $\\beta_0$ and $\\beta_1$, the\nintercept and slope terms for the linear regression model that uses\n`horsepower` to predict `mpg` in the  `Auto`  data set. We\nwill compare the estimates obtained using the bootstrap to those\nobtained using the formulas for ${\\rm SE}(\\hat{\\beta}_0)$ and\n${\\rm SE}(\\hat{\\beta}_1)$ described in Section~\\ref{Ch3:secoefsec}.\n\nTo use our `boot_SE()` function, we must write a function (its\nfirst argument)\nthat takes a data frame `D` and indices `idx`\nas its only arguments. But here we want to bootstrap a specific\nregression model, specified by a model formula and data. We show how\nto do this in a few simple steps.\n\nWe start by writing a generic\nfunction `boot_OLS()` for bootstrapping a regression model that takes a formula to\ndefine the corresponding regression. We use the `clone()` function to\nmake a copy of the formula that can be refit to the new dataframe. This means\nthat any derived features such as those defined by `poly()`\n(which we will see shortly),\nwill be re-fit on the resampled data frame.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef boot_OLS(model_matrix, response, D, idx):\n    D_ = D.loc[idx]\n    Y_ = D_[response]\n    X_ = clone(model_matrix).fit_transform(D_)\n    return sm.OLS(Y_, X_).fit().params\n```\n:::\n\n\n\nThis is not quite what is needed as the first argument to\n`boot_SE()`. The first two arguments which specify the model will not change in the\nbootstrap process, and we would like to *freeze* them.   The\nfunction `partial()` from the `functools` module  does precisely this: it takes a function\nas an argument, and freezes some of its arguments, starting from the\nleft. We use it to freeze the first two model-formula arguments of `boot_OLS()`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nhp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\n```\n:::\n\n\n\nTyping `hp_func?` will show that it has two arguments `D`\nand `idx` --- it is a version of `boot_OLS()` with the first\ntwo arguments frozen --- and hence is ideal as the first argument for `boot_SE()`.\n\nThe `hp_func()` function can now be used in order to create\nbootstrap estimates for the intercept and slope terms by randomly\nsampling from among the observations with replacement. We first\ndemonstrate its utility on 10 bootstrap samples.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrng = np.random.default_rng(0)\nnp.array([hp_func(Auto,\n          rng.choice(Auto.index,\n                     392,\n                     replace=True)) for _ in range(10)])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[39.12226577, -0.1555926 ],\n       [37.18648613, -0.13915813],\n       [37.46989244, -0.14112749],\n       [38.56723252, -0.14830116],\n       [38.95495707, -0.15315141],\n       [39.12563927, -0.15261044],\n       [38.45763251, -0.14767251],\n       [38.43372587, -0.15019447],\n       [37.87581142, -0.1409544 ],\n       [37.95949036, -0.1451333 ]])\n```\n\n\n:::\n:::\n\n\n\nNext, we use the `boot_SE()` {}  function to compute the standard\nerrors of 1,000 bootstrap estimates for the intercept and slope terms.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nhp_se = boot_SE(hp_func,\n                Auto,\n                B=1000,\n                seed=10)\nhp_se\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nintercept     0.731176\nhorsepower    0.006092\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n\n\nThis indicates that the bootstrap estimate for ${\\rm SE}(\\hat{\\beta}_0)$ is\n0.85, and that the bootstrap\nestimate for ${\\rm SE}(\\hat{\\beta}_1)$ is\n0.0074.  As discussed in\nSection~\\ref{Ch3:secoefsec}, standard formulas can be used to compute\nthe standard errors for the regression coefficients in a linear\nmodel. These can be obtained using the `summarize()`  function\nfrom `ISLP.sm`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nhp_model.fit(Auto, Auto['mpg'])\nmodel_se = summarize(hp_model.results_)['std err']\nmodel_se\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nintercept     0.717\nhorsepower    0.006\nName: std err, dtype: float64\n```\n\n\n:::\n:::\n\n\n\n\n\nThe standard error estimates for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$\nobtained using the formulas  from Section~\\ref{Ch3:secoefsec}  are\n0.717 for the\nintercept and\n0.006 for the\nslope. Interestingly, these are somewhat different from the estimates\nobtained using the bootstrap.  Does this indicate a problem with the\nbootstrap? In fact, it suggests the opposite.  Recall that the\nstandard formulas given in\n {Equation~\\ref{Ch3:se.eqn} on page~\\pageref{Ch3:se.eqn}}\nrely on certain assumptions. For example,\nthey depend on the unknown parameter $\\sigma^2$, the noise\nvariance. We then estimate $\\sigma^2$ using the RSS. Now although the\nformula for the standard errors do not rely on the linear model being\ncorrect, the estimate for $\\sigma^2$ does.  We see\n {in Figure~\\ref{Ch3:polyplot} on page~\\pageref{Ch3:polyplot}}  that there is\na non-linear relationship in the data, and so the residuals from a\nlinear fit will be inflated, and so will $\\hat{\\sigma}^2$.  Secondly,\nthe standard formulas assume (somewhat unrealistically) that the $x_i$\nare fixed, and all the variability comes from the variation in the\nerrors $\\epsilon_i$.  The bootstrap approach does not rely on any of\nthese assumptions, and so it is likely giving a more accurate estimate\nof the standard errors of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ than\nthe results from `sm.OLS`.\n\nBelow we compute the bootstrap standard error estimates and the\nstandard linear regression estimates that result from fitting the\nquadratic model to the data. Since this model provides a good fit to\nthe data (Figure~\\ref{Ch3:polyplot}), there is now a better\ncorrespondence between the bootstrap estimates and the standard\nestimates of ${\\rm SE}(\\hat{\\beta}_0)$, ${\\rm SE}(\\hat{\\beta}_1)$ and\n${\\rm SE}(\\hat{\\beta}_2)$.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nquad_model = MS([poly('horsepower', 2, raw=True)])\nquad_func = partial(boot_OLS,\n                    quad_model,\n                    'mpg')\nboot_SE(quad_func, Auto, B=1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nintercept                                  1.538641\npoly(horsepower, degree=2, raw=True)[0]    0.024696\npoly(horsepower, degree=2, raw=True)[1]    0.000090\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n\nWe  compare the results to the standard errors computed using `sm.OLS()`.\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nM = sm.OLS(Auto['mpg'],\n           quad_model.fit_transform(Auto))\nsummarize(M.fit())['std err']\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nintercept                                  1.800\npoly(horsepower, degree=2, raw=True)[0]    0.031\npoly(horsepower, degree=2, raw=True)[1]    0.000\nName: std err, dtype: float64\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}