{
  "hash": "704193015a0414d0ec11cc3a75ee08e2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Non-Linear Modeling\njupyter:\n  jupytext:\n    cell_metadata_filter: '-all'\n    main_language: python\n    notebook_metadata_filter: '-all'\n  kernelspec:\n    display_name: Python 3\n    language: python\n    name: python3\n---\n\n\n\n\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/intro-stat-learning/ISLP_labs/blob/v2.2/Ch07-nonlin-lab.ipynb\">\n<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/intro-stat-learning/ISLP_labs/v2.2?labpath=Ch07-nonlin-lab.ipynb)\n\n\nIn this lab, we demonstrate some of the nonlinear models discussed in\nthis chapter. We use the `Wage`  data as a running example, and show that many of the complex non-linear fitting procedures discussed can easily be implemented in `Python`.\n\nAs usual, we start with some of our standard imports.\n\n::: {#2a25592f .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:34.911110Z\",\"iopub.status.busy\":\"2024-06-04T23:19:34.910732Z\",\"iopub.status.idle\":\"2024-06-04T23:19:35.700317Z\",\"shell.execute_reply\":\"2024-06-04T23:19:35.700028Z\"}' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np, pandas as pd\nfrom matplotlib.pyplot import subplots\nimport statsmodels.api as sm\nfrom ISLP import load_data\nfrom ISLP.models import (summarize,\n                         poly,\n                         ModelSpec as MS)\nfrom statsmodels.stats.anova import anova_lm\n```\n:::\n\n\nWe again collect the new imports\nneeded for this lab. Many of these are developed specifically for the\n`ISLP` package.\n\n::: {#242dc5cb .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:35.702094Z\",\"iopub.status.busy\":\"2024-06-04T23:19:35.701965Z\",\"iopub.status.idle\":\"2024-06-04T23:19:35.722229Z\",\"shell.execute_reply\":\"2024-06-04T23:19:35.722033Z\"}' execution_count=2}\n``` {.python .cell-code}\nfrom pygam import (s as s_gam,\n                   l as l_gam,\n                   f as f_gam,\n                   LinearGAM,\n                   LogisticGAM)\n\nfrom ISLP.transforms import (BSpline,\n                             NaturalSpline)\nfrom ISLP.models import bs, ns\nfrom ISLP.pygam import (approx_lam,\n                        degrees_of_freedom,\n                        plot as plot_gam,\n                        anova as anova_gam)\n```\n:::\n\n\n## Polynomial Regression and Step Functions\nWe start by demonstrating how Figure~\\ref{Ch7:fig:poly} can be reproduced.\nLet's  begin by loading the data.\n\n::: {#1b9af84b .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:35.723538Z\",\"iopub.status.busy\":\"2024-06-04T23:19:35.723455Z\",\"iopub.status.idle\":\"2024-06-04T23:19:35.730561Z\",\"shell.execute_reply\":\"2024-06-04T23:19:35.730367Z\"}' execution_count=3}\n``` {.python .cell-code}\nWage = load_data('Wage')\ny = Wage['wage']\nage = Wage['age']\n```\n:::\n\n\nThroughout most of this lab, our response is `Wage['wage']`, which\nwe have stored as `y` above. \nAs in Section~\\ref{Ch3-linreg-lab:non-linear-transformations-of-the-predictors}, we will use the `poly()` function to create a model matrix\nthat will fit a $4$th degree polynomial in `age`.\n\n::: {#60856364 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:35.731830Z\",\"iopub.status.busy\":\"2024-06-04T23:19:35.731745Z\",\"iopub.status.idle\":\"2024-06-04T23:19:35.758719Z\",\"shell.execute_reply\":\"2024-06-04T23:19:35.758161Z\"}' execution_count=4}\n``` {.python .cell-code}\npoly_age = MS([poly('age', degree=4)]).fit(Wage)\nM = sm.OLS(y, poly_age.transform(Wage)).fit()\nsummarize(M)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>coef</th>\n      <th>std err</th>\n      <th>t</th>\n      <th>P&gt;|t|</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>intercept</th>\n      <td>111.7036</td>\n      <td>0.729</td>\n      <td>153.283</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[0]</th>\n      <td>447.0679</td>\n      <td>39.915</td>\n      <td>11.201</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[1]</th>\n      <td>-478.3158</td>\n      <td>39.915</td>\n      <td>-11.983</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[2]</th>\n      <td>125.5217</td>\n      <td>39.915</td>\n      <td>3.145</td>\n      <td>0.002</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[3]</th>\n      <td>-77.9112</td>\n      <td>39.915</td>\n      <td>-1.952</td>\n      <td>0.051</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\nThis polynomial is constructed using the function `poly()`,\nwhich creates\na special *transformer* `Poly()` (using `sklearn` terminology\nfor feature transformations such as `PCA()` seen in Section \\ref{Ch6-varselect-lab:principal-components-regression}) which\nallows for easy evaluation of the polynomial at new data points. Here `poly()` is referred to as a *helper* function, and sets up the transformation; `Poly()` is the actual workhorse that computes the transformation. See also \nthe \ndiscussion of transformations on\npage~\\pageref{Ch3-linreg-lab:using-transformations-fit-and-transform}. \n\nIn the code above, the first line executes the `fit()` method\nusing the dataframe\n`Wage`. This recomputes and stores as attributes any parameters needed by `Poly()`\non the training data, and these will be used on all subsequent\nevaluations of the `transform()` method. For example, it is used\non the second line, as well as in the plotting function developed below.\n\n\n    \n\nWe now create a grid of values for `age` at which we want\npredictions.\n\n::: {#19763611 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:35.762173Z\",\"iopub.status.busy\":\"2024-06-04T23:19:35.761857Z\",\"iopub.status.idle\":\"2024-06-04T23:19:35.766441Z\",\"shell.execute_reply\":\"2024-06-04T23:19:35.765894Z\"}' execution_count=5}\n``` {.python .cell-code}\nage_grid = np.linspace(age.min(),\n                       age.max(),\n                       100)\nage_df = pd.DataFrame({'age': age_grid})\n```\n:::\n\n\nFinally, we wish to plot the data and add the fit from the fourth-degree polynomial. As we will make\nseveral similar plots below, we first write a function\nto create all the ingredients and produce the plot.  Our function\ntakes in a model specification (here a basis specified by a\ntransform), as well as a grid of `age` values. The function\nproduces a fitted curve as well as 95% confidence bands. By using\nan argument for `basis` we can produce and plot the results with several different\ntransforms, such as the splines we will see shortly. \n\n::: {#fdb8b018 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:35.771011Z\",\"iopub.status.busy\":\"2024-06-04T23:19:35.770581Z\",\"iopub.status.idle\":\"2024-06-04T23:19:35.776229Z\",\"shell.execute_reply\":\"2024-06-04T23:19:35.775762Z\"}' execution_count=6}\n``` {.python .cell-code}\ndef plot_wage_fit(age_df, \n                  basis,\n                  title):\n\n    X = basis.transform(Wage)\n    Xnew = basis.transform(age_df)\n    M = sm.OLS(y, X).fit()\n    preds = M.get_prediction(Xnew)\n    bands = preds.conf_int(alpha=0.05)\n    fig, ax = subplots(figsize=(8,8))\n    ax.scatter(age,\n               y,\n               facecolor='gray',\n               alpha=0.5)\n    for val, ls in zip([preds.predicted_mean,\n                      bands[:,0],\n                      bands[:,1]],\n                     ['b','r--','r--']):\n        ax.plot(age_df.values, val, ls, linewidth=3)\n    ax.set_title(title, fontsize=20)\n    ax.set_xlabel('Age', fontsize=20)\n    ax.set_ylabel('Wage', fontsize=20);\n    return ax\n```\n:::\n\n\nWe include an argument `alpha` to `ax.scatter()`\nto add some transparency to the points. This provides a visual indication\nof density. Notice the use of the `zip()` function in the\n`for` loop above (see Section~\\ref{Ch2-statlearn-lab:for-loops}).\nWe have three lines to plot, each with different colors and line\ntypes. Here `zip()` conveniently bundles these together as\niterators in the loop. {In `Python`{} speak, an \"iterator\" is an object with a finite number of values, that can be iterated on, as in a loop.}\n\nWe now plot the fit of the fourth-degree polynomial using this\nfunction.\n\n::: {#29e9f8e0 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:35.779177Z\",\"iopub.status.busy\":\"2024-06-04T23:19:35.778961Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.009418Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.008910Z\"}' execution_count=7}\n``` {.python .cell-code}\nplot_wage_fit(age_df, \n              poly_age,\n              'Degree-4 Polynomial');\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-8-output-1.png){width=681 height=693}\n:::\n:::\n\n\nWith  polynomial regression we must decide on the degree of\nthe polynomial to use. Sometimes we just wing it, and decide to use\nsecond or third degree polynomials, simply to obtain a nonlinear fit. But we can\nmake such a decision in a more systematic way. One way to do this is through hypothesis\ntests, which we demonstrate here. We now fit a series of models ranging from\nlinear (degree-one) to degree-five polynomials,\nand look to determine the simplest model that is sufficient to\nexplain the relationship between `wage` and `age`. We use the\n`anova_lm()`  function, which performs a series of ANOVA\ntests.\nAn \\emph{analysis of\n  variance}  or ANOVA tests the null\nhypothesis that a model $\\mathcal{M}_1$ is sufficient to explain the\ndata against the alternative hypothesis that a more complex model\n$\\mathcal{M}_2$ is required. The determination is based on an F-test.\nTo perform the test, the models  $\\mathcal{M}_1$ and $\\mathcal{M}_2$ must be *nested*:\nthe space spanned by the predictors in $\\mathcal{M}_1$ must be a subspace of the\nspace spanned by the predictors in $\\mathcal{M}_2$. In this case, we\nfit five different polynomial\nmodels and sequentially compare the simpler model to the more complex\nmodel.\n\n::: {#039d77c4 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.012673Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.012313Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.072501Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.071990Z\"}' execution_count=8}\n``` {.python .cell-code}\nmodels = [MS([poly('age', degree=d)]) \n          for d in range(1, 6)]\nXs = [model.fit_transform(Wage) for model in models]\nanova_lm(*[sm.OLS(y, X_).fit()\n           for X_ in Xs])\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>df_resid</th>\n      <th>ssr</th>\n      <th>df_diff</th>\n      <th>ss_diff</th>\n      <th>F</th>\n      <th>Pr(&gt;F)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2998.0</td>\n      <td>5.022216e+06</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2997.0</td>\n      <td>4.793430e+06</td>\n      <td>1.0</td>\n      <td>228786.010128</td>\n      <td>143.593107</td>\n      <td>2.363850e-32</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2996.0</td>\n      <td>4.777674e+06</td>\n      <td>1.0</td>\n      <td>15755.693664</td>\n      <td>9.888756</td>\n      <td>1.679202e-03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2995.0</td>\n      <td>4.771604e+06</td>\n      <td>1.0</td>\n      <td>6070.152124</td>\n      <td>3.809813</td>\n      <td>5.104620e-02</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2994.0</td>\n      <td>4.770322e+06</td>\n      <td>1.0</td>\n      <td>1282.563017</td>\n      <td>0.804976</td>\n      <td>3.696820e-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNotice the `*` in the `anova_lm()` line above. This\nfunction takes a variable number of non-keyword arguments, in this case fitted models.\nWhen these models are provided as a list (as is done here), it must be \nprefixed by `*`.\n\nThe p-value comparing the linear `models[0]` to the quadratic\n`models[1]` is essentially zero, indicating that a linear\nfit is not sufficient. {Indexing starting at zero is confusing for the polynomial degree example, since `models[1]` is quadratic rather than linear!} Similarly the p-value comparing the quadratic\n`models[1]` to the cubic `models[2]` is very low (0.0017), so the\nquadratic fit is also insufficient. The p-value comparing the cubic\nand degree-four polynomials, `models[2]` and `models[3]`, is\napproximately 5%, while the degree-five polynomial `models[4]` seems\nunnecessary because its p-value is 0.37. Hence, either a cubic or a\nquartic polynomial appear to provide a reasonable fit to the data, but\nlower- or higher-order models are not justified.\n\nIn this case, instead of using the `anova()`  function, we could\nhave obtained these p-values more succinctly by exploiting the fact\nthat `poly()`  creates orthogonal polynomials.\n\n::: {#9c7cde0d .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.075597Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.075369Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.090416Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.089945Z\"}' execution_count=9}\n``` {.python .cell-code}\nsummarize(M)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>coef</th>\n      <th>std err</th>\n      <th>t</th>\n      <th>P&gt;|t|</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>intercept</th>\n      <td>111.7036</td>\n      <td>0.729</td>\n      <td>153.283</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[0]</th>\n      <td>447.0679</td>\n      <td>39.915</td>\n      <td>11.201</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[1]</th>\n      <td>-478.3158</td>\n      <td>39.915</td>\n      <td>-11.983</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[2]</th>\n      <td>125.5217</td>\n      <td>39.915</td>\n      <td>3.145</td>\n      <td>0.002</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[3]</th>\n      <td>-77.9112</td>\n      <td>39.915</td>\n      <td>-1.952</td>\n      <td>0.051</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\nNotice that the p-values are the same, and in fact the square of\nthe  t-statistics are equal to the F-statistics from the\n`anova_lm()`  function;  for example: \n\n::: {#d04c186c .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.092926Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.092726Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.096105Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.095602Z\"}' execution_count=10}\n``` {.python .cell-code}\n(-11.983)**2\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n143.59228900000002\n```\n:::\n:::\n\n\n\nHowever, the ANOVA method works whether or not we used orthogonal\npolynomials, provided the models are nested. For example, we can use\n`anova_lm()`  to compare the following three\nmodels, which all have a linear term in `education` and a\npolynomial in `age` of different degrees:\n\n::: {#51fda09c .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.098614Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.098427Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.147734Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.145914Z\"}' execution_count=11}\n``` {.python .cell-code}\nmodels = [MS(['education', poly('age', degree=d)])\n          for d in range(1, 4)]\nXEs = [model.fit_transform(Wage)\n       for model in models]\nanova_lm(*[sm.OLS(y, X_).fit() for X_ in XEs])\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>df_resid</th>\n      <th>ssr</th>\n      <th>df_diff</th>\n      <th>ss_diff</th>\n      <th>F</th>\n      <th>Pr(&gt;F)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2997.0</td>\n      <td>3.902335e+06</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2996.0</td>\n      <td>3.759472e+06</td>\n      <td>1.0</td>\n      <td>142862.701185</td>\n      <td>113.991883</td>\n      <td>3.838075e-26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2995.0</td>\n      <td>3.753546e+06</td>\n      <td>1.0</td>\n      <td>5926.207070</td>\n      <td>4.728593</td>\n      <td>2.974318e-02</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\nAs an alternative to using hypothesis tests and ANOVA, we could choose\nthe polynomial degree using cross-validation, as discussed in Chapter~\\ref{Ch5:resample}.\n\nNext we consider the task of predicting whether an individual earns\nmore than $250,000 per year. We proceed much as before, except\nthat first we create the appropriate response vector, and then apply\nthe `glm()`  function using the binomial family in order\nto fit a polynomial logistic regression model.\n\n::: {#28a83c6a .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.154502Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.154251Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.179360Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.177817Z\"}' execution_count=12}\n``` {.python .cell-code}\nX = poly_age.transform(Wage)\nhigh_earn = Wage['high_earn'] = y > 250 # shorthand\nglm = sm.GLM(y > 250,\n             X,\n             family=sm.families.Binomial())\nB = glm.fit()\nsummarize(B)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>coef</th>\n      <th>std err</th>\n      <th>z</th>\n      <th>P&gt;|z|</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>intercept</th>\n      <td>-4.3012</td>\n      <td>0.345</td>\n      <td>-12.457</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[0]</th>\n      <td>71.9642</td>\n      <td>26.133</td>\n      <td>2.754</td>\n      <td>0.006</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[1]</th>\n      <td>-85.7729</td>\n      <td>35.929</td>\n      <td>-2.387</td>\n      <td>0.017</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[2]</th>\n      <td>34.1626</td>\n      <td>19.697</td>\n      <td>1.734</td>\n      <td>0.083</td>\n    </tr>\n    <tr>\n      <th>poly(age, degree=4)[3]</th>\n      <td>-47.4008</td>\n      <td>24.105</td>\n      <td>-1.966</td>\n      <td>0.049</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\nOnce again, we make predictions using the `get_prediction()`  method.\n\n::: {#93c4af93 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.182205Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.181928Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.188474Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.187832Z\"}' execution_count=13}\n``` {.python .cell-code}\nnewX = poly_age.transform(age_df)\npreds = B.get_prediction(newX)\nbands = preds.conf_int(alpha=0.05)\n```\n:::\n\n\nWe now plot the estimated relationship.\n\n::: {#1254c9ea .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.191550Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.191370Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.311190Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.310416Z\"}' execution_count=14}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8,8))\nrng = np.random.default_rng(0)\nax.scatter(age +\n           0.2 * rng.uniform(size=y.shape[0]),\n           np.where(high_earn, 0.198, 0.002),\n           fc='gray',\n           marker='|')\nfor val, ls in zip([preds.predicted_mean,\n                  bands[:,0],\n                  bands[:,1]],\n                 ['b','r--','r--']):\n    ax.plot(age_df.values, val, ls, linewidth=3)\nax.set_title('Degree-4 Polynomial', fontsize=20)\nax.set_xlabel('Age', fontsize=20)\nax.set_ylim([0,0.2])\nax.set_ylabel('P(Wage > 250)', fontsize=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-15-output-1.png){width=693 height=693}\n:::\n:::\n\n\nWe have drawn the `age` values corresponding to the observations with\n`wage` values above 250 as gray marks on the top of the plot, and\nthose with `wage` values below 250 are shown as gray marks on the\nbottom of the plot. We added a small amount of noise to jitter\nthe `age` values a bit so that observations with the same `age`\nvalue do not cover each other up. This type of plot is often called a\n*rug plot*.\n\nIn order to fit a step function, as discussed in\nSection~\\ref{Ch7:sec:scolstep-function},   we first use the `pd.qcut()`\nfunction to discretize `age` based on quantiles.  Then  we use `pd.get_dummies()` to create the\ncolumns of the model matrix for this categorical variable. Note that this function will\ninclude *all* columns for a given categorical, rather than the usual approach which drops one\nof the levels.\n\n::: {#6c01d902 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.315065Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.314672Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.333569Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.333006Z\"}' execution_count=15}\n``` {.python .cell-code}\ncut_age = pd.qcut(age, 4)\nsummarize(sm.OLS(y, pd.get_dummies(cut_age)).fit())\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>coef</th>\n      <th>std err</th>\n      <th>t</th>\n      <th>P&gt;|t|</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>(17.999, 33.75]</th>\n      <td>94.1584</td>\n      <td>1.478</td>\n      <td>63.692</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(33.75, 42.0]</th>\n      <td>116.6608</td>\n      <td>1.470</td>\n      <td>79.385</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(42.0, 51.0]</th>\n      <td>119.1887</td>\n      <td>1.416</td>\n      <td>84.147</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(51.0, 80.0]</th>\n      <td>116.5717</td>\n      <td>1.559</td>\n      <td>74.751</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\nHere `pd.qcut()`  automatically picked the cutpoints based on the quantiles 25%, 50% and 75%, which results in four regions.  We could also have specified our own\nquantiles directly instead of the argument `4`. For cuts not based\non quantiles we would use the `pd.cut()` function.\nThe function `pd.qcut()` (and `pd.cut()`) returns an ordered categorical variable.\n  The regression model then creates a set of\ndummy variables for use in the regression. Since `age` is the only variable in the model, the value $94,158.40 is the average salary for those under 33.75 years of\nage, and the other coefficients are the average\nsalary for those in the other age groups.  We can produce\npredictions and plots just as we did in the case of the polynomial\nfit.\n\n## Splines\nIn order to fit regression splines, we use transforms\nfrom the `ISLP` package. The actual spline\nevaluation functions are in the `scipy.interpolate` package;\nwe have simply wrapped them as transforms\nsimilar to `Poly()` and `PCA()`.\n\nIn Section~\\ref{Ch7:sec:scolr-splin}, we saw\nthat regression splines can be fit by constructing an appropriate\nmatrix of basis functions.  The `BSpline()`  function generates the\nentire matrix of basis functions for splines with the specified set of\nknots. By default, the B-splines produced are cubic. To change the degree, use\nthe argument `degree`.\n\n::: {#c975b449 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.336489Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.336258Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.343862Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.343366Z\"}' execution_count=16}\n``` {.python .cell-code}\nbs_ = BSpline(internal_knots=[25,40,60], intercept=True).fit(age)\nbs_age = bs_.transform(age)\nbs_age.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n(3000, 7)\n```\n:::\n:::\n\n\nThis results in a seven-column matrix, which is what is expected for a cubic-spline basis with 3 interior knots. \nWe can form this same matrix using the `bs()` object,\nwhich facilitates adding this to a model-matrix builder (as in `poly()` versus its workhorse `Poly()`) described in Section~\\ref{Ch7-nonlin-lab:polynomial-regression-and-step-functions}.\n\nWe now fit a cubic spline model to the `Wage`  data. \n\n::: {#c6882511 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.346759Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.346545Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.380669Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.380119Z\"}' execution_count=17}\n``` {.python .cell-code}\nbs_age = MS([bs('age', internal_knots=[25,40,60])])\nXbs = bs_age.fit_transform(Wage)\nM = sm.OLS(y, Xbs).fit()\nsummarize(M)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>coef</th>\n      <th>std err</th>\n      <th>t</th>\n      <th>P&gt;|t|</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>intercept</th>\n      <td>60.4937</td>\n      <td>9.460</td>\n      <td>6.394</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age, internal_knots=[25, 40, 60])[0]</th>\n      <td>3.9805</td>\n      <td>12.538</td>\n      <td>0.317</td>\n      <td>0.751</td>\n    </tr>\n    <tr>\n      <th>bs(age, internal_knots=[25, 40, 60])[1]</th>\n      <td>44.6310</td>\n      <td>9.626</td>\n      <td>4.636</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age, internal_knots=[25, 40, 60])[2]</th>\n      <td>62.8388</td>\n      <td>10.755</td>\n      <td>5.843</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age, internal_knots=[25, 40, 60])[3]</th>\n      <td>55.9908</td>\n      <td>10.706</td>\n      <td>5.230</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age, internal_knots=[25, 40, 60])[4]</th>\n      <td>50.6881</td>\n      <td>14.402</td>\n      <td>3.520</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age, internal_knots=[25, 40, 60])[5]</th>\n      <td>16.6061</td>\n      <td>19.126</td>\n      <td>0.868</td>\n      <td>0.385</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe column names are a little cumbersome, and have caused us to truncate the printed summary. They can be set on construction using the `name` argument as follows.\n\n::: {#73b099bb .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.384846Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.384459Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.418050Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.415940Z\"}' execution_count=18}\n``` {.python .cell-code}\nbs_age = MS([bs('age',\n                internal_knots=[25,40,60],\n                name='bs(age)')])\nXbs = bs_age.fit_transform(Wage)\nM = sm.OLS(y, Xbs).fit()\nsummarize(M)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>coef</th>\n      <th>std err</th>\n      <th>t</th>\n      <th>P&gt;|t|</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>intercept</th>\n      <td>60.4937</td>\n      <td>9.460</td>\n      <td>6.394</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age)[0]</th>\n      <td>3.9805</td>\n      <td>12.538</td>\n      <td>0.317</td>\n      <td>0.751</td>\n    </tr>\n    <tr>\n      <th>bs(age)[1]</th>\n      <td>44.6310</td>\n      <td>9.626</td>\n      <td>4.636</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age)[2]</th>\n      <td>62.8388</td>\n      <td>10.755</td>\n      <td>5.843</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age)[3]</th>\n      <td>55.9908</td>\n      <td>10.706</td>\n      <td>5.230</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age)[4]</th>\n      <td>50.6881</td>\n      <td>14.402</td>\n      <td>3.520</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>bs(age)[5]</th>\n      <td>16.6061</td>\n      <td>19.126</td>\n      <td>0.868</td>\n      <td>0.385</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNotice that there are 6 spline coefficients rather than 7. This is because, by default,\n`bs()` assumes `intercept=False`, since we typically have an overall intercept in the model.\nSo it generates the spline basis with the given knots,  and then discards one of the basis functions to account for the intercept. \n\nWe could also use the `df` (degrees of freedom) option to\nspecify the complexity of the spline.  We see above that with 3 knots,\nthe spline basis has 6 columns or degrees of freedom.  When we specify\n`df=6` rather than the actual knots, `bs()` will produce a\nspline with 3 knots chosen at uniform quantiles of the training data.\nWe can see these chosen knots most easily using `Bspline()` directly:\n\n::: {#54b32a6f .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.421424Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.421207Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.429792Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.429086Z\"}' execution_count=19}\n``` {.python .cell-code}\nBSpline(df=6).fit(age).internal_knots_\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\narray([33.75, 42.  , 51.  ])\n```\n:::\n:::\n\n\n When asking for six degrees of freedom,\nthe transform chooses knots at ages 33.75, 42.0, and 51.0,\nwhich correspond to the 25th, 50th, and 75th percentiles of\n`age`.\n\nWhen using B-splines we need not limit ourselves to cubic polynomials\n(i.e. `degree=3`). For instance, using `degree=0` results\nin piecewise constant functions, as in our example with\n`pd.qcut()` above.\n\n::: {#5fe6a078 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.432272Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.432078Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.457934Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.457375Z\"}' execution_count=20}\n``` {.python .cell-code}\nbs_age0 = MS([bs('age',\n                 df=3, \n                 degree=0)]).fit(Wage)\nXbs0 = bs_age0.transform(Wage)\nsummarize(sm.OLS(y, Xbs0).fit())\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>coef</th>\n      <th>std err</th>\n      <th>t</th>\n      <th>P&gt;|t|</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>intercept</th>\n      <td>94.1584</td>\n      <td>1.478</td>\n      <td>63.687</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>bs(age, df=3, degree=0)[0]</th>\n      <td>22.3490</td>\n      <td>2.152</td>\n      <td>10.388</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>bs(age, df=3, degree=0)[1]</th>\n      <td>24.8076</td>\n      <td>2.044</td>\n      <td>12.137</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>bs(age, df=3, degree=0)[2]</th>\n      <td>22.7814</td>\n      <td>2.087</td>\n      <td>10.917</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThis fit should be compared with cell [15] where we use `qcut()`\nto create four bins by cutting at the 25%, 50% and 75% quantiles of\n`age`.  Since we specified `df=3` for degree-zero splines here, there will also be\nknots at the same three quantiles. Although the coefficients appear different, we\nsee that this is a result of the different coding. For example, the\nfirst coefficient is identical in both cases, and is the mean response\nin the first bin. For the second coefficient, we have\n$94.158 + 22.349 = 116.507 \\approx 116.611$, the latter being the mean\nin the second bin in cell [15]. Here the intercept is coded by a column\nof ones, so the second, third and fourth coefficients are increments\nfor those bins. Why is the sum not exactly the same? It turns out that\nthe `qcut()` uses $\\leq$, while `bs()` uses $<$ when\ndeciding bin membership.\n\n\n    \n \n\n \n\n    \n \n\nIn order to fit a natural spline, we use the `NaturalSpline()` \ntransform with the corresponding helper `ns()`.  Here we fit a natural spline with five\ndegrees of freedom (excluding the intercept) and plot the results.\n\n::: {#262a6938 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.460525Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.460295Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.489739Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.489248Z\"}' execution_count=21}\n``` {.python .cell-code}\nns_age = MS([ns('age', df=5)]).fit(Wage)\nM_ns = sm.OLS(y, ns_age.transform(Wage)).fit()\nsummarize(M_ns)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>coef</th>\n      <th>std err</th>\n      <th>t</th>\n      <th>P&gt;|t|</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>intercept</th>\n      <td>60.4752</td>\n      <td>4.708</td>\n      <td>12.844</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>ns(age, df=5)[0]</th>\n      <td>61.5267</td>\n      <td>4.709</td>\n      <td>13.065</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>ns(age, df=5)[1]</th>\n      <td>55.6912</td>\n      <td>5.717</td>\n      <td>9.741</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>ns(age, df=5)[2]</th>\n      <td>46.8184</td>\n      <td>4.948</td>\n      <td>9.463</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>ns(age, df=5)[3]</th>\n      <td>83.2036</td>\n      <td>11.918</td>\n      <td>6.982</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>ns(age, df=5)[4]</th>\n      <td>6.8770</td>\n      <td>9.484</td>\n      <td>0.725</td>\n      <td>0.468</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe now plot the natural spline using our plotting function.\n\n::: {#2587a859 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.493122Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.492786Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.668184Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.667651Z\"}' execution_count=22}\n``` {.python .cell-code}\nplot_wage_fit(age_df,\n              ns_age,\n              'Natural spline, df=5');\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-23-output-1.png){width=681 height=693}\n:::\n:::\n\n\n## Smoothing Splines and GAMs\nA smoothing spline is a special case of a GAM with squared-error loss\nand a single feature. To fit GAMs in `Python` we will use the\n`pygam` package which can be installed via `pip install pygam`. The\nestimator `LinearGAM()` uses squared-error loss.\nThe GAM is specified by associating each column\nof a model matrix with a particular smoothing operation:\n`s` for smoothing spline; `l` for linear, and `f` for factor or categorical variables.\nThe argument `0` passed to `s` below indicates that this smoother will\napply to the first column of a feature matrix. Below, we pass it a\nmatrix with a single column: `X_age`. The argument `lam` is the penalty parameter $\\lambda$ as discussed in Section~\\ref{Ch7:sec5.2}.\n\n::: {#540b5a89 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.671284Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.671050Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.705757Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.705247Z\"}' execution_count=23}\n``` {.python .cell-code}\nX_age = np.asarray(age).reshape((-1,1))\ngam = LinearGAM(s_gam(0, lam=0.6))\ngam.fit(X_age, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nLinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n   max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, \n   verbose=False)\n```\n:::\n:::\n\n\nThe `pygam` library generally expects a matrix of features so we reshape `age` to be a matrix (a two-dimensional array) instead\nof a vector (i.e. a one-dimensional array). The `-1` in the call to the `reshape()` method tells `numpy` to impute the\nsize of that dimension based on the remaining entries of the shape tuple.\n \nLet’s investigate how the fit changes with the smoothing parameter `lam`.\nThe function `np.logspace()` is similar to `np.linspace()` but spaces points\nevenly on the log-scale. Below we vary `lam` from $10^{-2}$ to $10^6$.\n\n::: {#8eb7e983 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.708880Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.708673Z\",\"iopub.status.idle\":\"2024-06-04T23:19:36.976882Z\",\"shell.execute_reply\":\"2024-06-04T23:19:36.976289Z\"}' execution_count=24}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8,8))\nax.scatter(age, y, facecolor='gray', alpha=0.5)\nfor lam in np.logspace(-2, 6, 5):\n    gam = LinearGAM(s_gam(0, lam=lam)).fit(X_age, y)\n    ax.plot(age_grid,\n            gam.predict(age_grid),\n            label='{:.1e}'.format(lam),\n            linewidth=3)\nax.set_xlabel('Age', fontsize=20)\nax.set_ylabel('Wage', fontsize=20);\nax.legend(title='$\\lambda$');\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-25-output-1.png){width=681 height=664}\n:::\n:::\n\n\nThe `pygam` package can perform a search for an optimal smoothing parameter.\n\n::: {#7e883437 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:36.979615Z\",\"iopub.status.busy\":\"2024-06-04T23:19:36.979395Z\",\"iopub.status.idle\":\"2024-06-04T23:19:37.195601Z\",\"shell.execute_reply\":\"2024-06-04T23:19:37.195005Z\"}' execution_count=25}\n``` {.python .cell-code}\ngam_opt = gam.gridsearch(X_age, y)\nax.plot(age_grid,\n        gam_opt.predict(age_grid),\n        label='Grid search',\n        linewidth=4)\nax.legend()\nfig\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0% (0 of 11) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--\r 36% (4 of 11) |#########                | Elapsed Time: 0:00:00 ETA:   0:00:00\r 72% (8 of 11) |##################       | Elapsed Time: 0:00:00 ETA:   0:00:00\r100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=25}\n![](Ch07-nonlin-lab_files/figure-html/cell-26-output-2.png){width=681 height=664}\n:::\n:::\n\n\nAlternatively, we can fix the degrees of freedom of the smoothing\nspline using a function included in the `ISLP.pygam` package. Below we\nfind a value of $\\lambda$ that gives us roughly four degrees of\nfreedom. We note here that these degrees of freedom include the\nunpenalized intercept and linear term of the smoothing spline, hence there are at least two\ndegrees of freedom.\n\n::: {#3a09ca1d .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:37.198482Z\",\"iopub.status.busy\":\"2024-06-04T23:19:37.198260Z\",\"iopub.status.idle\":\"2024-06-04T23:19:37.214885Z\",\"shell.execute_reply\":\"2024-06-04T23:19:37.214249Z\"}' execution_count=26}\n``` {.python .cell-code}\nage_term = gam.terms[0]\nlam_4 = approx_lam(X_age, age_term, 4)\nage_term.lam = lam_4\ndegrees_of_freedom(X_age, age_term)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n4.000000100003379\n```\n:::\n:::\n\n\n\nLet’s vary the degrees of freedom in a similar plot to above. We choose the degrees of freedom\nas the desired degrees of freedom plus one to account for the fact that these smoothing\nsplines always have an intercept term. Hence, a value of one for `df` is just a linear fit.\n\n::: {#6abbf76f .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:37.218081Z\",\"iopub.status.busy\":\"2024-06-04T23:19:37.217683Z\",\"iopub.status.idle\":\"2024-06-04T23:19:37.501731Z\",\"shell.execute_reply\":\"2024-06-04T23:19:37.500504Z\"}' execution_count=27}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8,8))\nax.scatter(X_age,\n           y,\n           facecolor='gray',\n           alpha=0.3)\nfor df in [1,3,4,8,15]:\n    lam = approx_lam(X_age, age_term, df+1)\n    age_term.lam = lam\n    gam.fit(X_age, y)\n    ax.plot(age_grid,\n            gam.predict(age_grid),\n            label='{:d}'.format(df),\n            linewidth=4)\nax.set_xlabel('Age', fontsize=20)\nax.set_ylabel('Wage', fontsize=20);\nax.legend(title='Degrees of freedom');\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-28-output-1.png){width=681 height=664}\n:::\n:::\n\n\n### Additive Models with Several Terms\nThe strength of generalized additive models lies in their ability to fit multivariate regression models with more flexibility than linear models. We demonstrate two approaches: the first in a more manual fashion using natural splines and piecewise constant functions, and the second  using the `pygam` package and smoothing splines.\n\nWe now fit a GAM by hand to predict\n`wage` using natural spline functions of `year` and `age`,\ntreating `education` as a qualitative predictor, as in (\\ref{Ch7:nsmod}).\nSince this is just a big linear regression model\nusing an appropriate choice of basis functions, we can simply do this\nusing the `sm.OLS()`  function.\n\nWe will build the model matrix in a more manual fashion here, since we wish\nto access the pieces separately when constructing partial dependence plots.\n\n::: {#e8924935 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:37.505073Z\",\"iopub.status.busy\":\"2024-06-04T23:19:37.504839Z\",\"iopub.status.idle\":\"2024-06-04T23:19:37.519804Z\",\"shell.execute_reply\":\"2024-06-04T23:19:37.519297Z\"}' execution_count=28}\n``` {.python .cell-code}\nns_age = NaturalSpline(df=4).fit(age)\nns_year = NaturalSpline(df=5).fit(Wage['year'])\nXs = [ns_age.transform(age),\n      ns_year.transform(Wage['year']),\n      pd.get_dummies(Wage['education']).values]\nX_bh = np.hstack(Xs)\ngam_bh = sm.OLS(y, X_bh).fit()\n```\n:::\n\n\nHere the function `NaturalSpline()` is the workhorse supporting\nthe `ns()` helper function.  We chose to use all columns of the\nindicator matrix for the categorical variable `education`, making an intercept redundant.\nFinally, we stacked the three component matrices horizontally to form the model matrix `X_bh`. \n\nWe now show how to construct partial dependence plots for each of the terms in our rudimentary GAM. We can do this by hand,\ngiven grids for `age` and `year`.\n We simply predict with new $X$ matrices, fixing all but one of the features at a time.\n\n::: {#0f35fe45 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:37.523112Z\",\"iopub.status.busy\":\"2024-06-04T23:19:37.522834Z\",\"iopub.status.idle\":\"2024-06-04T23:19:37.644337Z\",\"shell.execute_reply\":\"2024-06-04T23:19:37.643251Z\"}' execution_count=29}\n``` {.python .cell-code}\nage_grid = np.linspace(age.min(),\n                       age.max(),\n                       100)\nX_age_bh = X_bh.copy()[:100]\nX_age_bh[:] = X_bh[:].mean(0)[None,:]\nX_age_bh[:,:4] = ns_age.transform(age_grid)\npreds = gam_bh.get_prediction(X_age_bh)\nbounds_age = preds.conf_int(alpha=0.05)\npartial_age = preds.predicted_mean\ncenter = partial_age.mean()\npartial_age -= center\nbounds_age -= center\nfig, ax = subplots(figsize=(8,8))\nax.plot(age_grid, partial_age, 'b', linewidth=3)\nax.plot(age_grid, bounds_age[:,0], 'r--', linewidth=3)\nax.plot(age_grid, bounds_age[:,1], 'r--', linewidth=3)\nax.set_xlabel('Age')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of age on wage', fontsize=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-30-output-1.png){width=670 height=680}\n:::\n:::\n\n\nLet's explain in some detail what we did above. The idea is to create a new prediction matrix, where all but the columns belonging to `age` are constant (and set to  their training-data means). The four columns for `age` are filled in with the natural spline basis evaluated at the 100 values in `age_grid`.\n\n* We made a grid of length 100 in `age`, and created a matrix `X_age_bh` with 100 rows and the same number of columns as `X_bh`.\n* We replaced every row of this matrix with the column means of the original.\n* We then replace just the first four columns representing `age` with the natural spline basis computed at the values in `age_grid`. \n\nThe remaining steps should by now be familiar.\n\nWe also look at the effect of `year` on `wage`; the process is the same.\n\n::: {#5c5f7d18 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:37.648530Z\",\"iopub.status.busy\":\"2024-06-04T23:19:37.647173Z\",\"iopub.status.idle\":\"2024-06-04T23:19:37.756140Z\",\"shell.execute_reply\":\"2024-06-04T23:19:37.755581Z\"}' execution_count=30}\n``` {.python .cell-code}\nyear_grid = np.linspace(2003, 2009, 100)\nyear_grid = np.linspace(Wage['year'].min(),\n                        Wage['year'].max(),\n                        100)\nX_year_bh = X_bh.copy()[:100]\nX_year_bh[:] = X_bh[:].mean(0)[None,:]\nX_year_bh[:,4:9] = ns_year.transform(year_grid)\npreds = gam_bh.get_prediction(X_year_bh)\nbounds_year = preds.conf_int(alpha=0.05)\npartial_year = preds.predicted_mean\ncenter = partial_year.mean()\npartial_year -= center\nbounds_year -= center\nfig, ax = subplots(figsize=(8,8))\nax.plot(year_grid, partial_year, 'b', linewidth=3)\nax.plot(year_grid, bounds_year[:,0], 'r--', linewidth=3)\nax.plot(year_grid, bounds_year[:,1], 'r--', linewidth=3)\nax.set_xlabel('Year')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of year on wage', fontsize=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-31-output-1.png){width=662 height=680}\n:::\n:::\n\n\nWe now fit the model (\\ref{Ch7:nsmod})  using smoothing splines rather\nthan natural splines.  All of the\nterms in  (\\ref{Ch7:nsmod})  are fit simultaneously, taking each other\ninto account to explain the response. The `pygam` package only works with matrices, so we must convert\nthe categorical series `education` to its array representation, which can be found\nwith the `cat.codes` attribute of `education`. As `year` only has 7 unique values, we\nuse only seven basis functions for it.\n\n::: {#97437d64 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:37.759390Z\",\"iopub.status.busy\":\"2024-06-04T23:19:37.759130Z\",\"iopub.status.idle\":\"2024-06-04T23:19:37.788899Z\",\"shell.execute_reply\":\"2024-06-04T23:19:37.788370Z\"}' execution_count=31}\n``` {.python .cell-code}\ngam_full = LinearGAM(s_gam(0) +\n                     s_gam(1, n_splines=7) +\n                     f_gam(2, lam=0))\nXgam = np.column_stack([age,\n                        Wage['year'],\n                        Wage['education'].cat.codes])\ngam_full = gam_full.fit(Xgam, y)\n```\n:::\n\n\nThe two `s_gam()` terms result in smoothing spline fits, and use a default value for $\\lambda$  (`lam=0.6`), which is somewhat arbitrary. For the categorical term `education`, specified using a `f_gam()` term,  we specify `lam=0` to avoid any shrinkage.\nWe produce the partial dependence plot in `age` to see the effect of these choices.\n\nThe values for the plot\nare generated by the `pygam` package. We provide a `plot_gam()`\nfunction for partial-dependence plots in `ISLP.pygam`, which makes this job easier than in our last example with natural splines.\n\n::: {#87c0f3aa .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:37.792129Z\",\"iopub.status.busy\":\"2024-06-04T23:19:37.791875Z\",\"iopub.status.idle\":\"2024-06-04T23:19:37.909465Z\",\"shell.execute_reply\":\"2024-06-04T23:19:37.908764Z\"}' execution_count=32}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8,8))\nplot_gam(gam_full, 0, ax=ax)\nax.set_xlabel('Age')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of age on wage - default lam=0.6', fontsize=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-33-output-1.png){width=735 height=680}\n:::\n:::\n\n\nWe see that the function is somewhat wiggly. It is more natural to specify the `df` than a value for `lam`. \nWe refit a GAM using four degrees of freedom each for\n`age` and  `year`. Recall that the addition of one below takes into account the intercept\nof the smoothing spline.\n\n::: {#d9ba7fe3 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:37.912957Z\",\"iopub.status.busy\":\"2024-06-04T23:19:37.912683Z\",\"iopub.status.idle\":\"2024-06-04T23:19:37.946458Z\",\"shell.execute_reply\":\"2024-06-04T23:19:37.945953Z\"}' execution_count=33}\n``` {.python .cell-code}\nage_term = gam_full.terms[0]\nage_term.lam = approx_lam(Xgam, age_term, df=4+1)\nyear_term = gam_full.terms[1]\nyear_term.lam = approx_lam(Xgam, year_term, df=4+1)\ngam_full = gam_full.fit(Xgam, y)\n```\n:::\n\n\nNote that updating `age_term.lam` above updates it in `gam_full.terms[0]` as well! Likewise for `year_term.lam`.\n\nRepeating the plot for `age`, we see that it is much smoother.\nWe also produce the plot for `year`.\n\n::: {#23cfdce4 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:37.950913Z\",\"iopub.status.busy\":\"2024-06-04T23:19:37.950529Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.082439Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.081725Z\"}' execution_count=34}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8,8))\nplot_gam(gam_full,\n         1,\n         ax=ax)\nax.set_xlabel('Year')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of year on wage', fontsize=20)\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\nText(0.5, 0, 'Year')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\nText(0, 0.5, 'Effect on wage')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\nText(0.5, 1.0, 'Partial dependence of year on wage')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-35-output-4.png){width=659 height=680}\n:::\n:::\n\n\nFinally we plot `education`, which is categorical. The partial dependence plot is different, and more suitable for the set of fitted constants for each level of this variable. \n\n::: {#9f578c2a .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.085348Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.085138Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.186025Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.185091Z\"}' execution_count=35}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8, 8))\nax = plot_gam(gam_full, 2)\nax.set_xlabel('Education')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of wage on education',\n             fontsize=20);\nax.set_xticklabels(Wage['education'].cat.categories, fontsize=8);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-36-output-1.png){width=670 height=677}\n:::\n:::\n\n\n### ANOVA Tests for Additive Models\nIn all of our models, the function of `year` looks rather linear. We can\nperform a series of ANOVA tests in order to determine which of these\nthree models is best: a GAM that excludes `year` ($\\mathcal{M}_1$),\na GAM that uses a linear function of `year` ($\\mathcal{M}_2$), or a\nGAM that uses a spline function of `year` ($\\mathcal{M}_3$).\n\n::: {#d3282adc .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.189290Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.188978Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.226707Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.226124Z\"}' execution_count=36}\n``` {.python .cell-code}\ngam_0 = LinearGAM(age_term + f_gam(2, lam=0))\ngam_0.fit(Xgam, y)\ngam_linear = LinearGAM(age_term +\n                       l_gam(1, lam=0) +\n                       f_gam(2, lam=0))\ngam_linear.fit(Xgam, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\nLinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n   max_iter=100, scale=None, terms=s(0) + f(2) + intercept, \n   tol=0.0001, verbose=False)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\nLinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n   max_iter=100, scale=None, terms=s(0) + l(1) + f(2) + intercept, \n   tol=0.0001, verbose=False)\n```\n:::\n:::\n\n\nNotice our use of `age_term` in the expressions above. We do this because\nearlier we set the value for `lam` in this term to achieve four degrees of freedom.\n\nTo directly assess the effect of `year` we run an ANOVA on the\nthree models fit above.\n\n::: {#6565428b .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.229152Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.228941Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.236919Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.236171Z\"}' execution_count=37}\n``` {.python .cell-code}\nanova_gam(gam_0, gam_linear, gam_full)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deviance</th>\n      <th>df</th>\n      <th>deviance_diff</th>\n      <th>df_diff</th>\n      <th>F</th>\n      <th>pvalue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.714362e+06</td>\n      <td>2991.004005</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.696746e+06</td>\n      <td>2990.005190</td>\n      <td>17616.542840</td>\n      <td>0.998815</td>\n      <td>14.265131</td>\n      <td>0.002314</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.693143e+06</td>\n      <td>2987.007254</td>\n      <td>3602.893655</td>\n      <td>2.997936</td>\n      <td>0.972007</td>\n      <td>0.435579</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n We find that there is compelling evidence that a GAM with a linear\nfunction in `year` is better than a GAM that does not include\n`year` at all ($p$-value= 0.002). However, there is no\nevidence that a non-linear function of `year` is needed\n($p$-value=0.435).  In other words, based on the results of this\nANOVA, $\\mathcal{M}_2$ is preferred.\n\nWe can repeat the same process for `age` as well. We see there is very clear evidence that\na non-linear term is required for `age`.\n\n::: {#1de7f75a .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.239488Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.239277Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.264822Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.264344Z\"}' execution_count=38}\n``` {.python .cell-code}\ngam_0 = LinearGAM(year_term +\n                  f_gam(2, lam=0))\ngam_linear = LinearGAM(l_gam(0, lam=0) +\n                       year_term +\n                       f_gam(2, lam=0))\ngam_0.fit(Xgam, y)\ngam_linear.fit(Xgam, y)\nanova_gam(gam_0, gam_linear, gam_full)\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\nLinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n   max_iter=100, scale=None, terms=s(1) + f(2) + intercept, \n   tol=0.0001, verbose=False)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\nLinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n   max_iter=100, scale=None, terms=l(0) + s(1) + f(2) + intercept, \n   tol=0.0001, verbose=False)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=38}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>deviance</th>\n      <th>df</th>\n      <th>deviance_diff</th>\n      <th>df_diff</th>\n      <th>F</th>\n      <th>pvalue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.975443e+06</td>\n      <td>2991.000589</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.850247e+06</td>\n      <td>2990.000704</td>\n      <td>125196.137317</td>\n      <td>0.999884</td>\n      <td>101.270106</td>\n      <td>1.681120e-07</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.693143e+06</td>\n      <td>2987.007254</td>\n      <td>157103.978302</td>\n      <td>2.993450</td>\n      <td>42.447812</td>\n      <td>5.669414e-07</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThere is a (verbose) `summary()` method for the GAM fit.\n\n::: {#5e4a808e .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.267342Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.267144Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.271357Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.270406Z\"}' execution_count=39}\n``` {.python .cell-code}\ngam_full.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinearGAM                                                                                                 \n=============================================== ==========================================================\nDistribution:                        NormalDist Effective DoF:                                     12.9927\nLink Function:                     IdentityLink Log Likelihood:                                 -24117.907\nNumber of Samples:                         3000 AIC:                                            48263.7995\n                                                AICc:                                             48263.94\n                                                GCV:                                             1246.1129\n                                                Scale:                                           1236.4024\n                                                Pseudo R-Squared:                                   0.2928\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [465.0491]           20           5.1          1.11e-16     ***         \ns(1)                              [2.1564]             7            4.0          8.10e-03     **          \nf(2)                              [0]                  5            4.0          1.11e-16     ***         \nintercept                                              1            0.0          1.11e-16     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_16750/3870570873.py:1: UserWarning:\n\nKNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n \nPlease do not make inferences based on these values! \n\nCollaborate on a solution, and stay up to date at: \ngithub.com/dswah/pyGAM/issues/163 \n\n\n```\n:::\n:::\n\n\nWe can make predictions from `gam` objects, just like from\n`lm` objects, using the `predict()`  method for the class\n`gam`.  Here we make predictions on the training set.\n\n::: {#736770e8 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.274135Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.273728Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.283548Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.283095Z\"}' execution_count=40}\n``` {.python .cell-code}\nYhat = gam_full.predict(Xgam)\n```\n:::\n\n\nIn order to fit a logistic regression GAM, we use `LogisticGAM()` \nfrom `pygam`.\n\n::: {#51e40e74 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.286193Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.285994Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.372533Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.372014Z\"}' execution_count=41}\n``` {.python .cell-code}\ngam_logit = LogisticGAM(age_term + \n                        l_gam(1, lam=0) +\n                        f_gam(2, lam=0))\ngam_logit.fit(Xgam, high_earn)\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\nLogisticGAM(callbacks=[Deviance(), Diffs(), Accuracy()], \n   fit_intercept=True, max_iter=100, \n   terms=s(0) + l(1) + f(2) + intercept, tol=0.0001, verbose=False)\n```\n:::\n:::\n\n\n\n::: {#689476a9 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.375490Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.375048Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.476110Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.475521Z\"}' execution_count=42}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8, 8))\nax = plot_gam(gam_logit, 2)\nax.set_xlabel('Education')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of wage on education',\n             fontsize=20);\nax.set_xticklabels(Wage['education'].cat.categories, fontsize=8);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-43-output-1.png){width=687 height=677}\n:::\n:::\n\n\nThe model seems to be very flat, with especially high error bars for the first category.\nLet's look at the data a bit more closely.\n\n::: {#fb6bae28 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.478694Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.478494Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.491764Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.491357Z\"}' execution_count=43}\n``` {.python .cell-code}\npd.crosstab(Wage['high_earn'], Wage['education'])\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>education</th>\n      <th>1. &lt; HS Grad</th>\n      <th>2. HS Grad</th>\n      <th>3. Some College</th>\n      <th>4. College Grad</th>\n      <th>5. Advanced Degree</th>\n    </tr>\n    <tr>\n      <th>high_earn</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>False</th>\n      <td>268</td>\n      <td>966</td>\n      <td>643</td>\n      <td>663</td>\n      <td>381</td>\n    </tr>\n    <tr>\n      <th>True</th>\n      <td>0</td>\n      <td>5</td>\n      <td>7</td>\n      <td>22</td>\n      <td>45</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe see that there are no high earners in the first category of\neducation, meaning that the model will have a hard time fitting.  We\nwill fit a logistic regression GAM excluding all observations falling into this\ncategory. This provides more sensible results.\n\nTo do so,\nwe could subset the model matrix, though this will not remove the\ncolumn from `Xgam`. While we can deduce which column corresponds\nto this feature, for reproducibility’s sake we reform the model matrix\non this smaller subset.\n\n::: {#ec265ed1 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.494311Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.494116Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.498822Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.498423Z\"}' execution_count=44}\n``` {.python .cell-code}\nonly_hs = Wage['education'] == '1. < HS Grad'\nWage_ = Wage.loc[~only_hs]\nXgam_ = np.column_stack([Wage_['age'],\n                         Wage_['year'],\n                         Wage_['education'].cat.codes-1])\nhigh_earn_ = Wage_['high_earn']\n```\n:::\n\n\nIn the second-to-last line above, we subtract one  from the codes of the category, due to a bug in `pygam`. It just relabels\nthe education values and hence has no effect on the fit.\n \nWe now fit the model.\n\n::: {#687afd55 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.501236Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.501049Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.540965Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.540424Z\"}' execution_count=45}\n``` {.python .cell-code}\ngam_logit_ = LogisticGAM(age_term +\n                         year_term +\n                         f_gam(2, lam=0))\ngam_logit_.fit(Xgam_, high_earn_)\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\nLogisticGAM(callbacks=[Deviance(), Diffs(), Accuracy()], \n   fit_intercept=True, max_iter=100, \n   terms=s(0) + s(1) + f(2) + intercept, tol=0.0001, verbose=False)\n```\n:::\n:::\n\n\n\nLet’s look at the effect of `education`, `year` and `age` on high earner status now that we’ve\nremoved those observations.\n\n::: {#807c5fb5 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.543510Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.543313Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.635421Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.634852Z\"}' execution_count=46}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8, 8))\nax = plot_gam(gam_logit_, 2)\nax.set_xlabel('Education')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of high earner status on education', fontsize=20);\nax.set_xticklabels(Wage['education'].cat.categories[1:],\n                   fontsize=8);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-47-output-1.png){width=747 height=677}\n:::\n:::\n\n\n::: {#900d53eb .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.637945Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.637732Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.738536Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.737576Z\"}' execution_count=47}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8, 8))\nax = plot_gam(gam_logit_, 1)\nax.set_xlabel('Year')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of high earner status on year',\n             fontsize=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-48-output-1.png){width=704 height=680}\n:::\n:::\n\n\n::: {#7fbde2fd .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.741522Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.741306Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.853209Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.852962Z\"}' execution_count=48}\n``` {.python .cell-code}\nfig, ax = subplots(figsize=(8, 8))\nax = plot_gam(gam_logit_, 0)\nax.set_xlabel('Age')\nax.set_ylabel('Effect on wage')\nax.set_title('Partial dependence of high earner status on age', fontsize=20);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-49-output-1.png){width=700 height=680}\n:::\n:::\n\n\n\n## Local Regression\nWe illustrate the use of local regression using  the `lowess()` \nfunction from `sm.nonparametric`. Some implementations of\nGAMs allow terms to be local regression operators; this is not the case in `pygam`.\n\nHere we fit local linear regression models using spans of 0.2\nand 0.5; that is, each neighborhood consists of 20% or 50% of\nthe observations. As expected, using a span of 0.5 is smoother than 0.2.\n\n::: {#680b1ed1 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:19:38.854567Z\",\"iopub.status.busy\":\"2024-06-04T23:19:38.854494Z\",\"iopub.status.idle\":\"2024-06-04T23:19:38.953710Z\",\"shell.execute_reply\":\"2024-06-04T23:19:38.953441Z\"}' execution_count=49}\n``` {.python .cell-code}\nlowess = sm.nonparametric.lowess\nfig, ax = subplots(figsize=(8,8))\nax.scatter(age, y, facecolor='gray', alpha=0.5)\nfor span in [0.2, 0.5]:\n    fitted = lowess(y,\n                    age,\n                    frac=span,\n                    xvals=age_grid)\n    ax.plot(age_grid,\n            fitted,\n            label='{:.1f}'.format(span),\n            linewidth=4)\nax.set_xlabel('Age', fontsize=20)\nax.set_ylabel('Wage', fontsize=20);\nax.legend(title='span', fontsize=15);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch07-nonlin-lab_files/figure-html/cell-50-output-1.png){width=681 height=664}\n:::\n:::\n\n\n",
    "supporting": [
      "Ch07-nonlin-lab_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}