{
  "hash": "89000eb741bc89c7d936763a4577db64",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Multiple Testing\njupyter:\n  jupytext:\n    cell_metadata_filter: '-all'\n    main_language: python\n    notebook_metadata_filter: '-all'\n  kernelspec:\n    display_name: Python 3\n    language: python\n    name: python3\n---\n\n\n\n\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/intro-stat-learning/ISLP_labs/blob/v2.2/Ch13-multiple-lab.ipynb\">\n<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/intro-stat-learning/ISLP_labs/v2.2?labpath=Ch13-multiple-lab.ipynb)\n\n\n \n\n\nWe include our usual imports seen in earlier labs.\n\n::: {#50b1c0f6 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:06.119783Z\",\"iopub.status.busy\":\"2024-06-04T23:20:06.119425Z\",\"iopub.status.idle\":\"2024-06-04T23:20:06.998312Z\",\"shell.execute_reply\":\"2024-06-04T23:20:06.998023Z\"}' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom ISLP import load_data\n```\n:::\n\n\nWe also collect the new imports\nneeded for this lab.\n\n::: {#18092d7d .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:06.999889Z\",\"iopub.status.busy\":\"2024-06-04T23:20:06.999773Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.001596Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.001373Z\"}' execution_count=2}\n``` {.python .cell-code}\nfrom scipy.stats import \\\n    (ttest_1samp,\n     ttest_rel,\n     ttest_ind,\n     t as t_dbn)\nfrom statsmodels.stats.multicomp import \\\n     pairwise_tukeyhsd\nfrom statsmodels.stats.multitest import \\\n     multipletests as mult_test\n```\n:::\n\n\n\n## Review of Hypothesis Tests\nWe begin by performing some one-sample $t$-tests.\n\nFirst we create 100 variables, each consisting of 10 observations. The\nfirst 50 variables have mean $0.5$ and variance $1$, while the others\nhave mean $0$ and variance $1$.\n\n::: {#8db714a1 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.002832Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.002760Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.004664Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.004449Z\"}' execution_count=3}\n``` {.python .cell-code}\nrng = np.random.default_rng(12)\nX = rng.standard_normal((10, 100))\ntrue_mean = np.array([0.5]*50 + [0]*50)\nX += true_mean[None,:]\n```\n:::\n\n\nTo begin, we use `ttest_1samp()`  from the\n`scipy.stats` module to test $H_{0}: \\mu_1=0$, the null\nhypothesis that the first variable has mean zero.\n\n::: {#6fc3bd0e .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.005916Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.005847Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.009111Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.008893Z\"}' execution_count=4}\n``` {.python .cell-code}\nresult = ttest_1samp(X[:,0], 0)\nresult.pvalue\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n0.9307442156164141\n```\n:::\n:::\n\n\nThe $p$-value comes out to 0.931, which is not low enough to\nreject the null hypothesis at level $\\alpha=0.05$.  In this case,\n$\\mu_1=0.5$, so the null hypothesis is false. Therefore, we have made\na Type II error by failing to reject the null hypothesis when the null\nhypothesis is false. \n\nWe now test $H_{0,j}: \\mu_j=0$ for $j=1,\\ldots,100$. We compute the\n100 $p$-values, and then construct a vector recording whether the\n$j$th $p$-value is less than or equal to 0.05, in which case we reject\n$H_{0j}$, or greater than 0.05, in which case we do not reject\n$H_{0j}$, for $j=1,\\ldots,100$.\n\n::: {#65a7deb7 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.010301Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.010222Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.026475Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.026253Z\"}' execution_count=5}\n``` {.python .cell-code}\np_values = np.empty(100)\nfor i in range(100):\n   p_values[i] = ttest_1samp(X[:,i], 0).pvalue\ndecision = pd.cut(p_values,\n                  [0, 0.05, 1],\n                  labels=['Reject H0',\n                          'Do not reject H0'])\ntruth = pd.Categorical(true_mean == 0,\n                       categories=[True, False],\n                       ordered=True)\n```\n:::\n\n\nSince this is a simulated data set, we can create a $2 \\times 2$ table\nsimilar to  Table~\\ref{Ch12:tab-hypotheses}.\n\n::: {#7a5aaf44 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.027636Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.027567Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.034757Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.034523Z\"}' execution_count=6}\n``` {.python .cell-code}\npd.crosstab(decision,\n            truth,\n     rownames=['Decision'],\n     colnames=['H0'])\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>H0</th>\n      <th>True</th>\n      <th>False</th>\n    </tr>\n    <tr>\n      <th>Decision</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Reject H0</th>\n      <td>5</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>Do not reject H0</th>\n      <td>45</td>\n      <td>35</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nTherefore, at level $\\alpha=0.05$, we reject 15 of the 50 false\nnull hypotheses, and we incorrectly reject 5 of the true null\nhypotheses. Using the notation from Section~\\ref{sec:fwer}, we have\n$V=5$, $S=15$, $U=45$ and $W=35$.\nWe have set $\\alpha=0.05$, which means that we expect to reject around\n5% of the true null hypotheses. This is in line with the $2 \\times 2$\ntable above, which indicates that we rejected $V=5$ of the $50$ true\nnull hypotheses.\n\nIn the simulation above, for the false null hypotheses, the ratio of\nthe mean to the standard deviation was only $0.5/1 = 0.5$. This\namounts to quite a weak signal, and it resulted in a high number of\nType II errors.  Let’s instead simulate data with a stronger signal,\nso that the ratio of the mean to the standard deviation for the false\nnull hypotheses equals $1$. We make only 10 Type II errors.\n\n::: {#32f0da61 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.036106Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.036002Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.060212Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.059960Z\"}' execution_count=7}\n``` {.python .cell-code}\ntrue_mean = np.array([1]*50 + [0]*50)\nX = rng.standard_normal((10, 100))\nX += true_mean[None,:]\nfor i in range(100):\n   p_values[i] = ttest_1samp(X[:,i], 0).pvalue\ndecision = pd.cut(p_values,\n                  [0, 0.05, 1],\n                  labels=['Reject H0',\n                          'Do not reject H0'])\ntruth = pd.Categorical(true_mean == 0,\n                       categories=[True, False],\n                       ordered=True)\npd.crosstab(decision,\n            truth,\n            rownames=['Decision'],\n            colnames=['H0'])\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>H0</th>\n      <th>True</th>\n      <th>False</th>\n    </tr>\n    <tr>\n      <th>Decision</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Reject H0</th>\n      <td>2</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>Do not reject H0</th>\n      <td>48</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Family-Wise Error Rate\nRecall from  \\eqref{eq:FWER.indep}  that if the null hypothesis is true\nfor each of $m$ independent hypothesis tests, then the FWER is equal\nto $1-(1-\\alpha)^m$. We can use this expression to compute the FWER\nfor $m=1,\\ldots, 500$ and $\\alpha=0.05$, $0.01$, and $0.001$.\nWe plot the FWER for these values of $\\alpha$ in order to\nreproduce  Figure~\\ref{Ch12:fwer}.\n\n::: {#a4fec4f4 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.061515Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.061433Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.311200Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.310542Z\"}' execution_count=8}\n``` {.python .cell-code}\nm = np.linspace(1, 501)\nfig, ax = plt.subplots()\n[ax.plot(m,\n         1 - (1 - alpha)**m,\n         label=r'$\\alpha=%s$' % str(alpha))\n         for alpha in [0.05, 0.01, 0.001]]\nax.set_xscale('log')\nax.set_xlabel('Number of Hypotheses')\nax.set_ylabel('Family-Wise Error Rate')\nax.legend()\nax.axhline(0.05, c='k', ls='--');\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch13-multiple-lab_files/figure-html/cell-9-output-1.png){width=589 height=431}\n:::\n:::\n\n\nAs discussed previously, even for moderate values of $m$ such as $50$,\nthe FWER exceeds $0.05$ unless $\\alpha$ is set to a very low value,\nsuch as $0.001$.  Of course, the problem with setting $\\alpha$ to such\na low value is that we are likely to make a number of Type II errors:\nin other words, our power is very low.\n\nWe now conduct a one-sample $t$-test for each of the first five\nmanagers in the  \n`Fund`   dataset, in order to test the null\nhypothesis that the $j$th fund manager’s mean return equals zero,\n$H_{0,j}: \\mu_j=0$.\n\n::: {#6e26590e .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.314351Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.313832Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.344704Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.344468Z\"}' execution_count=9}\n``` {.python .cell-code}\nFund = load_data('Fund')\nfund_mini = Fund.iloc[:,:5]\nfund_mini_pvals = np.empty(5)\nfor i in range(5):\n    fund_mini_pvals[i] = ttest_1samp(fund_mini.iloc[:,i], 0).pvalue\nfund_mini_pvals\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([0.00620236, 0.91827115, 0.01160098, 0.6005396 , 0.75578151])\n```\n:::\n:::\n\n\nThe $p$-values are low for Managers One and Three, and high for the\nother three managers.  However, we cannot simply reject $H_{0,1}$ and\n$H_{0,3}$, since this would fail to account for the multiple testing\nthat we have performed. Instead, we will conduct Bonferroni’s method\nand Holm’s method to control the FWER.\n\nTo do this, we use the `multipletests()`  function from the\n`statsmodels` module (abbreviated to `mult_test()`). Given the $p$-values,\nfor methods like Holm and Bonferroni the function outputs\nadjusted $p$-values,  which\ncan be thought of as a new set of $p$-values that have been corrected\nfor multiple testing. If the adjusted $p$-value for a given hypothesis\nis less than or equal to $\\alpha$, then that hypothesis can be\nrejected while maintaining a FWER of no more than $\\alpha$. In other\nwords, for such methods, the adjusted $p$-values resulting from the `multipletests()`\nfunction can simply be compared to the desired FWER in order to\ndetermine whether or not to reject each hypothesis. We will later\nsee that we can use the same function to control FDR as well.\n\nThe `mult_test()` function takes $p$-values and a `method` argument, as well as an optional\n`alpha` argument. It returns the  decisions (`reject` below)\nas well as the adjusted $p$-values (`bonf`).\n\n::: {#fbe6b2de .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.346038Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.345960Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.347883Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.347675Z\"}' execution_count=10}\n``` {.python .cell-code}\nreject, bonf = mult_test(fund_mini_pvals, method = \"bonferroni\")[:2]\nreject\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([ True, False, False, False, False])\n```\n:::\n:::\n\n\n\nThe $p$-values `bonf` are simply the `fund_mini_pvalues` multiplied by 5 and truncated to be less than\nor equal to 1.\n\n::: {#3a8f84b8 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.349050Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.348979Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.351013Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.350820Z\"}' execution_count=11}\n``` {.python .cell-code}\nbonf, np.minimum(fund_mini_pvals * 5, 1)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n(array([0.03101178, 1.        , 0.05800491, 1.        , 1.        ]),\n array([0.03101178, 1.        , 0.05800491, 1.        , 1.        ]))\n```\n:::\n:::\n\n\nTherefore, using Bonferroni’s method, we are able to reject the null hypothesis only for Manager\nOne while controlling FWER at $0.05$.\n\nBy contrast, using Holm’s method, the adjusted $p$-values indicate\nthat we can  reject the null\nhypotheses for Managers One and Three at a FWER of $0.05$.\n\n::: {#d05ef091 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.352174Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.352109Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.381192Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.380965Z\"}' execution_count=12}\n``` {.python .cell-code}\nmult_test(fund_mini_pvals, method = \"holm\", alpha=0.05)[:2]\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n(array([ True, False,  True, False, False]),\n array([0.03101178, 1.        , 0.04640393, 1.        , 1.        ]))\n```\n:::\n:::\n\n\n\nAs discussed previously, Manager One seems to perform particularly\nwell, whereas Manager Two has poor performance.\n\n::: {#44766514 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.382469Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.382398Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.384782Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.384577Z\"}' execution_count=13}\n``` {.python .cell-code}\nfund_mini.mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nManager1    3.0\nManager2   -0.1\nManager3    2.8\nManager4    0.5\nManager5    0.3\ndtype: float64\n```\n:::\n:::\n\n\n\nIs there evidence of a meaningful difference in performance between\nthese two managers?  We can check this by performing a  paired $t$-test  using the `ttest_rel()` function\nfrom `scipy.stats`:\n\n::: {#18914401 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.385897Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.385834Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.388040Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.387835Z\"}' execution_count=14}\n``` {.python .cell-code}\nttest_rel(fund_mini['Manager1'],\n          fund_mini['Manager2']).pvalue\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n0.038391072368079586\n```\n:::\n:::\n\n\nThe test results in a $p$-value of 0.038,\nsuggesting a statistically significant difference.\n\nHowever, we decided to perform this test only after examining the data\nand noting that Managers One and Two had the highest and lowest mean\nperformances.  In a sense, this means that we have implicitly\nperformed ${5 \\choose 2} = 5(5-1)/2=10$ hypothesis tests, rather than\njust one, as discussed in  Section~\\ref{tukey.sec}.  Hence, we use the\n`pairwise_tukeyhsd()`  function from\n`statsmodels.stats.multicomp` to apply Tukey’s method\n  in order to adjust for multiple testing.  This function takes\nas input a fitted *ANOVA*  regression model, which is\nessentially just a linear regression in which all of the predictors\nare qualitative.  In this case, the response consists of the monthly\nexcess returns achieved by each manager, and the predictor indicates\nthe manager to which each return corresponds.\n\n::: {#09cb542e .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.389205Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.389140Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.776782Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.776519Z\"}' execution_count=15}\n``` {.python .cell-code}\nreturns = np.hstack([fund_mini.iloc[:,i] for i in range(5)])\nmanagers = np.hstack([[i+1]*50 for i in range(5)])\ntukey = pairwise_tukeyhsd(returns, managers)\nprint(tukey.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMultiple Comparison of Means - Tukey HSD, FWER=0.05\n===================================================\ngroup1 group2 meandiff p-adj   lower  upper  reject\n---------------------------------------------------\n     1      2     -3.1 0.1862 -6.9865 0.7865  False\n     1      3     -0.2 0.9999 -4.0865 3.6865  False\n     1      4     -2.5 0.3948 -6.3865 1.3865  False\n     1      5     -2.7 0.3152 -6.5865 1.1865  False\n     2      3      2.9 0.2453 -0.9865 6.7865  False\n     2      4      0.6 0.9932 -3.2865 4.4865  False\n     2      5      0.4 0.9986 -3.4865 4.2865  False\n     3      4     -2.3  0.482 -6.1865 1.5865  False\n     3      5     -2.5 0.3948 -6.3865 1.3865  False\n     4      5     -0.2 0.9999 -4.0865 3.6865  False\n---------------------------------------------------\n```\n:::\n:::\n\n\n\nThe `pairwise_tukeyhsd()` function provides confidence intervals\nfor the difference between each pair of managers (`lower` and\n`upper`), as well as a $p$-value. All of these quantities have\nbeen adjusted for multiple testing. Notice that the $p$-value for the\ndifference between Managers One and Two has increased from $0.038$ to\n$0.186$, so there is no longer clear evidence of a difference between\nthe managers’ performances.  We can plot the confidence intervals for\nthe pairwise comparisons using the `plot_simultaneous()` method\nof `tukey`. Any pair of intervals that don’t overlap indicates a significant difference at the nominal level of 0.05. In this case,\nno differences are considered significant as reported in the table above.\n\n::: {#c81eab4e .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.778173Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.778094Z\",\"iopub.status.idle\":\"2024-06-04T23:20:07.836461Z\",\"shell.execute_reply\":\"2024-06-04T23:20:07.836186Z\"}' execution_count=16}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(8,8))\ntukey.plot_simultaneous(ax=ax);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch13-multiple-lab_files/figure-html/cell-17-output-1.png){width=781 height=505}\n:::\n:::\n\n\n## False Discovery Rate\nNow we perform hypothesis tests for all 2,000 fund managers in the\n`Fund`  dataset. We perform a one-sample $t$-test\nof   $H_{0,j}: \\mu_j=0$, which states that the\n$j$th fund manager’s mean return is zero.\n\n::: {#a960a254 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:07.837958Z\",\"iopub.status.busy\":\"2024-06-04T23:20:07.837840Z\",\"iopub.status.idle\":\"2024-06-04T23:20:08.163274Z\",\"shell.execute_reply\":\"2024-06-04T23:20:08.162972Z\"}' execution_count=17}\n``` {.python .cell-code}\nfund_pvalues = np.empty(2000)\nfor i, manager in enumerate(Fund.columns):\n    fund_pvalues[i] = ttest_1samp(Fund[manager], 0).pvalue\n```\n:::\n\n\nThere are far too many managers to consider trying to control the FWER.\nInstead, we focus on controlling the FDR: that is, the expected fraction of rejected null hypotheses that are actually false positives.\nThe `multipletests()` function (abbreviated `mult_test()`) can be used to carry out the Benjamini--Hochberg procedure.\n\n::: {#e7ae2d2d .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:08.164892Z\",\"iopub.status.busy\":\"2024-06-04T23:20:08.164804Z\",\"iopub.status.idle\":\"2024-06-04T23:20:08.167176Z\",\"shell.execute_reply\":\"2024-06-04T23:20:08.166950Z\"}' execution_count=18}\n``` {.python .cell-code}\nfund_qvalues = mult_test(fund_pvalues, method = \"fdr_bh\")[1]\nfund_qvalues[:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([0.08988921, 0.991491  , 0.12211561, 0.92342997, 0.95603587,\n       0.07513802, 0.0767015 , 0.07513802, 0.07513802, 0.07513802])\n```\n:::\n:::\n\n\nThe  *q-values* output by the\nBenjamini--Hochberg procedure can be interpreted as the smallest FDR\nthreshold at which we would reject a particular null hypothesis. For\ninstance, a $q$-value of $0.1$ indicates that we can reject the\ncorresponding null hypothesis at an FDR of 10% or greater, but that\nwe cannot reject the null hypothesis at an FDR below 10%.\n\nIf we control the FDR at 10%, then for how many of the fund managers can we reject $H_{0,j}: \\mu_j=0$?\n\n::: {#6d008592 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:08.168349Z\",\"iopub.status.busy\":\"2024-06-04T23:20:08.168282Z\",\"iopub.status.idle\":\"2024-06-04T23:20:08.170285Z\",\"shell.execute_reply\":\"2024-06-04T23:20:08.170079Z\"}' execution_count=19}\n``` {.python .cell-code}\n(fund_qvalues <= 0.1).sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n146\n```\n:::\n:::\n\n\nWe find that 146 of the 2,000 fund managers have a $q$-value below\n0.1; therefore, we are able to conclude that 146 of the fund managers\nbeat the market at an FDR of 10%.  Only about 15 (10% of 146) of\nthese fund managers are likely to be false discoveries.\n\nBy contrast, if we had instead used Bonferroni’s method to control the\nFWER at level $\\alpha=0.1$, then we would have failed to reject any\nnull hypotheses!\n\n::: {#a14eb002 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:08.171469Z\",\"iopub.status.busy\":\"2024-06-04T23:20:08.171399Z\",\"iopub.status.idle\":\"2024-06-04T23:20:08.173322Z\",\"shell.execute_reply\":\"2024-06-04T23:20:08.173118Z\"}' execution_count=20}\n``` {.python .cell-code}\n(fund_pvalues <= 0.1 / 2000).sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n0\n```\n:::\n:::\n\n\n\nFigure~\\ref{Ch12:fig:BonferroniBenjamini} displays the ordered\n$p$-values, $p_{(1)} \\leq p_{(2)} \\leq \\cdots \\leq p_{(2000)}$, for\nthe  `Fund`  dataset, as well as the threshold for rejection by the\nBenjamini--Hochberg procedure.  Recall that the Benjamini--Hochberg\nprocedure identifies the largest $p$-value such that $p_{(j)}<qj/m$,\nand rejects all hypotheses for which the $p$-value is less than or\nequal to $p_{(j)}$. In the code below, we implement the\nBenjamini--Hochberg procedure ourselves, in order to illustrate how it\nworks. We first order the $p$-values. We then identify all $p$-values\nthat satisfy $p_{(j)}<qj/m$ (`sorted_set_`).  Finally, `selected_`\nis a boolean array indicating which $p$-values\n are less than or equal to the largest\n$p$-value in `sorted_[sorted_set_]`. Therefore, `selected_` indexes the\n$p$-values rejected by the Benjamini--Hochberg procedure.\n\n::: {#590dd2bb .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:08.174454Z\",\"iopub.status.busy\":\"2024-06-04T23:20:08.174389Z\",\"iopub.status.idle\":\"2024-06-04T23:20:08.176567Z\",\"shell.execute_reply\":\"2024-06-04T23:20:08.176360Z\"}' execution_count=21}\n``` {.python .cell-code}\nsorted_ = np.sort(fund_pvalues)\nm = fund_pvalues.shape[0]\nq = 0.1\nsorted_set_ = np.where(sorted_ < q * np.linspace(1, m, m) / m)[0]\nif sorted_set_.shape[0] > 0:\n    selected_ = fund_pvalues < sorted_[sorted_set_].max()\n    sorted_set_ = np.arange(sorted_set_.max())\nelse:\n    selected_ = []\n    sorted_set_ = []\n```\n:::\n\n\nWe now reproduce  the middle panel of Figure~\\ref{Ch12:fig:BonferroniBenjamini}.\n\n::: {#c8edba3b .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:08.177790Z\",\"iopub.status.busy\":\"2024-06-04T23:20:08.177725Z\",\"iopub.status.idle\":\"2024-06-04T23:20:08.323352Z\",\"shell.execute_reply\":\"2024-06-04T23:20:08.323097Z\"}' execution_count=22}\n``` {.python .cell-code}\nfig, ax = plt.subplots()\nax.scatter(np.arange(0, sorted_.shape[0]) + 1,\n           sorted_, s=10)\nax.set_yscale('log')\nax.set_xscale('log')\nax.set_ylabel('P-Value')\nax.set_xlabel('Index')\nax.scatter(sorted_set_+1, sorted_[sorted_set_], c='r', s=20)\nax.axline((0, 0), (1,q/m), c='k', ls='--', linewidth=3);\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch13-multiple-lab_files/figure-html/cell-23-output-1.png){width=599 height=431}\n:::\n:::\n\n\n\n## A Re-Sampling Approach\nHere, we implement the re-sampling approach to hypothesis testing\nusing the  `Khan`  dataset, which we investigated in\nSection~\\ref{sec:permutations}.  First, we merge the training and\ntesting data, which results in observations on 83 patients for\n2,308 genes.\n\n::: {#431c97eb .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:08.324883Z\",\"iopub.status.busy\":\"2024-06-04T23:20:08.324753Z\",\"iopub.status.idle\":\"2024-06-04T23:20:08.369825Z\",\"shell.execute_reply\":\"2024-06-04T23:20:08.369575Z\"}' execution_count=23}\n``` {.python .cell-code}\nKhan = load_data('Khan')      \nD = pd.concat([Khan['xtrain'], Khan['xtest']])\nD['Y'] = pd.concat([Khan['ytrain'], Khan['ytest']])\nD['Y'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nY\n2    29\n4    25\n3    18\n1    11\nName: count, dtype: int64\n```\n:::\n:::\n\n\n\nThere are four classes of cancer. For each gene, we compare the mean\nexpression in the second class (rhabdomyosarcoma) to the mean\nexpression in the fourth class (Burkitt’s lymphoma).  Performing a\nstandard two-sample $t$-test  \nusing `ttest_ind()`  from `scipy.stats` on the $11$th\ngene produces a test-statistic of -2.09 and an associated $p$-value\nof 0.0412, suggesting modest evidence of a difference in mean\nexpression levels between the two cancer types.\n\n::: {#b5068766 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:08.371125Z\",\"iopub.status.busy\":\"2024-06-04T23:20:08.371048Z\",\"iopub.status.idle\":\"2024-06-04T23:20:08.374282Z\",\"shell.execute_reply\":\"2024-06-04T23:20:08.374049Z\"}' execution_count=24}\n``` {.python .cell-code}\nD2 = D[lambda df:df['Y'] == 2]\nD4 = D[lambda df:df['Y'] == 4]\ngene_11 = 'G0011'\nobservedT, pvalue = ttest_ind(D2[gene_11],\n                              D4[gene_11],\n                              equal_var=True)\nobservedT, pvalue\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n(-2.0936330736768185, 0.04118643782678394)\n```\n:::\n:::\n\n\n\nHowever, this $p$-value relies on the assumption that under the null\nhypothesis of no difference between the two groups, the test statistic\nfollows a $t$-distribution with $29+25-2=52$ degrees of freedom.\nInstead of using this theoretical null distribution, we can randomly\nsplit the 54 patients into two groups of 29 and 25, and compute a new\ntest statistic.  Under the null hypothesis of no difference between\nthe groups, this new test statistic should have the same distribution\nas our original one.  Repeating this process 10,000 times allows us to\napproximate the null distribution of the test statistic.  We compute\nthe fraction of the time that our observed test statistic exceeds the\ntest statistics obtained via re-sampling.\n\n::: {#4aa84812 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:08.375701Z\",\"iopub.status.busy\":\"2024-06-04T23:20:08.375620Z\",\"iopub.status.idle\":\"2024-06-04T23:20:10.082601Z\",\"shell.execute_reply\":\"2024-06-04T23:20:10.082342Z\"}' execution_count=25}\n``` {.python .cell-code}\nB = 10000\nTnull = np.empty(B)\nD_ = np.hstack([D2[gene_11], D4[gene_11]])\nn_ = D2[gene_11].shape[0]\nD_null = D_.copy()\nfor b in range(B):\n    rng.shuffle(D_null)\n    ttest_ = ttest_ind(D_null[:n_],\n                       D_null[n_:],\n                       equal_var=True)\n    Tnull[b] = ttest_.statistic\n(np.abs(Tnull) < np.abs(observedT)).mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n0.9602\n```\n:::\n:::\n\n\n\nThis fraction, 0.0398,\nis our re-sampling-based $p$-value.\nIt is almost identical to the $p$-value of  0.0412 obtained using the theoretical null distribution.\nWe can plot  a histogram of the re-sampling-based test statistics in order to reproduce  Figure~\\ref{Ch12:fig-permp-1}.\n\n::: {#d8f38957 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:10.084069Z\",\"iopub.status.busy\":\"2024-06-04T23:20:10.083976Z\",\"iopub.status.idle\":\"2024-06-04T23:20:10.194167Z\",\"shell.execute_reply\":\"2024-06-04T23:20:10.193945Z\"}' execution_count=26}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(8,8))\nax.hist(Tnull,\n        bins=100,\n        density=True,\n        facecolor='y',\n        label='Null')\nxval = np.linspace(-4.2, 4.2, 1001)\nax.plot(xval,\n        t_dbn.pdf(xval, D_.shape[0]-2),\n        c='r')\nax.axvline(observedT,\n           c='b',\n           label='Observed')\nax.legend()\nax.set_xlabel(\"Null Distribution of Test Statistic\");\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch13-multiple-lab_files/figure-html/cell-27-output-1.png){width=653 height=651}\n:::\n:::\n\n\nThe re-sampling-based null distribution is almost identical to the theoretical null distribution, which is displayed in red.\n\nFinally, we implement the plug-in re-sampling FDR approach outlined in\nAlgorithm~\\ref{Ch12:alg-plugin-fdr}. Depending on the speed of your\ncomputer, calculating the FDR for all 2,308 genes in the `Khan`\ndataset may take a while.  Hence, we will illustrate the approach on a\nrandom subset of 100 genes.  For each gene, we first compute the\nobserved test statistic, and then produce 10,000 re-sampled test\nstatistics. This may take a few minutes to run.  If you are in a rush,\nthen you could set `B` equal to a smaller value (e.g. `B=500`).\n\n::: {#bff0199b .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:20:10.195499Z\",\"iopub.status.busy\":\"2024-06-04T23:20:10.195421Z\",\"iopub.status.idle\":\"2024-06-04T23:22:59.020789Z\",\"shell.execute_reply\":\"2024-06-04T23:22:59.020469Z\"}' execution_count=27}\n``` {.python .cell-code}\nm, B = 100, 10000\nidx = rng.choice(Khan['xtest'].columns, m, replace=False)\nT_vals = np.empty(m)\nTnull_vals = np.empty((m, B))\n\nfor j in range(m):\n    col = idx[j]\n    T_vals[j] = ttest_ind(D2[col],\n                          D4[col],\n                          equal_var=True).statistic\n    D_ = np.hstack([D2[col], D4[col]])\n    D_null = D_.copy()\n    for b in range(B):\n        rng.shuffle(D_null)\n        ttest_ = ttest_ind(D_null[:n_],\n                           D_null[n_:],\n                           equal_var=True)\n        Tnull_vals[j,b] = ttest_.statistic\n```\n:::\n\n\nNext, we compute the number of rejected null hypotheses $R$, the\nestimated number of false positives $\\widehat{V}$, and the estimated\nFDR, for a range of threshold values $c$ in\nAlgorithm~\\ref{Ch12:alg-plugin-fdr}. The threshold values are chosen\nusing the absolute values of the test statistics from the 100 genes.\n\n::: {#d75a506a .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:22:59.022409Z\",\"iopub.status.busy\":\"2024-06-04T23:22:59.022324Z\",\"iopub.status.idle\":\"2024-06-04T23:22:59.099406Z\",\"shell.execute_reply\":\"2024-06-04T23:22:59.099145Z\"}' execution_count=28}\n``` {.python .cell-code}\ncutoffs = np.sort(np.abs(T_vals))\nFDRs, Rs, Vs = np.empty((3, m))\nfor j in range(m):\n   R = np.sum(np.abs(T_vals) >= cutoffs[j])\n   V = np.sum(np.abs(Tnull_vals) >= cutoffs[j]) / B\n   Rs[j] = R\n   Vs[j] = V\n   FDRs[j] = V / R\n```\n:::\n\n\nNow, for any given FDR, we can find the genes that will be\nrejected. For example, with FDR controlled at 0.1, we reject 15 of the\n100 null hypotheses. On average, we would expect about one or two of\nthese genes (i.e. 10% of 15) to be false discoveries.  At an FDR of\n0.2, we can reject the null hypothesis for 28 genes, of which we\nexpect around six to be false discoveries.\n\nThe variable `idx` stores which\ngenes were included in our 100 randomly-selected genes. Let’s look at\nthe genes whose estimated FDR is less than 0.1.\n\n::: {#918d8abc .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:22:59.100800Z\",\"iopub.status.busy\":\"2024-06-04T23:22:59.100726Z\",\"iopub.status.idle\":\"2024-06-04T23:22:59.102948Z\",\"shell.execute_reply\":\"2024-06-04T23:22:59.102723Z\"}' execution_count=29}\n``` {.python .cell-code}\nsorted(idx[np.abs(T_vals) >= cutoffs[FDRs < 0.1].min()])\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n['G0097',\n 'G0129',\n 'G0182',\n 'G0714',\n 'G0812',\n 'G0941',\n 'G0982',\n 'G1020',\n 'G1022',\n 'G1090',\n 'G1320',\n 'G1634',\n 'G1697',\n 'G1853',\n 'G1854',\n 'G1994',\n 'G2017',\n 'G2115',\n 'G2193']\n```\n:::\n:::\n\n\nAt an FDR threshold of 0.2, more genes are selected, at the cost of having a higher expected\nproportion of false discoveries.\n\n::: {#fcd575a3 .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:22:59.104184Z\",\"iopub.status.busy\":\"2024-06-04T23:22:59.104114Z\",\"iopub.status.idle\":\"2024-06-04T23:22:59.106202Z\",\"shell.execute_reply\":\"2024-06-04T23:22:59.105984Z\"}' execution_count=30}\n``` {.python .cell-code}\nsorted(idx[np.abs(T_vals) >= cutoffs[FDRs < 0.2].min()])\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n['G0097',\n 'G0129',\n 'G0158',\n 'G0182',\n 'G0242',\n 'G0552',\n 'G0679',\n 'G0714',\n 'G0751',\n 'G0812',\n 'G0908',\n 'G0941',\n 'G0982',\n 'G1020',\n 'G1022',\n 'G1090',\n 'G1240',\n 'G1244',\n 'G1320',\n 'G1381',\n 'G1514',\n 'G1634',\n 'G1697',\n 'G1768',\n 'G1853',\n 'G1854',\n 'G1907',\n 'G1994',\n 'G2017',\n 'G2115',\n 'G2193']\n```\n:::\n:::\n\n\nThe next line  generates  Figure~\\ref{fig:labfdr}, which is similar\nto  Figure~\\ref{Ch12:fig-plugin-fdr},\nexcept that it is based on only  a subset of the genes.\n\n::: {#c531507f .cell execution='{\"iopub.execute_input\":\"2024-06-04T23:22:59.107343Z\",\"iopub.status.busy\":\"2024-06-04T23:22:59.107277Z\",\"iopub.status.idle\":\"2024-06-04T23:22:59.196949Z\",\"shell.execute_reply\":\"2024-06-04T23:22:59.196706Z\"}' execution_count=31}\n``` {.python .cell-code}\nfig, ax = plt.subplots()\nax.plot(Rs, FDRs, 'b', linewidth=3)\nax.set_xlabel(\"Number of Rejections\")\nax.set_ylabel(\"False Discovery Rate\");\n```\n\n::: {.cell-output .cell-output-display}\n![](Ch13-multiple-lab_files/figure-html/cell-32-output-1.png){width=589 height=429}\n:::\n:::\n\n\n",
    "supporting": [
      "Ch13-multiple-lab_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}