{
  "hash": "5e29ff93e63b3dc77e744827d8facdc4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Text Mining With Python\nsubtitle: a number of NLP tasks in Python\ndate: 2011-11-29\ncategories:\n    - python\n    - NLP\n    - text mining\n    - code\n#jupyter: \n#  kernelspec:\n#    name: \"ipykernel\"\n#    language: \"python\"\n#    display_name: \"Python 3 (ipykernel)\"\nlastmod: 2022-04-30\n---\n\n![](pexels-brett-jordan-8573113.jpg)\n\n::: {#ed4ff023 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np                           # library for scientific computing and matrix \nimport matplotlib.pyplot as plt              # visualization library\nimport string\nimport re\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer    \n\nnltk.download('twitter_samples')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /home/oren/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nTrue\n```\n:::\n:::\n\n\n::: {#d80a4a4a .cell execution_count=2}\n``` {.python .cell-code}\nnltk.download('stopwords') # <1>\n\n\ndef process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english') \n    \n    tweet = re.sub(r'\\$\\w*', '', tweet) # <2>\n    tweet = re.sub(r'^RT[\\s]+', '', tweet) # <3>\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)  # <4>\n    tweet = re.sub(r'#', '', tweet)             # <5>\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, # <6>\n                               reduce_len=True) # <6>\n    tweet_tokens = tokenizer.tokenize(tweet)    # <6>\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # <7>\n                word not in string.punctuation):  # <8>\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # <9>\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nTrue\n```\n:::\n:::\n\n\n1.  download the stopwords\n2.  remove stock market tickers like \\$GE\n3.  remove old style retweet text \"RT\"\n4.  remove hyperlinks\n5.  remove hashtags\n6.  tokenize tweets\n7.  remove stopwords\n8.  remove punctuation\n9.  stemming word\n\n::: {#159e12f7 .cell execution_count=3}\n``` {.python .cell-code}\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = defaultdict(int)\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs\n\ndef build_vocab(freqs):\n    vocab = [k for k, v in freq.items() if (v > 1 and k != '\\n')]\n    vocab.sort()\n    return vocab\n```\n:::\n\n\nprocessing unknown tokens\n\n::: {#5de31a89 .cell execution_count=4}\n``` {.python .cell-code}\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n```\n:::\n\n\n::: {#a260d8f6 .cell execution_count=5}\n``` {.python .cell-code}\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of tweets:  10000\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}