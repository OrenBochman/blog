{
  "hash": "d3371e19588a336de30a5173870c4844",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Tidy Text Mining With R\nsubtitle: an update on NLP with R\ndate: 2011-11-29\ncategories:\n    - R\n    - NLP\n    - Text Mining\n---\n\n\nComputational Linguistics tasks:\n\n-   [x] create a corpus\n-   [x] clean it up\n-   [ ] create a vocabulary\n-   [ ] create a frequency list\n-   [x] create a term document matrix TDF\n-   [ ] list n-grams\n-   [ ] generate word clouds\n-   [ ] mine TDF it for collocations\\\n-   similarity\n    -   cosine similarity\n    -   TDIDF\n    -   Nearest neighbour word clustering\n-   embeddings\n    -   \\[\\] word embeddings\n    -   \\[\\] sentence embeddings\n-   [ ] concordance\n    -   KWIC, keywords in context\n    -   KWOC, keywords out of context\n\n# Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire_install <- function(libs) {\n\n    for (i in libs){\n        if( !is.element(i, .packages(all.available = TRUE)) ) {\n            install.packages(i)\n        }\n        library(i,character.only = TRUE)\n        }\n}\n\nrequire_install(libs=c('SnowballC','tidytext','dplyr','wordcloud','janeaustenr','gutenbergr','quanteda'))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: RColorBrewer\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPackage version: 3.3.1\nUnicode version: 14.0\nICU version: 70.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nParallel computing: 16 of 16 threads used.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSee https://quanteda.io for tutorials and examples.\n```\n\n\n:::\n:::\n\n\n# Corpus\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoc1 <- \"drugs, hospitals, doctors\"\ndoc2 <- \"smog, pollution, micro-plastics, environment.\"\ndoc3 <- \"doctors, hospitals, healthcare\"\ndoc4 <- \"pollution, environment, water.\"\ndoc5 <- \"I love NLP with deep learning.\"\ndoc6 <- \"I love machine learning.\"\ndoc7 <- \"He said he was keeping the wolf from the door.\"\ndoc8 <- \"Time flies like an arrow, fruit flies like a banana.\"\ndoc9 <- \"pollution, greenhouse gasses, GHG, hydrofluorocarbons, ozone hole, global warming. Montreal Protocol.\"\ndoc10 <- \"greenhouse gasses, hydrofluorocarbons, perfluorocarbons, sulfur hexafluoride, carbon dioxide, carbon monoxide, CO2, hydrofluorocarbons, methane, nitrous oxide.\"\n\ntext <- c(doc1, doc2, doc3, doc4,doc5,doc6,doc7,doc8,doc9,doc10)\n\ntidy_corpus <- tibble(line = 1:10,text=text)   \ntidy_corpus\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n    line text                                                                   \n   <int> <chr>                                                                  \n 1     1 drugs, hospitals, doctors                                              \n 2     2 smog, pollution, micro-plastics, environment.                          \n 3     3 doctors, hospitals, healthcare                                         \n 4     4 pollution, environment, water.                                         \n 5     5 I love NLP with deep learning.                                         \n 6     6 I love machine learning.                                               \n 7     7 He said he was keeping the wolf from the door.                         \n 8     8 Time flies like an arrow, fruit flies like a banana.                   \n 9     9 pollution, greenhouse gasses, GHG, hydrofluorocarbons, ozone hole, glo…\n10    10 greenhouse gasses, hydrofluorocarbons, perfluorocarbons, sulfur hexafl…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_corpus %>% \n    unnest_tokens(word, text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 70 × 2\n    line word       \n   <int> <chr>      \n 1     1 drugs      \n 2     1 hospitals  \n 3     1 doctors    \n 4     2 smog       \n 5     2 pollution  \n 6     2 micro      \n 7     2 plastics   \n 8     2 environment\n 9     3 doctors    \n10     3 hospitals  \n# ℹ 60 more rows\n```\n\n\n:::\n:::\n\n\nnote `unnest_tokens` removes puctuation and lower cases\n\n3.  inspect the corpus\n\n## Text preprocessing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\n\noriginal_books <- austen_books() %>%\n  group_by(book) %>%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %>%\n  ungroup()\n\noriginal_books\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   <chr>                   <fct>                    <int>   <int>\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# ℹ 73,412 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\ntidy_books <- original_books %>%\n  unnest_tokens(word, text)\n\ntidy_books\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 725,055 × 4\n   book                linenumber chapter word       \n   <fct>                    <int>   <int> <chr>      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 and        \n 3 Sense & Sensibility          1       0 sensibility\n 4 Sense & Sensibility          3       0 by         \n 5 Sense & Sensibility          3       0 jane       \n 6 Sense & Sensibility          3       0 austen     \n 7 Sense & Sensibility          5       0 1811       \n 8 Sense & Sensibility         10       1 chapter    \n 9 Sense & Sensibility         10       1 1          \n10 Sense & Sensibility         13       1 the        \n# ℹ 725,045 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(stop_words)\n\ntidy_books <- tidy_books %>%\n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(word)`\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy_books\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   <fct>                    <int>   <int> <chr>      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# ℹ 217,599 more rows\n```\n\n\n:::\n:::\n\n\n6.  this removes stop words\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_books %>%\n  count(word, sort = TRUE) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13,914 × 2\n   word       n\n   <chr>  <int>\n 1 miss    1855\n 2 time    1337\n 3 fanny    862\n 4 dear     822\n 5 lady     817\n 6 sir      806\n 7 day      797\n 8 emma     787\n 9 sister   727\n10 house    699\n# ℹ 13,904 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\ntidy_books %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 600) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#devtools::install_github(\"ropensci/gutenbergr\")\nlibrary(gutenbergr)\n\n#hgwells <- gutenberg_download(c(35, 36,  159, 456, 1047, 3691, 5230, 11870, 12163, 23218, 28218, 35461,39585))\nhgwells <- gutenberg_download(c(35, 36,  159))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDetermining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`curl` package not installed, falling back to using `url()`\nUsing mirror http://aleph.gutenberg.org\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/5/159/159.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_hgwells <- hgwells %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(word)`\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_hgwells %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8,146 × 2\n   word         n\n   <chr>    <int>\n 1 time       328\n 2 people     205\n 3 martians   165\n 4 black      152\n 5 night      140\n 6 machine    133\n 7 found      110\n 8 white      108\n 9 road       105\n10 day        102\n# ℹ 8,136 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))\n\n\ntidy_bronte <- bronte %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(word)`\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy_bronte %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 23,213 × 2\n   word       n\n   <chr>  <int>\n 1 time    1065\n 2 miss     854\n 3 day      825\n 4 don’t    780\n 5 hand     767\n 6 eyes     714\n 7 night    648\n 8 heart    638\n 9 looked   601\n10 door     591\n# ℹ 23,203 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\n\nfrequency <- bind_rows(mutate(tidy_bronte, author = \"Brontë Sisters\"),\n                       mutate(tidy_hgwells, author = \"H.G. Wells\"), \n                       mutate(tidy_books, author = \"Jane Austen\")) %>% \n  mutate(word = str_extract(word, \"[a-z']+\")) %>%\n  count(author, word) %>%\n  group_by(author) %>%\n  mutate(proportion = n / sum(n)) %>% \n  select(-n) %>% \n  pivot_wider(names_from = author, values_from = proportion) %>%\n  pivot_longer(`Brontë Sisters`:`H.G. Wells`,\n               names_to = \"author\", values_to = \"proportion\")\n\nfrequency\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 54,120 × 4\n   word      `Jane Austen` author          proportion\n   <chr>             <dbl> <chr>                <dbl>\n 1 a            0.00000919 Brontë Sisters  0.0000665 \n 2 a            0.00000919 H.G. Wells      0.0000293 \n 3 aback       NA          Brontë Sisters  0.00000391\n 4 aback       NA          H.G. Wells     NA         \n 5 abaht       NA          Brontë Sisters  0.00000391\n 6 abaht       NA          H.G. Wells     NA         \n 7 abandon     NA          Brontë Sisters  0.0000313 \n 8 abandon     NA          H.G. Wells      0.0000293 \n 9 abandoned    0.00000460 Brontë Sisters  0.0000899 \n10 abandoned    0.00000460 H.G. Wells      0.000234  \n# ℹ 54,110 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(scales)\n\n# expect a warning about rows with missing values being removed\nggplot(frequency, aes(x = proportion, y = `Jane Austen`, \n                      color = abs(`Jane Austen` - proportion))) +\n  geom_abline(color = \"gray40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  scale_color_gradient(limits = c(0, 0.001), \n                       low = \"darkslategray4\", high = \"gray75\") +\n  facet_wrap(~author, ncol = 2) +\n  theme(legend.position=\"none\") +\n  labs(y = \"Jane Austen\", x = NULL)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 39274 rows containing missing values (`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 39276 rows containing missing values (`geom_text()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n  cor.test(data = frequency[frequency$author == \"Brontë Sisters\",], ~ proportion + `Jane Austen`)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 110.73, df = 10275, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7286645 0.7462983\nsample estimates:\n      cor \n0.7376071 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(data = frequency[frequency$author == \"H.G. Wells\",], \n         ~ proportion + `Jane Austen`)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 29.497, df = 4567, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3753856 0.4241064\nsample estimates:\n      cor \n0.4000286 \n```\n\n\n:::\n:::\n\n\nkwik and kwok\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quanteda)\nlibrary(gutenbergr)\n\nausten_works = gutenberg_works(author == \"Austen, Jane\")\nausten = gutenberg_download(austen_works$gutenberg_id)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/3/4/1342/1342.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(hgwells)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  gutenberg_id text              \n         <int> <chr>             \n1           35 \"The Time Machine\"\n2           35 \"\"                \n3           35 \"An Invention\"    \n4           35 \"\"                \n5           35 \"by H. G. Wells\"  \n6           35 \"\"                \n```\n\n\n:::\n\n```{.r .cell-code}\n# tidy_hgwells <- hgwells %>%\n#   unnest_tokens(word, text) %>%\n#   anti_join(stop_words)\n\n#head(tidy_hgwells)\n\nthe_corpus <- corpus(austen)\nthe_tokens <- tokens(the_corpus,case_insensitive = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: case_insensitive argument is not used.\n```\n\n\n:::\n\n```{.r .cell-code}\nkwic_table <- kwic(the_tokens,pattern = \"lady\",index = 1:100)\n#kwic_table <- kwic(tokens(tidy_hgwells$word),pattern = \"time\")\n\n#kwic_table <- kwic(tokens(tidy_hgwells$word),pattern = \"machine\",index = 1:400, case_insensitive = TRUE)\nnrow(kwic_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2008\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(kwic_table,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeyword-in-context with 10 matches.                                                          \n  [text61, 5]                Gloucester, by which | lady |\n  [text98, 7]                deserved by his own. | Lady |\n [text100, 8] youthful infatuation which made her | Lady |\n [text112, 6]            her kindness and advice, | Lady |\n [text118, 4]                   passed away since | Lady |\n [text122, 2]                                That | Lady |\n [text142, 2]                                  To | Lady |\n [text143, 8]              favourite, and friend. | Lady |\n [text169, 1]                                     | Lady |\n [text177, 3]                   immediately after | Lady |\n                                \n ( who died 1800 )              \n Elliot had been an excellent   \n Elliot, had never              \n Elliot mainly relied for the   \n Elliot’s death, and they       \n Russell, of steady age         \n Russell, indeed, she           \n Russell loved them all;        \n Russell’s temples had long been\n Russell out of all the         \n```\n\n\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}