{
  "hash": "151841cbe1cd8a5949b123d6d73fbefa",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'S3 with boto3'\ncategories: [s3, boto3]\ndraft: true\n---\n\n\n# boto3\nThe SDK is composed of two key Python packages: Botocore (the library providing the low-level functionality shared between the Python SDK and the AWS CLI) and Boto3 (the package implementing the Python SDK itself).\nIt suport a number of services, not just s3\n\n## install the package\n\n```{zsh}\npip install boto3[crt]\n```\n\n## configure\nthis is stored in  ~/.aws/credentials\n\n```{txt}\n[default]\naws_access_key_id = YOUR_ACCESS_KEY\naws_secret_access_key = YOUR_SECRET_KEY\nregion=us-east-1\n```\n\n## Using Boto3\n\n```{{python}}\n\n# import boto3 \nimport boto3\n\n# indicate which service or services you're going to use\n\n# let's use Amazon S3\ns3 = boto3.resource('s3')\n```\n## list all bucket\n```{{python}}\nimport boto3\ns3 = boto3.resource('s3')\n\n# Print out bucket names\nfor bucket in s3.buckets.all():\n    print(bucket.name)\n```\n\n# Upload a new file\n```{{python}}\nimport boto3\ns3 = boto3.resource('s3')\ndata = open('test.jpg', 'rb')\ns3.Bucket('my-bucket').put_object(Key='test.jpg', Body=data)\n```\n\n## creating a bucket\n```{{python}}\nimport logging\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndef create_bucket(bucket_name, region=None):\n    \"\"\"Create an S3 bucket in a specified region\n    If a region is not specified, the bucket is created in the S3 default\n    region (us-east-1).\n    :param bucket_name: Bucket to create\n    :param region: String region to create bucket in, e.g., 'us-west-2'\n    :return: True if bucket created, else False\n    \"\"\"\n    # Create bucket\n    try:\n        if region is None:\n            s3_client = boto3.client('s3')\n            s3_client.create_bucket(Bucket=bucket_name)\n        else:\n            s3_client = boto3.client('s3', region_name=region)\n            location = {'LocationConstraint': region}\n            s3_client.create_bucket(Bucket=bucket_name,\n                                    CreateBucketConfiguration=location)\n    except ClientError as e:\n        logging.error(e)\n        return False\n    return True\n```\n## List existing buckets¶\n\n```{{python}}\n\n# Retrieve the list of existing buckets\ns3 = boto3.client('s3')\nresponse = s3.list_buckets()\n\n# Output the bucket names\nprint('Existing buckets:')\nfor bucket in response['Buckets']:\n    print(f'  {bucket[\"Name\"]}')\n```\n## Uploading files\n\n```{{python}}\nimport logging\nimport boto3\nfrom botocore.exceptions import ClientError\nimport os\n\ndef upload_file(file_name, bucket, object_name=None):\n    \"\"\"Upload a file to an S3 bucket\n    :param file_name: File to upload\n    :param bucket: Bucket to upload to\n    :param object_name: S3 object name. If not specified then file_name is used\n    :return: True if file was uploaded, else False\n    \"\"\"\n    # If S3 object_name was not specified, use file_name\n    if object_name is None:\n        object_name = os.path.basename(file_name)\n    # Upload the file\n    s3_client = boto3.client('s3')\n    try:\n        response = s3_client.upload_file(file_name, bucket, object_name)\n    except ClientError as e:\n        logging.error(e)\n        return False\n    return True\n```\n```{{python}}\ns3 = boto3.client('s3')\nwith open(\"FILE_NAME\", \"rb\") as f:\n    s3.upload_fileobj(f, \"BUCKET_NAME\", \"OBJECT_NAME\")\n```\n### metadata, ACL\ns3 supports adding metadata to items one \n```{{python}}\ns3.upload_file(\n    'FILE_NAME', 'BUCKET_NAME', 'OBJECT_NAME',\n    ExtraArgs={'Metadata': {'mykey': 'myvalue'}}\n)\n```\n### proess callback\n```{{python}}\nimport os\nimport sys\nimport threading\nclass ProgressPercentage(object):\n    def __init__(self, filename):\n        self._filename = filename\n        self._size = float(os.path.getsize(filename))\n        self._seen_so_far = 0\n        self._lock = threading.Lock()\n    def __call__(self, bytes_amount):\n        # To simplify, assume this is hooked up to a single filename\n        with self._lock:\n            self._seen_so_far += bytes_amount\n            percentage = (self._seen_so_far / self._size) * 100\n            sys.stdout.write(\n                \"\\r%s  %s / %s  (%.2f%%)\" % (\n                    self._filename, self._seen_so_far, self._size,\n                    percentage))\n            sys.stdout.flush()\n```\n```{{python}}\ns3.upload_file(\n    'FILE_NAME', 'BUCKET_NAME', 'OBJECT_NAME',\n    Callback=ProgressPercentage('FILE_NAME')\n)\n```\n\n# dowloading files\n```{{python}}\nimport boto3\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n```\n```{{python}}\ns3 = boto3.client('s3')\nwith open('FILE_NAME', 'wb') as f:\n    s3.download_fileobj('BUCKET_NAME', 'OBJECT_NAME', f)\n```\ndownloads supportds extraArgs and Callback parameters\n## Advanced settings\nWhen uploading, downloading, or copying a file or S3 object, the AWS SDK for Python automatically manages retries and multipart and non-multipart transfers, but it is also possible to confugure how these are done.\n\n## Multipart transfers¶\n```{{python}}\nimport boto3\nfrom boto3.s3.transfer import TransferConfig\n\n# Set the desired multipart threshold value (5GB)\nGB = 1024 ** 3\nconfig = TransferConfig(multipart_threshold=5*GB)\n\n# Perform the transfer\ns3 = boto3.client('s3')\ns3.upload_file('FILE_NAME', 'BUCKET_NAME', 'OBJECT_NAME', Config=config)\n```\n\n## Concurrent transfer operations¶\n\n```{{python}}\n\n# To consume less downstream bandwidth, decrease the maximum concurrency\nconfig = TransferConfig(max_concurrency=5)\n\n# Download an S3 object\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME', Config=config)\n```\n\n## Threads\n```{{python}}\n\n# Disable thread use/transfer concurrency\nconfig = TransferConfig(use_threads=False)\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME', Config=config)\n```\n\n",
    "supporting": [
      "s3-boto3_files"
    ],
    "filters": [],
    "includes": {}
  }
}