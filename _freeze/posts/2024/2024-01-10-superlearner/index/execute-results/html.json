{
  "hash": "92c53d944fdf7c093e94cd150fd3d2f4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ndate: 2024-01-10\ntitle: SuperLearner\ndescription: SuperLearner is an ensambeleing library.\ncategories: [demos,code,r]\nengine: knitr\n\nformat:\n  html:\n    code-fold: False\n    code-tools: True\n    code-link: True\n    df-print: kable\nexecute:\n  cache: True\n  capture: True\n  freeze: auto  # re-render only when source changes\n---\n\n\n\nThis is a SuperLearner demo.\n\nSuperLearner is an ensambeleing library.\n\nThis page is taken out of the documentation\n\n## Setup dataset\n\n\n\n::: {.cell}\n\n```{#lst-mass-data-set .r .cell-code  lst-cap=\"dataset setup\"}\n#install.packages(c(\"SuperLearner\",\"caret\", \"glmnet\", \"randomForest\", \"ggplot2\", \"RhpcBLASctl\",\"xgboost\",\"ranger\"))\ndata(Boston, package = \"MASS\")\n\n?MASS::Boston  # Review info on the Boston dataset.>\n\ncolSums(is.na(Boston)) # Check for any missing data - looks like we don't have any.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   crim      zn   indus    chas     nox      rm     age     dis     rad     tax \n      0       0       0       0       0       0       0       0       0       0 \nptratio   black   lstat    medv \n      0       0       0       0 \n```\n\n\n:::\n\n```{#lst-mass-data-set .r .cell-code  lst-cap=\"dataset setup\"}\noutcome = Boston$medv #Extract our outcome variable from the dataframe.\n\ndata = subset(Boston, select = -medv) # Create a dataframe to contain our explanatory variables.\nhead(data)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|    crim| zn| indus| chas|   nox|    rm|  age|    dis| rad| tax| ptratio|  black| lstat|\n|-------:|--:|-----:|----:|-----:|-----:|----:|------:|---:|---:|-------:|------:|-----:|\n| 0.00632| 18|  2.31|    0| 0.538| 6.575| 65.2| 4.0900|   1| 296|    15.3| 396.90|  4.98|\n| 0.02731|  0|  7.07|    0| 0.469| 6.421| 78.9| 4.9671|   2| 242|    17.8| 396.90|  9.14|\n| 0.02729|  0|  7.07|    0| 0.469| 7.185| 61.1| 4.9671|   2| 242|    17.8| 392.83|  4.03|\n| 0.03237|  0|  2.18|    0| 0.458| 6.998| 45.8| 6.0622|   3| 222|    18.7| 394.63|  2.94|\n| 0.06905|  0|  2.18|    0| 0.458| 7.147| 54.2| 6.0622|   3| 222|    18.7| 396.90|  5.33|\n| 0.02985|  0|  2.18|    0| 0.458| 6.430| 58.7| 6.0622|   3| 222|    18.7| 394.12|  5.21|\n\n</div>\n:::\n\n```{#lst-mass-data-set .r .cell-code  lst-cap=\"dataset setup\"}\nstr(data) # Check structure of our dataframe.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t506 obs. of  13 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ black  : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n```\n\n\n:::\n\n```{#lst-mass-data-set .r .cell-code  lst-cap=\"dataset setup\"}\ndim(data) # Review our dimensions.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 506  13\n```\n\n\n:::\n\n```{#lst-mass-data-set .r .cell-code  lst-cap=\"dataset setup\"}\nset.seed(1)# Set a seed for reproducibility in this random sampling.\n\ntrain_obs = sample(nrow(data), 150) # Reduce to a dataset of 150 observations to speed up model fitting.\n\nx_train = data[train_obs, ] # X is our training sample.\n\n\n\nx_holdout = data[-train_obs, ] # Create a holdout set for evaluating model performance.\n# Note: cross-validation is even better than a single holdout sample.\n\noutcome_bin = as.numeric(outcome > 22) # Create a binary outcome variable: towns in which median home value is > 22,000.\n\ny_train = outcome_bin[train_obs]\ny_holdout = outcome_bin[-train_obs]\n\n\ntable(y_train, useNA = \"ifany\") # Review the outcome variable distribution.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ny_train\n 0  1 \n92 58 \n```\n\n\n:::\n:::\n\n\n\n## Review available models\n\n\n\n::: {.cell}\n\n```{#lst-model-review .r .cell-code  lst-cap=\"model review\"}\nlibrary(SuperLearner)\n\nlistWrappers() # Review available models.\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nAll prediction algorithm wrappers in SuperLearner:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"SL.bartMachine\"      \"SL.bayesglm\"         \"SL.biglasso\"        \n [4] \"SL.caret\"            \"SL.caret.rpart\"      \"SL.cforest\"         \n [7] \"SL.earth\"            \"SL.gam\"              \"SL.gbm\"             \n[10] \"SL.glm\"              \"SL.glm.interaction\"  \"SL.glmnet\"          \n[13] \"SL.ipredbagg\"        \"SL.kernelKnn\"        \"SL.knn\"             \n[16] \"SL.ksvm\"             \"SL.lda\"              \"SL.leekasso\"        \n[19] \"SL.lm\"               \"SL.loess\"            \"SL.logreg\"          \n[22] \"SL.mean\"             \"SL.nnet\"             \"SL.nnls\"            \n[25] \"SL.polymars\"         \"SL.qda\"              \"SL.randomForest\"    \n[28] \"SL.ranger\"           \"SL.ridge\"            \"SL.rpart\"           \n[31] \"SL.rpartPrune\"       \"SL.speedglm\"         \"SL.speedlm\"         \n[34] \"SL.step\"             \"SL.step.forward\"     \"SL.step.interaction\"\n[37] \"SL.stepAIC\"          \"SL.svm\"              \"SL.template\"        \n[40] \"SL.xgboost\"         \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAll screening algorithm wrappers in SuperLearner:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"All\"\n[1] \"screen.corP\"           \"screen.corRank\"        \"screen.glmnet\"        \n[4] \"screen.randomForest\"   \"screen.SIS\"            \"screen.template\"      \n[7] \"screen.ttest\"          \"write.screen.template\"\n```\n\n\n:::\n\n```{#lst-model-review .r .cell-code  lst-cap=\"model review\"}\nSL.glmnet\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (Y, X, newX, family, obsWeights, id, alpha = 1, nfolds = 10, \n    nlambda = 100, useMin = TRUE, loss = \"deviance\", ...) \n{\n    .SL.require(\"glmnet\")\n    if (!is.matrix(X)) {\n        X <- model.matrix(~-1 + ., X)\n        newX <- model.matrix(~-1 + ., newX)\n    }\n    fitCV <- glmnet::cv.glmnet(x = X, y = Y, weights = obsWeights, \n        lambda = NULL, type.measure = loss, nfolds = nfolds, \n        family = family$family, alpha = alpha, nlambda = nlambda, \n        ...)\n    pred <- predict(fitCV, newx = newX, type = \"response\", s = ifelse(useMin, \n        \"lambda.min\", \"lambda.1se\"))\n    fit <- list(object = fitCV, useMin = useMin)\n    class(fit) <- \"SL.glmnet\"\n    out <- list(pred = pred, fit = fit)\n    return(out)\n}\n<bytecode: 0x648245d66b50>\n<environment: namespace:SuperLearner>\n```\n\n\n:::\n:::\n\n\n\n## Fit individual models\n\nLet’s fit 2 separate models: lasso (sparse, penalized OLS) and random forest. We specify family = binomial() because we are predicting a binary outcome, aka classification. With a continuous outcome we would specify family = gaussian().\n\n\n\n::: {.cell}\n\n```{#lst-model-fit-lasso .r .cell-code  lst-cap=\"fit lasso\"}\nset.seed(1) # Set the seed for reproducibility.\n\nsl_lasso = SuperLearner(Y = y_train, X = x_train, family = binomial(),\n                        SL.library = \"SL.glmnet\") # Fit lasso model.\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: glmnet\n```\n\n\n:::\n\n```{#lst-model-fit-lasso .r .cell-code  lst-cap=\"fit lasso\"}\nsl_lasso\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nSuperLearner(Y = y_train, X = x_train, family = binomial(), SL.library = \"SL.glmnet\") \n\n                    Risk Coef\nSL.glmnet_All 0.08484849    1\n```\n\n\n:::\n\n```{#lst-model-fit-lasso .r .cell-code  lst-cap=\"fit lasso\"}\nnames(sl_lasso)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"call\"              \"libraryNames\"      \"SL.library\"       \n [4] \"SL.predict\"        \"coef\"              \"library.predict\"  \n [7] \"Z\"                 \"cvRisk\"            \"family\"           \n[10] \"fitLibrary\"        \"cvFitLibrary\"      \"varNames\"         \n[13] \"validRows\"         \"method\"            \"whichScreen\"      \n[16] \"control\"           \"cvControl\"         \"errorsInCVLibrary\"\n[19] \"errorsInLibrary\"   \"metaOptimizer\"     \"env\"              \n[22] \"times\"            \n```\n\n\n:::\n\n```{#lst-model-fit-lasso .r .cell-code  lst-cap=\"fit lasso\"}\nsl_lasso$cvRisk[which.min(sl_lasso$cvRisk)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSL.glmnet_All \n   0.08484849 \n```\n\n\n:::\n\n```{#lst-model-fit-lasso .r .cell-code  lst-cap=\"fit lasso\"}\nstr(sl_lasso$fitLibrary$SL.glmnet_All$object, max.level = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 12\n $ lambda    : num [1:90] 0.317 0.289 0.263 0.24 0.218 ...\n $ cvm       : num [1:90] 1.34 1.28 1.21 1.16 1.1 ...\n $ cvsd      : num [1:90] 0.0359 0.033 0.0304 0.0292 0.0291 ...\n $ cvup      : num [1:90] 1.38 1.31 1.24 1.18 1.13 ...\n $ cvlo      : num [1:90] 1.31 1.24 1.18 1.13 1.08 ...\n $ nzero     : Named int [1:90] 0 1 1 1 1 1 2 2 2 2 ...\n  ..- attr(*, \"names\")= chr [1:90] \"s0\" \"s1\" \"s2\" \"s3\" ...\n $ call      : language glmnet::cv.glmnet(x = X, y = Y, weights = obsWeights, lambda = NULL, type.measure = loss,      nfolds = nfolds, f| __truncated__\n $ name      : Named chr \"Binomial Deviance\"\n  ..- attr(*, \"names\")= chr \"deviance\"\n $ glmnet.fit:List of 13\n  ..- attr(*, \"class\")= chr [1:2] \"lognet\" \"glmnet\"\n $ lambda.min: num 0.0161\n $ lambda.1se: num 0.0652\n $ index     : int [1:2, 1] 33 18\n  ..- attr(*, \"dimnames\")=List of 2\n - attr(*, \"class\")= chr \"cv.glmnet\"\n```\n\n\n:::\n\n```{#lst-model-fit-lasso .r .cell-code  lst-cap=\"fit lasso\"}\nsl_rf = SuperLearner(Y = y_train, X = x_train, family = binomial(),\n                     SL.library = \"SL.ranger\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: ranger\n```\n\n\n:::\n\n```{#lst-model-fit-lasso .r .cell-code  lst-cap=\"fit lasso\"}\nnames(sl_lasso)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"call\"              \"libraryNames\"      \"SL.library\"       \n [4] \"SL.predict\"        \"coef\"              \"library.predict\"  \n [7] \"Z\"                 \"cvRisk\"            \"family\"           \n[10] \"fitLibrary\"        \"cvFitLibrary\"      \"varNames\"         \n[13] \"validRows\"         \"method\"            \"whichScreen\"      \n[16] \"control\"           \"cvControl\"         \"errorsInCVLibrary\"\n[19] \"errorsInLibrary\"   \"metaOptimizer\"     \"env\"              \n[22] \"times\"            \n```\n\n\n:::\n\n```{#lst-model-fit-lasso .r .cell-code  lst-cap=\"fit lasso\"}\nsl_lasso$cvRisk[which.min(sl_lasso$cvRisk)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSL.glmnet_All \n   0.08484849 \n```\n\n\n:::\n\n```{#lst-model-fit-lasso .r .cell-code  lst-cap=\"fit lasso\"}\n# Here is the raw glmnet result object:\nstr(sl_lasso$fitLibrary$SL.glmnet_All$object, max.level = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 12\n $ lambda    : num [1:90] 0.317 0.289 0.263 0.24 0.218 ...\n $ cvm       : num [1:90] 1.34 1.28 1.21 1.16 1.1 ...\n $ cvsd      : num [1:90] 0.0359 0.033 0.0304 0.0292 0.0291 ...\n $ cvup      : num [1:90] 1.38 1.31 1.24 1.18 1.13 ...\n $ cvlo      : num [1:90] 1.31 1.24 1.18 1.13 1.08 ...\n $ nzero     : Named int [1:90] 0 1 1 1 1 1 2 2 2 2 ...\n  ..- attr(*, \"names\")= chr [1:90] \"s0\" \"s1\" \"s2\" \"s3\" ...\n $ call      : language glmnet::cv.glmnet(x = X, y = Y, weights = obsWeights, lambda = NULL, type.measure = loss,      nfolds = nfolds, f| __truncated__\n $ name      : Named chr \"Binomial Deviance\"\n  ..- attr(*, \"names\")= chr \"deviance\"\n $ glmnet.fit:List of 13\n  ..- attr(*, \"class\")= chr [1:2] \"lognet\" \"glmnet\"\n $ lambda.min: num 0.0161\n $ lambda.1se: num 0.0652\n $ index     : int [1:2, 1] 33 18\n  ..- attr(*, \"dimnames\")=List of 2\n - attr(*, \"class\")= chr \"cv.glmnet\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit random forest.\nsl_rf = SuperLearner(Y = y_train, X = x_train, family = binomial(), SL.library = \"SL.ranger\")\n\nsl_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nSuperLearner(Y = y_train, X = x_train, family = binomial(), SL.library = \"SL.ranger\") \n\n                    Risk Coef\nSL.ranger_All 0.07620479    1\n```\n\n\n:::\n:::\n\n\n\n## Fit multiple models\n\n\n\n::: {.cell}\n\n```{#lst-fit-multiple-models .r .cell-code  lst-cap=\"lst fit multiple models\"}\nset.seed(1)\nsl = SuperLearner(Y = y_train, X = x_train, family = binomial(),\n  SL.library = c(\"SL.mean\", \"SL.glmnet\", \"SL.ranger\"))\n\nsl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nSuperLearner(Y = y_train, X = x_train, family = binomial(), SL.library = c(\"SL.mean\",  \n    \"SL.glmnet\", \"SL.ranger\")) \n\n                    Risk       Coef\nSL.mean_All   0.23773937 0.00000000\nSL.glmnet_All 0.08847869 0.02212495\nSL.ranger_All 0.07053788 0.97787505\n```\n\n\n:::\n\n```{#lst-fit-multiple-models .r .cell-code  lst-cap=\"lst fit multiple models\"}\nsl$times$everything\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  1.324   0.016   1.346 \n```\n\n\n:::\n:::\n\n\n\nAgain, the coefficient is how much weight SuperLearner puts on that model in the weighted-average. So if coefficient = 0 it means that model is not used at all. Here we see that random forest is given the most weight, following by lasso.\n\nSo we have an automatic ensemble of multiple learners based on the cross-validated performance of those learners, nice!\n\n#  Predict on new data\n\nNow that we have an ensemble let’s predict back on our holdout dataset and review the results.\n\n\n\n::: {.cell}\n\n```{#lst-predict .r .cell-code  lst-cap=\"predict\"}\n# onlySL is set to TRUE so we don't fit algorithms that had weight = 0, saving computation.\npred = predict(sl, x_holdout, onlySL = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: glmnet\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: ranger\n```\n\n\n:::\n\n```{#lst-predict .r .cell-code  lst-cap=\"predict\"}\n# Check the structure of this prediction object.\nstr(pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 2\n $ pred           : num [1:356, 1] 0.577 0.891 0.939 0.943 0.853 ...\n $ library.predict: num [1:356, 1:3] 0 0 0 0 0 0 0 0 0 0 ...\n```\n\n\n:::\n\n```{#lst-predict .r .cell-code  lst-cap=\"predict\"}\n# Review the columns of $library.predict.\nsummary(pred$library.predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       V1          V2                 V3           \n Min.   :0   Min.   :0.000008   Min.   :0.0002625  \n 1st Qu.:0   1st Qu.:0.026812   1st Qu.:0.0484115  \n Median :0   Median :0.310300   Median :0.3057313  \n Mean   :0   Mean   :0.404139   Mean   :0.4124046  \n 3rd Qu.:0   3rd Qu.:0.782474   3rd Qu.:0.8181667  \n Max.   :0   Max.   :0.998259   Max.   :0.9980000  \n```\n\n\n:::\n\n```{#lst-predict .r .cell-code  lst-cap=\"predict\"}\nlibrary(ggplot2)\nqplot(pred$pred[, 1]) + theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/lst-predict-1.png){width=672}\n:::\n\n```{#lst-predict .r .cell-code  lst-cap=\"predict\"}\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n# Scatterplot of original values (0, 1) and predicted values.\n# Ideally we would use jitter or slight transparency to deal with overlap.\nqplot(y_holdout, pred$pred[, 1]) + theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/lst-predict-2.png){width=672}\n:::\n\n```{#lst-predict .r .cell-code  lst-cap=\"predict\"}\n# Review AUC - Area Under Curve\npred_rocr = ROCR::prediction(pred$pred, y_holdout)\nauc = ROCR::performance(pred_rocr, measure = \"auc\", x.measure = \"cutoff\")@y.values[[1]]\nauc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9443598\n```\n\n\n:::\n:::\n\n\n\n# Fit ensemble with external cross-validation\n\nWhat we don’t have yet is an estimate of the performance of the ensemble itself. Right now we are just hopeful that the ensemble weights are successful in improving over the best single algorithm.\n\nIn order to estimate the performance of the SuperLearner ensemble we need an “external” layer of cross-validation, also called **nested cross-validation**. We generate a separate holdout sample that we don’t use to fit the SuperLearner, which allows it to be a good estimate of the SuperLearner’s performance on unseen data. Typically we would run 10 or 20-fold external cross-validation, but even 5-fold is reasonable.\n\nAnother nice result is that we get standard errors on the performance of the individual algorithms and can compare them to the SuperLearner.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\n# Don't have timing info for the CV.SuperLearner unfortunately.\n# So we need to time it manually.\n\nsystem.time({\n  # This will take about 2x as long as the previous SuperLearner.\n  cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),\n                          # For a real analysis we would use V = 10.\n                          V = 3,\n                          SL.library = c(\"SL.mean\", \"SL.glmnet\", \"SL.ranger\"))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  5.235   0.143   5.400 \n```\n\n\n:::\n\n```{.r .cell-code}\n# We run summary on the cv_sl object rather than simply printing the object.\nsummary(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nCV.SuperLearner(Y = y_train, X = x_train, V = 3, family = binomial(), SL.library = c(\"SL.mean\",  \n    \"SL.glmnet\", \"SL.ranger\")) \n\nRisk is based on: Mean Squared Error\n\nAll risk estimates are based on V =  3 \n\n     Algorithm      Ave       se      Min      Max\n Super Learner 0.074388 0.011392 0.064595 0.084275\n   Discrete SL 0.077512 0.011293 0.067146 0.086532\n   SL.mean_All 0.251267 0.010352 0.228500 0.289600\n SL.glmnet_All 0.081595 0.014502 0.072729 0.095522\n SL.ranger_All 0.077512 0.011293 0.067146 0.086532\n```\n\n\n:::\n\n```{.r .cell-code}\n# Review the distribution of the best single learner as external CV folds.\ntable(simplify2array(cv_sl$whichDiscreteSL))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSL.ranger_All \n            3 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the performance with 95% CIs (use a better ggplot theme).\nplot(cv_sl) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave(\"SuperLearner.png\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSaving 7 x 5 in image\n```\n\n\n:::\n:::\n\n\n\n\n## Customize a model hyperparameter\n\nHyperparameters are the configuration settings for an algorithm. OLS has no hyperparameters but essentially every other algorithm does.\n\nThere are two ways to customize a hyperparameter: make a new learner function, or use create.Learner().\n\nLet’s make a variant of random forest that fits more trees, which may increase our accuracy and can’t hurt it (outside of small random variation).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Review the function argument defaults at the top.\nSL.ranger\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (Y, X, newX, family, obsWeights, num.trees = 500, mtry = floor(sqrt(ncol(X))), \n    write.forest = TRUE, probability = family$family == \"binomial\", \n    min.node.size = ifelse(family$family == \"gaussian\", 5, 1), \n    replace = TRUE, sample.fraction = ifelse(replace, 1, 0.632), \n    num.threads = 1, verbose = T, ...) \n{\n    .SL.require(\"ranger\")\n    if (family$family == \"binomial\") {\n        Y = as.factor(Y)\n    }\n    if (is.matrix(X)) {\n        X = data.frame(X)\n    }\n    fit <- ranger::ranger(`_Y` ~ ., data = cbind(`_Y` = Y, X), \n        num.trees = num.trees, mtry = mtry, min.node.size = min.node.size, \n        replace = replace, sample.fraction = sample.fraction, \n        case.weights = obsWeights, write.forest = write.forest, \n        probability = probability, num.threads = num.threads, \n        verbose = verbose)\n    pred <- predict(fit, data = newX)$predictions\n    if (family$family == \"binomial\") {\n        pred = pred[, \"1\"]\n    }\n    fit <- list(object = fit, verbose = verbose)\n    class(fit) <- c(\"SL.ranger\")\n    out <- list(pred = pred, fit = fit)\n    return(out)\n}\n<bytecode: 0x5f8eb7044c60>\n<environment: namespace:SuperLearner>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a new function that changes just the ntree argument.\n# (We could do this in a single line.)\n# \"...\" means \"all other arguments that were sent to the function\"\nSL.rf.better = function(...) {\n  SL.randomForest(..., num.trees = 1000)\n}\n\nset.seed(1)\n\n# Fit the CV.SuperLearner.\n# We use V = 3 to save computation time; for a real analysis use V = 10 or 20.\ncv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(), V = 3,\n                        SL.library = c(\"SL.mean\", \"SL.glmnet\", \"SL.rf.better\", \"SL.ranger\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: glmnet\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: randomForest\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: ranger\n```\n\n\n:::\n\n```{.r .cell-code}\n# Review results.\nsummary(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nCV.SuperLearner(Y = y_train, X = x_train, V = 3, family = binomial(), SL.library = c(\"SL.mean\",  \n    \"SL.glmnet\", \"SL.rf.better\", \"SL.ranger\")) \n\nRisk is based on: Mean Squared Error\n\nAll risk estimates are based on V =  3 \n\n        Algorithm      Ave       se      Min      Max\n    Super Learner 0.075382 0.011316 0.066279 0.085697\n      Discrete SL 0.078154 0.011304 0.065446 0.090042\n      SL.mean_All 0.251267 0.010352 0.228500 0.289600\n    SL.glmnet_All 0.080752 0.014631 0.076383 0.088697\n SL.rf.better_All 0.078075 0.011301 0.065446 0.089805\n    SL.ranger_All 0.080011 0.011452 0.066660 0.090042\n```\n\n\n:::\n\n```{.r .cell-code}\n# Customize the defaults for random forest.\nlearners = create.Learner(\"SL.ranger\", params = list(num.trees = 1000))\n\n# Look at the object.\nlearners\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$grid\nNULL\n\n$names\n[1] \"SL.ranger_1\"\n\n$base_learner\n[1] \"SL.ranger\"\n\n$params\n$params$num.trees\n[1] 1000\n```\n\n\n:::\n\n```{.r .cell-code}\n# List the functions that were created\nlearners$names\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SL.ranger_1\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Review the code that was automatically generated for the function.\n# Notice that it's exactly the same as the function we made manually.\nSL.ranger_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (...) \nSL.ranger(..., num.trees = 1000)\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(1)\n\n# Fit the CV.SuperLearner.\n# We use V = 3 to save computation time; for a real analysis use V = 10 or 20.\ncv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),\n                        V = 3,\n                        SL.library = c(\"SL.mean\", \"SL.glmnet\", learners$names, \"SL.ranger\"))\n\n# Review results.\nsummary(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nCV.SuperLearner(Y = y_train, X = x_train, V = 3, family = binomial(), SL.library = c(\"SL.mean\",  \n    \"SL.glmnet\", learners$names, \"SL.ranger\")) \n\nRisk is based on: Mean Squared Error\n\nAll risk estimates are based on V =  3 \n\n       Algorithm      Ave       se      Min      Max\n   Super Learner 0.073994 0.010872 0.065059 0.083029\n     Discrete SL 0.077243 0.011034 0.065593 0.089188\n     SL.mean_All 0.251267 0.010352 0.228500 0.289600\n   SL.glmnet_All 0.080148 0.013766 0.076383 0.087422\n SL.ranger_1_All 0.077687 0.011077 0.065593 0.089188\n   SL.ranger_All 0.077231 0.011078 0.065675 0.089071\n```\n\n\n:::\n:::\n\n\n\n\n## Test algorithm with multiple hyperparameter settings\n\nThe performance of an algorithm varies based on its hyperparamters, which again are its configuration settings. Some algorithms may not vary much, and others might have far better or worse performance for certain settings. Often we focus our attention on 1 or 2 hyperparameters for a given algorithm because they are the most important ones.\n\nFor random forest there are two particularly important hyperparameters: mtry and maximum leaf nodes. Mtry is how many features are randomly chosen within each decision tree node - in other words, each time the tree considers making a split. Maximum leaf nodes controls how complex each tree can get.\n\nLet’s try 3 different mtry options.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sqrt(p) is the default value of mtry for classification.\nfloor(sqrt(ncol(x_train)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n\n```{.r .cell-code}\n## [1] 3\n# Let's try 3 multiplies of this default: 0.5, 1, and 2.\n(mtry_seq = floor(sqrt(ncol(x_train)) * c(0.5, 1, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 3 7\n```\n\n\n:::\n\n```{.r .cell-code}\n## [1] 1 3 7\nlearners = create.Learner(\"SL.ranger\", tune = list(mtry = mtry_seq))\n\n# Review the resulting object\nlearners\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$grid\n  mtry\n1    1\n2    3\n3    7\n\n$names\n[1] \"SL.ranger_1\" \"SL.ranger_2\" \"SL.ranger_3\"\n\n$base_learner\n[1] \"SL.ranger\"\n\n$params\nlist()\n```\n\n\n:::\n\n```{.r .cell-code}\nSL.ranger_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (...) \nSL.ranger(..., mtry = 1)\n```\n\n\n:::\n\n```{.r .cell-code}\nSL.ranger_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (...) \nSL.ranger(..., mtry = 3)\n```\n\n\n:::\n\n```{.r .cell-code}\nSL.ranger_3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (...) \nSL.ranger(..., mtry = 7)\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(1)\n\n# Fit the CV.SuperLearner.\n# We use V = 3 to save computation time; for a real analysis use V = 10 or 20.\ncv_sl = CV.SuperLearner(Y = y_train, X = x_train, \n                        family = binomial(), V = 3, \n                        SL.library = c(\"SL.mean\", \"SL.glmnet\", learners$names, \"SL.ranger\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: glmnet\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: ranger\n```\n\n\n:::\n\n```{.r .cell-code}\n# Review results.\nsummary(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nCV.SuperLearner(Y = y_train, X = x_train, V = 3, family = binomial(), SL.library = c(\"SL.mean\",  \n    \"SL.glmnet\", learners$names, \"SL.ranger\")) \n\nRisk is based on: Mean Squared Error\n\nAll risk estimates are based on V =  3 \n\n       Algorithm      Ave       se      Min      Max\n   Super Learner 0.067516 0.011105 0.056210 0.081632\n     Discrete SL 0.071761 0.011897 0.057843 0.092998\n     SL.mean_All 0.251267 0.010352 0.228500 0.289600\n   SL.glmnet_All 0.078227 0.015002 0.070147 0.087422\n SL.ranger_1_All 0.102194 0.011360 0.089415 0.111369\n SL.ranger_2_All 0.078006 0.011196 0.064298 0.093298\n SL.ranger_3_All 0.071761 0.011897 0.057843 0.092998\n   SL.ranger_All 0.078469 0.011270 0.065888 0.089501\n```\n\n\n:::\n:::\n\n\n\nWe see here that mtry = 7 performed a little bit better than mtry = 1 or mtry = 3, although the difference is not significant. If we used more data and more cross-validation folds we might see more drastic differences. A higher mtry does better when a small percentage of variables are predictive of the outcome, because it gives each tree a better chance of finding a useful variable.\n\nNote that SL.ranger and SL.ranger_2 have the same settings, and their performance is very similar - statistically a tie. It’s not exactly equivalent due to random variation in the two forests.\n\nA key difference with SuperLearner over caret or other frameworks is that we are not trying to choose the single best hyperparameter or model. Instead, we usually want the best weighted average. So we are including all of the different settings in our SuperLearner, and we may choose a weighted average that includes the same model multiple times but with different settings. That can give us better performance than choosing only the single best settings for a given algorithm, which has some random noise in any case.\n\nMulticore parallelization\nSuperLearner makes it easy to use multiple CPU cores on your computer to speed up the calculations. We first need to setup R for multiple cores, then tell CV.SuperLearner to divide its computations across those cores.\n\nThere are two ways to use multiple cores in R: the “multicore” system and the “snow” system. Windows only supports the “snow” system, which is more difficult to use, whereas macOS and Linux can use either one.\n\nFirst we show the “multicore” system version:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup parallel computation - use all cores on our computer.\n(num_cores = RhpcBLASctl::get_num_cores())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Use 2 of those cores for parallel SuperLearner.\n# Replace \"2\" with \"num_cores\" (without quotes) to use all cores.\noptions(mc.cores = 2)\n\n# Check how many parallel workers we are using (on macOS/Linux).\ngetOption(\"mc.cores\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n\n```{.r .cell-code}\n# We need to set a different type of seed that works across cores.\n# Otherwise the other cores will go rogue and we won't get repeatable results.\n# This version is for the \"multicore\" parallel system in R.\nset.seed(1, \"L'Ecuyer-CMRG\")\n\n# While this is running check CPU using in Activity Monitor / Task Manager.\nsystem.time({\n  cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),\n                          # For a real analysis we would use V = 10.\n                          V = 3,\n                          parallel = \"multicore\",\n                          SL.library = c(\"SL.mean\", \"SL.glmnet\", learners$names, \"SL.ranger\"))\n})\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: parallel\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  1.918   0.272   4.835 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Review results.\nsummary(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nCV.SuperLearner(Y = y_train, X = x_train, V = 3, family = binomial(), SL.library = c(\"SL.mean\",  \n    \"SL.glmnet\", learners$names, \"SL.ranger\"), parallel = \"multicore\") \n\nRisk is based on: Mean Squared Error\n\nAll risk estimates are based on V =  3 \n\n       Algorithm      Ave        se      Min      Max\n   Super Learner 0.089148 0.0149989 0.053080 0.107932\n     Discrete SL 0.097739 0.0174582 0.059888 0.125348\n     SL.mean_All 0.242600 0.0095617 0.226900 0.260500\n   SL.glmnet_All 0.103421 0.0164995 0.059888 0.125348\n SL.ranger_1_All 0.093995 0.0107773 0.070773 0.106005\n SL.ranger_2_All 0.080877 0.0120526 0.053931 0.099784\n SL.ranger_3_All 0.084269 0.0142342 0.044643 0.107982\n   SL.ranger_All 0.081619 0.0120937 0.052445 0.101650\n```\n\n\n:::\n:::\n\n\n\nHere is the “snow” equivalent:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a snow cluster\n# Again, replace 2 with num_cores to use all available cores.\ncluster = parallel::makeCluster(2)\n\n# Check the cluster object.\ncluster\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsocket cluster with 2 nodes on host 'localhost'\n```\n\n\n:::\n\n```{.r .cell-code}\n# Load the SuperLearner package on all workers so they can find\n# SuperLearner::All(), the default screening function which keeps all variables.\nparallel::clusterEvalQ(cluster, library(SuperLearner))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n [1] \"SuperLearner\" \"gam\"          \"foreach\"      \"splines\"      \"nnls\"        \n [6] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[11] \"methods\"      \"base\"        \n\n[[2]]\n [1] \"SuperLearner\" \"gam\"          \"foreach\"      \"splines\"      \"nnls\"        \n [6] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[11] \"methods\"      \"base\"        \n```\n\n\n:::\n\n```{.r .cell-code}\n# We need to explictly export our custom learner functions to the workers.\nparallel::clusterExport(cluster, learners$names)\n\n# We need to set a different type of seed that works across cores.\n# This version is for SNOW parallelization.\n# Otherwise the other cores will go rogue and we won't get repeatable results.\nparallel::clusterSetRNGStream(cluster, 1)\n\n# While this is running check CPU using in Activity Monitor / Task Manager.\nsystem.time({\n  cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),\n                          # For a real analysis we would use V = 10.\n                          V = 3,\n                          parallel = cluster,\n                          SL.library = c(\"SL.mean\", \"SL.glmnet\", learners$names, \"SL.ranger\"))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.007   0.000   5.484 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Review results.\nsummary(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nCV.SuperLearner(Y = y_train, X = x_train, V = 3, family = binomial(), SL.library = c(\"SL.mean\",  \n    \"SL.glmnet\", learners$names, \"SL.ranger\"), parallel = cluster) \n\nRisk is based on: Mean Squared Error\n\nAll risk estimates are based on V =  3 \n\n       Algorithm      Ave        se      Min      Max\n   Super Learner 0.074781 0.0141069 0.063216 0.091040\n     Discrete SL 0.070968 0.0132823 0.063216 0.079742\n     SL.mean_All 0.238600 0.0091846 0.229300 0.246100\n   SL.glmnet_All 0.097299 0.0185871 0.072560 0.120085\n SL.ranger_1_All 0.097233 0.0113360 0.093720 0.101704\n SL.ranger_2_All 0.076001 0.0119746 0.068762 0.082897\n SL.ranger_3_All 0.070968 0.0132823 0.063216 0.079742\n   SL.ranger_All 0.076301 0.0118938 0.069590 0.082703\n```\n\n\n:::\n\n```{.r .cell-code}\n# Stop the cluster workers now that we're done.\nparallel::stopCluster(cluster)\n```\n:::\n\n\n\nIf we want to use multiple cores for normal SuperLearner, not CV.SuperLearner (i.e. external cross-validation to estimate performance), we need to change the function name to mcSuperLearner (“multicore” version) or snowSuperLearner (“snow” version).\n\nFirst the “multicore” version (won’t be parallel on Windows):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set multicore compatible seed.\nset.seed(1, \"L'Ecuyer-CMRG\")\n\n# Fit the SuperLearner.\n(sl = mcSuperLearner(Y = y_train, X = x_train, family = binomial(),\n                    SL.library = c(\"SL.mean\", \"SL.glmnet\", learners$names, \"SL.ranger\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nmcSuperLearner(Y = y_train, X = x_train, family = binomial(), SL.library = c(\"SL.mean\",  \n    \"SL.glmnet\", learners$names, \"SL.ranger\")) \n\n                      Risk       Coef\nSL.mean_All     0.24253498 0.00000000\nSL.glmnet_All   0.09181320 0.02471622\nSL.ranger_1_All 0.09498042 0.00000000\nSL.ranger_2_All 0.07428239 0.00000000\nSL.ranger_3_All 0.07102085 0.92627825\nSL.ranger_All   0.07388773 0.04900552\n```\n\n\n:::\n\n```{.r .cell-code}\n# We see the time is reduced over our initial single-core superlearner.\nsl$times$everything\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  2.749   0.358   1.888 \n```\n\n\n:::\n:::\n\n\n\nNow the “snow” version, which should be parallel on all operating systems.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a snow cluster\n# Reminder: change \"2\" to \"num_cores\" (without quotes) to use all available cores.\n(cluster = parallel::makeCluster(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsocket cluster with 2 nodes on host 'localhost'\n```\n\n\n:::\n\n```{.r .cell-code}\n## socket cluster with 2 nodes on host 'localhost'\n# Load the SuperLearner package on all workers so they can find\n# SuperLearner::All(), the default screening function which keeps all variables.\nparallel::clusterEvalQ(cluster, library(SuperLearner))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n [1] \"SuperLearner\" \"gam\"          \"foreach\"      \"splines\"      \"nnls\"        \n [6] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[11] \"methods\"      \"base\"        \n\n[[2]]\n [1] \"SuperLearner\" \"gam\"          \"foreach\"      \"splines\"      \"nnls\"        \n [6] \"stats\"        \"graphics\"     \"grDevices\"    \"utils\"        \"datasets\"    \n[11] \"methods\"      \"base\"        \n```\n\n\n:::\n\n```{.r .cell-code}\n# We need to explictly export our custom learner functions to the workers.\nparallel::clusterExport(cluster, learners$names)\n\n# We need to set a different type of seed that works across cores.\n# This version is for SNOW parallelization.\n# Otherwise the other cores will go rogue and we won't get repeatable results.\nparallel::clusterSetRNGStream(cluster, 1)\n\n# Fit the SuperLearner.\n(sl = snowSuperLearner(Y = y_train, X = x_train, family = binomial(),\n                      cluster = cluster,\n                      SL.library = c(\"SL.mean\", \"SL.glmnet\", learners$names, \"SL.ranger\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nsnowSuperLearner(cluster = cluster, Y = y_train, X = x_train, family = binomial(),  \n    SL.library = c(\"SL.mean\", \"SL.glmnet\", learners$names, \"SL.ranger\")) \n\n                      Risk       Coef\nSL.mean_All     0.23878189 0.00000000\nSL.glmnet_All   0.08486885 0.13188912\nSL.ranger_1_All 0.09253551 0.00000000\nSL.ranger_2_All 0.07253553 0.01627661\nSL.ranger_3_All 0.06974784 0.85183427\nSL.ranger_All   0.07397735 0.00000000\n```\n\n\n:::\n\n```{.r .cell-code}\n# We see the time is reduced over our initial single-core superlearner.\nsl$times$everything\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.356   0.036   2.624 \n```\n\n\n:::\n:::\n\n\nSuperLearner also supports running across multiple computers at a time, called “multi-node” or “cluster” computing. We will skip that for now.\n\n## Weight distribution for SuperLearner\n\nThe weights or coefficients of the SuperLearner are stochastic - they will change as the data changes. So we don’t necessarily trust a given set of weights as being the “true” weights, but when we use CV.SuperLearner we at least have multiple samples from the distribution of the weights.\n\nWe can write a little function to extract the weights at each CV.SuperLearner iteration and summarize the distribution of those weights. This may be added to the SuperLearner package sometime in the future.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Review meta-weights (coefficients) from a CV.SuperLearner object\nreview_weights = function(cv_sl) {\n  meta_weights = coef(cv_sl)\n  means = colMeans(meta_weights)\n  sds = apply(meta_weights, MARGIN = 2,  FUN = sd)\n  mins = apply(meta_weights, MARGIN = 2, FUN = min)\n  maxs = apply(meta_weights, MARGIN = 2, FUN = max)\n  # Combine the stats into a single matrix.\n  sl_stats = cbind(\"mean(weight)\" = means, \"sd\" = sds, \"min\" = mins, \"max\" = maxs)\n  # Sort by decreasing mean weight.\n  sl_stats[order(sl_stats[, 1], decreasing = TRUE), ]\n}\n\nprint(review_weights(cv_sl), digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                mean(weight)     sd   min   max\nSL.ranger_3_All       0.6907 0.2955 0.411 1.000\nSL.glmnet_All         0.2588 0.2295 0.000 0.437\nSL.ranger_2_All       0.0504 0.0874 0.000 0.151\nSL.mean_All           0.0000 0.0000 0.000 0.000\nSL.ranger_1_All       0.0000 0.0000 0.000 0.000\nSL.ranger_All         0.0000 0.0000 0.000 0.000\n```\n\n\n:::\n:::\n\n\n\nNotice that in this case the ensemble never uses the mean nor the randomForest with mtry = 1. Also the LASSO (glmnet) was only used on a subset of the folds. Adding multiple configurations of randomForest was helpful because mtry = 7 was used. However, based on the minimum column we can see that no algorithm was used every single time.\n\nWe recommend reviewing the weight distribution for any SuperLearner project to better understand which algorithms are chosen for the ensemble.\n\n## Feature selection (screening)\n\nWhen datasets have many covariates our algorithms may benefit from first choosing a subset of available covariates, a step called feature selection. Then we pass only those variables to the modeling algorithm, and it may be less likely to overfit to variables that are not related to the outcome.\n\nLet’s revisit listWrappers() and check out the bottom section.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlistWrappers()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nAll prediction algorithm wrappers in SuperLearner:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"SL.bartMachine\"      \"SL.bayesglm\"         \"SL.biglasso\"        \n [4] \"SL.caret\"            \"SL.caret.rpart\"      \"SL.cforest\"         \n [7] \"SL.earth\"            \"SL.gam\"              \"SL.gbm\"             \n[10] \"SL.glm\"              \"SL.glm.interaction\"  \"SL.glmnet\"          \n[13] \"SL.ipredbagg\"        \"SL.kernelKnn\"        \"SL.knn\"             \n[16] \"SL.ksvm\"             \"SL.lda\"              \"SL.leekasso\"        \n[19] \"SL.lm\"               \"SL.loess\"            \"SL.logreg\"          \n[22] \"SL.mean\"             \"SL.nnet\"             \"SL.nnls\"            \n[25] \"SL.polymars\"         \"SL.qda\"              \"SL.randomForest\"    \n[28] \"SL.ranger\"           \"SL.ridge\"            \"SL.rpart\"           \n[31] \"SL.rpartPrune\"       \"SL.speedglm\"         \"SL.speedlm\"         \n[34] \"SL.step\"             \"SL.step.forward\"     \"SL.step.interaction\"\n[37] \"SL.stepAIC\"          \"SL.svm\"              \"SL.template\"        \n[40] \"SL.xgboost\"         \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAll screening algorithm wrappers in SuperLearner:\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"All\"\n[1] \"screen.corP\"           \"screen.corRank\"        \"screen.glmnet\"        \n[4] \"screen.randomForest\"   \"screen.SIS\"            \"screen.template\"      \n[7] \"screen.ttest\"          \"write.screen.template\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Review code for corP, which is based on univariate correlation.\nscreen.corP\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfunction (Y, X, family, obsWeights, id, method = \"pearson\", minPvalue = 0.1, \n    minscreen = 2, ...) \n{\n    listp <- apply(X, 2, function(x, Y, method) {\n        ifelse(var(x) <= 0, 1, cor.test(x, y = Y, method = method)$p.value)\n    }, Y = Y, method = method)\n    whichVariable <- (listp <= minPvalue)\n    if (sum(whichVariable) < minscreen) {\n        warning(\"number of variables with p value less than minPvalue is less than minscreen\")\n        whichVariable[rank(listp) <= minscreen] <- TRUE\n    }\n    return(whichVariable)\n}\n<bytecode: 0x60a974ffbef0>\n<environment: namespace:SuperLearner>\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(1)\n\n# Fit the SuperLearner.\n# We need to use list() instead of c().\ncv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),\n                        # For a real analysis we would use V = 10.\n                        V = 3,\n                        parallel = \"multicore\",\n                        SL.library = list(\"SL.mean\", \"SL.glmnet\", c(\"SL.glmnet\", \"screen.corP\")))\nsummary(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nCV.SuperLearner(Y = y_train, X = x_train, V = 3, family = binomial(), SL.library = list(\"SL.mean\",  \n    \"SL.glmnet\", c(\"SL.glmnet\", \"screen.corP\")), parallel = \"multicore\") \n\nRisk is based on: Mean Squared Error\n\nAll risk estimates are based on V =  3 \n\n             Algorithm     Ave        se      Min     Max\n         Super Learner 0.10463 0.0168593 0.065549 0.12602\n           Discrete SL 0.10492 0.0170909 0.066405 0.12602\n           SL.mean_All 0.24260 0.0095617 0.226900 0.26050\n         SL.glmnet_All 0.10492 0.0170909 0.066405 0.12602\n SL.glmnet_screen.corP 0.10392 0.0163746 0.066405 0.12303\n```\n\n\n:::\n:::\n\n\n\nWe see a small performance boost by first screening by univarate correlation with our outcome, and only keeping variables with a p-value less than 0.10. Try using some of the other screening algorithms as they may do even better for a particular dataset.\n\n## Optimize for AUC\n\nFor binary prediction we are typically trying to maximize AUC, which can be the best performance metric when our outcome variable has some imbalance. In other words, we don’t have exactly 50% 1s and 50% 0s in our outcome. Our SuperLearner is not targeting AUC by default, but it can if we tell it to by specifying our method.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\ncv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),\n                        # For a real analysis we would use V = 10.\n                        V = 3,\n                        method = \"method.AUC\",\n                        SL.library = list(\"SL.mean\", \"SL.glmnet\", c(\"SL.glmnet\", \"screen.corP\")))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: cvAUC\n```\n\n\n:::\n\n```{.r .cell-code}\n## Loading required package: cvAUC\nsummary(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nCV.SuperLearner(Y = y_train, X = x_train, V = 3, family = binomial(), SL.library = list(\"SL.mean\",  \n    \"SL.glmnet\", c(\"SL.glmnet\", \"screen.corP\")), method = \"method.AUC\") \n\nRisk is based on: Area under ROC curve (AUC)\n\nAll risk estimates are based on V =  3 \n\n             Algorithm     Ave se     Min     Max\n         Super Learner 0.92045 NA 0.89372 0.95429\n           Discrete SL 0.91989 NA 0.89372 0.95429\n           SL.mean_All 0.50000 NA 0.50000 0.50000\n         SL.glmnet_All 0.92322 NA 0.89372 0.95429\n SL.glmnet_screen.corP 0.91989 NA 0.89372 0.95429\n```\n\n\n:::\n:::\n\n\n\nThis conveniently shows us the AUC for each algorithm without us having to calculate it manually. But we aren’t getting SEs sadly.\n\nAnother important optimizer to consider is negative log likelihood, which is intended for binary outcomes and will often work better than NNLS (the default). This is specified by method = “NNloglik”.\n\n## XGBoost hyperparameter exploration\n\nXGBoost is a version of GBM that is even faster and has some extra settings. GBM’s adaptivity is determined by its configuration, so we want to thoroughly test a wide range of configurations for any given problem. Let’s do 27 now. This will take a good amount of time (~7 minutes on my computer) so we need to at least use multiple cores, if not multiple computers.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3 * 3 * 3 = 27 different configurations.\n# For a real analysis we would do 100, 500, or 1000 trees - this is just a demo.\ntune = list(ntrees = c(10, 20, 50),\n            max_depth = 1:3,\n            shrinkage = c(0.001, 0.01, 0.1))\n\n# Set detailed names = T so we can see the configuration for each function.\n# Also shorten the name prefix.\nlearners = create.Learner(\"SL.xgboost\", tune = tune, detailed_names = TRUE, name_prefix = \"xgb\")\n\n# 27 configurations - not too shabby.\nlength(learners$names)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 27\n```\n\n\n:::\n\n```{.r .cell-code}\nlearners$names\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"xgb_10_1_0.001\" \"xgb_20_1_0.001\" \"xgb_50_1_0.001\" \"xgb_10_2_0.001\"\n [5] \"xgb_20_2_0.001\" \"xgb_50_2_0.001\" \"xgb_10_3_0.001\" \"xgb_20_3_0.001\"\n [9] \"xgb_50_3_0.001\" \"xgb_10_1_0.01\"  \"xgb_20_1_0.01\"  \"xgb_50_1_0.01\" \n[13] \"xgb_10_2_0.01\"  \"xgb_20_2_0.01\"  \"xgb_50_2_0.01\"  \"xgb_10_3_0.01\" \n[17] \"xgb_20_3_0.01\"  \"xgb_50_3_0.01\"  \"xgb_10_1_0.1\"   \"xgb_20_1_0.1\"  \n[21] \"xgb_50_1_0.1\"   \"xgb_10_2_0.1\"   \"xgb_20_2_0.1\"   \"xgb_50_2_0.1\"  \n[25] \"xgb_10_3_0.1\"   \"xgb_20_3_0.1\"   \"xgb_50_3_0.1\"  \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confirm we have multiple cores configured. This should be > 1.\ngetOption(\"mc.cores\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n\n```{.r .cell-code}\n# Remember to set multicore-compatible seed.\nset.seed(1, \"L'Ecuyer-CMRG\")\n\n# Fit the CV.SuperLearner.\nsystem.time({\n  cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),\n                          # For a real analysis we would use V = 10.\n                          V = 3,\n                          parallel = \"multicore\",\n                          SL.library = c(\"SL.mean\", \"SL.glmnet\", learners$names, \"SL.ranger\"))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n333.391   0.667  48.655 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Review results.\nsummary(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  \nCV.SuperLearner(Y = y_train, X = x_train, V = 3, family = binomial(), SL.library = c(\"SL.mean\",  \n    \"SL.glmnet\", learners$names, \"SL.ranger\"), parallel = \"multicore\") \n\nRisk is based on: Mean Squared Error\n\nAll risk estimates are based on V =  3 \n\n          Algorithm      Ave         se      Min     Max\n      Super Learner 0.089735 0.01523004 0.052813 0.11879\n        Discrete SL 0.095907 0.01699764 0.064775 0.12535\n        SL.mean_All 0.242600 0.00956173 0.226900 0.26050\n      SL.glmnet_All 0.104150 0.01673952 0.064775 0.12535\n xgb_10_1_0.001_All 0.247864 0.00022090 0.247406 0.24828\n xgb_20_1_0.001_All 0.245773 0.00043925 0.244861 0.24661\n xgb_50_1_0.001_All 0.239599 0.00103601 0.237065 0.24182\n xgb_10_2_0.001_All 0.247864 0.00022090 0.247406 0.24828\n xgb_20_2_0.001_All 0.245773 0.00043925 0.244861 0.24661\n xgb_50_2_0.001_All 0.239599 0.00103601 0.237065 0.24182\n xgb_10_3_0.001_All 0.247864 0.00022090 0.247406 0.24828\n xgb_20_3_0.001_All 0.245773 0.00043925 0.244861 0.24661\n xgb_50_3_0.001_All 0.239599 0.00103601 0.237065 0.24182\n  xgb_10_1_0.01_All 0.229984 0.00200462 0.224800 0.23455\n  xgb_20_1_0.01_All 0.213539 0.00380533 0.203477 0.22257\n  xgb_50_1_0.01_All 0.179899 0.00791583 0.154434 0.20141\n  xgb_10_2_0.01_All 0.229984 0.00200462 0.224800 0.23455\n  xgb_20_2_0.01_All 0.213539 0.00380533 0.203477 0.22257\n  xgb_50_2_0.01_All 0.179899 0.00791583 0.154434 0.20141\n  xgb_10_3_0.01_All 0.229984 0.00200462 0.224800 0.23455\n  xgb_20_3_0.01_All 0.213539 0.00380533 0.203477 0.22257\n  xgb_50_3_0.01_All 0.179899 0.00791583 0.154434 0.20141\n   xgb_10_1_0.1_All 0.149806 0.01102873 0.113512 0.18562\n   xgb_20_1_0.1_All 0.138650 0.01162345 0.093478 0.18195\n   xgb_50_1_0.1_All 0.139119 0.01173685 0.092681 0.18077\n   xgb_10_2_0.1_All 0.149806 0.01102873 0.113512 0.18562\n   xgb_20_2_0.1_All 0.138650 0.01162345 0.093478 0.18195\n   xgb_50_2_0.1_All 0.139119 0.01173685 0.092681 0.18077\n   xgb_10_3_0.1_All 0.149806 0.01102873 0.113512 0.18562\n   xgb_20_3_0.1_All 0.138650 0.01162345 0.093478 0.18195\n   xgb_50_3_0.1_All 0.139119 0.01173685 0.092681 0.18077\n      SL.ranger_All 0.079853 0.01187862 0.049206 0.09760\n```\n\n\n:::\n\n```{.r .cell-code}\nreview_weights(cv_sl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   mean(weight)        sd       min       max\nSL.ranger_All         0.5323384 0.4480376 0.1069165 1.0000000\nSL.glmnet_All         0.4676616 0.4480376 0.0000000 0.8930835\nSL.mean_All           0.0000000 0.0000000 0.0000000 0.0000000\nxgb_10_1_0.001_All    0.0000000 0.0000000 0.0000000 0.0000000\nxgb_20_1_0.001_All    0.0000000 0.0000000 0.0000000 0.0000000\nxgb_50_1_0.001_All    0.0000000 0.0000000 0.0000000 0.0000000\nxgb_10_2_0.001_All    0.0000000 0.0000000 0.0000000 0.0000000\nxgb_20_2_0.001_All    0.0000000 0.0000000 0.0000000 0.0000000\nxgb_50_2_0.001_All    0.0000000 0.0000000 0.0000000 0.0000000\nxgb_10_3_0.001_All    0.0000000 0.0000000 0.0000000 0.0000000\nxgb_20_3_0.001_All    0.0000000 0.0000000 0.0000000 0.0000000\nxgb_50_3_0.001_All    0.0000000 0.0000000 0.0000000 0.0000000\nxgb_10_1_0.01_All     0.0000000 0.0000000 0.0000000 0.0000000\nxgb_20_1_0.01_All     0.0000000 0.0000000 0.0000000 0.0000000\nxgb_50_1_0.01_All     0.0000000 0.0000000 0.0000000 0.0000000\nxgb_10_2_0.01_All     0.0000000 0.0000000 0.0000000 0.0000000\nxgb_20_2_0.01_All     0.0000000 0.0000000 0.0000000 0.0000000\nxgb_50_2_0.01_All     0.0000000 0.0000000 0.0000000 0.0000000\nxgb_10_3_0.01_All     0.0000000 0.0000000 0.0000000 0.0000000\nxgb_20_3_0.01_All     0.0000000 0.0000000 0.0000000 0.0000000\nxgb_50_3_0.01_All     0.0000000 0.0000000 0.0000000 0.0000000\nxgb_10_1_0.1_All      0.0000000 0.0000000 0.0000000 0.0000000\nxgb_20_1_0.1_All      0.0000000 0.0000000 0.0000000 0.0000000\nxgb_50_1_0.1_All      0.0000000 0.0000000 0.0000000 0.0000000\nxgb_10_2_0.1_All      0.0000000 0.0000000 0.0000000 0.0000000\nxgb_20_2_0.1_All      0.0000000 0.0000000 0.0000000 0.0000000\nxgb_50_2_0.1_All      0.0000000 0.0000000 0.0000000 0.0000000\nxgb_10_3_0.1_All      0.0000000 0.0000000 0.0000000 0.0000000\nxgb_20_3_0.1_All      0.0000000 0.0000000 0.0000000 0.0000000\nxgb_50_3_0.1_All      0.0000000 0.0000000 0.0000000 0.0000000\n```\n\n\n:::\n:::\n\n\n\nWe can see how stochastic the weights are for each individual execution of SuperLearner.\n\nFinally, plot the performance for the different settings.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(cv_sl) + theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n## Troubleshooting\nIf you get an error about predict for xgb.Booster, you probably need to install the latest version of XGBoost from github.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}