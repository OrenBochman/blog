{
  "hash": "5bc5aaea520bf294987e7c392befa685",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"gradio local model\"\ndraft: true\ndate: 2024-03-27\nexecute: \n  error: true\n---\n\n::: {#3cf799c9 .cell execution_count=2}\n``` {.python .cell-code}\nimport gradio as gr\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\nfrom threading import Thread\n\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\nmodel = model.to('cuda:0')\n\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        stop_ids = [29, 0]\n        for stop_id in stop_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False\n\ndef predict(message, history):\n    history_transformer_format = history + [[message, \"\"]]\n    stop = StopOnTokens()\n\n    messages = \"\".join([\"\".join([\"\\n<human>:\"+item[0], \"\\n<bot>:\"+item[1]])\n                for item in history_transformer_format])\n\n    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n    generate_kwargs = dict(\n        model_inputs,\n        streamer=streamer,\n        max_new_tokens=1024,\n        do_sample=True,\n        top_p=0.95,\n        top_k=1000,\n        temperature=1.0,\n        num_beams=1,\n        stopping_criteria=StoppingCriteriaList([stop])\n        )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    partial_message = \"\"\n    for new_token in streamer:\n        if new_token != '<':\n            partial_message += new_token\n            yield partial_message\n\ngr.ChatInterface(predict).launch()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[1], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">import</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,0,255)\">gradio</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\">as</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,0,255)\">gr</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">import</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,0,255)\">torch</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">from</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,0,255)\">transformers</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\">import</span> AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n<span class=\"ansi-green-fg ansi-bold\">      4</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">from</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,0,255)\">threading</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\">import</span> Thread\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named 'torch'</pre>\n```\n:::\n\n:::\n:::\n\n\n",
    "supporting": [
      "gradio_local_files"
    ],
    "filters": [],
    "includes": {}
  }
}