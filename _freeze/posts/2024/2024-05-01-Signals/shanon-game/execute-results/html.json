{
  "hash": "acbef6b0d5b86a29060e3ad4a8d26e90",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"emergent communications\"\n---\n\nit seems that we might want to look at the emergent communications \nby considering\n1. a lewis signaling games to model coordination tasks for a basic communication system\n2. a shannon game to model the communication of information between agents in which the \n     learn a shared communication protocol potentially using error detection and correction\n     and corection.\n3. a chomsky game to model development of a shared grammar for complex signals.\n\n## Shannon Game\n\n::: {#bce09a18 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\nclass CommunicationAgent:\n    def __init__(self, num_strategies):\n        self.num_strategies = num_strategies\n        self.q_table = np.zeros((num_strategies, num_strategies))\n        self.learning_rate = 0.1\n        self.discount_factor = 0.9\n        self.epsilon = 0.1\n    \n    def choose_strategy(self):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(self.num_strategies)\n        else:\n            return np.argmax(self.q_table.sum(axis=1))\n    \n    def update_q_values(self, sender_strategy, receiver_strategy, reward):\n        max_future_q = np.max(self.q_table[receiver_strategy])\n        current_q = self.q_table[sender_strategy, receiver_strategy]\n        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_future_q - current_q)\n        self.q_table[sender_strategy, receiver_strategy] = new_q\n\n# Simulation parameters\nnum_strategies = 5\nnum_iterations = 1000\n\n# Initialize agents\nalice = CommunicationAgent(num_strategies)\nbob = CommunicationAgent(num_strategies)\n\nfor _ in range(num_iterations):\n    sender_strategy = alice.choose_strategy()\n    receiver_strategy = bob.choose_strategy()\n    \n    # Simulate message transmission and reception with noise\n    # This is a placeholder for actual encoding/decoding logic\n    success = np.random.rand() < 0.8  # Assume 80% chance of success\n    \n    reward = 1 if success else -1\n    alice.update_q_values(sender_strategy, receiver_strategy, reward)\n    bob.update_q_values(receiver_strategy, sender_strategy, reward)\n\nprint(\"Alice's Q-Table:\\n\", alice.q_table)\nprint(\"Bob's Q-Table:\\n\", bob.q_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAlice's Q-Table:\n [[5.51822968 2.35177511 2.90702021 4.76366083 3.37332381]\n [4.89761615 0.1        0.         0.         0.        ]\n [4.7879317  0.18171545 0.2552969  0.         0.        ]\n [5.58902632 0.         0.1        0.55136978 0.        ]\n [4.4114806  0.         0.         0.         0.19186254]]\nBob's Q-Table:\n [[5.51822968 2.72004242 3.63249823 4.14833012 3.24329082]\n [3.77818662 0.1        0.30752146 0.         0.        ]\n [4.86271285 0.         0.3543934  0.15511159 0.        ]\n [5.33458643 0.         0.         0.48117073 0.        ]\n [5.48950986 0.         0.         0.         0.33812458]]\n```\n:::\n:::\n\n\nThis example illustrates a basic game-theoretic approach where the sender and receiver iteratively learn better strategies for encoding and decoding messages over a noisy channel. The reinforcement learning framework allows both parties to adapt and improve their protocols, enhancing the reliability of communication over time. This model can be extended and refined to include more sophisticated encoding/decoding techniques and more complex noise models.\n\n::: {#b2075aae .cell execution_count=2}\n``` {.python .cell-code}\nfrom mesa import Agent, Model\nfrom mesa.time import RandomActivation\nfrom mesa.datacollection import DataCollector\nimport numpy as np\n\ndef hamming_distance(a, b):\n    return np.sum(a != b) / len(a)\n\nclass Sender(Agent):\n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n        self.protocol = self.random_protocol()\n    \n    def random_protocol(self):\n        # Define a random protocol for encoding\n        return lambda msg: msg  # Identity for simplicity\n    \n    def step(self):\n        message = np.random.randint(0, 2, self.model.message_length)\n        encoded_message = self.protocol(message)\n        self.model.sent_message = encoded_message\n\nclass Receiver(Agent):\n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n        self.protocol = self.random_protocol()\n    \n    def random_protocol(self):\n        # Define a random protocol for decoding\n        return lambda msg: msg  # Identity for simplicity\n    \n    def step(self):\n        noisy_message = self.model.sent_message ^ np.random.binomial(1, self.model.error_rate, self.model.message_length)\n        recovered_message = self.protocol(noisy_message)\n        self.model.recovered_message = recovered_message\n        self.evaluate_performance()\n    \n    def evaluate_performance(self):\n        original_message = self.model.original_message\n        recovered_message = self.model.recovered_message\n        distance = hamming_distance(original_message, recovered_message)\n        self.model.payoff += self.model.recovery_payoff(distance)\n        self.model.payoff += self.model.length_payoff(len(recovered_message))\n        self.model.payoff += self.model.early_recovery_payoff(self.model.current_step)\n    \nclass NoisyChannelModel(Model):\n    def __init__(self, message_length=10, error_rate=0.1, max_steps=100):\n        super().__init__()\n        self.message_length = message_length\n        self.error_rate = error_rate\n        self.current_step = 0\n        self.max_steps = max_steps\n        self.payoff = 0\n        \n        self.schedule = RandomActivation(self)\n        \n        sender = Sender(1, self)\n        receiver = Receiver(2, self)\n        self.schedule.add(sender)\n        self.schedule.add(receiver)\n        \n        self.original_message = np.random.randint(0, 2, self.message_length)\n        self.sent_message = None\n        self.recovered_message = None\n        \n        self.datacollector = DataCollector(\n            model_reporters={\"Payoff\": \"payoff\"}\n        )\n    \n    def recovery_payoff(self, distance):\n        return 1 - distance\n    \n    def length_payoff(self, length):\n        return 1 / length\n    \n    def early_recovery_payoff(self, step):\n        return (self.max_steps - step) / self.max_steps\n    \n    def step(self):\n        self.current_step += 1\n        self.schedule.step()\n        self.datacollector.collect(self)\n        if self.current_step >= self.max_steps:\n            self.running = False\n\n# Example of running the model\nmodel = NoisyChannelModel()\nwhile model.running:\n    model.step()\n\n# Retrieve results\nresults = model.datacollector.get_model_vars_dataframe()\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Payoff\n0     1.69\n1     3.47\n2     4.94\n3     6.50\n4     7.75\n..     ...\n95  105.34\n96  105.77\n97  106.29\n98  106.80\n99  107.10\n\n[100 rows x 1 columns]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/oren/work/blog/env/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:\n\nThe AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.\nWe would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919\n\n```\n:::\n:::\n\n\n",
    "supporting": [
      "shanon-game_files"
    ],
    "filters": [],
    "includes": {}
  }
}