{
  "hash": "1375204b8358e902aea5b0f0a0b192fc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-05-01\ntitle: \"Skryms Signals Summary and Models\"\nsubtitle: \"learing language games\"\nkeywords: [game theory, signaling games, partial pooling, evolution, reinforcement learning, signaling systems, evolution of language]\n#draft: True\n---\n\nIn [@Skyrms2010signals] philosopher and mathematician Brian Skyrms discusses how one can extend the concept of a signaling games into a full fledged signaling systems and to some extent a rudimentary language.\n\nI like many other found Signals to be a fascinating little book worth reading at least a couple of times. While Skyrms starts with a basic exposition motivated by Greek philosophers he eventually makes a deep dive into areas like reinforcement learning, replicator dynamics, mean field games and some other deep mathematical fields without much of introduction. In places the monographs seems incomplete and may require hunting the papers in the bibliography and possibly more recent work by the same authors.\n\nI slowly noticed it being cited in more and more papers which I read. This sort of indicated that intellectually more people we on the same path of thinking how to equip their problem solving with a signaling system or better yet to evolve a more sophisticated language.\\\n\nI went back several times to review the chapter on Complex signals, which I feel is the most interesting for real-world application. I began to think that the Lewis games are too rudimentary since signaling systems that evolve/learned from them are basically n-k maps of signals to meaning.\n\nWhat I wanted was a recipe for quickly agent that need to evolve and teach/learn a language for efficient communication.\n\nI wanted to go the relevant papers he covers on this area and then to see of there were newer results he did not cover. This turned out to be a bit of a challenge. In the mean time I also learned some courses on RL and even tried a couple of ideas from this book at work. I think I should summarize at least some of the more interesting results from the book.\n\nBesides a summary I also want to try to implement some of the keystone models in the book to see if I can derive the reductionist simple language learning game.\n\n## 1. Signals\n\n### Big Research Questions\n\n**Q1. How can interacting individuals spontaneously learn to signal?**\n\n**Q2. How can species spontaneously evolve signaling systems?**\n\n## Sender-Receiver\n\n> There are two players, the sender and the receiver.\\\n> Nature chooses a state at random and the sender observes the state chosen.\\\n> The sender then sends a signal to the receiver, who cannot observe the state directly but does observe the signal.\\\n> The receiver then chooses an act, the outcome of which affects them both, with the payoff depending on the state.\\\n> Both have pure common interest—they get the same payoff—and there is exactly one “correct” act for each state.\\\n> In the correct act-state combination they both get positive payoff; otherwise payoff is zero.\\\n> The simplest case is one where there are the same number of states, acts, and signals.\n\nA separating equilibrium is called a signaling system\n\n> If we start with a pair of sender and receiver strategies, and switch the messages around the same way in both, we get the same payoffs. In particular, permutation of messages takes one signaling-system equilibrium into another.\n\nWe can understand a signaling system as a encoding look-up table by the sender and a decoding lookup table for the reciever which is the inverse of the first. The product of two permutations is the identity matrix. Each permutation of the identity matrix gives a valid signaling system\n\n**Q3. Is there a most salient signaling system?**\n\nSalience is a concept from Schelling's Game theory that suggest that one solution to a coordination problem might be naturally better then others. (e.g. meeting a relative at the airport). This can be due to an externality to the pure coordination problem. Salience can also arise from non uniformity of the state distribution - by providing less frequent messages longer messages based on binary coding. The salience hierarchy might be grounded in risk - more urgent messages might be shorter and learned before the longer ones.\n\nmy thoughts on Salience:\n\n- Salience would arise in nature through the non-uniform distribution of states \n  which is ignored in most papers leading to equally salient signaling system. \n  When the states are not uniformly distributed then the signals will not be \n  uniformly distributed. The more common states should have more common signals. \n  e.g. if snakes are more common than eagles then the signal for snake should be \n  shorter/simpler/learned first than the signal for eagle. In another location\n  the distributions could be reversed leading to a different salience hierarchy.\n- Another way (of seeing this is that) salience would arise in nature to minimize\n  risks for the sender, who could become a target for a predator by sending a signal.\n- Two other source of salience are the risk of making mistakes and the cost of \n  sending a signal.\n- Finally there is nothing stopping the salience from being a function of all these\n  factors through a product of their probabilities. Though this is more easily\n  expressed in the language of fitness. Salience will select the language whose\n  speakers gain the highest expected progeny (fitness) by avoiding risks, conserving\n  energy and avoiding miscommunication for their habitat.\n- If the speakers migrate they might benefit from a language that is salient in \n  multiple habitats. This is a form of generalization.\n- If there are different cost for encoding and decoding then the salience will be \n  a function of the product of the encoding and decoding costs. This is a form of \n  cost minimization. In this scenario there may be a competition between the sender and\n  the receiver to minimize their costs. But the sender has the upper hand since the\n  sender chooses the signal. The sender is the causal agent in the signaling system.\n  \n\n**Q4. How can two agents with different signaling find a SS that is midway between them (including systems with both shared and unique states)?**\n\n-   Its fairly clear that under the rules of the Lewis game all valid signaling systems are isomorphic and none are more salient.\n-   In nature salience might arise and a systems leading to greatest fitness in its users would be the most salient.\n-   To find a signaling system that is midway between two signaling systems we could use the Cayley distance between the two permutations. This is the minimum number of transpositions required to transform one permutation into another. The median permutation would be the one that has half the Cayley distance to each signaling systems.\n- If the systems have salience we may want to also keep the most salient signals intact and now we have a more complex optimization problem. We could use the KL divergence between the two signaling systems to estimate the distance of the signaling distribution from a separating distribution.\n\nthe Cayley distance between two permutations is the minimum number of transpositions required to transform one permutation into another. it is a metric on the symmetric group.\n\n**Information in signals**\n\n**Q5. How can we minimally extend this framework to handle Errors and Deception**\n\n> Signals carry information. The natural way to measure the information in a signal is to measure the extent that the use of that particular signal changes probabilities. Accordingly, there are two kinds of information in the signals in Lewis sender-receiver games: information about what state the sender has observed and information about what act the receiver will take. The ﬁrst kind of infor- mation measures effectiveness of the sender’s use of signals to discriminate states; the second kind measures the effectiveness of the signal in changing the receiver’s probabilities of action.\n\n- [ ] TODO: estimate information content of each signal for sender and receiver for separating and partial pooling cases\n- [ ] TODO: use entropy for message level estimates of sender and receiver under separating signal, a synonym, a homonym.\n- [ ] TODO: use entropy KL divergence to estimate a the distance of the signaling distribution from a separating distribution.\n\nActually there are a number of extensions one would like to consider for the Lewis framework:\n\n1.  bottlenecks\n    1.  more state than signals - this is the interesting case and where complex signaling systems should arise\n    2.  more signals than states - this is the case where synonyms can arrise\n2.  basic logical reasoning, conjunctions, disjunctions, negations\n3.  multiple senders and or receivers\n    1.  rewarding coordination (each state requires different actions from the agents - they are learning different receiver maps )\n    2.  rewarding correlated equilibrium (sender lets the receivers pick from correlated states at random allowing the receivers avoid penalty of miscoordination.)\n    3. networks of agents per the goyal model in ch 11 and 13\n\ncomplex signals\n\n1.  conjunction of signals,\n2.  ordered signals,\n3.  recursive signals, group\n\n### Evolution\n\nIn Banes 1982\n\nWe first see two competing Signaling systems being tested in a population\n\n[@hofbauer1998evolutionary] Population dynamics - can be used to identify which dynamic equlibria are stable or unstable given an intial population of strategies\n\nThere is a figure showing the field dynamics with basins of attractions arrising from the population dynamics equations\n\nWe also see symmetry breaking selecting a signaling system to a system\n\n$$\n\\frac{dp(A)}{dt}=p(A)[U(A)-U]\n$$\n\nwhere\n\n-   U(A) is the average payoff to strategy A and\n-   U is the average payoff in the population.\n\n::: {#bd50bfb1 .cell execution_count=1}\n``` {.python .cell-code}\nfrom pylab import *\n\nalpha, beta = 1, 1\nxvalues, yvalues = meshgrid(arange(0, 2.1, 0.1), arange(0, 2.1, 0.1))\nxdot = xvalues * alpha - beta\nydot = yvalues * alpha - beta\nstreamplot(xvalues, yvalues, xdot, ydot)\nshow()\n```\n\n::: {.cell-output .cell-output-display}\n![](signals-summary_files/figure-html/cell-2-output-1.png){width=594 height=416}\n:::\n:::\n\n\nwe have a discussion of how signals might arise.\n\n## Evolution\n\n::: {#8d763ac8 .cell execution_count=2}\n``` {.python .cell-code}\nimport itertools\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation, RandomActivation\n#import random\n\n# agent_roles\nr_nature = 'nature'\nr_sender = 'sender'\nr_receiver = 'receiver'\n\nclass multiurn:\n\n  def __init__(self,options,balls=None):\n      self.options = options\n      if balls is not None:\n        self.balls = balls\n      else:\n        self.balls = {option: 1.0 for option in self.options}\n  \n  def get_filtered_urn(self, filter):\n    ''' filters urn's options by prefix and normalizes the weights\n        usege:\n        urn=urn.get_filtered_urn(1)\n        choice = model.random.choice(list(urn.keys()), p=list(urn.values()))\n    '''\n    assert type(filter ) == int, f\"filter must be a int\"\n    filtered_options = [k for k in self.options.keys() if k[0]==filter]\n    filtered_balls = {opt: self.weights[opt] for opt in filtered_options}\n    total_balls = sum(filtered_balls.values())\n    assert total_balls > 0.0, f\"total weights is {total=} after {filter=} on {self.balls}\"\n    filtered_probs = {opt: self.weights[opt]/total_balls for opt in filtered_options}\n    return filtered_probs\n\n  def get_filtered_urn(self, filter):\n    ''' filters urn's options by prefix and normalizes the weights\n        usege:\n        urn=urn.get_filtered_urn('A')\n        choice = model.random.choice(list(urn.keys()), p=list(urn.values()))\n    '''\n\nclass HerrnsteinRL(multiurn):\n    '''\n                                    The Urn model\n     nature            sender                 reciever     reward\n                       \n    | (0) | --{0}-->  | (0_a)  | --{a}--> | (a_0) | --{0}-->   1   \n    |     |           | (0_b)  | --{b}    | (a_1) | --{1}-->   0\n    |     |           +--------+    | +-->+-------+\n    |     |                         +-|-+  \n    | (1) | --{1}-->  | (1_a)  | --{a}+ +>| (b_0) | --{1}-->   1\n    |     |           | (1_b)  | --{b}--->| (b_1) | --{0}-->   0\n    +-----+           +--------+          +-------+\n    \n    \n    Herrnstein urn algorithm\n    ------------------------\n    \n    1. nature picks a state \n    2. sender  gets the state, chooses a signal by picking a ball in choose_option() from the stat'es urn\n    3. reciver gets the action, chooses an actuion by picking a ball in choose_option()\n    4. the balls in the urns are incremented if action == state\n    5. repeat\n    \n    '''\n    def __init__(self, options, learning_rate=1.0,verbose=False,name='Herrnstein matching law'):\n        super().__init__(options)\n        # filter options in choose option by input\n        self.verbose = verbose\n        self.name=name\n        self.learning_rate = learning_rate\n        self.options = options\n\n        if self.verbose:\n          print(f'LearningRule.__init__(Options: {options})')\n      \n    def choose_option(self,filter,random):\n        ''' choose an option from the urn based on the filter and the random choice\n            usage:\n            urn.choose_option(1,random=model.random)\n        '''\n        if self.verbose:\n          print(f'choose_option({filter=},{random=})')\n        urn = self.get_filtered_urn(filter)\n        if random:\n          return random.choices(list(self.balls.keys()), weights=list(self.balls.values()),k=1)\n        else:\n          throw(f\"random must be a random number generator\")\n        \n    def update_weights(self, option, reward):\n        old_balls = self.balls[option]\n        self.balls[option] += self.learning_rate * reward \n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_balls} -> {self.balls[option]}\")\n\n\nclass LewisAgent(Agent):\n  \n    def __init__(self, unique_id, model, game, role, verbose=False):\n        super().__init__(unique_id, model)\n        self.role = role #( one of nature, sender, receiver)\n        self.verbose = verbose\n        self.game = game\n        self.messages = []\n        self.actions = []\n        if role == \"sender\":\n          self.urn = HerrnsteinRL(model.states_signals, learning_rate=1.0,verbose=verbose,name='state_signal_weights')\n        elif role == \"receiver\":\n          self.urn = HerrnsteinRL(model.signals_actions, learning_rate=1.0,verbose=verbose,name='signal_action_weights')\n        else:\n          self.urn = None\n        \n    def step(self):\n      # reset agent state before step\n      self.messages = []\n      self.actions = []\n\n    def gen_state(self)-> None:\n        if self.role == \"nature\":\n          self.current_state = model.random.choice(self.model.states)\n          if self.verbose:\n                print(f\"Nature {self.unique_id} set state {self.current_state}\")\n                \n    @property\n    def state(self):\n        if self.role == r_nature:\n          return self.current_state\n\n    def choose_signal(self, filter):\n        if self.role != r_sender:\n          throw(f\"Only sender can send signals\")\n        self.signal = self.urn.choose_option(filter=filter,random=self.model.random)\n        if self.verbose:\n              print(f\"Sender {self.unique_id} choose_signal: {self.signal}\")\n        return self.signal\n          \n\n    def send_signal(self, filter, receiver):\n        if self.role == r_sender:\n          assert type(filter) == int, f\"filter must be a int\"\n          assert filter in model.states, f\"filter must be a valid state\"\n          signal = self.choose_signal(filter=filter)\n          assert signal is not None, f\"signal must be a valid signal\"\n          print(f\"Sender {self.unique_id} sends signal: {signal}\")\n          receiver.messages.append(signal)\n          if self.verbose:\n            print(f\"Sender {self.unique_id} sends signal: {signal}\")\n        else:\n          throw(f\"Only sender can send signals\")\n\n    def fuse_actions(self,actions):\n        ''' \n            # Message fusion logic:\n            1. single message:  if there is only one signal then the action is the action associated with the signal\n            2. ordered messages: if there are multiple signals then the action is the number from the string assocciated with the concatenated signal\n               if there are two signals possible per message we concat and covert binary string to number\n            3. is the messages are sets we could perform a intersetion and take the action associated with the intersection \n               currently this is not implemented\n            4. support for recursive signals is currently under research .\n        ''' \n        if self.role != r_receiver:\n          raise Exception(f\"Only receiver can set actions\")\n        \n        if len(actions) == 1: # single action no need to fuse\n          return actions[0]\n        else:\n          # fuse the actions into a binary number\n          action = 0\n          # if there are multiple signals\n          for i in range(len(actions)):\n            action += actions[i]*(2**i)\n          if self.verbose:\n              print(f\"Receiver {self.unique_id} fused actions : {self.actions} into action: {action}\")\n          return action\n\n    def decode_message(self,signal):\n        ''' first we need to get the filtered urn for the signal\n            and then choose the option based on the urn'''\n        if self.role != r_receiver:\n          raise Exception(f\"Only receiver can decode messages\")\n\n        message = self.urn.choose_option(filter=signal,random=self.model.random)\n        if self.verbose:\n              print(f\"Receiver {self.unique_id} received signal: {message}\")\n        return message\n\n    def set_action(self):\n        ''' first we need to use the urn to decode the signals \n            then need to fuse them to get the action '''\n        if self.role != r_receiver:\n          raise Exception(f\"Only receiver can set the action\")\n        self.actions = []\n        for signal in self.messages:\n          self.actions.append(self.decode_message(signal))\n        self.action = self.fuse_actions(self.actions)\n        if self.verbose:\n              print(f\"Receiver {self.unique_id} received signals: {self.messages} and action: {self.action}\")\n              \n    def set_reward(self,reward):\n        if self.role not in [r_receiver,r_sender]:\n          raise Exception(f\"Only sender and receiver can set rewards\")\n        self.reward = reward\n        if self.verbose:\n            print(f\"Receiver {self.unique_id} received reward: {self.reward}\")\n                \n    def calc_reward(self,correct_action):\n        if self.role != r_receiver:\n          raise Exception(f\"Only receiver can calculate rewards\")\n        self.reward = 1 if self.action == correct_action else 0\n        \n\nclass SignalingGame(Model):\n  \n    # TODO: add support for \n    # 1. bottle necks\n    # 2. rename k to state_count\n    # 3. state_per_sender = state_count/sender_count \n    # 2. partitioning states by signals => state/sender_count\n    def __init__(self, game_count=2, senders_count=1, recievers_count=1, state_count=3,signal_count=3,verbose=True):\n        super().__init__()\n        self.verbose = verbose\n        self.schedule = RandomActivation(self)\n        \n        \n        # Define the states, signals, and actions\n        self.states   = [i for i in range(state_count)]\n        print(f'{self.states=}')\n        self.signals  = [i for i in range(signal_count)]\n        print(f'{self.signals=}')\n        self.actions  = [i for i in range(state_count)]\n        print(f'{self.actions=}')\n        \n        # e.g., 1 -> 1, 2 -> 2, ...\n        self.states_signals =  [(state,signal) for state in self.states for signal in self.signals]\n        print(f'{self.states_signals=}')\n        self.signals_actions = [(signal,action) for signal in self.signals for action in self.actions] \n        print(f'{self.signals_actions=}')\n        \n        # Agents\n\n        self.uid=0\n        self.senders_count=senders_count\n        self.recievers_count=recievers_count\n\n        # Games each game has a nature, senders and receivers\n        self.games = []\n        # Create games        \n        for i in range(game_count):\n            game = {'senders': [], 'receivers': [], 'nature': None}\n            \n            # create nature agent\n            game['nature'] = LewisAgent(self.uid, self, game=i,role = r_nature,verbose=self.verbose)\n            self.schedule.add(game['nature'])\n            self.uid += 1\n            \n            # create sender agents\n            for j in range(senders_count):\n                sender = LewisAgent(self.uid, self, game=i,role = r_sender,verbose=self.verbose)\n                game['senders'].append(sender)\n                self.schedule.add(sender)\n                self.uid +=1\n                \n            # create receiver agents\n            for k in range (recievers_count):\n                reciever = LewisAgent(self.uid, self, game=i,role = r_receiver,verbose=self.verbose)\n                game['receivers'].append(reciever)\n                self.schedule.add(reciever)\n                self.uid +=1\n                \n            self.games.append(game)\n        \n    def step(self):\n      \n        for agent in model.schedule.agents:\n            # reset agent state before step\n            agent.step()\n        \n        \n        for game_counter,game in enumerate(self.games):\n            if self.verbose:\n                print(f\"--- Step {model.step_counter} Game {game_counter} ---\")\n                \n            nature = game['nature']\n            nature.gen_state()\n            state = nature.current_state\n            assert type(state) == int, f\"state must be a string\"\n            assert state in model.states, f\"state must be a valid state\"\n            if self.verbose:\n                print(f\"Nature {agent.unique_id} set state {state}\")\n            for sender in game['senders']:\n                for receiver in game['receivers']:\n                    sender.send_signal(filter=state, receiver=receiver)\n                    if self.verbose:\n                      print(f\"Sender {sender.unique_id} sends signal: {sender.signal}\")\n            for receiver in game['receivers']:\n                receiver.set_action()\n                if self.verbose:\n                    print(f\"Receiver {receiver.unique_id} action: {receiver.action}\")\n                reward=receiver.calc_reward(state)\n                if self.verbose:\n                    print(f\"Receiver {receiver.unique_id} received reward: {receiver.reward}\")\n            \n            for agent in itertools.chain(game['senders'],game['receivers']):\n                agent.set_reward(reward)\n                if self.verbose:\n                    print(f\"Sender {agent.unique_id} received reward: {reward}\")  \n                    \n\n        for i,game in enumerate(self.games):\n            print(f'game {i}, {self.expected_rewards(game)=}')\n\n    def expected_rewards(self,game):\n      return 0.25\n\n# Running the model\nstate_count= 2  # Number of states, signals, and actions\nsteps = 10\nmodel = SignalingGame(senders_count=1,recievers_count=1,state_count=state_count)\nmodel.step_counter = 0\nfor i in range(steps):\n    model.step_counter +=1\n    model.step()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nself.states=[0, 1]\nself.signals=[0, 1, 2]\nself.actions=[0, 1]\nself.states_signals=[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\nself.signals_actions=[(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)]\nLearningRule.__init__(Options: [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)])\nLearningRule.__init__(Options: [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)])\nLearningRule.__init__(Options: [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)])\nLearningRule.__init__(Options: [(0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)])\n--- Step 1 Game 0 ---\nNature 0 set state 0\nNature 5 set state 0\nchoose_option(filter=0,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(0, 2)]\nSender 1 sends signal: [(0, 2)]\nSender 1 sends signal: [(0, 2)]\nSender 1 sends signal: [(0, 2)]\nchoose_option(filter=[(0, 2)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(1, 1)]\nReceiver 2 received signals: [[(0, 2)]] and action: [(1, 1)]\nReceiver 2 action: [(1, 1)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 1 Game 1 ---\nNature 3 set state 1\nNature 2 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(1, 0)]\nSender 4 sends signal: [(1, 0)]\nSender 4 sends signal: [(1, 0)]\nSender 4 sends signal: [(1, 0)]\nchoose_option(filter=[(1, 0)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(2, 1)]\nReceiver 5 received signals: [[(1, 0)]] and action: [(2, 1)]\nReceiver 5 action: [(2, 1)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n--- Step 2 Game 0 ---\nNature 0 set state 1\nNature 5 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nchoose_option(filter=[(1, 0)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(0, 0)]\nReceiver 2 received signals: [[(1, 0)]] and action: [(0, 0)]\nReceiver 2 action: [(0, 0)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 2 Game 1 ---\nNature 3 set state 0\nNature 2 set state 0\nchoose_option(filter=0,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(1, 1)]\nSender 4 sends signal: [(1, 1)]\nSender 4 sends signal: [(1, 1)]\nSender 4 sends signal: [(1, 1)]\nchoose_option(filter=[(1, 1)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(0, 1)]\nReceiver 5 received signals: [[(1, 1)]] and action: [(0, 1)]\nReceiver 5 action: [(0, 1)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n--- Step 3 Game 0 ---\nNature 0 set state 0\nNature 5 set state 0\nchoose_option(filter=0,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nchoose_option(filter=[(1, 0)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(1, 1)]\nReceiver 2 received signals: [[(1, 0)]] and action: [(1, 1)]\nReceiver 2 action: [(1, 1)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 3 Game 1 ---\nNature 3 set state 1\nNature 2 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(1, 2)]\nSender 4 sends signal: [(1, 2)]\nSender 4 sends signal: [(1, 2)]\nSender 4 sends signal: [(1, 2)]\nchoose_option(filter=[(1, 2)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(1, 1)]\nReceiver 5 received signals: [[(1, 2)]] and action: [(1, 1)]\nReceiver 5 action: [(1, 1)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n--- Step 4 Game 0 ---\nNature 0 set state 1\nNature 5 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(1, 1)]\nSender 1 sends signal: [(1, 1)]\nSender 1 sends signal: [(1, 1)]\nSender 1 sends signal: [(1, 1)]\nchoose_option(filter=[(1, 1)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(0, 1)]\nReceiver 2 received signals: [[(1, 1)]] and action: [(0, 1)]\nReceiver 2 action: [(0, 1)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 4 Game 1 ---\nNature 3 set state 1\nNature 2 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(0, 0)]\nSender 4 sends signal: [(0, 0)]\nSender 4 sends signal: [(0, 0)]\nSender 4 sends signal: [(0, 0)]\nchoose_option(filter=[(0, 0)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(0, 1)]\nReceiver 5 received signals: [[(0, 0)]] and action: [(0, 1)]\nReceiver 5 action: [(0, 1)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n--- Step 5 Game 0 ---\nNature 0 set state 1\nNature 5 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nchoose_option(filter=[(1, 0)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(2, 1)]\nReceiver 2 received signals: [[(1, 0)]] and action: [(2, 1)]\nReceiver 2 action: [(2, 1)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 5 Game 1 ---\nNature 3 set state 1\nNature 2 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(0, 2)]\nSender 4 sends signal: [(0, 2)]\nSender 4 sends signal: [(0, 2)]\nSender 4 sends signal: [(0, 2)]\nchoose_option(filter=[(0, 2)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(0, 0)]\nReceiver 5 received signals: [[(0, 2)]] and action: [(0, 0)]\nReceiver 5 action: [(0, 0)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n--- Step 6 Game 0 ---\nNature 0 set state 1\nNature 5 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nSender 1 sends signal: [(1, 0)]\nchoose_option(filter=[(1, 0)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(0, 1)]\nReceiver 2 received signals: [[(1, 0)]] and action: [(0, 1)]\nReceiver 2 action: [(0, 1)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 6 Game 1 ---\nNature 3 set state 0\nNature 2 set state 0\nchoose_option(filter=0,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nchoose_option(filter=[(0, 1)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(0, 1)]\nReceiver 5 received signals: [[(0, 1)]] and action: [(0, 1)]\nReceiver 5 action: [(0, 1)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n--- Step 7 Game 0 ---\nNature 0 set state 0\nNature 5 set state 0\nchoose_option(filter=0,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(0, 1)]\nSender 1 sends signal: [(0, 1)]\nSender 1 sends signal: [(0, 1)]\nSender 1 sends signal: [(0, 1)]\nchoose_option(filter=[(0, 1)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(1, 1)]\nReceiver 2 received signals: [[(0, 1)]] and action: [(1, 1)]\nReceiver 2 action: [(1, 1)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 7 Game 1 ---\nNature 3 set state 0\nNature 2 set state 0\nchoose_option(filter=0,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nchoose_option(filter=[(0, 1)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(1, 1)]\nReceiver 5 received signals: [[(0, 1)]] and action: [(1, 1)]\nReceiver 5 action: [(1, 1)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n--- Step 8 Game 0 ---\nNature 0 set state 1\nNature 5 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(1, 1)]\nSender 1 sends signal: [(1, 1)]\nSender 1 sends signal: [(1, 1)]\nSender 1 sends signal: [(1, 1)]\nchoose_option(filter=[(1, 1)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(0, 1)]\nReceiver 2 received signals: [[(1, 1)]] and action: [(0, 1)]\nReceiver 2 action: [(0, 1)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 8 Game 1 ---\nNature 3 set state 0\nNature 2 set state 0\nchoose_option(filter=0,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(0, 2)]\nSender 4 sends signal: [(0, 2)]\nSender 4 sends signal: [(0, 2)]\nSender 4 sends signal: [(0, 2)]\nchoose_option(filter=[(0, 2)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(0, 0)]\nReceiver 5 received signals: [[(0, 2)]] and action: [(0, 0)]\nReceiver 5 action: [(0, 0)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n--- Step 9 Game 0 ---\nNature 0 set state 1\nNature 5 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(1, 2)]\nSender 1 sends signal: [(1, 2)]\nSender 1 sends signal: [(1, 2)]\nSender 1 sends signal: [(1, 2)]\nchoose_option(filter=[(1, 2)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(1, 0)]\nReceiver 2 received signals: [[(1, 2)]] and action: [(1, 0)]\nReceiver 2 action: [(1, 0)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 9 Game 1 ---\nNature 3 set state 1\nNature 2 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nchoose_option(filter=[(0, 1)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(1, 1)]\nReceiver 5 received signals: [[(0, 1)]] and action: [(1, 1)]\nReceiver 5 action: [(1, 1)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n--- Step 10 Game 0 ---\nNature 0 set state 1\nNature 5 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 1 choose_signal: [(1, 1)]\nSender 1 sends signal: [(1, 1)]\nSender 1 sends signal: [(1, 1)]\nSender 1 sends signal: [(1, 1)]\nchoose_option(filter=[(1, 1)],random=<random.Random object at 0x59325df7b330>)\nReceiver 2 received signal: [(0, 1)]\nReceiver 2 received signals: [[(1, 1)]] and action: [(0, 1)]\nReceiver 2 action: [(0, 1)]\nReceiver 2 received reward: 0\nReceiver 1 received reward: None\nSender 1 received reward: None\nReceiver 2 received reward: None\nSender 2 received reward: None\n--- Step 10 Game 1 ---\nNature 3 set state 1\nNature 2 set state 1\nchoose_option(filter=1,random=<random.Random object at 0x59325df7b330>)\nSender 4 choose_signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nSender 4 sends signal: [(0, 1)]\nchoose_option(filter=[(0, 1)],random=<random.Random object at 0x59325df7b330>)\nReceiver 5 received signal: [(1, 0)]\nReceiver 5 received signals: [[(0, 1)]] and action: [(1, 0)]\nReceiver 5 action: [(1, 0)]\nReceiver 5 received reward: 0\nReceiver 4 received reward: None\nSender 4 received reward: None\nReceiver 5 received reward: None\nSender 5 received reward: None\ngame 0, self.expected_rewards(game)=0.25\ngame 1, self.expected_rewards(game)=0.25\n```\n:::\n:::\n\n\nIn this simulation the agents are not learning - they are accessing the predefined signals and actions in the model hence rewards are always 1.\n\nPlayer in Lewis signaling games can reach three type of equilibria\n\n1.  Separating equilibrium in which receiver fully recovers the state from the signal and can take the appropriate action\n2.  Partial pooling equilibrium in which *synonyms* or *homophones* frustrate the receiver for always recovering the state.\n3.  Full pooling equilibrium in which all signals are the same and the agents are unable to communicate.\n\nA one word synonym for \"desired qualities\" derived from desire that used in academic literature is \"desiderata\".\n\nSkryms next considers bottle necks - which are cases where there are more signals than actions and vica versa.\n\n-   In the case of more signals than actions successful learning will result a partial polling equilibrium with some synonyms.\n-   In the case of more actions than signals the best an agent can learn is a partial pooling equilibrium with homophones.\n\nBoth synonyms and homophones have drawbacks however:\n\nWhile synonyms increase the cognitive load and the number of signals that need to be learned they do not prevent the recovery of the state being communicated. Homophones require the receiver to select an interpretation at random leading to lower payoffs since the receiver unable to recover the state cannot select the correct action. If the number of signal is the same as the number of actions, the pigeon hole principle guarantees that for every synonym there must be a homophone.\n\nIf we consider that for recoverability we need action and signals to be fully correlated it is easy to see that each failure to correlate\n\naction to signals results in a (partial) pooling solution. Thus there are far more partial pooling equilibria than separating equilibria. and it is thus no surprise that natural language is rife with homophones and synonyms.\n\nIn lieu of the fact that partial pooling equilibrium far out number the separating ones with and with out bottlenecks, setting up and later learning a separating signaling system with minimal homophones/synonyms is not trivial task. (If we also factor in cost/risk of miscommunication some homophones are clearly worse than others)\n\n-   Evolution for example may not be the best way for this.\n\n-   While researchers have very basic algorithms to do so, in terms of convergence rate and sample efficiency.\n\nAlthough not considered it is easy to see that there are far more partial pooling\n\nWe can conclude proceed to discuss the desiderata for learning algorithms.\n\nNote: Dropout Algorithm Introducing bottlenecks into neural networks tend to improve their ability to generalize by forcing them to avoid memorizing inputs and come up with more resiliant representations. This suggest that partial pooling equilibria may play a more significant role in structured/complex signaling systems.\n\n## Desiderata for learning algorithms of signaling systems\n\n1.  State recovery - we prefer the algorithm to learn a separating equilibrium and if avoid pooling equilibrium with homophones.\n2.  Convergence - we want the algorithm to quickly converge to the equilibrium.\n3.  Sample efficiency - we want the algorithm to learn after minimal exposure to stimuli.\n\nSome questions\n\n-   How different are the task of creating the signaling system from learning it?\n\n    -   the main difference perhaps is that one party has a mapping and it is up to the second to learn it. they can't find unused symbols and mach them to a new state.\n\n    -   there may be many speakers so making changes will be costly.\n\n-   Can switching roles of sender and receiver give better outcomes in learning ?\n\n    -   this may change for different extensions\n\n-   If there are multiple agent learning can create or learn the signaling system better or faster\n\n    -   what if they have groups with established signal systems\n\n    -   how can they find a new set of mapping with minimal permutation from their original\n\n-   If states used for reward are not random are there better schedules for learning are not random\n\nWhat if each has knowledge of a working signaling system already help adding more players seem to\n\n# 4 Evolution\n\nThe three essential factors in Darwin’s account are\n\n1.  natural variation - mutation, gene flow via migration, genetic drift and recombination in sexual reproduction.\n2.  differential reproduction - [@Taylor1978ESS] replicator dynamics\n3.  inheritance\n\n### ESS\n\nIn [@Smith1973LogicAnimalConflict] the authors introduced a novel solution concept - the ESS or Evolutionary stable strategy, improving on the notion of the Nash equilibrium by replacing agent level play dominance with statistical dominance of strategies.\n\n::: {#ex-ess-hak-dove}\n## ESS Motivating Example Hawk Dove Game\n\n|          | Hawk | Dove |\n|----------|------|------|\n| **Hawk** | 0    | 3    |\n| **Dove** | 1    | 2    |\n\n: Hawk Dove Game\n\nThis explains why hyper-aggressive Hawks type who can defeat more peaceful Doves type do not wipe them out. Hawks have an advantage if there are mostly doves. Once they are in a majority Hawk-Hawk interaction lead to serious injury and death. ESS is a frequency dependent equilibrium.\n:::\n\n## ESS Criteria\n\nIn [@Smith1973LogicAnimalConflict] the authors introduce the following criteria in terms of payoffs for a strategy to be an ESS.\n\nA strategy, S, is evolutionary stable if for any other strategy, M, either:\n\n1.  Fitness (S played against S) \\> Fitness (M played against S) or:\n2.  Fitnesses are equal against S, but Fitness(S against M) \\> Fitness(M against M)\n\nWhere under the first mutants are expelled quickly and under 2 less so.\n\n## Differential Reproduction - Replicator dynamics\n\nReplicator dynamics is driven by Darwinian ﬁtness—expected number of progeny.\n\nso $fitness \\sim \\mathbb E(|progeny|)$ where on average you get what you expect. For strategy $S$ the population\n\n$$\nx_{t}(S) = \\frac{x_{t-1}(S) \\times fitness(S)}{mean\\_fitness}\n$$\n\nand for continuous time[^2]\n\n[^2]: I think that we should consider a lewis hirarcy of games based on lewis games with\\\n    a. logic\\\n    b. conjuctive signals\n\n$$\n\\frac{dx}{dt} = x (fitness(S) - {mean\\_fitness})\n$$\n\nThe main outcomes of this chapter are that for a two state/signal/action Lewis game\n\n1.  Multiple isomorphic signaling systems we could call languages will arise leading to a population of agents split equaly\n2.  In a population of agents whose fitness depends on use of the language the stable state is one in which just one of the language is used by the entire population. Other equilibria are unstable which leads to spontaneous breaking of the symmetry and a gradual drift of the population towards one of the stable states.\n\nNotes:\n\n1.  The analysis fails to consider spatial dynamics. It seems that a in a local pockets of language 1, agents with language 2 might have lower fitness.\n2.  There is a cost of switching and agents typicaly are not born with a fully formed language ability they need to learn a language and that has costs and requires access to signalers with the said language.\n3.  In reality *Pidgeons* and *Creoles* are often formed. This is a language that is a mix of two or more languages. This is a partial pooling equilibrium. The existence of creoles suggest that the population dynamics of language formation is more complex than the simple Lewis game.\n\n## Langauge intergration problem:\n\n### **Problem Definition**\n\nGiven a set of signaling systems ${\\pi_1,\\pi_2,\\ldots,\\pi_𝑛}$, find a permutation $\\pi_m$ such that:\n\n$$\n\\pi_m =\\arg \\min_\\pi \\sum_{𝑖=1}^𝑛 d(\\pi,\\pi_i)\n$$ where d is the Cayley distance between permutations, i.e. the minimum number of transpositions required to transform one permutation into another.\n\n### **Solution Approach**\n\nFinding the exact median permutation is a computationally challenging task because the problem is NP-hard. However, there are heuristic and approximation methods to approach this problem. One common approach is to use a greedy algorithm that iteratively improves a candidate solution based on the distances to all permutations in the set.\n\nHere is a simple heuristic approach to estimate a solution:\n\n1.  **Start with an Initial Guess**: You can start with any permutation, such as 𝜋1π1​ or any permutation randomly chosen from the set.\n\n2.  **Iterative Improvement**:\n\n    -   For each element in the permutation, consider swapping it with every other element.\n    -   Calculate the new total distance after each possible swap.\n    -   If a swap results in a lower total distance, make the swap permanent.\n    -   Repeat this process until no improving swaps are found.\n\nThis approach doesn't guarantee an optimal solution but can often produce a good approximation in a reasonable time frame.\n\nHere's a Python function that demonstrates this basic heuristic:\n\n::: {#76897fad .cell execution_count=3}\n``` {.python .cell-code}\nimport itertools\n\ndef cayley_distance(pi, sigma):\n    \"\"\"Calculate the Cayley distance between two permutations.\"\"\"\n    count = 0\n    temp = list(pi)\n    for i in range(len(pi)):\n        while temp[i] != sigma[i]:\n            swap_index = temp.index(sigma[i])\n            temp[i], temp[swap_index] = temp[swap_index], temp[i]\n            count += 1\n    return count\n\ndef median_permutation(permutations):\n    n = len(permutations[0])  # Assuming all permutations are of the same length\n    current = list(permutations[0])  # Start with the first permutation as an initial guess\n    improving = True\n\n    while improving:\n        improving = False\n        best_distance = sum(cayley_distance(current, p) for p in permutations)\n        for i, j in itertools.combinations(range(n), 2):\n            current[i], current[j] = current[j], current[i]  # Swap elements\n            new_distance = sum(cayley_distance(current, p) for p in permutations)\n            if new_distance < best_distance:\n                best_distance = new_distance\n                improving = True\n            else:\n                current[i], current[j] = current[j], current[i]  # Swap back if no improvement\n\n    return current\n\n# Example usage\npermutations = [\n    [1, 2, 3, 4],\n    [2, 1, 4, 3],\n    [1, 3, 4, 2],\n    [4, 3, 2, 1]\n]\nprint(\"Median permutation:\", median_permutation(permutations))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedian permutation: [1, 2, 3, 4]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_943220/3341496467.py:21: DeprecationWarning:\n\nCalling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n\n/tmp/ipykernel_943220/3341496467.py:24: DeprecationWarning:\n\nCalling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n\n```\n:::\n:::\n\n\n# Learning\n\nTwo type of learning are considered.\n\n1.  Evolution learning using knowledge hard-coded into the genome of the agents. Learning happens though replicator dynamics incorporating randomization followed by natural selection. Also other biologically inspired ideas like mutation and use of a fitness function can come into play.\n\n    The down side of Evolution is that is takes many generation for many structures to emerge. (Richard Dawkings states that the evolution of different morphology of the eye are quick taking only 80 generation to evolve in a simulation from the most rudimentary light sensitive cell and elsewhere suggest that 8 generations are needed to see changes in this type of framework.\n\n2.  RL refers to the type of learning from experience by an organism during its lifetime.\n\n3.  Noam Chomsky and others Linguistics hypothesize that Language learning faculties are to a large extent passed through evolution and for this reason individuals can learn languages based on a rather minimal amount of stimulus. This has also be a reason why many in their field abandoned their work on solving linguistics and went on to research the mysteries of the human brain. I feel that to a large extent this book demonstrates that scientifically the notion of the brain requiring a specialized mechanism to evolve/learn complex language is an unnecessary assumption. (Of course it is possible that the brain has co-evolved together with language and that such mechanism do exist.)\n\n    1.  in one sense the book starts with very simple systems of communication with just a lexicon.\n\n    2.  The formation of more complex systems with syntax are treated in chapter 12 but these results here seem to satisfy a mathematician or a philosopher etc, without delving into different linguistic niceties that might satisfy a linguist.\n\n    3.  However the Lewis game needs only a small tweak (the receiver getting multiple partial signals) to allow a signaling system with a grammar to emmerge via Roth-Erev RL. We can also make a categorical statement that this type of RL is a general purpose learning mechanism not a language specific one.\n\nIn agents we have learning that is based on evolution and requires subsequent generations of agents becoming fitter.\n\nHere are two conceptual ideas to base RL on\n\nLaw of effect\n\n:   Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more ﬁrmly connected with the situation, so that, when it recurs, they will be more likely to recur. — Edward Thorndike, Animal Intelligence, 1911\n\nLaw of practice\n\n:   Learning slows down as reinforcements accrue\n\n## Roth–Erev RL alg:\n\n1.  set starting weight for each option\n2.  weights evolve by addition of rewards gotten\n3.  probability of choosing an alternative is proportional to its weight.\n\n```python RE-RL\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation\nimport random\nimport numpy as np\n\nclass LearningRule:\n    def __init__(self, options, learning_rate=0.1):\n        self.weights = {option: 1.0 for option in options}  # Start with equal weights for all options\n        self.learning_rate = learning_rate\n\n    def update_weights(self, option, reward):\n        # Update the weight of the chosen option by adding the reward scaled by the learning rate\n        old_weight = self.weights[option]\n        self.weights[option] += self.learning_rate * reward\n        print(f\"Updated weight for option {option}: {old_weight} -> {self.weights[option]}\")\n\n    def choose_option(self):\n        # Select an option based on the weighted probabilities\n        total = sum(self.weights.values())\n        probabilities = [self.weights[opt] / total for opt in self.weights]\n        return np.random.choice(list(self.weights.keys()), p=probabilities)\n\nclass LewisAgent(Agent):\n    def __init__(self, unique_id, model, learning_options):\n        super().__init__(unique_id, model)\n        self.message = None\n        self.action = None\n        self.reward = 0\n        self.learning_rule = LearningRule(learning_options, learning_rate=0.1)  # Initialize learning with given options\n\n    def set_reward(self):\n        print(f\"Agent {self.unique_id} received reward: {self.reward}\")\n\nclass Sender(LewisAgent):\n    def send(self):\n        state = self.model.get_state()\n        self.message = self.learning_rule.choose_option()  # Send a signal based on the learned weights\n        print(f\"Sender {self.unique_id} sends signal for state {state}: {self.message}\")\n\n    def update_learning(self):\n        self.learning_rule.update_weights(self.model.current_state, self.reward)  # Update weights based on the state and received reward\n\nclass Receiver(LewisAgent):\n    def receive(self):\n        self.received_signals = [sender.message for sender in self.model.senders]\n        if self.received_signals:\n            self.action = self.learning_rule.choose_option()  # Choose an action based on received signals and learned weights\n\n    def calc_reward(self):\n        correct_action = self.model.states_actions[self.model.current_state]\n        self.reward = 1 if self.action == correct_action else 0\n        print(f\"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}\")\n\n    def update_learning(self):\n        for signal in self.received_signals:\n            self.learning_rule.update_weights(signal, self.reward)  # Update weights based on signals and rewards\n\nclass SignalingGame(Model):\n    def __init__(self, senders_count=1, receivers_count=1, state_count=3):\n        super().__init__()\n        self.k = k\n        self.current_state = None\n\n        # Initialize the states, signals, and actions mapping\n        self.states_signals = list(range(k))  # States are simply numbers\n        self.signals_actions = list(chr(65 + i) for i in range(k))  # Signals are characters\n\n        self.states_actions = {i: i for i in range(k)}  # Mapping states to correct actions\n\n        self.senders = [Sender(i, self, self.signals_actions) for i in range(senders_count)]\n        self.receivers = [Receiver(i + senders_count, self, self.signals_actions) for i in range(receivers_count)]\n        \n        self.schedule = StagedActivation(self, stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])\n\n    def get_state(self):\n        return random.choice(self.states_signals)\n\n    def step(self):\n      \n        self.current_state = self.get_state()\n        print(f\"New state of the world: {self.current_state}\")\n        self.schedule.step()\n\n# Running the model\nmodel = SignalingGame(senders_count=1, receivers_count=1, state_count=3)\nfor i in range(10):\n    print(f\"--- Step {i+1} ---\")\n    model.step()\n    \n```\n\n## Bush–Mosteller RL\n\n1.  If an act is chosen and a reward is gotten the probability is incremented by adding some fraction of the distance between the original probability and probability one\n\n    $$\n    pr_{new}(A)=(1-\\alpha)pr_{old}(A) + a(1)\n    $$\n\n2.  Alternative action probabilities are decremented so that everything adds to one\n\n## Goldilocks RL\n\nWe consider if there is a Goldilocks point in the RL exploration exploitation dilemma which has a good balance of the two.\n\n-   If we stop learning too fast we are **too cold**\n\n-   If we exploring too much we are **too hot**\n\n-   At the limit is the Goldilocks RL point\n\n**Q: is there Goldilocks RL Alg?**\n\n-   Roth—Erev, Thompson sampling & UCB don't get stuck\n\n-   Epsilon greedy is too hot\n\n-   Bush–Mosteller is too cold\n\n## RL variants:\n\n-   BM variants like dynamically adjusting aspiration levels\n\n-   exponential response rule. The basic idea is to make probabilities proportional to the exponential of past reinforcements. [@Blume2002]\n\n-   best response dynamics, aka Cournot dynamics\n\n## Beyond the book:\n\n-   \\^\\[citation needed \\]\\^ investigating RL for this task also suggest that Roth-Erev with forgetting leads to more efficient learning.\n-   \\^\\[citation needed\\]\\^ Another paper suggest that a learning with a certain prior can be better than Roth-Erev learning.\n\nAdding Learning\n\n::: {#2b2f91d3 .cell execution_count=4}\n``` {.python .cell-code}\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation\nimport random\n\nclass LewisAgent(Agent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n        self.message = None\n        self.action= None\n\n    def send(self):\n      pass\n    \n    def recive(self):\n      pass\n\n    def calc_reward(self):\n      pass\n    \n    def set_reward(self):\n        self.reward = model.reward\n        # Placeholder for learning logic\n        print(f\"Agent {self.unique_id} received reward: {self.reward}\")\n \nclass Sender(LewisAgent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n\n    def send(self):\n        state = self.model.get_state()\n        # Learning to map states to signals\n        self.message = self.model.states_signals[state]\n        print(f\"Sender {self.unique_id} sends signal for state {state}: {self.message}\")\n\nclass Receiver(LewisAgent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n\n    def recive(self):\n      self.received_signals=[]\n      for sender in self.model.senders:\n        self.received_signals.append(sender.message)\n            # Learning to map signals to actions\n      if len(self.received_signals)==1:\n        self.action = self.model.signals_actions[self.received_signals[0]]\n      else:\n        self.action = self.model.signals_actions[self.received_signals[0]]\n      \n\n    def calc_reward(self):\n      action = self.model.signals_actions[self.received_signals[0]]\n      correct_action = self.model.states_actions[self.model.current_state]\n      reward = 1 if action == correct_action else 0\n      model.reward = reward\n\n\nclass SignalingGame(Model):\n    def __init__(self, senders_count=1, recievers_count=1, state_count=3):\n        \n        super().__init__()\n        self.senders_count=senders_count\n        self.recievers_count=recievers_count\n        self.num_agents = self.recievers_count+self.senders_count\n\n        # e.g., 0 -> A, 1 -> B, ...\n        self.states_signals = {i: chr(65 + i) for i in range(k)} \n\n        # e.g., A -> 0, B -> 1, ...\n        self.signals_actions = {chr(65 + i): i for i in range(k)}\n        \n        # state 0 needs action 0, state 1 needs action 1, ...\n        self.states_actions = {i: i for i in range(k)}  \n        \n        self.current_state = None\n\n        # Create agents\n        self.senders = []\n        self.receivers=[]\n        self.my_agents=[]\n        self.uid=0\n        for i in range(self.senders_count):\n            sender = Sender(self.uid, self)\n            self.senders.append(sender)\n            self.my_agents.append(sender)\n            self.uid +=1\n        for j in range (self.recievers_count):\n            reciever = Receiver(self.uid, self)\n            self.receivers.append(reciever)\n            self.my_agents.append(reciever)\n            self.uid +=1\n\n        self.schedule = StagedActivation(\n          model=self,\n          agents=self.my_agents, \n          stage_list = ['send','recive','calc_reward','set_reward']\n        )\n    \n    def get_state(self):\n        return self.current_state\n\n    def step(self):\n        self.current_state = random.choice(list(self.states_signals.keys()))\n        print(f\"New state of the world: {self.current_state}\")\n        self.schedule.step()\n\n# Running the model\nk = 3  # Number of states, signals, and actions\nsteps = 10\nmodel = SignalingGame(senders_count=2,recievers_count=1,state_count=k)\nfor i in range(steps):\n    print(f\"--- Step {i+1} ---\")\n    model.step()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--- Step 1 ---\nNew state of the world: 1\nSender 0 sends signal for state 1: B\nSender 1 sends signal for state 1: B\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n--- Step 2 ---\nNew state of the world: 2\nSender 0 sends signal for state 2: C\nSender 1 sends signal for state 2: C\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n--- Step 3 ---\nNew state of the world: 2\nSender 0 sends signal for state 2: C\nSender 1 sends signal for state 2: C\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n--- Step 4 ---\nNew state of the world: 2\nSender 0 sends signal for state 2: C\nSender 1 sends signal for state 2: C\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n--- Step 5 ---\nNew state of the world: 1\nSender 0 sends signal for state 1: B\nSender 1 sends signal for state 1: B\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n--- Step 6 ---\nNew state of the world: 2\nSender 0 sends signal for state 2: C\nSender 1 sends signal for state 2: C\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n--- Step 7 ---\nNew state of the world: 2\nSender 0 sends signal for state 2: C\nSender 1 sends signal for state 2: C\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n--- Step 8 ---\nNew state of the world: 2\nSender 0 sends signal for state 2: C\nSender 1 sends signal for state 2: C\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n--- Step 9 ---\nNew state of the world: 0\nSender 0 sends signal for state 0: A\nSender 1 sends signal for state 0: A\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n--- Step 10 ---\nNew state of the world: 1\nSender 0 sends signal for state 1: B\nSender 1 sends signal for state 1: B\nAgent 0 received reward: 1\nAgent 1 received reward: 1\nAgent 2 received reward: 1\n```\n:::\n:::\n\n\n# 11. Networks I: Logic and Information Processing\n\n## Logic\n\n## Information processing\n\n### Inventing the code Game\n\nThe world has say four states {S1...S4}. In this extended Lewis game where an agent is a receiver of two messages, each with a partial specification the first is {s1\\|\\|s2} or {s3\\|\\|s4} and the second {s1\\|\\|s3} or {s2\\|\\|s4}. The agent needs to process the two messages it to get the full state specification and take the appropriate action in response for getting a reward !\n\nThe added problem here is that the messages one of two flags, and one of two other flags do not have an established system for the message so learning the content of the signals needs to evolve together with the inference.\n\nThe sender can be two agents or one agent with a complex message.\n\nJeffrey Barrett in Barrett 2007a, 2007b. showed that this can be learned with Roth Erev RL\n\nthis is more interesting if there are errors:\n\n-   is a 10% chance of senders making mistakes with only 3% errors by the receiver?! Skyrms explains this due to the inference being like a taking a vote in a Condorcet signaling system.\n\n-   receiver errors are considered in [@Nowak1999] where the authors claim they lead to syntax formation.\n\n# 12. Complex Signals and Compositionality\n\nCCSS\n\n:   complex composeable signaling systems\n\n:   \n\n-   The use of complex signals is not unique to humans.\n\n-   In [@Nowak1999] the authors make a case that complex signals can increase the ﬁdelity of information transmission, by preventing simple signals getting crowded together as the space of potential signals gets ﬁlled up. Also some complex signalsing systems should be simpler to learn. (*can we specify a maximaly learnable family?*) and process inforamtion\n\n-   considered CCSS as conffering greater Darwinian fitness in contexts where *rich information processing is important.*\n\n    -   Q: **Is there a metric for measuring the advantage and or the importance of such information processing needs?**\n\n-   In [@batali1998] the author investigates the emergence of complex signals in populations of neural nets.\n\n-   in [@Kirby2000] the author, extends the model in a small population of interacting artiﬁcial agents.\n\n-   These two papers assume Structured meanings like \\<John, loves, Mary\\>. But I am more interested in the ability of evolving arbitrary structures like a sketch map of resources, a distribution of prices, a small bitmap etc.\n\n-   Skryms takes a similar reductionist POV: finding how to evolve a complex signaling system with minimal departure from the Lewis signaling game and other models already covered....\n\n-   It is suggested that the \"Inventing the code Game\" is a sufficient framework creating basic composeable messages. If the receiver considers a sequence of two partial signals as conjunction the and can integrated into one full message!\n\n    -   Red \\> Top\n\n    -   Green\\> Bottom\n\n    -   Yellow\\> Left\n\n    -   Blue \\> Right\n\n    to signal the state of \\<bottom, left\\> a sender can send \\<green,yellow\\> or \\<yellow,green\\> and the receiver can compose them.\n\n-   But if it is also possible to evolve and learn order for signals a richer form of composeability become possible. Subject–predicate or operator–sentence.\n\n-   Sensitivity to temporal order is something many organisms have already developed in responding to perceptual signals.\n\n-   More generally, we can say that temporal pattern recognition is a fundamental mechanism for anticipating the future.\n\nSkryms points out that temporal order is another mechanism that evolves and that they come together.\n\nUnfortunately Skryms seems to get sidetracked once he point out about order and does not explain how order sensitivity eveloves in \"Making the code game\".\n\n```python\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation\nimport random\n\nclass LewisAgent(Agent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n        self.message = None\n        self.action= None\n\n    def send(self):\n      pass\n    \n    def recive(self):\n      pass\n\n    def calc_reward(self):\n      pass\n    \n    def set_reward(self):\n        self.reward = model.reward\n        # Placeholder for learning logic\n        print(f\"Agent {self.unique_id} received reward: {self.reward}\")\n \nclass Sender(LewisAgent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n\n    def send(self):\n        state = self.model.get_state()\n        \n        # Learning to map states to signals\n        if type(state) is str:\n          self.message = self.model.states_signals[state]\n        else:\n          self.message = set()\n          while len(self.message)>0:\n            message = {model.states_signals[self.message.pop()]}\n            self.message = self.message.union(message)\n        print(f\"Sender {self.unique_id} sends signal for state {state}: {self.message}\")\n\nclass Receiver(LewisAgent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n\n    def recive(self):\n      self.received_signals=[]\n      self.action = set()\n      \n      for sender in self.model.senders:\n        self.received_signals.append(sender.message)\n        # Learning to map signals to actions\n        print(f'{self.received_signals=}')\n        print(f'{type(self.received_signals)=}')\n        \n        for signal_set in self.received_signals:\n          actions = set()\n          while len(signal_set)>0:\n            action  = {model.signals_actions[self.message.pop()]}\n            actions  = actions.union(action)\n          self.action =  self.action.intersection(actions)\n      print(f\"Reciever {self.unique_id} action : {self.action}\")\n\n    def calc_reward(self):\n      action = self.action\n      correct_action = self.model.states_actions[self.model.current_state]\n      reward = 1 if action == correct_action else 0\n      model.reward = reward\n      \n\nclass SignalingGame(Model):\n    def __init__(self, senders_count=1, recievers_count=1, state_count=3):\n        \n        super().__init__()\n        self.senders_count=senders_count\n        self.recievers_count=recievers_count\n        #self.num_agents = self.recievers_count+self.senders_count\n\n        self.states   = [f'{i}' for i in range(state_count)]\n        self.signals = [chr(65 + i) for i in range(state_count)]\n        self.actions = [f'{i}' for i in range(state_count)]\n\n        self.current_state = None\n\n        # Create agents\n        self.senders = []\n        self.receivers=[]\n        self.my_agents=[]\n        self.uid=0\n        \n        for i in range(self.senders_count):\n            sender = Sender(self.uid, self)\n            self.senders.append(sender)\n            self.my_agents.append(sender)\n            self.uid +=1\n        for j in range (self.recievers_count):\n            reciever = Receiver(self.uid, self)\n            self.receivers.append(reciever)\n            self.my_agents.append(reciever)\n            self.uid +=1\n\n        self.schedule = StagedActivation(\n          model=self,\n          agents=self.my_agents, \n          stage_list = ['send','recive','calc_reward','set_reward']\n        )\n    \n        \n\n    def step(self):\n        self.current_state = random.choice(list(self.states_signals.keys()))\n        self.current_state_set = {random.choice(list(self.states_signals.keys()))}\n        print(f\"New state of the world: {self.current_state}\")\n        self.schedule.step()\n\n    def get_state(self):\n        if self.senders_count ==1:\n          return self.current_state\n        else: \n          return {self.current_state, random.choice(list(self.states_signals.keys()))}\n\n\n# Running the model\nstate_count = 3  # Number of states, signals, and actions\nsteps = 10\nmodel = SignalingGame(senders_count=2,recievers_count=1,state_count=state_count)\nfor i in range(steps):\n    print(f\"--- Step {i+1} ---\")\n    model.step()\n\n```\n\n## Some thoughts\n\n1.  learning in the original Lewis language games is exponential in the size of the lexicon. It would seem that some complex signals systems should have orders of magnitude advantage in learning rates compared to the original variants. Lets consider a Lewis signaling system with 27 signals.\\\n    The learning is $O(e^{27})\\propto5\\times10^{12}$\n\n2.  Under a conjunctive structure with three messages a lexicon of 9 messages would be required.\\\n    The learning is $O(e^{9})\\propto 8.1\\times10^{3}$\n\n3.  Under Say we have a VSO complex signal with 3 signals per a positional POS category. This leads to 27 signal lexicon under the original lexicon. Using the complex system only 3 three signals need to be learned.\\\n    So that learning is $O(e^{3}) \\propto 20$\n\n    If we factor learning time as part of the costs of signaling we should expect complex signaling systems to emerge quickly. Also if we consider learning as part of In this case partial pooling states are acceptable and even desirable each signal now has three meaning depending on its position.\n\n4.  In NLP we never see such a perfect utilization of a SS where all synthetically messages are semantically meaningful. On the other hand NLP allow nesting so that sequence like V(VSO)(VSO) corresponding to 3^8^ messages and adding a sub-category modifier prefix (MVMSMO) leads to (3\\^6) signals 729 signals without\n\n5.  For a simulation - some predators can be introduced into the environment nearby agents will signal it presence. Receivers who take that appropriate action will survive. Those that do not may die. Agent have longevity and must learn the language. When agents die they are replaced by infants without a uniform signaling weights.\n\n6.  Another point is that seems obvious is that if we learn/evolve the lexicon with just one one new word at a time the task becomes trivial. We just need to learn one new state to signal and one new signal to action mapping. But learning just one is a one to one matching. If we have some sense of the salience of the signals we can just order them in that order and we keep increasing fitness.... till we reach some marginal rate of fitness where new signals do almost nothing for our survival.\n\n7.  If we can evolve a complex signaling system we can move to next steps like optimizing our lexicon and grammar for:\n    1.  minimizing communications errors, (error detection and correction)\n    2.  maximizing information transmission. (compression)\n    3.  minimizing cost of acquisition. (acquisition)\n    4.  the trade off between grammatic generalization and easily learnability v.s. making the system harder to learn but more efficient for communication.\n    5.  how do we handle inference (for logic)\n    6.  how do we take advantage of predictability for partial messages\n    7.  what about a convention for grammar - useful for agents that need to exchange data in different formats efficently.\n    8.  Costs of morphotactics - can we do all this in practive with human sound systems. Can we figure our metrics for human languages.\n    9.  Given a (human) language tree can we posit a most pasimonius path for its evolution.\n\n\n# Signals Bibliography - some annotations\n\nThe following bibliographical entries are on General and Evolutionary Game Theory:\n\n-   [@vonneumann1947] - The first book on game theory.\n-   [@Weibull1997] - A book on evolutionary game theory.\n-   [@hofbauer1998evolutionary] - Covers the mathematical theory of evolutionary games focusing on the lokta volterra model and the replicator dynamics.\n-   [@Samuelson1998Evolutionary] - Covers on evolutionary game theory in extensive forms.\n-   [@Nowak2006Evolutionary] - A book on evolutionary dynamics.\n-   **Aumann, R. (1987) \"Subjectivity and Correlation in Randomized Strategies.\" Journal of Mathematical Economics 1: 67–96.**\n-   **Aumann, R. (1987) \"Correlated Equilibrium as an Expression of Bayesian Rationality.\" Econometrica 55: 1–18.**\n\n\nTHe following are on Roth Erev RL:\n\n-   Bush, R. and F. Mosteller (1955) Stochastic Models of Learning. John Wiley & Sons: New York.\n-   Erev, I. and E. Haruvy (2005) “On the Potential Uses and Current Limitations of Data-Driven Learning Models.” Journal of Mathematical Psychology 49: 357–371.\n-   Erev, I. and A. Roth (1998) “Predicting How People Play Games: Reinforcement Learning in Experimental Games with Unique Mixed-Strategy Equilibria.” American Economic Review 88: 848–881.\n-   Roth, A. and I. Erev (1995) “Learning in Extensive Form Games: Experimental Data and Simple Dynamical Models in the Intermediate Term.” Games and Economic Behavior 8: 164–212.\n-   Herrnstein, R. J. (1961) “Relative and Absolute Strength of Response as a Function of Frequency of Reinforcement.” Journal of Experimental Analysis of Behavior 4: 267–272.\n-   Herrnstein, R. J. (1970) “On the Law of Effect.” Journal of the Experimental Analysis of Behavior 13: 243–266.\n\n\nIn [@Skyrms2010signals] the author discusses how a Lewis signaling games can be viewed as a mechanism in which a rudimentary signaling system can give rise to a simple language.\n\nThe languages arising from Lewis signaling games are limited.\n\nThe chapter on complex signals in terms tend to give rise to a\n\nThe bibliography\n\n-   Aldous, D. (1985) “Exchangeability and Related Topics.” In L’E´cole d’e´te´ de probabilite´s de Saint-Flour, XIII–1983 1–198. Berlin: Springer.\n-   Argiento, R., R. Pemantle, B. Skyrms, and S. Volkov (2009) “Learning to Signal: Analysis of a Micro-Level **Reinforcement Model**.” Stochastic Processes and their Applications 119: 373–390.\n-   ~~Aristotle Historia Animalium Book IX. Aristotle Physics Book II.~~\n-   Asher, N., I. Sher and M. Williams (2001) “Game Theoretical Foundations for Gricean Constraints.” In Proceedings of the Thirteenth Amsterdam Colloquium. Amsterdam: IILC.\n-   Bala, V. and S. Goya (2000) “A Noncooperative Model of Network Formation.” Econometrica 68: 1181–1231.\n-   [@Barrett2006Numerical] ~~Barrett, J. A. (2006) “Numerical Simulations of the Lewis Signaling Game: Learning Strategies, Pooling Equilibria, and the Evolution of Grammar.” Working Paper MBS06–09. University of California, Irvine.~~\n-   [@Barrett2009Evolution] ~~Barrett, J. A. (2007a) “The Evolution of Coding in Signaling Games.” Theory and Decision. DOI: 10.1007/s11238–007–9064–0.~~\n-   [@Barrett2007Dynamic] ~~Barrett, J. A. (2007b) “Dynamic Partitioning and the Conventionality of Kinds.” Philosophy of Science 74: 527–546.~~\n-   Barrett, J. A. and K. Zollman (2007) “The Role of Forgetting in the Evolution and Learning of Language.” preprint.\n-   @batali1998 ~~Batali, J. (1998) “Computational Simulations of the Evolution of Grammar.” In Approaches to the Evolution of Language: Social and Cognitive Bases, ed. J. R. Hurford et al. Cambridge: Cambridge University Press.~~\n-   Beggs, A. (2005) “On the Convergence of **Reinforcement Learning**.” Journal of Economic Theory 122: 1–36.\n-   Benaim, M. (1999) “Dynamics of Stochastic Approximation Algorithms.” In Seminaire de Probabilites 33. Berlin: Springer Verlag.\n-   Benaim, M., S. J. Shreiber, and P. Tarres (2004) “Generalized Urn Models of Evolutionary Processes.” Annals of Applied Probability 14: 1455–1478.\n-   Bereby-Meyer, Y. and I. Erev (1998) “On Learning How to be a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain.” Journal of Mathematical Psychology 42: 266–286.\n-   Berg, R. M. van den (2008) Proclus’ Commentary on the Cratylus in Context. Leiden: Brill.\n-   Bergstrom, T. (2002) “Evolution of Social Behavior: Individual and Group Selection Models.” Journal of Economic Perspectives 16: 231–238.\n-   Bergstrom, C. T. and M. Lachmann (1998) “Signaling Among Relatives III. Talk is Cheap.” Proceedings of the National Academy of Sciences USA 95: 5200–5105.\n-   Berleman, J. E., J. Scott, T. Chumley, and J. R. Kirby (2008) “Predataxis Behavior in Myxococcus Xanthus.” Proceedings of the National Academy of Sciences USA 105: 17127–17132.\n-   Berninghaus, S., K.-M. Ehrhart, M. Ott, and B. Vogt (2007) “Evolution of Networks–an Experimental Analysis.” Journal of Evolutionary Economics 17: 317–347.\n-   Bickerton, D. (1990) Language and Species. Chicago: University of Chicago Press.\n-   Bjornerstedt, J. and J. Weibull (1995) “Nash Equilibrium and Evolution by Imitation.” In K. Arrow et al. (eds.), 155–71, The Rational Foundations of Economic Behavior. New York: Macmillan.\n-   Bloch, F. and M. Jackson (2007) “The Formation of Networks with Transfers among Players.” Journal of Economic Theory 133: 83–110.\n-   Bloch, F. and B. Dutta (2009) “Communication Networks with Endogenous Link Strength.” Games and Economic Behavior 66: 39–56.\n-   Blume, A. (2000) “Coordination and Learning with a Partial Language.” Journal of Economic Theory 95: 1–36.\n-   Blume, A., D. DeJong, Y.-G. Kim, and G. B. Sprinkle (1998) “Experimental Evidence on the Evolution of the Meaning of Messages in Sender-Receiver Games.” American Economic Review 88: 1323–1340.\n-   Blume, A., D. DeJong, Y.-G. Kim, and G. B. Sprinkle (2001) “Evolution of Communication with Partial Common Interest.” Games and Economic Behavior 37: 79–120.\n-   [@Blume2002] ~~Blume, A., D. DeJong, G. Neumann, N. E. Savin (2002) “Learning and Communication in Sender-Receiver Games: An Econometric Investigation.” Journal of Applied Econometrics 17: 225–247.~~\n-   Borgers, T. and R. Sarin (1997) “**Learning through Reinforcement and the Replicator Dynamics**.” Journal of Economic Theory 74: 235–265.\n-   Borgers, T. and R. Sarin (2000) “Naive Reinforcement Learning with Endogenous Aspirations.” International Economic Review 41: 921–950.\n-   Brandman, O. and T. Meyer (2008) “Feedback Loops Shape Cellular Signals in Space and Time.” Science 322: 390–395. Brentano, F. (1874) Psychology from an Empirical Standpoint. London: Routledge.\n-   Brown, G. W. (1951) “Iterative Solutions of Games by Fictitious Play.” In Activity Analysis of Production and Allocation, ed. T. C. Koopmans. New York: Wiley.\n-   Bshary, R., A. Hohner, K. Ait-el-Djoudi, and H. Fricke (2006) “Interspecific Communicative and Coordinated Hunting between Groupers and Giant Moray Eels in the Red Sea.” PLoS Biology 4:2393–2398 4:e431, DOI:10:1371/journal.pbio.0040431.\n-   Callander, S. and C. R. Plott (2005) “Principles of Network Development and Evolution: An Experimental Study.” Journal of Public Economics 89: 1469–1495.\n-   Camerer, C. and T-H. Ho (1999) “Experience Weighted Attraction Learning in Normal Form Games.” Econometrica 67: 827–874.\n-   Campbell, G. (2003) Lucretius on Creation and Evolution. Oxford: Oxford University Press.\n-   Charrier, I. and C. B. Sturdy (2005) “Call-Based Species Recognition in the Black-Capped Chickadees.” Behavioural Processes 70: 271–281.\n-   Cheney, D. and R. Seyfarth (1990) How Monkeys See the World: Inside the Mind of Another Species. Chicago: University of Chicago Press.\n-   Cross, J. G. (1973) “A Stochastic Learning Model of Economic Behavior.” Quarterly Journal of Economics 87: 239–266.\n-   Crawford, V. and J. Sobel (1982) “Strategic Information Transmission.” Econometrica 50: 1431–1451.\n-   Cubitt, R. and R. Sugden (2003) “Common Knowledge, Salience and Convention: A Philosophical Reconstruction of David Lewis’ Game Theory.” Economics and Philosophy 19:175–210.\n-   Donaldson, M. C., M. Lachmann, and C. T. Bergstrom (2007) “The Evolution of Functionally Referential Meaning in a Structured World.” Journal of Theoretical Biology 246: 225–233.\n-   Dretske, F. (1981) Knowledge and the Flow of Information. Cambridge: MIT Press.\n-   Dugatkin, L. A. (1997) Cooperation Among Animals: An Evolutionary Perspective. Oxford: Oxford University Press.\n-   Dyer, F. C. and T. D. Seeley (1991) “Dance Dialects and Foraging Range in three Asian Honey Bee Species.” Behavioral Ecology and Sociobiology 28: 227–233.\n-   Edwards, W. (1961) “Probability Learning in 1000 Trials.” Journal of Experimental Psychology 62: 385–394.\n-   Estes, W. K. (1950) “Toward a Statistical Theory of Learning.” Psychological Review 57: 94–107.\n-   Evans, C. S., C. L. Evans and P. Marler (1994) “On the Meaning of Alarm Calls: Functional Reference in an Avian Vocal System.” Animal Behavior 73: 23–38.\n-   Falk, A. and M. Kosfeld (2003) “It’s All About Connections: Evidence on Network Formation.” IEW Working Paper 146. University of Zurich.\n-   Feltovich, N. (2000) “Reinforcement-Based vs. Belief-Based Learning Models in Experimental Asymmetric-Information Games.” Econometrica 68: 605–641.\n-   Flache, A. and M. Macy (2002) “Stochastic Collusion and the Power Law of Learning.” Journal of Conflict Resolution 46: 629–653.\n-   Floridi, L. (1997) “Skepticism, Animal Rationality, and the Fortune of Chrysippus’ Dog.” Archiv fu¨r Geschichte der Philosophie 79: 27–57.\n-   Frede, D. and B. Inwood (2005) Language and Learning: Philosophy of Language in the Hellenistic Age. Cambridge: Cambridge University Press. Fudenberg,\n-   D. and D. Levine (1998) A Theory of Learning in Games. Cambridge, MA: MIT Press.\n-   Galeotti, A. and S. Goyal (2008) “The Law of the Few.” Working paper, University of Essex.\n-   Galeotti, A., S. Goyal, and J. Kamphorst (2006) “Network Formation with Heterogeneous Players.” Games and Economic Behavior 54: 353–372.\n-   Gazda, S., R. C. Connor, R. K. Edgar, and F. Cox (2005) “A Division of Labour with Role Specialization in Group-hunting Bottlenose Dolphins (Tursiops truncatus) off Cedar Key, Florida.” Proceedings of the Royal Society B 272: 135–140.\n-   Gentner, T. Q., K. M. Fenn, D. Margoliash, and H. C. Nusbaum (2006) “Recursive Syntactic Pattern Learning by Songbirds.” Nature 440: 1204–1207.\n-   Gettier, E. (1963) “Is Justified True Belief Knowledge?” Analysis 23:121–123.\n-   Godfrey-Smith, P. (1989) “Misinformation.” Canadian Journal of Philosophy 19: 522–550.\n-   Godfrey-Smith, P. (2000a) “On the Theoretical Role of Genetic Coding.” Philosophy of Science 67: 26–44.\n-   Godfrey-Smith, P. (2000b) “Information, Arbitrariness and Selection: Comments on Maynard-Smith.” Philosophy of Science 67: 202–207.\n-   Good, I. J. (1950) Probability and the Weighing of Evidence. London: Charles Griffin.\n-   Good, I. J. (1983) Good Thinking: The Foundations of Probability and its Applications. Minneapolis: University of Minnesota Press.\n-   Gould, J. L. (1975) “Honey Bee Recruitment: the Dance Language Controversy.” Science 189: 685–693.\n-   Gould S. J., and N. Eldredge (1977) “Punctuated Equilibria: The Tempo and Mode of Evolution reconsidered.” Paleobiology 3: 115–151.\n-   Goryachev, A. B., D. J. Toh, and T. Lee (2006) “Systems Analysis of a Quorum Sensing Network: Design Constraints Imposed by the Functional Requirements, Network Topology and Kinetic Constants.” BioSystems 83: 178–187.\n-   Goyal, S. (2007) Connections: An Introduction to the Economics of Networks. Princeton: Princeton University Press.\n-   Grice, H. P. (1957) “Meaning.” Philosophical Review 66: 377–388.\n-   Grice, H. P. (1975) “Logic and Conversation.” In Syntax and Semantics, vol. 3, ed. P. Cole and J. L. Morgan, 41–58. New York: Academic Press.\n-   Grice, H. P. (1989) Studies in the Way of Words. Cambridge, MA: Harvard University Press.\n-   Griffiths, P. E. (2001) “Genetic Information: A Metaphor in Search of a Theory.” Philosophy of Science 68: 394–412.\n-   Grim, P., P. St. Denis, and T. Kokalis (2002) “Learning to Communicate: The Emergence of Signaling in Spatialized Arrays of Neural Nets.” Adaptive Behavior 10: 45–70.\n-   Grim, P., T. Kokalis, A. Alai-Tafti, A., and N. Kilb (2000) “Evolution of Communication in Perfect and Imperfect Worlds.” World Futures: The Journal of General Evolution 56: 179–197.\n-   Grim, P., T. Kokalis, A. Alai-Tafti, N. Kilb, and P. St. Denis (2004) “Making Meaning Happen.” Journal of Experimental and Theoretical Artificial Intelligence 16: 209–243.\n-   Gyger, M., P. Marler, and R. Pickert (1987) “Semantics of an Avian Alarm Call System: The Male Domestic Fowl,\n-   Gallus Domesticus.” Behavior 102: 15–20. 184\n-   Hadeler, K. P. (1981) “Stable Polymorphisms in a Selection Model with Mutation.” SIAM Journal of Applied Mathematics 41: 1–7.\n-   Hailman, J., M. Ficken, and R. Ficken (1985) “The ‘Chick-a-dee’ calls of Parus atricapillus.” Semiotica 56: 191–224.\n-   Hamilton, W. D. (1963) “The Evolution of Altruistic Behavior.” American Naturalist 97: 354–356.\n-   Hamilton, W. D. (1964) “The Genetical Evolution of Social Behavior I and II.” Journal of Theoretical Biology 7: 1–52.\n-   Hamilton, W. D. (1967) “Extraordinary Sex Ratios.” Science 156: 477–488.\n-   Hamilton, W. D. (1971) “Selection of Selfish and Altruistic Behavior in Some Extreme Models.” In Man and Beast, ed. J. F. Eisenberg and W. S. Dillon, 59–91. Washington, D.C.: Smithsonian Institution Press.\n-   Hamilton, W. D. (1995) Narrow Roads of Gene Land. vol. 1: Evolution of Social Behavior. New York: W. H. Freeman.\n-   Harley, C. B. (1981) “Learning the **Evolutionarily Stable Strategy**.” Journal of Theoretical Biology 89: 611–633.\n-   Harms, W. F. (2004) Information and Meaning in Evolutionary Processes. Cambridge: Cambridge University Press.\n-   Hauert, C., S. De Monte, J. Hofbauer, and K. Sigmund (2002) “Volunteering as Red Queen Mechanism for Cooperation in Public Goods Games.” Science 296, 1129–1132.\n\n-   Hauser, M. D. (1988) “How Infant Vervet Monkeys Learn to Recognize Starling Alarm Calls: The Role of Experience.” Behavior 105: 187–201.\n\n-   Hauser, M. D. (1997) The Evolution of Communication. Cambridge, MA: MIT Press.\n\n-   Hauser, M. D., N. Chomsky, and W. T. Fitch (2002) “The Faculty of Language: What is it, Who has it, and How did it Evolve.” Science 298: 1569–1579.\n\n-   Hebb, D. (1949) The Organization of Behavior. New York: Wiley.\n\n-   Herman, L. M., D. G. Richards, and J. P. Wolz (1984) “Comprehension of Sentences by Bottle-Nosed Dolphins.” Cognition 16: 129–219.\n\n-   Ho, T. H., X. Wang, and C. Camerer (2008) “Individual differences in EWA Learning with Partial Payoff Information.” The Economic Journal 118: 37–59.\n\n-   Hofbauer, J. (1985) “The Selection-Mutation Equation.” Journal of Mathematical Biology. 23: 41–53.\n\n-   Hofbauer, J. and S. Huttegger (2008) “Feasibility of Communication in Binary Signaling Games.” Journal of Theoretical Biology 254: 843–849.\n\n\n\n-   Hojman, D. A. and A. Szeidl (2008) “Core and Periphery in Networks.” Journal of Economic Theory. 139: 295–309.\n\n-   Holland, J. (1975) Natural and Artificial Systems. Ann Arbor, Michigan: University of Michigan Press.\n\n-   Holldobler, B. and E. O. Wilson (1990) The Ants. Cambridge, MA:\n\n-   Belknap. Hoppe, F. M. (1984) “Polya-like Urns and the Ewens Sampling Formula.” Journal of Mathematical Biology 20: 91–94.\n-   Hopkins, E. (2002) “Two Competing Models about How People Learn in Games.” Econometrica 70, 2141–2166.\n-   Hopkins, E. and M. Posch (2005) “Attainability of Boundary Points under Reinforcement Learning.” Games and Economic Behavior 53: 110–125. Hume, D. (1739) A Treatise of Human Nature. London: John Noon.\n-   Hurford, J. (1989) “Biological Evolution of the Saussurean Sign as a Component of the Language Acquisition Device.” Lingua 77: 187–222.\n-   Huttegger, S. (2007a) “Evolution and the Explanation of Meaning.” Philosophy of Science 74: 1–27.\n-   Huttegger, S. (2007b) “Evolutionary Explanations of Indicatives and Imperatives.” Erkenntnis 66: 409–436.\n-   Huttegger, S. (2007c) “Robustness in Signaling Games.” Philosophy of Science 74: 839–847.\n-   Huttegger, S. and B. Skyrms (2008) “Emergence of Information Transfer by Inductive Learning.” Studia Logica 89: 237–256.\n-   Huttegger, S., B. Skyrms, R. Smead, and K. Zollman (2009) “Evolutionary Dynamics of Lewis Signaling Games: Signaling Systems vs. Partial Pooling.” Synthese. DOI: 10.1007/s11229–009–9477–0\n-   Izquierdo, L., D. Izquierdo, N. Gotts, and J. G. Polhill (2007) “Transient and Asymptotic Dynamics of Reinforcement Learning in Games.” Games and Economic Behavior 61: 259–276. 186 SIGNALS: EVOLUTION, LEARNING, AND INFORMATION\n-   Jackendoff, R. (2002) Foundations of Language. Oxford: Oxford University Press.\n-   Jackson, M. (2008) Social and Economic Networks. Princeton: Princeton University Press.\n-   Jackson, M. and A. Watts (2002) “On the Formation of Interaction Networks in Social Coordination Games.” Games and Economic Behavior 41: 265–291.\n-   Kavanaugh, M. (1980) “Invasion of the Forest by an African Savannah Monkey: Behavioral Adaptations.” Behavior 73: 239–60. Kirby, S. (2000) “Syntax without Natural Selection: How Compositionality Emerges from Vocabulary in a Population of Learners.” In The Evolutionary Emergence of Language, ed. C. Knight, 303–323. Cambridge: Cambridge University Press.\n-   Kirby, S. (2007) “The Evolution of Meaning-Space Structure through Iterated Learning.” In Emergence of Communication and Language, ed. C. Lyon et al., 253–268. Berlin: Springer Verlag.\n-   Kirkup, B. C. and M. A. Riley (2004) “Antibiotic-Mediated Antagonism Leads to a Bacterial Game of Rock-Paper-Scissors in vivo.” Nature 428: 412–414.\n-   Komarova, N. and P. Niyogi (2004) “Optimizing the Mutual Intelligibility of Linguistic Agents in a Shared World.” Artificial Intelligence 154: 1–42.\n-   **Komarova, N., P. Niyogi, and M. Nowak (2001) “The Evolutionary Dynamics of Grammar Acquisition.” Journal of Theoretical Biology 209: 43–59.**\n-   Kosfeld, M. (2004) “Economic Networks in the Laboratory.” Review of Network Economics 3: 20–41.\n-   Kullback, S. and R. A. Leibler (1951) “On Information and Sufficiency.” Annals of Mathematical Statistics 22: 79–86.\n-   Kullback, S. (1959) Information Theory and Statistics. New York: John Wiley.\n-   Levi-Strauss, C. (1969) The Elementary Structures of Kinship. Boston: Beacon Press.\n-   Lewis, D. K. (1969) Convention. Cambridge, MA: Harvard University Press.\n-   Liggett, T. M. and S. Rolles (2004) “An Infinite Stochastic Model of Social Network Formation.” Stochastic Processes and their Applications 113: 65–80.\n-   Lindley, D. (1956) “On a Measure of the Information Provided by an Experiment.” The Annals of Mathematical Statistics 27: 986–1005.\n-   Luce, R. D. (1959) Individual Choice Behavior. John Wiley & Sons: New York.\n-   Macedonia, J. M. (1990) “What is Communicated in the Antipredator Calls of Lemurs: Evidence from Antipredator Call Playbacks to Ringtailed and Ruffed Lemurs.” Ethology 86: 177–190.\n-   McKinnon, S. (1991) From a Shattered Sun. Madison: University of Wisconsin Press.\n-   Macy, M. (1991) “Learning to Cooperate: Stochastic and Tacit Collusion in Financial Exchange.” American Journal of Sociology 97: 808–843.\n-   Macy, M. and A. Flache (2002) “Learning Dynamics in Social Dilemmas.” Proceedings of the National Academy of Sciences of the USA 99: 7229– 7236.\n-   Marden, J. P., H. P. Young, G. Arslan, and J. S. Shamma (2009) “Payoffbased dynamics for Multiplayer Weakly Acyclic Games.” SIAM Journal on Control and Optimization 48: 373–396. 188\n\n-   [@Smith1973LogicAnimalConflict] ~~Maynard Smith, J. and G. R. Price (1973) “The Logic of Animal Conflict.” Nature 246: 15–18.~~\n-   Maynard Smith, J. and G. A. Parker (1976) “The Logic of Asymmetric Contests.” Animal Behaviour 24: 159–175.\n-   Maynard Smith, J. (1982) Evolution and the Theory of Games. Cambridge: Cambridge University Press.\n-   Maynard Smith, J. (2000) “The Concept of Information in Biology.” Philosophy of Science 67: 177–194.\n-   Maynard Smith, J. and D. Harper (2003) Animal Signals. Oxford: Oxford University Press.\n-   Mayor, J. (1898) “King James I On the Reasoning Faculty in Dogs.” The Classical Review 12: 93–96.\n-   McGregor, P. (2005) Animal Communication Networks. Cambridge University Press: Cambridge.\n-   Merin, A. (1999) “Information, Relevance, and Social Decisionmaking: Some Principles and Results of Decision-Theoretic Semantics.” In L. Moss, J. Ginzburg, M. de Rijke (eds.), 179–221, Logic, Language, and Computation, vol. 2. Stanford: CSLI.\n-   Miller, M. B. and B. Bassler (2001) “Quorum Sensing In Bacteria.” Annual Review of Microbiology 55: 165–199.\n-   Millikan, R. G. (1984) Language, Thought and Other Biological Categories. Cambridge, MA: MIT Press.\n-   Milo, R., S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon (2002) “Network Motifs: Simple Building Blocks of Complex Networks.” Science 298: 824–827.\n-   [@doi:10.1073/pnas.96.14.8028] ~~**Nowak, M. A. and D. Krakauer (1999) “The Evolution of Language.” Proceedings of the National Academy of Sciences of the USA 96: 8028–8033.**~~\n-   Nowak, M., J. Plotkin, and D. Krakauer (1999) “The Evolutionary Language Game.” Journal of Theoretical Biology 200: 147–162.\n-   Nowak, M. and K. Sigmund (1993) “A Strategy of Win-stay, Lose-shift that Outperforms Tit-for-tat in the Prisoner’s Dilemma Game.” Nature 364: 56–58.\n-   Oliphant, M. (1994) “The Dilemma of Saussurean Communication.” Biosystems 37: 31–38.\n-   Othmer, H. G. and A. Stevens (1997) “Aggregation, Blow Up and Collapse: The ABC’s of Taxis in Reinforced Random Walks.” SIAM Journal on Applied Mathematics 57: 1044–1081.\n-   Papineau, D. (1984) “Representation and Explanation.” Philosophy of Science 51: 550–72.\n-   Papineau, D. (1987) Reality and Representation. Oxford: Blackwell.\n-   Parikh, P. (2001) The Use of Language. Stanford: CSLI.\n-   Pawlowitsch, C. (2008) “Why Evolution Does Not Always Lead to an Optimal Signaling System.” Games and Economic Behavior 63: 203–226.\n-   Pemantle, R. (1990) “Nonconvergence to Unstable Points in Urn Models and Stochastic Approximations.” Annals of Probability 18: 698–712.\n-   Pemantle, R. (2007) “A Survey of Random Processes with Reinforcement.” Probability Surveys 4: 1–79.\n-   Pemantle, R. and B. Skyrms (2004a) “Network Formation by Reinforcement Learning: The Long and the Medium Run.” Mathematical Social Sciences 48: 315–327.\n-   Pemantle, R. and B. Skyrms (2004b) “Time to Absorption in Discounted Reinforcement Models” Stochastic Processes and Their Applications 109: 1–12.\n-   Pinker, S., and R. Jackendoff (2005) “The Faculty of Language: What’s Special About It?” Cognition 95: 201–236.\n-   Quine, W. V. O. (1936) “Truth by Convention.” In Philosophical Essays for A. N. Whitehead, ed. O. H. Lee. 90–124.\n-   Quine, W. V. O. (1969) “Epistemology Naturalized.” In Ontological Relativity and Other Essays. New York: Columbia University Press.\n-   Rainey, H. J., K. Zuberbu¨hler, and P. J. B. Slater (2004) “Hornbills Can Distinguish between Primate Alarm Calls.” Proceedings of the Royal Society of London B 271: 755–759.\n-   J. Riley, R. U. Greggers, A. D. Smith, D. R. Reynolds, and R. Menzel (2005) “The Flight Paths of Honeybees Recruited by the Waggle Dance.” Nature 435: 205–207.\n-   Robbins, H. (1952) “Some Aspects of the Sequential Design of Experiments.” Bulletin of the American Mathematical Society 58: 527–535. 190\n-   van Rooy, Robert. (2003) “Questioning to Resolve Decision Problems.” Linguistics and Philosophy 26:727–763.\n\n-   Russell, B. (1921) The Analysis of Mind. (Lecture X) London: George Allen and Unwin.\n\n-   Russell, B. (1948) Human Knowledge, Its Scope and Limits. New York: Simon and Schuster.\n\n-   Salmon, T. C. (2001) “An Evaluation of Econometric Models of Adaptive Learning.” Econometrica 1597–1628.\n\n-   Schlag, K. (1998) “Why Imitate and If So, How? A Bounded Rational Approach to Many Armed Bandits.” Journal of Economic Theory 78, 130–156.\n\n-   Schreiber, Sebastian J. (2001) “Urn Models, Replicator Processes, and Random Genetic Drift”, SIAM Journal on Applied Mathematics, 61.6: 2148–2167.\n\n-   Schultz, W. (2004) “Neural Coding of Basic Reward Terms of Animal Learning Theory, Game Theory, Microeconomics and Behavioural Ecology.” Current Opinion in Neurobiology 14:139–147.\n\n-   Searcy, W. A. and S. Nowicki (2005) The Evolution of Animal Communication: Reliability and Deception in Signaling Systems. Princeton: Princeton University Press.\n\n-   **Selten, R. and W. Massimo (2007) “The Emergence of Simple Languages in an Experimental Coordination Game.” Proceedings of the National Academy of Sciences of the USA 104: 7361–7366.**\n\n-   Seyfarth, R. M. and D. L. Cheney (1990) “The Assessment by Vervet Monkeys of Their Own and Other Species’ Alarm Calls.” Animal Behavior 40: 754–764.\n\n-   **Shannon, C. (1948) “A Mathematical Theory of Communication.” The Bell System Mathematical Journal 27: 379–423, 623–656.**\n\n-   **Shannon, C. and W. Weaver (1949) The Mathematical Theory of Communication. Urbana: University of Illinois Press.**\n\n-   Shreiber, S. (2001) “Urn Models, Replicator Processes and Random Genetic Drift.” SIAM Journal on Applied Mathematics 61: 2148–2167.\n\n-   Sinervo, B. and C. M. Lively (1996) “The Rock-Paper-Scissors Game and the Evolution of Alternative Male Strategies.” Nature 380: 240–243.\n\n-   Skyrms, B. (1996) Evolution of the Social Contract. Cambridge: Cambridge University Press. Skyrms, B. (1998) “Salience and Symmetry-Breaking in the Evolution of Convention.” Law and Philosophy 17: 411–418.\n\n-   Skyrms, B. (1999) “Stability and Explanatory Significance of Some Simple Evolutionary Models.” Philosophy of Science 67: 94–113.\n\n-   Skyrms, B. (2000) “Evolution of Inference.” In Dynamics of Human and Primate Societies, ed. Tim Kohler and George Gumerman, 77–88. New York: Oxford University Press.\n\n-   Skyrms, B. (2004) The Stag Hunt and the Evolution of Social Structure. Cambridge: Cambridge University Press.\n\n-   Skyrms, B. (2005) “Dynamics of Conformist Bias.” Monist 88: 260–269.\n\n-   Skyrms, B. (2007) “Dynamic Networks and the Stag Hunt: Some Robustness Considerations.” Biological Theory 2: 7–9.\n\n-   Skyrms, B. (2009) “Evolution of Signaling Systems with Multiple Senders and Receivers.” Philosophical Transactions of the Royal Society B doi:10.1098/rstb.2008.0258, 364: 771–779.\n\n-   Skyrms, B. (2009) “Presidential Address: Signals.” Philosophy of Science 75:489–500.\n\n-   Skyrms, B. and R. Pemantle (2000) “A Dynamic Model of Social Network Formation.” Proceedings of the National Academy of Sciences of the USA. 97: 9340–9346 192\n\n-   Skyrms, B. and S. L. Zabell (forthcoming) “Inventing New Signals.”\n\n-   Slobodchikoff, C. N., J. Kiriazis, C. Fischer, and E. Creef (1991) “Semantic Information Distinguishing Individual Predators in the Alarm Calls of Gunnison’s Prairie Dogs.” Animal Behaviour 42: 713–719.\n\n-   Smith, A. (1983) \\[1761\\] Considerations Concerning the First Formation of Languages. Reprinted in Lectures on Rhetoric and Belles Lettres, ed. J. C. Bryce. Oxford: Oxford University Press.\n\n-   Snowdon, C. T. (1990) “Language Capacities of Nonhuman Animals.” Yearbook of Physical Anthropology 33: 215–243.\n\n-   Sorabji, R. (1993) Animal Minds and Human Morals: The Origins of the Western Debate. Ithaca: Cornell University Press.\n\n-   Stander, P. E. (1990s) “Cooperative Hunting in Lions: The Role of the Individual.” Behavioral Ecology and Sociobiology 29: 445–454.\n\n-   Stanford, P. K. (2007) Exceeding Our Grasp. Oxford: Oxford University Press.\n\n-   @steels1997synthetic ~~Steels, L. (1997) “The Synthetic Modeling of Language Origins.” Evolution of Communication 1: 1–35.~~\n\n-   @steels1998origins ~~Steels, L. (1998) “The Origins of Syntax in Visually Grounded Robotic Agents.” Artificial Intelligence 103: 133–156.~~\n\n-   @steels2001LanguageGames ~~Steels, L (2001) \"Language games for autonomous robots\". IEEE Intelligent Systems, September-October\n    2001:17–22~~\n\n-   Sterelny, K. (2000) “The ‘Genetic Program’ Program: A Commentary on Maynard-Smith on Information in Biology.” Philosophy of Science 67: 195–201.\n\n-   Sterelny, K. (2003) Thought in a Hostile World: The Evolution of Human Cognition. Oxford: Blackwell.\n\n-   Struhsaker, T. T. (1967) “Auditory Communication among Vervet Monkeys Cercopithecus aethiops.” In Social Communication among Primates, ed. S.A. Altmann, 281–324. Chicago: University of Chicago Press.\n\n-   Sugden, R. (2005) The Economics of Rights, Co-operation and Welfare (Basingstoke: Macmillan).\n\n-   Suppes, P. and R. Atkinson (1960) Markov Learning Models for Multiperson Interactions. Palo Alto, CA: Stanford University Press.\n\n-   Taga, M. E. and B. L. Bassler (2003) “Chemical Communication Among Bacteria.” Proceedings of the National Academy of Sciences of the USA 100 Suppl. 2, 14549–14554.\n\n-   [@Taylor1978ESS]\n\n-   Tempelton, C., E. Greene and K. Davis (2005) “Allometry of Alarm Calls: Black-Capped Chickadees Encode Information about Predator Size.” Science 308: 1934–1937.\n\n-   Thorndike, E. L. (1911) Animal Intelligence. New York: Macmillan.\n\n-   Thorndike, E. L. (1927) “The Law of Effect.” American Journal of Psychology 39: 212–222.\n\n-   **Trapa, P. and M. Nowak (2000) “Nash Equilibria for an Evolutionary Language Game.” Journal of Mathematical Biology 41: 172–188.**\n\n-   Vanderschraaf, P. (1998) “Knowledge, Equilibrium and Convention.” Erkenntnis 49: 337–369.\n\n-   [@Wagner2013Costly]\n\n-   **Wagner, E. (2009) “Communication and Structured Correlation.” Erkenntnis doi 10.1007/s10670–009–9157–y.**\n\n-   Wa¨rneryd, K. (1993) “Cheap Talk, Coordination, and Evolutionary Stability.” Games and Economic Behavior 5: 532–546.\n\n-   Watts, A. (2001) “A Dynamic Model of Network Formation.” Games and Economic Behavior 34: 331–341.\n\n-   Weber, R. and C. Camerer (2003) “Cultural Conflict and Merger Failure: An Experimental Approach.” Management Science 49: 400–415.\n\n-   Wei, L. and S. Durham (1978) “The Randomized Play-the-winner Rule in Medical Trials.” Journal of the American Statistical Association 73: 840–843.\n\n-   Young, H. P. (2009) “Learning by Trial and Error.” Games and Economic Behavior 65: 626–643. 194\n\n-   Zabell, S. L. (1992) “Predicting the Unpredictable.” Synthese 90: 205–232.\n\n-   Zabell, S. L. (2005) Symmetry and Its Discontents: Essays in the History of Inductive Probability. Cambridge: Cambridge University Press.\n\n-   Zeeman, E. C. (1980) “Population Dynamics from Game Theory.” In Global Theory of Dynamical Systems, Springer Lecture Notes on Mathematics 819.\n\n-   Zollman, K. (2005) “Talking to Neighbors: The Evolution of Regional Meaning.” Philosophy of Science 72: 69–85\n\n-   Zuidema, W. (2003) “Optimal Communication in a Noisy and Heterogeneous Environment.” In Proceedings Lecture Notes on Artificial Intelligence v. 2801 Berlin: Springer 553–563.\n\n",
    "supporting": [
      "signals-summary_files"
    ],
    "filters": [],
    "includes": {}
  }
}