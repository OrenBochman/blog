{
  "hash": "f473872651055f6317b9e2828b3c497b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-05-01\ntitle: \"Signals Experiment\"\nsubtitle: \"learing language games\"\nkeywords: [game theory, signaling games, partial pooling, evolution, reinforcement learning, signaling systems, evolution of language]\n#draft: True\n---\n\n::: {#9cf6f10a .cell execution_count=1}\n``` {.python .cell-code}\nfrom mesa import Agent, Model\nfrom mesa.time import RandomActivation\n\n\nclass Urn:\n    def __init__(self, options, balls=None):\n        self.options = options\n        if balls is not None:\n            self.balls = balls\n        else:\n            self.balls = {option: 1.0 for option in self.options}\n    \n    def get_filtered_urn(self, filter):\n        '''Filters urn's options by prefix and normalizes the weights.'''\n        filtered_options = [k for k in self.balls.keys() if k.startswith(filter)]\n        assert len(filtered_options) > 0, f\"no options found for filter={filter} on {self.balls}\"\n        filtered_balls = {opt: self.balls[opt] for opt in filtered_options}\n        total_balls = sum(filtered_balls.values())\n        assert total_balls > 0.0, f\"total weights is {total_balls} after filter={filter} on {self.balls}\"\n        filtered_probs = {opt: self.balls[opt]/total_balls for opt in filtered_options}\n        return filtered_probs\n\n\n    def choose_option(self, filter, model):\n          \n        '''Filters the urn based on a option prefix (state for sender, signal for reciever).\n        \n          In the litrature agents have multiple urns to support learning conditional probabilites for differnt context.\n            - sender need one urns per state, and \n            - recievers need one urn per signal.\n          I choose a simpler representation by implemented multiple urns as a single matrix\n          To get the wieghts coresponding to a for a given prefix we filter the urn based on the prefix.\n          allow updating the conditional probabilities for each signal given a state.\n          We have one urn and estimate the conditional probabilities by filtering the urn based on the prefix.\n        '''\n        \n        \n        if self.verbose:\n            print(f'choose_option({filter=})')\n        urn = self.get_filtered_urn(filter)\n        return model.random.choices(list(urn.keys()), list(urn.values()))\n\n    def update_weights(self, option, reward):\n        old_balls = self.balls[option]\n        self.balls[option] += reward \n        if self.verbose:\n            print(f\"Updated weight for option {option}: {old_balls} -> {self.balls[option]}\")\n\n\n\nclass HerrnsteinRL(Urn):\n    ''' Herrnstein matching law with learning rate.'''\n    def __init__(self, options, learning_rate=1.0, verbose=False, name='Herrnstein matching law'):\n        #add docstring\n        '''\n        Herrnstein matching law with learning rate.\n        \n        Parameters:\n          options: list of options\n          learning_rate: float, default 1.0 should behave like the parent urn model\n          verbose: bool, default False\n          name: str, the rule name 'Herrnstein matching law'\n        '''\n        super().__init__(options)\n        self.verbose = verbose\n        self.name = name\n        self.learning_rate = learning_rate\n        self.options = options\n        if self.verbose:\n            print(f'LearningRule.__init__(Options: {options})')\n\n    def update_weights(self, option, reward):\n        ''' this adds the learning rate to the update'''\n        old_balls = self.balls[option]\n        self.balls[option] += self.learning_rate * reward \n        if self.verbose:\n            print(f\"Updated weight for option {option}: {old_balls} -> {self.balls[option]}\")\n\n\nclass LewisAgent(Agent):\n    def __init__(self, unique_id, model, game, role, verbose=False):\n        '''Agent for Lewis signaling game.\n        \n        Parameters:\n          unique_id: int, unique identifier\n          model: SignalingGame, the model\n          game: int, the game number\n          role: str, the role of the agent\n          verbose: bool, default False\n        '''\n        \n          \n        super().__init__(unique_id, model)\n        self.role = role\n        self.verbose = verbose\n        self.message = None\n        self.action = None\n        self.game = game\n        self.current_state = None\n        if role == \"sender\":\n            self.urn = HerrnsteinRL(model.states_signals, learning_rate=1.0, verbose=verbose, name='state_signal_weights')\n        elif role == \"receiver\":\n            self.urn = HerrnsteinRL(model.signals_actions, learning_rate=1.0, verbose=verbose, name='signal_action_weights')\n        else:\n            # consider adding an urn for nature to use for choosing states\n            # this way one could use simple modifcation of the urn class to to support some basic distribution via their urn model.\n            # and we could also visualize the urns and their weights using a simple schematic\n            self.urn = HerrnsteinRL(model.states, learning_rate=0.0, verbose=verbose, name='state_weights')\n        self.messages = []\n        self.actions = []\n        self.reward = 0\n        \n    def step(self):\n        self.messages = []\n        self.actions = []\n\n    def gen_state(self):\n        if self.role == \"nature\":\n            #self.current_state = self.model.random.choice(self.model.states)\n            #use the urn to choose the state\n            self.current_state = self.urn.choose_option(filter='', model=self.model)[0]\n            if self.verbose:\n                print(f\"Nature {self.unique_id} set state {self.current_state}\")\n\n    @property\n    def state(self):\n        if self.role == \"nature\":\n            return self.current_state\n\n    def choose_signal(self, state):\n        if self.role == \"sender\":\n            self.option = self.urn.choose_option(filter=state, model=self.model)\n            self.signal = self.option[0].split('_')[1]\n            \n            if True:#self.verbose:\n                print(f\"Sender {self.unique_id} sends signal: {self.signal}\")\n            return self.signal\n\n    def send_signal(self, state, receiver):\n        if self.role == \"sender\":\n            assert type(state) == str, f\"state must be a string\"\n            assert len(state) > 0, f\"state must be a non-empty string\"\n            assert receiver is not None, f\"receiver must be a valid agent\"\n            assert state in self.model.states, f\"{state=} must be in {self.model.states}\"\n            signal = self.choose_signal(state)\n            receiver.messages.append(signal)\n            if self.verbose:\n                print(f\"Sender {self.unique_id} sends signal: {signal}\")\n\n    def fuse_actions(self, actions):\n        self.action = 0\n        if self.role == \"receiver\":\n            if len(actions) == 1:\n                self.action = actions[0]\n            else:\n                for i in range(len(actions)):\n                    self.action += int(actions[i]) * (2 ** i)\n        return self.action\n\n    def decode_message(self, signal):\n        if self.role == \"receiver\":\n            message = self.urn.choose_option(filter=signal, model=self.model)\n            if self.verbose:\n                print(f\"Receiver {self.unique_id} received signal: {self.message}\")\n            return message\n\n    def set_action(self):\n        '''Receiver decodes each message then \n           then fuses them into one action'''\n           \n        if self.role == \"receiver\":\n            for signal in self.messages:\n                assert type(signal) == str, f\"{signal=} must be a string\"\n                self.actions.append(self.decode_message(signal))\n            action = self.fuse_actions(self.actions)\n            if self.verbose:\n                print(f\"Receiver {self.unique_id} decided on action: {action}\")\n\n    def set_reward(self,reward):\n      if self.role != \"nature\":\n          self.reward = reward\n          if self.verbose:\n              print(f\"agent {self.unique_id} received reward: {self.reward}\")\n\n    def calc_reward(self,state):\n        ''' only reveiver calculates reward'''\n        if self.role == \"receiver\":\n            action = self.action\n            reward = 1.0 if action == state else 0.0\n            self.model.reward = reward\n\n\n\n\nclass SignalingGame(Model):\n  \n  \n    def __init__(self, game_count=2, senders_count=1, receivers_count=1, state_count=3,verbose=False):\n        super().__init__()\n        self.verbose = verbose\n        self.schedule = RandomActivation(self)\n        # states, signals, and actions\n        self.states = [f'{i}' for i in range(state_count)]\n        self.signals = [chr(65 + i) for i in range(state_count)]\n        self.actions = [f'{i}' for i in range(state_count)]\n        # urn options for sender and receiver\n        self.states_signals = [f'{state}_{signal}' for state in self.states for signal in self.signals]\n        self.signals_actions = [f'{signal}_{action}' for signal in self.signals for action in self.actions]\n        \n        \n        self.current_state = None\n        self.games = []\n        self.uid = 0\n        self.senders_count = senders_count\n        self.receivers_count = receivers_count\n        \n        for i in range(game_count):\n            game = {'senders': [], 'receivers': [], 'nature': None}\n            nature = LewisAgent(self.uid, self, game=i, role=\"nature\")\n            game['nature'] = nature\n            self.schedule.add(nature)\n            self.uid += 1\n            for j in range(senders_count):\n                sender = LewisAgent(self.uid, self, game=i, role=\"sender\")\n                game['senders'].append(sender)\n                self.schedule.add(sender)\n                self.uid += 1\n            for j in range(receivers_count):\n                receiver = LewisAgent(self.uid, self, game=i, role=\"receiver\")\n                game['receivers'].append(receiver)\n                self.schedule.add(receiver)\n                self.uid += 1\n            self.games.append(game)\n    \n    def step(self):\n        for agent in self.schedule.agents:\n          agent.step()\n          if agent.role == 'nature':\n                agent.gen_state()\n        \n        for agent in self.schedule.agents:\n          if agent.role == 'sender':\n                state = self.games[agent.game]['nature'].current_state\n                for receiver in self.games[agent.game]['receivers']:\n                    agent.send_signal(state, receiver)\n        \n        for agent in self.schedule.agents:\n          if agent.role == 'receiver':\n                agent.set_action()\n                state = self.games[agent.game]['nature'].current_state\n                agent.calc_reward(state=state)\n                agent.calc_reward(state)\n                \n        for agent in self.schedule.agents:\n          reward = self.games[agent.game]['receivers'][0].reward\n          agent.set_reward(reward)\n        \n        for i, game in enumerate(self.games):\n            print(f'Game {i}, expected_rewards={self.expected_rewards(game)}')\n\n    def expected_rewards(self, game):\n        return 0.25\n\n\n# Running the model\nstate_count = 2  # Number of states, signals, and actions\nsteps = 10\nmodel = SignalingGame(senders_count=1, receivers_count=1, state_count=state_count, game_count=3,verbose=True)\nfor i in range(steps):\n    print(f\"--- Step {i+1} ---\")\n    model.step()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--- Step 1 ---\nSender 1 sends signal: B\nSender 4 sends signal: A\nSender 7 sends signal: A\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n--- Step 2 ---\nSender 1 sends signal: A\nSender 4 sends signal: B\nSender 7 sends signal: B\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n--- Step 3 ---\nSender 1 sends signal: A\nSender 4 sends signal: B\nSender 7 sends signal: B\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n--- Step 4 ---\nSender 1 sends signal: B\nSender 4 sends signal: B\nSender 7 sends signal: B\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n--- Step 5 ---\nSender 1 sends signal: B\nSender 4 sends signal: B\nSender 7 sends signal: A\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n--- Step 6 ---\nSender 1 sends signal: B\nSender 4 sends signal: A\nSender 7 sends signal: A\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n--- Step 7 ---\nSender 1 sends signal: A\nSender 4 sends signal: A\nSender 7 sends signal: A\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n--- Step 8 ---\nSender 1 sends signal: A\nSender 4 sends signal: A\nSender 7 sends signal: B\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n--- Step 9 ---\nSender 1 sends signal: B\nSender 4 sends signal: A\nSender 7 sends signal: B\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n--- Step 10 ---\nSender 1 sends signal: A\nSender 4 sends signal: A\nSender 7 sends signal: A\nGame 0, expected_rewards=0.25\nGame 1, expected_rewards=0.25\nGame 2, expected_rewards=0.25\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/oren/.local/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:\n\nThe AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.\nWe would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919\n\n```\n:::\n:::\n\n\nsome refactoring ideas:\n\n- in the urn class add support a matrix based representation of the weights\n\nlets fix bugs\n- remove unused items from the agent\n- replace the expected_rewards for a game with a code that calculates the expected rewards\n  as follows:\n\n",
    "supporting": [
      "experiment_files"
    ],
    "filters": [],
    "includes": {}
  }
}