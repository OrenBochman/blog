{
  "hash": "8cb9d6726de4744d8efeeca1d1633833",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"emergent communications\"\n---\n\n\n\n\nit seems that we might want to look at the emergent communications \nby considering\n1. a Lewis signaling games to model coordination tasks for a basic communication system\n2. a Shannon game to model the communication of information between agents in which the \n     learn a shared communication protocol potentially using error detection and correction\n     and corection.\n3. a Chomsky game to model development of a shared grammar for complex signals.\n\n## Shannon Game\n\nShanon games are about emergence of randomized communication protocols.\nA randomised communication protocol is a probability distribution over the set of possible\ndeterministic communication protocols.\n\nWe can model any deterministic communication protocol as a pair of decision rees, one for the sender\nand one for the receiver. The sender's decision tree maps each possible message to a signal, and the\nreceiver's decision tree maps each possible signal to a message. \n\n\n\n\n\nmessages that the sender can send. The sender samples a message from this distribution and sends it to the receiver. The receiver then uses a decoding function to map the received message back to the original signal. The goal of the game is for the sender and receiver to coordinate on a communication protocol that maximizes their payoff, which is typically based on the accuracy of message transmission and reception.\nIt is a protocol that uses randomness to encode and decode messages. \nThis randomness can be used to introduce redundancy in the message, which can help in error detection and correction.\n\n::: {#f4dc8e19 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\nclass CommunicationAgent:\n    def __init__(self, num_strategies):\n        self.num_strategies = num_strategies\n        self.q_table = np.zeros((num_strategies, num_strategies))\n        self.learning_rate = 0.1\n        self.discount_factor = 0.9\n        self.epsilon = 0.1\n    \n    def choose_strategy(self):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(self.num_strategies)\n        else:\n            return np.argmax(self.q_table.sum(axis=1))\n    \n    def update_q_values(self, sender_strategy, receiver_strategy, reward):\n        max_future_q = np.max(self.q_table[receiver_strategy])\n        current_q = self.q_table[sender_strategy, receiver_strategy]\n        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_future_q - current_q)\n        self.q_table[sender_strategy, receiver_strategy] = new_q\n\n# Simulation parameters\nnum_strategies = 5\nnum_iterations = 1000\n\n# Initialize agents\nalice = CommunicationAgent(num_strategies)\nbob = CommunicationAgent(num_strategies)\n\nfor _ in range(num_iterations):\n    sender_strategy = alice.choose_strategy()\n    receiver_strategy = bob.choose_strategy()\n    \n    # Simulate message transmission and reception with noise\n    # This is a placeholder for actual encoding/decoding logic\n    success = np.random.rand() < 0.8  # Assume 80% chance of success\n    \n    reward = 1 if success else -1\n    alice.update_q_values(sender_strategy, receiver_strategy, reward)\n    bob.update_q_values(receiver_strategy, sender_strategy, reward)\n\nprint(\"Alice's Q-Table:\\n\", alice.q_table)\nprint(\"Bob's Q-Table:\\n\", bob.q_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAlice's Q-Table:\n [[ 1.1870612   0.          0.          0.1         0.        ]\n [ 1.56945522  1.54181517  0.97338873  0.82764688  1.00508109]\n [ 0.63863811 -0.02636762  0.          0.19736455  0.1607126 ]\n [ 0.56019999  0.          0.          0.          0.        ]\n [ 0.80594973  0.25923685  0.14809569  0.          0.        ]]\nBob's Q-Table:\n [[ 1.50847687  1.20172521  0.6687741   0.64135768  0.55690049]\n [ 0.          0.7344698  -0.0829      0.          0.15625959]\n [ 0.          0.94187947  0.          0.          0.15028825]\n [ 0.15690016  0.91314851  0.22940322  0.          0.        ]\n [ 0.          0.89534698  0.14117867  0.          0.        ]]\n```\n:::\n:::\n\n\nThis example illustrates a basic game-theoretic approach where the sender and receiver iteratively learn better strategies for encoding and decoding messages over a noisy channel. The reinforcement learning framework allows both parties to adapt and improve their protocols, enhancing the reliability of communication over time. This model can be extended and refined to include more sophisticated encoding/decoding techniques and more complex noise models.\n\n::: {#538b0ef5 .cell execution_count=2}\n``` {.python .cell-code}\nfrom mesa import Agent, Model\nfrom mesa.time import RandomActivation\nfrom mesa.datacollection import DataCollector\nimport numpy as np\n\ndef hamming_distance(a, b):\n    return np.sum(a != b) / len(a)\n\nclass Sender(Agent):\n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n        self.protocol = self.random_protocol()\n    \n    def random_protocol(self):\n        # Define a random protocol for encoding\n        return lambda msg: msg  # Identity for simplicity\n    \n    def step(self):\n        message = np.random.randint(0, 2, self.model.message_length)\n        encoded_message = self.protocol(message)\n        self.model.sent_message = encoded_message\n\nclass Receiver(Agent):\n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n        self.protocol = self.random_protocol()\n    \n    def random_protocol(self):\n        # Define a random protocol for decoding\n        return lambda msg: msg  # Identity for simplicity\n    \n    def step(self):\n        noisy_message = self.model.sent_message ^ np.random.binomial(1, self.model.error_rate, self.model.message_length)\n        recovered_message = self.protocol(noisy_message)\n        self.model.recovered_message = recovered_message\n        self.evaluate_performance()\n    \n    def evaluate_performance(self):\n        original_message = self.model.original_message\n        recovered_message = self.model.recovered_message\n        distance = hamming_distance(original_message, recovered_message)\n        self.model.payoff += self.model.recovery_payoff(distance)\n        self.model.payoff += self.model.length_payoff(len(recovered_message))\n        self.model.payoff += self.model.early_recovery_payoff(self.model.current_step)\n    \nclass NoisyChannelModel(Model):\n    def __init__(self, message_length=10, error_rate=0.1, max_steps=100):\n        super().__init__()\n        self.message_length = message_length\n        self.error_rate = error_rate\n        self.current_step = 0\n        self.max_steps = max_steps\n        self.payoff = 0\n        \n        self.schedule = RandomActivation(self)\n        \n        sender = Sender(1, self)\n        receiver = Receiver(2, self)\n        self.schedule.add(sender)\n        self.schedule.add(receiver)\n        \n        self.original_message = np.random.randint(0, 2, self.message_length)\n        self.sent_message = None\n        self.recovered_message = None\n        \n        self.datacollector = DataCollector(\n            model_reporters={\"Payoff\": \"payoff\"}\n        )\n    \n    def recovery_payoff(self, distance):\n        return 1 - distance\n    \n    def length_payoff(self, length):\n        return 1 / length\n    \n    def early_recovery_payoff(self, step):\n        return (self.max_steps - step) / self.max_steps\n    \n    def step(self):\n        self.current_step += 1\n        self.schedule.step()\n        self.datacollector.collect(self)\n        if self.current_step >= self.max_steps:\n            self.running = False\n\n# Example of running the model\nmodel = NoisyChannelModel()\nwhile model.running:\n    model.step()\n\n# Retrieve results\nresults = model.datacollector.get_model_vars_dataframe()\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Payoff\n0     1.39\n1     2.97\n2     4.54\n3     6.10\n4     7.55\n..     ...\n95  105.44\n96  106.07\n97  106.89\n98  107.40\n99  107.90\n\n[100 rows x 1 columns]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/oren/.local/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:\n\nThe AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.\nWe would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919\n\n```\n:::\n:::\n\n\nso this is a variant that uses a noisy channel model to simulate the transmission of messages between a sender and receiver. The agents have protocols for encoding and decoding messages, and the model tracks the performance of the communication system based on the accuracy of message recovery, message length, and early recovery. This example demonstrates how to model and analyze the performance of communication systems in the presence of noise and other challenges.\n\nWhat we don't have is a way to pick different protocols or to improve them over time. \n\nI would break this down into a few steps:\n1. identify the environmental factors that would encourage the agents to evolve\n   diverse and efficient transmission protocols.\n   a. noisy channels\n   b. limited bandwidth\n   c. limited computational resources\n   d. time constraints\n   e. risks of predation.\n   \n2. allow agents randomly generate candidate protocols and evaluate their performance.\n\n\n\ndef random_protocol():\n    # Define a random protocol for encoding/decoding\n    return lambda msg: np.random.randint(0, 2, len(msg))\n\n# which  would be used as follows\n\nclass Sender(Agent):\n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n        self.protocol = random_protocol()\n    \n    def step(self):\n        message = np.random.randint(0, 2, self.model.message_length)\n        encoded_message = self.protocol(message)\n        self.model.sent_message = encoded_message\n\n\n\nThis could be done by introducing reinforcement learning techniques to allow the agents to adapt and learn better encoding/decoding strategies based on feedback from the environment. This would enable the agents to optimize their protocols for improved communication performance in noisy channels.\n\n",
    "supporting": [
      "shanon-game_files"
    ],
    "filters": [],
    "includes": {}
  }
}