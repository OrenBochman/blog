{
  "hash": "7245ca913c55e3687d5cd87cfdfd4eec",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-05-01\ntitle: \"Skryms Signals Summary and Models\"\nsubtitle: \"learing language games\"\nkeywords: [game theory, signaling games, partial pooling, evolution, reinforcement learning, signaling systems, emergent languages, evolution of language]\nexecute:\n  error: true\n---\n\n\n\n\n\n\n1. [] TODO: fix mesa related errors\n\nIn [@Skyrms2010signals] philosopher and mathematician Brian Skyrms discusses how one can extend the concept of a signaling games into a full fledged signaling systems and to some extent a rudimentary language.\n\nI like many other found Signals to be a fascinating little book worth reading at least a couple of times. While Skyrms starts with a basic exposition motivated by Greek philosophers he eventually makes a deep dive into areas like reinforcement learning, replicator dynamics, mean field games and some other deep mathematical fields without much of introduction. In places the monographs seems incomplete and may require hunting the papers in the bibliography and possibly more recent work by the same authors.\n\nI slowly noticed it being cited in more and more papers which I read. This sort of indicated that intellectually more people we on the same path of thinking how to equip their problem solving with a signaling system or better yet to evolve a more sophisticated language.\\\n\nI went back several times to review the chapter on Complex signals, which I feel is the most interesting for real-world application. I began to think that the Lewis games are too rudimentary since signaling systems that evolve/learned from them are basically n-k maps of signals to meaning.\n\nWhat I wanted was a recipe for quickly agent that need to evolve and teach/learn a language for efficient communication.\n\nI wanted to go the relevant papers he covers on this area and then to see of there were newer results he did not cover. This turned out to be a bit of a challenge. In the mean time I also learned some courses on RL and even tried a couple of ideas from this book at work. I think I should summarize at least some of the more interesting results from the book.\n\nBesides a summary I also want to try to implement some of the keystone models in the book to see if I can derive the reductionist simple language learning game.\n\n## 1. Signals\n\n### Big Research Questions\n\n**Q1. How can interacting individuals spontaneously learn to signal?**\n\n**Q2. How can species spontaneously evolve signaling systems?**\n\n## Sender-Receiver\n\n> There are two players, the sender and the receiver.\\\n> Nature chooses a state at random and the sender observes the state chosen.\\\n> The sender then sends a signal to the receiver, who cannot observe the state directly but does observe the signal.\\\n> The receiver then chooses an act, the outcome of which affects them both, with the payoff depending on the state.\\\n> Both have pure common interest‚Äîthey get the same payoff‚Äîand there is exactly one ‚Äúcorrect‚Äù act for each state.\\\n> In the correct act-state combination they both get positive payoff; otherwise payoff is zero.\\\n> The simplest case is one where there are the same number of states, acts, and signals.\n\nA separating equilibrium is called a signaling system\n\n> If we start with a pair of sender and receiver strategies, and switch the messages around the same way in both, we get the same payoffs. In particular, permutation of messages takes one signaling-system equilibrium into another.\n\nWe can understand a signaling system as a encoding look-up table by the sender and a decoding lookup table for the reciever which is the inverse of the first. The product of two permutations is the identity matrix. Each permutation of the identity matrix gives a valid signaling system\n\n**Q3. Is there a most salient signaling system?**\n\nSalience is a concept from Schelling's Game theory that suggest that one solution to a coordination problem might be naturally better then others. (e.g. meeting a relative at the airport). This can be due to an externality to the pure coordination problem. Salience can also arise from non uniformity of the state distribution - by providing less frequent messages longer messages based on binary coding. The salience hierarchy might be grounded in risk - more urgent messages might be shorter and learned before the longer ones.\n\nmy thoughts on Salience:\n\n- Salience would arise in nature through the non-uniform distribution of states \n  which is ignored in most papers leading to equally salient signaling system. \n  When the states are not uniformly distributed then the signals will not be \n  uniformly distributed. The more common states should have more common signals. \n  e.g. if snakes are more common than eagles then the signal for snake should be \n  shorter/simpler/learned first than the signal for eagle. In another location\n  the distributions could be reversed leading to a different salience hierarchy.\n- Another way (of seeing this is that) salience would arise in nature to minimize\n  risks for the sender, who could become a target for a predator by sending a signal.\n- Two other source of salience are the risk of making mistakes and the cost of \n  sending a signal.\n- Finally there is nothing stopping the salience from being a function of all these\n  factors through a product of their probabilities. Though this is more easily\n  expressed in the language of fitness. Salience will select the language whose\n  speakers gain the highest expected progeny (fitness) by avoiding risks, conserving\n  energy and avoiding miscommunication for their habitat.\n- If the speakers migrate they might benefit from a language that is salient in \n  multiple habitats. This is a form of generalization.\n- If there are different cost for encoding and decoding then the salience will be \n  a function of the product of the encoding and decoding costs. This is a form of \n  cost minimization. In this scenario there may be a competition between the sender and\n  the receiver to minimize their costs. But the sender has the upper hand since the\n  sender chooses the signal. The sender is the causal agent in the signaling system.\n  \n\n**Q4. How can two agents with different signaling find a SS that is midway between them (including systems with both shared and unique states)?**\n\n-   Its fairly clear that under the rules of the Lewis game all valid signaling systems are isomorphic and none are more salient.\n-   In nature salience might arise and a systems leading to greatest fitness in its users would be the most salient.\n-   To find a signaling system that is midway between two signaling systems we could use the Cayley distance between the two permutations. This is the minimum number of transpositions required to transform one permutation into another. The median permutation would be the one that has half the Cayley distance to each signaling systems.\n- If the systems have salience we may want to also keep the most salient signals intact and now we have a more complex optimization problem. We could use the KL divergence between the two signaling systems to estimate the distance of the signaling distribution from a separating distribution.\n\nthe Cayley distance between two permutations is the minimum number of transpositions required to transform one permutation into another. it is a metric on the symmetric group.\n\n**Information in signals**\n\n**Q5. How can we minimally extend this framework to handle Errors and Deception**\n\n> Signals carry information. The natural way to measure the information in a signal is to measure the extent that the use of that particular signal changes probabilities. Accordingly, there are two kinds of information in the signals in Lewis sender-receiver games: information about what state the sender has observed and information about what act the receiver will take. The Ô¨Årst kind of infor- mation measures effectiveness of the sender‚Äôs use of signals to discriminate states; the second kind measures the effectiveness of the signal in changing the receiver‚Äôs probabilities of action.\n\n- [ ] TODO: estimate information content of each signal for sender and receiver for separating and partial pooling cases\n- [ ] TODO: use entropy for message level estimates of sender and receiver under separating signal, a synonym, a homonym.\n- [ ] TODO: use entropy KL divergence to estimate a the distance of the signaling distribution from a separating distribution.\n\nActually there are a number of extensions one would like to consider for the Lewis framework:\n\n1.  bottlenecks\n    1.  more state than signals - this is the interesting case and where complex signaling systems should arise\n    2.  more signals than states - this is the case where synonyms can arise\n2.  basic logical reasoning, conjunctions, disjunctions, negations\n3.  multiple senders and or receivers\n    1.  rewarding coordination (each state requires different actions from the agents - they are learning different receiver maps )\n    2.  rewarding correlated equilibrium (sender lets the receivers pick from correlated states at random allowing the receivers avoid penalty of mis-coordination.)\n    3. networks of agents per the Goyal model in ch 11 and 13\n\ncomplex signals\n\n1.  conjunction of signals,\n2.  ordered signals,\n3.  recursive signals, group\n\n### Evolution\n\nWe first see two competing Signaling systems being tested in a population\n\n[@hofbauer1998evolutionary] Population dynamics - can be used to identify which dynamic equilibria are stable or unstable given an initial population of strategies\n\nThere is a figure showing the field dynamics with basins of attractions arising from the population dynamics equations\n\nWe also see symmetry breaking selecting a signaling system to a system\n\n$$\n\\frac{dp(A)}{dt}=p(A)[U(A)-U]\n$$\n\nwhere\n\n-   U(A) is the average payoff to strategy A and\n-   U is the average payoff in the population.\n\n::: {#445af87d .cell execution_count=2}\n``` {.python .cell-code}\nfrom pylab import *\n\nalpha, beta = 1, 1\nxvalues, yvalues = meshgrid(arange(0, 2.1, 0.1), arange(0, 2.1, 0.1))\nxdot = xvalues * alpha - beta\nydot = yvalues * alpha - beta\nstreamplot(xvalues, yvalues, xdot, ydot)\nshow()\n```\n\n::: {.cell-output .cell-output-display}\n![](signals-summary_files/figure-html/cell-2-output-1.png){width=594 height=416}\n:::\n:::\n\n\nwe have a discussion of how signals might arise.\n\n## Evolution\n\n::: {#25e0456e .cell execution_count=3}\n``` {.python .cell-code}\nimport itertools\nimport functools\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation, RandomActivation\nfrom mesa.datacollection import DataCollector\nimport matplotlib.pyplot as plt\n\n# agent_roles\nr_nature = 'nature'\nr_sender = 'sender'\nr_receiver = 'receiver'\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCould not import SolaraViz. If you need it, install with 'pip install --pre mesa[viz]'\n```\n:::\n:::\n\n\n## Lewis Signaling Game Model\n\nThe Lewis signaling game is a model of communication between two agents, a sender and a receiver.\nNature picks a state, the sender observes the state, chooses a signal, and sends the signal to the receiver who then takes an action based on the signal.\nIf the action of the receiver is a match with the state obseved by the sender, agents get a reward of 1, otherwise, they get a reward of 0.\nstate, the sender and receiver get a reward of 1, otherwise, they get a reward of 0.\nis a match with the state, the sender and receiver get a reward of 1, otherwise, they get a reward of 0.\n\n::: {#1906dfd6 .cell execution_count=4}\n``` {.python .cell-code}\nclass HerrnsteinRL():\n    '''\n                                    The Urn model\n     nature            sender                 reciever     reward\n                       \n    | (0) | --{0}-->  | (0_a)  | --{a}--> | (a_0) | --{0}-->   1   \n    |     |           | (0_b)  | --{b}    | (a_1) | --{1}-->   0\n    |     |           +--------+    | +-->+-------+\n    |     |                         +-|-+  \n    | (1) | --{1}-->  | (1_a)  | --{a}+ +>| (b_0) | --{1}-->   1\n    |     |           | (1_b)  | --{b}--->| (b_1) | --{0}-->   0\n    +-----+           +--------+          +-------+\n    \n    \n    Herrnstein Urn algorithm\n    ------------------------\n    \n    1. nature picks a state \n    2. sender gets the state, chooses a signal by picking a ball in choose_option() from the stat'es urn\n    3. reciver gets the action, chooses an actuion by picking a ball in choose_option()\n    4. the balls in the urns are incremented if action == state\n    5. repeat\n    \n    '''\n    def __init__(self, options, learning_rate=1.0,verbose=False,name='Herrnstein matching law', balls=None):\n        \n        # filter options in choose option by input\n        self.verbose = verbose\n        self.name=name\n        self.learning_rate = learning_rate\n        self.options = options\n        if balls is not None:\n          self.balls = balls\n        else:\n          self.balls = {option: 1.0 for option in self.options}\n        if self.verbose:\n          print(f'LearningRule.__init__(Options: {options})')\n    \n    def get_filtered_urn(self, filter):\n      ''' filters urn's options by prefix and normalizes the weights\n          usege:\n          urn=urn.get_filtered_urn(1)\n          choice = model.random.choice(list(urn.keys()), p=list(urn.values()))\n      '''\n      assert type(filter) == int, f\"filter must be a int\"\n      filtered_options = [key for key in self.balls.keys() if key[0] == filter]\n      if not filtered_options:\n        raise ValueError(f\"No options found with filter {filter}\")\n      if self.verbose:\n        print(f\"in get_filtered_urn({filter=}) --- filtered_options: {filtered_options=}\")\n      filtered_balls = {opt: self.balls[opt] for opt in filtered_options}\n      if self.verbose:\n        print(f\"in get_filtered_urn({filter=}) --- filtered_balls: {filtered_balls=}\")\n      total = functools.reduce(lambda a,b: a+b, filtered_balls.values())\n      #total = sum(filtered_balls.values())\n      if self.verbose:\n        print(f\"in get_filtered_urn({filter=}) --- total: {total=}\")\n      assert total > 0.0, f\"total weights is {total=} after {filter=} on {self.balls}\"      \n      normalized_balls = {option: weight / total for option, weight in filtered_balls.items()}\n      if self.verbose:\n        print(f\"in get_filtered_urn({filter=}) --- returning : {normalized_balls=}\")\n      return normalized_balls\n     \n    def choose_option(self,filter,random):\n        ''' chooses an option from the urn based on the filter and the random choice\n            \n            usage:\n            urn.choose_option(filter=1,random=model.random)\n        '''\n       \n        urn = self.get_filtered_urn(filter)\n        if random:\n          options = random.choices(list(urn.keys()), weights=list(urn.values()),k=1)\n          option = options[0]\n          \n          if self.verbose:\n            print(f'in HerrnsteinRL.choose_option({filter=}) --- chose {option=} from {urn=}')\n\n          return option\n        else:\n          raise Exception(f\"random must be a random number generator\")\n        \n    def update_weights(self, option, reward):\n        old_balls = self.balls[option]\n        self.balls[option] += self.learning_rate * reward \n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_balls} -> {self.balls[option]}\")\n```\n:::\n\n\n::: {#997796d3 .cell execution_count=5}\n``` {.python .cell-code}\nclass LewisAgent(Agent):\n  \n    def __init__(self, unique_id, model, game, role, verbose=False):\n        super().__init__(unique_id, model)\n        self.role = role #( one of nature, sender, receiver)\n        self.verbose = verbose\n        self.game = game\n        self.messages = []\n        self.actions = []\n        if role == \"sender\":\n          self.urn = HerrnsteinRL(model.states_signals, learning_rate=1.0,verbose=verbose,name='state_signal_weights')\n        elif role == \"receiver\":\n          self.urn = HerrnsteinRL(model.signals_actions, learning_rate=1.0,verbose=verbose,name='signal_action_weights')\n        else:\n          self.urn = None\n        \n    def step(self):\n      # reset agent state before step\n      self.messages = []\n      self.actions = []\n\n    def gen_state(self)-> None:\n        if self.role == r_nature:\n          self.current_state = model.random.choice(self.model.states)\n          if self.verbose:\n                print(f\"Nature {self.unique_id} set state {self.current_state}\")\n                \n    @property\n    def state(self):\n        if self.role == r_nature:\n          return self.current_state\n\n    def choose_signal(self, filter):\n        # sanity checks for filter\n        assert type(filter) == int, f\"filter must be a int\"\n        assert filter in model.states, f\"filter must be a valid state\"\n        \n        \n        if self.role != r_sender:\n          throw(f\"Only sender can send signals\")\n        self.option = self.urn.choose_option(filter=filter,random=self.model.random)\n        signal = self.option[1] # the prefix is the urn context we want the suffix\n        assert type(signal) == int, f\"signal {signal=} must be a int\"\n        self.signal = signal\n        if self.verbose:\n              print(f\"Sender {self.unique_id} got filter {filter} choose option: {self.option} and signaled: {self.signal}\")\n        return self.signal\n          \n\n    def send_signal(self, filter, receiver):\n        ''' \n            # Message sending logic:\n            1. sender chooses a signal based on the state\n            2. sender sends the signal to the receiver\n        '''\n        if self.role != r_sender:\n          raise Exception(f\"Only sender can send signals\")\n         \n        assert type(filter) == int, f\"filter must be a int\"\n        assert filter in model.states, f\"filter must be a valid state\"\n        signal = self.choose_signal(filter=filter)\n        assert signal is not None, f\"signal must be a valid signal\"\n        if self.verbose:\n          print(f\"Sender {self.unique_id} chose signal: {signal}\")\n        receiver.messages.append(signal)\n        if self.verbose:\n          print(f\"Sender {self.unique_id} sends signal: {signal} to receiver {receiver.unique_id}\")\n\n    def fuse_actions(self,actions):\n        ''' \n            # Message fusion logic:\n            1. single message:  if there is only one signal then the action is the action associated with the signal\n            2. ordered messages: if there are multiple signals then the action is the number from the string assocciated with the concatenated signal\n               if there are two signals possible per message we concat and covert binary string to number\n            3. is the messages are sets we could perform a intersetion and take the action associated with the intersection \n               currently this is not implemented\n            4. support for recursive signals is currently under research .\n        ''' \n        if self.role != r_receiver:\n          raise Exception(f\"Only receiver can set actions\")\n        \n        if len(actions) == 1: # single action no need to fuse\n          return actions[0]\n        else:\n          # fuse the actions into a binary number\n          action = 0\n          # if there are multiple signals\n          for i in range(len(actions)):\n            action += actions[i]*(2**i)\n          if self.verbose:\n              print(f\"Receiver {self.unique_id} fused actions : {self.actions} into action: {action}\")\n          return action\n\n    def decode_message(self,signal):\n        ''' first we need to get the filtered urn for the signal\n            and then choose the option based on the urn'''\n        if self.role != r_receiver:\n          raise Exception(f\"Only receiver can decode messages\")\n        option = self.urn.choose_option(filter=signal,random=self.model.random)\n        action = option[1]\n        if self.verbose:\n              print(f\"in decode_message({signal=}) Receiver {self.unique_id} got option: {option} and decoded action: {action}\")\n        return action\n\n    def set_action(self):\n        ''' first we need to use the urn to decode the signals \n            then need to fuse them to get the action '''\n        if self.role != r_receiver:\n          raise Exception(f\"Only receiver can set the action\")\n        self.actions = []\n        for signal in self.messages:\n          self.actions.append(self.decode_message(signal))          \n        self.action = self.fuse_actions(self.actions)\n        # which option to reinforce \n        self.option = (self.messages[0],self.action)\n        if self.verbose:\n              print(f\"Receiver {self.unique_id} received signals: {self.messages} and action: {self.action}\")\n              \n    def set_reward(self,reward):\n        if self.role not in [r_receiver,r_sender]:\n          raise Exception(f\"Only sender and receiver can set rewards\")\n        self.reward = reward\n        if self.verbose:\n            print(f\"Receiver {self.unique_id} received reward: {self.reward}\")\n                \n    def calc_reward(self,correct_action):\n        if self.role != r_receiver:\n          raise Exception(f\"Only receiver can calculate rewards\")\n        self.reward = 1 if self.action == correct_action else 0        \n\nclass SignalingGame(Model):\n  \n    # TODO: add support for \n    # 1. bottle necks\n    # 2. rename k to state_count\n    # 3. state_per_sender = state_count/sender_count \n    # 2. partitioning states by signals => state/sender_count\n\n    def __init__(self, game_count=2, senders_count=1, recievers_count=1, state_count=3,signal_count=3,verbose=True):\n        super().__init__()\n        self.verbose = verbose\n        self.step_counter = 0\n        self.schedule = RandomActivation(self)\n        \n        \n        # Define the states, signals, and actions\n        self.states   = [i for i in range(state_count)]\n        print(f'{self.states=}')\n        self.signals  = [i for i in range(signal_count)]\n        print(f'{self.signals=}')\n        self.actions  = [i for i in range(state_count)]\n        print(f'{self.actions=}')\n        \n        # e.g., 1 -> 1, 2 -> 2, ...\n        self.states_signals =  [(state,signal) for state in self.states for signal in self.signals]\n        print(f'{self.states_signals=}')\n        self.signals_actions = [(signal,action) for signal in self.signals for action in self.actions] \n        print(f'{self.signals_actions=}')\n        \n        # Agents\n\n        self.uid=0\n        self.senders_count=senders_count\n        self.recievers_count=recievers_count\n\n        # Games each game has a nature, senders and receivers\n        self.games = []\n        # Create games        \n        for i in range(game_count):\n            game = {\n              r_nature: None,\n              r_sender: [],\n              r_receiver: []\n            }\n            \n            # create nature agent\n            game[r_nature] = LewisAgent(self.uid, self, game=i,role = r_nature,verbose=self.verbose)\n            self.schedule.add(game[r_nature])\n            self.uid += 1\n            \n            # create sender agents\n            for j in range(senders_count):\n                sender = LewisAgent(self.uid, self, game=i,role = r_sender,verbose=self.verbose)\n                game[r_sender].append(sender)\n                self.schedule.add(sender)\n                self.uid +=1\n                \n            # create receiver agents\n            for k in range (recievers_count):\n                reciever = LewisAgent(self.uid, self, game=i,role = r_receiver,verbose=self.verbose)\n                game[r_receiver].append(reciever)\n                self.schedule.add(reciever)\n                self.uid +=1\n                \n            self.games.append(game)\n\n            self.total_reward = 0\n        \n\n        # Define what data to collect\n        self.datacollector = DataCollector(\n            model_reporters={\"TotalReward\": lambda m: m.total_reward},  # A function to call \n            agent_reporters={\"Reward\": \"reward\"}  # An agent attribute\n        )\n\n    def compute_total_reward(self,model):\n        return \n        \n    def step(self):\n      \n        for agent in model.schedule.agents:\n            # reset agent state before step\n            agent.step()\n            \n        for game_counter, game in enumerate(self.games):\n            if self.verbose:\n                print(f\"--- Step {model.step_counter} Game {game_counter} ---\")\n            nature = game[r_nature]\n            nature.gen_state()\n            state = nature.current_state\n            assert type(state) == int, f\"state must be a int\"\n            assert state in model.states, f\"state must be a valid state\"\n            if self.verbose:\n                print(f\"in model.step() --- game {game_counter} --- Nature {agent.unique_id} set state {state} in game {game_counter}\")\n            for sender in game[r_sender]:\n                for receiver in game[r_receiver]:                    \n                    sender.send_signal(filter = state, receiver=receiver)\n            for receiver in game[r_receiver]:\n                assert receiver.role == r_receiver, f\"receiver role must be receiver not {receiver.role}\"\n                receiver.set_action()\n                if self.verbose:\n                    print(f\"in model.step() --- game {game_counter} --- Receiver {receiver.unique_id} action: {receiver.action}\")\n                receiver.calc_reward(correct_action=state)\n                reward = receiver.reward\n                assert type(reward) == int, f\"reward must be a int not {type(reward)}\"\n                assert reward in [0,1], f\"reward must be 0 or 1 not {reward}\"\n                print(f\"in model.step() --- game {game_counter} --- Receiver {receiver.unique_id} received reward: {receiver.reward}\")\n            \n            for agent in itertools.chain(game[r_sender],game[r_receiver]):\n                agent.set_reward(reward)\n                if self.verbose:\n                    print(f\"in model.step() --- game {game_counter} --- Sender {agent.unique_id} received reward: {reward}\")\n                agent.urn.update_weights(agent.option, reward)\n\n            #print(f'in model.step() --- game {game_counter}, {self.expected_rewards(game)=}')\n                    # Collect data\n        \n        self.total_reward += sum(agent.reward for agent in self.schedule.agents if agent.role == r_receiver)\n\n        self.datacollector.collect(self)\n\n\n    def expected_rewards(self,game):\n      return 0.25\n\n    def run_model(self, steps):\n\n        \"\"\"Run the model until the end condition is reached. Overload as\n        needed.\n        \"\"\"\n        while self.running:\n            self.step()\n            steps -= 1\n            if steps == 0:\n                self.running = False\n```\n:::\n\n\n::: {#1a1d6728 .cell execution_count=6}\n``` {.python .cell-code}\n# Running the model\nstate_count= 3  # Number of states, signals, and actions\nsignal_count= 3\nsteps = 1000\n\nmodel = SignalingGame(senders_count=1,recievers_count=1,state_count=state_count,signal_count=signal_count,verbose=True,game_count=2)\nmodel.run_model(steps)  # Run the model for the desired number of steps\n\n# Get the reward data\nreward_data = model.datacollector.get_model_vars_dataframe()\n\n# Plot the data\nplt.figure(figsize=(10, 8))\nplt.plot(reward_data['TotalReward'])\nplt.xlabel('Step')\nplt.ylabel('Total Reward')\nplt.title('Total Reward over Time')\nplt.grid(True)  # Add gridlines\nplt.xlim(left=0)  # Start x-axis from 0\nplt.ylim(bottom=0,top=1000)  # Start y-axis from 0\nplt.show()     \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nself.states=[0, 1, 2]\nself.signals=[0, 1, 2]\nself.actions=[0, 1, 2]\nself.states_signals=[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\nself.signals_actions=[(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_111005/3700252942.py:143: DeprecationWarning:\n\nThe time module and all its Schedulers are deprecated and will be removed in Mesa 3.1. They can be replaced with AgentSet functionality. See the migration guide for details. https://mesa.readthedocs.io/latest/migration_guide.html#time-and-schedulers\n\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[5], line 6</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> signal_count<span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(98,98,98)\">3</span>\n<span class=\"ansi-green-fg ansi-bold\">      4</span> steps <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(98,98,98)\">1000</span>\n<span class=\"ansi-green-fg\">----&gt; 6</span> model <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">SignalingGame</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">senders_count</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">1</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\">recievers_count</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">1</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\">state_count</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">state_count</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\">signal_count</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">signal_count</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\">verbose</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">True</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\">game_count</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">2</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      7</span> model<span style=\"color:rgb(98,98,98)\">.</span>run_model(steps)  <span style=\"font-style:italic;color:rgb(95,135,135)\"># Run the model for the desired number of steps</span>\n<span class=\"ansi-green-fg ansi-bold\">      9</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Get the reward data</span>\n\nCell <span class=\"ansi-green-fg\">In[4], line 177</span>, in <span class=\"ansi-cyan-fg\">SignalingGame.__init__</span><span class=\"ansi-blue-fg\">(self, game_count, senders_count, recievers_count, state_count, signal_count, verbose)</span>\n<span class=\"ansi-green-fg ansi-bold\">    170</span> game <span style=\"color:rgb(98,98,98)\">=</span> {\n<span class=\"ansi-green-fg ansi-bold\">    171</span>   r_nature: <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>,\n<span class=\"ansi-green-fg ansi-bold\">    172</span>   r_sender: [],\n<span class=\"ansi-green-fg ansi-bold\">    173</span>   r_receiver: []\n<span class=\"ansi-green-fg ansi-bold\">    174</span> }\n<span class=\"ansi-green-fg ansi-bold\">    176</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># create nature agent</span>\n<span class=\"ansi-green-fg\">--&gt; 177</span> game[r_nature] <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">LewisAgent</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">uid</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">game</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">i</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\">role</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">r_nature</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\">verbose</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">verbose</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    178</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>schedule<span style=\"color:rgb(98,98,98)\">.</span>add(game[r_nature])\n<span class=\"ansi-green-fg ansi-bold\">    179</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>uid <span style=\"color:rgb(98,98,98)\">+</span><span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(98,98,98)\">1</span>\n\nCell <span class=\"ansi-green-fg\">In[4], line 4</span>, in <span class=\"ansi-cyan-fg\">LewisAgent.__init__</span><span class=\"ansi-blue-fg\">(self, unique_id, model, game, role, verbose)</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">__init__</span>(<span style=\"color:rgb(0,135,0)\">self</span>, unique_id, model, game, role, verbose<span style=\"color:rgb(98,98,98)\">=</span><span style=\"font-weight:bold;color:rgb(0,135,0)\">False</span>):\n<span class=\"ansi-green-fg\">----&gt; 4</span>     <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">super</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span style=\"color:rgb(0,0,255)\" class=\"ansi-yellow-bg\">__init__</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">unique_id</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">model</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      5</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>role <span style=\"color:rgb(98,98,98)\">=</span> role <span style=\"font-style:italic;color:rgb(95,135,135)\">#( one of nature, sender, receiver)</span>\n<span class=\"ansi-green-fg ansi-bold\">      6</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>verbose <span style=\"color:rgb(98,98,98)\">=</span> verbose\n\nFile <span class=\"ansi-green-fg\">~/work/blog/.venv/lib/python3.10/site-packages/mesa/agent.py:64</span>, in <span class=\"ansi-cyan-fg\">Agent.__init__</span><span class=\"ansi-blue-fg\">(self, model, *args, **kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">     51</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">__init__</span>(<span style=\"color:rgb(0,135,0)\">self</span>, model: Model, <span style=\"color:rgb(98,98,98)\">*</span>args, <span style=\"color:rgb(98,98,98)\">*</span><span style=\"color:rgb(98,98,98)\">*</span>kwargs) <span style=\"color:rgb(98,98,98)\">-</span><span style=\"color:rgb(98,98,98)\">&gt;</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>:\n<span class=\"ansi-green-fg ansi-bold\">     52</span> <span style=\"color:rgb(188,188,188)\">    </span><span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Create a new agent.</span>\n<span class=\"ansi-green-fg ansi-bold\">     53</span> \n<span class=\"ansi-green-fg ansi-bold\">     54</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    Args:</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     62</span> \n<span class=\"ansi-green-fg ansi-bold\">     63</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    \"\"\"</span>\n<span class=\"ansi-green-fg\">---&gt; 64</span>     <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">super</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span style=\"color:rgb(0,0,255)\" class=\"ansi-yellow-bg\">__init__</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     66</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>model: Model <span style=\"color:rgb(98,98,98)\">=</span> model\n<span class=\"ansi-green-fg ansi-bold\">     67</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>unique_id: <span style=\"color:rgb(0,135,0)\">int</span> <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">next</span>(<span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_ids[model])\n\n<span class=\"ansi-red-fg\">TypeError</span>: object.__init__() takes exactly one argument (the instance to initialize)</pre>\n```\n:::\n\n:::\n:::\n\n\nIn this simulation the agents are not learning - they are accessing the predefined signals and actions in the model hence rewards are always 1.\n\nPlayer in Lewis signaling games can reach three type of equilibria\n\n1.  Separating equilibrium in which receiver fully recovers the state from the signal and can take the appropriate action\n2.  Partial pooling equilibrium in which *synonyms* or *homophones* frustrate the receiver for always recovering the state.\n3.  Full pooling equilibrium in which all signals are the same and the agents are unable to communicate.\n\nA one word synonym for \"desired qualities\" derived from desire that used in academic literature is \"desiderata\".\n\nSkryms next considers bottle necks - which are cases where there are more signals than actions and vica versa.\n\n-   In the case of more signals than actions successful learning will result a partial polling equilibrium with some synonyms.\n-   In the case of more actions than signals the best an agent can learn is a partial pooling equilibrium with homophones.\n\nBoth synonyms and homophones have drawbacks however:\n\nWhile synonyms increase the cognitive load and the number of signals that need to be learned they do not prevent the recovery of the state being communicated. Homophones require the receiver to select an interpretation at random leading to lower payoffs since the receiver unable to recover the state cannot select the correct action. If the number of signal is the same as the number of actions, the pigeon hole principle guarantees that for every synonym there must be a homophone.\n\nIf we consider that for recoverability we need action and signals to be fully correlated it is easy to see that each failure to correlate\n\naction to signals results in a (partial) pooling solution. Thus there are far more partial pooling equilibria than separating equilibria. and it is thus no surprise that natural language is rife with homophones and synonyms.\n\nIn lieu of the fact that partial pooling equilibrium far out number the separating ones with and with out bottlenecks, setting up and later learning a separating signaling system with minimal homophones/synonyms is not trivial task. (If we also factor in cost/risk of miscommunication some homophones are clearly worse than others)\n\n-   Evolution for example may not be the best way for this.\n\n-   While researchers have very basic algorithms to do so, in terms of convergence rate and sample efficiency.\n\nAlthough not considered it is easy to see that there are far more partial pooling\n\nWe can conclude proceed to discuss the desiderata for learning algorithms.\n\nNote: Dropout Algorithm Introducing bottlenecks into neural networks tend to improve their ability to generalize by forcing them to avoid memorizing inputs and come up with more resiliant representations. This suggest that partial pooling equilibria may play a more significant role in structured/complex signaling systems.\n\n## Desiderata for learning algorithms of signaling systems\n\n1.  State recovery - we prefer the algorithm to learn a separating equilibrium and if avoid pooling equilibrium with homophones.\n2.  Convergence - we want the algorithm to quickly converge to the equilibrium.\n3.  Sample efficiency - we want the algorithm to learn after minimal exposure to stimuli.\n\nSome questions\n\n-   How different are the task of creating the signaling system from learning it?\n\n    -   the main difference perhaps is that one party has a mapping and it is up to the second to learn it. they can't find unused symbols and mach them to a new state.\n\n    -   there may be many speakers so making changes will be costly.\n\n-   Can switching roles of sender and receiver give better outcomes in learning ?\n\n    -   this may change for different extensions\n\n-   If there are multiple agent learning can create or learn the signaling system better or faster\n\n    -   what if they have groups with established signal systems\n\n    -   how can they find a new set of mapping with minimal permutation from their original\n\n-   If states used for reward are not random are there better schedules for learning are not random\n\nWhat if each has knowledge of a working signaling system already help adding more players seem to\n\n# 4 Evolution\n\nThe three essential factors in Darwin‚Äôs account are\n\n1.  natural variation - mutation, gene flow via migration, genetic drift and recombination in sexual reproduction.\n2.  differential reproduction - [@Taylor1978ESS] replicator dynamics\n3.  inheritance\n\n### ESS\n\nIn [@Smith1973LogicAnimalConflict] the authors introduced a novel solution concept - the ESS or Evolutionary stable strategy, improving on the notion of the Nash equilibrium by replacing agent level play dominance with statistical dominance of strategies.\n\n::: {#ex-ess-hak-dove}\n## ESS Motivating Example Hawk Dove Game\n\n|          | Hawk | Dove |\n|----------|------|------|\n| **Hawk** | 0    | 3    |\n| **Dove** | 1    | 2    |\n\n: Hawk Dove Game\n\nThis explains why hyper-aggressive Hawks type who can defeat more peaceful Doves type do not wipe them out. Hawks have an advantage if there are mostly doves. Once they are in a majority Hawk-Hawk interaction lead to serious injury and death. ESS is a frequency dependent equilibrium.\n:::\n\n## ESS Criteria\n\nIn [@Smith1973LogicAnimalConflict] the authors introduce the following criteria in terms of payoffs for a strategy to be an ESS.\n\nA strategy, S, is evolutionary stable if for any other strategy, M, either:\n\n1.  Fitness (S played against S) \\> Fitness (M played against S) or:\n2.  Fitnesses are equal against S, but Fitness(S against M) \\> Fitness(M against M)\n\nWhere under the first mutants are expelled quickly and under 2 less so.\n\n## Differential Reproduction - Replicator dynamics\n\nReplicator dynamics is driven by Darwinian Ô¨Åtness‚Äîexpected number of progeny.\n\nso $fitness \\sim \\mathbb E(|progeny|)$ where on average you get what you expect. For strategy $S$ the population\n\n$$\nx_{t}(S) = \\frac{x_{t-1}(S) \\times fitness(S)}{mean\\_fitness}\n$$\n\nand for continuous time[^2]\n\n[^2]: I think that we should consider a lewis hirarcy of games based on lewis games with\\\n    a. logic\\\n    b. conjuctive signals\n\n$$\n\\frac{dx}{dt} = x (fitness(S) - {mean\\_fitness})\n$$\n\nThe main outcomes of this chapter are that for a two state/signal/action Lewis game\n\n1.  Multiple isomorphic signaling systems we could call languages will arise leading to a population of agents split equaly\n2.  In a population of agents whose fitness depends on use of the language the stable state is one in which just one of the language is used by the entire population. Other equilibria are unstable which leads to spontaneous breaking of the symmetry and a gradual drift of the population towards one of the stable states.\n\nNotes:\n\n1.  The analysis fails to consider spatial dynamics. It seems that a in a local pockets of language 1, agents with language 2 might have lower fitness.\n2.  There is a cost of switching and agents typicaly are not born with a fully formed language ability they need to learn a language and that has costs and requires access to signalers with the said language.\n3.  In reality *Pidgeons* and *Creoles* are often formed. This is a language that is a mix of two or more languages. This is a partial pooling equilibrium. The existence of creoles suggest that the population dynamics of language formation is more complex than the simple Lewis game.\n\n## Langauge intergration problem:\n\n### **Problem Definition**\n\nGiven a set of signaling systems ${\\pi_1,\\pi_2,\\ldots,\\pi_ùëõ}$, find a permutation $\\pi_m$ such that:\n\n$$\n\\pi_m =\\arg \\min_\\pi \\sum_{ùëñ=1}^ùëõ d(\\pi,\\pi_i)\n$$ where d is the Cayley distance between permutations, i.e. the minimum number of transpositions required to transform one permutation into another.\n\n### **Solution Approach**\n\nFinding the exact median permutation is a computationally challenging task because the problem is NP-hard. However, there are heuristic and approximation methods to approach this problem. One common approach is to use a greedy algorithm that iteratively improves a candidate solution based on the distances to all permutations in the set.\n\nHere is a simple heuristic approach to estimate a solution:\n\n1.  **Start with an Initial Guess**: You can start with any permutation, such as ùúã1œÄ1‚Äã or any permutation randomly chosen from the set.\n\n2.  **Iterative Improvement**:\n\n    -   For each element in the permutation, consider swapping it with every other element.\n    -   Calculate the new total distance after each possible swap.\n    -   If a swap results in a lower total distance, make the swap permanent.\n    -   Repeat this process until no improving swaps are found.\n\nThis approach doesn't guarantee an optimal solution but can often produce a good approximation in a reasonable time frame.\n\nHere's a Python function that demonstrates this basic heuristic:\n\n::: {#b8d00e9f .cell execution_count=7}\n``` {.python .cell-code}\nimport itertools\n\ndef cayley_distance(pi, sigma):\n    \"\"\"Calculate the Cayley distance between two permutations.\"\"\"\n    count = 0\n    temp = list(pi)\n    for i in range(len(pi)):\n        while temp[i] != sigma[i]:\n            swap_index = temp.index(sigma[i])\n            temp[i], temp[swap_index] = temp[swap_index], temp[i]\n            count += 1\n    return count\n\ndef median_permutation(permutations):\n    n = len(permutations[0])  # Assuming all permutations are of the same length\n    current = list(permutations[0])  # Start with the first permutation as an initial guess\n    improving = True\n\n    while improving:\n        improving = False\n        best_distance = sum(cayley_distance(current, p) for p in permutations)\n        for i, j in itertools.combinations(range(n), 2):\n            current[i], current[j] = current[j], current[i]  # Swap elements\n            new_distance = sum(cayley_distance(current, p) for p in permutations)\n            if new_distance < best_distance:\n                best_distance = new_distance\n                improving = True\n            else:\n                current[i], current[j] = current[j], current[i]  # Swap back if no improvement\n\n    return current\n\n# Example usage\npermutations = [\n    [1, 2, 3, 4],\n    [2, 1, 4, 3],\n    [1, 3, 4, 2],\n    [4, 3, 2, 1]\n]\nprint(\"Median permutation:\", median_permutation(permutations))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedian permutation: [1, 2, 3, 4]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_111005/3341496467.py:21: DeprecationWarning:\n\nCalling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n\n/tmp/ipykernel_111005/3341496467.py:24: DeprecationWarning:\n\nCalling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n\n```\n:::\n:::\n\n\n# Learning\n\nTwo type of learning are considered.\n\n1.  Evolution learning using knowledge hard-coded into the genome of the agents. Learning happens though replicator dynamics incorporating randomization followed by natural selection. Also other biologically inspired ideas like mutation and use of a fitness function can come into play.\n\n    The down side of Evolution is that is takes many generation for many structures to emerge. (Richard Dawkings states that the evolution of different morphology of the eye are quick taking only 80 generation to evolve in a simulation from the most rudimentary light sensitive cell and elsewhere suggest that 8 generations are needed to see changes in this type of framework.\n\n2.  RL refers to the type of learning from experience by an organism during its lifetime.\n\n3.  Noam Chomsky and others Linguistics hypothesize that Language learning faculties are to a large extent passed through evolution and for this reason individuals can learn languages based on a rather minimal amount of stimulus. This has also be a reason why many in their field abandoned their work on solving linguistics and went on to research the mysteries of the human brain. I feel that to a large extent this book demonstrates that scientifically the notion of the brain requiring a specialized mechanism to evolve/learn complex language is an unnecessary assumption. (Of course it is possible that the brain has co-evolved together with language and that such mechanism do exist.)\n\n    1.  in one sense the book starts with very simple systems of communication with just a lexicon.\n\n    2.  The formation of more complex systems with syntax are treated in chapter 12 but these results here seem to satisfy a mathematician or a philosopher etc, without delving into different linguistic niceties that might satisfy a linguist.\n\n    3.  However the Lewis game needs only a small tweak (the receiver getting multiple partial signals) to allow a signaling system with a grammar to emmerge via Roth-Erev RL. We can also make a categorical statement that this type of RL is a general purpose learning mechanism not a language specific one.\n\nIn agents we have learning that is based on evolution and requires subsequent generations of agents becoming fitter.\n\nHere are two conceptual ideas to base RL on\n\nLaw of effect\n\n:   Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more Ô¨Årmly connected with the situation, so that, when it recurs, they will be more likely to recur. ‚Äî Edward Thorndike, Animal Intelligence, 1911\n\nLaw of practice\n\n:   Learning slows down as reinforcements accrue\n\n## Roth‚ÄìErev RL alg:\n\n1.  set starting weight for each option\n2.  weights evolve by addition of rewards gotten\n3.  probability of choosing an alternative is proportional to its weight.\n\n```python RE-RL\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation\nimport random\nimport numpy as np\n\nclass LearningRule:\n    def __init__(self, options, learning_rate=0.1):\n        self.weights = {option: 1.0 for option in options}  # Start with equal weights for all options\n        self.learning_rate = learning_rate\n\n    def update_weights(self, option, reward):\n        # Update the weight of the chosen option by adding the reward scaled by the learning rate\n        old_weight = self.weights[option]\n        self.weights[option] += self.learning_rate * reward\n        print(f\"Updated weight for option {option}: {old_weight} -> {self.weights[option]}\")\n\n    def choose_option(self):\n        # Select an option based on the weighted probabilities\n        total = sum(self.weights.values())\n        probabilities = [self.weights[opt] / total for opt in self.weights]\n        return np.random.choice(list(self.weights.keys()), p=probabilities)\n\nclass LewisAgent(Agent):\n    def __init__(self, unique_id, model, learning_options):\n        super().__init__(unique_id, model)\n        self.message = None\n        self.action = None\n        self.reward = 0\n        self.learning_rule = LearningRule(learning_options, learning_rate=0.1)  # Initialize learning with given options\n\n    def set_reward(self):\n        print(f\"Agent {self.unique_id} received reward: {self.reward}\")\n\nclass Sender(LewisAgent):\n    def send(self):\n        state = self.model.get_state()\n        self.message = self.learning_rule.choose_option()  # Send a signal based on the learned weights\n        print(f\"Sender {self.unique_id} sends signal for state {state}: {self.message}\")\n\n    def update_learning(self):\n        self.learning_rule.update_weights(self.model.current_state, self.reward)  # Update weights based on the state and received reward\n\nclass Receiver(LewisAgent):\n    def receive(self):\n        self.received_signals = [sender.message for sender in self.model.senders]\n        if self.received_signals:\n            self.action = self.learning_rule.choose_option()  # Choose an action based on received signals and learned weights\n\n    def calc_reward(self):\n        correct_action = self.model.states_actions[self.model.current_state]\n        self.reward = 1 if self.action == correct_action else 0\n        print(f\"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}\")\n\n    def update_learning(self):\n        for signal in self.received_signals:\n            self.learning_rule.update_weights(signal, self.reward)  # Update weights based on signals and rewards\n\nclass SignalingGame(Model):\n    def __init__(self, senders_count=1, receivers_count=1, state_count=3):\n        super().__init__()\n        self.k = k\n        self.current_state = None\n\n        # Initialize the states, signals, and actions mapping\n        self.states_signals = list(range(k))  # States are simply numbers\n        self.signals_actions = list(chr(65 + i) for i in range(k))  # Signals are characters\n\n        self.states_actions = {i: i for i in range(k)}  # Mapping states to correct actions\n\n        self.senders = [Sender(i, self, self.signals_actions) for i in range(senders_count)]\n        self.receivers = [Receiver(i + senders_count, self, self.signals_actions) for i in range(receivers_count)]\n        \n        self.schedule = StagedActivation(self, stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])\n\n    def get_state(self):\n        return random.choice(self.states_signals)\n\n    def step(self):\n      \n        self.current_state = self.get_state()\n        print(f\"New state of the world: {self.current_state}\")\n        self.schedule.step()\n\n# Running the model\nmodel = SignalingGame(senders_count=1, receivers_count=1, state_count=3)\nfor i in range(10):\n    print(f\"--- Step {i+1} ---\")\n    model.step()\n    \n```\n\n## Bush‚ÄìMosteller RL\n\n1.  If an act is chosen and a reward is gotten the probability is incremented by adding some fraction of the distance between the original probability and probability one\n\n    $$\n    pr_{new}(A)=(1-\\alpha)pr_{old}(A) + a(1)\n    $$\n\n2.  Alternative action probabilities are decremented so that everything adds to one\n\n## Goldilocks RL\n\nWe consider if there is a Goldilocks point in the RL exploration exploitation dilemma which has a good balance of the two.\n\n-   If we stop learning too fast we are **too cold**\n\n-   If we exploring too much we are **too hot**\n\n-   At the limit is the Goldilocks RL point\n\n**Q: is there Goldilocks RL Alg?**\n\n-   Roth‚ÄîErev, Thompson sampling & UCB don't get stuck\n\n-   Epsilon greedy is too hot\n\n-   Bush‚ÄìMosteller is too cold\n\n## RL variants:\n\n-   BM variants like dynamically adjusting aspiration levels\n\n-   exponential response rule. The basic idea is to make probabilities proportional to the exponential of past reinforcements. [@Blume2002]\n\n-   best response dynamics, aka Cournot dynamics\n\n## Beyond the book:\n\n-   \\^\\[citation needed \\]\\^ investigating RL for this task also suggest that Roth-Erev with forgetting leads to more efficient learning.\n-   \\^\\[citation needed\\]\\^ Another paper suggest that a learning with a certain prior can be better than Roth-Erev learning.\n\nAdding Learning\n\n::: {#036bba09 .cell execution_count=8}\n``` {.python .cell-code}\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation\nimport random\n\nclass LewisAgent(Agent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n        self.message = None\n        self.action= None\n\n    def send(self):\n      pass\n    \n    def recive(self):\n      pass\n\n    def calc_reward(self):\n      pass\n    \n    def set_reward(self):\n        self.reward = model.reward\n        # Placeholder for learning logic\n        print(f\"Agent {self.unique_id} received reward: {self.reward}\")\n \nclass Sender(LewisAgent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n\n    def send(self):\n        state = self.model.get_state()\n        # Learning to map states to signals\n        self.message = self.model.states_signals[state]\n        print(f\"Sender {self.unique_id} sends signal for state {state}: {self.message}\")\n\nclass Receiver(LewisAgent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n\n    def recive(self):\n      self.received_signals=[]\n      for sender in self.model.senders:\n        self.received_signals.append(sender.message)\n            # Learning to map signals to actions\n      if len(self.received_signals)==1:\n        self.action = self.model.signals_actions[self.received_signals[0]]\n      else:\n        self.action = self.model.signals_actions[self.received_signals[0]]\n      \n\n    def calc_reward(self):\n      action = self.model.signals_actions[self.received_signals[0]]\n      correct_action = self.model.states_actions[self.model.current_state]\n      reward = 1 if action == correct_action else 0\n      model.reward = reward\n\n\nclass SignalingGame(Model):\n    def __init__(self, senders_count=1, recievers_count=1, state_count=3):\n        \n        super().__init__()\n        self.senders_count=senders_count\n        self.recievers_count=recievers_count\n        self.num_agents = self.recievers_count+self.senders_count\n\n        # e.g., 0 -> A, 1 -> B, ...\n        self.states_signals = {i: chr(65 + i) for i in range(k)} \n\n        # e.g., A -> 0, B -> 1, ...\n        self.signals_actions = {chr(65 + i): i for i in range(k)}\n        \n        # state 0 needs action 0, state 1 needs action 1, ...\n        self.states_actions = {i: i for i in range(k)}  \n        \n        self.current_state = None\n\n        # Create agents\n        self.senders = []\n        self.receivers=[]\n        self.my_agents=[]\n        self.uid=0\n        for i in range(self.senders_count):\n            sender = Sender(self.uid, self)\n            self.senders.append(sender)\n            self.my_agents.append(sender)\n            self.uid +=1\n        for j in range (self.recievers_count):\n            reciever = Receiver(self.uid, self)\n            self.receivers.append(reciever)\n            self.my_agents.append(reciever)\n            self.uid +=1\n\n        self.schedule = StagedActivation(\n          model=self,\n          agents=self.my_agents, \n          stage_list = ['send','recive','calc_reward','set_reward']\n        )\n    \n    def get_state(self):\n        return self.current_state\n\n    def step(self):\n        self.current_state = random.choice(list(self.states_signals.keys()))\n        print(f\"New state of the world: {self.current_state}\")\n        self.schedule.step()\n\n# Running the model\nk = 3  # Number of states, signals, and actions\nsteps = 10\nmodel = SignalingGame(senders_count=2,recievers_count=1,state_count=k)\nfor i in range(steps):\n    print(f\"--- Step {i+1} ---\")\n    model.step()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[7], line 112</span>\n<span class=\"ansi-green-fg ansi-bold\">    110</span> k <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(98,98,98)\">3</span>  <span style=\"font-style:italic;color:rgb(95,135,135)\"># Number of states, signals, and actions</span>\n<span class=\"ansi-green-fg ansi-bold\">    111</span> steps <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(98,98,98)\">10</span>\n<span class=\"ansi-green-fg\">--&gt; 112</span> model <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">SignalingGame</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">senders_count</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">2</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\">recievers_count</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">1</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\">state_count</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">k</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    113</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> i <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\">range</span>(steps):\n<span class=\"ansi-green-fg ansi-bold\">    114</span>     <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">--- Step </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span>i<span style=\"color:rgb(98,98,98)\">+</span><span style=\"color:rgb(98,98,98)\">1</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\"> ---</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n\nCell <span class=\"ansi-green-fg\">In[7], line 85</span>, in <span class=\"ansi-cyan-fg\">SignalingGame.__init__</span><span class=\"ansi-blue-fg\">(self, senders_count, recievers_count, state_count)</span>\n<span class=\"ansi-green-fg ansi-bold\">     83</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>uid<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">0</span>\n<span class=\"ansi-green-fg ansi-bold\">     84</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> i <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\">range</span>(<span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>senders_count):\n<span class=\"ansi-green-fg\">---&gt; 85</span>     sender <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">Sender</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">uid</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     86</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>senders<span style=\"color:rgb(98,98,98)\">.</span>append(sender)\n<span class=\"ansi-green-fg ansi-bold\">     87</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>my_agents<span style=\"color:rgb(98,98,98)\">.</span>append(sender)\n\nCell <span class=\"ansi-green-fg\">In[7], line 29</span>, in <span class=\"ansi-cyan-fg\">Sender.__init__</span><span class=\"ansi-blue-fg\">(self, unique_id, model)</span>\n<span class=\"ansi-green-fg ansi-bold\">     28</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">__init__</span>(<span style=\"color:rgb(0,135,0)\">self</span>, unique_id, model):\n<span class=\"ansi-green-fg\">---&gt; 29</span>     <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">super</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span style=\"color:rgb(0,0,255)\" class=\"ansi-yellow-bg\">__init__</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">unique_id</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">model</span><span class=\"ansi-yellow-bg\">)</span>\n\nCell <span class=\"ansi-green-fg\">In[7], line 8</span>, in <span class=\"ansi-cyan-fg\">LewisAgent.__init__</span><span class=\"ansi-blue-fg\">(self, unique_id, model)</span>\n<span class=\"ansi-green-fg ansi-bold\">      7</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">__init__</span>(<span style=\"color:rgb(0,135,0)\">self</span>, unique_id, model):\n<span class=\"ansi-green-fg\">----&gt; 8</span>     <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">super</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span style=\"color:rgb(0,0,255)\" class=\"ansi-yellow-bg\">__init__</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">unique_id</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">model</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">      9</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>message <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">     10</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>action<span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n\nFile <span class=\"ansi-green-fg\">~/work/blog/.venv/lib/python3.10/site-packages/mesa/agent.py:64</span>, in <span class=\"ansi-cyan-fg\">Agent.__init__</span><span class=\"ansi-blue-fg\">(self, model, *args, **kwargs)</span>\n<span class=\"ansi-green-fg ansi-bold\">     51</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span style=\"color:rgb(0,0,255)\">__init__</span>(<span style=\"color:rgb(0,135,0)\">self</span>, model: Model, <span style=\"color:rgb(98,98,98)\">*</span>args, <span style=\"color:rgb(98,98,98)\">*</span><span style=\"color:rgb(98,98,98)\">*</span>kwargs) <span style=\"color:rgb(98,98,98)\">-</span><span style=\"color:rgb(98,98,98)\">&gt;</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>:\n<span class=\"ansi-green-fg ansi-bold\">     52</span> <span style=\"color:rgb(188,188,188)\">    </span><span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"Create a new agent.</span>\n<span class=\"ansi-green-fg ansi-bold\">     53</span> \n<span class=\"ansi-green-fg ansi-bold\">     54</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    Args:</span>\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">     62</span> \n<span class=\"ansi-green-fg ansi-bold\">     63</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">    \"\"\"</span>\n<span class=\"ansi-green-fg\">---&gt; 64</span>     <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">super</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span style=\"color:rgb(0,0,255)\" class=\"ansi-yellow-bg\">__init__</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     66</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>model: Model <span style=\"color:rgb(98,98,98)\">=</span> model\n<span class=\"ansi-green-fg ansi-bold\">     67</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>unique_id: <span style=\"color:rgb(0,135,0)\">int</span> <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">next</span>(<span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_ids[model])\n\n<span class=\"ansi-red-fg\">TypeError</span>: object.__init__() takes exactly one argument (the instance to initialize)</pre>\n```\n:::\n\n:::\n:::\n\n\n# 11. Networks I: Logic and Information Processing\n\n## Logic\n\n## Information processing\n\n### Inventing the code Game\n\nThe world has say four states {S1...S4}. In this extended Lewis game where an agent is a receiver of two messages, each with a partial specification the first is {s1\\|\\|s2} or {s3\\|\\|s4} and the second {s1\\|\\|s3} or {s2\\|\\|s4}. The agent needs to process the two messages it to get the full state specification and take the appropriate action in response for getting a reward !\n\nThe added problem here is that the messages one of two flags, and one of two other flags do not have an established system for the message so learning the content of the signals needs to evolve together with the inference.\n\nThe sender can be two agents or one agent with a complex message.\n\nJeffrey Barrett in Barrett 2007a, 2007b. showed that this can be learned with Roth Erev RL\n\nthis is more interesting if there are errors:\n\n-   is a 10% chance of senders making mistakes with only 3% errors by the receiver?! Skyrms explains this due to the inference being like a taking a vote in a Condorcet signaling system.\n\n-   receiver errors are considered in [@Nowak1999] where the authors claim they lead to syntax formation.\n\n# 12. Complex Signals and Compositionality\n\nCCSS\n\n:   complex composeable signaling systems\n\n:   \n\n-   The use of complex signals is not unique to humans.\n\n-   In [@Nowak1999] the authors make a case that complex signals can increase the Ô¨Ådelity of information transmission, by preventing simple signals getting crowded together as the space of potential signals gets Ô¨Ålled up. Also some complex signalsing systems should be simpler to learn. (*can we specify a maximaly learnable family?*) and process inforamtion\n\n-   considered CCSS as conffering greater Darwinian fitness in contexts where *rich information processing is important.*\n\n    -   Q: **Is there a metric for measuring the advantage and or the importance of such information processing needs?**\n\n-   In [@batali1998] the author investigates the emergence of complex signals in populations of neural nets.\n\n-   in [@Kirby2000] the author, extends the model in a small population of interacting artiÔ¨Åcial agents.\n\n-   These two papers assume Structured meanings like \\<John, loves, Mary\\>. But I am more interested in the ability of evolving arbitrary structures like a sketch map of resources, a distribution of prices, a small bitmap etc.\n\n-   Skryms takes a similar reductionist POV: finding how to evolve a complex signaling system with minimal departure from the Lewis signaling game and other models already covered....\n\n-   It is suggested that the \"Inventing the code Game\" is a sufficient framework creating basic composeable messages. If the receiver considers a sequence of two partial signals as conjunction the and can integrated into one full message!\n\n    -   Red \\> Top\n\n    -   Green\\> Bottom\n\n    -   Yellow\\> Left\n\n    -   Blue \\> Right\n\n    to signal the state of \\<bottom, left\\> a sender can send \\<green,yellow\\> or \\<yellow,green\\> and the receiver can compose them.\n\n-   But if it is also possible to evolve and learn order for signals a richer form of composeability become possible. Subject‚Äìpredicate or operator‚Äìsentence.\n\n-   Sensitivity to temporal order is something many organisms have already developed in responding to perceptual signals.\n\n-   More generally, we can say that temporal pattern recognition is a fundamental mechanism for anticipating the future.\n\nSkryms points out that temporal order is another mechanism that evolves and that they come together.\n\nUnfortunately Skryms seems to get sidetracked once he point out about order and does not explain how order sensitivity eveloves in \"Making the code game\".\n\n```python\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation\nimport random\n\nclass LewisAgent(Agent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n        self.message = None\n        self.action= None\n\n    def send(self):\n      pass\n    \n    def recive(self):\n      pass\n\n    def calc_reward(self):\n      pass\n    \n    def set_reward(self):\n        self.reward = model.reward\n        # Placeholder for learning logic\n        print(f\"Agent {self.unique_id} received reward: {self.reward}\")\n \nclass Sender(LewisAgent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n\n    def send(self):\n        state = self.model.get_state()\n        \n        # Learning to map states to signals\n        if type(state) is str:\n          self.message = self.model.states_signals[state]\n        else:\n          self.message = set()\n          while len(self.message)>0:\n            message = {model.states_signals[self.message.pop()]}\n            self.message = self.message.union(message)\n        print(f\"Sender {self.unique_id} sends signal for state {state}: {self.message}\")\n\nclass Receiver(LewisAgent):\n  \n    def __init__(self, unique_id, model):\n        super().__init__(unique_id, model)\n\n    def recive(self):\n      self.received_signals=[]\n      self.action = set()\n      \n      for sender in self.model.senders:\n        self.received_signals.append(sender.message)\n        # Learning to map signals to actions\n        print(f'{self.received_signals=}')\n        print(f'{type(self.received_signals)=}')\n        \n        for signal_set in self.received_signals:\n          actions = set()\n          while len(signal_set)>0:\n            action  = {model.signals_actions[self.message.pop()]}\n            actions  = actions.union(action)\n          self.action =  self.action.intersection(actions)\n      print(f\"Reciever {self.unique_id} action : {self.action}\")\n\n    def calc_reward(self):\n      action = self.action\n      correct_action = self.model.states_actions[self.model.current_state]\n      reward = 1 if action == correct_action else 0\n      model.reward = reward\n      \n\nclass SignalingGame(Model):\n    def __init__(self, senders_count=1, recievers_count=1, state_count=3):\n        \n        super().__init__()\n        self.senders_count=senders_count\n        self.recievers_count=recievers_count\n        #self.num_agents = self.recievers_count+self.senders_count\n\n        self.states   = [f'{i}' for i in range(state_count)]\n        self.signals = [chr(65 + i) for i in range(state_count)]\n        self.actions = [f'{i}' for i in range(state_count)]\n\n        self.current_state = None\n\n        # Create agents\n        self.senders = []\n        self.receivers=[]\n        self.my_agents=[]\n        self.uid=0\n        \n        for i in range(self.senders_count):\n            sender = Sender(self.uid, self)\n            self.senders.append(sender)\n            self.my_agents.append(sender)\n            self.uid +=1\n        for j in range (self.recievers_count):\n            reciever = Receiver(self.uid, self)\n            self.receivers.append(reciever)\n            self.my_agents.append(reciever)\n            self.uid +=1\n\n        self.schedule = StagedActivation(\n          model=self,\n          agents=self.my_agents, \n          stage_list = ['send','recive','calc_reward','set_reward']\n        )\n    \n        \n\n    def step(self):\n        self.current_state = random.choice(list(self.states_signals.keys()))\n        self.current_state_set = {random.choice(list(self.states_signals.keys()))}\n        print(f\"New state of the world: {self.current_state}\")\n        self.schedule.step()\n\n    def get_state(self):\n        if self.senders_count ==1:\n          return self.current_state\n        else: \n          return {self.current_state, random.choice(list(self.states_signals.keys()))}\n\n\n# Running the model\nstate_count = 3  # Number of states, signals, and actions\nsteps = 10\nmodel = SignalingGame(senders_count=2,recievers_count=1,state_count=state_count)\nfor i in range(steps):\n    print(f\"--- Step {i+1} ---\")\n    model.step()\n\n```\n\n## Some thoughts\n\n1.  learning in the original Lewis language games is exponential in the size of the lexicon. It would seem that some complex signals systems should have orders of magnitude advantage in learning rates compared to the original variants. Lets consider a Lewis signaling system with 27 signals.\\\n    The learning is $O(e^{27})\\propto5\\times10^{12}$\n\n2.  Under a conjunctive structure with three messages a lexicon of 9 messages would be required.\\\n    The learning is $O(e^{9})\\propto 8.1\\times10^{3}$\n\n3.  Under Say we have a VSO complex signal with 3 signals per a positional POS category. This leads to 27 signal lexicon under the original lexicon. Using the complex system only 3 three signals need to be learned.\\\n    So that learning is $O(e^{3}) \\propto 20$\n\n    If we factor learning time as part of the costs of signaling we should expect complex signaling systems to emerge quickly. Also if we consider learning as part of In this case partial pooling states are acceptable and even desirable each signal now has three meaning depending on its position.\n\n4.  In NLP we never see such a perfect utilization of a SS where all synthetically messages are semantically meaningful. On the other hand NLP allow nesting so that sequence like V(VSO)(VSO) corresponding to 3^8^ messages and adding a sub-category modifier prefix (MVMSMO) leads to (3\\^6) signals 729 signals without\n\n5.  For a simulation - some predators can be introduced into the environment nearby agents will signal it presence. Receivers who take that appropriate action will survive. Those that do not may die. Agent have longevity and must learn the language. When agents die they are replaced by infants without a uniform signaling weights.\n\n6.  Another point is that seems obvious is that if we learn/evolve the lexicon with just one one new word at a time the task becomes trivial. We just need to learn one new state to signal and one new signal to action mapping. But learning just one is a one to one matching. If we have some sense of the salience of the signals we can just order them in that order and we keep increasing fitness.... till we reach some marginal rate of fitness where new signals do almost nothing for our survival.\n\n7.  If we can evolve a complex signaling system we can move to next steps like optimizing our lexicon and grammar for:\n    1.  minimizing communications errors, (error detection and correction)\n    2.  maximizing information transmission. (compression)\n    3.  minimizing cost of acquisition. (acquisition)\n    4.  the trade off between grammatic generalization and easily learnability v.s. making the system harder to learn but more efficient for communication.\n    5.  how do we handle inference (for logic)\n    6.  how do we take advantage of predictability for partial messages\n    7.  what about a convention for grammar - useful for agents that need to exchange data in different formats efficently.\n    8.  Costs of morphotactics - can we do all this in practive with human sound systems. Can we figure our metrics for human languages.\n    9.  Given a (human) language tree can we posit a most pasimonius path for its evolution.\n\n\nIn [@Skyrms2010signals] the author discusses how a Lewis signaling games can be viewed as a mechanism in which a rudimentary signaling system can give rise to a simple language.\n\nThe languages arising from Lewis signaling games are limited.\n\n\n\n\n\n---\ntldr:\n\nThe book is aptly nameed signals. It is primarily looking at signaling systems not at anything resembling a fully fledged languages.\n\n- What is missing -\n  - real distribution of states - very large set of messages\n  - real distribution of messages - a subset of phonology\n  - a communication protocol over a lossy amd risky channel which can lead to \n    - sender may err (most of the RL models in the book - unkike mine lead to making mistakes at a certain frequency)\n      - mistakes can happen at either end\n      - lewis games have shared reward but mistakes will impact agents assymetricaly\n        - recivers taking the wrong action will face greater risks\n        - senders making maistakes can \n    - sender\n    - transmission errors\n    - risk related to message length\n    - risk associaded with mistakes in the protocol.    \n\n    - posibility of partial messages and pressure on shorter messages \n      - consideation of \"learning by correcting mistakes\"\n  - a grammar \n  - group dynamics - there are many languages. Languages may have emereged over and over in different locations and grooups seems to\n    have played a major role here. I beleive that a basic model of many agents should lead to rapid \n  - some constraints\n    - distibutional contraints already menthioned\n    - basic languages learning must be rapid even if learnig the full system takes years.\n      - morphology speed up learning of the lexicon \n        - learning k lexemes + a morphology of m productions => $k \\times m$ lexicon with a coordiantion cost of $k+m$ \n      - syntax allows makes more efficent use of the \n\nThe main formalism is the Lewis Signing game. It covers in essense nothing more than coordinating a shared lexcion of signals. \nThere is a lot of effort made to consider the seperating equilibria. \nBut there are many more seperating equilibria and some of these are not only better suited for communications but also\nemergence of categories a type of messages that can be sepcilized/disambiguated using another signal. However this requires\nsetting up a communication protocol. Lewis signaling can help with the coordination task for that by picking a subset of \nprotocol from many possible options. But there is a second, more salient part - what is the protocol?\n\n",
    "supporting": [
      "signals-summary_files"
    ],
    "filters": [],
    "includes": {}
  }
}