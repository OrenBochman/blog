{
  "hash": "d671fed2c7078646246ad2cc6eb9c741",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-05-09\ntitle: \"Roth Erev learning in Lewis signaling games\"\ncategories: \n    - signaling games\n    - reinforcement learning\n    - Lewis signaling games\n    - language evolution\n    - signaling systems\n    - neural networks\n    - emergent languages\n    - deep learning\nkeywords: \n    - Roth Erev learning\n    - topographic similarity\n    - positional disentanglement\n    - bag-of-symbols disentanglement\n    - information gap disentanglement\nexecute: \n  cache: false\n\n---\n\n\n\n\n\n## Learning in Lewis signaling games\n\nI wish now to implement learning in the Lewis signaling game. In the book some reinforcement learning RL algorithms are presented in some detail and a few variations are mentioned. It worthwhile pointing out that the book statement of the algorithms is good enough to understand how the algorithms operate in general. However some of the details required to implement the algorithms were glossed over. As my one time collage Yuri Stool like to point out, \"the devil is in the details.\"\n\nI ended up implementing the algorithms a number of times - once to get it to work, second time to develop my own algorithm as I gained new insights into the problems. A third time after reading more of the papers whihc suggested how more details on conducting experiments which led to a deeper understanding of enumerating and ranking the partial pooling equilibria. The point here is that natural language is mostly a separating equilibrium - most words are unambiguous but there are a significant subset of words that have multiple meaning and there are many synonyms. Mechanisms in the lexicon seem to eventually resolves some ambiguities while letting others persist indefinitely. So while the separating equilibria are of primary interests in reality users if signaling systems satisfice with a systems that is good enough. This are the much more common partial pooling variants with high degree of separation plus a context based disambiguation mechanism. I consider the erosion of English and Latin conjugation and declination after the classical period as a simpler contextual disambiguation mechanism dismantling a nearly perfect signaling subsystem with a rather degenerate one with high degree of partial pooling. A simulation might show how a few prepositions and auxilary verbs are more efficent to learn and process than fully inflected systems of case and verb ending (especially if modified by phonetics). But my guess is that this happened as more speakers had to master an use a core language, without access to resources for learning the classical forms. I guess the dark ages and a decline in literacy likely speed up the process.\n\nAdding better analysis, estimating expected returns for a set of weights, tracking regret during learning. Considering different path to salience via differntial risks/costs for signals, and non uniform state distribution.\n\nThe big question seems to be:\n\nWhat is a simple rl algorithm to evolve and disseminate a signaling system with certain added requirements like\n\n-   complex signals\n\n    -   conjunctive signal aggregation\n\n    -   ordered signal aggregation via learning a grammar like SVO.\n\n    -   recursive signal aggregation replacing linear ordered with a partial order.\n\n-   resolving ambiguity by context\n\n-   mechanism for correcting errors (vowel harmony, agreement)\n\n-   simple growth of the lexicon (black bead leads to mutation in the urn model)\n\n-   sufficient capacity,\n\n-   minimal burden for processing (extending inference mechanism to reduce cognitive load, compress messages, erode unneeded structures)\n\n-   minimal burden in learning (e.g. by generalization via regularity in morphology, and syntax)\n\n-   high accuracy for transmission of messages\n\n-   saliencey - a information theoretic measure of more efficient transition subset of states/messages pairs.\n\nWhere the great unknown seems to be to find a minimal extension to the Lewis game in which all these might evlove.\n\nHaving stated the problem in detail lets me make the following two observations:\n\n1.  The aggregation rules for complex signaling should be made to arise by imposing costs on systems under which agents more frequently fail to make good inference with high probability of a partials message's describing risky states for sender and or receiver.\n\n2.  A second cost to fitness is the role of mistakes in signaling and or receiving. (ie. adding an small chance for decoding similar sounding signals (homophones, short vs long sounds, hissed and hushed, round, front and back vouwels). This may lead to excluding simple signals from places they might be confused, is it (a,a) (a.a) or (aa,a), (a,\\_,a) are avoided if signal 'a' is excluded from the first positions (say verb class). here dot might be a short pause, comma a long pause, undescore an unmarked slot, and two aa no pause. (either two a or a long a.) if we prefix V with v S with s and P with C\n\n    we end up with a system that is much more robust. And we may have the added bonus that we can easily detect a tree formation based on multiple Vprefix in the sentence....\n\n    1.  word grammar\n    2.  sub word grammar - a complex morphology - highly regular yet differented complex signals\n    3.  this could lead to redundancy based Error correction like subject verb agreement, noun adjective agreement or vowel harmony.\n    4.  Concord - case agreement (nouns pronouns and adjective are in agreement)\n\n3.  Ease of processing\n\n    1.  agreement can also ease processing\n    2.  assimilation and elision\n    3.  limiting processing/disabihation context windows.\n    4.  word order\n    5.  however redundencies add overhead, making signals longer and may make learning much longer (this is when we students who generelize are wrong and then need to learn via negative examples.\n\n4.  If many we have different complex signaling systems with minimal mistakes are possible one would prefer a system that is easier to learn. (Shorter lexicon, with lower chances of collision. Shorter grammar, fewer negtive examples, more room for expansion)\n\n### Richard Herrnsteinâ€™s Matching law\n\n1.  we start with some initial weights, perhaps equal.\n2.  An act is chosen with probability proportional to its weight.\n3.  The payoff gained is added to the weight for the act that was chosen,\n4.  and the process repeats\n\n### Roth-Erev learning algorithm\n\n1.  set starting weight for each option\n2.  weights evolve by addition of rewards gotten\n3.  probability of choosing an alternative is proportional to its weight.\n\n### Bush-Mosteller learning\n\n1.  set starting weight for each option\n2.  weights evolve by addition of rewards gotten\n3.  probability of choosing an alternative is proportional to its weight.\n4.  if the reward is 0 the weight is multiplied by a forgetting factor.\n\n### Roth-Erev learning with forgetting:\n\n1.  set starting weight for each option\n2.  weights evolve by addition of rewards gotten\n3.  probability of choosing an alternative is proportional to its weight.\n4.  if the reward is 0 the weight is multiplied by a forgetting factor.\n\n### ARP learning\n\n### Bochman 8-Rooks RL\n\nthis is a special purpose rl algorithm for coordination problems where agents need to establish a convention like in the Lewis signaling game. The idea is that the matrix is similar to a placing 8 rooks on on a chess board with no two under attack. In this case once an option has been chosen we want to exclude all options that shares a row or a collumm. So we set to zero any weights which share the same prefix or suffix as a reward 1 option.\n\n1.  set starting weight for each option (state_signal) for the sender and (signal_action) for the receiver, perhaps to 1\n2.  weights evolve by\n\n-   addition of rewards gotten for a correct choice and\n-   zeroing of options with the same prefix or suffix to exclude them from the choice set.\n\n3.  probability of choosing an alternative is proportional to its weight.\n\n::: {#cf7950d1 .cell execution_count=1}\n``` {.python .cell-code}\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom abc import ABC, abstractmethod\n\n# let's define a lambda to take a list of options and intilize the weights uniformly \nuniform_init = lambda options, w : {option: w for option in options}\nrandom_init  = lambda options, w : {option: random.uniform(0,1) for option in options}\n\n# lets make LeaningRule an abstract class with all the methods that are common to all learning rules\n# then we can subclass it to implement the specific learning rules\nclass LearningRule(ABC):\n  \n    def __init__(self, options, learning_rate=0.1,verbose=False,name='LearningRule',init_weight=uniform_init):\n        self.verbose = verbose\n        self.name=name\n        self.learning_rate = learning_rate\n        if self.verbose:\n          print(f'LearningRule.__init__(Options: {options})')\n        self.options = options\n        self.weights = init_weight(options,1.0) # Start with one ball per option \n        \n        \n    def get_filtered_weights(self, filter):\n        if self.verbose:\n          print(f'get_filtered_weights({filter=})')\n        # if filter is int convert to string\n        if isinstance(filter, int):\n            filter = str(filter)\n        filter_keys = [k for k in self.weights.keys() if k.startswith(filter)]\n        weights = {opt: self.weights[opt] for opt in filter_keys}\n        return weights\n      \n    @abstractmethod\n    def choose_option(self,filter):\n        pass\n      \n    @abstractmethod\n    def update_weights(self, option, reward):\n        pass\n      \nclass HerrnsteinRL(LearningRule):\n    '''\n                                    The Urn model\n     nature            sender                 reciever     reward\n                       \n    | (0) | --{0}-->  | (0_a)  | --{a}--> | (a_0) | --{0}-->   1   \n    |     |           | (0_b)  | --{b}    | (a_1) | --{1}-->   0\n    |     |           +--------+    | +-->+-------+\n    |     |                         +-|-+  \n    | (1) | --{1}-->  | (1_a)  | --{a}+ +>| (b_0) | --{1}-->   1\n    |     |           | (1_b)  | --{b}--->| (b_1) | --{0}-->   0\n    +-----+           +--------+          +-------+\n    \n    \n    Herrnstein urn algorithm\n    ------------------------\n    \n    1. nature picks a state \n    2. sender  gets the state, chooses a signal by picking a ball in choose_option() from the stat'es urn\n    3. reciver gets the action, chooses an actuion by picking a ball in choose_option()\n    4. the balls in the urns are incremented if action == state\n    5. repeat\n    \n    '''\n    def __init__(self, options, learning_rate=1.0,verbose=False,name='Herrnstein matching law'):\n      \n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n    def update_weights(self, option, reward):\n      \n        old_weight = self.weights[option]\n        self.weights[option] += self.learning_rate * reward \n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_weight} -> {self.weights[option]}\")\n          \n    def choose_option(self,filter):\n      \n        '''\n        \n        '''\n        # subseting the weights by the filter simulates different urns per state or signal\n        weights = self.get_filtered_weights(filter)\n\n        # calculate their probabilities then\n        total = sum(weights.values())\n        assert total > 0.0, f\"total weights is {total=} after {filter=} on {self.weights} \"\n        probabilities = [weights[opt] / total for opt in weights]\n        # then drawn an option from the filtered option using the probabilities\n        return np.random.choice(list(weights.keys()), p=probabilities)\n\n\nclass RothErevRL(LearningRule):\n\n    def __init__(self, options, learning_rate=0.1,verbose=False,name='Roth Erev RL'):\n      \n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n    def update_weights(self, option, reward):\n        old_weight = self.weights[option]\n        if reward == 1:\n          self.weights[option] += self.learning_rate * reward\n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_weight} -> {self.weights[option]}\")\n\n    def choose_option(self,filter):\n        # we subset the weights by the filter, calculate their probabilities then\n        # then drawn an option from the filtered option using the probabilities\n        weights = self.get_filtered_weights(filter)\n        total = sum(weights.values())\n        probabilities = [weights[opt] / total for opt in weights]\n        return np.random.choice(list(weights.keys()), p=probabilities)\n  \n\nclass RothErevForget_RL(LearningRule):\n\n    def __init__(self, options, learning_rate=0.1,verbose=False,name='Roth Erev with forgetting'):\n      \n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n    def update_weights(self, option, reward):\n        old_weight = self.weights[option]\n        if reward == 1:\n          self.weights[option] += self.learning_rate * reward\n        else:\n          self.weights[option] *= self.learning_rate \n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_weight} -> {self.weights[option]}\")\n\n    def choose_option(self,filter):\n        weights = self.get_filtered_weights(filter)\n        total = sum(weights.values())\n        probabilities = [weights[opt] / total for opt in weights]\n        return np.random.choice(list(weights.keys()), p=probabilities)\n  \nclass EightRooksRL(LearningRule):\n    def __init__(self, options, learning_rate=0.1,verbose=False,name='Eight Rooks RL'):\n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n\n    def update_weights(self, option, reward):\n        self.prefix = option.split('_')[0]\n        self.suffix = option.split('_')[1]\n        \n        old_weights=self.weights.copy()\n        \n        for test_option in self.options:\n          if reward == 1:\n            if test_option == option:\n            # increment the weight of the good option \n              self.weights[test_option] += self.learning_rate * reward\n            elif test_option.startswith(self.prefix) or test_option.endswith(self.suffix) :\n            # decrement all other options with same prefix  or suffix\n               # if self.weights[test_option] < 0.000001:\n               #   self.weights[test_option] = 0.0\n               # else:\n                self.weights[test_option] *= self.learning_rate \n          # elif test_option == option:\n          #   # decrement the weights of the bad option combo\n          #   self.weights[option] *= self.learning_rate \n\n        if self.verbose:\n          print()\n          for option in self.options:\n            if old_weights[option] != self.weights[option]:\n              print(f\"{option}: weight {old_weights[option]} -> {self.weights[option]}\")\n          #print(f\"Updated weight {old_weights} -> {self.weights}\")\n\n\n    def choose_option(self,filter):\n        weights = self.get_filtered_weights(filter)\n        total = sum(weights.values())\n        probabilities = [weights[opt] / total for opt in weights]\n        # if there is a max weight return it otherwise return a random option from the max wights\n        if len([opt for opt in weights if weights[opt]==max(weights.values())]) == 1:\n          return max(weights, key=weights.get)\n        else:\n          return np.random.choice([opt for opt in weights if weights[opt]==max(weights.values())])\n\nclass LewisAgent(Agent):\n    def __init__(self, unique_id, model, learning_options, learning_rule, verbose=False):\n        super().__init__(unique_id, model)\n        self.message = None\n        self.action = None\n        self.reward = 0\n        self.learning_rule = learning_rule\n        self.verbose = verbose\n        \n    def send(self):\n      return\n    \n    def receive(self):\n      return\n    \n    def calc_reward(self):\n      return\n\n    def set_reward(self):\n        self.reward = self.model.reward\n        if self.verbose:\n          print(f\"Agent {self.unique_id} received reward: {self.reward}\")\n        \n    def update_learning(self):\n        self.learning_rule.update_weights(self.option, self.reward)  # Update weights based on signals and rewards        \n\nclass Sender(LewisAgent):\n    def send(self):\n        state = self.model.get_state()\n        #self.message = self.learning_rule.choose_option(filter=state)  # Send a signal based on the learned weights\n        \n        self.option = self.learning_rule.choose_option(filter=state)  # Send a signal based on the learned weights\n        self.message = self.option.split('_')[1]\n        if self.verbose:\n          print(f\"Sender {self.unique_id} sends signal for state {state}: {self.message}\")\n\nclass Receiver(LewisAgent):\n    def receive(self):\n        self.received_signals = [sender.message for sender in self.model.senders]  # Receive signals from all senders\n        #print(f\"Receiver {self.unique_id} receives signals: {self.received_signals}\")\n        if self.received_signals:\n            for signal in self.received_signals:\n                self.option = self.learning_rule.choose_option(filter=signal)  # Choose an action based on received signals and learned weights\n                self.action = int(self.option.split('_')[1])\n                if self.verbose:\n                  print(f\"Receiver {self.unique_id} receives signals: {self.received_signals} and chooses action: {self.action}\")\n\n\n    def calc_reward(self):\n        correct_action = self.model.current_state\n        self.model.reward = 1 if self.action == correct_action else 0\n        if self.verbose:\n          print(f\"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}\")\n\nclass SignalingGame(Model):\n    def __init__(self, \n                senders_count=1, \n                receivers_count=1, k=3,\n                learning_rule=LearningRule,\n                learning_rate=0.1,\n                verbose=False):\n        super().__init__()\n        self.verbose = verbose\n        self.k = k\n        self.current_state = None\n        self.learning_rate=learning_rate\n\n        # Initialize the states, signals, and actions mapping\n        self.states = list(range(k))  # States are simply numbers\n        self.signals = list(chr(65 + i) for i in range(k))  # Signals are characters\n        self.actions = list(range(k))  # Actions are simply numbers\n\n        # generate a list of state_signal keys for the sender's weights\n        self.states_signals_keys = [f'{state}_{signal}' for state in self.states for signal in self.signals]\n        # generate a list of signal_action keys for the receiver's weights\n        self.signals_actions_keys = [f'{signal}_{action}' for signal in self.signals for action in self.actions]\n        \n        self.senders = [Sender(i, self, learning_options=self.states_signals_keys, \n                                  learning_rule=learning_rule(self.states_signals_keys, self.learning_rate,verbose=self.verbose)\n                              ) for i in range(senders_count)]\n        self.receivers = [Receiver(i + senders_count, self, learning_options=self.signals_actions_keys, \n                                  learning_rule=learning_rule(self.signals_actions_keys, self.learning_rate,verbose=self.verbose)\n                              ) for i in range(receivers_count)]\n        \n        self.schedule = StagedActivation(self, \n          agents = self.senders + self.receivers, \n          stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])\n\n    def get_state(self):\n        return random.choice(self.states)\n\n    def step(self):\n        self.current_state = self.get_state()\n        if self.verbose:\n          print(f\"Current state of the world: {self.current_state}\")\n        self.schedule.step()\n\n\n# function to plot agent weights side by side\n\ndef plot_weights(sender,reciver,title='Agent'):\n    fig, ax = plt.subplots(1,2,figsize=(9,5))\n    weights = sender.learning_rule.weights\n    ax[0].bar(weights.keys(), weights.values())\n    ax[0].set_xlabel('Options')\n    ax[0].set_ylabel('Weights')\n    ax[0].set_title(f'Sender {sender.unique_id} weights: {title}')\n    \n    weights = reciver.learning_rule.weights\n    ax[1].bar(weights.keys(), weights.values())\n    ax[1].set_xlabel('Options')\n    ax[1].set_ylabel('Weights')\n    ax[1].set_title(f'Receiver {reciver.unique_id} weights: {title}')\n    plt.show()\n\n\n# Running the model\nk=2\nverbose = False\nfor LR in [HerrnsteinRL,\n           RothErevRL,\n           RothErevForget_RL,\n           EightRooksRL\n           ]:\n  print(f\"--- {LR.__name__} ---\")\n  if LR == HerrnsteinRL:\n    learning_rate=1.\n  else:\n    learning_rate=.1\n  model = SignalingGame(senders_count=1, receivers_count=1, k=k, learning_rule=LR,learning_rate=learning_rate,verbose=verbose)\n  for i in range(10000):\n      if verbose:\n        print(f\"--- Step {i+1} ---\")\n      model.step()\n      # \n      #print the agent weights\n  #print('Sender weights:',model.senders[0].learning_rule.weights)\n  # plot weights side by side\n  \n  plot_weights(model.senders[0],model.receivers[0],title=LR.__name__)\n  #print('Receiver weights:',model.receivers[0].learning_rule.weights)\n  #plot_weights(model.receivers[0],title=LR.__name__)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--- HerrnsteinRL ---\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-2-output-2.png){width=750 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n--- RothErevRL ---\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-2-output-4.png){width=742 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n--- RothErevForget_RL ---\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-2-output-6.png){width=751 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n--- EightRooksRL ---\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-2-output-8.png){width=742 height=449}\n:::\n:::\n\n\ncurrently only the eight rooks learning rule is producing consistently good signaling systems. The other learning rules are not learning to signal correctly.\n\nPlease suggest how to fix this - according to the literature the Roth-Erev with forgetting learning rule should work well in this case.\n\nTODO: implement Bush-Mosteller learning - as this is a match for population dynamics.\n\nTODO: also implement population dynamics as it may not be clear that BM RL is a perfect fit for population dynamics under all lewis game conditions.\n\nTODO: implement ARP learning.\n\nTODO: implement epsilon-greedy, UCB and thompson sampling urn schemes, and Contextual bandits associative search (that's our multiurn bandit)\n\n## Estimating the Gittins index for a Lewis games.\n\n::: {#dba0c7ac .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass ContextualBandit:\n    def __init__(self, n_states, n_actions):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.rewards = np.zeros((n_states, n_actions))\n        self.counts = np.ones((n_states, n_actions))\n\n    def update(self, state, action, reward):\n        self.counts[state, action] += 1\n        self.rewards[state, action] += reward\n\n    def get_gittins_index(self, state, action):\n        # Simplified Gittins index computation\n        total_reward = self.rewards[state, action]\n        total_count = self.counts[state, action]\n        return total_reward / total_count + np.sqrt(2 * np.log(np.sum(self.counts)) / total_count)\n\n    def select_action(self, state):\n        gittins_indices = [self.get_gittins_index(state, a) for a in range(self.n_actions)]\n        return np.argmax(gittins_indices)\n\ndef sample_state(distribution, n_states):\n    if distribution == \"uniform\":\n        return np.random.randint(n_states)\n    elif distribution == \"normal\":\n        state = int(np.random.normal(loc=n_states/2, scale=n_states/6))\n        return np.clip(state, 0, n_states - 1)\n    else:\n        raise ValueError(\"Unsupported distribution type\")\n\n\n# Example usage\nn_states = 5\nn_actions = 5\nn_iterations = 1000\n\nsender_bandit = ContextualBandit(n_states, n_actions)\nreceiver_bandit = ContextualBandit(n_actions, n_states)\nstate_distribution = \"uniform\"  # Change to \"normal\" for normal distribution\n\nrewards = []\nregrets = []\ntotal_reward = 0\ntotal_regret = 0\nsender_gittins_indices = [[] for _ in range(n_actions)]\nreceiver_gittins_indices = [[] for _ in range(n_states)]\n\n# Simulate the learning process\nfor t in range(n_iterations):\n    state = sample_state(state_distribution, n_states)\n    sender_action = sender_bandit.select_action(state)\n    receiver_action = receiver_bandit.select_action(sender_action)\n    \n    reward = 1 if receiver_action == state else 0\n    total_reward += reward\n    total_regret += 1 - reward\n    \n    rewards.append(total_reward)\n    regrets.append(total_regret)\n    \n    sender_bandit.update(state, sender_action, reward)\n    receiver_bandit.update(sender_action, receiver_action, reward)\n    \n    for action in range(n_actions):\n        sender_gittins_indices[action].append(sender_bandit.get_gittins_index(state, action))\n    \n    for state in range(n_states):\n        receiver_gittins_indices[state].append(receiver_bandit.get_gittins_index(sender_action, state))\n\n# Print final policy\nprint(\"Sender policy:\")\nfor state in range(n_states):\n    print(f\"State {state}: Action {sender_bandit.select_action(state)}\")\n\nprint(\"Receiver policy:\")\nfor action in range(n_actions):\n    print(f\"Action {action}: State {receiver_bandit.select_action(action)}\")\n\n# Plot the total rewards and regrets over time\nplt.figure(figsize=(12, 6))\nplt.plot(rewards, label='Total Rewards')\nplt.plot(regrets, label='Total Regret')\nplt.xlabel('Time Step')\nplt.ylabel('Total Rewards/Regret')\nplt.title('Total Rewards and Regret Over Time')\nplt.legend()\nplt.show()\n\n# Plot the Gittins indices over time for the sender\nplt.figure(figsize=(12, 6))\nfor action in range(n_actions):\n    plt.plot(sender_gittins_indices[action], label=f'Sender Gittins Index (Action {action})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Sender Gittins Indices Over Time')\nplt.legend()\nplt.show()\n\n# Plot the Gittins indices over time for the receiver\nplt.figure(figsize=(12, 6))\nfor state in range(n_states):\n    plt.plot(receiver_gittins_indices[state], label=f'Receiver Gittins Index (State {state})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Receiver Gittins Indices Over Time')\nplt.legend()\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSender policy:\nState 0: Action 3\nState 1: Action 0\nState 2: Action 1\nState 3: Action 2\nState 4: Action 4\nReceiver policy:\nAction 0: State 1\nAction 1: State 4\nAction 2: State 3\nAction 3: State 0\nAction 4: State 4\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0, 0.5, 'Total Rewards/Regret')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 1.0, 'Total Rewards and Regret Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-3-output-5.png){width=965 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0, 0.5, 'Gittins Index')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 1.0, 'Sender Gittins Indices Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-3-output-9.png){width=969 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0, 0.5, 'Gittins Index')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nText(0.5, 1.0, 'Receiver Gittins Indices Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-3-output-13.png){width=969 height=523}\n:::\n:::\n\n\n## Making it Bayesian\n\nAccording to [@sutton2018reinforcement] Gitting's index are usually associated with the Bayesian paradigm.\n\nAs such one should be able to we could use a Bayesian updating scheme to learn expected rewards based on success counts. Since we are tracking successes vs failures we can use beta-binomial conjugate distributions to keep track of successes, failures and their likelihood.\n\nThis most basic form is like so:\n\n::: {#tbl-panel layout-ncol=\"2\"}\n| State/Signal | 0   | 1   | 2   |\n|--------------|-----|-----|-----|\n| 0            | 0,0 | 0,0 | 0,0 |\n| 1            | 0,0 | 0,0 | 0,0 |\n| 2            | 0,0 | 0,0 | 0,0 |\n\n: sender **alpha**, **beta** {#tbl-sender-prior .striped .hover}\n\n| Signal/Action | 0   | 1   | 2   |\n|---------------|-----|-----|-----|\n| 0             | 0,0 | 0,0 | 0,0 |\n| 1             | 0,0 | 0,0 | 0,0 |\n| 2             | 0,0 | 0,0 | 0,0 |\n\n: receiver **alpha**, **beta** {#tbl-receiver-prior .striped .hover}\n\nsender & receiver prior\n:::\n\nWhere we have a table of independent beta-binomial priors for each state/signal and signal/action pair.\n\nAfter 5 failures we update the beta distribution for the sender and receiver as follows:\n\n::: {layout-ncol=\"2\"}\n| State/Signal | 0   | 1   | 2   |\n|--------------|-----|-----|-----|\n| 0            | 0,1 | 0,2 | 0,0 |\n| 1            | 0,0 | 0,1 | 0,0 |\n| 2            | 0,0 | 0,0 | 0,1 |\n\n: sender **alpha**, **beta** {#tbl-sender-posterior .striped .hover}\n\n| Signal/Action | 0   | 1   | 2   |\n|---------------|-----|-----|-----|\n| 0             | 0,1 | 0,0 | 0,1 |\n| 1             | 0,0 | 0,1 | 0,0 |\n| 2             | 0,2 | 0,0 | 0,0 |\n\n: receiver **alpha**, **beta** {#tbl-receiver-posterior .striped .hover}\n\nsender & receiver posterior\n:::\n\nFailures are outcomes of uncorrelated signal action pairs and are basically like adding noise to the distribution on the loss side. Failures here tend to have a confounding effect - they reduce the probabilities associated with reward signals. And the model is not aware of the order of rewards/failures recency.\n\nNow lets update for 2 success as follows:\n\n::: {layout-ncol=\"2\"}\n| State/Signal | 0   | 1                         | 2                           |\n|--------------|-----|---------------------------|-----------------------------|\n| 0            | 1,1 | [1]{style=\"color: red\"},2 | 0,0                         |\n| 1            | 0,0 | 0,1                       | [1]{style=\"color: green\"},0 |\n| 2            | 0,0 | 0,0                       | 0,1                         |\n\n: sender **alpha**, **beta** {#tbl-sender-posterior .striped .hover}\n\n| Signal/Action | 0                         | 1                           | 2   |\n|---------------|---------------------------|-----------------------------|-----|\n| 0             | 0,1                       | 0,0                         | 0,1 |\n| 1             | [1]{style=\"color: red\"},0 | 0,1                         | 0,0 |\n| 2             | 0,2                       | [1]{style=\"color: green\"},0 | 0,0 |\n\n: receiver **alpha**, **beta** {#tbl-receiver-posterior .striped .hover}\n\nsender & receiver posterior\n:::\n\nThe Rewards are for *Corralated* **signals/action** pairs. However before learning progresses signal/action pairs are picked by chance. And so if different signal/action pairs are picked for the same state we will get a synonym and consequently will be missing a state/signal pair for one of the other states which will need to be shared (homonym).\n\nNote that if we have a ties (between two signal/action pairs for a state then the next success or failure can be a spontaneous symmetry breaking event.\n\nThis will result in a a partial pooling equilibrium.\n\nThe Gittin's index might help here by picking an options with the greatest expected return. If we set it up so it can recognize that a separating equilibria have the greatest expected return we should eventual learn these.\n\nThe problem is that micommunications (may confound the learning, until the pattern due to rewards are sufficiently reinforced.)\n\n::: {#100c071a .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass BayesianContextualBandit:\n    def __init__(self, n_states, n_actions):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.alpha = np.ones((n_states, n_actions))\n        self.beta = np.ones((n_states, n_actions))\n\n    def update(self, state, action, reward):\n        if reward == 1:\n            self.alpha[state, action] += 1\n        else:\n            self.beta[state, action] += 1\n\n    def get_expected_reward(self, state, action):\n        return self.alpha[state, action] / (self.alpha[state, action] + self.beta[state, action])\n\n    def get_gittins_index(self, state, action):\n        total_reward = self.alpha[state, action]\n        total_count = self.alpha[state, action] + self.beta[state, action]\n        return total_reward / total_count + np.sqrt(2 * np.log(np.sum(self.alpha + self.beta)) / total_count)\n\n    def select_action(self, state):\n        gittins_indices = [self.get_gittins_index(state, a) for a in range(self.n_actions)]\n        return np.argmax(gittins_indices)\n\ndef sample_state(distribution, n_states):\n    if distribution == \"uniform\":\n        return np.random.randint(n_states)\n    elif distribution == \"normal\":\n        state = int(np.random.normal(loc=n_states/2, scale=n_states/6))\n        return np.clip(state, 0, n_states - 1)\n    else:\n        raise ValueError(\"Unsupported distribution type\")\n\ndef run_experiment(n_states, n_actions, n_iterations, state_distribution, k):\n    all_rewards = np.zeros((k, n_iterations))\n    all_regrets = np.zeros((k, n_iterations))\n    all_sender_gittins_indices = np.zeros((k, n_actions, n_iterations))\n    all_receiver_gittins_indices = np.zeros((k, n_states, n_iterations))\n    \n    for i in range(k):\n        sender_bandit = BayesianContextualBandit(n_states, n_actions)\n        receiver_bandit = BayesianContextualBandit(n_actions, n_states)\n        \n        total_reward = 0\n        total_regret = 0\n        \n        for t in range(n_iterations):\n            state = sample_state(state_distribution, n_states)\n            sender_action = sender_bandit.select_action(state)\n            receiver_action = receiver_bandit.select_action(sender_action)\n            \n            reward = 1 if receiver_action == state else 0\n            total_reward += reward\n            total_regret += 1 - reward\n            \n            all_rewards[i, t] = total_reward\n            all_regrets[i, t] = total_regret\n            \n            sender_bandit.update(state, sender_action, reward)\n            receiver_bandit.update(sender_action, receiver_action, reward)\n            \n            for action in range(n_actions):\n                all_sender_gittins_indices[i, action, t] = sender_bandit.get_gittins_index(state, action)\n            \n            for s in range(n_states):\n                all_receiver_gittins_indices[i, s, t] = receiver_bandit.get_gittins_index(sender_action, s)\n    \n    mean_rewards = np.mean(all_rewards, axis=0)\n    mean_regrets = np.mean(all_regrets, axis=0)\n    mean_sender_gittins_indices = np.mean(all_sender_gittins_indices, axis=0)\n    mean_receiver_gittins_indices = np.mean(all_receiver_gittins_indices, axis=0)\n    \n    return all_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_gittins_indices, mean_receiver_gittins_indices\n\n# Parameters\nn_states = 5\nn_actions = 5\nn_iterations = 1000\nstate_distribution = \"uniform\"  # Change to \"normal\" for normal distribution\nk = 50  # Number of experiment runs\n\n# Run the experiment\nall_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_gittins_indices, mean_receiver_gittins_indices = run_experiment(n_states, n_actions, n_iterations, state_distribution, k)\n\n# Plot the mean total rewards and regrets over time along with individual curves\nplt.figure(figsize=(12, 6))\nfor i in range(k):\n    plt.plot(all_rewards[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_rewards, label='Mean Total Rewards', color='blue', linewidth=2)\nfor i in range(k):\n    plt.plot(all_regrets[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_regrets, label='Mean Total Regret', color='red', linewidth=2)\nplt.xlabel('Time Step')\nplt.ylabel('Total Rewards/Regret')\nplt.title('Total Rewards and Regret Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Gittins indices over time for the sender\nplt.figure(figsize=(12, 6))\nfor action in range(n_actions):\n    plt.plot(mean_sender_gittins_indices[action], label=f'Mean Sender Gittins Index (Action {action})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Mean Sender Gittins Indices Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Gittins indices over time for the receiver\nplt.figure(figsize=(12, 6))\nfor state in range(n_states):\n    plt.plot(mean_receiver_gittins_indices[state], label=f'Mean Receiver Gittins Index (State {state})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Mean Receiver Gittins Indices Over Time')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0, 0.5, 'Total Rewards/Regret')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 1.0, 'Total Rewards and Regret Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-4-output-4.png){width=965 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0, 0.5, 'Gittins Index')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 1.0, 'Mean Sender Gittins Indices Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-4-output-8.png){width=961 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0, 0.5, 'Gittins Index')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 1.0, 'Mean Receiver Gittins Indices Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-4-output-12.png){width=961 height=523}\n:::\n:::\n\n\nOf course there is no reason to use independent probabilities for for learning.\n\nThe schemes described in the book condition state for the sender and on the signal for the receiver. I.E. a success for a signal/action pair implies:\n\n1.  a failure for the other state/signals options with the same states for the sender.\n2.  a failure for the other signal/action options with the same signal for the receiver.\n\nIn my algorithm I went further and added the logic that a success for a signals/action pair also implies:\n\n1.  a failure for the other state/signals options with the same signal but different states for the sender.\n2.  a failure for the other signal/action options with the same action but different signals for the receiver.\n\nalso implies that the signal wasn't available for other states.\n\nI'm not sure if there is a distribution that updates like that, though it isn't that hard to implement either of the two schemes and they should work an extended beta distribution.\n\n## Derichlet-Multinomial variant\n\n::: {#bb559cea .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass BayesianContextualBandit:\n    def __init__(self, n_states, n_actions):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.alpha = np.ones((n_states, n_actions))\n\n    def update(self, state, action, reward):\n        if reward == 1:\n            self.alpha[state, action] += 1\n\n    def get_expected_reward(self, state, action):\n        alpha_sum = np.sum(self.alpha[state])\n        return self.alpha[state, action] / alpha_sum\n\n    def get_gittins_index(self, state, action):\n        alpha_sum = np.sum(self.alpha[state])\n        return self.alpha[state, action] / alpha_sum + np.sqrt(2 * np.log(alpha_sum) / (self.alpha[state, action] + 1))\n\n    def select_action(self, state):\n        gittins_indices = [self.get_gittins_index(state, a) for a in range(self.n_actions)]\n        return np.argmax(gittins_indices)\n\ndef sample_state(distribution, n_states):\n    if distribution == \"uniform\":\n        return np.random.randint(n_states)\n    elif distribution == \"normal\":\n        state = int(np.random.normal(loc=n_states/2, scale=n_states/6))\n        return np.clip(state, 0, n_states - 1)\n    else:\n        raise ValueError(\"Unsupported distribution type\")\n\ndef run_experiment(n_states, n_actions, n_iterations, state_distribution, k):\n    all_rewards = np.zeros((k, n_iterations))\n    all_regrets = np.zeros((k, n_iterations))\n    all_sender_gittins_indices = np.zeros((k, n_actions, n_iterations))\n    all_receiver_gittins_indices = np.zeros((k, n_states, n_iterations))\n    \n    for i in range(k):\n        sender_bandit = BayesianContextualBandit(n_states, n_actions)\n        receiver_bandit = BayesianContextualBandit(n_actions, n_states)\n        \n        total_reward = 0\n        total_regret = 0\n        \n        for t in range(n_iterations):\n            state = sample_state(state_distribution, n_states)\n            sender_action = sender_bandit.select_action(state)\n            receiver_action = receiver_bandit.select_action(sender_action)\n            \n            reward = 1 if receiver_action == state else 0\n            total_reward += reward\n            total_regret += 1 - reward\n            \n            all_rewards[i, t] = total_reward\n            all_regrets[i, t] = total_regret\n            \n            sender_bandit.update(state, sender_action, reward)\n            receiver_bandit.update(sender_action, receiver_action, reward)\n            \n            for action in range(n_actions):\n                all_sender_gittins_indices[i, action, t] = sender_bandit.get_gittins_index(state, action)\n            \n            for s in range(n_states):\n                all_receiver_gittins_indices[i, s, t] = receiver_bandit.get_gittins_index(sender_action, s)\n    \n    mean_rewards = np.mean(all_rewards, axis=0)\n    mean_regrets = np.mean(all_regrets, axis=0)\n    mean_sender_gittins_indices = np.mean(all_sender_gittins_indices, axis=0)\n    mean_receiver_gittins_indices = np.mean(all_receiver_gittins_indices, axis=0)\n    \n    return all_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_gittins_indices, mean_receiver_gittins_indices\n\n# Parameters\nn_states = 5\nn_actions = 5\nn_iterations = 1000\nstate_distribution = \"uniform\"  # Change to \"normal\" for normal distribution\nk = 50  # Number of experiment runs\n\n# Run the experiment\nall_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_gittins_indices, mean_receiver_gittins_indices = run_experiment(n_states, n_actions, n_iterations, state_distribution, k)\n\n# Plot the mean total rewards and regrets over time along with individual curves\nplt.figure(figsize=(12, 6))\nfor i in range(k):\n    plt.plot(all_rewards[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_rewards, label='Mean Total Rewards', color='blue', linewidth=2)\nfor i in range(k):\n    plt.plot(all_regrets[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_regrets, label='Mean Total Regret', color='red', linewidth=2)\nplt.xlabel('Time Step')\nplt.ylabel('Total Rewards/Regret')\nplt.title('Total Rewards and Regret Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Gittins indices over time for the sender\nplt.figure(figsize=(12, 6))\nfor action in range(n_actions):\n    plt.plot(mean_sender_gittins_indices[action], label=f'Mean Sender Gittins Index (Action {action})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Mean Sender Gittins Indices Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Gittins indices over time for the receiver\nplt.figure(figsize=(12, 6))\nfor state in range(n_states):\n    plt.plot(mean_receiver_gittins_indices[state], label=f'Mean Receiver Gittins Index (State {state})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Mean Receiver Gittins Indices Over Time')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0, 0.5, 'Total Rewards/Regret')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 1.0, 'Total Rewards and Regret Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-5-output-4.png){width=965 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0, 0.5, 'Gittins Index')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 1.0, 'Mean Sender Gittins Indices Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-5-output-8.png){width=961 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0, 0.5, 'Gittins Index')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 1.0, 'Mean Receiver Gittins Indices Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-5-output-12.png){width=961 height=523}\n:::\n:::\n\n\n## Thompson sampling\n\n::: {#11221b82 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass ThompsonSamplingContextualBandit:\n    def __init__(self, n_states, n_actions):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.alpha = np.ones((n_states, n_actions))\n        self.beta = np.ones((n_states, n_actions))\n\n    def update(self, state, action, reward):\n        if reward == 1:\n            self.alpha[state, action] += 1\n        else:\n            self.beta[state, action] += 1\n\n    def select_action(self, state):\n        samples = [np.random.beta(self.alpha[state, a], self.beta[state, a]) for a in range(self.n_actions)]\n        return np.argmax(samples)\n\ndef sample_state(distribution, n_states):\n    if distribution == \"uniform\":\n        return np.random.randint(n_states)\n    elif distribution == \"normal\":\n        state = int(np.random.normal(loc=n_states/2, scale=n_states/6))\n        return np.clip(state, 0, n_states - 1)\n    else:\n        raise ValueError(\"Unsupported distribution type\")\n\ndef run_experiment(n_states, n_actions, n_iterations, state_distribution, k):\n    all_rewards = np.zeros((k, n_iterations))\n    all_regrets = np.zeros((k, n_iterations))\n    all_sender_ts_indices = np.zeros((k, n_actions, n_iterations))\n    all_receiver_ts_indices = np.zeros((k, n_states, n_iterations))\n    \n    for i in range(k):\n        sender_bandit = ThompsonSamplingContextualBandit(n_states, n_actions)\n        receiver_bandit = ThompsonSamplingContextualBandit(n_actions, n_states)\n        \n        total_reward = 0\n        total_regret = 0\n        \n        for t in range(n_iterations):\n            state = sample_state(state_distribution, n_states)\n            sender_action = sender_bandit.select_action(state)\n            receiver_action = receiver_bandit.select_action(sender_action)\n            \n            reward = 1 if receiver_action == state else 0\n            total_reward += reward\n            total_regret += 1 - reward\n            \n            all_rewards[i, t] = total_reward\n            all_regrets[i, t] = total_regret\n            \n            sender_bandit.update(state, sender_action, reward)\n            receiver_bandit.update(sender_action, receiver_action, reward)\n            \n            for action in range(n_actions):\n                all_sender_ts_indices[i, action, t] = np.random.beta(sender_bandit.alpha[state, action], sender_bandit.beta[state, action])\n            \n            for s in range(n_states):\n                all_receiver_ts_indices[i, s, t] = np.random.beta(receiver_bandit.alpha[sender_action, s], receiver_bandit.beta[sender_action, s])\n    \n    mean_rewards = np.mean(all_rewards, axis=0)\n    mean_regrets = np.mean(all_regrets, axis=0)\n    mean_sender_ts_indices = np.mean(all_sender_ts_indices, axis=0)\n    mean_receiver_ts_indices = np.mean(all_receiver_ts_indices, axis=0)\n    \n    return all_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_ts_indices, mean_receiver_ts_indices\n\n# Parameters\nn_states = 5\nn_actions = 5\nn_iterations = 1000\nstate_distribution = \"uniform\"  # Change to \"normal\" for normal distribution\nk = 50  # Number of experiment runs\n\n# Run the experiment\nall_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_ts_indices, mean_receiver_ts_indices = run_experiment(n_states, n_actions, n_iterations, state_distribution, k)\n\n# Plot the mean total rewards and regrets over time along with individual curves\nplt.figure(figsize=(12, 6))\nfor i in range(k):\n    plt.plot(all_rewards[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_rewards, label='Mean Total Rewards', color='blue', linewidth=2)\nfor i in range(k):\n    plt.plot(all_regrets[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_regrets, label='Mean Total Regret', color='red', linewidth=2)\nplt.xlabel('Time Step')\nplt.ylabel('Total Rewards/Regret')\nplt.title('Total Rewards and Regret Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Thompson Sampling indices over time for the sender\nplt.figure(figsize=(12, 6))\nfor action in range(n_actions):\n    plt.plot(mean_sender_ts_indices[action], label=f'Mean Sender TS Index (Action {action})')\nplt.xlabel('Time Step')\nplt.ylabel('Thompson Sampling Index')\nplt.title('Mean Sender Thompson Sampling Indices Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Thompson Sampling indices over time for the receiver\nplt.figure(figsize=(12, 6))\nfor state in range(n_states):\n    plt.plot(mean_receiver_ts_indices[state], label=f'Mean Receiver TS Index (State {state})')\nplt.xlabel('Time Step')\nplt.ylabel('Thompson Sampling Index')\nplt.title('Mean Receiver Thompson Sampling Indices Over Time')\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0, 0.5, 'Total Rewards/Regret')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 1.0, 'Total Rewards and Regret Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-6-output-4.png){width=965 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0, 0.5, 'Thompson Sampling Index')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 1.0, 'Mean Sender Thompson Sampling Indices Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-6-output-8.png){width=969 height=523}\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 0, 'Time Step')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0, 0.5, 'Thompson Sampling Index')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 1.0, 'Mean Receiver Thompson Sampling Indices Over Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](re-rel_files/figure-html/cell-6-output-12.png){width=961 height=523}\n:::\n:::\n\n\n",
    "supporting": [
      "re-rel_files"
    ],
    "filters": [],
    "includes": {}
  }
}