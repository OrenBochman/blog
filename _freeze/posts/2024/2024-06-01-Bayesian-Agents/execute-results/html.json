{
  "hash": "dd658069064531578176721c1af68bba",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Bayesian Agents\"\ndate: 2024-06-01\n---\n\nI want to create a baysian and updating scheme for Lewis signaling games\nthat supports fast learning of signaling systems.\n\nOne direction is to use hierachial model. \nFirst I wanted to draw the intial wights from a prior \nis there a prior one can use for hierarchical learning in the Lewis signaling game.\n\nthe name of a prior for an distribution that is like an identity matrix?\n\n\nSome thought on modeling games with agents.\n\n1. Idealy one should be able to plug in a minimal amount of information about the agents and then be able to \nsimulate the game and identify the optimal strategies for the agents.\n2. One should be able to simulate the game with different solution concepts and see how the agents behave - like making mistakes or introducing private information.\n\nFor example for two player games we can provide a payoff matrix.\nthen we can simulate the players playing the game in turn or at the same time\nonce and repeatedly (with memory) and see how the agents behave.\n\nThis could cover a wide range of games from the prisoner's dilemma, stag hunt, battle of the sexes, Lewis signaling game with two signals.\nA simple bandit algorithm could be used to simulate the agents playing the game and identify the optimal strategies for the agents.\n\n\n## prisoners dilemma\n\nthe payoff matrix for the prisoners dilemma is:\n\n::: {#826aceb5 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\nprisoners_dillema_payoff_matrix = np.array([[(-1, -1), (-3, 0)], [(0, -3), (-2, -2)]])\nstag_hunt_payoff_matrix = np.array([[(-1, -1), (-3, 0)], [(0, -3), (-2, -2)]])\nlewis_signaling_game_payoff_matrix = np.array([[(1, 1), (0, 0)], [(0, 0), (1, 1)]])\nbattle_of_the_sexes_payoff_matrix = np.array([[(2, 1), (0, 0)], [(0, 0), (1, 2)]])\ndove_hawk_payoff_matrix = np.array([[(3, 3), (0, 4)], [(4, 0), (1, 1)]])\nsuppot_oppose_evade_payoff_matrix = np.array([[(6, 4), (2, 8),(8,2)],\n                                              [(8, 2), (25, 7.5),(7.5,2.5)], \n                                              [(3.5, 6.5), (3, 7),(4,6)]])\nchicken_payoff_matrix = np.array([[(0, 0), (-1, 1)], [(1, -1), (-10, -10)]])\na=100\nb=10\nrobber_guards_payoff_matrix = np.array([[(0, 0), (a, -1*a)], [(b, -1*b), (0, 0)]])  # mixed stategy\n```\n:::\n\n\nFor games with incomplete information we can provide a prior distribution over the possible payoffs and then update the distribution based on the agents actions.\n\n\nion exploring the space of possible games and strategies\none should be able to identify the optimal strategies for the agents.\n2. \n\n\nSome thoughts on developing the Bayesian agents:\n\nPareto improvement\n:   In welfare economics, a Pareto improvement formalizes the idea of an outcome being \"better in every possible way\". A change is called a Pareto improvement if it leaves everyone in a society better-off (or at least as well-off as they were before). \n\nPareto efficient or Pareto optimality\n:   A situation is called Pareto efficient or Pareto optimal if all possible Pareto improvements have already been made; in other words, there are no longer any ways left to make one person better-off, unless we are willing to make some other person worse-off\n\nMulti-objective optimization or Pareto optimization \n: is an area of multiple-criteria decision making that is concerned with mathematical optimization problems involving more than one objective function to be optimized simultaneously.\n\nAdmissible decision rule\n:   In statistical decision theory, an admissible decision rule is a rule for making a decision such that there is no other rule that is always \"better\" than it, in the precise sense of \"better\" defined below. This concept is analogous to Pareto efficiency.\n\ne.g. The James–Stein estimator is a nonlinear estimator of the mean of Gaussian random vectors and can be shown to dominate the ordinary least squares technique with respect to a mean-squared-error loss function. Therefore in this context the James–Stein estimator is admissible, while the ordinary least squares estimator is inadmissible.\n\n\n\n\n## Hiererchy of solution concepts\n\n1. What is the hierarchy of solution concepts - in the sense that one solution concept can provide better solutions for a broader class of games than another?\n\nOne of the tricky aspects is that games can seem very different at first yet \nwhen we work out the optimal strategies, it turns out that the crucial aspects \nof the games are the same.\n\nSolution concepts typically apply to a given class of games and these classes \ncan be used to provide a multidimensional hierarchy of solution concepts.\n\n\n\n\n\n\n\nstrict dominance, weak dominance, iterated dominance, Nash equilibrium, correlated equilibrium, subgame perfect equilibrium, Bayesian Nash equilibrium, trembling hand perfect equilibrium, sequential equilibrium, perfect Bayesian equilibrium, \n\npareto optimality, ESS, backward induction, minimax, maxmin, risk dominance, quantal response equilibrium, level-k reasoning, cognitive hierarchy, iterated elimination of dominated strategies, rationalizability, sequential equilibrium, trembling hand perfect equilibrium, proper equilibrium, sequential equilibrium, perfect Bayesian equilibrium,\ncore, Shapley value, nucleolus, kernel, bargaining set, von Neumann-Morgenstern solution, Nash bargaining solution, Kalai-Smorodinsky solution, egalitarian solution, competitive equilibrium, Walrasian equilibrium, Arrow-Debreu equilibrium, Radner, \n\nfor non-coopertaive game: Mertens stable equilibrium > forward induction, backward induction \n\n1. Given a set of agent, with a schedule, action and payoff - can we define a 'formal models' for game in extensive and normal form.\n2. For the formal game can we identifying all the different equlibria for a game is specified?\n3. Implementing different solution concepts for game theoretic agents.\n4. For games with incomplete information, can we implement a bayesian updating scheme for agents.\n5. Can we implement a learning scheme for agents in a game.\n\n::: {#b7aa6e3c .cell execution_count=2}\n``` {.python .cell-code}\nimport warnings\nfrom warnings import simplefilter\nwarnings.filterwarnings('ignore', message='The AgentSet is experimental*')\n\n# Import necessary modules\nfrom mesa import Agent, Model\nfrom mesa.time import RandomActivation\nfrom mesa.space import MultiGrid\nfrom mesa.datacollection import DataCollector\nimport numpy as np\n\nclass BayesianUpdater:\n    def __init__(self, prior=None):\n        if prior is None:\n            # Default prior: uniform distribution over actions 'A' and 'B'\n            prior = {'A': 0.5, 'B': 0.5}\n        self.prior = prior\n        self.belief = prior.copy()\n\n    def update_belief(self, observation, likelihoods):\n        # Update belief using Bayesian updating for each action\n        for action in self.belief:\n            self.belief[action] *= likelihoods[action]\n        \n        # Normalize to get new belief\n        total = sum(self.belief.values())\n        for action in self.belief:\n            self.belief[action] /= total\n\n    def make_decision(self):\n        # Example decision rule: choose action with highest belief\n        return max(self.belief, key=self.belief.get)\n\nclass BayesianAgent(Agent):\n    def __init__(self, unique_id, model, prior=None):\n        super().__init__(unique_id, model)\n        self.bayesian_updater = BayesianUpdater(prior)\n        self.observed_actions = []\n        self.action = None\n\n    def step(self):\n        # Make a decision based on current belief\n        self.action = self.bayesian_updater.make_decision()\n        \n        # Update belief based on the observed outcome\n        observation = self.model.observe(self)\n        likelihoods = {action: self.model.likelihood(observation, action) for action in self.bayesian_updater.prior}\n        self.bayesian_updater.update_belief(observation, likelihoods)\n        \n        # Observe actions of all other agents\n        self.observe_other_agents()\n        \n        # Print detailed output\n        print(f\"Agent {self.unique_id} action: {self.action}\")\n        print(f\"Agent {self.unique_id} belief: {self.bayesian_updater.belief}\")\n\n    def observe_other_agents(self):\n        # Observe actions of all other agents in the model\n        self.observed_actions = [agent.action for agent in self.model.schedule.agents if agent != self]\n        print(f\"Agent {self.unique_id} observed actions: {self.observed_actions}\")\n\n    def update_belief_about_others(self):\n        # Update belief about the world based on observed actions\n        for action in self.observed_actions:\n            likelihoods = {'A': self.model.likelihood(True, 'A'), 'B': self.model.likelihood(True, 'B')}\n            self.bayesian_updater.update_belief(True, likelihoods)\n\nclass BayesianModel(Model):\n    def __init__(self, N):\n        super().__init__()\n        self.num_agents = N\n        self.schedule = RandomActivation(self)\n        self.grid = MultiGrid(10, 10, True)\n        \n        # Define priors for three types of agents\n        prior_type_1 = {'A': 0.8, 'B': 0.2}\n        prior_type_2 = {'A': 0.5, 'B': 0.5}\n        prior_type_3 = {'A': 0.2, 'B': 0.8}\n\n        # Create agents with different priors\n        for i in range(self.num_agents):\n            if i % 3 == 0:\n                prior = prior_type_1\n            elif i % 3 == 1:\n                prior = prior_type_2\n            else:\n                prior = prior_type_3\n\n            agent = BayesianAgent(i, self, prior)\n            self.schedule.add(agent)\n            x = self.random.randrange(self.grid.width)\n            y = self.random.randrange(self.grid.height)\n            self.grid.place_agent(agent, (x, y))\n        \n        self.datacollector = DataCollector(\n            agent_reporters={\"Belief\": lambda a: a.bayesian_updater.belief}\n        )\n\n    def step(self):\n        self.datacollector.collect(self)\n        self.schedule.step()\n        for agent in self.schedule.agents:\n            agent.update_belief_about_others()\n\n    def observe(self, agent):\n        # Simulate an observation based on the agent's action\n        if agent.action == 'A':\n            return self.random.random() < 0.7  # 70% chance of success\n        else:\n            return self.random.random() < 0.3  # 30% chance of success\n\n    def likelihood(self, observation, action):\n        # Return likelihood of observation given action\n        if action == 'A':\n            return 0.7 if observation else 0.3\n        else:\n            return 0.3 if observation else 0.7\n\n# Run the model\nif __name__ == \"__main__\":\n    model = BayesianModel(10)\n    for i in range(10):  # Reduced the number of steps for brevity\n        print(f\"\\n--- Step {i + 1} ---\")\n        model.step()\n    \n    # Extract and print data\n    data = model.datacollector.get_agent_vars_dataframe()\n    print(data.tail())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n--- Step 1 ---\nAgent 7 observed actions: [None, None, None, None, None, None, None, None, None]\nAgent 7 action: A\nAgent 7 belief: {'A': 0.3, 'B': 0.7}\nAgent 8 observed actions: ['A', None, None, None, None, None, None, None, None]\nAgent 8 action: B\nAgent 8 belief: {'A': 0.09677419354838711, 'B': 0.903225806451613}\nAgent 3 observed actions: ['A', 'B', None, None, None, None, None, None, None]\nAgent 3 action: A\nAgent 3 belief: {'A': 0.903225806451613, 'B': 0.09677419354838711}\nAgent 0 observed actions: ['A', 'B', 'A', None, None, None, None, None, None]\nAgent 0 action: A\nAgent 0 belief: {'A': 0.903225806451613, 'B': 0.09677419354838711}\nAgent 5 observed actions: ['A', 'B', 'A', 'A', None, None, None, None, None]\nAgent 5 action: B\nAgent 5 belief: {'A': 0.09677419354838711, 'B': 0.903225806451613}\nAgent 4 observed actions: ['A', 'B', 'A', 'A', 'B', None, None, None, None]\nAgent 4 action: A\nAgent 4 belief: {'A': 0.7, 'B': 0.3}\nAgent 6 observed actions: ['A', 'B', 'A', 'A', 'B', 'A', None, None, None]\nAgent 6 action: A\nAgent 6 belief: {'A': 0.631578947368421, 'B': 0.3684210526315789}\nAgent 1 observed actions: ['A', 'B', 'A', 'A', 'B', 'A', 'A', None, None]\nAgent 1 action: A\nAgent 1 belief: {'A': 0.3, 'B': 0.7}\nAgent 9 observed actions: ['A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', None]\nAgent 9 action: A\nAgent 9 belief: {'A': 0.903225806451613, 'B': 0.09677419354838711}\nAgent 2 observed actions: ['A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A']\nAgent 2 action: B\nAgent 2 belief: {'A': 0.09677419354838711, 'B': 0.903225806451613}\n\n--- Step 2 ---\nAgent 1 observed actions: ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A']\nAgent 1 action: A\nAgent 1 belief: {'A': 0.997351434488271, 'B': 0.0026485655117290162}\nAgent 6 observed actions: ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A']\nAgent 6 action: A\nAgent 6 belief: {'A': 0.9998780740955051, 'B': 0.00012192590449486855}\nAgent 4 observed actions: ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A']\nAgent 4 action: A\nAgent 4 belief: {'A': 0.9999104188867107, 'B': 8.958111328923861e-05}\nAgent 5 observed actions: ['A', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A']\nAgent 5 action: A\nAgent 5 belief: {'A': 0.9980527468371296, 'B': 0.0019472531628704443}\nAgent 2 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A']\nAgent 2 action: A\nAgent 2 belief: {'A': 0.9980527468371296, 'B': 0.0019472531628704443}\nAgent 9 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A']\nAgent 9 action: A\nAgent 9 belief: {'A': 0.9998780740955052, 'B': 0.00012192590449486855}\nAgent 3 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A']\nAgent 3 action: A\nAgent 3 belief: {'A': 0.9999776032169311, 'B': 2.239678306888071e-05}\nAgent 8 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 8 action: A\nAgent 8 belief: {'A': 0.9980527468371296, 'B': 0.0019472531628704443}\nAgent 7 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 7 action: A\nAgent 7 belief: {'A': 0.997351434488271, 'B': 0.0026485655117290162}\nAgent 0 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 0 action: A\nAgent 0 belief: {'A': 0.9999776032169311, 'B': 2.239678306888071e-05}\n\n--- Step 3 ---\nAgent 7 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 7 action: A\nAgent 7 belief: {'A': 0.9999994448703694, 'B': 5.551296306621704e-07}\nAgent 5 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 5 action: A\nAgent 5 belief: {'A': 0.999999592149599, 'B': 4.078504009625729e-07}\nAgent 4 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 4 action: A\nAgent 4 belief: {'A': 0.9999999812721682, 'B': 1.8727831821417877e-08}\nAgent 3 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 3 action: A\nAgent 3 belief: {'A': 0.999999995318042, 'B': 4.6819580211166604e-09}\nAgent 6 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 6 action: A\nAgent 6 belief: {'A': 0.9999998612175346, 'B': 1.3878246544723665e-07}\nAgent 0 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 0 action: A\nAgent 0 belief: {'A': 0.9999999745093402, 'B': 2.5490659806763155e-08}\nAgent 9 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 9 action: A\nAgent 9 belief: {'A': 0.9999999745093402, 'B': 2.5490659806763158e-08}\nAgent 8 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 8 action: A\nAgent 8 belief: {'A': 0.999999592149599, 'B': 4.078504009625729e-07}\nAgent 1 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 1 action: A\nAgent 1 belief: {'A': 0.9999994448703694, 'B': 5.551296306621704e-07}\nAgent 2 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 2 action: A\nAgent 2 belief: {'A': 0.9999977794851753, 'B': 2.220514824627957e-06}\n\n--- Step 4 ---\nAgent 0 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 0 action: A\nAgent 0 belief: {'A': 0.9999999999946714, 'B': 5.328601407091876e-12}\nAgent 8 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 8 action: A\nAgent 8 belief: {'A': 0.9999999999147423, 'B': 8.525762250665542e-11}\nAgent 3 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 3 action: A\nAgent 3 belief: {'A': 0.9999999999946715, 'B': 5.328601407091877e-12}\nAgent 6 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 6 action: A\nAgent 6 belief: {'A': 0.9999999999709888, 'B': 2.9011274326813147e-11}\nAgent 7 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 7 action: A\nAgent 7 belief: {'A': 0.9999999998839549, 'B': 1.1604509729715271e-10}\nAgent 2 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 2 action: A\nAgent 2 belief: {'A': 0.9999999974727957, 'B': 2.5272043350445003e-09}\nAgent 1 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 1 action: A\nAgent 1 belief: {'A': 0.9999999998839549, 'B': 1.1604509729715271e-10}\nAgent 5 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 5 action: A\nAgent 5 belief: {'A': 0.9999999999147423, 'B': 8.525762250665542e-11}\nAgent 4 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 4 action: A\nAgent 4 belief: {'A': 0.999999999996085, 'B': 3.914890829705688e-12}\nAgent 9 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 9 action: A\nAgent 9 belief: {'A': 0.9999999999946714, 'B': 5.328601407091876e-12}\n\n--- Step 5 ---\nAgent 4 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 4 action: A\nAgent 4 belief: {'A': 0.9999999999999956, 'B': 4.455591569214526e-15}\nAgent 8 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 8 action: A\nAgent 8 belief: {'A': 0.9999999999999031, 'B': 9.703288306288511e-14}\nAgent 7 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 7 action: A\nAgent 7 belief: {'A': 0.9999999999999757, 'B': 2.425822076572305e-14}\nAgent 1 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 1 action: A\nAgent 1 belief: {'A': 0.9999999999998679, 'B': 1.3207253528003347e-13}\nAgent 3 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 3 action: A\nAgent 3 belief: {'A': 0.9999999999999939, 'B': 6.064555191430872e-15}\nAgent 0 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 0 action: A\nAgent 0 belief: {'A': 0.9999999999999939, 'B': 6.064555191430872e-15}\nAgent 5 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 5 action: A\nAgent 5 belief: {'A': 0.9999999999999822, 'B': 1.7822366276857863e-14}\nAgent 2 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 2 action: A\nAgent 2 belief: {'A': 0.9999999999994716, 'B': 5.282901411199244e-13}\nAgent 6 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 6 action: A\nAgent 6 belief: {'A': 0.999999999999967, 'B': 3.301813382001164e-14}\nAgent 9 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 9 action: A\nAgent 9 belief: {'A': 0.9999999999999989, 'B': 1.1138978923036352e-15}\n\n--- Step 6 ---\nAgent 9 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 9 action: A\nAgent 9 belief: {'A': 1.0, 'B': 2.3285069001793296e-19}\nAgent 7 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 7 action: A\nAgent 7 belief: {'A': 1.0, 'B': 2.760861761644727e-17}\nAgent 8 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 8 action: A\nAgent 8 belief: {'A': 1.0, 'B': 2.028388233045105e-17}\nAgent 1 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 1 action: A\nAgent 1 belief: {'A': 1.0, 'B': 2.7608617616447268e-17}\nAgent 3 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 3 action: A\nAgent 3 belief: {'A': 1.0, 'B': 1.2677426456531907e-18}\nAgent 4 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 4 action: A\nAgent 4 belief: {'A': 1.0, 'B': 5.0709705826127634e-18}\nAgent 0 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 0 action: A\nAgent 0 belief: {'A': 1.0, 'B': 6.9021544041118146e-18}\nAgent 5 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 5 action: A\nAgent 5 belief: {'A': 1.0, 'B': 3.725611040286927e-18}\nAgent 2 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 2 action: A\nAgent 2 belief: {'A': 0.9999999999999994, 'B': 6.012543392026289e-16}\nAgent 6 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 6 action: A\nAgent 6 belief: {'A': 1.0, 'B': 6.9021544041118146e-18}\n\n--- Step 7 ---\nAgent 4 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 4 action: A\nAgent 4 belief: {'A': 1.0, 'B': 5.771341975641892e-21}\nAgent 1 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 1 action: A\nAgent 1 belief: {'A': 1.0, 'B': 3.1421750756272515e-20}\nAgent 7 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 7 action: A\nAgent 7 belief: {'A': 1.0, 'B': 3.1421750756272515e-20}\nAgent 6 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 6 action: A\nAgent 6 belief: {'A': 1.0, 'B': 1.4428354939104723e-21}\nAgent 5 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 5 action: A\nAgent 5 belief: {'A': 1.0, 'B': 7.7880666393501565e-22}\nAgent 3 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 3 action: A\nAgent 3 belief: {'A': 1.0, 'B': 1.4428354939104723e-21}\nAgent 2 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 2 action: A\nAgent 2 belief: {'A': 1.0, 'B': 1.2568700302509001e-19}\nAgent 0 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 0 action: A\nAgent 0 belief: {'A': 1.0, 'B': 7.855437689068126e-21}\nAgent 8 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 8 action: A\nAgent 8 belief: {'A': 1.0, 'B': 4.240169614757307e-21}\nAgent 9 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 9 action: A\nAgent 9 belief: {'A': 1.0, 'B': 4.867541649593848e-23}\n\n--- Step 8 ---\nAgent 5 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 5 action: A\nAgent 5 belief: {'A': 1.0, 'B': 8.863706695300737e-25}\nAgent 3 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 3 action: A\nAgent 3 belief: {'A': 1.0, 'B': 3.0161224171509453e-25}\nAgent 2 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 2 action: A\nAgent 2 belief: {'A': 1.0, 'B': 1.4304612194724779e-22}\nAgent 9 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 9 action: A\nAgent 9 belief: {'A': 1.0, 'B': 1.01751735022585e-26}\nAgent 0 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 0 action: A\nAgent 0 belief: {'A': 1.0, 'B': 1.6421110937821813e-24}\nAgent 6 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 6 action: A\nAgent 6 belief: {'A': 1.0, 'B': 3.0161224171509453e-25}\nAgent 1 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 1 action: A\nAgent 1 belief: {'A': 1.0, 'B': 6.568444375128727e-24}\nAgent 7 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 7 action: A\nAgent 7 belief: {'A': 1.0, 'B': 6.568444375128727e-24}\nAgent 8 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 8 action: A\nAgent 8 belief: {'A': 1.0, 'B': 4.8257958674415124e-24}\nAgent 4 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 4 action: A\nAgent 4 belief: {'A': 1.0, 'B': 1.2064489668603783e-24}\n\n--- Step 9 ---\nAgent 9 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 9 action: A\nAgent 9 belief: {'A': 1.0, 'B': 2.127031739105971e-30}\nAgent 5 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 5 action: A\nAgent 5 belief: {'A': 1.0, 'B': 1.8528809816212014e-28}\nAgent 4 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 4 action: A\nAgent 4 belief: {'A': 1.0, 'B': 1.373076307676699e-27}\nAgent 2 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 2 action: A\nAgent 2 belief: {'A': 1.0, 'B': 2.990255070051478e-26}\nAgent 1 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 1 action: A\nAgent 1 belief: {'A': 1.0, 'B': 1.373076307676699e-27}\nAgent 6 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 6 action: A\nAgent 6 belief: {'A': 1.0, 'B': 6.304942229127699e-29}\nAgent 8 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 8 action: A\nAgent 8 belief: {'A': 1.0, 'B': 1.0087907566604319e-27}\nAgent 0 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 0 action: A\nAgent 0 belief: {'A': 1.0, 'B': 3.4326907691917473e-28}\nAgent 7 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 7 action: A\nAgent 7 belief: {'A': 1.0, 'B': 1.373076307676699e-27}\nAgent 3 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 3 action: A\nAgent 3 belief: {'A': 1.0, 'B': 6.304942229127699e-29}\n\n--- Step 10 ---\nAgent 8 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 8 action: A\nAgent 8 belief: {'A': 1.0, 'B': 2.10878955239161e-31}\nAgent 2 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 2 action: A\nAgent 2 belief: {'A': 1.0, 'B': 6.2508687843114266e-30}\nAgent 3 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 3 action: A\nAgent 3 belief: {'A': 1.0, 'B': 1.3179934702447563e-32}\nAgent 6 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 6 action: A\nAgent 6 belief: {'A': 1.0, 'B': 1.3179934702447563e-32}\nAgent 7 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 7 action: A\nAgent 7 belief: {'A': 1.0, 'B': 2.8702968907552474e-31}\nAgent 1 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 1 action: A\nAgent 1 belief: {'A': 1.0, 'B': 2.8702968907552474e-31}\nAgent 4 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 4 action: A\nAgent 4 belief: {'A': 1.0, 'B': 2.8702968907552474e-31}\nAgent 9 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 9 action: A\nAgent 9 belief: {'A': 1.0, 'B': 2.4208043331026137e-33}\nAgent 0 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 0 action: A\nAgent 0 belief: {'A': 1.0, 'B': 7.175742226888119e-32}\nAgent 5 observed actions: ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\nAgent 5 action: A\nAgent 5 belief: {'A': 1.0, 'B': 3.873286932964182e-32}\n                                               Belief\nStep AgentID                                         \n9    6         {'A': 1.0, 'B': 6.428685662431004e-36}\n     8        {'A': 1.0, 'B': 1.0285897059889607e-34}\n     0         {'A': 1.0, 'B': 3.500062193990214e-35}\n     7        {'A': 1.0, 'B': 1.4000248775960855e-34}\n     3         {'A': 1.0, 'B': 6.428685662431004e-36}\n```\n:::\n:::\n\n\n",
    "supporting": [
      "2024-06-01-Bayesian-Agents_files"
    ],
    "filters": [],
    "includes": {}
  }
}