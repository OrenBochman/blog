{
  "hash": "ab59ccddab38b938a104c32ce68173a9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-06-11\ntitle: \"Risk-constrained Markov decision processes\"\nsubtitle: \"paper review\"\ncategories: [\"paper review\", \"risk-constrained MDPs\", \"stochastic optimization\", \"CVaR\", \"decision-making under uncertainty\"]\n---\n\n\n\n\n::: {.callout-caution collapse=\"true\"}\n### TODO:\n\n1.  [x] get a copy of the paper\n2.  [x] look for talk in the paper - not found\n3.  [x] add a citation [@Borkar2010RiskconstrainedMD]\n4.  [x] review this paper\n5.  [ ] further work - perhaps [this paper](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Km1V8WwAAAAJ&citation_for_view=Km1V8WwAAAAJ:mB3voiENLucC)\n::: \n\n### Summary of \"Risk-Constrained Markov Decision Processes\" by [Vivek Borkar](https://en.wikipedia.org/wiki/Vivek_Borkar) and [Rahul Jain](https://scholar.google.com/citations?user=NIj18UQAAAAJ&hl=en&oi=sra)\n\n**Abstract:** The paper introduces a new framework for constrained Markov decision processes (MDPs) with risk-type constraints, specifically using Conditional Value-at-Risk (CVaR) as the risk metric. It proposes an offline iterative algorithm to find the optimal risk-constrained control policy and sketches a stochastic approximation-based learning variant, proving its convergence to the optimal policy.\n\n**Key Concepts:**\n\n1.  **Constrained Markov Decision Processes (CMDPs):**\n\n    -   CMDPs extend MDPs to include constraints, which can be challenging due to the complexity of handling multiple stages and constraints that have different forms.\n\n    -   Traditional methods often fail when constraints involve conditional expectations or probabilities.\n\n2.  **Risk Measures:**\n\n    -   CVaR is used instead of the traditional Value-at-Risk (VaR) because it is a coherent risk measure and convex, making it suitable for optimization.\n\n    -   CVaR measures the expected loss given that a loss exceeds a certain value, thus addressing the shortcomings of VaR.\n\n3.  **Problem Formulation:**\n\n    -   The objective is to maximize the expected total reward over a finite time horizon while ensuring that the CVaR of the total cost remains bounded.\n\n    -   This is particularly relevant for decision-making problems where risk management is crucial, such as finance and reinsurance.\n\n4.  **Algorithmic Solution:**\n\n    -   An offline iterative algorithm is proposed to solve the risk-constrained MDP (rMDP) problem.\n\n    -   The algorithm operates in multiple time scales, adjusting dual variables and iterating until convergence.\n\n    -   A proof of convergence for the proposed algorithm under certain conditions is provided.\n\n5.  **Online Learning Algorithm:**\n\n    -   An online learning variant of the algorithm uses stochastic approximation to find the optimal control policy in a sample-based manner.\n\n    -   The convergence of the online algorithm is also established, ensuring it behaves similarly to the offline algorithm.\n\n6.  **Applications and Relevance:**\n\n    -   The framework is useful in areas where decisions must be made under uncertainty with potential catastrophic risks, such as power systems with renewable energy integration and financial risk management.\n\n    -   The paper aims to stimulate further research in risk-constrained MDPs, offering a new approach to handling risk in dynamic decision-making problems.\n\nThe two algorithms are:\n\n## Offline Iterative Algorithm (iRMDP)\n\n\n\n\n``` {pseudocode}\n#| label: offline-iterative-algorithm\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n\\begin{algorithm}\n\\caption{iRMDP: Offline Iterative Algorithm}\n\\begin{algorithmic}[1]\n\\STATE Initialize $\\lambda^0, $\\beta^0$\n\\FOR{$m = 1, 2, \\ldots$ until convergence}\n    \\FOR{$n = 1, 2, \\ldots$ until convergence}\n        \\FOR{$t = T, \\ldots, 0$}:\n            \\STATE $J_{t}^{n,m}(x, y) = \\max_{u} \\left( r(x, u) + \\int \\int p(dx'|x, u) J_{t+1}^{n,m}(x', y + c(x, u) + s) \\phi(s) ds \\right)$\n            \\STATE $u_{t}^{n,m}(z) \\in \\arg\\max_{u} \\left( r(x, u) + \\int \\int p(dx'|x, u) J_{t+1}^{n,m}(x', y + c(x, u) + s) \\phi(s) ds \\right)$\n            \\STATE $V_{t}^{n,m}(z) = \\int p(dz'|z, u_{t}^{n,m}(z)) V_{t+1}^{n,m}(z')$\n            \\STATE $Q_{t}^{m}(z) = \\frac{c(z) V_{t}^{n,m}(z)}{\\alpha} + \\int p(dz'|z, u_{t}^{n,m}(z)) Q_{t+1}^{m}(z')$\n        \\ENDFOR\n        \\STATE Update $\\beta_{n+1}^{m}$: $ \\beta_{n+1}^{m} = \\beta_{n}^{m} - \\gamma_{n} (\\alpha - V_{0}^{n,m}(z_0))$\n    \\ENDFOR\n    \\STATE Update $\\lambda_{m+1} = \\left( \\lambda_{m} - \\eta_{m} (C_{\\alpha} - Q_{0}^{m}(z_0)) \\right)^+$\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n\n\n\n## Online Learning Algorithm (oRMDP)\n\n``` pseudocode\n#| label: online-iterative-algorithm\n#| html-indent-size: \"1.2em\"\n#| html-comment-delimiter: \"#\"\n#| html-line-number: true\n#| html-line-number-punc: \":\"\n#| html-no-end: false\n\n\\begin{algorithm}\n\\caption{oRMDP: Online Learning Algorithm}\n\\begin{algorithmic}[1]\n\n\\STATE Initialize $\\lambda^0$, $\\beta^0$\n\\FOR{$k = 1, 2, \\ldots$}\n    \\FOR{$t = T, T-1, \\ldots, 0$}\n      \\STATE $J_{t}^k(x, y, u)= J_{t}^{k-1}(x, y, u) + a_k I\\{X_t^k = x, Y_t^k = y, u_t^k = u\\} \\left( r(x, u) + \\max_{u'} J_{t+1}^k(X_{t+1}^k, Y_{t+1}^k, u') - J_{t}^{k-1}(x, y, u) \\right)$\n      \\STATE $u_t^k = v_{t+1}^k = \\arg\\max J_{t}^k(X_t^k, Y_t^k, \\cdot)$\n      \\STATE $V_{t}^k(z) = V_{t}^{k-1}(z) + a_k I\\{Z_t^k = z\\} (V_{t+1}^k(Z_{t+1}^k) - V_{t}^{k-1}(z))$\n      \\STATE $Q_{t}^k(z) = Q_{t}^{k-1}(z) + a_k I\\{Z_t^k = z\\} \\left( V_{t}^k(z) c(z) + Q_{t+1}^k(Z_{t+1}^k) - Q_{t}^{k-1}(z) \\right)$\n    \\ENDFOR\n    \\STATE $\\beta^k = \\beta^{k-1} - \\gamma_k (\\alpha - V_{0}^k(z_0))$\n    \\STATE $\\lambda^k = \\left( \\lambda^{k-1} - \\eta_k (C_{\\alpha} - Q_{0}^k(z_0)) \\right)^+$\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n\n### Conclusion:\n\nThe paper contributes to the field of stochastic optimization by addressing the gap in handling risk constraints in MDPs. It presents a robust algorithmic solution and lays the groundwork for future research in risk-constrained decision-making processes.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}