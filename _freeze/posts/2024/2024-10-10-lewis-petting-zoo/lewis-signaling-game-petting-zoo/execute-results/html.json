{
  "hash": "6395cf9bb6fc87834ba7bef229058b67",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lewis Signaling Game for PettingZoo\"\nsubtitle: \"Paper Review\"\ndate: 2025-01-01\ncategories: [review,compositionality,neural networks,signaling systems,language evolution]\nkeywords: \n    compositionality\n    naive compositionality\n    language emergence\n    deep learning\n    neural networks\n    signaling systems \n    emergent languages\n    topographic similarity\n    positional disentanglement\n    bag-of-symbols disentanglement\n    information gap disentanglement    \nbibliography: ./bibliography.bib\n---\n\n::: {#52f8fe3f .cell execution_count=1}\n``` {.python .cell-code}\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nfrom pettingzoo.utils.env import AECEnv\nfrom pettingzoo.utils import wrappers\nfrom pettingzoo.utils.agent_selector import agent_selector\nimport pettingzoo\n\nclass Sender:\n    def __init__(self, num_signals):\n        self.num_signals = num_signals\n        self.action_space = spaces.Discrete(num_signals)\n\n    def act(self, observation):\n        return self.action_space.sample()\n\nclass Receiver:\n    def __init__(self, num_states):\n        self.num_states = num_states\n        self.action_space = spaces.Discrete(num_states)\n\n    def act(self, observation):\n        if observation == self.num_states:\n            return self.action_space.sample()\n        else:\n            return observation\n\nclass LewisSignalingEnv(AECEnv):\n    metadata = {\"render_modes\": [\"human\"], \"name\": \"lewis_signaling_v0\"}\n\n    def __init__(self, num_signals=3, num_states=3, max_cycles=100,debug=False):\n        super().__init__()\n        self.debug = debug\n        self.possible_agents = [\"sender\", \"receiver\"]\n        self.agent_name_mapping = dict(zip(self.possible_agents, list(range(len(self.possible_agents)))))\n        self.num_signals = num_signals\n        self.num_states = num_states\n        self.max_cycles = max_cycles\n        self.sender = Sender(num_signals)\n        self.receiver = Receiver(num_states)\n        self.state = None\n        self.signal = None\n        self.cycles = 0\n\n        self.observation_spaces = {\n            \"sender\": spaces.Discrete(1),\n            \"receiver\": spaces.Discrete(self.num_signals + 1)\n        }\n        self.action_spaces = {\n            \"sender\": self.sender.action_space,\n            \"receiver\": self.receiver.action_space,\n        }\n\n    def observation_space(self, agent):\n        return self.observation_spaces[agent]\n\n    def action_space(self, agent):\n        return self.action_spaces[agent]\n\n    def observe(self, agent):\n        if agent == \"sender\":\n            return 0\n        elif agent == \"receiver\":\n            if self.signal is None:\n                return self.num_signals\n            else:\n                return self.signal\n        else:\n            raise ValueError(f\"Unknown agent: {agent}\")\n\n    def reset(self, seed=None, options=None):\n        self.agents = self.possible_agents[:]\n        self.rewards = {agent: 0 for agent in self.agents}\n        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n        self.terminations = {agent: False for agent in self.agents}\n        self.truncations = {agent: False for agent in self.agents}\n        self.infos = {agent: {} for agent in self.agents}\n        self.state = np.random.randint(self.num_states)\n        self.signal = None\n        self.cycles = 0\n        self._agent_selector = agent_selector(self.agents)\n        self.agent_selection = self._agent_selector.next()\n        self._clear_rewards() # Clear rewards in reset\n        return {agent: self.observe(agent) for agent in self.agents}\n\n\n    def step(self, action):\n        \n        if self.terminations[\"sender\"] or self.terminations[\"receiver\"]:\n            return\n\n        current_agent = self.agent_selection\n\n        if current_agent == \"sender\":\n\n            self.signal = action\n            if self.debug:\n                print(f\"Sender sent signal: {self.signal}, State: {self.state}\")\n            self.agent_selection = self._agent_selector.next()\n            return\n\n        elif current_agent == \"receiver\":\n            reward = 0\n            guess = action\n            if self.debug:\n                print(f\"Receiver guessed: {guess}, State: {self.state}, Signal: {self.signal}\")\n            if guess == self.state:\n                reward = 1\n                if self.debug:\n                    print(f\"Reward assigned: {reward}\")                \n            else:\n                reward = 0\n\n            for agent in self.agents:\n                self.rewards[agent] = reward\n                self._cumulative_rewards[agent] += self.rewards[agent]\n\n            if self._agent_selector.is_last():\n                self.cycles += 1\n                if self.cycles >= self.max_cycles:\n                    for agent in self.agents:\n                        self.truncations[agent] = True\n                self.state = np.random.randint(self.num_states)\n                self._agent_selector.reinit(self.agents)\n            else:\n                self.agent_selection = self._agent_selector.next()\n\n\n    def _clear_rewards(self):\n        #print(\"Clearing rewards\")  # Print when rewards are cleared\n        super()._clear_rewards()\n\n    def close(self):\n        if hasattr(self, \"_agent_selector\"):\n            del self._agent_selector\n        pass\n\ndef env(**kwargs):\n    env = LewisSignalingEnv(**kwargs)\n    if pettingzoo.__version__ >= \"1.18.1\":\n        env = wrappers.OrderEnforcingWrapper(env)\n    else:\n        env = wrappers.order_enforcing(env)\n    env = wrappers.AssertOutOfBoundsWrapper(env)\n    return env\n\n# --- Main execution in the notebook ---\nnum_episodes = 10\nmean_rewards = {\"sender\": 0, \"receiver\": 0}\n\nenv_instance = env(num_signals=3, num_states=3, max_cycles=10) # Reduced max cycles for faster testing\n\nfor episode in range(num_episodes):\n    observations = env_instance.reset()\n    unwrapped_env = env_instance.unwrapped\n    print(f\"Starting episode {episode+1}, New State: {unwrapped_env.state}\")\n    for agent in env_instance.agent_iter():\n        observation, reward, termination, truncation, info = env_instance.last()\n        if termination or truncation:\n            break\n        if agent == \"sender\":\n            action = env_instance.sender.act(observation)\n        elif agent == \"receiver\":\n            action = env_instance.receiver.act(observation)\n        env_instance.step(action)\n\n    # Calculate mean rewards AFTER the episode:\n    for agent in env_instance.possible_agents:\n        mean_rewards[agent] += env_instance.rewards[agent]\n\nfor agent in env_instance.possible_agents:\n    mean_rewards[agent] /= num_episodes\nprint(f\"Mean rewards over {num_episodes} episodes:\")\nprint(f\"Sender: {mean_rewards['sender']}\")\nprint(f\"Receiver: {mean_rewards['receiver']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStarting episode 1, New State: 0\nStarting episode 2, New State: 0\nStarting episode 3, New State: 0\nStarting episode 4, New State: 2\nStarting episode 5, New State: 0\nStarting episode 6, New State: 0\nStarting episode 7, New State: 1\nStarting episode 8, New State: 1\nStarting episode 9, New State: 2\nStarting episode 10, New State: 1\nMean rewards over 10 episodes:\nSender: 0.7\nReceiver: 0.7\n```\n:::\n:::\n\n\nThe above is a basic version of the Lewis Signaling Game implemented in PettingZoo. The game consists of a sender and one or more receivers. \n\nWhat would be nice is to:\n\n1. have agents that learn via various algorithms\n    1. Herrnstein\n    $$\n    \\pi(a) \\leftarrow \\frac{\\pi(a)}{\\pi(a)+\\pi(\\bar{a})}=\\frac{R(a)}{R(a)+R(\\bar{a})}\n    $$\n    Note: Herrnstein considered just two possible actions.\n    2. Roth–Erev reinforcement (Has a Goldilocks property) similar a softmax policy with a linear preference.\n    $$\n    \\begin{align}\n    h'(a) & \\leftarrow \\alpha h(a) + \\mathbb{1}_{a\\ taken} r \\\\\n    \\pi(a) & \\leftarrow \\frac{e^{h(a)/\\tau}}{\\sum_{a'} e^{h(a')/\\tau}}\n    \\end{align}\n    $$\n    note: I re-interpreted A the update attraction $A$ as the preference $h$, and $\\psi$ the forgetting/recency parameter as $\\alpha$ a learning rate as they are used as what goes into a  Softmax which is parameterized by a preference in policy gradient methods.\n    3. ARP - Adaptive Reinforcement Pursuit by Yoella Bereby-Meyer and Ido Erev\n    $$\n    \\begin{align}\n    h'(a) & \\leftarrow \\alpha h(a) + \\mathbb{1}_{a\\ taken} r \\\\\n    \\pi(a) & \\leftarrow \\frac{e^{\\beta h(a)/\\tau}}{\\sum_{a'} e^{\\beta h(a')/\\tau}}\n    \\end{align}    \n    $$\n    Note here we add $\\beta$ which can be the average reward or regret.\n    3. Bush–Mosteller Reinforcement similar to policy gradient with linear reward function : \n    \n    $$\n    \\pi'(a) \\leftarrow \\pi(a) + \\alpha[\\mathbb{1}_{a\\ taken} R - \\pi(a)]  \n    $$\n\n    3. Bochman fastest coordination\n    4. Bochman belief based coordination\n    5. Bochman adaptive huffman coding coordination\n    6. Bochman adaptive arithmetic coding coordination  \n    7. Tabular Monte Carlo RL\n    8. Policy Gradient or Gradient Bandit\n1. expected return metrics for the signaling system \n1. entropy metrics for the signaling system\n1. topographic similarity metrics for the signaling system\n1. positional disentanglement metrics for the signaling system\n1. bag-of-symbols disentanglement metrics for the signaling system\n1. learning rate per cycle \n1. learning rather per state space size\n1. state space generators + distribution for states.    \n    1. simple - \n    1. structured - group action for feature morphology\n    1. structured and simple (generate atomic states, then combinations)\n    1. trees - atoms and trees of atoms based on a one rule grammar.\n    1. problem space - states and actions from an MDP.    \n1. have multiple recievers that share information to speed up learning\n1. support for injecting errors in communication\n1. support for injecting risks into communication\n1. suport for different signal aggregation functions.\n    1. bad of symbols\n    1. sequence of symbols\n    1. symbol parse trees ??\n    1. DAGs ????\n    1. custom - user defined\n\n",
    "supporting": [
      "lewis-signaling-game-petting-zoo_files"
    ],
    "filters": [],
    "includes": {}
  }
}