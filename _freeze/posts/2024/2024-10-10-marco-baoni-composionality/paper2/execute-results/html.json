{
  "hash": "75d8e58b87513d63ac1f1bd40db0b229",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Compositionality and Generalization in Emergent Languages\"\nsubtitle: \"Paper Review\"\ndate: 2025-01-01\ncategories: [review,compositionality,neural networks,signaling systems,language evolution]\nkeywords: \n    compositionality\n    naive compositionality\n    language emergence\n    deep learning\n    neural networks\n    signaling systems \n    emergent languages\n    topographic similarity\n    positional disentanglement\n    bag-of-symbols disentanglement\n    information gap disentanglement    \nbibliography: ./bibliography.bib\n---\n\n\n\n\n\n## Review of \"Compositionality and Generalization in Emergent Languages\"\n\nVery exciting - this is a paper with a lot of interesting ideas. It comes with a a lot of code in the form of a library called EGG as well as many JuPyteR notebooks. There is also a video of the talk at NeurIPS 2020.\n\nIn [@chaabouni-etal-2020-compositionality] the authors look at ideas from representation learning and apply them to emergent languages in deep networks. THey come up with a number of results.\n\n\n## Abstract\n\n> Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages [inspired by disentanglement in representation learning]{.mark}, we establish three main results. \n>\n>First, [given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts.]{.mark} \n>\n>Second, there is [no correlation between the degree of compositionality of an emergent language and its ability to generalize]{.mark}. \n>\n>Third, [while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission]{.mark}: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive \n>\n> --- [@chaabouni-etal-2020-compositionality]\n\n\n## Outline\n\nHere is the outline of the paper:\n\n### Introduction\n- Describes a variant of Lewis signaling game used to study the emergence of reference to composite concepts in deep multi-agent simulations.\n- Discusses two specific and intuitive compositionality strategies that capture common compositional structures in natural languages.     \n- Introduces two new compositionality measures, positional disentanglement (posdis) and bag-of-symbols disentanglement (bosdis), inspired by work on disentanglement in representation learning.\n\n### Measurements\n\n- Describes the commonly used **topographic similarity** (topsim) metric.\n- Introduces and defines two new measures of compositionality: \n    - posdis - **positional disentanglement** and \n    - bosdis - **bag-of-symbols disentanglement**.\n- Explains how the new measures are similar to the **Information Gap disentanglement measure** used in representation learning.\n- Illustrates the behavior of the three compositionality metrics on three miniature languages in the Appendix.\n\n\n::: {#note-topsim .callout-note}\n \n## Topographic Similarity\n\n> Given these two lists, the topographic similarity is defined as their negative Spearman ρ correlation (since we are correlating distances with similarities, negative values of correlation indicate topographic similarity of the two spaces). [Intuitively, if similar objects share much of the message structure (e.g., common prefixes or suffixes), and dissimilar objects have little common structure in their respective messages, then the topographic similarity should be high]{.mark}, the highest possible value being 1 -- [@lazaridou2018emergence]\n\n$$\n\\mathit{topsim}=\\rho\\left(\\left\\{d\\left(\\mathbf{x}^{(i)}, \\mathbf{x}^{(j)}\\right), d\\left(\\mathbf{m}^{(i)}, \\mathbf{m}^{(j)}\\right)\\right\\}_{i, j=1}^{n}\\right)\n$$\n\n:::\n\n::: {#note-posdis .callout-note}\n \n## Positional Disentanglement\n\n> **positional disentanglement** (posdis) metric measures whether symbols in specific positions tend to univocally refer to the values of a specific attribute. This order-dependent strategy is commonly encountered in natural language structures (and it is a pre-condition for sophisticated syntactic\nstructures to emerge) -- [@chaabouni-etal-2020-compositionality]\n\n$$\n\\mathit{posdis}=\\frac{1}{c_{len}} \\sum_{j=1}^{c_{len}} \\frac{\\mathcal{I}(s_j,a^j_1)-\\mathcal{I}(s_j,a^j_2)}{\\mathcal{H}(s_j)} \n$$\n\n- $s_j$ the j^th^ symbol of a message and \n- $a^j_1$ the attribute that has the highest mutual information with $s_j : a^j_1 = arg max_a \\mathcal{I}(s_j ; a)$\n- $a^j_2$ the attribute that has the second highest mutual information with $s_j : a^j_2 = arg max_{a \\neq a^j_1} \\mathcal{I}(s_j ; a)$\n- $\\mathcal{H}(s_j)$ the entropy of j-th position (used as a normalizing term)\n\npositions with zero entropy are ignored in the computation.\n:::\n\n::: {#note-bosdis .callout-note}\n \n## Bag-of-symbols Disentanglement\n\n> Posdis assumes that a language uses positional information to disambiguate symbols. However, we can easily imagine a language where symbols\nunivocally refer to distinct input elements independently of where they occur, making order irrelevant.3 Hence, we also introduce **bag-of-symbols disentanglement** (bosdis). The latter maintains the requirement for symbols to univocally refer to distinct meanings, but captures the intuition of a permutation-invariant language, where only symbol counts are informative -- [@chaabouni-etal-2020-compositionality]\n\n\n$$\n\\mathit{bodis}=\\frac{1}{c_{voc}} \\sum_{j=1}^{c_{voc}} \\frac{\\mathcal{I}(n_j,a^j_1)-\\mathcal{I}(n_j,a^j_2)}{\\mathcal{H}(n_j)} \n$$\n\n- $n_j$ a counter of the j-th symbol in a message\n\n:::\n\n### Generalization Emerges “Naturally” if the Input Space is Large\n- Presents an experiment showing that emergent languages are able to generalize to unseen combinations as long as input size is sufficiently large.\n- Discusses how the results challenge claims in the recent literature that deep networks fail to generalize.\n- Notes that the minimum channel capacity required for the emergence of a generalizing language is significantly larger than the minimum channel capacity required for a perfectly compositional language.\n- Presents additional experiments in the Appendix analyzing the effects of agent capacity and input density on generalization.\n\n### Generalization Does Not Require Compositionality\n\n- Presents results showing that there is no correlation between compositionality and generalization ability.\n- Analyzes the language of a specific run with near-perfect generalization accuracy and medium posdis score.\n- Discusses how the analyzed language uses a \"leaky disentanglement\" strategy where two positions largely specialize as predictors of two attributes, respectively, but a third more entangled position is still necessary for perfect communication.\n- Briefly analyzes in the Appendix a language with near-perfect generalization accuracy and very low posdis score.\n\n### Compositionality and Ease of Transmission\n\n- Discusses the hypothesis that compositional languages are easier to decode and transmit to new learners.\n- Presents an experiment where new Receivers are trained on frozen Senders that achieved a high level of generalization accuracy.\n- Finds that learning speed and generalization accuracy of new Receivers are strongly positively correlated with degree of compositionality.\n- Mentions further experiments in the Appendix that replicate the ease-of-transmission analysis across various channel capacities.\n\n### Discussion\n\n- Summarizes the main findings of the paper, highlighting the results that challenge common assumptions in the emergent language literature.\n- Relates the findings to the ongoing debate on the origins of compositionality in natural language.\n- Discusses the potential benefits of compositionality for developing languages that are quickly usable by wide communities of artificial agents.\n- Highlights the connection between compositionality and disentanglement in representation learning.\n\n\n## My Thoughts\n\nWe have three main results:\n\n1. **Generalization Emerges “Naturally” if the Input Space is Large**\n2. **No Correlation Between Compositionality and Generalization**\n3. **Compositionality increases Ease of Transmission**\n\nFrom a simple analysis of the lewis signaling game. The cost of coordination for an non compositional language is exponential in the number of states.\nFor a compositional language the cost can reduced an exponential of the size of the lexicon (number of atomic signals) plus some constant factor for learning the grammar if it is some small set of  aggregation rules. (Usually one rule is sufficient to support compositionality).\n\nThe lewis signaling game is more or less guaranteed to converge to some a signaling system^[one that has an expected payoff of 1] with the suitable algorithm.\nWithout a good algorithm the game is more likely to converge to a partial pooling equilibrium where payoffs are less than 1 due to one side or both being unable to conflate different states.\n\nWhat this means is that all things being equal the compositional language will emerge much sooner the the non compositional language given the oopportunity to do so. So why don't we see this. \n\n1. When the number of states isn't larger than the number of basic signals it is easier to learn a non compositional language. So we should put a cap on the number of basic signals.\n2. The multiplier for learning grammar may be big especially if we use a neural network, more so if the grammar is complicates.  So if we don't see compositionality perhaps we need to make the grammar simpler or the state space bigger.\n3. This is theoretical - perhaps since we use a neural network rather then a tabular RL solution we need lots of data to learn anything. As we go though the epocs there may be enough rounds in the lewis game for use to establish a convetion without the need for compositionality.\n4. Ok Let's say we have a fast learning algorithm and a big but managable input space. Do we get compositionality. The answer is not necessarily. \n\nLet dive deeper into this last point:\n\nDespite what Chat GPT will tell you if you ask the lewis signaling game only outputs simple languages. No complex signals, no grammar, no recursion and no compositionality. It has many possible equilibria and none correspond to a language complex signals or grammar. \n\nFor grammer and complex signals to emerge you need to tweak the lewis signaling game. [@skyrms2010signals] reports on a couple of papers which produce complex signals. Most of the iteresting work came out after the book. Regardless in the papers the agents were given a simple aggregation rule to follow. The conjuction leads to a bag of words. The concatenation leads to sequences. But what they don't seem to stress is that for a complex signals we want a state that decomposes into a way we can match in our aggregation rule. Think group homomorphism. And there may be multiple decompositions so think normal subgroups.\n\nThere isn't a how to guide to get the agents to use some arbitrary grammar. (Not AFAIK). There are a bunch of books and many papers but they don't seem to have a the kind of recepies that are needed here. In my view most of these books look for the answers based on what they know rather what they need to know. They may have fascinating points but lead to greater confusion rather then more clarity.\n\nOne abstraction I came across is that the notion of a grammar is essentialy a decision tree mapping the signals back into the state. Decision trees sounds simple enough but this tree is not given but needs to be learned by trial and error. Signals due to sucesses are sparse. \nThere might be a setting in which a sender can construct the tree and then the reciever just needs to learn it. But it requires the sender to have access to the distribution of states and sub states. This distribution can be used to come up with a tree that is optimal for a given set of signals. \nIf the sender and receiver don't have access to this distribution the can learn it. But my work indicates that to learn the distribution to a high degree of accuracy requires more turns then my lewis signaling algorithm does. At least for simple signaling systems.\n\nFor a simple signaling system I developed two algorithms. THe first learened a signaling system. The second first enumerated all signaling systems of a certain size and then selected one from those it believed were optimal. Each new state would reduce the belief until the sender and reciever had a common belief. This may not scale but can adapt to new distributions seemlessly. I thought about a similar approch for working with complex grammars. But In this case I did not have an efficent way to enumerate all possible grammars. However there seems to be a way to do this. Instead of considering all decision trees, we can instead consider just huffman trees. These means that the sender and reciever use the lewis signaling game to learn a shared huffman tree. The outcome is that the tree should compress the state space. The only problem is that such a grammar is not likely to be compositional and would be very difficult to learn for humans.\n\nSo what we need is for the agents to learn the tree interactively. Two approaches come to mind and these are \n1. huffman coding - which builds the tree but doesn't update it to account for distributional shits.\n2. Vitter algorithm for adaptive huffman coding - which updates the tree as new states are seen. THis is\n3. adaptive arithmetic coding - which is a generalization of adaptive huffman coding.\n\nOne point to consider is that such a grammar is likely to provide a good compression of the state space. This is due to the these algorithms also being compression algorithms.\n\nI would imagine that the output of such a grammar to be a binary sequence. This suggest that this would lead to a entangled representation with no discernable compositional structure. \n\nNow there are reputedly many languages with very simple grammars. But the ones we are familiar with are not simple. They also  have large lexicons. We need to put that aside and look for ways to work with simple grammars. It is quite possible to come up with two or three rules that can generate both morphology and a recursive syntax. It might be possible with one rule.\n\n\nOk lets briefly consider the other two points.\n\n## The paper\n\n![Compositionality and Generalization in Emergent Languages](./paper2.pdf){.col-page width=800px height=1000px}\n\n\nthere is also a video at \n\n[video](https://slideslive.com/38928781/compositionality-and-generalization-in-emergent-languages)\n\n## The code\n\nand code at\n\n[papers with code](https://paperswithcode.com/paper/compositionality-and-generalization-in)\n\nand even the colab link to\n\n[EGG walkthrough](https://colab.research.google.com/github/facebookresearch/EGG/blob/main/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb)\n\n::: {#a2a889cf .cell execution_count=1}\n``` {.python .cell-code}\nimport heapq\n\nclass Node:\n    def __init__(self, char, freq):\n        self.char = char\n        self.freq = freq\n        self.left = None\n        self.right = None\n\n    def __lt__(self, other):\n        return self.freq < other.freq\n\ndef build_huffman_tree(chars_freq):\n    \"\"\"\n    Builds the Huffman tree for given character frequencies.\n\n    Args:\n        chars_freq: A dictionary of characters and their frequencies.\n\n    Returns:\n        The root of the Huffman tree.\n    \"\"\"\n    nodes = []\n    for char, freq in chars_freq.items():\n        heapq.heappush(nodes, Node(char, freq))\n\n    while len(nodes) > 1:\n        left = heapq.heappop(nodes)\n        right = heapq.heappop(nodes)\n        parent = Node(None, left.freq + right.freq)\n        parent.left = left\n        parent.right = right\n        heapq.heappush(nodes, parent)\n\n    return nodes[0]\n\ndef update_huffman_tree(root, updated_freqs):\n    \"\"\"\n    Updates the Huffman tree with new character frequencies.\n\n    Args:\n        root: The root of the current Huffman tree.\n        updated_freqs: A dictionary of characters and their updated frequencies.\n\n    Returns:\n        The root of the updated Huffman tree.\n    \"\"\"\n    # 1. Extract leaf nodes and their frequencies\n    leaf_nodes = []\n    def get_leaf_nodes(node):\n        if node is None:\n            return\n        if node.char is not None:\n            leaf_nodes.append((node, node.freq))\n        get_leaf_nodes(node.left)\n        get_leaf_nodes(node.right)\n    get_leaf_nodes(root)\n\n    # 2. Update frequencies of leaf nodes\n    for node, old_freq in leaf_nodes:\n        new_freq = updated_freqs.get(node.char, old_freq)  # Use old freq if not updated\n        node.freq = new_freq\n\n    # 3. Rebuild the Huffman tree\n    return build_huffman_tree({node.char: node.freq for node, _ in leaf_nodes})\n\n\ndef encode_char(root, char, code=''):\n    \"\"\"\n    Encodes a character using Huffman codes.\n\n    Args:\n        root: The root of the Huffman tree.\n        char: The character to encode.\n        code: The current code (initially empty).\n\n    Returns:\n        The Huffman code for the character.\n    \"\"\"\n    if root is None:\n        return ''\n\n    if root.char == char:\n        return code\n\n    left_code = encode_char(root.left, char, code + '0')\n    if left_code != '':\n        return left_code\n\n    right_code = encode_char(root.right, char, code + '1')\n    return right_code\n\ndef decode_char(root, code):\n    \"\"\"\n    Decodes a Huffman code to get the character.\n\n    Args:\n        root: The root of the Huffman tree.\n        code: The Huffman code to decode.\n\n    Returns:\n        The decoded character.\n    \"\"\"\n    current = root\n    for bit in code:\n        if bit == '0':\n            current = current.left\n        else:\n            current = current.right\n\n    if current.char is not None:\n        return current.char\n\ndef encode_message(root, message):\n    \"\"\"\n    Encodes a message using Huffman codes.\n\n    Args:\n        root: The root of the Huffman tree.\n        message: The message to encode.\n\n    Returns:\n        The encoded message.\n    \"\"\"\n    encoded_message = ''\n    for char in message:\n        encoded_message += encode_char(root, char)\n    return encoded_message\n\ndef decode_message(root, encoded_message):\n    \"\"\"\n    Decodes a Huffman-encoded message.\n\n    Args:\n        root: The root of the Huffman tree.\n        encoded_message: The encoded message.\n\n    Returns:\n        The decoded message.\n    \"\"\"\n    decoded_message = ''\n    current = root\n    for bit in encoded_message:\n        if bit == '0':\n            current = current.left\n        else:\n            current = current.right\n\n        if current.char is not None:\n            decoded_message += current.char\n            current = root\n\n    return decoded_message\n\n\ndef print_tree(node, level=0):\n    \"\"\"\n    Prints the Huffman tree in a visually appealing format.\n    \"\"\"\n    if node is None:\n        return\n\n    print_tree(node.right, level + 1)\n    print(\" \" * 4 * level + f\"[{node.char or ''}: {node.freq}]\")\n    print_tree(node.left, level + 1)\n\n```\n:::\n\n\n::: {#abcd3bdf .cell execution_count=2}\n``` {.python .cell-code}\n# Example usage\nchars_freq = {'a': 45, 'b': 13, 'c': 12, 'd': 10, 'e': 9, 'f': 5, 'g': 2, 'h':1}\nroot = build_huffman_tree(chars_freq)\nupdated_freqs = {'a': 45, 'b': 55, 'c': 12, 'd': 10, 'e': 9, 'f': 5, 'g': 2, 'h':1}\n\n#message = \"abcdef\"\ndef test(root, freqs=None):\n    if freqs is not None:\n        root = update_huffman_tree(root, freqs)    \n        print_tree(root)\n    for message in [\"a\", \"ab\" , \"aba\", \"abc\", \"abcd\"]:\n\n        print(\"Original message\", message)\n\n        encoded_message = encode_message(root, message)\n        print(\"Encoded message:\", encoded_message)\n\n        decoded_message = decode_message(root, encoded_message)\n        print(\"Decoded message:\", decoded_message)\n\nprint_tree(root)\ntest(root)\ntest(root, updated_freqs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                [e: 9]\n            [: 17]\n                    [f: 5]\n                [: 8]\n                        [g: 2]\n                    [: 3]\n                        [h: 1]\n        [: 30]\n            [b: 13]\n    [: 52]\n            [c: 12]\n        [: 22]\n            [d: 10]\n[: 97]\n    [a: 45]\nOriginal message a\nEncoded message: 0\nDecoded message: a\nOriginal message ab\nEncoded message: 0110\nDecoded message: ab\nOriginal message aba\nEncoded message: 01100\nDecoded message: aba\nOriginal message abc\nEncoded message: 0110101\nDecoded message: abc\nOriginal message abcd\nEncoded message: 0110101100\nDecoded message: abcd\n        [a: 45]\n    [: 84]\n                [c: 12]\n            [: 22]\n                [d: 10]\n        [: 39]\n                [e: 9]\n            [: 17]\n                    [f: 5]\n                [: 8]\n                        [g: 2]\n                    [: 3]\n                        [h: 1]\n[: 139]\n    [b: 55]\nOriginal message a\nEncoded message: 11\nDecoded message: a\nOriginal message ab\nEncoded message: 110\nDecoded message: ab\nOriginal message aba\nEncoded message: 11011\nDecoded message: aba\nOriginal message abc\nEncoded message: 1101011\nDecoded message: abc\nOriginal message abcd\nEncoded message: 11010111010\nDecoded message: abcd\n```\n:::\n:::\n\n\n",
    "supporting": [
      "paper2_files"
    ],
    "filters": [],
    "includes": {}
  }
}