{
  "hash": "3c276af349f5b53b89e82efe545acebc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-01-04\ntitle: \"Understanding Emergent Languages\"\ncategories:\n    - NLP\n    - language evolution\n    - emergent languages\nkeywords: \n    - compositionality\n    - naive compositionality\n    - language emergence\n    - deep learning\n    - neural networks\n    - signaling systems \n    - emergent languages\n    - topographic similarity\n    - positional disentanglement\n    - bag-of-symbols disentanglement\n    - information gap disentanglement    \nbibliography: ./bibliography.bib\ndraft: true\n---\n\n\n\n\n\nOkay after taking a break, today I've been rethinking Complex Lewis signaling systems and the equilibria.  I slow make progress in this area byc considering a number of research questions that seem trivial for a simple two-state two-signal Lewis signaling game but become important as we delve deeper into extensions in which complex systems emerge.\n\nI've made a bit of study of the original Lewis signaling game and built some intuition by reproducing some key results. I've developed urn based RL algorithms that are fast and Bayesian RL algorithms that are versatile for this tasks. I've spent a long time trying to figure out the complex games from a mathematical perspective. \n\nThere is a big hurdle between the original lewis signaling game and one that can generate a complex signaling system that is a first approximation of a natural language, or even a good representation of a formal language like first order logic. It certainly possible to design some variants that lead to specific complex signaling systems. But a more general solution seems elusive. In this article I've made another step towards finding this more general solution.\n\nRecently I also reviewed a number of talks on emergent languages. Considered the challenges they put forth and reviewed the main papers they were based on and read many others. \n\n::: {.callout-note}\n## TL-DR Emergent Languages In a Nutshell\n\n![Emergent Languages In a Nutshell](/images/in_the_nut_shell_coach_retouched.jpg)\n\nThis article is my first take on figuring out what modifications to the Lewis signaling game used in a number of research papers resulted in a emergent languages. By emergent languages we mean an equilibria of the modified Lewis signaling game that has been learned by the agents through reinforcement learning.\n\nWhy are these equilibria called **emergent languages** ?\n\nLet's start with **language**. Agent with a suitable RL algorithm [@cn] that play a Lewis signaling games will learning a Convention that is they will have shared lexicon which maps states to symbols and symbols to actions. This then allows the agents to interact by communicating successfully at every turn. In complex games though agent learn **a language** which  can be imperfect in the sense that one might not expected communication to succeed 100% of the time. Emergent languages may also develop more structure than just a lexicon snd this is the main themes of this and upcoming article. Next is the **emergence** part.\n\nIn [@de2011self] the authors discuss emergence in languages in terms of a self-organizing systems that operate through selection pressure, exploration, feedback, preferential attachment etc. \nThough as I pointed out with the lewis signaling game and decent RL algorithms a perfect signaling system is inevitable. So that calling such a system emergent needs to be justified and one should perhaps talk about emergent properties like compositionality.\n\nThis article takes a step back and reconsiders the what one should expect from a game modified to create a complex signaling system. With enough insight the process may look less like emergence and more like design or engineering. \n\nAt this point I am trying to find better research questions which slowly have been collecting in the next section. \n\nI then go over the main terminology and main ideas that are being discussed in the context of emergent languages. This helps clear up the intellectual landscape and will help us touch base for the rest of this article.\n\nNext I try to puzzle out how emergent languages have been generated through trial and error. And what is thier secret sauce. This is primarily an exercise in hypothesis generation.\n\nIt isn't really easy to figure out complex systems like the complex lewis signaling game in general. I've made some progress that way and noticed that some challenges from the simple game become advantages in the complex game. So at this point though I will consider some examples and try to take them apart to see if I can get at some general principles.\n\nSome of these are rathe long and will be broken up into smaller articles that cover experiments to check the hypothesis.\n\nFinally I think I can make a couple of hypothesis that go beyond understanding the mechanisms needed to make emergent languages. These are more in line with the design and engineering approach.\n\n:::\n\n\n\nWithin the complex games we are dealing with sequences of signals based on some alphabet. Here are a few of the research questions that raise their heads as we consider the complexity of the signaling systems:\n\n\n## Research questions {#sec-research-questions}\n\n\n### Semantics \n\nIn the simple game semantics (meaning) of signals are not easy to define. But the gist of it is that each symbols encodes an action that corresponds uniquely to some state. In nature the actions are real action and in the game they correspond to simply recognizing the original states. So the semantics of the simple games are captured by the lexicon. But how do they arrise ? There can be many ways (which I am exploring in a working paper) But let's consider the answer presented by David Lewis that semantics arbitrarily emerges though spontaneous symmetry breaking.\n\nLet's explain a bit. In a mathematical sense a signaling system with N states and symbols is one of any permutation permutation from stated to symbols and it inverse. These are premuations as a set from the symmetric group. Picking one is a form of symmetry breaking. But in the game we do this in steps. THis also figures with the fact that one can decompose a permutation into a product of transpositions. Each time we fix a transposition we are breaking a smaller symmetry. Eventually ending up with a fixed system.\n\nIn one sense we do the same for complex systems. But if are interested in a self organizing signaling system to emerge we only go so far with trial and error that arbitrary system breaking \nfollows.  Why is this tricky. Whatever system that will emerge even is highly organized and structured may have numerous degenerate systems that are isomorphic to it. In this sense nothing is different - we will still go though a series of symmetry breaking steps whose order is dictated by nature. But it seems there is a greater role for planning step by the sender to select a set of symbols using a mapping that preserves the structure of the states. This type of signaling system is a group homomorphism.\n\nSo we have three paradigms in mind for the creation of semantics in the complex signaling system. \n\n1. Spontaneous symmetry breaking - that learns a system that is just a lexicon.\n2. Self-organizing - a blind watch maker that can pick a structure that is a group homomorphism.\n3. Planning - a sender that can plan a signaling system that is a Normality-Preserving Homomorphism homomorphism.\n\n\nI.e. IF the semantics arise via self-organization using RL we may end up with systems that are do not correspond to different stages in the sequence of normal subgroups. And with a good algorithms we likely get the smallest symmetries most frequently and fastest and the largest symmetries least frequently and slowest. With planning the sender should be able to plan a signaling system that is a normality preserving homomorphism i.e. leads to a system that captures the structure of the states. \n\nOne point about self organization. \n\nI envision this in few ways.\n\nThe first is that sender may decide to swap the signal between states and jump closer to a homomorphic equilibrium. This can happen using epsilon greedy exploration and may be reinfoced over time if the outcome is better.\nA second is some sort of preferential attachment or rich get richer situation where the states are.\nUnevenly distributed and we use the initial bias to learn a system that has not just symmetry but a semantics to the symbols. \nOther ways for a system to self organize is to replicate structures though templates. These might arise from using priors in a hierarchial bayesian learning system.\n\nAnother way the system may self orgenize if there is a bias in the states that is reflected in the rewards. This bias may be reflected in the distribution of the states or in the rewards. If the states are distributed unevenly the system may self orgenize if rewards encourge shorter messages or messages built from shorter symbols (think morse code).\nOne way I envision this is that the signaling system is at first a 2x2 system. One system might arise spontaneously. Next each of these states becomes two states with a new symmetry. If learning is a hierarchial and bayesian the bias learned intially can be used to predispose learning to learn one systems with a symmetry (perhaps even if there isn't one) \nAnother ideas is if the states are dirtibuted unevenly, the system may self orgenize if rewards encourge shorter messages or messages built from shorter symbols (think morse code).\nThe sender sees this he might use the old system as a template for additional states via a template or some prior. This is a could be planning but it could also be though some contraint. A prior may be just a bias for listing the more comon substates first reusing the existing symbols in a sequence. This may lead to a systems thar leands itseslf\n\n4. Using templates - reusing existing structures and teaching/learning through analogy or using an established bias.\n\nAll these approaches can be incorporated into planning. Just that a planning agent may be able to simulate many states drawn from the distribution of states and seek out a signaling system that is a optimal for some criteria - faster learning though homomorphisms, better generalization, safer communication, error correction etc.\n\nGiven all that there could be further structures in the states that are not even captured by normal subgroups (i.e. not symmetries perhaps order and hierarchy) To capture these we may need better planning or to design rewards strucures that allow these to emerge naturally though self-orgenization\n\n\nHere are some more questions pn semantics?\n\n\n1. How do semantics arise for sequences?^[this is a natural question for anyone who has stated learning a new language that is from a different language family, and in this case we may be better equipped to answer it. I've outlined three mechanism there may be more and there may be additional subtleties.]\n\n    - Does **Simple lexicon^[semantics for atomic symbols, it kind of does in propositional calculus]** suffice ? \n    - Is an aggregation rule sufficient to create semantics? Does a simple lexicon help?\n    - What aggregation types lead to compositionality, entanglement and disentanglement of meaning?\n    - Is having a grammar^[rules imposing structural correctness on a sequence] sufficient to get a useful syntax sufficient?\n    - can we rely on syntax for semantics?\n    - if not how do we get a general framework for semantics?\n    - To what extents do pre-linguistic structure determine the ability of agents \n    - Will agents become great if they have greatness thrust upon them? i.e. If they get a nice signaling system early on will the be able to extend it or will it wither away^[c.f. erosion of the verb in [romance languages](https://en.wikipedia.org/wiki/Romance_verbs)]? ^[This seems more of an algorithmic question about how we reinforce sub-state coding in the algorithms that is can do generalization. c.f [section on planning](@sec-planning)]    \n1. Are there subset of sequences that are:\n    - Easier acquired by one or multiple generations of learners? ;\n    - Better suited for communication between agents ? Perhaps exhibiting greater resilience to errors, reducing risk for certain signal, handle saliency, able to compress sequence to make best use of the comms channel?\n    - More able to generalize to new or unseen states (moran process for the alphabet)\n    - a better match to represent the states.^[can we leverage representation theory to handle symmetries within the states?] \n    - easier to interpret/translate/transfer    \n1. Emergent languages may be subject to selection pressure when the Lewis game is composed with some external framing game. What choices are more conducive for agents to lean quickly and communicate effectively using **robust** learning . I.r. algorithms that lead to more stable language whose lexicon, grammar and semantic persist over time?\n    - Do we need to tinker with rewards structure to realign the incentives of the agents?^[can agents that are at odds evolve a signaling system or will deception lead to the collapse of the signaling system to perfectly pooling equilibria?]\n    - Can these be grounded within the external framing game? ^[will we get the full benefits of signaling systems in our framing game to re-shape the over-all equilibria]\n    - resilience to distributional shifts in the states distribution. (e.g. changes in the framing game)\n    - resilience to co-adaptation between agents. (persistence of the lexicon, grammar, and semantics).\n1. More crucially, what interventions can we make as designers can to encourage quick emergence of a signaling system that is conducive to perfect communication, fast learning and generalization?\n1. Can we impose additional structure on the sequences (e.g. a formal grammar that decides a sequence is well formed) Does this imbue the signaling system with additional desiderata?\n1.  Natural Languages develop between many agent and evolve over long time frames. (Hebrew students in primary school read the bible written in Hebrew thousands of years ago using just thier knowledge of modern hebrew.) What about having many senders and receivers speeds things up. I.e. what choices can we make as designers to leverage this.\n1. Can the the sender plan all this machinery in advance and hide it in the the sequences allowing the receiver to learn in the same way as the simple game. Can the receiver infer the machinery from the data? Are there other paths to learning. What if they need to make revisions can they handle those too?\n1. Can we locate set of states with symmetries where we have more more equilibria that preserve the structure of the states. \n   - Is subgroups enough or is there benefits to having normal subgroups?\n   - How can we quantify the amount of learning needed to learn a signaling system for a given set of states?\n   - How can we quantify the likelihood of reinforcement given set of states for signal/action action/signal pairs?\n   - Can we use s-vector semantics to aid in quantifying this.\n1. What can we say about basins of attraction and stability for equilibria in a lattice from maximally symmetric to minimally symmetric equilibria?\n1. Are there some some collections of states for whcih we can expect radicaly different equilibria to exist in the complex signaling system?\n\nSo I ended up with more questions then I bargained for. These are questions within question. These questions suggest new and intriguing desiderata for (complex) signaling systems as some novel  path for signaling and possibly new settings for learning them.\n\n## Are Complex signaling games hard to visualize?\n\nIn reality as I progress writing this article adding more examples. I keep finding more questions and I have little in the way of answers. Some what similar to how I felt at the end of my BA in mathematics. In reality I've made lots of progress with the lewis game and so I do believe I can make more progress. The reality is that to make progress you need to ask the right questions. **I have been lucky to be able to look at the work of many  others, criticize but later ask different questions than they did and so I was able to crack the issues related to the simple game**. I also developed good intuitions on the simple lewis game and over and over I realize that the complex game is in many aspects just the simple game in most respects.\n\n![ultimate tic tac toe](ultimate-tic-tac-toe.webp){.column-margin}\n\nI was looking over books this week and I came across two or three by math teacher, author and blogger [Ben Orlin](https://mathwithbaddrawings.com/ultimate-tic-tac-toe-original-post/). I felt they are too basic for me to buy but I did get the idea that with the kind of sketches he used I may be able to make more progress with these research questions. I confess am not very good at visualizing thing in my mind. So unless I can put them down on paper so it can be a bit of a struggle to get the ideas out of my head. \n\nThis ultimate tic-tac-toe game seems to be similar to the coordination task in the lewis game with three states and three signals. In tic-tac-toe the game is [zero sum](https://en.wikipedia.org/wiki/Zero-sum_game) and in the Lewis game the agents are trying to [cooperate](https://en.wikipedia.org/wiki/Cooperative_game_theory). But nature (chance) can have the effect that they don't end up with a good solution or at least take a long time to lean a good system. \n\nOne way to visualize this could be viewed as a pattern that is like a three rook problem in chess in red with the blue corresponding to all the misses they made along the way. Of course lewis games can start on a two by two grid but tend to be much bigger.\n\nA second aspect of visualizing is the learning in the game. I use urns or matrices with heatmap to visualize the learning in the simple game. \n\nNow this ultimate tic tac toe suggest something extra. In complex lewis games we are working with sequences.\nThe simplest way to think about sequences is to put each at the top of the matrix and all the states at the side. Agents then look for an n-rook solution in the matrix. And that is a signaling system.\nHowever  there may be other ways to view sequences, perhaps using nesting or recursion which is a way to establish hierarchy and organize a language\n\nSay we had a sequence of three sysmbols we\n\n\n::: {.column-margin #fig-deep-reinforcement-learning}\n\n\n\n{{< video https://youtu.be/YOh9iIQ5Qco title='Math with Bad Drawings | Ben Orlin | Talks at Google' >}}\n\n\n\n\n\n\nTalk titled 'Math with Bad Drawings' by Ben Orlin at Google from Jan 29, 2019\n:::\n\nI have been doing some similar work myself trying to explain basic statistics visually. And I used my own urn models to workout the different lewis algorithms. So it's not a big surprise how looking at his books I got the idea that I need to make this more visual and concrete. \n\nWhat I hope is that by posing these many questions I might be able to find some organizing idea to answer them. I seems that one way to move forward is to extend the petting zoo environment and build agents that can participate in experiments that will help me answer these questions. A second challenge is to add metrics that lead to easy evaluation of communication and robustness of the equilibria in the game. A third challenge is to test different algorithms for learning the signaling system and see how they perform in some different framing scenarios. Another challenge seems to be  even harder -- that of interpreting the many emerging languages.\n\n\n## Looking fo the Organizing Ideas\n\nEarlier this week I tried to explain the main issues around compositionality to a friend. This helped to cement my understanding of the problems that researchers in this field are facing. I noticed that in many talks by RL researchers they like to presents their work from first principles. This means most of the RL talks wastes a big chunk of their time on introductory stuff. But it does allow them to talk about advanced concepts with all the more confidence their readers understand what they mean. \n\nLet's start with these concepts that come up quite a bit in the literature and might be less intuitive to the uninitiated. I realize that big words can be intimidating so I will make an effort to explain the main concepts in a simple way and if I do use more big words in these definitions, I'll mark them in italics to indicate that they are not essential to understanding the concept. Some readers might know the terms in italics and this might be helpful to them.\n\n1. **Signaling systems** - these are steady states corresponding to *separating equilibria* in to the [Lewis signaling game](https://en.wikipedia.org/wiki/Lewis_signaling_game) that allow agents to communicate about the world with each other using symbols and without making mistakes. Human languages are rife with ambiguity and do not fit into this category.\n1. Besides signaling systems the Lewis signaling game has many equilibria  that are less then conducive for perfect communication. These correspond to **partial pooling equilibria** and we can rank them by their likelihood of the agents to correctly interpret each other messages. We can generally pinpoint the issues as homonyms in the lexicon. \n1. Is the agents ignore the states or the messages then the game collapses to a **perfect pooling equilibrium**. This is a state where agents can't do better then random guessing. \n1. Even when agents are **deceptive** and messages are cheap talk, it is possible to agent to coordinate by looking at thier actions and infer from that some kind of signal. So even under a zero sum game it may be possible that they might end up with an system of communication that is better then random guessing. c.f. [@jaques2019socialinfluenceintrinsicmotivation] However this is an aside on deception and will not be pursued further. \n1. Simple signaling systems - these are signaling systems that comprise of just a lexicon of symbols and their corresponding states.\n1. Complex signaling systems - these are signaling systems that comprise of a lexicon of sequences of symbols drawn from an alphabet and each sequence represents some corresponding states. \n\nIt not simple to explain how these differ differences from simple signaling systems except to say that while simple signaling systems are just a lexicon complex ones may be able to capture some or possibly all of the nonces of a natural language.\n\nOne way is that perhaps some the symbols comprising the alphabet might be included in the lexicon as length one sequences, and when they are used in a sequence their semantics may be assumed to be the same as when they are used alone, unless there is an explicit entry in the lexicon that says otherwise. This would mean that the lexicon now has potential to express\nWhat is different from simple signaling systems is that should the states have a sub-states with some structure. In such a case if agents are able to coordinate a signaling system that respects this structure they may be able to learn to communicate much faster. Also if the system gain evolves and new states are intorduced with this structure the agents may be able to learn to communicate about these states much faster then if the structure is ignored.\n\nIt worthwhile to recall that these three definitions\n\nHowever in this case the lexicon might have additional structure that matched the structure of the states. Idealy this structure is codified using rules so that using these rules the semantics of the atoms can be combined to give the semantics of the whole.\n\n5. **Sub states** - Pre-linguistic objects are sometimes called states and we would like to look into these and discern if they have structure like key-values pairs, some kind of hierarchy, some symmetries, a temporal structure, etc. If such a structure exists and follows some steady distribution we the lewis signaling game may have signaling systems that are conducive to perfect communication, faster learning and generalization.\nfor we are primarily interested in complex signaling systems.\n\n6. **Framing Game** in movies we often have a framing device - some kind of story that is used to introduce the main story as a flashback. In MA-RL the lewis signaling game may be a means to achieve coordination between agents in a more complex game. This bigger game is called the framing game. Though in in the life long learning setting there may be many games taking place simultaneously.\n\n6. **Entanglement** - when a sequence of symbols are combined in a way that their meaning is different from the sum of their meaning we say that they are entangled. For example idioms like \"kick the bucket\" or \"keep the wolf from the door\" are non-compositional and highly entangled. We cannot assign a specific word (atomic symbol) that captures part of thier meaning. This is a non-compositional way of encoding information. Although entanglement is explained using idioms, it can happen at different levels and may be an artefact of some selection pressure in the environment to compress information about certain states. If some bound colocation is used very frequently. i.e. where both words are used together exclusively then the speakers may have a benefit in fitness or communication for encoding them as a single symbol. Another form of weak entanglement might be exemplified by the compound verbs in English. In the phrases Look out for, Look up, etc the meaning of the verb look changes in a way that is not a simple sum of the usual meaning of preposition that follows it. This is  a cliché. But the bottom line is that entanglement representation require the agents to learn the meaning of the entangled symbols together. \n7. **Disentanglement** - when the meaning symbols can be interpreted regardless of thier neighbors. There are two problems with disentanglement. The frist is that it is rather vague definition and secondly it is not a property of languages. Sure many words in a language can be assigned a definition that is independent of the other words in the language. But the lexicon if full of colocations, idioms, compound verbs etc. Verbs and Nouns have a stem and affixes that encode multiple units of meaning. Describing relations and using adjectives  and adjectives is done using phrases, i.e. they are spread across multiple words. But the problem with this term is that semantics of one usint isn't a signle bit of information (i.e. discreate) Some symbols can contain more information, then other. This is other grammatical symbols perhaps less but seem to operate on others.\n The problem with this term is that semantics of one unit isn't a signls bit of information (i.e. discreate) Some symbols can contain more information, then other. This is other grammatical symbols perhaps less but seem to operate on others.\n8. **Compositional**- when the meaning of a symbol can be decomposed into the meaning of its parts. This is a compositional way of encoding information. One system that is compositional is first order logic.\n9. **Generality** - We take here the meaning from machine learning, where a model is said to be general if it can perform well on unseen data. What researchers tend to look at is if agents have have coordinated a signaling system that is a good representation of some subset of pre-linguitic objects which we call the training set, how well would that system generalize to unseen objects? \n10. **Catagories** - In a simple lewis game we might have multiple states assigned to a single symbol. We call this a homonym and consider it a defect. If though in the framing game we have a survival benefit to acting early, if such a homonym encodes a number of states that require the same action then we might have a benefit to splitting the states into two parts the category and the subcategory. Also we might have additional benefits here by using a shorter signal to encode the category and a longer signal to encode the sub-category. Categories or in more general form Hierarchies are a ubiquitous feature of natural languages. Another facet of this idea is the use of prefixes and suffixes in natural languages. In both cases we have a benefit from using a shorter signal to encode some category and a longer signal to encode the subcategory. But is the prefix perhaps we need prefer to know the category first and in the suffix we might prefer to know the subcategory first.\n11. Aggregation rule - an aggregation rule is a rule that takes an input with a number of symbols and reconstructs from them a state. In one sense this is what we think of as a grammar, but I'd like to keep them seperate and think about the agrregation rule as something more like a serelization protocol. It takes a number of inputs -- possible from differnt senders and likely each with some meaning or prehaps just a cue - a partial meaning that can't be interpreted without the other cues. \nTwo examples of aggregation rules are \n    1. the disjunction leading to the bag-of-symbols \n    2. the idea of serilization of incoming audio signals by the reciever by appending them order in which they are recieved.\n    3. and serilization converting OOP into a sequence - perhaps for images.\nWe can think about a recursive aggregation rule but I'd like to call these a grammar and keep them seperate. Perhaps later I'll be able to explain why I think this is a good idea.\nNote that complex signaling systems do not require an aggregation rule or a grammar, but they may benefit from them. Without an aggregation rule we are dealing with a signaling system that is fully entangled and that is little different than a simple signaling system.\n\ntip: currently my mental models for Aggregation rules are {disjunction, serialization, prefix coding}.\n\n12. **Formal Grammar** - describes which strings drawn from an alphabet are valid according to the language's syntax. Note that the formal grammar is in charge of the syntax and not the semantics. Thus a grammar can be considered as a **language generator**. If the speaker uses such a language generator then the resulting language will have a syntax. And further more is the grammar is unambiguous then the language will be a signaling system. And Aside is that for propositional logic a formal grammar is enough to define the semantics as they arise directly from the syntax. For FOL we need a model theory to define the semantics. However if we construct a Lexicon with semantics from the model of the FOL we end up with a grammar whose semantics are defined from the syntax. \nMy mental model for a formal grammar in the lewis game is propositional logic on its sub-states.\n\nThe main takeaways is that if we generate the lexicon without using an unambiguous formal grammar we are putting the syntax of the language at risk. And if we don't have a a lexicon for the alphabet we may be putting the semantics of the language at risk. \n\n\nAn interesting issue is that we can have differently likelihoods of having different constructs depending on what we include in out Lewis game extention....\n\nWe might want to find a metric that measures the use of categories in a signaling system.\n\nAnother couple of ideas.\n\nWe can have examples of complex signaling systems that are on a spectrum from being fully  compositional to being fully entangled. \n\nAnother thing we can use is a complex compositional language for pre-linguistic objects that don't have a simple disentangled structure and don't fit with the positivistic view of an objective orthogonal and disentangled dimensions that can be measured with full certainty. Ie we can use a easy to learn compositional complex signaling system to encode the pre-linguistic objects that are not easy to interpret like arrays of raw pixels. But also we might be less likely to learn such a language if this is our only input. We would have to come up with it by change.\n\nOne way this might happen is that we could learn a grammar (classifier) that \n\n## Transcripts:\n\n### Baroni Talk\n\nI've had this idea about Complex signaling systems, unfortunately. I'm wasted a lot of time, installing a transcription app and lost focus. Let see \n\nI'm trying to refocus and get back into it.... Yes, I think I got it. \n\nSo yesterday after talking with my friend Eyal which got me rethinking about complex signaling systems and trying to realize how despite the many perhaps misguided attempts a growing number of researchers have been able to come up with agents that use complex and even compositional signaling systems. How these agentic systems seem to be a solution to a problem that is hard to frame.  \n\n\nSo I walked over my thinking about the baroni paper. I got to realize that these results are problematic. That it is not so easy to explain why. But to get started some of thier ideas and metrics are borrowed from another domain -- representation learning. The ideas they were thinking seem as best as I can intuit as this was not adequately motivated of explained in the papers mentioned in the talk -- has to do with embeddings. Embeddings map the space of words sequences and thier context in a language think Gazillion dimensions to a space of 300 dimensions. The way to intuit this is that we do a non linear PCA on the high dimensional space and then tak the leading 300 dimensions. This way we get to keep the most important information and throw away the rest. The main problem here is that each dimension in the embedding space is a some summary of a gazillion dimensions in the original space. We can't say what any one dimension signifies. That is similar to an entangled state in quantum physics. What embeddings tell us in deep learning is that these distributed represeantation are very powerful. They are able to capture the meaning of words and phrases and even sentences. But they are not interpretable. A lot of people, me included wanted to enjoy the power of embeddings but to have them interpretable. It turns out that not only that this is hard but that it is a trade off. The more interpretable the less powerful. The more powerful the less interpretable. What a linguist might want is to break the embeddings into dedicated regions dealing with phonology, morphology, syntax, semantics, pragmatics etc and perhaps also  some of the regions overlapping to represent the interfaces between these regions. It would be even better If specific dimensions represented specific features. Some ideas were to use algorithms that would learn to extract such representation from the embeddings or learn them direcrly from the high dimensional space. There are aslo ideas about learning semantic atoms using non-negative matrix factorization. Other ideas about distiling word sense level embeddings from the word level embeddings. But the main idea is that we want to have a representation that is interpretable. So we can do more with them. Given all this it seems to be an interesting to learn disentangled representations. \n\nWhat's the problem then is that emergent languages that are not interpretable, the main pain point of Baronis group, are nothing like\nembeddings. Lewis signaling game at its core is just a way for agents to \ncoordinate on a shared convetion. The default setup leads the sender to  to map the pre-linguistic object to a symbol and the reciever to learn the opposite mapping. If they succeed they get en expected payoff of 1. If they fail they get an expected payoff of $1/|\\mathcal{S}|$. The game defaults to learning a tabular mapping without any ability to generalize. From N states to the next. To get this to happen the game needs to be modified. \n\nOne simple way to do this is to use a bottle neck in the form of a small alphabet of signals. The message is then split up and each part is encoded spepretly (either by different agents or by one agent sending the state as a sequence from a restircted alphabet) In [@skyrms2010signals] chapter 12 coevers this. The problem is that in those research there isn't a a clear explnation of what is going on. How can agents learn to break down the some arbitrary state. How to serialize this state into a sequence. How to decode it.  \n\nNow some basic checkss about signaling systems indicate that there can be massive speed ups to learning and the ability to generelize if agents can learn nicely composable signaling systems. This though needs spelling out: it can only happen to if they can breakdown the state into sub-states with a structure and the sequences uses as faithfully represent this structure.\n\nThink homo-morphism. In [@skyrms2010signals] there is also a discussion of agents learn 'logical' representation in  variants of the game.\nOr at least some representation that are grounded into a simple logical scheme. \n\nAgain we don't know enough to say if for some structured  state space there is a some easy to learn equilibria. What seems to be self evident is that as we have bigger state spaces any signaling system is going to be very rare amongst possible equilibria. That complex signaling systems emerge, and that they are composable suggest that they may be a mechanism that can invert^[describe competing signaling via inversion for Propositional logic (PL) a system with alphabets {NOR,A,B,C,T}, {NAND,A,B,C,T}, should emerge before {NOT,AND,A,B,C,T}, {NOT,OR,A,B,C,T} which should arise faster than {NOT,OR,AND,IFF,IMP,A,B,C,T,F,(,)} even though all are equivalent - since the first ones need to learn one binary function, and will start to reinfoce much faster then ones with two or (4*2*2)+1 which needs NOT?(NOT?A {OR|AND|IFF|IMP}NOT?B) > will emerge much later as cooridnatinon is slower many options to try and reinforcement is slower - repeated success are far less likely to arise due to nature.] the population of equilibria. At least for signaling systems so that smaller alphabets may be better at reinfocing then ones with many symbols. Another aspect of this is that might be is that for only a few wff in PL most sequences should be rejected. If we use reward shaping to learn from negative outcomes we might come up with algs that can reinfoce  at almost every turn.\n\n\n\nAnd despite lots of talk, it doen't seem to be relevant to the problems they were facing. The effect are that the  most of what is happening in lewis signaling game is not realy   That  explain entanglement disentanglement. Compositional, non-compositional and generality. In a nice concise way. And that deserves. Good. Write up inside of the baroni post. And maybe inside of the guide to the perplexed. About compositionality. Uh, in fact, I don't think Baroni deserves this because He's just all about making things to make him confusing. Uh, then there is this Gondola girl. Whatever name is. And, Thinking about her work. Got me. To make a bit of progress. I think.\n02:26\nWhat we could do there? Is say this.\n\n\n\n----\n\n## The Question of Grammar\n\n\nThis bit of thinking is about the deep mind emergent languages talk and paper [@lazaridou2018emergence]. by Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, Stephen Clark titled  \"Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input\". I seems to have been rather influential on the field of emergent languages. And while this paper seems fairly sound, some of the follow up work seems less so.\n\nWhat I noticed there. Is that? The talking about emerging languages. And I'm talking about, Engine re-engineering in Duluth game. The change at equilibria. So, as to. Support complex, signaling systems. And this is quite tricky. Particularly. When we have, Both simple and complex.\n00:45\nSignals and we have a third kind of thing. Which is the so-called aggregation. And there's yet another thing. Which is the grammar. Is grammar and aggregation the same thing. I don't think so. I think that. The related. But only coincidentally, The aggregation. As scrims describes, it can be conjunctive, which is weak. We go, it leads the weaker representation and the sequential one. Which leads to Richer. Representation. Um,\n01:38\nBut that said, It doesn't necessarily lead to a grammar. Although, It's definitely sufficient to act as a kind of grammar.\n01:54\nUm,\n01:59\nWhat what is an example of a concatitative grammar? Uh, Hungarian. Is a glue. Native language. You simply add. Morphological units to form. Ah, very powerful.\n02:25\nUm, representation of a word, which is able to Essentially exist. Or rather resist changes in the order. Or for freeze. And, That doesn't mean that they don't care about that. There isn't extra meaning, due to the order like the word, which is at the focus position. But basically Once we have the markings, in the words with all the affixes that you have in Hungarian, It's quite possible to shuffle the words. And not disrupt. The meaning of the That. Morphology is encoded. So, that's kind of\n03:24\nThe power of the sequence I suppose. Though. It also, we also have in the sequence this special morphological markers.\n03:39\nBut we could consider that these morphological markers are just basically. Certain words.\n03:51\nWhich we put in certain positions. But, So, we're still talking about this idea of\n04:04\nCompetitive grammar. So another thing in the concoctative grammar is Uh, it could be. Ordered or disordered. If things are marked or unmarked, we can have it. Uh, resistant reordering. So the meaning is preserved and if it isn't like in the example from The unfolding of languages from Old Turkish or Babylonian. You might have a very long sequence of Slots. And, We should be able to. Composite into these slots. A whole wealth of words, a whole wealth of meaning. And, I think we have something similar in German. We basically assemble a whole sentence. By gluing together, bits and pieces.\n05:06\nOf.\n05:12\nInto one long word. And this actually makes sense. If you think about,\n05:24\nAbout all societies in which The nobody wrote down the language if you don't write down the language. Yes. It's all overall. And, We could think of a word is something. That's just a sequence. Okay. So, all of this Tries to highlight that. Um, We can have grammars just by concatenating. More films or like themes. And,\n06:06\nThat's what we call compositionality really, or at least. That's a very basic form of Compositionality.\n06:19\nSo, what is grammar if we have just a compositional\n06:26\nRules. I would say that grammar. Um,\n06:35\nIs different ways we use. Create isolate bits of meaning.\n06:49\nYeah, it's Seems to be a hard thing to Define properly, but\n06:58\nThe kind of ideas I'm thinking about is that we might have this recursive. Recursive set of rules. Because the grammar, Yeah, so in former languages, I think that's the direction that's the direction I'm bent to in. Informal languages, grammar defines. Using recursion, usually.\n07:30\nSet of sequences, the sequences are defined by the grammar. You can call them sequences, we control them sets. But I think they usually order set, so we can call them sequences and these sequences I lost to take. Find that alphabet and create. Uh, infinite number of\n08:00\nMessages. How do we do that? Quite simply put. We have this operation. With a simple set where we can take the power of the set, the power set. Which is.\n08:22\nAll the pairs, I think. And all the triplets and so on and so on. Basically, all the subsets rather. Yeah, the power set is the central subsets. If we look at just\n08:46\nBut the power. Yeah, if we look at all the subsets we can create, Bigger more complex constructs.\n08:56\nUm, If we look at, Sequences that we can form. We can also have a grammar for that. And, Some things human grammars have. This thing, this notion of agreement. I see agreement. This Serving two purposes. One is to identify. To maintain a correlation between lexical units. That have a relation. To show us assumings that\n09:43\nThat the Mexico. Units or even two phrases. Are related using this? Correlation of gender number and so on. Whatever the agreement is keeping, And that way. We can. Poke into it. Into the slots between them. Uh, additional structures. And, The agreement. Allows us to maintain. The relationship. This, of course, breaks down. If we poke in, And they'll literally arbitrarily Large number of, Uh, phrases. He put in a very big tree. This isn't effective. Another way that I'm looking at it. Is that in terms of the pragmatics? If we look at the pragmatic side of communication,\n10:57\nThis is just the redundancy, which allows To do our correction. I suppose. In the big picture though. These two things. These two phenomena agreement. And error, Corrections are\n11:22\nDual aspects of the same thing. You put. These markers. They allow. They also make the language. More resistant.\n11:39\nErrors. They help us. Uh, disambiguate certain messages.\n11:50\nThrough this types of agreement. And,\n11:57\nUh, we usually don't need these\n12:03\nIf there is, let's say less chance of an error or less chance of confusing. Some pronoun referencing, some other battle piece. Uh, multiple bits and pieces of the sentence. We might not need to mark this thing with.\n12:27\nWith an agreement. To understand what's going on and we might end up with a determiner. Which is unmarked. All it says is So what's the difference between a and there or that? Definite and indefinite. All right, this or that.\n12:54\nSo, The unmarked distance in any way that they don't have. Um, what don't they have? They don't reflect number, they don't reflect. Gender and so on.\n13:14\nThe other hand we do have one, we have mine. These are.\n13:24\nMarkers. These are, I don't know, pronouns or particles that.\n13:31\nMark possession. The Mach number, the micro number the marked for person. Right.\n13:43\nIn Hebrew, they marked with gender. So they get marked with a lot of things and that kind of makes it significantly easier.\n13:56\nMakes it much easier. To discuss. There's ambiguous thing. Called position.\n14:08\nAnd in Hungarian we can, The position and the possessor. Using additional information. Which is, uh, stored in the suffix. Along with the singular plural. So, they mark that. But they don't Mark gender. Which is. How do you say? It's useful as it's on a, on a pig or something. Right. So Enough said about that. Um, So, let's get back. So now we've discussed Grammar. Kind of try to Define grammar. And, Three rolls the formal role. Being. That it is.\n15:13\nA. That allows us to. To make use of a finite set of symbols. Into an infinite set of messages. But not necessarily saying anything about Their semantics. Although if we look at, I don't know. The grandma. First saw the logic. It does allow us to Define by induction.\n15:48\nThe Logical.\n15:53\nThe Logical. The Logical meanings of. I'll betually long phrases. Okay.\n16:07\nUm, So, it not only allows us to generate sequences. But to propagate the atomic meanings into more complicated meanings.\n16:26\nAggregate meetings. This.\n16:34\nUm, Is more complicated and simple. Aggregation in simple. Aggregation we're saying something after something after something just have this blasting. Logic, we have. And the no with brackets.\n17:02\nAnd we can build with this very specific functions. Of truth functions. And Truth functions correspond to a big chunk of semantics. No doubt about that.\n17:24\nSo, if you want to look at grammar, Starting with a former language. It's not the worst thing. But if we're thinking about, This thing we call uh, what do we call it? Um,\n17:44\nEmergent language. We may be interested in having some additional useful properties, and this is what I collect and the ziturata This. Space. Definitely consider. The issue of learnability. Vulnerability.\n18:14\nBecomes Paramount. When we have Collective generational, collectives of Agents,\n18:27\nBasically. When we? To transfer. Disability. And The ability to communicate.\n18:43\nUh, between agents and Even if we don't have Generations, even if we have Continuing tasks. Which is. Similar to what's happening in the real world. Uh, we still The. You want? We still would like, to be able to Handle.\n19:16\nDistributional shifts. Within the language. So If we. If you and I are talking for a long time. We're gonna have. Who had adaptation of our language will have accent, and then would have a dialect. And then pretty soon. We're gonna ask someone about. This and that and other people. Won't be able to know what we're talking about. We'll have our own jokes. We'll have our own idioms. And,\n19:58\nMade upwards. So we'll have Words that other people know, but that now have completely new meanings. So, that's this. Shift. I'm referencing. Distributional shift. And we'd like, to be able to Uh, communicate with other agents. The distributional shift. Is also annoying for researcher. In the sense that Let's say, you understood what the language means now. But What happens after? The our allegiance.\n20:42\nHave had another. Half a million turns playing. Let's say some kind of diary game or something. Now, the language might mean. Other things. The words and phrases might be the same but the semantics have changed, that's a big headache or the grammar might have changed.\n21:08\nOne of the algorithms I came about was to Full languages was Try to swap out. Pairs of signals.\n21:24\nAnd the meanings based on.\n21:30\nDistribution of. And this. Get us to a point where Um, We can have. Shared bits and pieces. Shared button pieces. These shirts bits and pieces.\n21:56\nThen be able to spread. And then we would have a compositionality. Basically. Should bits and pieces. Become prefixes and prefixes become categories. Vice versa. Let's say, categories. Semantic categories and maybe later they become.\n22:29\nGrammatical categories part of speech. And so on and so forth. And That way, we might be able to evolve Um, Rules that simplify learning. We might learn construct. Let the language become. Uh, fixed. So, we don't have A big issue with. With that language needs to evolve, but we do have an issue with this core adaptation.\n23:10\nCertain bits and pieces of the language not changed so much. We'd like, To find an optimal grammar. Then stick with it. We'd like to Have close categories that not change those. We'd like to use. Affixes efficiently. And we might want to and prefixes and so on. And we like to stick with those. Otherwise.\n23:45\nOur verbs. Might be. More prone to. Withering. And to make the language learnable. It's gonna be much simpler if the verb and the noun, Morphology. Is regular. And more or less constant? And then, We can block in a stem or a root. You can get. A new verb or a new noun, or maybe even both. And all we have to do is learn the meaning of the stem more or less. And we understand all the rest. And that's the power compositionality. So,\n24:39\nOne old idea, I had. About trying to induct. Buffalologies. In unsupervised or semi-supervised way. Was to try and find some kind of\n25:04\nThat's a good fit.\n25:09\nHandling morphologies for. Handling tables. Did you follow the same structures?\n25:20\nThis is.\n25:25\nUh, something I don't want to explore here. But this kind of a loss. Might be useful. To encourage.\n25:43\nHola. The agents to coordinate on the language. With a given structure and yeah. A further refinement of this idea. Which I've had more recently. Is to consider that. This morphology. To high degree mirrors a group action. And so we can Define This loss in terms of, Homomorphic loss. I lost that preserves.\n26:28\nStructure.\n26:34\nBut that's pretty much.\n26:38\nJust handling structure. You could probably do even better. If we. Also, preserve distances. Distances between. Semantic unit. I suppose. Such.\n27:02\nStructure would be. Also a home homeomorphism topologically preserver. Something. To think about.\n27:18\nAnd yet. These are approximate.\n27:25\nWhy is that? Because, Because, Um, Because of the irregularities irregularities. Are usually going to be deviations from this group structure. From the topology. From the simple distributional forms.\n27:52\nFor compositionality. We generally assume. Independence. But three languages.\n28:04\nAre not uniformly, distributed.\n28:08\nBiggest structures are not. Uh, often\n28:19\nShow conditional. Probabilities. So, They don't follow.\n28:33\nIndependent structure.\n28:38\nAnd these are. Aspect of the language, we usually want to preserve In other words, It's often. To our benefit. To have.\n28:56\nThese other things in the language.\n29:07\nWhy is that? Because these oddities often encode. Not always, but often. Then code.\n29:26\nFrequent the most frequent. Elements. Irregular verbs. To be.\n29:37\nTo do. In French, a regular.\n29:46\nIn English. I'm not sure, I think they are also. Is that good makes learning harder? But it's only a little chunk and it probably makes air correction better.\n30:06\nAlthough, I'm sure we could do better than that. Um, So there are all sorts of plays going on here. And, Uh, one thing you can be sure about Is that? To come up with a good grammar. Isn't. Necessarily very difficult.\n30:38\nBut,\n30:43\nIt appears to be. Product of planning. Once you have the language, Or laid out in other words.\n30:56\nExpand the rules of the grammar. You'll have vast constructs. And to change. And we see the changes in these contracts are often localized. Of the localized. Which means? It's very hard to change the grammar. Just change the grammar, you will have to change.\n31:25\nAll the application of all these constructs. For that particular, aspect of the rule. There might be subtle changes, that not make big, big, big changes. Usually,\n31:44\nNatural language grammars. Nothing. Like The world behaved formal grammars. Which we see, and I think, This is primarily because of what I just described. That it takes a Big effort to make these changes because you have to get these changes to happen in the heads.\n\nThe most.\n\nOh, at least localized area. Where people are talking? Um, Yeah. Um, but they do happen. So they happen, they happen because The forces. Evolutionary forces. Acting over. Certain time frames. That. We'll do this, because The original system might be inefficient. Of equilibrium in terms of signaling systems. And each interaction entails a tiny amount of of friction. And,\nIf it is likely, the changes will happen in such a way.\n33:32\nFollow some kind of path of lift resistance. A domino effect Chain Reaction. We see the vowel, shifts. How can that possibly happen?\n33:51\nOne wond. But,\n2\nSpeaker 2\n33:59\nI don't know. But\n1\nSpeaker 1\n34:07\nThey did happen. They have happened. And, One would think this is. Something that's happening, step by step.\n34:20\nCertainly. Words. With a dominant meaning resist changes.\n34:34\nTo conform with the grammar. Until?\n34:42\nPerhaps that word. Is replaced. With another word of the use of that term. Falls into. The shoes. Allowing us to.\n35:00\nUh, assign a new meaning or Reduce the variation. We see in Hebrew.\n35:10\nSomething even more drastic has happened. Over time certain of the phonemes. Have become.\n35:25\nThe.\n35:30\nWhere we have three different sounds in Arabic, we have only one song in Hebrew. And, This is cause number of verbs to become conflated.\n35:44\nYou can only imagine the chaos. This is created in terms of meanings.\n35:52\nIf only, we could take a step backwards, Teach everybody, how to Pronounce. These sounds in three distinct ways, everybody. And then, teach everybody. Which verb? Fits with which sound.\n36:15\nWe could. Make the language, much more.\n36:23\nMuch less ambiguous. But at the same time, It will probably be. Harder to learn. So, If there's one conclusion from all, this rather long story, Is that all these things? Represent trade-offs. And,\n36:58\nEvery languages, the equilibria.\n37:07\nCorresponding to these different things. Equilibria. That is stable. One change so much, but You can see from the research by schemes and others. That.\n37:33\nThe. Most of the equilibrio at least in the simple models. Are unstable or semi-stable. And serve the language. Will shift over time.\n37:53\nAll that you need is the right kind of pressure. I suppose.\n38:03\nEnough said. So now we can get back to So,\n38:15\nI've covered a lot about. Aspects of grammars. Now, let's get to Signal existence, let's get back to signaling systems. It looks like. Does not look like the simplest games. There's a bunch of. Through specific equilibriums possible. Very specific. They're essentially infinite. Infinite number, and yet very specific. What do I mean? The three types of purequalibia. Perfect, pooling completely useless. Partial pulling. Imperfect.\n39:05\nAnd, And,\n39:15\nSeparate the equilibria, the so-called signaling systems. That's to, at least. The simple. Games were simple States.\n39:33\nSo-Called.\n39:40\nUh, simple. The basic.\n39:50\nIf you. Something more sophisticated.\n39:58\nWe need to look for some other game. We usually call this. Modification. What we should ask is. Does our modification allow us?\n40:16\nTo have. Um,\n2\nSpeaker 2\n40:22\nDoes it allow us to have new types of equilibrium?\n40:29\nI think there's\n41:18\nAnd just upload those.\n41:36\nSo, the the big, the big question again.\n41:55\nSince we do see compositionals, Languages.\n42:05\nCan we\n3\nSpeaker 3\n42:06\nsay?\n2\nSpeaker 2\n42:11\nUsing complex symbols. And not just the coincidence.\n42:20\nNo, that's not the paper. The big question is\n42:26\nIf we make some change to the significance,\n42:41\nOf the game.\n42:46\nTo have. New kinds\n1\nSpeaker 1\n42:48\nof Bolivia. Furthermore.\n42:55\nThis\n2\nSpeaker 2\n42:57\nis equilibrium. Follow approximate, let's say. Use the approximate.\n43:25\nWe really wish the property. That are close together.\n43:40\nConform to some grammar. To the approximate, some kind of drama. Is there sometimes a genre. Death is. More likely to emerge. And so on.\n44:00\nWhy do we ask this? Because we don't know. Nice and simple way to extend the game to get this. I don't think.\n1\nSpeaker 1\n44:23\nI\n2\nSpeaker 2\n44:24\nthink you would apply this to the agents. They create is such an algorithm These agents. Lexus. No.\n3\nSpeaker 3\n44:38\nHave order of magazine.\n44:44\nAnd it comes. What the future is to transfer.\n2\nSpeaker 2\n44:56\nOh, it would alone. For these abilities.\n45:14\nPolicies.\n45:19\nThe values and action values in terms of, A language where design in terms of getting used to be really interested. Objects. And there's a bit to flip.\n45:42\nThey will be able to transfer. To have a translation. Between languages.\n46:02\nFrom the line is just one problem.\n46:15\nAnother I know injection. We've seen in enforcement learning. That when you run the same algorithm, Some seats. So, Just by chance. We might have. Low performance. And another right here.\n46:45\nLord, that pitched.\n46:54\nWell, not much except that. When we talk about language, we talk about state that unstable equilibrium and\n47:11\nWhat time vision is?\n47:16\nThis could do. We understand. The Scorplex signaling systems. You should also be able to figure out How to create?\n47:35\nVery many designers including one with three stabilities.\n47:42\nIf we are able to use this stability of our Policy value functions equivalent function.\n47:57\nBecause we now have\n48:02\nMore abstract.\n48:07\nWe're probably going to be more powerful.\n48:14\nThen.\n48:27\nSo, given all this\n48:33\nWhat did you change? With these so-called.\n48:48\nResearch.\n48:56\nComposable. So complex. To work this country.\n49:31\nBecause,\n49:37\nUh, because We can easily show that. Languages with these properties.\n49:51\nThe ability to capture very much and for Concepts using this small relatively small.\n50:03\nThey're very easy to learn. They're very easy to teach. So if you have all these benefits, maybe some of them deserve distance and structure.\n50:19\nAnd maybe they are also.\n50:29\nEasier to To. Translate.\n50:49\nSo, we would facilitate Uh, the best faciliters of transfer learning. Maybe they. Group instructions for many things that might not be in the learnable in one second. So you might be working with Space Invaders, you might not have. And language.\n51:23\nYou get that. So if you're working with soccer band, you might have other kinds of Concepts\n51:34\nUseful, which\n51:40\nAnd,\n51:45\nI, We would like to find some kind of core.\n51:57\nA human language, which has\n52:12\nDidn't describe States. Well enough.\n52:22\nI\n1\nSpeaker 1\n52:24\nhave also properties, which bar\n52:34\nUh, gravity stability.\n52:58\nAnd yes, all of this might follow one. Three, two, maybe just one uh, geometrical rule.\n53:33\nYeah. Anyway, anyhow\n53:49\nSo that's us. What? What does this? What does this?\nTranscribed by Pixel\n\n---\n\n## The second question, seems to be in what way is The referential game different from the signaling game?\n\n\nThe second question, seems to be \"in what way is The referential game different from the signaling game?\"\n\nI postulate that this is due two changes and more significantly, the way these two things come together. The alternative hypothesis is that just one suffices for the emergence of a signaling system.\n\n1. There is the classifier.\n2. Is the Message generator - as described in [@lazaridou2018emergence] can generate any sequence from a given alphabet of symbols.\n\nIf these symbols are already imbued with semantics from a pre-ante lewis game, then what we now have is a complex signaling system. However, if the symbols are not imbued with semantics then we can still use the lewis game to imbue them with semantics. It uncertain though if these semantics will be different from the semantics due to a simple lewis game.\n\nInitially I liked the first hypothesis as it mirrored my thinking of using an extensive game with two steps as my modified lewis game. Soon though I had an small epiphany, and decided to test the second hypothesis, this being more in line with the reductionist approach I had espoused all along. Particularly as this is the smaller and less powerful extension then the first.\n\nNow we can think of the lewis signaling game as using a generator with some set of symbols and sequences of length 1. \n\n::: {.callout-tip}\n## To shuffle or not to shuffle? ::cards:: {.unnumbered}\n\nMore so we might want to shuffle the sequences so they come out in arbitrary orders allowing all the lewis game to unfold in all possible ways. I.e. all forms of symmetry breaking. Alternatively we might want to enumerate the different equilibria and thus only use one canonical order.\n\n:::\n\n## Some sequences and thier interpretations\n\nHere is some code that takes an alphabet $\\mathcal{A}$ and generates all the sequences $\\mathcal{L}$ of length N exactly\n\n::: {#02aa8b9c .cell execution_count=2}\n``` {.python .cell-code}\nfrom itertools import product\n\ndef generate_sequences_generator(alphabet, n):\n    \"\"\"\n    A generator to yield all possible sequences of length n from the given alphabet.\n\n    Args:\n        alphabet (list): List of symbols representing the alphabet.\n        n (int): Length of the sequences to generate.\n\n    Yields:\n        str: A sequence of length n from the alphabet.\n    \"\"\"\n    for seq in product(alphabet, repeat=n):\n        yield ''.join(seq)\n\n# Example usage snippet\n\n# alphabet = ['A', 'B', 'C']  # Example alphabet\n# n = 3  # Sequence length\n\n# print(\"Generated sequences:\")\n# for sequence in generate_sequences_generator(alphabet, n):\n#     print(sequence)\n```\n:::\n\n\n### Tenery Sequences\n\n::: {#fig-tenery-sequences .cell execution_count=3}\n``` {.python .cell-code}\nlang= []\nprint(\"Generated sequences:\")\nfor sequence in generate_sequences_generator(['A','B','C'], 3):\n    lang.append(sequence)\n\n# print lang 3 sequences per line\nprint(\"\\n\".join([\",\".join(lang[i:i+9]) for i in range(0, len(lang), 9)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerated sequences:\nAAA,AAB,AAC,ABA,ABB,ABC,ACA,ACB,ACC\nBAA,BAB,BAC,BBA,BBB,BBC,BCA,BCB,BCC\nCAA,CAB,CAC,CBA,CBB,CBC,CCA,CCB,CCC\n```\n:::\n:::\n\n\nInterpreting the sequences.\n\nIn this case we might interpret each sequence as a message indicating a sub-state A, B, or C took place. If this is their meaning we might need to remove duplicates and order them to decode it. Also we have many alternative messages for equivalent states. This would slow down the learning process.\n\nAnother way to go is to treat A,B,C as 0,1,2 and we can interpret them as a ternary number. Now each sequence is unique and can be interpreted as a corresponding to some state. If we had again three binary sub-states we could use this system with 3^3 symbols to encode the 2^3 states as follows:\n\nA in the first position indicates true for the first sub-state, B indicates False, and the same for the second and third sub-states. We don't need states with C. This is a more efficient encoding and will speed up the learning process. \n\nLet say we used the restricted system to start with and 0,1 to encode False and True\n\n\n$$\n\\begin{align*}\nAAA \\to ['A':T 'B':F,'C':F] \\\\\nAAB \\to ['A':T 'B':F,'C':T] \\\\\nABC \\to ['A':T 'B':F,'C':T] \\\\\n\\vdots\\\\\nCCC \\to ['A':F 'B':F,'C':F]\n\\end{align*}\n$$\n\n### Binary Strings\n\n::: {#fig-binary-sequences .cell execution_count=4}\n``` {.python .cell-code}\ndef gen_all_sequences(alphabet = ['0', '1'], min_length = 1,max_length = 4,col_size=4):\n        \n    lang= []\n    print(\"Generated sequences:\")\n    print(f\"From alphabet: {alphabet}\")\n\n    for i in range(min_length, max_length+1):\n        seq=[]\n        for sequence in generate_sequences_generator(alphabet, i):\n            seq.append(sequence)\n        # print lang 3 sequences per line\n        print(f'Sequences of length {i}:')\n        print(\"\\n\".join([\",\".join(seq[i:i+col_size]) for i in range(0, len(seq), col_size)]))\n        print(\"\\n\")\n        lang=lang+seq\n    print(lang)\n    return lang\n\nbin_lan_1_4=gen_all_sequences()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerated sequences:\nFrom alphabet: ['0', '1']\nSequences of length 1:\n0,1\n\n\nSequences of length 2:\n00,01,10,11\n\n\nSequences of length 3:\n000,001,010,011\n100,101,110,111\n\n\nSequences of length 4:\n0000,0001,0010,0011\n0100,0101,0110,0111\n1000,1001,1010,1011\n1100,1101,1110,1111\n\n\n['0', '1', '00', '01', '10', '11', '000', '001', '010', '011', '100', '101', '110', '111', '0000', '0001', '0010', '0011', '0100', '0101', '0110', '0111', '1000', '1001', '1010', '1011', '1100', '1101', '1110', '1111']\n```\n:::\n:::\n\n\nNotes:\n\n1. that that the above system might be learned if the Lewis signaling game took place and the sequences were generated in an order corresponding to these states. Otherwise we would get an equivalent system to the first one\n1. When we generate all sequences of length up to 4 we get in the first row \n   the two atomic symbols.\n1. Since these are binary sequence of numbers I naturally interpret these as each being a more general version of the previous one. \n1. In one sense '0', '01', and '001' mean the same thing to me. Behind it is an aggregation of the digits weighted be powers of 2. This leads to a semantic\n1. However this interpretation is arbitrary. Binary sequences can encode pretty much anything. We use them for general purpose commutation.\n\n::: {#1d047435 .cell execution_count=5}\n``` {.python .cell-code}\nDNA=gen_all_sequences('ACGT', 3,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerated sequences:\nFrom alphabet: ACGT\nSequences of length 3:\nAAA,AAC,AAG,AAT\nACA,ACC,ACG,ACT\nAGA,AGC,AGG,AGT\nATA,ATC,ATG,ATT\nCAA,CAC,CAG,CAT\nCCA,CCC,CCG,CCT\nCGA,CGC,CGG,CGT\nCTA,CTC,CTG,CTT\nGAA,GAC,GAG,GAT\nGCA,GCC,GCG,GCT\nGGA,GGC,GGG,GGT\nGTA,GTC,GTG,GTT\nTAA,TAC,TAG,TAT\nTCA,TCC,TCG,TCT\nTGA,TGC,TGG,TGT\nTTA,TTC,TTG,TTT\n\n\n['AAA', 'AAC', 'AAG', 'AAT', 'ACA', 'ACC', 'ACG', 'ACT', 'AGA', 'AGC', 'AGG', 'AGT', 'ATA', 'ATC', 'ATG', 'ATT', 'CAA', 'CAC', 'CAG', 'CAT', 'CCA', 'CCC', 'CCG', 'CCT', 'CGA', 'CGC', 'CGG', 'CGT', 'CTA', 'CTC', 'CTG', 'CTT', 'GAA', 'GAC', 'GAG', 'GAT', 'GCA', 'GCC', 'GCG', 'GCT', 'GGA', 'GGC', 'GGG', 'GGT', 'GTA', 'GTC', 'GTG', 'GTT', 'TAA', 'TAC', 'TAG', 'TAT', 'TCA', 'TCC', 'TCG', 'TCT', 'TGA', 'TGC', 'TGG', 'TGT', 'TTA', 'TTC', 'TTG', 'TTT']\n```\n:::\n:::\n\n\n- if we use four symbols alphabet we can encode the four nucleotides of DNA. \n- this is the basis of the language of DNA!\n \n\n### Base 4 Sequences \n\n![Amino Acids Table](Aminoacids_table.svg){.column-margin}\n\nthe following generate all the sequences of length 3 comprised of 0,1 and a space symbol\n\n### Propositional Logic\n\nThe following generates all the logical propositions of length 4 using the with three clauses and symbols for negation, conjunction, disjunction for it alphabet\n\n::: {#ece7c8e4 .cell execution_count=6}\n``` {.python .cell-code}\nbrackets='()'\nmodal='◻◊'\nstd_connectives='∧∨⟹⟺¬'\nconnectives=std_connectives+modal\nprops='⊤⊥ABC'\nlogic_alphabet=brackets+connectives+props\n```\n:::\n\n\nIn this case we can see that almost all the sequences are not well formed formulas. Clearly if we wanted to use the a language of propositional logic we would need to reject the sequences that are not well formed formulas.\n\nSecondly we see that this alphabet which has only three atomic symbols for propositions is bigger - it has 10 symbols. If we wanted to consider `((A∧B)∨C)`\nwe need to use a sequence of length 7. I f we also need to gengate everything we are at length 10. This is 10^10 sequences to go through. And it is likely to exceed the memory capacity of most computers to store all these sequences. We need a smarter way to generate the sequences.\n\n\n\nAlso we can see that this language is not very efficient. We can encode the same information in a more compact way. For example we can use the following encoding: \n\n\n\nthe following function checks a string for well-formedness\n\n::: {#c2269cd0 .cell execution_count=7}\n``` {.python .cell-code}\ndef is_well_formed(brackets, connectives, props, s):\n    \"\"\"\n    Checks if a given string 's' is a well-formed formula (WFF) under propositional calculus.\n\n    Args:\n        brackets: A string containing the opening and closing bracket characters (e.g., '()').\n        connectives: A string containing valid connective symbols (e.g., '∧∨⟹⟺¬').\n        props: A string containing valid propositional symbols (e.g., 'ABC⊤⊥').\n        s: The string to be checked for WFF status.\n\n    Returns:\n        True if 's' is a WFF, False otherwise.\n    \"\"\"\n\n    # 1. Reject consecutive negation\n    if \"¬¬\" in s:\n        return False\n\n    # 2. Quick check for invalid characters or unbalanced brackets\n    logic_alphabet = brackets + connectives + props\n    if not all(ch in logic_alphabet for ch in s):\n        return False\n    if not are_brackets_balanced(s, brackets[0], brackets[1]):\n        return False\n\n    # 3. Recursive checker\n    return check_subformula(s, brackets, connectives, props)\n\ndef are_brackets_balanced(s, open_br, close_br):\n    \"\"\"Return True if brackets are balanced in s, otherwise False.\"\"\"\n    stack = []\n    for ch in s:\n        if ch == open_br:\n            stack.append(ch)\n        elif ch == close_br:\n            if not stack:\n                return False\n            stack.pop()\n    return len(stack) == 0\n\ndef check_subformula(s, brackets, connectives, props):\n    \"\"\"\n    Recursively check if 's' is a well-formed formula under the rules:\n      1. A single proposition in `props` is a WFF.\n      2. ⊤ or ⊥ is a WFF.\n      3. If φ is a WFF, then ¬φ is a WFF.\n      4. If φ and ψ are WFFs and ° is a binary connective, then (φ°ψ) is a WFF.\n    \"\"\"\n    # Base cases: single proposition or constant\n    if s in props:\n        return len(s) == 1\n    #if s == '⊤' or s == '⊥':\n    #    return True\n\n    # Negation\n    if s.startswith('¬'):\n        # must have something after '¬'\n        if len(s) == 1:\n            return False\n        return check_subformula(s[1:], brackets, connectives, props)\n\n    # Parenthesized binary formula: must be of the form (X ° Y)\n    open_br, close_br = brackets[0], brackets[1]\n    if s.startswith(open_br) and s.endswith(close_br):\n        # Strip outer parentheses\n        inside = s[1:-1]\n        # Find the main connective at depth 0\n        depth = 0\n        main_connective_pos = -1\n        \n        for i, ch in enumerate(inside):\n            if ch == open_br:\n                depth += 1\n            elif ch == close_br:\n                depth -= 1\n            # A binary connective at top-level (depth == 0)\n            elif ch in connectives and ch != '¬' and depth == 0:\n                main_connective_pos = i\n                break\n\n        # If we never found a binary connective, not a valid (φ°ψ) form\n        if main_connective_pos == -1:\n            return False\n        \n        # Split around the main connective\n        left_part = inside[:main_connective_pos]\n        op = inside[main_connective_pos]\n        right_part = inside[main_connective_pos + 1 :]\n\n        # Ensure left and right parts are non-empty and op is truly binary\n        if not left_part or not right_part:\n            return False\n        if op not in connectives or op == '¬':  # '¬' is unary, so reject it here\n            return False\n\n        # Check each side recursively\n        return (check_subformula(left_part, brackets, connectives, props) and\n                check_subformula(right_part, brackets, connectives, props))\n\n    # If it doesn't match any of the rules above, it's not a WFF\n    return False\n```\n:::\n\n\n#### Test Cases for the WFF checker\n\n::: {#b57c83c6 .cell execution_count=8}\n``` {.python .cell-code}\n# ====================\n# Test Cases\n# ====================\n\ntest_cases = [\n    (\"A ∧ B\", False),       # Reject spaces around binary connective\n    (\"(A ∧ B)\", False),     # Reject spaces\n    (\"(A ∧ B) ⟹ C\", False), # Reject spaces\n    (\"A ∧\", False),         # reject spaces and single operand\n    (\"A∧\", False),          # reject bin op with one prop \n    (\"A∧B\", False),         # reject bin op without brackets\n    (\"(A∧B)\", True),\n    (\"(A∧B)⟹C\", False),   # missing outer brackets\n    (\"((A∧B)⟹C)\", True),\n    (\"¬¬A\", False),         # Reject multiple consecutive nots \n    (\"⊤\", True),\n    (\"⊥\", True),\n    (\"∨\", False),           # Reject single connective not allowed\n    (\"()\", False),          # Empty brackets\n    (\"⊤⊤\", False),          # Reject multiple constants\n    (\"⊤⊥⊤\", False),         # Multiple constants    \n    (\"A\", True),            # Accept single propositions\n    (\"AB\", False),          # Reject adjacent propositions\n    (\"ABC\", False),         # Reject adjacent propositions\n    (\"A¬B\", False),         # Reject adjacency\n    (\"¬AB\", False),         # Reject adjacency\n    (\"¬A¬B\", False),        # Reject adjacency\n    (\"⊥¬⊤\", False),         # Reject adjacency\n    (\"⊥A\", False),          # Reject adjacency\n    (\"⊥¬A\", False),         # Reject adjacency\n    (\"(B)C\", False),        # Reject adjacency\n    (\"(A) ∧ (B)\", False),   # Reject spaces\n    (\"A∧(B)\", False),       # Must have parentheses around entire binary expression, not half\n    (\"(A)∧B\", False),       \n    (\"A∧(B)⟹C\", False),\n    (\"(A)∧(B)⟹(C)\", False),\n    (\"((A))\", False)        # Reject double parentheses around a single prop\n]\n\nfor i, (formula, expected_result) in enumerate(test_cases):\n    result = is_well_formed(brackets, connectives, props, formula)\n    print(f\"{i}: {formula}: {result} (Expected: {expected_result})\")\n    assert result == expected_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0: A ∧ B: False (Expected: False)\n1: (A ∧ B): False (Expected: False)\n2: (A ∧ B) ⟹ C: False (Expected: False)\n3: A ∧: False (Expected: False)\n4: A∧: False (Expected: False)\n5: A∧B: False (Expected: False)\n6: (A∧B): True (Expected: True)\n7: (A∧B)⟹C: False (Expected: False)\n8: ((A∧B)⟹C): True (Expected: True)\n9: ¬¬A: False (Expected: False)\n10: ⊤: True (Expected: True)\n11: ⊥: True (Expected: True)\n12: ∨: False (Expected: False)\n13: (): False (Expected: False)\n14: ⊤⊤: False (Expected: False)\n15: ⊤⊥⊤: False (Expected: False)\n16: A: True (Expected: True)\n17: AB: False (Expected: False)\n18: ABC: False (Expected: False)\n19: A¬B: False (Expected: False)\n20: ¬AB: False (Expected: False)\n21: ¬A¬B: False (Expected: False)\n22: ⊥¬⊤: False (Expected: False)\n23: ⊥A: False (Expected: False)\n24: ⊥¬A: False (Expected: False)\n25: (B)C: False (Expected: False)\n26: (A) ∧ (B): False (Expected: False)\n27: A∧(B): False (Expected: False)\n28: (A)∧B: False (Expected: False)\n29: A∧(B)⟹C: False (Expected: False)\n30: (A)∧(B)⟹(C): False (Expected: False)\n31: ((A)): False (Expected: False)\n```\n:::\n:::\n\n\n::: {#68ab1cca .cell execution_count=9}\n``` {.python .cell-code}\n# Example of generating all sequences that are WFF up to length 5\nfrom itertools import product\n\ndef generate_sequences_generator(brackets, connectives, props, n, is_wwf=None):\n    alphabet = brackets + connectives + props\n    for seq in product(alphabet, repeat=n):\n        candidate = ''.join(seq)\n        if is_wwf is None or is_wwf(brackets, connectives, props, candidate):\n            yield candidate\n\ndef gen_all_sequences(brackets, connectives, props, \n                      min_length=1, max_length=4, col_size=4, is_wwf=None):\n    alphabet = brackets + connectives + props\n    lang = []\n    print(\"Generated sequences:\")\n    print(f\"From alphabet: {alphabet}\\n\")\n\n    for length in range(min_length, max_length + 1):\n        seq_list = []\n        for sequence in generate_sequences_generator(brackets, connectives, props, length, is_wwf):\n            seq_list.append(sequence)\n        print(f\"Sequences of length {length}:\")\n        for i in range(0, len(seq_list), col_size):\n            print(\",\".join(seq_list[i : i + col_size]))\n        print()\n        lang.extend(seq_list)\n    return lang\n\nprint(\"\\nGenerating WFF (is_well_formed) up to length 5:\\n\")\nwff = gen_all_sequences(brackets, connectives, props, 1, 5, 4, is_wwf=is_well_formed)\nprint(\"\\nAll WFFs up to length 5:\")\nprint(wff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nGenerating WFF (is_well_formed) up to length 5:\n\nGenerated sequences:\nFrom alphabet: ()∧∨⟹⟺¬◻◊⊤⊥ABC\n\nSequences of length 1:\n⊤,⊥,A,B\nC\n\nSequences of length 2:\n¬⊤,¬⊥,¬A,¬B\n¬C\n\nSequences of length 3:\n\nSequences of length 4:\n\nSequences of length 5:\n(⊤∧⊤),(⊤∧⊥),(⊤∧A),(⊤∧B)\n(⊤∧C),(⊤∨⊤),(⊤∨⊥),(⊤∨A)\n(⊤∨B),(⊤∨C),(⊤⟹⊤),(⊤⟹⊥)\n(⊤⟹A),(⊤⟹B),(⊤⟹C),(⊤⟺⊤)\n(⊤⟺⊥),(⊤⟺A),(⊤⟺B),(⊤⟺C)\n(⊤◻⊤),(⊤◻⊥),(⊤◻A),(⊤◻B)\n(⊤◻C),(⊤◊⊤),(⊤◊⊥),(⊤◊A)\n(⊤◊B),(⊤◊C),(⊥∧⊤),(⊥∧⊥)\n(⊥∧A),(⊥∧B),(⊥∧C),(⊥∨⊤)\n(⊥∨⊥),(⊥∨A),(⊥∨B),(⊥∨C)\n(⊥⟹⊤),(⊥⟹⊥),(⊥⟹A),(⊥⟹B)\n(⊥⟹C),(⊥⟺⊤),(⊥⟺⊥),(⊥⟺A)\n(⊥⟺B),(⊥⟺C),(⊥◻⊤),(⊥◻⊥)\n(⊥◻A),(⊥◻B),(⊥◻C),(⊥◊⊤)\n(⊥◊⊥),(⊥◊A),(⊥◊B),(⊥◊C)\n(A∧⊤),(A∧⊥),(A∧A),(A∧B)\n(A∧C),(A∨⊤),(A∨⊥),(A∨A)\n(A∨B),(A∨C),(A⟹⊤),(A⟹⊥)\n(A⟹A),(A⟹B),(A⟹C),(A⟺⊤)\n(A⟺⊥),(A⟺A),(A⟺B),(A⟺C)\n(A◻⊤),(A◻⊥),(A◻A),(A◻B)\n(A◻C),(A◊⊤),(A◊⊥),(A◊A)\n(A◊B),(A◊C),(B∧⊤),(B∧⊥)\n(B∧A),(B∧B),(B∧C),(B∨⊤)\n(B∨⊥),(B∨A),(B∨B),(B∨C)\n(B⟹⊤),(B⟹⊥),(B⟹A),(B⟹B)\n(B⟹C),(B⟺⊤),(B⟺⊥),(B⟺A)\n(B⟺B),(B⟺C),(B◻⊤),(B◻⊥)\n(B◻A),(B◻B),(B◻C),(B◊⊤)\n(B◊⊥),(B◊A),(B◊B),(B◊C)\n(C∧⊤),(C∧⊥),(C∧A),(C∧B)\n(C∧C),(C∨⊤),(C∨⊥),(C∨A)\n(C∨B),(C∨C),(C⟹⊤),(C⟹⊥)\n(C⟹A),(C⟹B),(C⟹C),(C⟺⊤)\n(C⟺⊥),(C⟺A),(C⟺B),(C⟺C)\n(C◻⊤),(C◻⊥),(C◻A),(C◻B)\n(C◻C),(C◊⊤),(C◊⊥),(C◊A)\n(C◊B),(C◊C)\n\n\nAll WFFs up to length 5:\n['⊤', '⊥', 'A', 'B', 'C', '¬⊤', '¬⊥', '¬A', '¬B', '¬C', '(⊤∧⊤)', '(⊤∧⊥)', '(⊤∧A)', '(⊤∧B)', '(⊤∧C)', '(⊤∨⊤)', '(⊤∨⊥)', '(⊤∨A)', '(⊤∨B)', '(⊤∨C)', '(⊤⟹⊤)', '(⊤⟹⊥)', '(⊤⟹A)', '(⊤⟹B)', '(⊤⟹C)', '(⊤⟺⊤)', '(⊤⟺⊥)', '(⊤⟺A)', '(⊤⟺B)', '(⊤⟺C)', '(⊤◻⊤)', '(⊤◻⊥)', '(⊤◻A)', '(⊤◻B)', '(⊤◻C)', '(⊤◊⊤)', '(⊤◊⊥)', '(⊤◊A)', '(⊤◊B)', '(⊤◊C)', '(⊥∧⊤)', '(⊥∧⊥)', '(⊥∧A)', '(⊥∧B)', '(⊥∧C)', '(⊥∨⊤)', '(⊥∨⊥)', '(⊥∨A)', '(⊥∨B)', '(⊥∨C)', '(⊥⟹⊤)', '(⊥⟹⊥)', '(⊥⟹A)', '(⊥⟹B)', '(⊥⟹C)', '(⊥⟺⊤)', '(⊥⟺⊥)', '(⊥⟺A)', '(⊥⟺B)', '(⊥⟺C)', '(⊥◻⊤)', '(⊥◻⊥)', '(⊥◻A)', '(⊥◻B)', '(⊥◻C)', '(⊥◊⊤)', '(⊥◊⊥)', '(⊥◊A)', '(⊥◊B)', '(⊥◊C)', '(A∧⊤)', '(A∧⊥)', '(A∧A)', '(A∧B)', '(A∧C)', '(A∨⊤)', '(A∨⊥)', '(A∨A)', '(A∨B)', '(A∨C)', '(A⟹⊤)', '(A⟹⊥)', '(A⟹A)', '(A⟹B)', '(A⟹C)', '(A⟺⊤)', '(A⟺⊥)', '(A⟺A)', '(A⟺B)', '(A⟺C)', '(A◻⊤)', '(A◻⊥)', '(A◻A)', '(A◻B)', '(A◻C)', '(A◊⊤)', '(A◊⊥)', '(A◊A)', '(A◊B)', '(A◊C)', '(B∧⊤)', '(B∧⊥)', '(B∧A)', '(B∧B)', '(B∧C)', '(B∨⊤)', '(B∨⊥)', '(B∨A)', '(B∨B)', '(B∨C)', '(B⟹⊤)', '(B⟹⊥)', '(B⟹A)', '(B⟹B)', '(B⟹C)', '(B⟺⊤)', '(B⟺⊥)', '(B⟺A)', '(B⟺B)', '(B⟺C)', '(B◻⊤)', '(B◻⊥)', '(B◻A)', '(B◻B)', '(B◻C)', '(B◊⊤)', '(B◊⊥)', '(B◊A)', '(B◊B)', '(B◊C)', '(C∧⊤)', '(C∧⊥)', '(C∧A)', '(C∧B)', '(C∧C)', '(C∨⊤)', '(C∨⊥)', '(C∨A)', '(C∨B)', '(C∨C)', '(C⟹⊤)', '(C⟹⊥)', '(C⟹A)', '(C⟹B)', '(C⟹C)', '(C⟺⊤)', '(C⟺⊥)', '(C⟺A)', '(C⟺B)', '(C⟺C)', '(C◻⊤)', '(C◻⊥)', '(C◻A)', '(C◻B)', '(C◻C)', '(C◊⊤)', '(C◊⊥)', '(C◊A)', '(C◊B)', '(C◊C)']\n```\n:::\n:::\n\n\nok lets also consider the following minimal connectives example:\n\n::: {#96959377 .cell execution_count=10}\n``` {.python .cell-code}\nmax_length=5\nprint(\"\\nGenerating WFF (is_well_formed) up to length 5:\\n\")\nwff = gen_all_sequences(brackets='()', connectives='∨¬', props='1ABC', min_length=1, max_length=max_length, col_size=4, is_wwf=is_well_formed)\nprint(f\"\\nAll WFFs up to length {max_length}:\")\nprint(wff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nGenerating WFF (is_well_formed) up to length 5:\n\nGenerated sequences:\nFrom alphabet: ()∨¬1ABC\n\nSequences of length 1:\n1,A,B,C\n\nSequences of length 2:\n¬1,¬A,¬B,¬C\n\nSequences of length 3:\n\nSequences of length 4:\n\nSequences of length 5:\n(1∨1),(1∨A),(1∨B),(1∨C)\n(A∨1),(A∨A),(A∨B),(A∨C)\n(B∨1),(B∨A),(B∨B),(B∨C)\n(C∨1),(C∨A),(C∨B),(C∨C)\n\n\nAll WFFs up to length 5:\n['1', 'A', 'B', 'C', '¬1', '¬A', '¬B', '¬C', '(1∨1)', '(1∨A)', '(1∨B)', '(1∨C)', '(A∨1)', '(A∨A)', '(A∨B)', '(A∨C)', '(B∨1)', '(B∨A)', '(B∨B)', '(B∨C)', '(C∨1)', '(C∨A)', '(C∨B)', '(C∨C)']\n```\n:::\n:::\n\n\nas max_length increases the number of sequences we need to generate and test quickly becomes prohibitive.  It takes too long.\n\nWe can however make our generator more efficient by generating the sequences in a more structured way. We can generate all the sequences of length 1, then all the sequences of length 2, then all the sequences of length 3 and so on. This way we can generate all the sequences of length 5 by first generating all the sequences of length 4 and then adding a new symbol to each of these sequences. This is a more efficient way to generate the sequences. Particularly as we now avoid validating sequences. But we also avoid the exponetial growth inherent the set $|\\mathcal{A}|^N$\n\n::: {#ddefda69 .cell execution_count=11}\n``` {.python .cell-code}\nfrom collections import defaultdict\n\ndef generate_wffs_up_to_length(props, connectives_unary, connectives_binary, max_len):\n    \"\"\"\n    Construct all well-formed formulas up to 'max_len' in length.\n    props: single-character propositions or constants (e.g. {'A','B','C','⊤','⊥'})\n    connectives_unary: unary connectives (e.g. {'¬'})\n    connectives_binary: binary connectives (e.g. {'∧','∨','⟹','⟺'})\n    max_len: maximum length of formula strings to generate\n\n    Returns: A dict length -> set of WFF strings of that length\n    \"\"\"\n    wffs_by_length = defaultdict(set)\n    \n    # 1. base case: length = 1\n    # any single-character proposition or constant is a WFF\n    for p in props:\n        wffs_by_length[1].add(p)\n\n    # 2. build up from length=2 to length=max_len\n    for length in range(2, max_len + 1):\n        # (a) unary expansions: for each WFF of length-1, prepend '¬'\n        # new length = old length + 1\n        if length - 1 in wffs_by_length:\n            for w in wffs_by_length[length - 1]:\n                for un_op in connectives_unary:\n                    if not w.startswith(un_op):  # e.g. un_op == '¬'\n                        new_wff = un_op + w\n                        wffs_by_length[length].add(new_wff)\n\n        # (b) binary expansions: for each pair (i, j) with i + j + 3 = length\n        for i in range(1, length - 2):\n            j = length - 2 - i\n            if i in wffs_by_length and j in wffs_by_length:\n                for left_w in wffs_by_length[i]:\n                    for right_w in wffs_by_length[j]:\n                        for bin_op in connectives_binary:\n                            # (φ°ψ) => +3 chars for parentheses + operator\n                            candidate = f\"({left_w}{bin_op}{right_w})\"\n                            wffs_by_length[length].add(candidate)\n\n    return wffs_by_length\n\n# Example usage:\n\nprops = {'A', 'B', 'C', '⊤'}           # single-character props/consts\nunary_connectives = {'¬'}                 # just negation\nbinary_connectives = {'∨'#,'⟹'\n} # typical binary ops\n\nmax_len = 8\nwffs_generated = generate_wffs_up_to_length(\n    props, \n    unary_connectives, \n    binary_connectives, \n    max_len\n)\n\ntot_wffs =0 \nfor length in range(1, max_len + 1):\n    print(f\"There are {len(wffs_generated[length])} WFFs of length {length}:\")\n    print(wffs_generated[length])\n    print()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere are 4 WFFs of length 1:\n{'C', '⊤', 'B', 'A'}\n\nThere are 4 WFFs of length 2:\n{'¬B', '¬A', '¬⊤', '¬C'}\n\nThere are 0 WFFs of length 3:\nset()\n\nThere are 16 WFFs of length 4:\n{'(A∨A)', '(⊤∨⊤)', '(B∨⊤)', '(B∨B)', '(A∨C)', '(⊤∨B)', '(A∨⊤)', '(⊤∨C)', '(C∨B)', '(⊤∨A)', '(C∨C)', '(B∨A)', '(C∨A)', '(B∨C)', '(A∨B)', '(C∨⊤)'}\n\nThere are 48 WFFs of length 5:\n{'¬(C∨B)', '¬(B∨A)', '¬(A∨⊤)', '¬(B∨⊤)', '¬(⊤∨B)', '¬(B∨B)', '¬(⊤∨A)', '(¬⊤∨C)', '(¬C∨⊤)', '(¬A∨C)', '(C∨¬C)', '¬(A∨A)', '¬(C∨A)', '(⊤∨¬A)', '(¬B∨B)', '(¬C∨B)', '(¬B∨A)', '¬(A∨C)', '¬(⊤∨C)', '(¬C∨A)', '(¬B∨C)', '(B∨¬C)', '(¬C∨C)', '(⊤∨¬B)', '(B∨¬A)', '(¬A∨A)', '(¬⊤∨B)', '¬(B∨C)', '(¬⊤∨A)', '(C∨¬A)', '(A∨¬⊤)', '(¬B∨⊤)', '(¬A∨⊤)', '(¬⊤∨⊤)', '(B∨¬B)', '(B∨¬⊤)', '(A∨¬A)', '¬(⊤∨⊤)', '(A∨¬C)', '(A∨¬B)', '¬(A∨B)', '(C∨¬B)', '(¬A∨B)', '(C∨¬⊤)', '(⊤∨¬⊤)', '¬(C∨C)', '(⊤∨¬C)', '¬(C∨⊤)'}\n\nThere are 48 WFFs of length 6:\n{'¬(⊤∨¬A)', '(¬C∨¬⊤)', '¬(¬C∨C)', '¬(B∨¬B)', '¬(⊤∨¬⊤)', '(¬A∨¬C)', '¬(C∨¬A)', '¬(¬C∨A)', '¬(C∨¬B)', '¬(A∨¬C)', '¬(C∨¬⊤)', '¬(¬A∨A)', '¬(¬⊤∨A)', '¬(¬A∨⊤)', '(¬A∨¬A)', '(¬A∨¬⊤)', '(¬⊤∨¬B)', '(¬B∨¬C)', '(¬⊤∨¬⊤)', '(¬⊤∨¬C)', '(¬C∨¬A)', '¬(¬C∨B)', '¬(¬B∨A)', '¬(C∨¬C)', '¬(B∨¬A)', '¬(¬B∨⊤)', '¬(A∨¬⊤)', '¬(⊤∨¬C)', '(¬A∨¬B)', '¬(⊤∨¬B)', '(¬B∨¬⊤)', '¬(¬⊤∨B)', '¬(¬C∨⊤)', '¬(A∨¬A)', '¬(¬A∨B)', '¬(¬⊤∨C)', '¬(B∨¬C)', '¬(¬⊤∨⊤)', '¬(B∨¬⊤)', '(¬B∨¬A)', '¬(¬B∨C)', '(¬C∨¬B)', '¬(¬A∨C)', '¬(A∨¬B)', '(¬⊤∨¬A)', '(¬C∨¬C)', '(¬B∨¬B)', '¬(¬B∨B)'}\n\nThere are 144 WFFs of length 7:\n{'(A∨(A∨C))', '((A∨B)∨A)', '((C∨⊤)∨A)', '¬(¬C∨¬⊤)', '¬(¬⊤∨¬⊤)', '((⊤∨B)∨⊤)', '(B∨(A∨A))', '((⊤∨A)∨B)', '(A∨(⊤∨A))', '¬(¬B∨¬B)', '(A∨(B∨B))', '((C∨C)∨B)', '((A∨A)∨B)', '(C∨(⊤∨C))', '¬(¬A∨¬B)', '(C∨(⊤∨B))', '(A∨(C∨A))', '(B∨(C∨⊤))', '(B∨(C∨A))', '(B∨(C∨B))', '((A∨B)∨C)', '((B∨⊤)∨C)', '(⊤∨(⊤∨A))', '¬(¬⊤∨¬B)', '(⊤∨(C∨B))', '(A∨(C∨⊤))', '(⊤∨(⊤∨C))', '((A∨C)∨B)', '(B∨(B∨⊤))', '(⊤∨(A∨A))', '((C∨A)∨C)', '(B∨(⊤∨C))', '(B∨(⊤∨B))', '(A∨(⊤∨C))', '((⊤∨C)∨B)', '(⊤∨(A∨⊤))', '(A∨(A∨⊤))', '((B∨A)∨A)', '(A∨(B∨A))', '(⊤∨(⊤∨B))', '¬(¬A∨¬⊤)', '((C∨⊤)∨⊤)', '(A∨(B∨C))', '((⊤∨A)∨⊤)', '(A∨(⊤∨⊤))', '(⊤∨(C∨A))', '(A∨(A∨A))', '¬(¬A∨¬A)', '((B∨A)∨C)', '((B∨C)∨⊤)', '(B∨(A∨C))', '((⊤∨C)∨⊤)', '((B∨C)∨A)', '((B∨⊤)∨⊤)', '((B∨C)∨B)', '(⊤∨(A∨C))', '(B∨(B∨B))', '((C∨B)∨B)', '¬(¬⊤∨¬A)', '(⊤∨(B∨A))', '(⊤∨(B∨B))', '(B∨(B∨C))', '((⊤∨B)∨B)', '((C∨B)∨C)', '(B∨(A∨B))', '((⊤∨⊤)∨A)', '(C∨(B∨B))', '(C∨(C∨⊤))', '((B∨B)∨⊤)', '(B∨(A∨⊤))', '((A∨A)∨C)', '((B∨⊤)∨A)', '((A∨⊤)∨B)', '(⊤∨(A∨B))', '(⊤∨(B∨C))', '((⊤∨A)∨A)', '((B∨A)∨⊤)', '((C∨B)∨⊤)', '(C∨(C∨B))', '¬(¬A∨¬C)', '(C∨(C∨C))', '¬(¬C∨¬C)', '(C∨(⊤∨A))', '(C∨(C∨A))', '(C∨(B∨A))', '(B∨(⊤∨A))', '(C∨(B∨⊤))', '((⊤∨A)∨C)', '((C∨C)∨C)', '(⊤∨(C∨C))', '((⊤∨⊤)∨B)', '((C∨A)∨⊤)', '((A∨A)∨⊤)', '((C∨⊤)∨B)', '¬(¬C∨¬B)', '((⊤∨⊤)∨C)', '¬(¬B∨¬⊤)', '((B∨A)∨B)', '((A∨C)∨C)', '((A∨B)∨⊤)', '(A∨(⊤∨B))', '(C∨(B∨C))', '((⊤∨⊤)∨⊤)', '(A∨(C∨C))', '¬(¬⊤∨¬C)', '((A∨C)∨⊤)', '((A∨⊤)∨⊤)', '(A∨(B∨⊤))', '((⊤∨B)∨C)', '((B∨B)∨A)', '((A∨A)∨A)', '(A∨(A∨B))', '(B∨(B∨A))', '((⊤∨C)∨C)', '((B∨B)∨B)', '(C∨(A∨B))', '(C∨(A∨C))', '((C∨C)∨⊤)', '((B∨C)∨C)', '(B∨(C∨C))', '((⊤∨C)∨A)', '¬(¬C∨¬A)', '(C∨(A∨⊤))', '(⊤∨(⊤∨⊤))', '(B∨(⊤∨⊤))', '¬(¬B∨¬A)', '((C∨C)∨A)', '((C∨A)∨A)', '(A∨(C∨B))', '((C∨A)∨B)', '((A∨B)∨B)', '(⊤∨(B∨⊤))', '((C∨⊤)∨C)', '((B∨⊤)∨B)', '((C∨B)∨A)', '(C∨(A∨A))', '((A∨C)∨A)', '((B∨B)∨C)', '(C∨(⊤∨⊤))', '((⊤∨B)∨A)', '(⊤∨(C∨⊤))', '((A∨⊤)∨A)', '¬(¬B∨¬C)', '((A∨⊤)∨C)'}\n\nThere are 640 WFFs of length 8:\n{'¬((A∨A)∨A)', '(¬(⊤∨C)∨⊤)', '((C∨¬C)∨A)', '¬((C∨C)∨A)', '¬(C∨(B∨A))', '¬(A∨(C∨B))', '(B∨¬(B∨A))', '(A∨(¬C∨A))', '(A∨(¬⊤∨B))', '(¬(C∨B)∨⊤)', '(A∨¬(⊤∨C))', '((A∨¬C)∨A)', '((C∨¬B)∨B)', '(¬(C∨C)∨A)', '¬((B∨C)∨A)', '(¬(A∨B)∨⊤)', '(¬(C∨⊤)∨B)', '(¬(B∨A)∨B)', '(A∨(¬B∨C))', '(¬C∨(B∨B))', '¬((⊤∨⊤)∨B)', '¬((A∨A)∨B)', '¬(A∨(⊤∨C))', '((B∨¬C)∨C)', '(¬(A∨A)∨⊤)', '(B∨(B∨¬B))', '(¬(⊤∨B)∨C)', '(C∨(A∨¬A))', '(A∨(¬A∨B))', '(¬(C∨⊤)∨C)', '((¬C∨A)∨B)', '¬(⊤∨(A∨A))', '(C∨(¬B∨⊤))', '(¬(C∨C)∨C)', '(¬(⊤∨A)∨A)', '¬((⊤∨B)∨⊤)', '¬((⊤∨C)∨C)', '((B∨¬A)∨A)', '¬(B∨(⊤∨A))', '¬(C∨(A∨⊤))', '(B∨(⊤∨¬A))', '((A∨¬A)∨A)', '((A∨¬B)∨⊤)', '¬((B∨A)∨C)', '((¬C∨C)∨B)', '(A∨(¬B∨⊤))', '(B∨(B∨¬⊤))', '(¬C∨(C∨C))', '(¬(B∨C)∨B)', '¬((B∨B)∨A)', '¬(C∨(A∨A))', '¬(B∨(C∨⊤))', '(C∨¬(C∨⊤))', '((¬C∨A)∨⊤)', '¬(C∨(C∨⊤))', '(B∨(C∨¬A))', '((B∨C)∨¬B)', '((⊤∨B)∨¬B)', '¬((C∨C)∨⊤)', '¬((C∨B)∨A)', '((¬A∨⊤)∨A)', '((¬C∨B)∨A)', '(A∨¬(⊤∨A))', '((C∨¬A)∨C)', '((¬A∨C)∨⊤)', '¬((B∨A)∨⊤)', '(⊤∨(⊤∨¬B))', '((¬⊤∨B)∨C)', '((⊤∨⊤)∨¬B)', '((C∨B)∨¬A)', '((C∨¬B)∨⊤)', '(¬(A∨C)∨C)', '¬((B∨B)∨C)', '¬(B∨(B∨⊤))', '(A∨(⊤∨¬⊤))', '(¬(A∨C)∨⊤)', '¬((B∨C)∨B)', '¬((C∨⊤)∨A)', '(A∨¬(A∨A))', '¬((⊤∨⊤)∨C)', '(¬B∨(B∨B))', '((B∨A)∨¬A)', '¬((C∨⊤)∨B)', '((C∨¬⊤)∨⊤)', '(¬⊤∨(⊤∨B))', '((B∨¬C)∨B)', '((¬A∨C)∨A)', '(A∨¬(A∨B))', '((¬A∨B)∨⊤)', '((¬C∨⊤)∨⊤)', '¬(⊤∨(C∨A))', '(⊤∨¬(B∨⊤))', '((B∨¬⊤)∨C)', '¬((C∨B)∨⊤)', '(B∨(¬B∨B))', '((⊤∨¬B)∨A)', '(A∨(A∨¬B))', '((⊤∨B)∨¬⊤)', '(¬(C∨⊤)∨A)', '((⊤∨¬⊤)∨C)', '(B∨(¬B∨A))', '(B∨(B∨¬A))', '((A∨¬C)∨C)', '¬((⊤∨A)∨B)', '(⊤∨¬(C∨A))', '(¬B∨(C∨B))', '(¬(C∨A)∨A)', '¬(A∨(⊤∨A))', '(¬(C∨B)∨B)', '(¬B∨(B∨C))', '(B∨(⊤∨¬B))', '(C∨(¬C∨B))', '¬(A∨(A∨C))', '(¬⊤∨(⊤∨A))', '((¬⊤∨C)∨B)', '((¬⊤∨⊤)∨C)', '(⊤∨(¬⊤∨⊤))', '(B∨¬(⊤∨A))', '(¬C∨(A∨⊤))', '((C∨C)∨¬A)', '¬(⊤∨(⊤∨B))', '(¬B∨(A∨⊤))', '((A∨C)∨¬A)', '¬(B∨(A∨A))', '(B∨(¬B∨⊤))', '(¬B∨(C∨⊤))', '(¬A∨(B∨A))', '(⊤∨¬(⊤∨C))', '((C∨¬B)∨A)', '(C∨(¬C∨⊤))', '¬((⊤∨A)∨A)', '(⊤∨(¬B∨A))', '((⊤∨¬C)∨B)', '(A∨(¬C∨C))', '(B∨(C∨¬C))', '¬(C∨(B∨⊤))', '((C∨⊤)∨¬A)', '((¬⊤∨B)∨A)', '((¬C∨⊤)∨B)', '((¬⊤∨⊤)∨B)', '((⊤∨¬C)∨A)', '(B∨(C∨¬⊤))', '(¬A∨(C∨A))', '(A∨(¬⊤∨⊤))', '((⊤∨¬⊤)∨A)', '((B∨C)∨¬C)', '¬((A∨A)∨⊤)', '(A∨(¬C∨B))', '(A∨¬(A∨C))', '(¬A∨(C∨B))', '¬((A∨⊤)∨B)', '((¬B∨B)∨A)', '((¬A∨A)∨C)', '((C∨C)∨¬B)', '((B∨¬B)∨C)', '¬((A∨B)∨B)', '¬((A∨⊤)∨C)', '(¬⊤∨(A∨A))', '(⊤∨(C∨¬⊤))', '(¬C∨(A∨A))', '¬((⊤∨C)∨⊤)', '((¬⊤∨B)∨⊤)', '((⊤∨C)∨¬⊤)', '(B∨(B∨¬C))', '((¬A∨B)∨A)', '(¬(A∨B)∨B)', '(¬(⊤∨⊤)∨⊤)', '(C∨(¬B∨A))', '(¬⊤∨(C∨C))', '((A∨A)∨¬C)', '(¬(⊤∨A)∨C)', '(¬(B∨B)∨A)', '¬(C∨(⊤∨B))', '¬((A∨C)∨A)', '(C∨(C∨¬⊤))', '((C∨¬C)∨⊤)', '((⊤∨A)∨¬⊤)', '(¬A∨(⊤∨B))', '(B∨¬(C∨C))', '(¬A∨(A∨A))', '((¬B∨B)∨C)', '((¬B∨A)∨A)', '(B∨(A∨¬A))', '¬(⊤∨(C∨⊤))', '((¬B∨C)∨B)', '¬((B∨C)∨C)', '((B∨¬C)∨A)', '((¬A∨C)∨B)', '(B∨(⊤∨¬⊤))', '¬(⊤∨(A∨⊤))', '((⊤∨¬⊤)∨⊤)', '(⊤∨(¬C∨C))', '((B∨⊤)∨¬⊤)', '(¬C∨(C∨A))', '(C∨(⊤∨¬B))', '(A∨(A∨¬A))', '(¬⊤∨(C∨A))', '(¬C∨(⊤∨C))', '((¬B∨⊤)∨⊤)', '((B∨¬⊤)∨⊤)', '(A∨(B∨¬A))', '(¬(A∨C)∨A)', '(C∨(⊤∨¬A))', '¬(B∨(B∨B))', '(C∨¬(B∨B))', '((B∨¬⊤)∨B)', '(A∨(¬⊤∨C))', '((¬⊤∨A)∨⊤)', '(C∨(¬A∨B))', '¬(C∨(C∨B))', '(¬A∨(A∨C))', '(¬⊤∨(A∨C))', '(¬C∨(C∨B))', '((⊤∨¬C)∨⊤)', '(B∨¬(A∨⊤))', '¬(A∨(A∨⊤))', '((B∨A)∨¬⊤)', '((¬B∨⊤)∨C)', '((B∨¬B)∨A)', '(C∨(B∨¬C))', '((B∨A)∨¬B)', '(¬(⊤∨B)∨A)', '(⊤∨(¬B∨C))', '(⊤∨¬(A∨B))', '((A∨C)∨¬C)', '(¬(B∨A)∨A)', '(¬(B∨⊤)∨C)', '¬(C∨(B∨B))', '((B∨¬C)∨⊤)', '(C∨(¬A∨⊤))', '(B∨(¬⊤∨A))', '((A∨A)∨¬B)', '(B∨(¬⊤∨C))', '((C∨⊤)∨¬B)', '((C∨¬A)∨B)', '(¬(C∨A)∨B)', '((¬⊤∨A)∨A)', '((C∨B)∨¬⊤)', '((¬⊤∨C)∨⊤)', '(B∨¬(A∨A))', '(A∨(B∨¬⊤))', '((⊤∨¬B)∨⊤)', '(A∨(B∨¬B))', '¬(⊤∨(B∨B))', '(¬(A∨⊤)∨A)', '(¬(B∨B)∨B)', '¬(A∨(B∨⊤))', '(¬(B∨⊤)∨A)', '(⊤∨(⊤∨¬A))', '(B∨¬(A∨C))', '(⊤∨(B∨¬C))', '(⊤∨¬(C∨⊤))', '(¬(C∨C)∨B)', '(¬B∨(C∨C))', '¬(A∨(B∨A))', '((⊤∨⊤)∨¬⊤)', '((A∨¬A)∨B)', '¬((⊤∨C)∨B)', '(C∨(¬⊤∨B))', '¬(C∨(C∨C))', '(B∨¬(⊤∨⊤))', '(¬B∨(A∨B))', '(⊤∨(B∨¬⊤))', '¬(B∨(C∨C))', '¬((A∨C)∨C)', '(¬⊤∨(B∨C))', '((C∨C)∨¬C)', '¬((B∨B)∨B)', '¬((⊤∨C)∨A)', '(¬(B∨A)∨C)', '(A∨(A∨¬⊤))', '¬(A∨(B∨B))', '((B∨¬B)∨⊤)', '((¬B∨C)∨A)', '(⊤∨(¬A∨⊤))', '(B∨¬(B∨⊤))', '(⊤∨(¬C∨A))', '(C∨(¬⊤∨A))', '(A∨(A∨¬C))', '¬((B∨⊤)∨C)', '(B∨(¬A∨C))', '(⊤∨(¬B∨B))', '((B∨⊤)∨¬C)', '(¬A∨(B∨C))', '(¬(B∨C)∨C)', '(C∨(⊤∨¬⊤))', '((A∨A)∨¬A)', '(C∨¬(B∨A))', '((¬A∨A)∨B)', '(¬⊤∨(A∨⊤))', '((A∨¬B)∨A)', '((¬C∨A)∨C)', '(¬(A∨A)∨A)', '(¬(A∨B)∨C)', '(¬B∨(⊤∨⊤))', '(A∨¬(B∨C))', '(C∨¬(⊤∨B))', '¬(B∨(C∨A))', '(¬⊤∨(C∨⊤))', '¬(B∨(C∨B))', '¬((C∨⊤)∨⊤)', '(A∨(C∨¬B))', '¬((C∨C)∨B)', '(¬C∨(⊤∨A))', '((⊤∨¬B)∨C)', '(¬(A∨A)∨C)', '((A∨¬B)∨B)', '((⊤∨⊤)∨¬A)', '((¬⊤∨A)∨C)', '(B∨¬(C∨B))', '(¬⊤∨(B∨B))', '(A∨(C∨¬⊤))', '((C∨¬⊤)∨B)', '¬(B∨(A∨C))', '(B∨(¬A∨B))', '(B∨(¬C∨A))', '(¬(A∨⊤)∨⊤)', '((B∨¬A)∨C)', '(C∨(¬⊤∨⊤))', '(C∨¬(⊤∨⊤))', '((C∨¬⊤)∨A)', '¬(C∨(⊤∨A))', '((¬B∨C)∨⊤)', '(C∨(¬B∨C))', '¬((A∨B)∨⊤)', '¬((B∨A)∨A)', '(C∨¬(C∨A))', '(B∨¬(A∨B))', '(A∨(⊤∨¬B))', '((B∨B)∨¬C)', '(C∨(B∨¬B))', '(C∨(⊤∨¬C))', '(B∨(¬B∨C))', '(⊤∨¬(B∨A))', '¬((⊤∨B)∨B)', '(A∨(⊤∨¬C))', '(¬B∨(B∨A))', '(C∨(¬A∨A))', '(B∨(¬C∨⊤))', '(C∨(¬A∨C))', '(B∨(A∨¬C))', '((¬A∨A)∨⊤)', '(⊤∨(B∨¬B))', '((C∨¬A)∨A)', '(¬(B∨A)∨⊤)', '(B∨(¬A∨⊤))', '((⊤∨¬A)∨⊤)', '((¬C∨C)∨⊤)', '((⊤∨C)∨¬A)', '((¬B∨A)∨⊤)', '¬(B∨(⊤∨⊤))', '(¬(⊤∨B)∨⊤)', '((C∨A)∨¬⊤)', '((B∨C)∨¬⊤)', '(¬(B∨⊤)∨⊤)', '(B∨(¬A∨A))', '(⊤∨(⊤∨¬⊤))', '(⊤∨¬(B∨C))', '((B∨⊤)∨¬B)', '((C∨¬A)∨⊤)', '(¬⊤∨(B∨⊤))', '(¬C∨(C∨⊤))', '(⊤∨(C∨¬C))', '((¬C∨C)∨A)', '(⊤∨(B∨¬A))', '((A∨⊤)∨¬A)', '((¬A∨⊤)∨C)', '(¬⊤∨(B∨A))', '(⊤∨¬(A∨A))', '(B∨¬(B∨B))', '((¬B∨A)∨B)', '((A∨C)∨¬B)', '((B∨¬A)∨⊤)', '((B∨¬⊤)∨A)', '((C∨¬B)∨C)', '(A∨¬(B∨B))', '(¬(A∨B)∨A)', '((C∨¬⊤)∨C)', '((C∨B)∨¬C)', '¬(C∨(A∨C))', '((¬B∨B)∨B)', '(A∨(¬C∨⊤))', '((C∨B)∨¬B)', '((A∨⊤)∨¬C)', '¬((⊤∨⊤)∨⊤)', '(⊤∨¬(C∨B))', '((A∨⊤)∨¬B)', '¬(B∨(A∨⊤))', '¬(⊤∨(⊤∨C))', '¬((B∨A)∨B)', '(B∨¬(C∨⊤))', '¬(B∨(B∨C))', '¬((⊤∨B)∨C)', '(¬⊤∨(A∨B))', '((⊤∨C)∨¬B)', '(¬(B∨B)∨⊤)', '(¬(⊤∨⊤)∨A)', '¬(⊤∨(C∨C))', '((¬A∨⊤)∨B)', '(B∨¬(C∨A))', '(¬(⊤∨C)∨C)', '((A∨¬⊤)∨B)', '((⊤∨A)∨¬B)', '(¬A∨(⊤∨C))', '((C∨¬C)∨C)', '(¬(⊤∨A)∨B)', '((¬A∨⊤)∨⊤)', '¬(⊤∨(A∨C))', '((A∨B)∨¬A)', '(⊤∨¬(⊤∨B))', '(C∨¬(B∨⊤))', '¬((A∨C)∨B)', '(A∨(B∨¬C))', '(¬(C∨A)∨⊤)', '¬(C∨(A∨B))', '(¬(B∨C)∨⊤)', '(B∨(¬C∨C))', '((¬B∨⊤)∨A)', '(¬(C∨A)∨C)', '((¬B∨C)∨C)', '(C∨(C∨¬C))', '((¬B∨B)∨⊤)', '(C∨¬(C∨B))', '(C∨(B∨¬A))', '¬((B∨⊤)∨⊤)', '(¬A∨(C∨C))', '((¬B∨A)∨C)', '¬((B∨C)∨⊤)', '((A∨¬⊤)∨A)', '((¬A∨B)∨C)', '((⊤∨¬⊤)∨B)', '(¬(⊤∨C)∨B)', '((C∨A)∨¬C)', '(A∨¬(A∨⊤))', '¬((A∨B)∨A)', '(¬B∨(B∨⊤))', '((C∨⊤)∨¬⊤)', '(¬⊤∨(C∨B))', '((⊤∨¬B)∨B)', '(A∨¬(C∨A))', '¬(B∨(⊤∨C))', '(¬A∨(⊤∨⊤))', '¬((B∨B)∨⊤)', '((¬C∨⊤)∨A)', '((¬C∨B)∨⊤)', '(⊤∨¬(⊤∨⊤))', '((¬A∨C)∨C)', '(¬(⊤∨⊤)∨B)', '((⊤∨¬C)∨C)', '¬((C∨A)∨C)', '¬((C∨B)∨B)', '((A∨A)∨¬⊤)', '¬((B∨⊤)∨B)', '¬((⊤∨A)∨C)', '((A∨B)∨¬⊤)', '¬(A∨(B∨C))', '(⊤∨(A∨¬B))', '¬((C∨B)∨C)', '¬((A∨⊤)∨⊤)', '¬(⊤∨(C∨B))', '(C∨¬(C∨C))', '((¬⊤∨⊤)∨⊤)', '((⊤∨¬A)∨C)', '(C∨¬(B∨C))', '(A∨(⊤∨¬A))', '¬(⊤∨(B∨⊤))', '((B∨C)∨¬A)', '¬(⊤∨(A∨B))', '(A∨(¬⊤∨A))', '(A∨¬(C∨C))', '¬((A∨A)∨C)', '((¬C∨B)∨B)', '((C∨C)∨¬⊤)', '(B∨(⊤∨¬C))', '(¬B∨(A∨C))', '(A∨¬(C∨B))', '((A∨¬B)∨C)', '(A∨¬(⊤∨⊤))', '(⊤∨(A∨¬C))', '¬(A∨(A∨A))', '(¬(⊤∨A)∨⊤)', '(C∨(A∨¬⊤))', '¬((B∨⊤)∨A)', '(¬A∨(B∨⊤))', '((¬B∨⊤)∨B)', '(⊤∨(¬A∨C))', '(⊤∨(A∨¬⊤))', '((⊤∨B)∨¬A)', '(¬A∨(A∨B))', '(⊤∨¬(⊤∨A))', '(⊤∨¬(A∨⊤))', '((A∨⊤)∨¬⊤)', '((¬⊤∨A)∨B)', '¬(⊤∨(⊤∨⊤))', '(¬(B∨B)∨C)', '(⊤∨(¬⊤∨B))', '(A∨(¬B∨A))', '(B∨(A∨¬B))', '((⊤∨A)∨¬A)', '(¬(⊤∨⊤)∨C)', '(⊤∨(¬⊤∨C))', '¬(A∨(⊤∨⊤))', '¬(⊤∨(B∨A))', '(⊤∨(¬B∨⊤))', '(¬C∨(B∨⊤))', '(⊤∨(¬A∨A))', '(⊤∨(¬C∨⊤))', '(C∨(C∨¬B))', '(B∨(¬⊤∨B))', '((⊤∨A)∨¬C)', '((¬C∨B)∨C)', '(¬(A∨A)∨B)', '((¬⊤∨B)∨B)', '¬(A∨(A∨B))', '(B∨(A∨¬⊤))', '((A∨¬⊤)∨⊤)', '(¬⊤∨(⊤∨C))', '(⊤∨¬(A∨C))', '(C∨¬(A∨⊤))', '(A∨¬(B∨⊤))', '(C∨(¬C∨C))', '¬(C∨(C∨A))', '(C∨¬(A∨A))', '(¬B∨(⊤∨B))', '(C∨¬(⊤∨A))', '((A∨C)∨¬⊤)', '¬((A∨B)∨C)', '¬(A∨(C∨A))', '(A∨(¬A∨⊤))', '(⊤∨(A∨¬A))', '(A∨(C∨¬A))', '(C∨(B∨¬⊤))', '(¬C∨(⊤∨B))', '¬((⊤∨B)∨A)', '((⊤∨C)∨¬C)', '(¬B∨(⊤∨A))', '(¬A∨(A∨⊤))', '(⊤∨¬(B∨B))', '((¬C∨⊤)∨C)', '((⊤∨¬A)∨B)', '((¬⊤∨⊤)∨A)', '(¬(C∨B)∨C)', '(⊤∨¬(C∨C))', '(B∨¬(B∨C))', '(¬(A∨C)∨B)', '(¬C∨(A∨B))', '((A∨¬⊤)∨C)', '(¬(⊤∨B)∨B)', '¬((⊤∨A)∨⊤)', '(⊤∨(¬⊤∨A))', '(⊤∨(⊤∨¬C))', '((A∨¬A)∨⊤)', '¬((A∨C)∨⊤)', '(C∨¬(⊤∨C))', '(¬A∨(C∨⊤))', '(¬C∨(B∨A))', '(C∨¬(A∨B))', '(¬C∨(⊤∨⊤))', '¬(⊤∨(⊤∨A))', '((¬⊤∨C)∨A)', '((B∨B)∨¬⊤)', '¬(C∨(⊤∨C))', '(B∨¬(⊤∨B))', '(⊤∨(¬C∨B))', '(¬B∨(⊤∨C))', '¬(⊤∨(B∨C))', '(A∨¬(⊤∨B))', '¬(C∨(⊤∨⊤))', '(¬B∨(C∨A))', '¬(A∨(C∨⊤))', '((B∨B)∨¬B)', '(A∨(¬A∨A))', '¬((C∨⊤)∨C)', '¬(C∨(B∨C))', '((¬⊤∨C)∨C)', '(¬(C∨B)∨A)', '(A∨(¬A∨C))', '(C∨(A∨¬C))', '((A∨B)∨¬C)', '¬((C∨A)∨⊤)', '¬((C∨A)∨B)', '(¬(⊤∨C)∨A)', '((A∨¬C)∨B)', '(⊤∨(C∨¬B))', '(A∨¬(C∨⊤))', '((¬A∨B)∨B)', '¬((C∨C)∨C)', '((⊤∨B)∨¬C)', '(C∨(¬⊤∨C))', '(C∨(C∨¬A))', '((B∨¬A)∨B)', '¬(B∨(B∨A))', '(¬(C∨C)∨⊤)', '((C∨A)∨¬B)', '((C∨¬C)∨B)', '((¬A∨A)∨A)', '((A∨B)∨¬B)', '((⊤∨¬A)∨A)', '¬((C∨A)∨A)', '¬(B∨(⊤∨B))', '(⊤∨(¬A∨B))', '((B∨⊤)∨¬A)', '((⊤∨⊤)∨¬C)', '(¬(B∨⊤)∨B)', '(¬A∨(B∨B))', '(¬(A∨⊤)∨B)', '(¬(B∨C)∨A)', '(¬(C∨⊤)∨⊤)', '(C∨(¬C∨A))', '((B∨B)∨¬A)', '(A∨(C∨¬C))', '(A∨(¬B∨B))', '((A∨¬A)∨C)', '¬(B∨(A∨B))', '(C∨(¬B∨B))', '¬((A∨⊤)∨A)', '¬((⊤∨⊤)∨A)', '(⊤∨(C∨¬A))', '¬(A∨(C∨C))', '(¬C∨(B∨C))', '(¬C∨(A∨C))', '(B∨(C∨¬B))', '((C∨A)∨¬A)', '((¬C∨C)∨C)', '(B∨(¬⊤∨⊤))', '(¬B∨(A∨A))', '(¬A∨(⊤∨A))', '(C∨(A∨¬B))', '((B∨¬B)∨B)', '((C∨⊤)∨¬C)', '(¬⊤∨(⊤∨⊤))', '(B∨(¬C∨B))', '((A∨¬C)∨⊤)', '(¬(A∨⊤)∨C)', '((¬C∨A)∨A)', '¬(A∨(⊤∨B))', '(B∨¬(⊤∨C))', '(A∨¬(B∨A))', '(C∨¬(A∨C))', '((B∨A)∨¬C)'}\n\n```\n:::\n:::\n\n\nGiven that.\n01:47\nIt can.\n1\nSpeaker 1\n01:58\nGenerate\n2\nSpeaker 2\n01:59\ncompatible patterns.\n02:05\nWith a certain signaling. System.\n02:16\nPatterns, that are incompatible with a certain signaling State.\n02:25\nGiven this.\n1\nSpeaker 1\n02:32\nThe signaling\n2\nSpeaker 2\n02:33\nstate that will emerge. Will be. That is compatible.\n02:42\nThe sequence is generated.\n1\nSpeaker 1\n03:01\nIt will\n2\nSpeaker 2\n03:02\nbe. Compositional.\n1\nSpeaker 1\n03:17\nIf the\n2\nSpeaker 2\n03:17\npatter, Generated.\n03:22\nCompatible.\n03:27\nWith the Of homophism.\n03:35\nHomomorphism.\n03:41\nIn other words, if\n03:46\nReserves the structure of the pre-linguistic object.\n03:55\nPreserves the structure. Over the pre-linguistic object.\n04:05\nOtherwise.\n1\nSpeaker 1\n04:15\nDespite\n2\nSpeaker 2\n04:16\nusing. A language built over in alphabet of\n04:26\nAtomic signals. We end up. For the simple noise, sign in the game. That is not discriminate.\n04:40\nBetween symbols.\n04:46\nAnd that's called the Nissan, the full sequences.\n1\nSpeaker 1\n04:55\nCoordinates\n2\nSpeaker 2\n04:56\nover. Complete sequences rather than considering coordinating on the atomics. Symbols.\n05:20\nAbout that.\n06:36\nNow, if we just consider arbitrary symbols,\n06:46\nOr rather arbitrary sequences are generated. China, likely. The sequence to the homorphism.\n07:01\nThis doesn't mean.\n07:15\nThat we can't ever have. Compositionality baked in. It that boils down. If we're able. Um, Discard bad early choices. And, Make use of. Better ones down the line. What do I?\n08:07\nLet's simplify things by imagining the only two possible signaling systems possible. The. It's compositional and the second is,\n08:24\nFully Tangled, for example. Fully entangled means that it is a simple signaling systems. There are no composite signals Rather no complexing no complex signals.\n08:50\nSo we have a complex system and a simple system.\n09:01\nThe, the chance of emitting a sequence that's compatible with the complex system is going to be\n09:14\nBut fixed. Particularly if we don't have a lot of messages.\n09:28\nIf we do have a lot of messages, that's a different story.\n09:36\nWhich we should consider.\n09:46\nWe do realize that we're more likely to have a complex system emerge with larger. Input spaces.\n10:06\nLet's suppose we keep emitting random signals. And,\n10:26\nEven if the Receiver. Guess is the correct answer. There is a small probability Epsilon. Of emitting. Another sequence.\n10:48\nPossibly.\n10:55\nWhat that is already news. What that is already in use.\n11:13\nAny given message?\n11:20\nOn the other hand.\n11:30\nThe receiver.\n11:45\nWell, in this case.\n11:53\nHave to make guess. Six. Sequence. Seeing this new sequence.\n12:16\nGive it that. There is already. Signaling system in place. And, This side usually gets classifier. This side via its classifier. What the speeds?\n12:40\nWhat this means. What this sequence means, right?\n12:57\nThen we might end up by chance.\n13:04\nSecond signal. Well compatible. More compatible.\n13:21\nThat. B system. Which is the complex system rather than with the simple system.\n13:37\nIn other words, It satisfies certain symmetry. Extend with The linguistic object.\n13:55\nThe classifier. Has already picked up. On the Symmetry. It is.\n14:18\nReinforce this aspect of the classifier,\n14:27\nNotice that the classifier. If this case is not killed with the messages, Specifically worked with\n14:48\nDecode. The prolinguistic object.\n15:13\nNot that this is important. The important thing is, That.\n15:21\nThe classifier sides. The classifier, assigns These messages.\n15:37\nThe correct.\n15:42\nPre-Linguistic object. So now, let's imagine we have two of these.\n16:11\nThe inductive bar of the classifier. The inductive bias of the classifier.\n16:22\nMight be. Similar to A3. A decision tree. What does that mean for us?\n16:48\nSpeeds the Is traditionally. The most likely splitified. For this eyelid.\n17:48\nActually it's a mistake they're trying to dig in into this un. We build it this way specifically. So let's set this aside. Just consider this Stochastic. Let's imagine that we have a stochastic. Decision mechanism.\n18:17\nIf we have this stochastic decision mechanism,\n18:25\nPerhaps. The distribution. The. Possible. Classifiers. And that we are somehow able to pick.\n18:46\nRandomly out of a subset of. Um, Classifiers. The ones which Those compatible with the historic data. In such a setting. Our future. Predictions. To be dependent on previous ones. But so long, As we keep on going. We will go to more and more compatible signals. With complex. Signaling rather than with, Simple.\n19:34\nHow is that? That's actually.\n19:43\nIt's actually wrong. Yeah, we're gonna get more incompatible ones. That's bad news. But\n20:02\nAssuming. That there is. Very large number. Of both types.\n20:18\nOr perhaps, even an infinite number of both types doesn't really matter, does it. So we keep on going. Uh, we have\n20:35\nWe have initially a kind of System was A few complex signals baked in, but mostly Um, Simple entangled representations but we keep on going because our classifier is not so good. It keeps needs to get more good examples. Can only train if it gets a good example. So, it's slow.\n21:17\nAnd during this, we keep on coordinating and what that means is we get a chance to switch Signals with meetings, using the exploration. Okay.\n21:48\nThis. Yeah. I suppose we Have multiple wins. Multiple ways for some signal, for example. Red cylinder.\n22:22\nA bunch of them are.\n22:29\nHe's compatible.\n22:34\nWhich the complex signaling system, but most of them are sorry but one of them is good. And let's say this is true.\n22:51\nFor another. One. And another one. And another one.\n23:02\nDoes my hypothesis.\n23:10\nIn such a case. The reinforcement capabilities. In the algorithm. Can be such that in the limit. We will forget the incompatible ones. And remember the compatible ones. How is this possible? Simply stated. Simply stated. This can be possible if.\n23:54\nRather, before we say, simply stated, let's consider that having both of Signaling systems at once. Is a mixed equilibrium. And this is the real hypothesis. Given that you have a mixed equilibrium. For partial. For the partially observed language.\n24:26\nWhich contains both a composite. And,\n24:41\nSimple equilibrium. And, The input space is sufficiently large.\n24:54\nThat the attraction.\n25:00\nOr stability. Yeah. The, the complex system is going to be should be Were attractive more stable and thus attract. Attract. What does that mean? It means as we learn. This process. Continues. The mixed equilibrium should. Converge. To a pure equilibrium on. Complex. Signaling system. Rather than\n25:51\nMixed one. Um,\n26:01\nOther one, the Entangled representation. Given that. Given that. The classifier can disentangle things, if it can't. This won't happen. In other words, we're saying that. If the classifier Is able to learn. To represent within itself. The substates.\n26:44\nIt will be able to reinforce not just the full symbol but the substates. If it can reinforce on the sub States, Will eventually be able to. Reject forget whatever the bad States. The entangled.\n27:10\nAnd keep the good ones. Compound ones. Another interesting thing is, suppose the leading Is entangled.\n27:28\nAnd uses the most. Uses. This an atomic symbol. Just one. This might be. Beneficial. You might not want to learn. A. Compound. Symbol for it. Why is this? It's more efficient. Even given the possibility of full. Regular system. Might be more efficient to code. The signaling system. Ignore by ignoring this.\n28:13\nBy ignoring us. So, by ignoring The. Regular. Compound form and keeping I'm not by keeping a simple. A simple. Signal. Right, we use an atomic symbol here. But this would be to the detriment.\n28:48\nBeing unable to use this. As the final. Symbol. In all the other. Encodings. Might still be efficient if you only use this once.\n29:10\nAnd we might be able to use this second atomic symbol. In this fashion.\n29:22\nPerhaps for the second, most Common state, assuming that, it's These first three states are significantly. Orders of magnitude more likely than Either states, right? So we give A and B. And then we use. For petting, the rest of the messages.\n29:55\nCc. Ccpa. And Force free seasons, a and three C's with a b. C's.\n30:15\nCab. Sorry CBA. Ccba and so on.\n30:29\nAnyhow.\n30:34\nI think this is the way that we could have. Complex. Signaling system emerge.\n30:48\nA complex signaling systems with a few persistent. A few persistent.\n31:06\nA few persistent irregular forms.\n\n---\n\n\n## The Third Question - Signals\n\nThe third question. Third question. Has to do with what? Of this. Referential game. Makes the big Improvement over the plain vanilla Lewis game. But clearly. The multiple choice aspect of it. Speeds are flirting makes the decision. Quicker.\n00:37\nUm, but they think that That's not what introduces. In equilibrium.\n00:53\nIt's not what? Introduces new equilibriums.\n01:02\nSo, after the notifications, do we have? Well, one bit Is that we have image classifiers?\n01:18\nIt's that. Steadiness.\n01:26\nMaybe.\n01:31\nSo, introducing a classifier,\n01:39\nMay well be equivalent. To introducing Grandma.\n01:49\nNot Grandma a grammar.\n02:09\nWhy is that? Because the grammar is Can be viewed as well. Can be viewed as a decision rule. And,\n02:28\nWhat grammar does essentially?\n2\nSpeaker 2\n02:30\nUh,\n1\nSpeaker 1\n02:31\ntaking a bunch of inputs and\n02:36\nClassifies. This certain state?\n02:46\nClassifies the inputs into a specific. The state is semantic. But,\n02:59\nIn the world of neural networks that state made the what we need to reconstruct an image. Or some other pretty linguistic options.\n03:13\nThree linguistic object.\n03:19\nWow, this is a very bad transcription.\n03:39\nAnyway, since we can't transcribe pre-linguistic objects, what we should. Say, is a\n2\nSpeaker 2\n03:51\nSo\n1\nSpeaker 1\n03:52\nwe can transcribe it so we can.\n03:58\nUse discommerce a decision rule, which takes inputs. And, and The coached them into a state.\n04:10\nThe coach them into a state. Decode. The interstate. Into a state.\n04:28\nOkay.\n04:38\nIf we have a certain, Classify working on the inputs. We should be able to recover the original state, the slowest.\n04:56\nThe senders encoded them in a faithful way in a way, which we haven't. And the essential information.\n05:12\nIf the coding also has a\n05:19\nHomophism and homomorphismic properties.\n05:27\nThen our classifier should be able to learn.\n05:34\nWe cover the state much more easily. Taking advantage of the symmetries.\n05:46\nInherent in. Message.\n06:03\nWhat else do we have in this signaling game? Or rather.\n06:17\nReferential game. So, we have\n06:28\nA protocol.\n06:34\nNo, we have something else. We have this thing called the random. Message generally.\n06:45\nSurprisingly enough.\n06:51\nThis is. A great situation for bl. Blind watchmaker to position themselves.\n07:03\nWhat they mean here is that\n07:11\nThe sender serendipitously. Generates a Message.\n07:22\nThat can be easily. Broken down. Into units.\n07:34\nThe receiver will end. We'll learn this structure. Or rather we learned this message.\n07:48\nAnd since it is a classifier, If it gets a bunch of these,\n07:57\nIt will be much more likely to. Interpret.\n08:07\nSimilar messages. Correctly and unlikely to interpret different messages correctly.\n08:26\nI considered. Situations. The sender. Makes percentage persistent use. Of the same messages for this, for subsequent States.\n08:46\nBut if he fails And there is an unlimited number of available signals. He could. Generate the new signal.\n09:05\nThis makes learning slower.\n09:14\nThis allows.\n09:20\nThe receiver. More latitude.\n09:28\nIn the sense that\n09:34\nBut randomly generating new signals. If we come with a signal, That's a good fit. For the classifier. Based on what it is, already figured out. We're gonna be more likely.\n09:55\nCoordinate.\n10:00\nA signal. Following the structure. That we learned and maybe the semantics that we learned.\n10:13\nSo, we can think of this. S exploration.\n10:19\nAnother idea.\n10:24\nWhich I had for simpler signaling systems, is to try to swap out signals. This was inspired by Hoffman Coatings. Frequent.\n10:44\nUh, signals would get shorter, encodings If? Distributions evolved over time. And we realized that the rare signal initially is actually now quite common. We would want to Swap it. With some other signal, which we thought was quite common but now we see us Infrequent.\n11:23\nThis ability to swap. Would need to be coordinated, too.\n11:32\nBut we could have a protocol for doing this automatically.\n11:43\nWe could at every time step. Uh, reconsider All signals.\n11:57\nFrequencies and variances. And use these two details. To reorder. Their semantics.\n12:12\nIn other words to associate shorter signals with Frequent States and longer signals with infrequent States.\n12:33\nWhat about longer signals? How do we optimize these? We need the most sophisticated exploratory strategy.\n12:50\nOne smart way might be to use some kind of a function So,\n12:59\nI would call this. Looking at the search base. Homomorphisms. And homeomorphisms. Both. That would make more sense in the realm of the lie algebra.\n13:24\nSuppose we do have. Operators. Which preserve Allah. We might want to. Try to use these.\n13:38\nComponent of Sequence generator. We will want to use these to map the state to sequence.\n13:53\nFor the sequence. With the the image of this layout in the loops, Another set.\n14:09\nLie, algebra group.\n14:14\nOn the set.\n14:23\nI don't think my job here is to Solve this, but It appears that.\n14:36\nThat, what what? What, what? What?\n14:44\nYes, yes. Yes. Yeah, it would appear that. The two beds.\n15:01\nReferential game to capture Coptic signals.\n15:11\nComplex signals are\n15:17\nA generator.\n15:21\nIs aminable. To create.\n15:32\nNice.\n15:37\nRepresentations. Nice sequences which are the compass composable.\n15:49\nWhich are composable.\n15:54\nThey're not composable, but they are. Amountable. To disentanglement. Of meaning.\n16:07\nThis is all fine.\n16:13\nVia trend and error. As long as we don't have to, We use symbols. If, for yourself, we want to reuse symbols, you really need to have Planning generator. In other words, we want the sender to\n16:41\nThe messages. So that\n16:50\nWhen he sends blue banana. He's already thought about.\n16:58\nGreen banana and yellow banana.\n17:06\nAnd reserved. The symbols for banana green, blue, and yellow. And we purchased these symbols. Reserve these symbols.\n17:28\nWhen such States arise in the future.\n17:35\nSo this is the planning. I've been thinking about, In the late signal games.\n17:52\nWhen it comes to arbitration, when it comes to arbitrary. When it comes to arbitrary States.\n18:06\nPlanning is more challenging.\n18:11\nAnd still, we will. Similar states to have similar embeddings. In other words to be close.\n18:24\nThe linguistic manifold as well as\n18:32\nThree-Linguistic manifold. Pretty linguistic manifold. The state manifold.\n19:03\nBetter yet if we can also. Support. Structure. If we support structure, we get learnability. We also capture salience. And I think salience is at the root of semantics. Or at least. One of the corners of semantics.\n19:34\nWhen it comes to, Thinking about threats. Saliency. Captures the greater threats. In that sense. It's similar to the. Reward hypothesis. Perhaps.\n20:01\nOkay, so we say this, Suppose we have.\n20:10\nSuppose we have this.\n20:15\nSmart. Generator. Of messages is from. And suppose.\n20:30\nOur Center gauges in planning, he can decompose the state. For example, if it's a dick dictionary of key values.\n20:42\nIn such a way that It is easy to learn. To recover. These things.\n20:58\nThen.\n21:04\nWe will probably. Have a nice representation.\n21:13\nYes, and\n21:22\nThen we can look at learnability.\n21:32\nWe could request teaching sessions in a teaching session. The sender gets to pick the state. The center gets to pick the state.\n21:48\nThe sender gets to pick the state.\n21:55\nPossibly a partial.\n22:03\nAnd then, Sender and receiver. Exchange, an atomic symbol.\n22:16\nWe're learning the language. We might not have a state corresponding to a pronoun. Just sentences. But we still\n2\nSpeaker 2\n22:25\nwant to be able to teach a pronoun.\n1\nSpeaker 1\n22:28\nSo, we want Send a partial.\n22:38\nIf we can. Teach our student partial States.\n22:50\nThat's great. Alternatively, we might not be able to do this in this game. Another approach. Would be to give them the capacity to\n23:03\nConsider these partial States.\n23:08\nThis can happen in the classifier. Or it can happen.\n23:16\nIn a specialized components. What does that mean? Specialist component here. Um, Would be some internally motivated linguist. Perhaps. Something that looks at these sequences.\n23:41\nAnd, Tries to. Assign each bit. Semantic semantics.\n23:57\nAnyhow.\n24:11\nA third. Aspect. Is that we? Perhaps. The generator. In the classifier.\n24:29\nThe generator, and the classifier. To have their own actor critic.\n24:38\nArchitecture.\n24:45\nThe linguist is the critic. The actor. Is the generator.\n24:58\nThat's an interesting architecture.\n25:17\nThis seems to be leading towards. The fourth problem.\n3\nSpeaker 3\n25:25\nThe fourth problem.\n1\nSpeaker 1\n25:28\nIt's composition. Of noise game.\n25:39\nOther games. What we discussed so far has been. In my view. Good position of a loose game. With classifier, perhaps a classification game.\n26:02\nWe could break it down even further. Perhaps.\n26:09\nSerializer. The serializer is.\n26:16\nA simple.\n26:22\nConverter.\n26:28\nThe serial.\n26:46\nGrammar. Is really what's in charge of?\n26:59\nBreaking down and assembling things in a meaning preserving form. So if we talked about homomorphisms, And the hobo milk prisms,\n27:15\nThis is not arbitrary, but You're responding to some.\n27:25\nSemantics some meaning. What's the meaning?\n27:36\nIn the most General way. The meaning comes from, And then DP, being an MDP. That we need to take decisions that we have states that the transitions leads to rewards and that there is a goal. And, We want to be. Maximizing. These Maximizing. This, the This go.\n28:14\nMaximizing. The expected. That's the MDP view in Pac-Man. We shift from\n28:29\nGeneralities of the mdp2, specifics. Damn. And in this sense,\n28:43\nSemantics shift from a specifics on the general to the specific.\n28:51\nI think. I'm almost done here. Um,\n29:03\nWe probably want to be able to, Capture.\n29:13\nA framing game Beyond just. Crucification.\n29:20\nWhich is happening in the MVP. Particularly, in the case of\n29:30\nLifelong agents. There may be multiple\n29:36\nMultiple games. Happening.\n29:42\nIn parallel. We wish we need to interact.\n29:57\nWould be good tomorrow.\n30:02\nLoose game interacting with this other kind of game.\n30:08\nAnd implement it. Using a simple obstruction.\n\n\n---\n\n## Grounding \nThe fourth and Final  question is one of grounding\n\nTHis means aligning semantics coordinated using a lewis game with the semantics of some external system. This could be users who whish to understand and interpret the signals. It might be some language model that uses the lewis game to mediate with some other game. It might be more simple some external framing game, e.g. one where we learn to classify images and use the lewis game to interpret messages that indicate the the right one to pick.\n\nOne aspect of grounding I got from reviewing [@]\n\nGrounding can be thought of as happening in the lewis game. But we might want the semantics to be able to permeate into the framing games neural network at the many levels is uses to learn it's representations. This hypotheticaly might allow the all seeing eye of the CNN or vision transformer to have neurons corresponding to semantic sub-states in layer correspoing to the required receptive fields needed to identify these sub-state represnations.\n\nI realy don't have much to say about grounding.\n\nOk tha't a bit of a stretch. There is this:\n\nWe talk about states and pre-linguistic objects. But where do these come from? Can't say much about it in general. But in RL or MARL we like to think that states come from a  Markov Chain part of out Markov Decision Process. This means something wonderful - we can if we want think of the States coming from an MDP. Learning RL I have been doing so much thinking about MDPs. Tabular states, States with continuous variables. States with action values. States with Features and using them to build Function approximators, temporal abstraction over states. Markov aggregations of states. \n\nSorry to bore you with this long chain of thought -- Lewis games are ofthen considered stateless and can be solved by bandit like algorithms. Sure [Emergence of lanGuage in Games](https://github.com/facebookresearch/EGG), c.f. (@kharitonov:etal:2021) uses [reinforce](), however it still seems overkill. But when we get to complex signals we need the ability to learn aggregation rules, grammars, homomorphisms, homeomorphisms, and the like.\n\n\n## Planning in the Lewis Game {#sec-planning}\n\nHow can the sender use planing in the lewis signaling game to build a signaling system? \n\n- in a simple game there is a single action, the sender sends a message.\n- in more complex games there is an alphabet and the sender needs to construct a message from the alphabet. \n- we can abstract it as sending one letter at a time but that is problematic because we wont get feedback till it is all sent. And we may have to wait a very long time untill recievers learns to decode.\n- we need the sender to map the message to a sequence and send that sequence.\n- he could then invert it (cheating) and get the reward of 1.\n- he could then repeat untill there is a signaling system. \n- then he can play real games untill everything is coordinated with the reciever.\n\n- The only problem is that the sender isn't doing anything to make the system homomorphic. Since rewards are perfect there is no loss or signal to\nindicate that some system is better.\n- We might intorduce noise. If there are errors that might help. But then again we will probably learn to ignore the noise not to plan.\n- We might use some loss fuction to test the full signaling system.\n\nAnother question is how would an agent learn to encode the state in a way\nthat is east to decode, and that is homomorphic?\n\nAnother point of view is to consider this in terms of learning loss.\nCAn we repersent the complex signaling systems in terms of learning complexity? This then might allow us to plan for a system that is least complex to learn.\n\nLearning complexity seems to be recursive. We have the alphabet and its\nsemantics (or lack of). Then we want to represnt the siganling system as \nfunctions we can compose recursively. This is a decompostion into a lexicon and a grammar. The functions should be homomorphisms of the normal subgroups of the pre-linguistic object.\n\ne.g. PL\n\nvariables = {A,B,C}\nfunctions = {AND, OR, NOT}\n\nto learn a linear parametrised representation \nfor a boolean function we need to learn the weights of the functions.\n\n$b + w_1 \\times A + w_2 \\times B = f(A,B)$\n\nNote: functions like XOR and NAND are not linearly separable this means\nwe can't use the following representation which I learned in the RL specialisation on Coursera.\n\n$b + w_1 \\times A + w_2 \\times B = f(A,B)$\n\nbeacuse \n\n$b + w_1 \\times A = XOR(A)$\n\nOne way to learn a suitable approximation is to use a two layer feed forward neural network with a non-linear activation function. However I want to avoid using neural networks and stick to a model that is more like a regression.\n\n\n$b + w_1 \\times A = NOT(A)$\n\nif $A = 0$\n$b + w_1 0 = 1$ so  $b = 1$\nso the function is $NOT(A) = 1 -1 \\times A$\n\nfor Boolean fuction f \n\n$b + w_1 \\times A + w_2 \\times B + w_3 \\times A \\times B = f(A,B)$\n \nFor AND OR and XOR we can learn the weights.\n\n| A | B | AND | OR | XOR |\n|---|---|-----|----|-----|\n| 0 | 0 |  0  |  0 |  0  |\n| 0 | 1 |  0  |  1 |  1  |\n| 1 | 0 |  0  |  1 |  1  |\n| 1 | 1 |  1  |  1 |  0  |\n\n\nb + w_1 \\times A + w_2 \\times B + w_3 \\times A \\times B = f(A,B)\n\nfor AND using the first row:\n\n$b + w_1 \\times 0 + w_2 \\times 0 + w_3 \\times 0 \\times 0 = 0$ so $b = 0$\n\nusing the second row:\n\n$0 + w_1 \\times 0 + w_2 \\times 1 + w_3 \\times 0 \\times 1 = 1$ so $w_2 = 1$\n\nusing the third row:\n\n$0 + w_1 \\times 1 + w_2 \\times 0 + w_3 \\times 1 \\times 0 = 1$ so $w_1 = 1$\n\nusing the fourth row:\n\n$0 + w_1 \\times 1 + w_2 \\times 1 + w_3 \\times 1 \\times 1 = 1$ so $w_3 = -1$\n\nso $AND (A,B) = 1 \\times A + 1 \\times B - 1 \\times A \\times B$\n\nfor OR using the first row:\n\n$b + w_1 \\times 0 + w_2 \\times 0 + w_3 \\times 0 \\times 0 = 0$ so $b = 0$\n\nusing the second row:\n\n$0 + w_1 \\times 0 + w_2 \\times 1 + w_3 \\times 0 \\times 1 = 1$ so $w_2 = 1$\n\nusing the third row:\n\n$0 + w_1 \\times 1 + 1 \\times 0 + w_3 \\times 1 \\times 0 = 1$ so $w_1 = 1$\n\nusing the fourth row:\n\n$0 + 1 \\times 1 + 1 \\times 1 + w_3 \\times 1 \\times 1 = 1$ so $w_3 = -1$\n\nso we we have $OR (A,B) = 1 \\times A + 1 \\times B - 1 \\times A \\times B $\n\n\nNow for XOR using the first row:\n\n$b + w_1 \\times 0 + w_2 \\times 0 + w_3 \\times 0 \\times 0 = 0$ so $b = 0$\n\nusing the second row:\n\n$0 + w_1 \\times 0 + w_2 \\times 1 + w_3 \\times 0 \\times 1 = 1$ so $w_2 = 1$\n\nusing the third row:\n\n$0 + w_1 \\times 1 + 1 \\times 0 + w_3 \\times 1 \\times 0 = 1$ so $w_1 = 1$\n\nusing the fourth row:\n\n$0 + 1 \\times 1 + 1 \\times 1 + w_3 \\times 1 \\times 1 = 0$ so $w_3 = -2$\n\nso we we have $XOR (A,B) = 1 \\times A + 1 \\times B - 2 \\times A \\times B$\n\nSo it appears that by adding a term for the interaction of the variables we can learn the weights for the functions. I used algebra to solve them but we could use a regression or stochastic gradient descent to learn the weights.\n\nHow do we approximate the weights for a more complex function?\n\nThe natural way is to extend our model to include more terms. \n\n$$\nF(A,B,C) = b + w_1 \\times A + w_2 \\times B + w_3 \\times C + w_4 \\times A \\times B + w_5 \\times A \\times C + w_6 \\times B \\times C + w_7 \\times A \\times B \\times C\n$$\n\nWith this model we need to learn eight weights and we can use the eight  rows of data to learn from so should be fine.\n\nBut perhaps we can do better. If we think about it learning these function is more like learning a grammar than a lexicon. And our grammar may be powerful enough to express $F(A,B,C)$ using what we have learned so far. But we now need to learn a rule that lets us express an arbitrary function of $A,B,C$ in terms of functions over pairs of variables. \n\nIf I was stumped^[sure I know about seq2seq, rnns and transformers, and markovian models.] about how to create sequence models from states now there seems to be a first hint.\n\n\n\nSome thoguht about regression:\n\n**Logistic regression** has binary output and a non linearity built into it. So we might be able to learn a simpler model using logistic regression.\n\n\n**Bayesian regression** can\n\n1. Be used for logistic regression by introducing a link function.\n2. Let us learn both weights and their uncertainty. This might be useful for estimating regrets i.e. the cost of learning in our algorithm for learning the weights of the functions. This can help to understand how much we need to learn for different  equilibria.\n3. More interesting is that we may be able to find a prior that speed up learning by using a prior. e.g. one that is the postretior of some   function close to the one we want to approximate. This is a way to implement templates in the learning algorithm.\n4. Let us compose functions hierarchically. We can learn the weights of the functions and then use them as inputs to another regression.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}