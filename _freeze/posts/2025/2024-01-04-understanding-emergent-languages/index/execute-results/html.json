{
  "hash": "ff76c2e07a2884755bc412e60e67d3dd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2024-01-04\ntitle: \"Understanding Emergent Languages\"\nkeywords: \n    compositionality\n    naive compositionality\n    language emergence\n    deep learning\n    neural networks\n    signaling systems \n    emergent languages\n    topographic similarity\n    positional disentanglement\n    bag-of-symbols disentanglement\n    information gap disentanglement    \nbibliography: ./bibliography.bib\n---\n\n\n\n\nOkay, today I've been rethinking Complex signaling systems and thier Lewis Signaling Game. As I make slow progress on this I can say that I am considering a number of research questions that seem trivial for a simple two-state two-signal Lewis signaling game but become important as we delve deeper. \nWithin the complex games we are dealing with sequences of signals based on some alphabet. Here are a few of the research questions that raise their heads as we consider the complexity of the signaling systems:\n\n\n## Research questions {#sec-research-questions}\n\n1. How do semantics arise for sequences?^[this is a natural question for anyone who has stated learning a new language that is from a different language family, and in this case we may be better equipped to answer it]\n    - Does **Simple lexicon^[semantics for atomic symbols, it kind of does in propositional calculus]** suffice ? \n    - Is an aggregation rule sufficient to create semantics? Does a simple lexicon help?\n    - What aggregation types lead to compositionality, entanglement and disentanglement of meaning?\n    - Is having a grammar^[rules imposing structural correctness on a sequence] sufficient to get a useful syntax sufficient?\n    - can we rely on syntax for semantics?\n    - if not how do we get a general framework for semantics?\n    - To what extents do pre-linguistic structure determine the ability of agents \n    - Will agents become great if they have greatness thrust upon them? i.e. If they get a nice signaling system early on will the be able to extend it or will it wither away^[c.f. erosion of the verb in [romance languages](https://en.wikipedia.org/wiki/Romance_verbs)]? ^[This seems more of an algorithmic question about how we reinforce sub-state coding in the algorithms that is can do generalization. c.f [section on planning](@sec-planning)]    \n1. Are there subset of sequences that are:\n    - Easier acquired by one or multiple generations of learners? ;\n    - Better suited for communication between agents ? Perhaps exhibiting greater resilience to errors, reducing risk for certain signal, handle saliency, able to compress sequence to make best use of the comms channel?\n    - More able to generalize to new or unseen states (moran process for the alphabet)\n    - a better match to represent the states.^[can we leverage representation theory to handle symmetries within the states?] \n    - easier to interpret/translate/transfer    \n1. Emergent languages may be subject to selection pressure when the Lewis game is composed with some external framing game. What choices are more conducive for agents to lean quickly and communicate effectively using **robust** learning . I.e. algorithms that lead to more stable language whose lecion, grammar and semantic persist over time?\n    - Do we need to tinker with rewards structure to realign the incentives of the agents?^[can agents that are at odds evolve a signaling system or will deception lead to the collapse of the signaling system to prefectly pooling equilibria?]\n    - Can these be grounded within the external framing game? ^[will we get the full benefits of signaling systems in our framing game to re-shape the over-all equilibria]\n    - resilience to distributional shifts in the states distribution. (e.g. changes in the framing game)\n    - resilience to co-adaptation between agents. (persistence of the lexicon, grammar, and semantics).\n1. More crucially, what interventions can we make as designers can to encourage quick emergence of a signaling system that is conducive to perfect communication, fast learning and generalization?\n1. Can we impose additional structure on the sequences (e.g. a formal grammar that decides a sequence is well formed) Does this imbue the signaling system with additional desiderata?\n1.  Natural Languages develop between many agent and evolve over long time frames. (Hebrew students in primary school read the bible written in Hebrew thousands of years ago using just thier knowledge of modern hebrew.) What about having many senders and receivers speeds things up. I.e. what choices can we make as designers to leverage this.\n1. Can the the sender plan all this machinery in advance and hide it in the the sequences allowing the receiver to learn in the same way as the simple game. Can the receiver infer the machinery from the data? Are there other paths to learning. What if they need to make revisions can they handle those too?\n\nSo I ended op with more questions then I bargained for. These are questions within question. These questions suggest new and intriguing desiderata for (complex) signaling systems as some novel  path for signaling and possibly new settings for learning them.\n\nIn reality I have no idea about almost all of these questions yet here they are. Somewhat similar to how I felt at the end of my BA in mathematics. In reality I've made lots of progress with the lewis game and so I do believe I can make more progress. The reality is that to make progress you need to ask the right questions. **I have been lucky to be able to look at the work of many  others, criticize but later ask different questions than they did and so I was able to crack the issues related to the simple game**. I also developed good intuitions on the simple lewis game and over and over I realize that the complex game is in many aspects just the simple game in most respects.\n\n![ultimate tic tac toe](ultimate-tic-tac-toe.webp){.column-margin}\n\nI was looking over books this week and I came across two or three by math teacher, author and blogger [Ben Orlin](https://mathwithbaddrawings.com/ultimate-tic-tac-toe-original-post/). I felt they are too basic for me to buy but I did get the idea that with the kind of sketches he used I may be able to make more progress with these research questions. I confess am not very good at visualizing thing unless I can put them down on paper so it can be a bit of a struggle to get the ideas out of my head. The tic-tac-toe game seems to be similar to the corrdination task in the lewis game with three states and three signals. If tic-tac-toe the game thoguh is zero sum and in the lewis game the agents are trying to cooperate. But nature (chance) can have the effect that they don't end up with a good solution - which could be viewed as a pattern that is like a three rook problem in chess in red with the blue corresponding to all the misses they made along the way. Of course lewis games can start on a two by two grid but tend to be much bigger.\n\n\n\n::: {.column-margin #fig-deep-reinforcement-learning}\n\n\n\n{{< video https://youtu.be/YOh9iIQ5Qco title='Math with Bad Drawings | Ben Orlin | Talks at Google' >}}\n\n\n\n\n\n\nTalk titled 'Math with Bad Drawings' by Ben Orlin at Google from Jan 29, 2019\n:::\n\nI have been doing some similar work myself trying to explain basic statistics visually. And I used my own urn models to workout the different lewis algorithms. So it's not a big surprise how looking at his books I got the idea that I need to make this more visual and concrete. \n\nWhat I hope is that by posing these many questions I might be able to find some organizing idea to answer them. I seems that one way to move forward is to extend the petting zoo environment and build agents that can participate in experiments that will help me answer these questions. A second challenge is to add metrics that lead to easy evaluation of communication and robustness of the equilibria in the game. A third challenge is to test different algorithms for learning the signaling system and see how they perform in some different framing scenarios. Another challenge seems to be  even harder -- that of interpreting the many emerging languages.\n\n\n## Looking fo the Organizing Ideas\n\nEarlier this week I tried to explain the main issues around compositionality to a friend. This helped to cement my understanding of the problems that researchers in this field are facing. I noticed that in many talks by RL researchers they like to presents their work from first principles. This means most of the RL talks wastes a big chunk of their time on introductory stuff. But it does allow them to talk about advanced concepts with all the more confidence their readers understand what they mean. \n\nLet's start with these concepts that come up quite a bit in the literature and might be less intuitive to the uninitiated. I realize that big words can be intimidating so I will make an effort to explain the main concepts in a simple way and if I do use more big words in these definitions, I'll mark them in italics to indicate that they are not essential to understanding the concept. Some readers might know the terms in italics and this might be helpful to them.\n\n1. **Signaling systems** - these are steady states corresponding to *separating equilibria* in to the [Lewis signaling game](https://en.wikipedia.org/wiki/Lewis_signaling_game) that allow agents to communicate about the world with each other using symbols and without making mistakes. Human languages are rife with ambiguity and do not fit into this category.\n1. Besides signaling systems the Lewis signaling game has many equilibria  that are less then conducive for perfect communication. These correspond to **partial pooling equilibria** and we can rank them by their likelihood of the agents to correctly interpret each other messages. We can generally pinpoint the issues as homonyms in the lexicon. \n1. Is the agents ignore the states or the messages then the game collapses to a **perfect pooling equilibrium**. This is a state where agents can't do better then random guessing. \n1. Even when agents are **deceptive** and messages are cheap talk, it is possible to agent to coordinate by looking at thier actions and infer from that some kind of signal. So even under a zero sum game it may be possible that they might end up with an system of communication that is better then random guessing. c.f. [@jaques2019socialinfluenceintrinsicmotivation] However this is an aside on deception and will not be pursued further. \n1. Simple signaling systems - these are signaling systems that comprise of just a lexicon of symbols and their corresponding states.\n1. Complex signaling systems - these are signaling systems that comprise of a lexicon of sequences of symbols drawn from an alphabet and each sequence represents some corresponding states. \n\nIt not simple to explain how these differ differences from simple signaling systems except to say that while simple signaling systems are just a lexicon complex ones may be able to capture some or possibly all of the nonces of a natural language.\n\nOne way is that perhaps some the symbols comprising the alphabet might be included in the lexicon as length one sequences, and when they are used in a sequence their semantics may be assumed to be the same as when they are used alone, unless there is an explicit entry in the lexicon that says otherwise. This would mean that the lexicon now has potential to express\nWhat is different from simple signaling systems is that should the states have a sub-states with some structure. In such a case if agents are able to coordinate a signaling system that respects this structure they may be able to learn to communicate much faster. Also if the system gain evloves and new states are intorduced with this structure the agents may be able to learn to communicate about these states much faster then if the structure is ignored.\n\nIt worthwhile to recall that these three definitions\n\nHowever in this case the lexicon might have additional structure that matched the structure of the states. Idealy this structure is codified using rules so that using these rules the semantics of the atoms can be combined to give the semantics of the whole.\n\n5. **Sub states** - Pre-linguistic objects are sometimes called states and we would like to look into these and discern if they have structure like key-values pairs, some kind of hierarchy, some symmetries, a temporal structure, etc. If such a structure exists and follows some steady distribution we the lewis signaling game may have signaling systems that are conducive to perfect communication, faster learning and generalization.\nfor we are primarily interested in complex signaling systems.\n\n6. **Framing Game** in movies we often have a framing device - some kind of story that is used to introduce the main story as a flashback. In MA-RL the lewis signaling game may be a means to achieve coordination between agents in a more complex game. This bigger game is called the framing game. Though in in the life long learning setting there may be many games taking place simultaneously.\n\n6. **Entanglement** - when a sequence of symbols are combined in a way that their meaning is different from the sum of their meaning we say that they are entangled. For example idioms like \"kick the bucket\" or \"keep the wolf from the door\" are non-compositional and highly entangled. We cannot assign a specific word (atomic symbol) that captures part of thier meaning. This is a non-compositional way of encoding information. Although entanglement is explained using idioms, it can happen at different levels and may be an artefact of some selection pressure in the environment to compress information about certain states. If some bound colocation is used very frequently. i.e. where both words are used together exclusively then the speakers may have a benefit in fitness or communication for encoding them as a single symbol. Another form of weak entanglement might be exemplified by the compound verbs in English. In the phrases Look out for, Look up, etc the meaning of the verb look changes in a way that is not a simple sum of the usual meaning of preposition that follows it. This is  a cliché. But the bottom line is that entanglement representation require the agents to learn the meaning of the entangled symbols together. \n7. **Disentanglement** - when the meaning symbols can be interpreted regardless of thier neighbors. There are two problems with disentanglement. The frist is that it is rather vague definition and secondly it is not a property of languages. Sure many words in a language can be assigned a definition that is independent of the other words in the language. But the lexicon if full of colocations, idioms, compound verbs etc. Verbs and Nouns have a stem and affixes that encode multiple units of meaning. Describing relations and using adjectives  and adjectives is done using phrases, i.e. they are spread across multiple words. But the problem with this term is that semantics of one usint isn't a signle bit of information (i.e. discreate) Some symbols can contain more information, then other. This is other grammatical symbols perhaps less but seem to operate on others.\n The problem with this term is that semantics of one unit isn't a signls bit of information (i.e. discreate) Some symbols can contain more information, then other. This is other grammatical symbols perhaps less but seem to operate on others.\n8. **Compositional**- when the meaning of a symbol can be decomposed into the meaning of its parts. This is a compositional way of encoding information. One system that is compositional is first order logic.\n9. **Generality** - We take here the meaning from machine learning, where a model is said to be general if it can perform well on unseen data. What researchers tend to look at is if agents have have coordinated a signaling system that is a good representation of some subset of pre-linguitic objects which we call the training set, how well would that system generalize to unseen objects? \n10. **Catagories** - In a simple lewis game we might have multiple states assigned to a single symbol. We call this a homonym and consider it a defect. If though in the framing game we have a survival benefit to acting early, if such a homonym encodes a number of states that require the same action then we might have a benefit to splitting the states into two parts the category and the subcategory. Also we might have additional benefits here by using a shorter signal to encode the category and a longer signal to encode the sub-category. Categories or in more general form Hierarchies are a ubiquitous feature of natural languages. Another facet of this idea is the use of prefixes and suffixes in natural languages. In both cases we have a benefit from using a shorter signal to encode some category and a longer signal to encode the subcategory. But is the prefix perhaps we need prefer to know the category first and in the suffix we might prefer to know the subcategory first.\n11. Aggregation rule - an aggregation rule is a rule that takes an input with a number of symbols and reconstructs from them a state. In one sense this is what we think of as a grammar, but I'd like to keep them seperate and think about the agrregation rule as something more like a serelization protocol. It takes a number of inputs -- possible from differnt senders and likely each with some meaning or prehaps just a cue - a partial meaning that can't be interpreted without the other cues. \nTwo examples of aggregation rules are \n    1. the disjunction leading to the bag-of-symbols \n    2. the idea of serilization of incoming audio signals by the reciever by appending them order in which they are recieved.\n    3. and serilization converting OOP into a sequence - perhaps for images.\nWe can think about a recursive aggregation rule but I'd like to call these a grammar and keep them seperate. Perhaps later I'll be able to explain why I think this is a good idea.\nNote that complex signaling systems do not require an aggregation rule or a grammar, but they may benefit from them. Without an aggregation rule we are dealing with a signaling system that is fully entangled and that is little different than a simple signaling system.\n\ntip: currently my mental models for Aggregation rules are {disjunction, serialization, prefix coding}.\n\n12. **Formal Grammar** - describes which strings drawn from an alphabet are valid according to the language's syntax. Note that the formal grammar is in charge of the syntax and not the semantics. Thus a grammar can be considered as a **language generator**. If the speaker uses such a language generator then the resulting language will have a syntax. And further more is the grammar is unambiguous then the language will be a signaling system. And Aside is that for propositional logic a formal grammar is enough to define the semantics as they arise directly from the syntax. For FOL we need a model theory to define the semantics. However if we construct a Lexicon with semantics from the model of the FOL we end up with a grammar whose semantics are defined from the syntax. \nMy mental model for a formal grammar in the lewis game is propositional logic on its sub-states.\n\nThe main takeaways is that if we generate the lexicon without using an unambiguous formal grammar we are putting the syntax of the language at risk. And if we don't have a a lexicon for the alphabet we may be putting the semantics of the language at risk. \n\n\nAn intersting issue is that we can have differently likelihoods of having different constructs depending on what we include in out lewis game extention....\n\nWe might want to find a metric that measures the use of categories in a signaling system.\n\nAnother couple of ideas.\n\nWe can have examples of complex signaling systems that are on a spectrum from being fully  compositional to being fully entangled. \n\nAnother thing we can unse is a complex compositional language for pre-linguistic objects that don't have a simple disentangled structure and don't fit with the positivistic view of an objective orthogonal and disentangled dimensions that can be measured with full certainty. Ie we can use a easy to learn compositional complex signaling system to encode the pre-linguistic objects that are not easy to interpret like arrays of raw pixels. But also we might be less likely to learn such a language if this is our only input. We would have to come up with it by change.\n\nOne way this might happen is that we could learn a grammar (classifier) that tends to \n\n## Transcripts:\n\n### Baroni Talk\n\nI've had this idea about Complex signaling systems, unfortunately. I'm wasted a lot of time, installing a transcription app and lost focus. Let see ---  I'm trying to refocus and get back into it.... Yes, I think I got it. \n\nSo yesterday after talking with my friend Eyal which got me rethinking about complex signaling systems and trying to realize how despite the many perhaps misguided attempts a growing number of researchers have been able to come up with agents that use complex and even compositional signaling systems. How these agentic systems seem to be a solution to a problem that is hard to frame.  Uh, from deepmind. By, what's your name? I had the second good idea. Uh, which I wanted to write up.\n01:09\nFor her. Stuff.\n01:15\nNo, let's let's uh, so so the two happy things. So the the chat with, uh, Got me. To be able to explain entanglement disentanglement. Compositional, non-compositional and generality. In a nice concise way. And that deserves. Good. Write up inside of the baroni post. And maybe inside of the guide to the perplexed. About compositionality. Uh, in fact, I don't think Baroni deserves this because He's just all about making things to make him confusing. Uh, then there is this Gondola girl. Whatever name is. And, Thinking about her work. Got me. To make a bit of progress. I think.\n02:26\nWhat we could do there? Is say this.\n02:34\nIf? Um,\n\n\n----\n\n## The Question of Grammar\n\n\nOkay, so this is the About the deep line talk and paper. Um, and what I noticed there. Is that? The talking about emerging languages. And I'm talking about, Engine re-engineering in Duluth game. The change at equilibria. So, is stew. Support complex, signaling systems. And this is quite tricky. Particularly. When we have, Both simple and complex.\n00:45\nSignals and we have a third kind of thing. Which is the so-called aggregation. And there's yet another thing. Which is the grammar. Is grammar and aggregation the same thing. I don't think so. I think that. The related. But only coincidentally, The aggregation. As scrims describes, it can be conjunctive, which is weak. We go, it leads the weaker representation and the sequential one. Which leads to Richer. Representation. Um,\n01:38\nBut that said, It doesn't necessarily lead to a grammar. Although, It's definitely sufficient to act as a kind of grammar.\n01:54\nUm,\n01:59\nWhat what is an example of a concatitative grammar? Uh, Hungarian. Is a glue. Native language. You simply add. Morphological units to form. Ah, very powerful.\n02:25\nUm, representation of a word, which is able to Essentially exist. Or rather resist changes in the order. Or for freeze. And, That doesn't mean that they don't care about that. There isn't extra meaning, due to the order like the word, which is at the focus position. But basically Once we have the markings, in the words with all the affixes that you have in Hungarian, It's quite possible to shuffle the words. And not disrupt. The meaning of the That. Morphology is encoded. So, that's kind of\n03:24\nThe power of the sequence I suppose. Though. It also, we also have in the sequence this special morphological markers.\n03:39\nBut we could consider that these morphological markers are just basically. Certain words.\n03:51\nWhich we put in certain positions. But, So, we're still talking about this idea of\n04:04\nCompetitive grammar. So another thing in the concoctative grammar is Uh, it could be. Ordered or disordered. If things are marked or unmarked, we can have it. Uh, resistant reordering. So the meaning is preserved and if it isn't like in the example from The unfolding of languages from Old Turkish or Babylonian. You might have a very long sequence of Slots. And, We should be able to. Composite into these slots. A whole wealth of words, a whole wealth of meaning. And, I think we have something similar in German. We basically assemble a whole sentence. By gluing together, bits and pieces.\n05:06\nOf.\n05:12\nInto one long word. And this actually makes sense. If you think about,\n05:24\nAbout all societies in which The nobody wrote down the language if you don't write down the language. Yes. It's all overall. And, We could think of a word is something. That's just a sequence. Okay. So, all of this Tries to highlight that. Um, We can have grammars just by concatenating. More films or like themes. And,\n06:06\nThat's what we call compositionality really, or at least. That's a very basic form of Compositionality.\n06:19\nSo, what is grammar if we have just a compositional\n06:26\nRules. I would say that grammar. Um,\n06:35\nIs different ways we use. Create isolate bits of meaning.\n06:49\nYeah, it's Seems to be a hard thing to Define properly, but\n06:58\nThe kind of ideas I'm thinking about is that we might have this recursive. Recursive set of rules. Because the grammar, Yeah, so in former languages, I think that's the direction that's the direction I'm bent to in. Informal languages, grammar defines. Using recursion, usually.\n07:30\nSet of sequences, the sequences are defined by the grammar. You can call them sequences, we control them sets. But I think they usually order set, so we can call them sequences and these sequences I lost to take. Find that alphabet and create. Uh, infinite number of\n08:00\nMessages. How do we do that? Quite simply put. We have this operation. With a simple set where we can take the power of the set, the power set. Which is.\n08:22\nAll the pairs, I think. And all the triplets and so on and so on. Basically, all the subsets rather. Yeah, the power set is the central subsets. If we look at just\n08:46\nBut the power. Yeah, if we look at all the subsets we can create, Bigger more complex constructs.\n08:56\nUm, If we look at, Sequences that we can form. We can also have a grammar for that. And, Some things human grammars have. This thing, this notion of agreement. I see agreement. This Serving two purposes. One is to identify. To maintain a correlation between lexical units. That have a relation. To show us assumings that\n09:43\nThat the Mexico. Units or even two phrases. Are related using this? Correlation of gender number and so on. Whatever the agreement is keeping, And that way. We can. Poke into it. Into the slots between them. Uh, additional structures. And, The agreement. Allows us to maintain. The relationship. This, of course, breaks down. If we poke in, And they'll literally arbitrarily Large number of, Uh, phrases. He put in a very big tree. This isn't effective. Another way that I'm looking at it. Is that in terms of the pragmatics? If we look at the pragmatic side of communication,\n10:57\nThis is just the redundancy, which allows To do our correction. I suppose. In the big picture though. These two things. These two phenomena agreement. And error, Corrections are\n11:22\nDual aspects of the same thing. You put. These markers. They allow. They also make the language. More resistant.\n11:39\nErrors. They help us. Uh, disambiguate certain messages.\n11:50\nThrough this types of agreement. And,\n11:57\nUh, we usually don't need these\n12:03\nIf there is, let's say less chance of an error or less chance of confusing. Some pronoun referencing, some other battle piece. Uh, multiple bits and pieces of the sentence. We might not need to mark this thing with.\n12:27\nWith an agreement. To understand what's going on and we might end up with a determiner. Which is unmarked. All it says is So what's the difference between a and there or that? Definite and indefinite. All right, this or that.\n12:54\nSo, The unmarked distance in any way that they don't have. Um, what don't they have? They don't reflect number, they don't reflect. Gender and so on.\n13:14\nThe other hand we do have one, we have mine. These are.\n13:24\nMarkers. These are, I don't know, pronouns or particles that.\n13:31\nMark possession. The Mach number, the micro number the marked for person. Right.\n13:43\nIn Hebrew, they marked with gender. So they get marked with a lot of things and that kind of makes it significantly easier.\n13:56\nMakes it much easier. To discuss. There's ambiguous thing. Called position.\n14:08\nAnd in Hungarian we can, The position and the possessor. Using additional information. Which is, uh, stored in the suffix. Along with the singular plural. So, they mark that. But they don't Mark gender. Which is. How do you say? It's useful as it's on a, on a pig or something. Right. So Enough said about that. Um, So, let's get back. So now we've discussed Grammar. Kind of try to Define grammar. And, Three rolls the formal role. Being. That it is.\n15:13\nA. That allows us to. To make use of a finite set of symbols. Into an infinite set of messages. But not necessarily saying anything about Their semantics. Although if we look at, I don't know. The grandma. First saw the logic. It does allow us to Define by induction.\n15:48\nThe Logical.\n15:53\nThe Logical. The Logical meanings of. I'll betually long phrases. Okay.\n16:07\nUm, So, it not only allows us to generate sequences. But to propagate the atomic meanings into more complicated meanings.\n16:26\nAggregate meetings. This.\n16:34\nUm, Is more complicated and simple. Aggregation in simple. Aggregation we're saying something after something after something just have this blasting. Logic, we have. And the no with brackets.\n17:02\nAnd we can build with this very specific functions. Of truth functions. And Truth functions correspond to a big chunk of semantics. No doubt about that.\n17:24\nSo, if you want to look at grammar, Starting with a former language. It's not the worst thing. But if we're thinking about, This thing we call uh, what do we call it? Um,\n17:44\nEmergent language. We may be interested in having some additional useful properties, and this is what I collect and the ziturata This. Space. Definitely consider. The issue of learnability. Vulnerability.\n18:14\nBecomes Paramount. When we have Collective generational, collectives of Agents,\n18:27\nBasically. When we? To transfer. Disability. And The ability to communicate.\n18:43\nUh, between agents and Even if we don't have Generations, even if we have Continuing tasks. Which is. Similar to what's happening in the real world. Uh, we still The. You want? We still would like, to be able to Handle.\n19:16\nDistributional shifts. Within the language. So If we. If you and I are talking for a long time. We're gonna have. Who had adaptation of our language will have accent, and then would have a dialect. And then pretty soon. We're gonna ask someone about. This and that and other people. Won't be able to know what we're talking about. We'll have our own jokes. We'll have our own idioms. And,\n19:58\nMade upwards. So we'll have Words that other people know, but that now have completely new meanings. So, that's this. Shift. I'm referencing. Distributional shift. And we'd like, to be able to Uh, communicate with other agents. The distributional shift. Is also annoying for researcher. In the sense that Let's say, you understood what the language means now. But What happens after? The our allegiance.\n20:42\nHave had another. Half a million turns playing. Let's say some kind of diary game or something. Now, the language might mean. Other things. The words and phrases might be the same but the semantics have changed, that's a big headache or the grammar might have changed.\n21:08\nOne of the algorithms I came about was to Full languages was Try to swap out. Pairs of signals.\n21:24\nAnd the meanings based on.\n21:30\nDistribution of. And this. Get us to a point where Um, We can have. Shared bits and pieces. Shared button pieces. These shirts bits and pieces.\n21:56\nThen be able to spread. And then we would have a compositionality. Basically. Should bits and pieces. Become prefixes and prefixes become categories. Vice versa. Let's say, categories. Semantic categories and maybe later they become.\n22:29\nGrammatical categories part of speech. And so on and so forth. And That way, we might be able to evolve Um, Rules that simplify learning. We might learn construct. Let the language become. Uh, fixed. So, we don't have A big issue with. With that language needs to evolve, but we do have an issue with this core adaptation.\n23:10\nCertain bits and pieces of the language not changed so much. We'd like, To find an optimal grammar. Then stick with it. We'd like to Have close categories that not change those. We'd like to use. Affixes efficiently. And we might want to and prefixes and so on. And we like to stick with those. Otherwise.\n23:45\nOur verbs. Might be. More prone to. Withering. And to make the language learnable. It's gonna be much simpler if the verb and the noun, Morphology. Is regular. And more or less constant? And then, We can block in a stem or a root. You can get. A new verb or a new noun, or maybe even both. And all we have to do is learn the meaning of the stem more or less. And we understand all the rest. And that's the power compositionality. So,\n24:39\nOne old idea, I had. About trying to induct. Buffalologies. In unsupervised or semi-supervised way. Was to try and find some kind of\n25:04\nThat's a good fit.\n25:09\nHandling morphologies for. Handling tables. Did you follow the same structures?\n25:20\nThis is.\n25:25\nUh, something I don't want to explore here. But this kind of a loss. Might be useful. To encourage.\n25:43\nHola. The agents to coordinate on the language. With a given structure and yeah. A further refinement of this idea. Which I've had more recently. Is to consider that. This morphology. To high degree mirrors a group action. And so we can Define This loss in terms of, Homomorphic loss. I lost that preserves.\n26:28\nStructure.\n26:34\nBut that's pretty much.\n26:38\nJust handling structure. You could probably do even better. If we. Also, preserve distances. Distances between. Semantic unit. I suppose. Such.\n27:02\nStructure would be. Also a home homeomorphism topologically preserver. Something. To think about.\n27:18\nAnd yet. These are approximate.\n27:25\nWhy is that? Because, Because, Um, Because of the irregularities irregularities. Are usually going to be deviations from this group structure. From the topology. From the simple distributional forms.\n27:52\nFor compositionality. We generally assume. Independence. But three languages.\n28:04\nAre not uniformly, distributed.\n28:08\nBiggest structures are not. Uh, often\n28:19\nShow conditional. Probabilities. So, They don't follow.\n28:33\nIndependent structure.\n28:38\nAnd these are. Aspect of the language, we usually want to preserve In other words, It's often. To our benefit. To have.\n28:56\nThese other things in the language.\n29:07\nWhy is that? Because these oddities often encode. Not always, but often. Then code.\n29:26\nFrequent the most frequent. Elements. Irregular verbs. To be.\n29:37\nTo do. In French, a regular.\n29:46\nIn English. I'm not sure, I think they are also. Is that good makes learning harder? But it's only a little chunk and it probably makes air correction better.\n30:06\nAlthough, I'm sure we could do better than that. Um, So there are all sorts of plays going on here. And, Uh, one thing you can be sure about Is that? To come up with a good grammar. Isn't. Necessarily very difficult.\n30:38\nBut,\n30:43\nIt appears to be. Product of planning. Once you have the language, Or laid out in other words.\n30:56\nExpand the rules of the grammar. You'll have vast constructs. And to change. And we see the changes in these contracts are often localized. Of the localized. Which means? It's very hard to change the grammar. Just change the grammar, you will have to change.\n31:25\nAll the application of all these constructs. For that particular, aspect of the rule. There might be subtle changes, that not make big, big, big changes. Usually,\n31:44\nNatural language grammars. Nothing. Like The world behaved formal grammars. Which we see, and I think, This is primarily because of what I just described. That it takes a Big effort to make these changes because you have to get these changes to happen in the heads.\n\nThe most.\n\nOh, at least localized area. Where people are talking? Um, Yeah. Um, but they do happen. So they happen, they happen because The forces. Evolutionary forces. Acting over. Certain time frames. That. We'll do this, because The original system might be inefficient. Of equilibrium in terms of signaling systems. And each interaction entails a tiny amount of of friction. And,\nIf it is likely, the changes will happen in such a way.\n33:32\nFollow some kind of path of lift resistance. A domino effect Chain Reaction. We see the vowel, shifts. How can that possibly happen?\n33:51\nOne wond. But,\n2\nSpeaker 2\n33:59\nI don't know. But\n1\nSpeaker 1\n34:07\nThey did happen. They have happened. And, One would think this is. Something that's happening, step by step.\n34:20\nCertainly. Words. With a dominant meaning resist changes.\n34:34\nTo conform with the grammar. Until?\n34:42\nPerhaps that word. Is replaced. With another word of the use of that term. Falls into. The shoes. Allowing us to.\n35:00\nUh, assign a new meaning or Reduce the variation. We see in Hebrew.\n35:10\nSomething even more drastic has happened. Over time certain of the phonemes. Have become.\n35:25\nThe.\n35:30\nWhere we have three different sounds in Arabic, we have only one song in Hebrew. And, This is cause number of verbs to become conflated.\n35:44\nYou can only imagine the chaos. This is created in terms of meanings.\n35:52\nIf only, we could take a step backwards, Teach everybody, how to Pronounce. These sounds in three distinct ways, everybody. And then, teach everybody. Which verb? Fits with which sound.\n36:15\nWe could. Make the language, much more.\n36:23\nMuch less ambiguous. But at the same time, It will probably be. Harder to learn. So, If there's one conclusion from all, this rather long story, Is that all these things? Represent trade-offs. And,\n36:58\nEvery languages, the equilibria.\n37:07\nCorresponding to these different things. Equilibria. That is stable. One change so much, but You can see from the research by schemes and others. That.\n37:33\nThe. Most of the equilibrio at least in the simple models. Are unstable or semi-stable. And serve the language. Will shift over time.\n37:53\nAll that you need is the right kind of pressure. I suppose.\n38:03\nEnough said. So now we can get back to So,\n38:15\nI've covered a lot about. Aspects of grammars. Now, let's get to Signal existence, let's get back to signaling systems. It looks like. Does not look like the simplest games. There's a bunch of. Through specific equilibriums possible. Very specific. They're essentially infinite. Infinite number, and yet very specific. What do I mean? The three types of purequalibia. Perfect, pooling completely useless. Partial pulling. Imperfect.\n39:05\nAnd, And,\n39:15\nSeparate the equilibria, the so-called signaling systems. That's to, at least. The simple. Games were simple States.\n39:33\nSo-Called.\n39:40\nUh, simple. The basic.\n39:50\nIf you. Something more sophisticated.\n39:58\nWe need to look for some other game. We usually call this. Modification. What we should ask is. Does our modification allow us?\n40:16\nTo have. Um,\n2\nSpeaker 2\n40:22\nDoes it allow us to have new types of equilibrium?\n40:29\nI think there's\n41:18\nAnd just upload those.\n41:36\nSo, the the big, the big question again.\n41:55\nSince we do see compositionals, Languages.\n42:05\nCan we\n3\nSpeaker 3\n42:06\nsay?\n2\nSpeaker 2\n42:11\nUsing complex symbols. And not just the coincidence.\n42:20\nNo, that's not the paper. The big question is\n42:26\nIf we make some change to the significance,\n42:41\nOf the game.\n42:46\nTo have. New kinds\n1\nSpeaker 1\n42:48\nof Bolivia. Furthermore.\n42:55\nThis\n2\nSpeaker 2\n42:57\nis equilibrium. Follow approximate, let's say. Use the approximate.\n43:25\nWe really wish the property. That are close together.\n43:40\nConform to some grammar. To the approximate, some kind of drama. Is there sometimes a genre. Death is. More likely to emerge. And so on.\n44:00\nWhy do we ask this? Because we don't know. Nice and simple way to extend the game to get this. I don't think.\n1\nSpeaker 1\n44:23\nI\n2\nSpeaker 2\n44:24\nthink you would apply this to the agents. They create is such an algorithm These agents. Lexus. No.\n3\nSpeaker 3\n44:38\nHave order of magazine.\n44:44\nAnd it comes. What the future is to transfer.\n2\nSpeaker 2\n44:56\nOh, it would alone. For these abilities.\n45:14\nPolicies.\n45:19\nThe values and action values in terms of, A language where design in terms of getting used to be really interested. Objects. And there's a bit to flip.\n45:42\nThey will be able to transfer. To have a translation. Between languages.\n46:02\nFrom the line is just one problem.\n46:15\nAnother I know injection. We've seen in enforcement learning. That when you run the same algorithm, Some seats. So, Just by chance. We might have. Low performance. And another right here.\n46:45\nLord, that pitched.\n46:54\nWell, not much except that. When we talk about language, we talk about state that unstable equilibrium and\n47:11\nWhat time vision is?\n47:16\nThis could do. We understand. The Scorplex signaling systems. You should also be able to figure out How to create?\n47:35\nVery many designers including one with three stabilities.\n47:42\nIf we are able to use this stability of our Policy value functions equivalent function.\n47:57\nBecause we now have\n48:02\nMore abstract.\n48:07\nWe're probably going to be more powerful.\n48:14\nThen.\n48:27\nSo, given all this\n48:33\nWhat did you change? With these so-called.\n48:48\nResearch.\n48:56\nComposable. So complex. To work this country.\n49:31\nBecause,\n49:37\nUh, because We can easily show that. Languages with these properties.\n49:51\nThe ability to capture very much and for Concepts using this small relatively small.\n50:03\nThey're very easy to learn. They're very easy to teach. So if you have all these benefits, maybe some of them deserve distance and structure.\n50:19\nAnd maybe they are also.\n50:29\nEasier to To. Translate.\n50:49\nSo, we would facilitate Uh, the best faciliters of transfer learning. Maybe they. Group instructions for many things that might not be in the learnable in one second. So you might be working with Space Invaders, you might not have. And language.\n51:23\nYou get that. So if you're working with soccer band, you might have other kinds of Concepts\n51:34\nUseful, which\n51:40\nAnd,\n51:45\nI, We would like to find some kind of core.\n51:57\nA human language, which has\n52:12\nDidn't describe States. Well enough.\n52:22\nI\n1\nSpeaker 1\n52:24\nhave also properties, which bar\n52:34\nUh, gravity stability.\n52:58\nAnd yes, all of this might follow one. Three, two, maybe just one uh, geometrical rule.\n53:33\nYeah. Anyway, anyhow\n53:49\nSo that's us. What? What does this? What does this?\nTranscribed by Pixel\n\n---\n\n## The second question, seems to be in what way is The referential game different from the signaling game?\n\n\nThe second question, seems to be \"in what way is The referential game different from the signaling game?\"\n\nI postulate that this is due two changes and more significantly, the way these two things come together. The alternative hypothesis is that just one suffices for the emergence of a signaling system.\n\n1. There is the classifier.\n2. Is the Message generator - as described in [@lazaridou2018emergence] can generate any sequence from a given alphabet of symbols.\n\nIf these symbols are already imbued with semantics from a pre-ante lewis game, then what we now have is a complex signaling system. However, if the symbols are not imbued with semantics then we can still use the lewis game to imbue them with semantics. It uncertain though if these semantics will be different from the semantics due to a simple lewis game.\n\nInitially I liked the first hypothesis as it mirrored my thinking of using an extensive game with two steps as my modified lewis game. Soon though I had an small epiphany, and decided to test the second hypothesis, this being more in line with the reductionist approach I had espoused all along. Particularly as this is the smaller and less powerful extension then the first.\n\nNow we can think of the lewis signaling game as using a generator with some set of symbols and sequences of length 1. \n\n::: {.callout-tip}\n## To shuffle or not to shuffle? ::cards:: {.unnumbered}\n\nMore so we might want to shuffle the sequences so they come out in arbitrary orders allowing all the lewis game to unfold in all possible ways. I.e. all forms of symmetry breaking. Alternatively we might want to enumerate the different equilibria and thus only use one canonical order.\n:::\n\nHere is some code that generates all the sequences $\\mathcall{L}$ of length $A$\n\n::: {#eeab8c28 .cell execution_count=2}\n``` {.python .cell-code}\nfrom itertools import product\n\ndef generate_sequences_generator(alphabet, n):\n    \"\"\"\n    A generator to yield all possible sequences of length n from the given alphabet.\n\n    Args:\n        alphabet (list): List of symbols representing the alphabet.\n        n (int): Length of the sequences to generate.\n\n    Yields:\n        str: A sequence of length n from the alphabet.\n    \"\"\"\n    for seq in product(alphabet, repeat=n):\n        yield ''.join(seq)\n\n# Example usage snippet\n\n# alphabet = ['A', 'B', 'C']  # Example alphabet\n# n = 3  # Sequence length\n\n# print(\"Generated sequences:\")\n# for sequence in generate_sequences_generator(alphabet, n):\n#     print(sequence)\n```\n:::\n\n\n::: {#f1f85a4b .cell execution_count=3}\n``` {.python .cell-code}\nlang= []\nprint(\"Generated sequences:\")\nfor sequence in generate_sequences_generator(['A','B','C'], 3):\n    lang.append(sequence)\n\n# print lang 3 sequences per line\nprint(\"\\n\".join([\",\".join(lang[i:i+9]) for i in range(0, len(lang), 9)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerated sequences:\nAAA,AAB,AAC,ABA,ABB,ABC,ACA,ACB,ACC\nBAA,BAB,BAC,BBA,BBB,BBC,BCA,BCB,BCC\nCAA,CAB,CAC,CBA,CBB,CBC,CCA,CCB,CCC\n```\n:::\n:::\n\n\nInterpreting the sequences.\n\nIn this case we might interpret each sequence as a message indicating a sub-state A, B, or C took place. If this is their meaning we might need to remove duplicates and order them to decode it. Also we have many alternative messages for equivalent states. This would slow down the learning process.\n\nAnother way to go is to treat A,B,C as 0,1,2 and we can interpret them as a ternary number. Now each sequence is unique and can be interpreted as a corresponding to some state. If we had again three binary sub-states we could use this system with 3^3 symbols to encode the 2^3 states as follows:\n\nA in the first position indicates true for the first sub-state, B indicates False, and the same for the second and third sub-states. We don't need states with C. This is a more efficient encoding and will speed up the learning process. \n\nLet say we used the restricted system to start with and 0,1 to encode False and True\n\n::: {#01211688 .cell execution_count=4}\n``` {.python .cell-code}\nlang= []\nprint(\"Generated sequences:\")\nfor sequence in generate_sequences_generator(['0','1'], 3):\n    lang.append(sequence)\n\n# print lang 3 sequences per line\nprint(\"\\n\".join([\",\".join(lang[i:i+4]) for i in range(0, len(lang), 4)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerated sequences:\n000,001,010,011\n100,101,110,111\n```\n:::\n:::\n\n\nNote that that the above system might be learned if the Lewis signaling game took place and the sequences were generated in an order corresponding to these states. Otherwise we would get an equivalent system to the first one\n\n$$\n\\begin{align*}\nAAA \\to ['A':T 'B':F,'C':F] \\\\\nAAB \\to ['A':T 'B':F,'C':T] \\\\\nABC \\to ['A':T 'B':F,'C':T] \\\\\n\\vdots\\\\\nCCC \\to ['A':F 'B':F,'C':F]\n\\end{align*}\n$$\n\nthe following generate all the sequences of length 3 comprised of 0,1 and a space symbol\n\n::: {#eb6e16ad .cell execution_count=5}\n``` {.python .cell-code}\nprint(\"Generated sequences:\")\nfor sequence in generate_sequences_generator(['0','1',' '], 3):\n    print(sequence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGenerated sequences:\n000\n001\n00 \n010\n011\n01 \n0 0\n0 1\n0  \n100\n101\n10 \n110\n111\n11 \n1 0\n1 1\n1  \n 00\n 01\n 0 \n 10\n 11\n 1 \n  0\n  1\n   \n```\n:::\n:::\n\n\nThe following generates all the logical propositions of length 4 using the with three clauses and symbols for negation, conjunction, disjunction for it alphabet\n\n::: {#c38a1f86 .cell execution_count=6}\n``` {.python .cell-code}\nfor sequence in generate_sequences_generator(['!','&','|','a','b','c'], 4):\n    print(sequence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n!!!!\n!!!&\n!!!|\n!!!a\n!!!b\n!!!c\n!!&!\n!!&&\n!!&|\n!!&a\n!!&b\n!!&c\n!!|!\n!!|&\n!!||\n!!|a\n!!|b\n!!|c\n!!a!\n!!a&\n!!a|\n!!aa\n!!ab\n!!ac\n!!b!\n!!b&\n!!b|\n!!ba\n!!bb\n!!bc\n!!c!\n!!c&\n!!c|\n!!ca\n!!cb\n!!cc\n!&!!\n!&!&\n!&!|\n!&!a\n!&!b\n!&!c\n!&&!\n!&&&\n!&&|\n!&&a\n!&&b\n!&&c\n!&|!\n!&|&\n!&||\n!&|a\n!&|b\n!&|c\n!&a!\n!&a&\n!&a|\n!&aa\n!&ab\n!&ac\n!&b!\n!&b&\n!&b|\n!&ba\n!&bb\n!&bc\n!&c!\n!&c&\n!&c|\n!&ca\n!&cb\n!&cc\n!|!!\n!|!&\n!|!|\n!|!a\n!|!b\n!|!c\n!|&!\n!|&&\n!|&|\n!|&a\n!|&b\n!|&c\n!||!\n!||&\n!|||\n!||a\n!||b\n!||c\n!|a!\n!|a&\n!|a|\n!|aa\n!|ab\n!|ac\n!|b!\n!|b&\n!|b|\n!|ba\n!|bb\n!|bc\n!|c!\n!|c&\n!|c|\n!|ca\n!|cb\n!|cc\n!a!!\n!a!&\n!a!|\n!a!a\n!a!b\n!a!c\n!a&!\n!a&&\n!a&|\n!a&a\n!a&b\n!a&c\n!a|!\n!a|&\n!a||\n!a|a\n!a|b\n!a|c\n!aa!\n!aa&\n!aa|\n!aaa\n!aab\n!aac\n!ab!\n!ab&\n!ab|\n!aba\n!abb\n!abc\n!ac!\n!ac&\n!ac|\n!aca\n!acb\n!acc\n!b!!\n!b!&\n!b!|\n!b!a\n!b!b\n!b!c\n!b&!\n!b&&\n!b&|\n!b&a\n!b&b\n!b&c\n!b|!\n!b|&\n!b||\n!b|a\n!b|b\n!b|c\n!ba!\n!ba&\n!ba|\n!baa\n!bab\n!bac\n!bb!\n!bb&\n!bb|\n!bba\n!bbb\n!bbc\n!bc!\n!bc&\n!bc|\n!bca\n!bcb\n!bcc\n!c!!\n!c!&\n!c!|\n!c!a\n!c!b\n!c!c\n!c&!\n!c&&\n!c&|\n!c&a\n!c&b\n!c&c\n!c|!\n!c|&\n!c||\n!c|a\n!c|b\n!c|c\n!ca!\n!ca&\n!ca|\n!caa\n!cab\n!cac\n!cb!\n!cb&\n!cb|\n!cba\n!cbb\n!cbc\n!cc!\n!cc&\n!cc|\n!cca\n!ccb\n!ccc\n&!!!\n&!!&\n&!!|\n&!!a\n&!!b\n&!!c\n&!&!\n&!&&\n&!&|\n&!&a\n&!&b\n&!&c\n&!|!\n&!|&\n&!||\n&!|a\n&!|b\n&!|c\n&!a!\n&!a&\n&!a|\n&!aa\n&!ab\n&!ac\n&!b!\n&!b&\n&!b|\n&!ba\n&!bb\n&!bc\n&!c!\n&!c&\n&!c|\n&!ca\n&!cb\n&!cc\n&&!!\n&&!&\n&&!|\n&&!a\n&&!b\n&&!c\n&&&!\n&&&&\n&&&|\n&&&a\n&&&b\n&&&c\n&&|!\n&&|&\n&&||\n&&|a\n&&|b\n&&|c\n&&a!\n&&a&\n&&a|\n&&aa\n&&ab\n&&ac\n&&b!\n&&b&\n&&b|\n&&ba\n&&bb\n&&bc\n&&c!\n&&c&\n&&c|\n&&ca\n&&cb\n&&cc\n&|!!\n&|!&\n&|!|\n&|!a\n&|!b\n&|!c\n&|&!\n&|&&\n&|&|\n&|&a\n&|&b\n&|&c\n&||!\n&||&\n&|||\n&||a\n&||b\n&||c\n&|a!\n&|a&\n&|a|\n&|aa\n&|ab\n&|ac\n&|b!\n&|b&\n&|b|\n&|ba\n&|bb\n&|bc\n&|c!\n&|c&\n&|c|\n&|ca\n&|cb\n&|cc\n&a!!\n&a!&\n&a!|\n&a!a\n&a!b\n&a!c\n&a&!\n&a&&\n&a&|\n&a&a\n&a&b\n&a&c\n&a|!\n&a|&\n&a||\n&a|a\n&a|b\n&a|c\n&aa!\n&aa&\n&aa|\n&aaa\n&aab\n&aac\n&ab!\n&ab&\n&ab|\n&aba\n&abb\n&abc\n&ac!\n&ac&\n&ac|\n&aca\n&acb\n&acc\n&b!!\n&b!&\n&b!|\n&b!a\n&b!b\n&b!c\n&b&!\n&b&&\n&b&|\n&b&a\n&b&b\n&b&c\n&b|!\n&b|&\n&b||\n&b|a\n&b|b\n&b|c\n&ba!\n&ba&\n&ba|\n&baa\n&bab\n&bac\n&bb!\n&bb&\n&bb|\n&bba\n&bbb\n&bbc\n&bc!\n&bc&\n&bc|\n&bca\n&bcb\n&bcc\n&c!!\n&c!&\n&c!|\n&c!a\n&c!b\n&c!c\n&c&!\n&c&&\n&c&|\n&c&a\n&c&b\n&c&c\n&c|!\n&c|&\n&c||\n&c|a\n&c|b\n&c|c\n&ca!\n&ca&\n&ca|\n&caa\n&cab\n&cac\n&cb!\n&cb&\n&cb|\n&cba\n&cbb\n&cbc\n&cc!\n&cc&\n&cc|\n&cca\n&ccb\n&ccc\n|!!!\n|!!&\n|!!|\n|!!a\n|!!b\n|!!c\n|!&!\n|!&&\n|!&|\n|!&a\n|!&b\n|!&c\n|!|!\n|!|&\n|!||\n|!|a\n|!|b\n|!|c\n|!a!\n|!a&\n|!a|\n|!aa\n|!ab\n|!ac\n|!b!\n|!b&\n|!b|\n|!ba\n|!bb\n|!bc\n|!c!\n|!c&\n|!c|\n|!ca\n|!cb\n|!cc\n|&!!\n|&!&\n|&!|\n|&!a\n|&!b\n|&!c\n|&&!\n|&&&\n|&&|\n|&&a\n|&&b\n|&&c\n|&|!\n|&|&\n|&||\n|&|a\n|&|b\n|&|c\n|&a!\n|&a&\n|&a|\n|&aa\n|&ab\n|&ac\n|&b!\n|&b&\n|&b|\n|&ba\n|&bb\n|&bc\n|&c!\n|&c&\n|&c|\n|&ca\n|&cb\n|&cc\n||!!\n||!&\n||!|\n||!a\n||!b\n||!c\n||&!\n||&&\n||&|\n||&a\n||&b\n||&c\n|||!\n|||&\n||||\n|||a\n|||b\n|||c\n||a!\n||a&\n||a|\n||aa\n||ab\n||ac\n||b!\n||b&\n||b|\n||ba\n||bb\n||bc\n||c!\n||c&\n||c|\n||ca\n||cb\n||cc\n|a!!\n|a!&\n|a!|\n|a!a\n|a!b\n|a!c\n|a&!\n|a&&\n|a&|\n|a&a\n|a&b\n|a&c\n|a|!\n|a|&\n|a||\n|a|a\n|a|b\n|a|c\n|aa!\n|aa&\n|aa|\n|aaa\n|aab\n|aac\n|ab!\n|ab&\n|ab|\n|aba\n|abb\n|abc\n|ac!\n|ac&\n|ac|\n|aca\n|acb\n|acc\n|b!!\n|b!&\n|b!|\n|b!a\n|b!b\n|b!c\n|b&!\n|b&&\n|b&|\n|b&a\n|b&b\n|b&c\n|b|!\n|b|&\n|b||\n|b|a\n|b|b\n|b|c\n|ba!\n|ba&\n|ba|\n|baa\n|bab\n|bac\n|bb!\n|bb&\n|bb|\n|bba\n|bbb\n|bbc\n|bc!\n|bc&\n|bc|\n|bca\n|bcb\n|bcc\n|c!!\n|c!&\n|c!|\n|c!a\n|c!b\n|c!c\n|c&!\n|c&&\n|c&|\n|c&a\n|c&b\n|c&c\n|c|!\n|c|&\n|c||\n|c|a\n|c|b\n|c|c\n|ca!\n|ca&\n|ca|\n|caa\n|cab\n|cac\n|cb!\n|cb&\n|cb|\n|cba\n|cbb\n|cbc\n|cc!\n|cc&\n|cc|\n|cca\n|ccb\n|ccc\na!!!\na!!&\na!!|\na!!a\na!!b\na!!c\na!&!\na!&&\na!&|\na!&a\na!&b\na!&c\na!|!\na!|&\na!||\na!|a\na!|b\na!|c\na!a!\na!a&\na!a|\na!aa\na!ab\na!ac\na!b!\na!b&\na!b|\na!ba\na!bb\na!bc\na!c!\na!c&\na!c|\na!ca\na!cb\na!cc\na&!!\na&!&\na&!|\na&!a\na&!b\na&!c\na&&!\na&&&\na&&|\na&&a\na&&b\na&&c\na&|!\na&|&\na&||\na&|a\na&|b\na&|c\na&a!\na&a&\na&a|\na&aa\na&ab\na&ac\na&b!\na&b&\na&b|\na&ba\na&bb\na&bc\na&c!\na&c&\na&c|\na&ca\na&cb\na&cc\na|!!\na|!&\na|!|\na|!a\na|!b\na|!c\na|&!\na|&&\na|&|\na|&a\na|&b\na|&c\na||!\na||&\na|||\na||a\na||b\na||c\na|a!\na|a&\na|a|\na|aa\na|ab\na|ac\na|b!\na|b&\na|b|\na|ba\na|bb\na|bc\na|c!\na|c&\na|c|\na|ca\na|cb\na|cc\naa!!\naa!&\naa!|\naa!a\naa!b\naa!c\naa&!\naa&&\naa&|\naa&a\naa&b\naa&c\naa|!\naa|&\naa||\naa|a\naa|b\naa|c\naaa!\naaa&\naaa|\naaaa\naaab\naaac\naab!\naab&\naab|\naaba\naabb\naabc\naac!\naac&\naac|\naaca\naacb\naacc\nab!!\nab!&\nab!|\nab!a\nab!b\nab!c\nab&!\nab&&\nab&|\nab&a\nab&b\nab&c\nab|!\nab|&\nab||\nab|a\nab|b\nab|c\naba!\naba&\naba|\nabaa\nabab\nabac\nabb!\nabb&\nabb|\nabba\nabbb\nabbc\nabc!\nabc&\nabc|\nabca\nabcb\nabcc\nac!!\nac!&\nac!|\nac!a\nac!b\nac!c\nac&!\nac&&\nac&|\nac&a\nac&b\nac&c\nac|!\nac|&\nac||\nac|a\nac|b\nac|c\naca!\naca&\naca|\nacaa\nacab\nacac\nacb!\nacb&\nacb|\nacba\nacbb\nacbc\nacc!\nacc&\nacc|\nacca\naccb\naccc\nb!!!\nb!!&\nb!!|\nb!!a\nb!!b\nb!!c\nb!&!\nb!&&\nb!&|\nb!&a\nb!&b\nb!&c\nb!|!\nb!|&\nb!||\nb!|a\nb!|b\nb!|c\nb!a!\nb!a&\nb!a|\nb!aa\nb!ab\nb!ac\nb!b!\nb!b&\nb!b|\nb!ba\nb!bb\nb!bc\nb!c!\nb!c&\nb!c|\nb!ca\nb!cb\nb!cc\nb&!!\nb&!&\nb&!|\nb&!a\nb&!b\nb&!c\nb&&!\nb&&&\nb&&|\nb&&a\nb&&b\nb&&c\nb&|!\nb&|&\nb&||\nb&|a\nb&|b\nb&|c\nb&a!\nb&a&\nb&a|\nb&aa\nb&ab\nb&ac\nb&b!\nb&b&\nb&b|\nb&ba\nb&bb\nb&bc\nb&c!\nb&c&\nb&c|\nb&ca\nb&cb\nb&cc\nb|!!\nb|!&\nb|!|\nb|!a\nb|!b\nb|!c\nb|&!\nb|&&\nb|&|\nb|&a\nb|&b\nb|&c\nb||!\nb||&\nb|||\nb||a\nb||b\nb||c\nb|a!\nb|a&\nb|a|\nb|aa\nb|ab\nb|ac\nb|b!\nb|b&\nb|b|\nb|ba\nb|bb\nb|bc\nb|c!\nb|c&\nb|c|\nb|ca\nb|cb\nb|cc\nba!!\nba!&\nba!|\nba!a\nba!b\nba!c\nba&!\nba&&\nba&|\nba&a\nba&b\nba&c\nba|!\nba|&\nba||\nba|a\nba|b\nba|c\nbaa!\nbaa&\nbaa|\nbaaa\nbaab\nbaac\nbab!\nbab&\nbab|\nbaba\nbabb\nbabc\nbac!\nbac&\nbac|\nbaca\nbacb\nbacc\nbb!!\nbb!&\nbb!|\nbb!a\nbb!b\nbb!c\nbb&!\nbb&&\nbb&|\nbb&a\nbb&b\nbb&c\nbb|!\nbb|&\nbb||\nbb|a\nbb|b\nbb|c\nbba!\nbba&\nbba|\nbbaa\nbbab\nbbac\nbbb!\nbbb&\nbbb|\nbbba\nbbbb\nbbbc\nbbc!\nbbc&\nbbc|\nbbca\nbbcb\nbbcc\nbc!!\nbc!&\nbc!|\nbc!a\nbc!b\nbc!c\nbc&!\nbc&&\nbc&|\nbc&a\nbc&b\nbc&c\nbc|!\nbc|&\nbc||\nbc|a\nbc|b\nbc|c\nbca!\nbca&\nbca|\nbcaa\nbcab\nbcac\nbcb!\nbcb&\nbcb|\nbcba\nbcbb\nbcbc\nbcc!\nbcc&\nbcc|\nbcca\nbccb\nbccc\nc!!!\nc!!&\nc!!|\nc!!a\nc!!b\nc!!c\nc!&!\nc!&&\nc!&|\nc!&a\nc!&b\nc!&c\nc!|!\nc!|&\nc!||\nc!|a\nc!|b\nc!|c\nc!a!\nc!a&\nc!a|\nc!aa\nc!ab\nc!ac\nc!b!\nc!b&\nc!b|\nc!ba\nc!bb\nc!bc\nc!c!\nc!c&\nc!c|\nc!ca\nc!cb\nc!cc\nc&!!\nc&!&\nc&!|\nc&!a\nc&!b\nc&!c\nc&&!\nc&&&\nc&&|\nc&&a\nc&&b\nc&&c\nc&|!\nc&|&\nc&||\nc&|a\nc&|b\nc&|c\nc&a!\nc&a&\nc&a|\nc&aa\nc&ab\nc&ac\nc&b!\nc&b&\nc&b|\nc&ba\nc&bb\nc&bc\nc&c!\nc&c&\nc&c|\nc&ca\nc&cb\nc&cc\nc|!!\nc|!&\nc|!|\nc|!a\nc|!b\nc|!c\nc|&!\nc|&&\nc|&|\nc|&a\nc|&b\nc|&c\nc||!\nc||&\nc|||\nc||a\nc||b\nc||c\nc|a!\nc|a&\nc|a|\nc|aa\nc|ab\nc|ac\nc|b!\nc|b&\nc|b|\nc|ba\nc|bb\nc|bc\nc|c!\nc|c&\nc|c|\nc|ca\nc|cb\nc|cc\nca!!\nca!&\nca!|\nca!a\nca!b\nca!c\nca&!\nca&&\nca&|\nca&a\nca&b\nca&c\nca|!\nca|&\nca||\nca|a\nca|b\nca|c\ncaa!\ncaa&\ncaa|\ncaaa\ncaab\ncaac\ncab!\ncab&\ncab|\ncaba\ncabb\ncabc\ncac!\ncac&\ncac|\ncaca\ncacb\ncacc\ncb!!\ncb!&\ncb!|\ncb!a\ncb!b\ncb!c\ncb&!\ncb&&\ncb&|\ncb&a\ncb&b\ncb&c\ncb|!\ncb|&\ncb||\ncb|a\ncb|b\ncb|c\ncba!\ncba&\ncba|\ncbaa\ncbab\ncbac\ncbb!\ncbb&\ncbb|\ncbba\ncbbb\ncbbc\ncbc!\ncbc&\ncbc|\ncbca\ncbcb\ncbcc\ncc!!\ncc!&\ncc!|\ncc!a\ncc!b\ncc!c\ncc&!\ncc&&\ncc&|\ncc&a\ncc&b\ncc&c\ncc|!\ncc|&\ncc||\ncc|a\ncc|b\ncc|c\ncca!\ncca&\ncca|\nccaa\nccab\nccac\nccb!\nccb&\nccb|\nccba\nccbb\nccbc\nccc!\nccc&\nccc|\nccca\ncccb\ncccc\n```\n:::\n:::\n\n\nGiven that.\n01:47\nIt can.\n1\nSpeaker 1\n01:58\nGenerate\n2\nSpeaker 2\n01:59\ncompatible patterns.\n02:05\nWith a certain signaling. System.\n02:16\nPatterns, that are incompatible with a certain signaling State.\n02:25\nGiven this.\n1\nSpeaker 1\n02:32\nThe signaling\n2\nSpeaker 2\n02:33\nstate that will emerge. Will be. That is compatible.\n02:42\nThe sequence is generated.\n1\nSpeaker 1\n03:01\nIt will\n2\nSpeaker 2\n03:02\nbe. Compositional.\n1\nSpeaker 1\n03:17\nIf the\n2\nSpeaker 2\n03:17\npatter, Generated.\n03:22\nCompatible.\n03:27\nWith the Of homophism.\n03:35\nHomomorphism.\n03:41\nIn other words, if\n03:46\nReserves the structure of the pre-linguistic object.\n03:55\nPreserves the structure. Over the pre-linguistic object.\n04:05\nOtherwise.\n1\nSpeaker 1\n04:15\nDespite\n2\nSpeaker 2\n04:16\nusing. A language built over in alphabet of\n04:26\nAtomic signals. We end up. For the simple noise, sign in the game. That is not discriminate.\n04:40\nBetween symbols.\n04:46\nAnd that's called the Nissan, the full sequences.\n1\nSpeaker 1\n04:55\nCoordinates\n2\nSpeaker 2\n04:56\nover. Complete sequences rather than considering coordinating on the atomics. Symbols.\n05:20\nAbout that.\n06:36\nNow, if we just consider arbitrary symbols,\n06:46\nOr rather arbitrary sequences are generated. China, likely. The sequence to the homorphism.\n07:01\nThis doesn't mean.\n07:15\nThat we can't ever have. Compositionality baked in. It that boils down. If we're able. Um, Discard bad early choices. And, Make use of. Better ones down the line. What do I?\n08:07\nLet's simplify things by imagining the only two possible signaling systems possible. The. It's compositional and the second is,\n08:24\nFully Tangled, for example. Fully entangled means that it is a simple signaling systems. There are no composite signals Rather no complexing no complex signals.\n08:50\nSo we have a complex system and a simple system.\n09:01\nThe, the chance of emitting a sequence that's compatible with the complex system is going to be\n09:14\nBut fixed. Particularly if we don't have a lot of messages.\n09:28\nIf we do have a lot of messages, that's a different story.\n09:36\nWhich we should consider.\n09:46\nWe do realize that we're more likely to have a complex system emerge with larger. Input spaces.\n10:06\nLet's suppose we keep emitting random signals. And,\n10:26\nEven if the Receiver. Guess is the correct answer. There is a small probability Epsilon. Of emitting. Another sequence.\n10:48\nPossibly.\n10:55\nWhat that is already news. What that is already in use.\n11:13\nAny given message?\n11:20\nOn the other hand.\n11:30\nThe receiver.\n11:45\nWell, in this case.\n11:53\nHave to make guess. Six. Sequence. Seeing this new sequence.\n12:16\nGive it that. There is already. Signaling system in place. And, This side usually gets classifier. This side via its classifier. What the speeds?\n12:40\nWhat this means. What this sequence means, right?\n12:57\nThen we might end up by chance.\n13:04\nSecond signal. Well compatible. More compatible.\n13:21\nThat. B system. Which is the complex system rather than with the simple system.\n13:37\nIn other words, It satisfies certain symmetry. Extend with The linguistic object.\n13:55\nThe classifier. Has already picked up. On the Symmetry. It is.\n14:18\nReinforce this aspect of the classifier,\n14:27\nNotice that the classifier. If this case is not killed with the messages, Specifically worked with\n14:48\nDecode. The prolinguistic object.\n15:13\nNot that this is important. The important thing is, That.\n15:21\nThe classifier sides. The classifier, assigns These messages.\n15:37\nThe correct.\n15:42\nPre-Linguistic object. So now, let's imagine we have two of these.\n16:11\nThe inductive bar of the classifier. The inductive bias of the classifier.\n16:22\nMight be. Similar to A3. A decision tree. What does that mean for us?\n16:48\nSpeeds the Is traditionally. The most likely splitified. For this eyelid.\n17:48\nActually it's a mistake they're trying to dig in into this un. We build it this way specifically. So let's set this aside. Just consider this Stochastic. Let's imagine that we have a stochastic. Decision mechanism.\n18:17\nIf we have this stochastic decision mechanism,\n18:25\nPerhaps. The distribution. The. Possible. Classifiers. And that we are somehow able to pick.\n18:46\nRandomly out of a subset of. Um, Classifiers. The ones which Those compatible with the historic data. In such a setting. Our future. Predictions. To be dependent on previous ones. But so long, As we keep on going. We will go to more and more compatible signals. With complex. Signaling rather than with, Simple.\n19:34\nHow is that? That's actually.\n19:43\nIt's actually wrong. Yeah, we're gonna get more incompatible ones. That's bad news. But\n20:02\nAssuming. That there is. Very large number. Of both types.\n20:18\nOr perhaps, even an infinite number of both types doesn't really matter, does it. So we keep on going. Uh, we have\n20:35\nWe have initially a kind of System was A few complex signals baked in, but mostly Um, Simple entangled representations but we keep on going because our classifier is not so good. It keeps needs to get more good examples. Can only train if it gets a good example. So, it's slow.\n21:17\nAnd during this, we keep on coordinating and what that means is we get a chance to switch Signals with meetings, using the exploration. Okay.\n21:48\nThis. Yeah. I suppose we Have multiple wins. Multiple ways for some signal, for example. Red cylinder.\n22:22\nA bunch of them are.\n22:29\nHe's compatible.\n22:34\nWhich the complex signaling system, but most of them are sorry but one of them is good. And let's say this is true.\n22:51\nFor another. One. And another one. And another one.\n23:02\nDoes my hypothesis.\n23:10\nIn such a case. The reinforcement capabilities. In the algorithm. Can be such that in the limit. We will forget the incompatible ones. And remember the compatible ones. How is this possible? Simply stated. Simply stated. This can be possible if.\n23:54\nRather, before we say, simply stated, let's consider that having both of Signaling systems at once. Is a mixed equilibrium. And this is the real hypothesis. Given that you have a mixed equilibrium. For partial. For the partially observed language.\n24:26\nWhich contains both a composite. And,\n24:41\nSimple equilibrium. And, The input space is sufficiently large.\n24:54\nThat the attraction.\n25:00\nOr stability. Yeah. The, the complex system is going to be should be Were attractive more stable and thus attract. Attract. What does that mean? It means as we learn. This process. Continues. The mixed equilibrium should. Converge. To a pure equilibrium on. Complex. Signaling system. Rather than\n25:51\nMixed one. Um,\n26:01\nOther one, the Entangled representation. Given that. Given that. The classifier can disentangle things, if it can't. This won't happen. In other words, we're saying that. If the classifier Is able to learn. To represent within itself. The substates.\n26:44\nIt will be able to reinforce not just the full symbol but the substates. If it can reinforce on the sub States, Will eventually be able to. Reject forget whatever the bad States. The entangled.\n27:10\nAnd keep the good ones. Compound ones. Another interesting thing is, suppose the leading Is entangled.\n27:28\nAnd uses the most. Uses. This an atomic symbol. Just one. This might be. Beneficial. You might not want to learn. A. Compound. Symbol for it. Why is this? It's more efficient. Even given the possibility of full. Regular system. Might be more efficient to code. The signaling system. Ignore by ignoring this.\n28:13\nBy ignoring us. So, by ignoring The. Regular. Compound form and keeping I'm not by keeping a simple. A simple. Signal. Right, we use an atomic symbol here. But this would be to the detriment.\n28:48\nBeing unable to use this. As the final. Symbol. In all the other. Encodings. Might still be efficient if you only use this once.\n29:10\nAnd we might be able to use this second atomic symbol. In this fashion.\n29:22\nPerhaps for the second, most Common state, assuming that, it's These first three states are significantly. Orders of magnitude more likely than Either states, right? So we give A and B. And then we use. For petting, the rest of the messages.\n29:55\nCc. Ccpa. And Force free seasons, a and three C's with a b. C's.\n30:15\nCab. Sorry CBA. Ccba and so on.\n30:29\nAnyhow.\n30:34\nI think this is the way that we could have. Complex. Signaling system emerge.\n30:48\nA complex signaling systems with a few persistent. A few persistent.\n31:06\nA few persistent irregular forms.\n\n---\n\n\n## The Third Question - Signals\n\nThe third question. Third question. Has to do with what? Of this. Referential game. Makes the big Improvement over the plain vanilla Lewis game. But clearly. The multiple choice aspect of it. Speeds are flirting makes the decision. Quicker.\n00:37\nUm, but they think that That's not what introduces. In equilibrium.\n00:53\nIt's not what? Introduces new equilibriums.\n01:02\nSo, after the notifications, do we have? Well, one bit Is that we have image classifiers?\n01:18\nIt's that. Steadiness.\n01:26\nMaybe.\n01:31\nSo, introducing a classifier,\n01:39\nMay well be equivalent. To introducing Grandma.\n01:49\nNot Grandma a grammar.\n02:09\nWhy is that? Because the grammar is Can be viewed as well. Can be viewed as a decision rule. And,\n02:28\nWhat grammar does essentially?\n2\nSpeaker 2\n02:30\nUh,\n1\nSpeaker 1\n02:31\ntaking a bunch of inputs and\n02:36\nClassifies. This certain state?\n02:46\nClassifies the inputs into a specific. The state is semantic. But,\n02:59\nIn the world of neural networks that state made the what we need to reconstruct an image. Or some other pretty linguistic options.\n03:13\nThree linguistic object.\n03:19\nWow, this is a very bad transcription.\n03:39\nAnyway, since we can't transcribe pre-linguistic objects, what we should. Say, is a\n2\nSpeaker 2\n03:51\nSo\n1\nSpeaker 1\n03:52\nwe can transcribe it so we can.\n03:58\nUse discommerce a decision rule, which takes inputs. And, and The coached them into a state.\n04:10\nThe coach them into a state. Decode. The interstate. Into a state.\n04:28\nOkay.\n04:38\nIf we have a certain, Classify working on the inputs. We should be able to recover the original state, the slowest.\n04:56\nThe senders encoded them in a faithful way in a way, which we haven't. And the essential information.\n05:12\nIf the coding also has a\n05:19\nHomophism and homomorphismic properties.\n05:27\nThen our classifier should be able to learn.\n05:34\nWe cover the state much more easily. Taking advantage of the symmetries.\n05:46\nInherent in. Message.\n06:03\nWhat else do we have in this signaling game? Or rather.\n06:17\nReferential game. So, we have\n06:28\nA protocol.\n06:34\nNo, we have something else. We have this thing called the random. Message generally.\n06:45\nSurprisingly enough.\n06:51\nThis is. A great situation for bl. Blind watchmaker to position themselves.\n07:03\nWhat they mean here is that\n07:11\nThe sender serendipitously. Generates a Message.\n07:22\nThat can be easily. Broken down. Into units.\n07:34\nThe receiver will end. We'll learn this structure. Or rather we learned this message.\n07:48\nAnd since it is a classifier, If it gets a bunch of these,\n07:57\nIt will be much more likely to. Interpret.\n08:07\nSimilar messages. Correctly and unlikely to interpret different messages correctly.\n08:26\nI considered. Situations. The sender. Makes percentage persistent use. Of the same messages for this, for subsequent States.\n08:46\nBut if he fails And there is an unlimited number of available signals. He could. Generate the new signal.\n09:05\nThis makes learning slower.\n09:14\nThis allows.\n09:20\nThe receiver. More latitude.\n09:28\nIn the sense that\n09:34\nBut randomly generating new signals. If we come with a signal, That's a good fit. For the classifier. Based on what it is, already figured out. We're gonna be more likely.\n09:55\nCoordinate.\n10:00\nA signal. Following the structure. That we learned and maybe the semantics that we learned.\n10:13\nSo, we can think of this. S exploration.\n10:19\nAnother idea.\n10:24\nWhich I had for simpler signaling systems, is to try to swap out signals. This was inspired by Hoffman Coatings. Frequent.\n10:44\nUh, signals would get shorter, encodings If? Distributions evolved over time. And we realized that the rare signal initially is actually now quite common. We would want to Swap it. With some other signal, which we thought was quite common but now we see us Infrequent.\n11:23\nThis ability to swap. Would need to be coordinated, too.\n11:32\nBut we could have a protocol for doing this automatically.\n11:43\nWe could at every time step. Uh, reconsider All signals.\n11:57\nFrequencies and variances. And use these two details. To reorder. Their semantics.\n12:12\nIn other words to associate shorter signals with Frequent States and longer signals with infrequent States.\n12:33\nWhat about longer signals? How do we optimize these? We need the most sophisticated exploratory strategy.\n12:50\nOne smart way might be to use some kind of a function So,\n12:59\nI would call this. Looking at the search base. Homomorphisms. And homeomorphisms. Both. That would make more sense in the realm of the lie algebra.\n13:24\nSuppose we do have. Operators. Which preserve Allah. We might want to. Try to use these.\n13:38\nComponent of Sequence generator. We will want to use these to map the state to sequence.\n13:53\nFor the sequence. With the the image of this layout in the loops, Another set.\n14:09\nLie, algebra group.\n14:14\nOn the set.\n14:23\nI don't think my job here is to Solve this, but It appears that.\n14:36\nThat, what what? What, what? What?\n14:44\nYes, yes. Yes. Yeah, it would appear that. The two beds.\n15:01\nReferential game to capture Coptic signals.\n15:11\nComplex signals are\n15:17\nA generator.\n15:21\nIs aminable. To create.\n15:32\nNice.\n15:37\nRepresentations. Nice sequences which are the compass composable.\n15:49\nWhich are composable.\n15:54\nThey're not composable, but they are. Amountable. To disentanglement. Of meaning.\n16:07\nThis is all fine.\n16:13\nVia trend and error. As long as we don't have to, We use symbols. If, for yourself, we want to reuse symbols, you really need to have Planning generator. In other words, we want the sender to\n16:41\nThe messages. So that\n16:50\nWhen he sends blue banana. He's already thought about.\n16:58\nGreen banana and yellow banana.\n17:06\nAnd reserved. The symbols for banana green, blue, and yellow. And we purchased these symbols. Reserve these symbols.\n17:28\nWhen such States arise in the future.\n17:35\nSo this is the planning. I've been thinking about, In the late signal games.\n17:52\nWhen it comes to arbitration, when it comes to arbitrary. When it comes to arbitrary States.\n18:06\nPlanning is more challenging.\n18:11\nAnd still, we will. Similar states to have similar embeddings. In other words to be close.\n18:24\nThe linguistic manifold as well as\n18:32\nThree-Linguistic manifold. Pretty linguistic manifold. The state manifold.\n19:03\nBetter yet if we can also. Support. Structure. If we support structure, we get learnability. We also capture salience. And I think salience is at the root of semantics. Or at least. One of the corners of semantics.\n19:34\nWhen it comes to, Thinking about threats. Saliency. Captures the greater threats. In that sense. It's similar to the. Reward hypothesis. Perhaps.\n20:01\nOkay, so we say this, Suppose we have.\n20:10\nSuppose we have this.\n20:15\nSmart. Generator. Of messages is from. And suppose.\n20:30\nOur Center gauges in planning, he can decompose the state. For example, if it's a dick dictionary of key values.\n20:42\nIn such a way that It is easy to learn. To recover. These things.\n20:58\nThen.\n21:04\nWe will probably. Have a nice representation.\n21:13\nYes, and\n21:22\nThen we can look at learnability.\n21:32\nWe could request teaching sessions in a teaching session. The sender gets to pick the state. The center gets to pick the state.\n21:48\nThe sender gets to pick the state.\n21:55\nPossibly a partial.\n22:03\nAnd then, Sender and receiver. Exchange, an atomic symbol.\n22:16\nWe're learning the language. We might not have a state corresponding to a pronoun. Just sentences. But we still\n2\nSpeaker 2\n22:25\nwant to be able to teach a pronoun.\n1\nSpeaker 1\n22:28\nSo, we want Send a partial.\n22:38\nIf we can. Teach our student partial States.\n22:50\nThat's great. Alternatively, we might not be able to do this in this game. Another approach. Would be to give them the capacity to\n23:03\nConsider these partial States.\n23:08\nThis can happen in the classifier. Or it can happen.\n23:16\nIn a specialized components. What does that mean? Specialist component here. Um, Would be some internally motivated linguist. Perhaps. Something that looks at these sequences.\n23:41\nAnd, Tries to. Assign each bit. Semantic semantics.\n23:57\nAnyhow.\n24:11\nA third. Aspect. Is that we? Perhaps. The generator. In the classifier.\n24:29\nThe generator, and the classifier. To have their own actor critic.\n24:38\nArchitecture.\n24:45\nThe linguist is the critic. The actor. Is the generator.\n24:58\nThat's an interesting architecture.\n25:17\nThis seems to be leading towards. The fourth problem.\n3\nSpeaker 3\n25:25\nThe fourth problem.\n1\nSpeaker 1\n25:28\nIt's composition. Of noise game.\n25:39\nOther games. What we discussed so far has been. In my view. Good position of a loose game. With classifier, perhaps a classification game.\n26:02\nWe could break it down even further. Perhaps.\n26:09\nSerializer. The serializer is.\n26:16\nA simple.\n26:22\nConverter.\n26:28\nThe serial.\n26:46\nGrammar. Is really what's in charge of?\n26:59\nBreaking down and assembling things in a meaning preserving form. So if we talked about homomorphisms, And the hobo milk prisms,\n27:15\nThis is not arbitrary, but You're responding to some.\n27:25\nSemantics some meaning. What's the meaning?\n27:36\nIn the most General way. The meaning comes from, And then DP, being an MDP. That we need to take decisions that we have states that the transitions leads to rewards and that there is a goal. And, We want to be. Maximizing. These Maximizing. This, the This go.\n28:14\nMaximizing. The expected. That's the MDP view in Pac-Man. We shift from\n28:29\nGeneralities of the mdp2, specifics. Damn. And in this sense,\n28:43\nSemantics shift from a specifics on the general to the specific.\n28:51\nI think. I'm almost done here. Um,\n29:03\nWe probably want to be able to, Capture.\n29:13\nA framing game Beyond just. Crucification.\n29:20\nWhich is happening in the MVP. Particularly, in the case of\n29:30\nLifelong agents. There may be multiple\n29:36\nMultiple games. Happening.\n29:42\nIn parallel. We wish we need to interact.\n29:57\nWould be good tomorrow.\n30:02\nLoose game interacting with this other kind of game.\n30:08\nAnd implement it. Using a simple obstruction.\n\n\n---\n\n## Grounding \nThe fourth and Final  question is one of grounding\n\nTHis means aligning semantics coordinated using a lewis game with the semantics of some external system. This could be users who whish to understand and interpret the signals. It might be some language model that uses the lewis game to mediate with some other game. It might be more simple some external framing game, e.g. one where we learn to classify images and use the lewis game to interpret messages that indicate the the right one to pick.\n\nOne aspect of grounding I got from reviewing [@]\n\nGrounding can be thought of as happening in the lewis game. But we might want the semantics to be able to permeate into the framing games neural network at the many levels is uses to learn it's representations. This hypotheticaly might allow the all seeing eye of the CNN or vision transformer to have neurons corresponding to semantic sub-states in layer correspoing to the required receptive fields needed to identify these sub-state represnations.\n\nI realy don't have much to say about grounding.\n\nOk tha't a bit of a stretch. There is this:\n\nWe talk about states and pre-linguistic objects. But where do these come from? Can't say much about it in general. But in RL or MARL we like to think that states come from a  Markov Chain part of out Markov Decision Process. This means something wonderful - we can if we want think of the States coming from an MDP. Learning RL I have been doing so much thinking about MDPs. Tabular states, States with continuous variables. States with action values. States with Features and using them to build Function approximators, temporal abstraction over states. Markov aggregations of states. \n\nSorry to bore you with this long chain of thought -- Lewis games are ofthen considered stateless and can be solved by bandit like algorithms. Sure [Emergence of lanGuage in Games](https://github.com/facebookresearch/EGG), c.f. (@kharitonov:etal:2021) uses [reinforce](), however it still seems overkill. But when we get to complex signals we need the ability to learn aggregation rules, grammars, homomorphisms, homeomorphisms, and the like.\n\n\n## Planning in the Lewis Game {#sec-planning}\n\nHow can the sender use planing in the lewis signaling game to build a signaling system\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}