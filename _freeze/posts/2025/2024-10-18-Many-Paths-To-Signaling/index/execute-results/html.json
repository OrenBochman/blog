{
  "hash": "c775e7c574de6f17fd93862ea2ccdd53",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: The Many Path To A Signaling System\nbibliography: ./bibliography.bib\ncategories: [signaling systems, lewis signaling game, reinforcement learning, bayesian games, information theory, game theory, bayesian reinforcement learning]\nkeywords: [compositionality, partial pooling equilibria,huffman codes, spontaneous symmetry breaking]\nformat: \n    html: \n        code-fold: true\n---\n\n\n\n\n## TL;DR\n\nWhile reviewing the literature on lewis games and thier extensions I realized that there are a many ways that signaling systems can arise. Ofttimes people make assumption that the one they envision is the only one while in reality there are many nuanced ways that signaling systems can arise. \n\nOne fascinating aspect the Lewis signaling game is that although there are many theoretical equilibria intially  the agents will fail to coordinate and they can only reach the optimal signaling systems after some iterations of the game in which they either evolve or use reinfocement learning to coordinate a themselves to a common signaling strategy. In the prisoners dillema agents can learn to coopertate if the game is iterated. In the Lewis signaling game agents can learn to coordinate on a signaling system if the game is iterated. \n\nTo reach a good signaling system requires some kind of algorithm as well some number of iterations. I don't recall seeing a discussion of the the minimum or the expected number of iterations required to reach a signaling system under different algorithms. In other words most researchers have considered the complexity of coordination in signaling systems. This is actually a fairly simple problem to solve in the most common settings.\n\nAnother two point primerily addressed by the evolutionary game theory community who view evolution in terms of replicator dynamics is that of stability of equilibria and the notions of evolutionarily stable strategies. \nThe first has to do with convergence of learning to an optimal signaling system. \nThe second has to do with the ability of an equilibrium to resist invasion by a mutant strategy.\n\nA related issues is that of enumerating different types of equilibriums in larger games. For basic lewis signaling games this is not very diffucult but once one imposes a structure on the state and requires complex signals to emerge we get to a point where it may be quite challenging to enumerate all the possible equilibria.\n\nAnother point of interest to me is to consider the emergence of grammer and of a morphology. In [@Nowak1999] The authors give a result for the emergence of garammar in a signaling system. This is that there are many more \n\nI think it worth while to list them in this space --- particularly as I believe that signaling systems are a key for transfer learning in reinforcement learning which together with learning to represent complex states may be the key to AGI.\n\n\n## Introduction\n\n- Listing number of different scenarios on how signaling systems can arise in the Lewis signaling games. \n- I will start with a story \n- Next add some details like some variants and look some basic analysis. \n- Finally I'll try to place it into the context of MARL. Note that we will be dealing with partially observed multi agent RL. But each scenario can have a different setting.\n\n![lewis signaling game](./lewis_extensive_form.svg){.center}\n\nIn The book signals [@skyrms2010signals] the author, Skryms, discusses how Lewis challenged the skepticism of his advisor Quine regarding the  meaning and convention may arise via an arbitrary mechanism like symmetry breaking.\n\nWhen I considered solving some additional issues surrounding the fundamentals of signaling systems I realized that I had a few different scenarios in mind and that writing them down with some semblance of formalism might be helpful. It turns out that indeed this turns out to be a stepping stone towards developing an optimal algorithms for learning signaling system in different rl settings.\n\nLet's face it under different settings the task of acquiring a signaling system can be easier or harder. In [@skyrms2010signals] the author points out that at symmetry breaking all the different signaling systems that could be learned are equivalent. However if there is an asymmetry in the form of a non-uniform distribution of states or different signaling risks then we we might prefer some signaling systems over others and there might even be a unique optimal signaling system. Furthermore like in reality one would expect that with time distributions of states might change and the optimal signaling system might change as well.\n\n## 1. The Oracle of Saliency\n\n\n::: {.callout-note}\n\n### Story: The Oracle of Saliency\n\nSender and Receivers consult an \"Oracle\" (perhaps in book form). The oracle tells them how to map states to action, the oracle provides the sender with a one to one mapping of states to actions and to the receiver with the transpose, a mapping of signals to actions. The sender and receiver can then use this information to infer the signaling system.\n:::\n\nIn many situation where agents share some experience or can consult the same oracle they can infer the same signaling system and avoid the cost of lengthy coordination required to reach a common signaling system. This is the easiest case and the most likely scenario for the evolution of signaling systems.\n\nTwo cases come to mind.\n\n1. They have booth been observing the state space long enough to infer the distribution of states to a high degree of confidence. [**coordinate via the state distribution**]{.column-margin}\n2. They can listen to a third party who knows the distribution and learn to signal from them. [**coordinate by imitation**]{.column-margin}\n3. They can access a state classifier and send it random noise thus deriving an empirical distribution of states in the classifier (not nature) and use it to learn the signaling system. [**coordinate via a classifier**]{.column-margin}\n\nOnce a distribution of states in known it can be used to create huffman codes using 0 and 1. These signals are then ranked.\n\nThere is a distribution of the states of the word known to all players.\n\n-   In the easiest case each state has a different probability of occurring. -It is easiest because all players can infer a `canonical signal system` from such a distribution of states.\n    - They order states and corresponding actions in decreasing expected value. The canonical system is the one mapping between the states and the actions.\n    - Thus the salience distribution breaks the symmetry of all viable signaling systems and leaves just one option.[^1]\n-   In each subsequently harder case there are two or more states with equal probability of occurring. These probabilistic symmetry of these states cannot be broken as before and require the use of coordination. The coordinators can break the symmetry by trial and error when that state arises. Once all the symmetries have been coordinated the players can infer the rest via the canonical signal system from the distribution of states.\n-   In the worst case all states have equal probability of occurring. This is the hardest case because after each state signal pair the problem is still maximally symmetric. The players need to solve this by using trial and error.\n\n[^1]: This is notion of a most salient mapping acts as an optimal policy for agents who need to quickly avoid the long run costs of a non salient signaling system\n\n::: {.callout-tip}\n\n### MARL Formulation\n\nIn terms of the MARL formulation:\n\n- A PMDP has states $S$ and actions $A$. \n    - States are observed by agents of type S whose actions are signals \n    - Actions are performed by agents of type R.\n    - Rewards are assigned symmetrically to both All senders and receivers when the receiver action matches the sender observed state.\n\n- States can be uniformly distributed or be drawn from a distribution. \n- We like to call such a distribution the saliency distribution after Schelling notion of a focal point AKA (Schelling point) in his book The Strategy of Conflict. In a lewis signaling game there are n! signaling systems if there are n states, signals and actions. If the states are uniformly distributed then all signaling systems are equivalent. But if the states probabilities are monotonicaly distributed then there is a unique optimal signaling system which is precisely the Schelling point.\n\n- Since saliency \n:::\n\n## 2. Learning the Saliency distribution.\n\n\n::: {.callout-note}\n\n### Story: Creation of the Oracle of Bayes\n\nIn another tribe where agents are too busy with their routine to coordinate on a signaling system. But they vigilantly observing and tallied thier environment. all the agents sooner or later will record the same empirical distribution of states.  Whenever a state's probability emerges into 'significance' it becomes common knowledge which allows all to order it along with the others and to enumerate with its 'canonical' signal. As the states's distribution evolves over time so does the  signaling system.\n\n:::\n\nAnother point is to consider that if agents just observe states long enough they should eventually learn to approximate the state distribution. How long would this take ?\n\nIf there least common state has probability $\\alpha$ and the agents want to know the distribution with confidence $\\epsilon$ they would need, according to Hoeffdingâ€™s Inequality\n\n$K\\ge\\frac{log(2/\\epsilon)}{2\\alpha^2} \\qquad \\text{(samples to learn S)}$\n\nalso recall that although there is no lower bound on $\\alpha$ when $S\\sim Uniform[N]$ the upper bound is $1/N$\n\n$K\\ge\\frac{N^2log(2/\\epsilon)}{2} \\qquad \\text{(samples to learn uniform S)}$\n\n::: {#upper_bound_estimation .cell execution_count=1}\n``` {.python .cell-code}\nimport math\n\n# Given values\nK = 8 # states\nepsilon = 0.34 # confidence\n\n\n# Calculate time to learn the saliency distribution \n# N using the formula N >= (K^2 * log(2 / epsilon)) / 2\nN = (K**2 * math.log(2 / epsilon)) / 2\nprint(f'Expected time {int(N)} to learn a {K} state distribution with confidence {epsilon}')  \n\n# Expected time to learn a signaling system with N states\n\nT = K * math.log(K)\nprint(f'Expected time {int(T)} to learn a {K} signaling system  ')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExpected time 56 to learn a 8 state distribution with confidence 0.34\nExpected time 16 to learn a 8 signaling system  \n```\n:::\n:::\n\n\nSo learning a signaling systems is easier then learning the distribution of states. Once they they know how to signal states it is easy to use this system to communicate the distribution to all the receivers.\n\nWe have not put a cost on learning the signaling system. But if there was a cost associated with learning we could use it to model when agents would prefer to learn the signaling system or just wait until they can infer the distribution of states and infer they systems from that.\n\n<!-- simulate --> \n\nA third point is that if they are bayesian they could start to infer the signaling system after viewing a few stats and update thier system as they update their beliefs regarding the distribution of states.\n\n<!-- simulate --> \n\n### Bringing Up Baby\n\n::: {.callout-note}\n\n### Story: Bringing Up Baby \n\nHere the sender is tha parent and the receiver the child. Each time the child learn a new action a new signal is added to the signaling system. Since the other signals are known the child can learn the new signal in a single step. This is another trivial case where learning is easy.\n\n:::\n\n### Hoppes Urn \n\n::: {.callout-note}\n\n### Incremental Learning\nIn RL this is called incremental learning. We can also assign such signals to sequences of actions which we call capabilities. The child can learn a new capability in a single step. This is the most efficient way to learn a signaling system incrementally.\n\n:::\n\n\nSkryms discusses two methods that agents can use to learn a signaling system incrementally. First is the Chinese restaurant process and the second is the Hoppe urn. He suggest that they are equivalent. I too came up with the Hoppe urn model - as I had already investigated how to codify the most common probability distributions as urn models.\n\nAnother way to make learning easier is to always have just one action in context when we need to learn. This allows the receiver to learn the signal system in a single step. It might work with a student learning to signal and act in tandem.[**incremental learning with one new action**]{.column-margin}\n\nIn this case urn used in learning have an Hoppe urn with a black stone indicating that a new state action pair is being learned. If the receiver learns the new signal action pair, the agents keep track of it otherwise the new signal and action are discarded.\n\nNote that if the there is only one new state and action a suitable algorithm can learn it immediately. IF there is an exploration - this may cause an error.\n\nWe retain this mechanism and might use it for expanding a signaling systems incrementally in the presence of new data.\n\nNote: if there are saliency distributions is being used a new signal would be the last signal in the saliency distribution or in the last group. Over time signals that are not in use might be discarded if thier saliency is bellow the minimum saliency threshold.\n\n## 3. Ship of Fools\n\n::: {.callout-note}\n\n### Story: Ship of Fools\n\nSenders and Recievers lack all prior knowledge. They follow an optimal strategy for a related game the battle of the sexes.\nIs a state is uncoordinated senders will explore  randomly pick a signal and recievers will randomly pick an action until they get a reward and exclude the signal action pair from exploration.\n\n:::\n\n\n\nThis strategy is not the best one for senders, but it is easier to anlyse.\n\nIf the state is T and there are N states, signals and actions then are $N\\times N$ choices for sender and recievers of which the ones with action A=T get a reward. So the expected reward is 1/N chance of getting a reward. \n\nThe expected rewards are 1/N but since the sender is randomizing each turn is independent. Can they do better?\n\n\n## 3. The steady navigator\n\nIndeed they can do better. If the sender picks a signal and sticks with it the receiver can eliminate an action each turn. This is the optimal strategy for this, the most common setting of the Lewis signaling game.\n\n::: {.callout-note}\n\n### Story: The Steady navigator\n\nSenders and Recievers lack all prior knowledge. For each new state, the sender picks a signal at random but if the state is the same as the last state the sender sticks to the same signal. The receiver must explore an action at random but if the signal is the same as the a previous seen signal the receiver will explore an an untested action for the signal until they get a reward. \n:::\n\nLets estimate the expected rewards under this strategy for a state T and N states, signals and actions.\n\n- Sender has 1 signal and\n- Since the sender sticks with the same signal the receiver can eliminate an action each turn.\n- Receiver has N choices intially with 1 correct choice so we has a expected chance of 1/N of getting a reward.\n- Next he can eliminate his first choice and has N-1 choices with 1 correct choice so we has a expected chance of 1/(N-1) of getting a reward.\n- And after k tries he has N-k+1 choices with 1 correct choice so we has a expected chance of 1/(N-k+1) of getting a reward.\n- In the worst case he will have to try all N actions but\n- The Expected number of steps \n$$\n\\begin{aligned}\n\\mathbb{E}[steps] &= \\sum_{k=1}^{N} \\frac{1}{P_{\\text{success k}}} \\times P_\\text{failure up to k} \\newline\n&= \\sum_{k=1}^{N} \\frac{1}{{N-(k-1)}} \\underbrace{\\times \\prod_{i=1}^{k-1} \\frac{N-i}{N-i+1}}_{\\text{telescopic product}} \\newline\n&= \\sum_{k=1}^{N} \\frac{1}{\\cancel{{N-(k-1)}}} \\times \\frac{\\cancel{{N-(k-1)}}}{N} \\newline\n\\end{aligned}\n$$\n\n\n\n\n::: {.callout-tip}\n\n### MARL Formulation\n\nThis is basicaly an optimistic initialization strategy. The sender does not explore. The reciever intilizes all signal action pairs optimisticaly with value of 0.5.  This way he will keep exploring untill he gets a reward of 1.0 At this point exploration ends.\n\n:::\n\n\nSo we can expect that the number of steps needed to learn to signal the state T is N.\nThey should pick a signal for a state and stick with with it. \n\n\n\n## The Guru's Prior\n\nThe Sender is a privileged elder who knows the distribution of the states, the associated risk and cost of signaling to sender and receiver and figures our the optimal signaling systems. As such he selects a specific signaling system. This means that students need to coordinate to this system.\n\n-   This means that whenever the state $s_i$ arises we will get signal $sig_i=Send(s_i)$ rather then some random signal. This means that the student for a mistake the *receiver* can use a negative reinforcement for $<sig_i,action_j>$ is the return is 0. This should allow the receiver to narrow down the actions chosen for the next time we he gets that signal.\n\nThis is second hardest learning scenario but also most realistic. We don't want to have to learn a new language for every person we meet.\n\nWhat could happen - the distribution of states could evolve over time.\n\n## The prophet's prior\n\nThe sender knows the distribution of the states and how it evolves over time. He choses the currently optimal signaling system. The receivers must learn the signaling system but once a change in the state distribution is observed they will switch to the the new optimal signaling system.\n\nImagine a world with many predators troubling the signaler. To avoid becoming prey agents must send a risky signals to their neighbors. They should use the signaling with the least expected cost. This cost combines the predator risk and its frequency. Signals can be 1 or 0. 1 is risky and 0 is safe. As frequency of the predators change the optimal signaling system will change as well.\n\n## The Gurus' Posterior\n\nHere there are multiple gurus with knowledge of different distribution. Can they coordinate on the most salient signaling system with respect to thier common knowledge ? \n\nThis should be the signaling system that is most salient for a mixture distribution with weight $w_i$ for each guru.\n\nLets perhaps assume that there are a very large N and a cutoff $\\epsilon$ probability for which the gurus won't bother to include rare sates.\n\n\nIn the second setting two or more students must come up with any signaling systems as fast as possible.\n\n\n## Babylon Consensus\n\nMultiple senders and receivers take shelter in common ground and need to arrive at a common signaling system.\n\n1. They can want to learn the least costly signaling system in terms of learning.\n2. They want to learn the most salient signaling system in terms of the distribution of states.\n    3. There is an agent who knows the current distribution of states and the optimal signaling system. \n    4. There isn't such an agent but the senders want to use a \n\n::: {.callout-note}\n\n### Cost of learning a second dialect\n\n\n1. for each agent and for each signal that is different from the target signalaling system add a cost of 1.\n\n$$\nC = \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\delta_{ij} \\\\\n$$ {#eq-cost}\n\nwhere $\\delta_{ij}$ is 1 if the signal $j$ is different from the target signal for state $i$ and 0 otherwise.   \n\n:::\n\n## POMDP\n\nIn this settings one or multiple senders only a partial state. \n\nAgain we consider a hypothetical case where the state describe predators and that it can be partitioned into disjoint parts like <type, proximity> or <type, proximity, number> or <type, proximity, number, direction>. This partioning is also at the basis of compositionality in signaling systems.\n \nSkyryms first considers three different settings.\n\n1. **observation one of mutually exclusive partition:** the case where each sender views one part of the partitioned state.\n2. **observation of all  mutually exclusive partition** the case where senders see all the parts of the state but don't have a mechanism in place to coordinate who sends which part of the state.\n3. **observations of all mutually exclusive partition with coordination** the case where one sender see all the parts of the state but lacks symbols to send the full state and needs to send each part. He must send the parts one at a time resulting in a sequence of signals.\n\nIn the first settings the receiver somehow knows that he should first aggregate the signals using a logical and then decode the state.\n\nIn the first settings \n\n\nwhere the agent again observe the full state but don't have a a coordination mechanism for picking differnt parts of the message.\n\n\nThey send a partial signal to the receiver who must infer the state and take the appropriate action. The receiver must \n\n1. aggregate the messages\n2. infer the state\n3. take the appropriate action\n\nnote:\n\n\nIn the first case so long as each part of the state is a unique signal the state can be infered by the reciever using conjunction.\nThe second case if more problematic and shows us a new way that some signaling systems can be better then others. \n\npart the agent can't infer the state better then chance. However reinforcement of random partition the senders can learn to send  they both need to learn a decorelated partition for each state the state and send different parts of the state. The issues is if the semantics are composeable.\n\n- An issue here is that there is no guarantte that the senders will send the same part of the state at each turn. If the aggregation rules is conjunction, i.e. logical and, then the receiver will be able to decode the state so long as he gets all the pieces.\n\n\n## Bayesian Adversarial Signaling\n\nThere are multiple senders and each state is known to more than one sender.\nEach sender has a voracity parameter $\\nu$, this is the probability that they send a faithful signal. \nAt one extreme senders make small mistakes and at the other they are completely deceptive.\nAt the extreme the agents have types (like knights and knaves) and the receivers must learn to classify the agents by type and then learn the signaling system.\nAgents need to learn a\n\n\n## Babbling Bayesian Babies\n\nBabies in the babbling stage of language development are learning to signal. They are sending all possible phonemes and the parents and thier parents either respond or talk to each other. The babies are collecting the feedback and reinforecing both poitively and negatively until they only use the phonemes that are in the language of thier parents. They start with over 300 phonemes and end up with 40-50. \n\nIn this scenario the sender operates at random. Both the sender and the receiver must observe the rewards and reinfoce state signal action triplets.\n\n\n\n\n\n\n\n---\n\n\n\n## The evolution of signaling systems\n\nIn this section I want to address some of the questions that drive my research on signaling systems.\n\n### When do we expect signaling systems to evolve?\n\nWhen agents fitness is increasingly predicated on coordination or communication they will get a benefit for evolving signaling systems. I.e. a evolutionary pressure to communicate will lead to the evolution of signaling systems.\n\n### What are the main desiderata for signaling systems?\n\n<!-- this section now has it's own file - consider removing/merging-->\n\nHere are some of the main desiderata for signaling systems:\n\n-   **Efficiency** - the signaling system should be as short as possible. \n-   **Salience** - the signaling system should be most salient for the distribution of states.\n-   **Cost** - the signaling system should be as cheap as possible to learn and use.\n-   **Robustness** - the signaling system should be robust to noise and deception.\n-   **Adaptability** - the signaling system should be able to adapt to changes in the distribution of states.\n-   **Compositionality** - the signaling system should be able to be combined with other \n                           RL activities to form\n    - more complex signaling system.\n    - more complex policies.\n\n\n\n\nThis is most clearly illustrated in:\n\n- The **predation scenario** where \n    - Agent's short term survival is predicated on their ability to respond to signals indicating the presence of predators by take the appropriate precautions. Of course signals need a source. \n    - Agents can send a signals for the state they perceive or to stay mute.\n    - Agents can repeat signals they receive or stay mute.\n    - As predation increases, selection pressure may induce signaling systems to evolve.\n- The **Dowery/Courtship scenario** where:\n    - The game can be cooperative or competitive.\n        - In the competitive case only the fittest agents get a mate.\n        - In the cooperative case all agents get to mate but some will mate more often, or with more desirable mates.        \n    - Agent must collect resources (e.g. a bill of goods for a dowery) before they can reproducing from a changing landscape.\n    - Only the top n dowries will generate an offspring. (bills of goods slowly perish but the size and diversity of is important).\n    - Alternatively only the agent that is the the best at courtship n times can generate an offspring. (this time there are smaller bills of good that quickly perish)\n    - Resources are plentiful but evanescent.\n    - Agent that can signal would be able to collect a dowery faster and increase thier fitness.\n    - As competition increases benefits signaling systems should evolve.\n    - This is interesting as the exploration/exploitation dilemma caps the rate at which agents can reproduce. Yet signaling will allow agents to over come this cap. \n    - This is also a case where agents may get a benefit from sending false signals if the receiver is a serious contender. So that the receiver will waste time and resources.\n    - The agents must learn to discriminate \n    To handle deception agents may also develop a model of the mind of the sender to predict the likelihood of deception. They may also want to tally if the sender has been deceptive in the past.\n    - Or \n- The **Knights & Knaves** scenario where:\n    - Agents need to: \n        1. Classify agent by type. (knight or knave, monkey, insane, etc.) to interpret the semantics of their signals.\n        2. Assemble the state from messages with different semantics to recover the state of the world.\n    - This scenario does assumes the agents have an underlying motivation to learn to signal.\n    - And now add a selection pressure on the evolution of basic logic and semantics.\n    \n\n\nAgents that communicate can spend less time exploring and more time exploiting.\n. In this case the agents will evolve a signaling system that is most salient for the distribution of states. This is the most likely scenario for the evolution of signaling systems.\nThe reason why agents might want to learn a signaling system is to maximize their fitness\n\n\n-   What are the main parameters that affect the learning of signaling systems?\n    - state distribution (these are the states of the world and signaling is used to share these states with others to maximize fitness - the expected progeny)\n    - saliency distribution (weights for states ranking thier risk)\n    - voracity of senders.\n    - cost of signaling (risk of predation).\n-   What are the different settings for learning signaling systems?\n\nSome other questions within these contexts might be:\n\n-   What are the number of signaling systems for a given number of states and actions?\n-   What are the number of pooling equilibria for a given number of states and actions?\n    -   Let's break these down by the degeneracy of the pooling equilibrium. This might suggest the minimal number of signals needed in an experiment to learn the signaling system. It might also suggest the thresholds of success for optimal signaling systems in different settings.\n-   Can we estimate the regret for different RL algorithms ?\n    -   What is the expected signaling success for each of the above?\n    -   What is the expected and the mean number of steps to acquire a signaling system for a given number of states and actions under different settings?\n-   How does having more senders or receivers affect the above?\n    -   What is the complexity of n-agents to come up with a common signaling system?\n        -   under full communication\n        -   under partial communication\n-   How does locality affect the time to a universal signaling systems?\n    -   if there is full observability\n    -   if communications are one to one\n    -   if communication are different neighborhood, Von Neuman, Moore, hexagonal, other lattices, chains, rings, random graphs. (need to use optimal dynamics)\n\nAnother question that like a lemma on time needed for an agent to become experienced enough to setup an optimal signaling system?\n\n-   Given distribution S of states with k states and some the rarest state $s'$ having probability $p(s') = \\alpha$ what is the expected number of observations needed for agents to approximate the distribution of states to within some credible interval $\\epsilon<\\alpha$?\n\n-   Note while there is no lower bound on alpha the upper bound is $\\alpha = 1/k$ for a uniform distribution of states. I think this is the Bayesian version of an empirical distribution. This would be a waiting time for becoming experienced.\n\n-   After this waiting time a steady state distribution should be known to all agents.\n\nUnder partial observability the agents need to cooperate to learn the signaling system in a distributed manner. If the agents are on a grid or on a graph what are the bounds on coordination time for learning the signaling system - using a gossip protocol - i.e. each agent can only communicate with its neighbors - using a broadcast protocol - i.e. each agent can communicate with all other agents - using a random walk protocol - i.e. each agent can communicate with a random agent - using a central coordinator - i.e. each agent can communicate with a central coordinator - using an ambassador - i.e. each agent can communicate with an ambassador who can communicate with many other agents per Ramzey's theory\n\nWhile reviewing a paper of this subject I had realized that there are a number of hypothetical scenarios for signaling systems to arise.\n\nIn RL we have different setting for learning optimal strategies. Some of theres different scenarios can be framed in this form.\n\nI wanted to list them here so I can reference them later\n\nBut thinking as I list these I notice that some provide an easy solutions to problems that others don't.\n\nOne point of interest. If the agents are concerned with picking the right action for each state, they should collapse any states which share the same optimal action into a single signal. This will reduce the number of signals they must be learned and reduce the overall message length and cost of signaling. So in reality we should not be overly concerned with the number of actions exceeding the number of states.\n\nWhen there are not enough signals agent need to learn to aggregate signals.\n\n\nadd \n\n1. learning by evolution:\n    - replicator dynamics with\n    - agents have random signaling systems assigned and the systems with most payoffs is selected through population dynamics.\n    - children learn thier parent matrix via sampling.\n        - one parent (perfect and imperfect transmission)\n        - two parents \n    - pidgins via shared dictionaries\n    - creoles shared grammars and dictionaries\n    - adding some mutation - adding mutations to the childrerns signaling system.\n    - based on paper by (Nowak and Krakauer)\n2. learning via reinforcement learning     \n1. spontaneous symmetry breaking scenarios vs planning\n   1. If there are N signals, states and actions is there an advantage to planning a signaling system vs letting it evolve in terms of the number of steps needed to learn the signaling system? \n    - random signaling means that each step is an independent trial. \n     - Sender can send N signals and\n     - Receiver can guess N Actions \n     - So there are N^2 combinations per turn.\n     - So there are Only the ones with A=T get a reward so there are N good combinations. So there is a N/N^2 = 1/N chance of getting a reward. So we can expect that the number of steps needed to learn to signal the state T is N.\n    - planning means that the sender picks one signal and sticks to it. In this case Receiver gets to systematicaly eliminate an action every time.\n    - sender has 1 signal and\n    - receiver can guess N at first and N-1 at second and N-k-1 at kth turn.\n    - So there are n+1/2  \n     actions giving 1*N combinations and only ones with A=T get the payoff. So there is a 1/N chance of getting a reward. So we can expect that the number of steps needed to learn to signal the state T is N.\n    - Thus planning is faster than random signaling.\n    \n    - random signaling means that there are (2n/n*n)^n = 2\n\n    - is agent use positive reinforcement only then \n\n   2. are there conditions where the signaler/reciever gets to detrmines the signaling system?\n     - if Sender sends random signals from L-{coordinated} R must guess the state From L-{coordinated}.\n     - if S wants to switch X and Y ? and does so R get 0 . If R is epsilon greedy he will find the new semantics.\n     - A meta protocol would require a code switching signal be \"Swap X Y\"\n\n1. Source coding scenario errors in encoding & decoding -  based on paper by (Nowak and Krakauer)\n2. errors in the transmission channel  based on paper by (Nowak and Krakauer)\n\n3. risks - there are signals with monotonicaly increasing risk.\n    - payoffs for signals are symmetric\n    - cost associated with the risky signals are borne by the sender \n    - if recievers can respond correctly after getting a partial message they get a bonus.\n    - we can also consider sharing cost and rewards symmetrically.\n--- creating a complex system with compositionality using self play\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}