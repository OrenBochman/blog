{
  "hash": "c8bc5bdcd4d2a5f843f489cab93d80e7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: \"2025-01-05\"\ntitle: The Many Path To A Signaling System\nbibliography: ./bibliography.bib\ncategories: [signaling systems, lewis signaling game, reinforcement learning, bayesian games, information theory, game theory, bayesian reinforcement learning,emergent languages]\nkeywords: [compositionality, partial pooling equilibria, Huffman codes, spontaneous symmetry breaking, Sapir-Whorf hypothesis]\nformat: \n    html: \n        code-fold: true\n---\n\n\n\n\n\nWhile reviewing the literature on Lewis games and thier extensions I realized that there are a many ways that languages can emerge from an interaction between agents. Ofttimes people make assumption that the one they envision is the only one while in reality there are many nuanced ways that signaling systems can arise. \n\n::: {.callout-important}\n\n## TL;DR {.unnumbered}\n\n![Emergent Languages in](/images/in_the_nut_shell_coach_retouched.jpg)\n\nOne fascinating aspect the Lewis signaling game [@lewis1969convention] is that although there are many theoretical equilibria initially the agents will inevitably fail to coordinate and they can only reach the optimal signaling systems after some iterations of the game in which they either evolve or use reinforcement learning to coordinate a themselves to a common signaling strategy. In the prisoners dilemma agents can learn to cooperate if the game is iterated. In the Lewis signaling game agents can learn to coordinate on a signaling system if the game is iterated. \n\n\nGenerally to find a good signaling system requires some kind of algorithm and at least between N and $N^2$ steps as well some number of iterations. I don't recall seeing a discussion of the the minimum or the expected number of iterations required to reach a signaling system under different algorithms. In other words most researchers have considered the complexity of coordination in signaling systems. This is actually a fairly simple problem to solve in the most common settings.\n\n\n:::\n\n\nAnother two point primarily addressed by the evolutionary game theory community who view evolution in terms of replicator dynamics is that of stability of equilibria and the notions of evolutionarily stable strategies. \n\nThe first has to do with convergence of learning to an optimal signaling system. \n\nThe second has to do with the ability of an equilibrium to resist invasion by a mutant strategy.\n\n### Enumerating the different type of signaling systems and the other types of equilibria.\n\nA related issues is that of enumerating different types of equilibriums in larger games. For basic Lewis Signaling games this is not very difficult as there are N! signaling systems in games with N signals and N states, \n\nFor a complex signaling system with N states and M signals we can enumerate the signals as the first N base M numbers. Once again we deal with N! permutations. However the sender may chose any set of base N base M numbers. This creates an potentially unbounded number of signaling systems. This is perhaps a reflection of Wilhelm von Humboldt characterization of  \"infinite use of finite means\" meaning that a language as  systems in which a finite number of symbols can be combined in an unbounded number of ways, c.f. [@von1999humboldt]\n\nThis perhaps makes the complex signaling game special as a game theoretic problem. At least in the sense of what we consider bounded rationality. It is not at all clear what solution concept could be used to create an optimal signaling system, in this case it should require deep insights into group theory, topology, information theory, category theory, probability theory. Also though I consider the problem of equlibria in terms of an enumeration of states via numeric signals, it is does not at all follow that this is the best way to consider the problem. \nIf we use a an alphabet of M phonemes we may for instance run into phonotactic constraints that are not at all present in the numeric representation. Thus another source of complexity may arise in terms of the actual realization of the signaling system.\nThis is perhaps why this needs to be a working paper -- in which new ideas can be added as they come to me.\n\nFor complex signaling systems we need to consider\n\n1. Are the infinite number of signaling systems equivalent up to an isomorphism? I believe that the answer to this is yes by the following rationale. Any signaling system can be viewed as a permutation of signals to states. And according to the Cayley's theorem any groups can be represented as a permutation. Hence we can view any complex signaling system as some group! And groups are equivalent if they are related by a group homomorphism.\nHowever there are still a couple of conundrums to consider. If the prelinguistic objects we call the states have a group structure and this is preserved this seems like a signaling systems that is a faithful representation. But it is also possible that there is a mismatch - that some of the structure is lost or that some additional structure in the language is added that is not in the \noriginal pre linguistic objects. I think that some of these might be viewed as happy accident while others may be failures in therms of signaling systems.\n2. Can the Lewis signaling game together with the pre-linguistic object imbue the language with semantics ?\n3. At what point can we view signaling systems universal in terms of the Sapir-Whorf hypothesis. I.e. when does the semantics of the signaling system becomes capable of representing the semantics of any natural language?\n\n\n\nIT is entirely possible to \n\n\n\nAnother point of interest to me is to consider the emergence of grammer and of a morphology. In [@Nowak1999] The authors give a result for the emergence of garammar in a signaling system. This is that there are many more \n\nI think it worth while to list them in this space --- particularly as I believe that signaling systems are a key for transfer learning in reinforcement learning which together with learning to represent complex states may be the key to AGI.\n\n\n## Introduction\n\n- Listing number of different scenarios on how signaling systems can arise in the Lewis signaling games. \n- I will start with a story \n- Next add some details like some variants and look some basic analysis. \n- Finally I'll try to place it into the context of MARL. Note that we will be dealing with partially observed multi agent RL. But each scenario can have a different setting.\n\n![lewis signaling game](./lewis_extensive_form.svg){.center}\n\nIn The book signals [@skyrms2010signals] the author, Skryms, discusses how Lewis challenged the skepticism of his advisor Quine regarding the  meaning and convention may arise via an arbitrary mechanism like symmetry breaking.\n\nWhen I considered solving some additional issues surrounding the fundamentals of signaling systems I realized that I had a few different scenarios in mind and that writing them down with some semblance of formalism might be helpful. It turns out that indeed this turns out to be a stepping stone towards developing an optimal algorithms for learning signaling system in different rl settings.\n\nLet's face it under different settings the task of acquiring a signaling system can be easier or harder. In [@skyrms2010signals] the author points out that at symmetry breaking all the different signaling systems that could be learned are equivalent. However if there is an asymmetry in the form of a non-uniform distribution of states or different signaling risks then we we might prefer some signaling systems over others and there might even be a unique optimal signaling system. Furthermore like in reality one would expect that with time distributions of states might change and the optimal signaling system might change as well.\n\n## 1. The Oracle of Saliency\n\n\n::: {.callout-note}\n\n### Story: The Oracle of Saliency\n\nSender and Receivers consult an \"Oracle\" (perhaps in book form). The oracle tells them how to map states to action, the oracle provides the sender with a one to one mapping of states to actions and to the receiver with the transpose, a mapping of signals to actions. The sender and receiver can then use this information to infer the signaling system.\n:::\n\nIn many situation where agents share some experience or can consult the same oracle they can infer the same signaling system and avoid the cost of lengthy coordination required to reach a common signaling system. This is the easiest case and the most likely scenario for the evolution of signaling systems.\n\nTwo cases come to mind.\n\n1. They have booth been observing the state space long enough to infer the distribution of states to a high degree of confidence. [**coordinate via the state distribution**]{.column-margin}\n2. They can listen to a third party who knows the distribution and learn to signal from them. [**coordinate by imitation**]{.column-margin}\n3. They can access a state classifier and send it random noise thus deriving an empirical distribution of states in the classifier (not nature) and use it to learn the signaling system. [**coordinate via a classifier**]{.column-margin}\n\nOnce a distribution of states in known it can be used to create huffman codes using 0 and 1. These signals are then ranked.\n\nThere is a distribution of the states of the word known to all players.\n\n-   In the easiest case each state has a different probability of occurring. -It is easiest because all players can infer a `canonical signal system` from such a distribution of states.\n    - They order states and corresponding actions in decreasing expected value. The canonical system is the one mapping between the states and the actions.\n    - Thus the salience distribution breaks the symmetry of all viable signaling systems and leaves just one option.[^1]\n-   In each subsequently harder case there are two or more states with equal probability of occurring. These probabilistic symmetry of these states cannot be broken as before and require the use of coordination. The coordinators can break the symmetry by trial and error when that state arises. Once all the symmetries have been coordinated the players can infer the rest via the canonical signal system from the distribution of states.\n-   In the worst case all states have equal probability of occurring. This is the hardest case because after each state signal pair the problem is still maximally symmetric. The players need to solve this by using trial and error.\n\n[^1]: This is notion of a most salient mapping acts as an optimal policy for agents who need to quickly avoid the long run costs of a non salient signaling system\n\n::: {.callout-tip}\n\n### MARL Formulation\n\nIn terms of the MARL formulation:\n\n- A PMDP has states $S$ and actions $A$. \n    - States are observed by agents of type S whose actions are signals \n    - Actions are performed by agents of type R.\n    - Rewards are assigned symmetrically to both All senders and receivers when the receiver action matches the sender observed state.\n\n- States can be uniformly distributed or be drawn from a distribution. \n- We like to call such a distribution the saliency distribution after Schelling notion of a focal point AKA (Schelling point) in his book The Strategy of Conflict. In a lewis signaling game there are n! signaling systems if there are n states, signals and actions. If the states are uniformly distributed then all signaling systems are equivalent. But if the states probabilities are monotonicaly distributed then there is a unique optimal signaling system which is precisely the Schelling point.\n\n- Since saliency \n:::\n\n## 2. Learning the Saliency distribution.\n\n\n::: {.callout-note}\n\n### Story: Creation of the Oracle of Bayes\n\nIn another tribe where agents are too busy with their routine to coordinate on a signaling system. But they vigilantly observing and tallied thier environment. all the agents sooner or later will record the same empirical distribution of states.  Whenever a state's probability emerges into 'significance' it becomes common knowledge which allows all to order it along with the others and to enumerate with its 'canonical' signal. As the states's distribution evolves over time so does the  signaling system.\n\n:::\n\nAnother point is to consider that if agents just observe states long enough they should eventually learn to approximate the state distribution. How long would this take ?\n\nIf there least common state has probability $\\alpha$ and the agents want to know the distribution with confidence $\\epsilon$ they would need, according to Hoeffdingâ€™s Inequality\n\n$K\\ge\\frac{log(2/\\epsilon)}{2\\alpha^2} \\qquad \\text{(samples to learn S)}$\n\nalso recall that although there is no lower bound on $\\alpha$ when $S\\sim Uniform[N]$ the upper bound is $1/N$\n\n$K\\ge\\frac{N^2log(2/\\epsilon)}{2} \\qquad \\text{(samples to learn uniform S)}$\n\n::: {#upper_bound_estimation .cell execution_count=2}\n``` {.python .cell-code}\nimport math\n\n# Given values\nK = 8 # states\nepsilon = 0.34 # confidence\n\n\n# Calculate time to learn the saliency distribution \n# N using the formula N >= (K^2 * log(2 / epsilon)) / 2\nN = (K**2 * math.log(2 / epsilon)) / 2\nprint(f'Expected time {int(N)} to learn a {K} state distribution with confidence {epsilon}')  \n\n# Expected time to learn a signaling system with N states\n\nT = K * math.log(K)\nprint(f'Expected time {int(T)} to learn a {K} signaling system  ')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExpected time 56 to learn a 8 state distribution with confidence 0.34\nExpected time 16 to learn a 8 signaling system  \n```\n:::\n:::\n\n\nSo learning a signaling systems is easier then learning the distribution of states. Once they they know how to signal states it is easy to use this system to communicate the distribution to all the receivers.\n\nWe have not put a cost on learning the signaling system. But if there was a cost associated with learning we could use it to model when agents would prefer to learn the signaling system or just wait until they can infer the distribution of states and infer they systems from that.\n\n<!-- simulate --> \n\nA third point is that if they are bayesian they could start to infer the signaling system after viewing a few stats and update thier system as they update their beliefs regarding the distribution of states.\n\n<!-- simulate --> \n\n### Bringing Up Baby\n\n::: {.callout-note}\n\n### Story: Bringing Up Baby \n\nHere the sender is tha parent and the receiver the child. Each time the child learn a new action a new signal is added to the signaling system. Since the other signals are known the child can learn the new signal in a single step. This is another trivial case where learning is easy.\n\n:::\n\n### Hoppes Urn \n\n::: {.callout-note}\n\n### Incremental Learning\nIn RL this is called incremental learning. We can also assign such signals to sequences of actions which we call capabilities. The child can learn a new capability in a single step. This is the most efficient way to learn a signaling system incrementally.\n\n:::\n\n\nSkryms discusses two methods that agents can use to learn a signaling system incrementally. First is the Chinese restaurant process and the second is the Hoppe urn. He suggest that they are equivalent. I too came up with the Hoppe urn model - as I had already investigated how to codify the most common probability distributions as urn models.\n\nAnother way to make learning easier is to always have just one action in context when we need to learn. This allows the receiver to learn the signal system in a single step. It might work with a student learning to signal and act in tandem.[**incremental learning with one new action**]{.column-margin}\n\nIn this case urn used in learning have an Hoppe urn with a black stone indicating that a new state action pair is being learned. If the receiver learns the new signal action pair, the agents keep track of it otherwise the new signal and action are discarded.\n\nNote that if the there is only one new state and action a suitable algorithm can learn it immediately. IF there is an exploration - this may cause an error.\n\nWe retain this mechanism and might use it for expanding a signaling systems incrementally in the presence of new data.\n\nNote: if there are saliency distributions is being used a new signal would be the last signal in the saliency distribution or in the last group. Over time signals that are not in use might be discarded if thier saliency is bellow the minimum saliency threshold.\n\n## 3. Ship of Fools\n\n::: {.callout-note}\n\n### Story: Ship of Fools\n\nSenders and Recievers lack all prior knowledge. They follow an optimal strategy for a related game the battle of the sexes.\nIs a state is uncoordinated senders will explore  randomly pick a signal and recievers will randomly pick an action until they get a reward and exclude the signal action pair from exploration.\n\n:::\n\n\n\nThis strategy is not the best one for senders, but it is easier to anlyse.\n\nIf the state is T and there are N states, signals and actions then are $N\\times N$ choices for sender and recievers of which the ones with action A=T get a reward. So the expected reward is 1/N chance of getting a reward. \n\nThe expected rewards are 1/N but since the sender is randomizing each turn is independent. Can they do better?\n\n\n## 3. The steady navigator\n\nIndeed they can do better. If the sender picks a signal and sticks with it the receiver can eliminate an action each turn. This is the optimal strategy for this, the most common setting of the Lewis signaling game.\n\n::: {.callout-note}\n\n### Story: The Steady navigator\n\nSenders and Receivers lack all prior knowledge. For each new state, the sender picks a signal at random but if the state is the same as the last state the sender sticks to the same signal. The receiver must explore an action at random but if the signal is the same as the a previous seen signal the receiver will explore an an untested action for the signal until they get a reward. \n:::\n\nLets estimate the expected rewards under this strategy for a state T and N states, signals and actions.\n\n- Sender has 1 signal and\n- Since the sender sticks with the same signal the receiver can eliminate an action each turn.\n- Receiver has N choices initially with 1 correct choice so we has a expected chance of 1/N of getting a reward.\n- Next he can eliminate his first choice and has N-1 choices with 1 correct choice so we has a expected chance of 1/(N-1) of getting a reward.\n- And after k tries he has N-k+1 choices with 1 correct choice so we has a expected chance of 1/(N-k+1) of getting a reward.\n- In the worst case he will have to try all N actions but\n- The Expected number of steps \n$$\n\\begin{aligned}\n\\mathbb{E}[steps] &= \\sum_{k=1}^{N} \\frac{1}{P_{\\text{success k}}} \\times P_\\text{failure up to k} \\newline\n&= \\sum_{k=1}^{N} \\frac{1}{{N-(k-1)}} \\underbrace{\\times \\prod_{i=1}^{k-1} \\frac{N-i}{N-i+1}}_{\\text{telescopic product}} \\newline\n&= \\sum_{k=1}^{N} \\frac{1}{\\cancel{{N-(k-1)}}} \\times \\frac{\\cancel{{N-(k-1)}}}{N} \\newline\n\\end{aligned}\n$$\n\n\n\n\n::: {.callout-tip}\n\n### MARL Formulation\n\nThis is basicaly an optimistic initialization strategy. The sender does not explore. The reciever intilizes all signal action pairs optimisticaly with value of 0.5.  This way he will keep exploring untill he gets a reward of 1.0 At this point exploration ends.\n\n:::\n\n\nSo we can expect that the number of steps needed to learn to signal the state T is N.\nThey should pick a signal for a state and stick with with it. \n\n\n\n## The Guru's Prior\n\nThe Sender is a privileged elder who knows the distribution of the states, the associated risk and cost of signaling to sender and receiver and figures our the optimal signaling systems. As such he selects a specific signaling system. This means that students need to coordinate to this system.\n\n-   This means that whenever the state $s_i$ arises we will get signal $sig_i=Send(s_i)$ rather then some random signal. This means that the student for a mistake the *receiver* can use a negative reinforcement for $<sig_i,action_j>$ is the return is 0. This should allow the receiver to narrow down the actions chosen for the next time we he gets that signal.\n\nThis is second hardest learning scenario but also most realistic. We don't want to have to learn a new language for every person we meet.\n\nWhat could happen - the distribution of states could evolve over time.\n\n## The prophet's prior\n\nThe sender knows the distribution of the states and how it evolves over time. He choses the currently optimal signaling system. The receivers must learn the signaling system but once a change in the state distribution is observed they will switch to the the new optimal signaling system.\n\nImagine a world with many predators troubling the signaler. To avoid becoming prey agents must send a risky signals to their neighbors. They should use the signaling with the least expected cost. This cost combines the predator risk and its frequency. Signals can be 1 or 0. 1 is risky and 0 is safe. As frequency of the predators change the optimal signaling system will change as well.\n\n## The Gurus' Posterior\n\nHere there are multiple gurus with knowledge of different distribution. Can they coordinate on the most salient signaling system with respect to thier common knowledge ? \n\nThis should be the signaling system that is most salient for a mixture distribution with weight $w_i$ for each guru.\n\nLets perhaps assume that there are a very large N and a cutoff $\\epsilon$ probability for which the gurus won't bother to include rare sates.\n\n\nIn the second setting two or more students must come up with any signaling systems as fast as possible.\n\n\n## Babylon Consensus\n\nMultiple senders and receivers take shelter in common ground and need to arrive at a common signaling system.\n\n1. They can want to learn the least costly signaling system in terms of learning.\n2. They want to learn the most salient signaling system in terms of the distribution of states.\n    3. There is an agent who knows the current distribution of states and the optimal signaling system. \n    4. There isn't such an agent but the senders want to use a \n\n::: {.callout-note}\n\n### Cost of learning a second dialect\n\n\n1. for each agent and for each signal that is different from the target signalaling system add a cost of 1.\n\n$$\nC = \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\delta_{ij} \\\\\n$$ {#eq-cost}\n\nwhere $\\delta_{ij}$ is 1 if the signal $j$ is different from the target signal for state $i$ and 0 otherwise.   \n\n:::\n\n## POMDP\n\nIn this settings one or multiple senders only a partial state. \n\nAgain we consider a hypothetical case where the state describe predators and that it can be partitioned into disjoint parts like <type, proximity> or <type, proximity, number> or <type, proximity, number, direction>. This partioning is also at the basis of compositionality in signaling systems.\n \nSkyryms first considers three different settings.\n\n1. **observation one of mutually exclusive partition:** the case where each sender views one part of the partitioned state.\n2. **observation of all  mutually exclusive partition** the case where senders see all the parts of the state but don't have a mechanism in place to coordinate who sends which part of the state.\n3. **observations of all mutually exclusive partition with coordination** the case where one sender see all the parts of the state but lacks symbols to send the full state and needs to send each part. He must send the parts one at a time resulting in a sequence of signals.\n\nIn the first settings the receiver somehow knows that he should first aggregate the signals using a logical and then decode the state.\n\nIn the first settings \n\n\nwhere the agent again observe the full state but don't have a a coordination mechanism for picking differnt parts of the message.\n\n\nThey send a partial signal to the receiver who must infer the state and take the appropriate action. The receiver must \n\n1. aggregate the messages\n2. infer the state\n3. take the appropriate action\n\nnote:\n\n\nIn the first case so long as each part of the state is a unique signal the state can be infered by the reciever using conjunction.\nThe second case if more problematic and shows us a new way that some signaling systems can be better then others. \n\npart the agent can't infer the state better then chance. However reinforcement of random partition the senders can learn to send  they both need to learn a decorelated partition for each state the state and send different parts of the state. The issues is if the semantics are composeable.\n\n- An issue here is that there is no guarantte that the senders will send the same part of the state at each turn. If the aggregation rules is conjunction, i.e. logical and, then the receiver will be able to decode the state so long as he gets all the pieces.\n\n\n## Bayesian Adversarial Signaling\n\nThere are multiple senders and each state is known to more than one sender.\nEach sender has a voracity parameter $\\nu$, this is the probability that they send a faithful signal. \nAt one extreme senders make small mistakes and at the other they are completely deceptive.\nAt the extreme the agents have types (like knights and knaves) and the receivers must learn to classify the agents by type and then learn the signaling system.\nAgents need to learn a\n\n\n## Babbling Bayesian Babies\n\nBabies in the babbling stage of language development are learning to signal. They are sending all possible phonemes and the parents and thier parents either respond or talk to each other. The babies are collecting the feedback and reinforecing both poitively and negatively until they only use the phonemes that are in the language of thier parents. They start with over 300 phonemes and end up with 40-50. \n\nIn this scenario the sender operates at random. Both the sender and the receiver must observe the rewards and reinfoce state signal action triplets.\n\n\n\n\n\n\n\n---\n\n\n\n## The evolution of signaling systems\n\nIn this section I want to address some of the questions that drive my research on signaling systems.\n\n### When do we expect signaling systems to evolve?\n\nWhen agents fitness is increasingly predicated on coordination or communication they will get a benefit for evolving signaling systems. I.e. a evolutionary pressure to communicate will lead to the evolution of signaling systems.\n\n### What are the main desiderata for signaling systems?\n\n<!-- this section now has it's own file - consider removing/merging-->\n\nHere are some of the main desiderata for signaling systems:\n\n-   **Efficiency** - the signaling system should be as short as possible. \n-   **Salience** - the signaling system should be most salient for the distribution of states.\n-   **Cost** - the signaling system should be as cheap as possible to learn and use.\n-   **Robustness** - the signaling system should be robust to noise and deception.\n-   **Adaptability** - the signaling system should be able to adapt to changes in the distribution of states.\n-   **Compositionality** - the signaling system should be able to be combined with other \n                           RL activities to form\n    - more complex signaling system.\n    - more complex policies.\n\n\n\n\nThis is most clearly illustrated in:\n\n- The **predation scenario** where \n    - Agent's short term survival is predicated on their ability to respond to signals indicating the presence of predators by take the appropriate precautions. Of course signals need a source. \n    - Agents can send a signals for the state they perceive or to stay mute.\n    - Agents can repeat signals they receive or stay mute.\n    - As predation increases, selection pressure may induce signaling systems to evolve.\n- The **Dowery/Courtship scenario** where:\n    - The game can be cooperative or competitive.\n        - In the competitive case only the fittest agents get a mate.\n        - In the cooperative case all agents get to mate but some will mate more often, or with more desirable mates.        \n    - Agent must collect resources (e.g. a bill of goods for a dowery) before they can reproducing from a changing landscape.\n    - Only the top n dowries will generate an offspring. (bills of goods slowly perish but the size and diversity of is important).\n    - Alternatively only the agent that is the the best at courtship n times can generate an offspring. (this time there are smaller bills of good that quickly perish)\n    - Resources are plentiful but evanescent.\n    - Agent that can signal would be able to collect a dowery faster and increase thier fitness.\n    - As competition increases benefits signaling systems should evolve.\n    - This is interesting as the exploration/exploitation dilemma caps the rate at which agents can reproduce. Yet signaling will allow agents to over come this cap. \n    - This is also a case where agents may get a benefit from sending false signals if the receiver is a serious contender. So that the receiver will waste time and resources.\n    - The agents must learn to discriminate \n    To handle deception agents may also develop a model of the mind of the sender to predict the likelihood of deception. They may also want to tally if the sender has been deceptive in the past.\n    - Or \n- The **Knights & Knaves** scenario where:\n    - Agents need to: \n        1. Classify agent by type. (knight or knave, monkey, insane, etc.) to interpret the semantics of their signals.\n        2. Assemble the state from messages with different semantics to recover the state of the world.\n    - This scenario does assumes the agents have an underlying motivation to learn to signal.\n    - And now add a selection pressure on the evolution of basic logic and semantics.\n    \n\n\nAgents that communicate can spend less time exploring and more time exploiting.\n. In this case the agents will evolve a signaling system that is most salient for the distribution of states. This is the most likely scenario for the evolution of signaling systems.\nThe reason why agents might want to learn a signaling system is to maximize their fitness\n\n\n-   What are the main parameters that affect the learning of signaling systems?\n    - state distribution (these are the states of the world and signaling is used to share these states with others to maximize fitness - the expected progeny)\n    - saliency distribution (weights for states ranking thier risk)\n    - voracity of senders.\n    - cost of signaling (risk of predation).\n-   What are the different settings for learning signaling systems?\n\nSome other questions within these contexts might be:\n\n-   What are the number of signaling systems for a given number of states and actions?\n-   What are the number of pooling equilibria for a given number of states and actions?\n    -   Let's break these down by the degeneracy of the pooling equilibrium. This might suggest the minimal number of signals needed in an experiment to learn the signaling system. It might also suggest the thresholds of success for optimal signaling systems in different settings.\n-   Can we estimate the regret for different RL algorithms ?\n    -   What is the expected signaling success for each of the above?\n    -   What is the expected and the mean number of steps to acquire a signaling system for a given number of states and actions under different settings?\n-   How does having more senders or receivers affect the above?\n    -   What is the complexity of n-agents to come up with a common signaling system?\n        -   under full communication\n        -   under partial communication\n-   How does locality affect the time to a universal signaling systems?\n    -   if there is full observability\n    -   if communications are one to one\n    -   if communication are different neighborhood, Von Neuman, Moore, hexagonal, other lattices, chains, rings, random graphs. (need to use optimal dynamics)\n\nAnother question that like a lemma on time needed for an agent to become experienced enough to setup an optimal signaling system?\n\n-   Given distribution S of states with k states and some the rarest state $s'$ having probability $p(s') = \\alpha$ what is the expected number of observations needed for agents to approximate the distribution of states to within some credible interval $\\epsilon<\\alpha$?\n\n-   Note while there is no lower bound on alpha the upper bound is $\\alpha = 1/k$ for a uniform distribution of states. I think this is the Bayesian version of an empirical distribution. This would be a waiting time for becoming experienced.\n\n-   After this waiting time a steady state distribution should be known to all agents.\n\nUnder partial observability the agents need to cooperate to learn the signaling system in a distributed manner. If the agents are on a grid or on a graph what are the bounds on coordination time for learning the signaling system - using a gossip protocol - i.e. each agent can only communicate with its neighbors - using a broadcast protocol - i.e. each agent can communicate with all other agents - using a random walk protocol - i.e. each agent can communicate with a random agent - using a central coordinator - i.e. each agent can communicate with a central coordinator - using an ambassador - i.e. each agent can communicate with an ambassador who can communicate with many other agents per Ramzey's theory\n\nWhile reviewing a paper of this subject I had realized that there are a number of hypothetical scenarios for signaling systems to arise.\n\nIn RL we have different setting for learning optimal strategies. Some of theres different scenarios can be framed in this form.\n\nI wanted to list them here so I can reference them later\n\nBut thinking as I list these I notice that some provide an easy solutions to problems that others don't.\n\nOne point of interest. If the agents are concerned with picking the right action for each state, they should collapse any states which share the same optimal action into a single signal. This will reduce the number of signals they must be learned and reduce the overall message length and cost of signaling. So in reality we should not be overly concerned with the number of actions exceeding the number of states.\n\nWhen there are not enough signals agent need to learn to aggregate signals.\n\n\nadd \n\n1. learning by evolution:\n    - replicator dynamics with\n    - agents have random signaling systems assigned and the systems with most payoffs is selected through population dynamics.\n    - children learn thier parent matrix via sampling.\n        - one parent (perfect and imperfect transmission)\n        - two parents \n    - pidgins via shared dictionaries\n    - creoles shared grammars and dictionaries\n    - adding some mutation - adding mutations to the childrerns signaling system.\n    - based on paper by (Nowak and Krakauer)\n2. learning via reinforcement learning     \n1. spontaneous symmetry breaking scenarios vs planning\n   1. If there are N signals, states and actions is there an advantage to planning a signaling system vs letting it evolve in terms of the number of steps needed to learn the signaling system? \n    - random signaling means that each step is an independent trial. \n     - Sender can send N signals and\n     - Receiver can guess N Actions \n     - So there are N^2 combinations per turn.\n     - So there are Only the ones with A=T get a reward so there are N good combinations. So there is a N/N^2 = 1/N chance of getting a reward. So we can expect that the number of steps needed to learn to signal the state T is N.\n    - planning means that the sender picks one signal and sticks to it. In this case Receiver gets to systematicaly eliminate an action every time.\n    - sender has 1 signal and\n    - receiver can guess N at first and N-1 at second and N-k-1 at kth turn.\n    - So there are n+1/2  \n     actions giving 1*N combinations and only ones with A=T get the payoff. So there is a 1/N chance of getting a reward. So we can expect that the number of steps needed to learn to signal the state T is N.\n    - Thus planning is faster than random signaling.\n    \n    - random signaling means that there are (2n/n*n)^n = 2\n\n    - is agent use positive reinforcement only then \n\n   2. are there conditions where the signaler/reciever gets to detrmines the signaling system?\n     - if Sender sends random signals from L-{coordinated} R must guess the state From L-{coordinated}.\n     - if S wants to switch X and Y ? and does so R get 0 . If R is epsilon greedy he will find the new semantics.\n     - A meta protocol would require a code switching signal be \"Swap X Y\"\n\n1. Source coding scenario errors in encoding & decoding -  based on paper by (Nowak and Krakauer)\n2. errors in the transmission channel  based on paper by (Nowak and Krakauer)\n\n3. risks - there are signals with monotonically increasing risk.\n    - payoffs for signals are symmetric\n    - cost associated with the risky signals are borne by the sender \n    - if receivers can respond correctly after getting a partial message they get a bonus.\n    - we can also consider sharing cost and rewards symmetrically.\n--- creating a complex system with compositionality using self play\n\n\n### The Bayesian view of the signaling systems\n\nIn this section let's consider a view of the Lewis signaling game in terms of a bayesian game theory. \nThis is a perspective I used in a article on planning in the complex signaling game and helped me think more precisely about the \nhow a signaling system might evolve in a formal setting.\n\nThe game has n states and n signals. Internality the a the agents will learn a permutation of the states and signals and its inversion. \nSo there are n! signaling systems that can be learned. In the world of bayesian agents each such permutation characterizes an agent type.\nTHe game starts by nature picking a type for the agent. I say this this is because the agent needs to define a strategy which is a response for each state! Now the same is true for the receiver. The receiver's strategy is to pick an action for each signal. After that it can use bayesian updating to update it's belief about the type of the sender. These probabilities can guide it in the process of learning the signaling system. As the pair make progress, the receiver is able to update it's belief about the sender's type, discarding options that are inconsistent with the signals it has received! Once it has finds n-1 signals it can be certain about the sender's type and it will have an expected payoff of 1.\n\n## Framing games and evolution of domain specific languages\n\nIn this scenario we considers if learning a shared language could be a game changer in some strategic interaction like a social dilemma. We may ask be able to not only interpret different signaling system as embodying different semantics derived from the framing game but also consider if these create linguistic relativism where that agent's languages shapes thier perception of the framing game by allowing them to develop newer strategies. We can also consider the beginning of ethics in such a system by considering if the introduction of language allows increases or decreases the overall welfare of the agents.\n\n::: {.callout-note}\n\n### Story: Framing Games\n\n\nAgents tasked with maximizing a reward signal under conditions of strategic interaction. We call this the framing game and it might be as simple as a 2x2 matrix game like the battle of the sexes, a social language like the iterated prisoner's dilemma or as complex as a Sugarscape simulation . At some point, perhaps at the start send and received actions are introduced into the agents action space. They can now assume the role of the sender and receiver in a Lewis Signaling game.\n\nIf the incentives are right e.g. the framing game is cooperative they could learn to signal to each other. Note though that it is conceivable that agents could learn to signal if they are in a competitive game if they are sufficiently driven to explore the send and receive actions. However the resulting equilibrium might not be a perfectly separating one if the agents are not suitably incentivize to use the language to coordinate.\n\nFurthermore these signals may then be incorporated into the planning and allow the agents to coordinate on the framing game. \n\nOne expects that the language that arises under such circumstance would be limited to the domain of the framing game and that its semantics would be inherited from the framing game. However larger framing games with many generation of agents might lead to dynamics that lead to the emergence of a more general language.\n:::\n\nThis kind of scenario actually contains a rich set of paths to the emergence of many different languages. For agents in the lifelong settings the emergent language might gain additional strata of semantics from multiple domain and then evolve to a more general language. \n\nA questions then arises what simple framing games can lead to agents to develop languages that are imbued with sufficiently rich semantics that the language has the Sapir-Worf property of being able to express semantics from any other language.\n\n\nAnother idea I have been using liberally in my thinking is that of a framing game for the lewis signaling game. This idea comes from the field of Multi agent  Reinforcement learning however it should also be valid in terms of Game theory.\n\nSimply put an agent may be tasked some general problem like playing chess or solving a maze. In the past I worked on wizards that configure servers or home networks for telecoms. \n\nI could envision an RL agent learning to do these job by learning from experience. However it seems that if it can play a lewis game and learn a signaling system that is a subset of  english that approximate its domain then it can chat with people rather then relay on a some user interface. \n\nFor a home networks it might need a smaller subset of english and for a server configuration it might larger one. What seems to be the point is that the agent tasked with some external task \nmight be able to learn a signaling system that has semantics inherited from the task. If such task is a strategic interaction we may view it as a game. And together we can view the framing game and the lewis signaling game as single iterated game in which the agent learns to play a new variant of the framing game in which it has access to a coordination mechanism that is a domain specific language.\n\nI think that we if we naively combine a game like the battle of the sexes a pure coordination problem with the lewis signaling game the agents will learn a language like 'football', 'opera'. And that these can arise within three iterations and allow the agents to then coordinate on the battle of the sexes so as to score the highest payoffs. This could happen both if the agents alternate signaling or one always gets to signal first, and always picks opera. \n\nOn the other hand with iterated prisoners dilemma signaling might not make a difference as the language may not be able to change the payoffs sufficiently to make the agents act any differently. In this case it is entirely possible that a signaling system will not arise at all regardless of what the agents say they will act in their own best interest. This leads to a completely pooling equilibrium.\n\nSo the question that comes to mind is this -\n1. can we setup up the signaling game so that the agents will always learn a signaling system if coordination is a benefit.\n2. How can we encode the signaling system so that its prelinguistic object will be the states of the world and the actions of the agents. I.e. we want them to be able to talk about the outcomes of the framing game in the signaling game.\n3. We want cost and benefits of signaling to be decoupled from the framing game - i.e. we may deduct the payoff for signaling success once the signaling system is learned.\n4. We do want the agents in the framing game to aware of the outcome of the signaling game.\n5. Finally we want to identify if there are strategies in which coordination increase of decrease overall welfare.\n\ne.g. in the battle of the sexes we should expect perfect rewards\ne.g. in a three way traffic junction game we might expect the agents to signal thier intentions to turn left or right and to go straight. This would allow them to avoid accidents. One such mechanism might be a game of paper rock scissors to determine the priority of the agents.\ne.g. in the [Braess's Paradox](https://en.wikipedia.org/wiki/Braess%27s_paradox) establishment a high way though a city we might end up increasing the traffic jams. \n\n\n## Co-adaptaion and Semantic Drift and grounding.\n\nc.f. [@rita2022emergent] paper of co-adaptation loss!\nc.f. [@meta2022human] solving the game of diplomacy\nc.f. [@barrett2017self] on templates by skryms\n\n\n::: {.callout-note}\n\n### Story: Co-adaption\n\nFamily members, best friends and members of closely knit societies tend to develop a language that is unique to them. IT can start with in jokes, invented words and phrases and co-opting the meaning of existing words to mean something else. This is a form of co-adaption where the language and the society co-evolve. If allowed to evolve the language can drift so that a stranger would be at odds to understand what the speakers are saying and this is called semantic drift. \n\nFor RL agents it is possible for them to develop a language that is unique to them as suggested above. It is also possible that as conditions change e.g. the framing game is switched from Battle of the sexes to Prisoner's dilemma the languages will remain a 4 state 4 signal language but the meaning of the signals will drift.\n\n:::\n\n\nthis story is more about something we might want to avoid.\n\n1. having more agents should reduce co-adaption.\n2. Semantic drift is inherent in the evolution of language. However we may want to allow the language to evolve but for certain aspects to remain fixed. This is one of the desiderata for emergent languages. What we would prefer that grammar and much of to be stable over multiple generation so that great grandfathers can still communicate with thier great grand children. Why is this a problem? In agentic systems we design language emergence to be fast. In most cases every agents need to learn it from scratch, they enjoy the benefits of perfect recall and a noiseless channel. This also means that languages might change very quickly to and that we as researchers will have a tough time understanding the agents over the course of thier simulation. However in natural languages we have a similar situation and for language to work for large populations and for record to make sense for thousands of years we want much of the language to be stable with possiblies for evolution at the fringes....\n3. How do we ensure semantics persist over time ? We call this is the idea of grounding. Imagine all the most important ideas were written down in a book and that book was passed down from generation to generation. Everyone might need to learn the book a little after they learned basic language skill in school. soon the book becomes cannon and no one may change it. Over time though it might be permitted to add bits when new concepts were discovered and proved important enough to preserve.\n4. Short of starting a religion for our agents we may want good mechanism that will keep the language grounded so that cooadaption and semantic drift are kept in check.\n\n5. Another point here is that if our agents are aware that the framing game has been swapped from battle of the sexes to Prisoners dilemma that they may want to keep their semantics for the Battle of the sexes intact and use them as a template or prior for the Prisoners dilemma. Since prisoners dilemma is non-cooperative, there may not even be a perfectly separating equilibrium for framing game so that assigning a language from a template prior might actually be of benefit. .\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}