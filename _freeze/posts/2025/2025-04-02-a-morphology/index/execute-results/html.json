{
  "hash": "9c696380c54df988fce2a2b6fb44fb2c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: 2025-04-02\ntitle: \"Base line Morphology Model\" \ncategories: ['code']\n---\n\n\n\n\n\nSo in this note I'd like to create a vanilla implementation of a morphology and syntax that might be used as a inductive bias for the emergent language.\n\n## Morphology\n\nLet's:\n\n1. [x] code a regular morphology for the emergent language.\n1. [ ] look at using it as a part of the inductive bias for the emergent language.\n1. [x] add inputs:\n    - part of speech\n        - open\n        - closed\n    - nouns\n    - cases\n    - tenses\n    - aspects\n    - moods\n1. [x] export for use with [egg] ?\n1. [ ] an algorithm for an agent to associate the morphology with the states (for senders perhaps) what is the minimal steps to identify the morphology to the states. (a form of grounding)\n\n\n### A baseline generative model for morphology\n\n::: {#be3e5a73 .cell execution_count=1}\n``` {.python .cell-code}\nimport random\nimport csv\n\n\nrandom.seed(45)\n# start with a simple morphology and then add more complexity.\n\n## TODO: check we don't over flow the phoneme space\n\nclass base_morphology:\n\n    def __init__(self,\n                    vowels=None, \n                    consonants=None,\n                    parts_of_speech_closed=None,\n                    parts_of_speech_open=None,\n                    declensions=None, \n                    nouns=None,\n                    ):\n\n        # define the phonemes\n        if not vowels:\n            self.vowels = ['a','e', 'i','o', 'u', 'aa','ee', 'ii','oo', 'uu','ai','au','ei','ou','ia','ua']\n        else:\n            self.vowels = vowels\n        if not consonants:\n            self.consonants = ['b', 'c','cs','ch', 'd','dh', 'dzh','f', 'g','gh', 'h','hw','ny', 'j', 'k','kw' 'l','ld','lh', 'm','mb', 'n','nc', 'nd','ng','ngw','nqu','nqt','nt', 'p', 'q', 'r','rd','rh', 's', 'sh', 't','tsh' 'v', 'w', 'x', 'y', 'z']\n        else:\n            self.consonants = consonants\n\n        # define the parts of speech\n        if not parts_of_speech_closed:\n            self.parts_of_speech_closed = ['pronoun','article','preposition','conjunction','numeral']\n        else:\n            self.parts_of_speech_closed = parts_of_speech_closed\n        \n        if not parts_of_speech_open:\n            self.parts_of_speech_open = ['noun','verb','adjective','adverb']\n        else:\n            self.parts_of_speech_open = parts_of_speech_open\n\n        if not declensions:\n            self.declensions = [\n                'nominative',  # subject\n                'accusative',  # direct object\n                'dative',      # indirect object\n                'instrumental',# with, by means of\n                'causal',      # for, for the purpose of\n                'translative', # into\n                'terminative', # as far as, up to\n                'essive',      # as \n                'inessive',    # in\n                'superessive', # on  \n                'adessive',    # by, at\n                'illative',    # into\n                'sublative',   # onto\n                'allative',    # to\n                'elative',     # out of\n                'delative',    # off, about\n                'ablative',    # from, away from\n                'genitive',    # of, 's \n                'locative',    # location\n                'vocative',    # object being addressed \n                'partitive',   # partialness\n                'abessive',   # without\n                'comitative', # with\n            ]\n        else:\n            self.declensions = declensions\n\n        # define the nouns\n        if not nouns:\n            self.nouns = ['monkey','falcon','puma','conda','tilapia','banana','kiwi','coconut','pear','river','mountain','ocean','lake','forest','clearing','valley','one','two','many',]\n        else:\n            self.nouns = nouns\n        \n        self.gen_parts_of_speech_dict()\n        self.gen_dec_dict()\n        self.gen_noun_dict()\n        self.gen_plurals_dict()\n        self.gen_inf_markers_dict()\n        self.gen_tense_dict()\n        self.gen_mood_dict()\n        self.gen_aspect_dict()\n\n    def generate_rnd_phone(self):   \n        # generate a random phoneme\n        return random.choice(self.consonants) + random.choice(self.vowels)\n\n    def generate_num_phoneme(self,consonant, vowel):\n        # pick a consonants cons from consonants\n        c = self.consonants[consonant % len(self.consonants)]\n        # pick a vowel from vowels\n        v = self.vowels[vowel % len(self.vowels)]\n        return c + v \n\n    def generate_rnd_stem(self,k=3):\n        # generate a random word with k phonemes\n        word = ''\n        for i in range(k):\n            word += self.generate_rnd_phone()\n        return word \n\n    def gen_parts_of_speech_dict(self):\n        # generate a dictionary of parts of speech        \n        pos_markers = [\"\"]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.parts_of_speech_open)-1)]\n        self.pos_dict = {pos_markers[i]:self.parts_of_speech_open[i] for i in range(len(self.parts_of_speech_open))}\n        # currently the closed pos are ignored\n\n    # The criterion for an ending to be a case (according to today's generative linguistic grammars of Hungarian) is that a word with that ending can be a compulsory argument of a verb. This difference is usually unimportant for average learners of the language.\n\n    def gen_dec_dict(self):\n        # generate a dictionary of declensions        \n        markers = [\"\"]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.declensions)-1)]\n        self.declenations_dict = {markers[i]:self.declensions[i] for i in range(len(self.declensions))}\n\n    def gen_plurals_dict(self):\n        # generate a dictionary for plurals affixes\n        ## TODO make a parameter\n        self.numbers = ['singular','plural']    \n        markers = [\"\"]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.numbers)-1)]\n        self.plu_markers_dict = {markers[i]:self.numbers[i] for i in range(len(self.numbers))}\n\n    def gen_inf_markers_dict(self):\n        # generate a dictionary for plurals affixes\n        ## TODO make a parameter\n        self.inflections = ['1ps','2ps','3ps','1pp','2pp','3pp'] \n        markers = [\"\"]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.inflections)-1)]\n        self.inf_markers_dict = {markers[i] : self.inflections[i] for i in range(len(self.inflections))}\n\n    def gen_tense_dict(self):\n        # generate a dictionary for tenses affixes\n        ## TODO make a parameter\n        self.tenses = ['past','present','future']\n        markers = [\"\"]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.tenses)-1)]\n        self.tense_markers_dict = {markers[i] : self.tenses[i] for i in range(len(self.tenses))}\n\n    def gen_mood_dict(self):\n        # generate a dictionary for tenses affixes\n        ## TODO make a parameter\n        self.moods = ['indicative','subjunctive','imperative','conditional','optative','jussive','interrogative','exclamatory']\n        markers = [\"\"]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.moods)-1)]\n        self.mood_markers_dict = {markers[i] : self.moods[i] for i in range(len(self.moods))}\n\n    def gen_aspect_dict(self):\n        # generate a dictionary for tenses affixes\n        ## TODO make a parameter\n        self.aspects = ['perfective','imperfective','progressive','habitual','frequentative','iterative']        \n        markers = [\"\"]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.aspects)-1)]\n        self.aspects_dict = {markers[i] : self.aspects[i] for i in range(len(self.aspects))}\n\n\n    def gen_noun_dict(self):\n\n        self.nouns = ['monkey','falcon','puma','conda','tilapia','banana','kiwi','coconut','pear','river','mountain','ocean','lake','forest','clearing','valley','one','two','many',]\n\n        ## 1. generate a stem for each noun\n        stems = [self.generate_rnd_stem(3) for i in range(len(self.nouns))]\n\n        ## 2. a dictionary of nouns\n\n        self.nouns_dict = {stems[i]:self.nouns[i] for i in range(len(self.nouns))}\n\n\n    def gen_lexicon(self):\n\n        lexicon = {}\n\n        for stem in (nouns_dict):\n            print(f'\\n\\nlemma: {stem} = {nouns_dict[stem]}')\n            for pos in pos_dict: \n                #print(pos)\n                if pos_dict[pos] == 'noun': \n                    for declension in declenations_dict: \n                        for plural in plu_markers_dict: \n                            lexeme = f'{stem}\\'{pos}{declension}{plural}'\n                            features =  f'{nouns_dict[stem]},{pos_dict[pos]},{declenations_dict[declension]},{plu_markers_dict[plural]}'\n                            lexicon[lexeme] = features\n                            print(f'{lexeme} = {features}')\n                elif pos_dict[pos] == 'verb':\n                    for mood in mood_markers_dict:\n                        for tense in tense_markers_dict:\n                            for inflection in inf_markers_dict:\n                                lexeme = f'{stem}\\'{pos}{mood}{tense}{inflection}'\n                                features = f'{nouns_dict[stem]},{pos_dict[pos]},{mood_markers_dict[mood]},{tense_markers_dict[tense]},{inf_markers_dict[inflection]}'\n                                lexicon[lexeme] = features\n                                print(f'{lexeme} = {features}')\n                else:\n                    lexeme= f'{stem}\\'{pos}'\n                    features = f'{nouns_dict[stem]},{pos_dict[pos]}'\n                    lexicon[lexeme] = features\n                    print(f'{lexeme} = {features}')\n        \n    def export_lemmas(self,lexicon,filename='lexicon.csv'):\n        # export the lexicon to a csv file\n        with open(filename, 'w') as f:\n            writer = csv.writer(f)\n            writer.writerow(['lemma', 'features'])\n            for lemma, features in self.lexicon.items():\n                writer.writerow([lemma, features])       \n\nbase = base_morphology(\n    vowels=['a','e', 'i','o', 'u',],\n    consonants=['b', 'c', 'd','f', 'g','h', 'j', 'k', 'l','m','n', 'p', 'q'],\n    parts_of_speech_closed=['pronoun','article','preposition'],\n    parts_of_speech_open=['noun','verb'],\n    declensions=['nominative'],\n    nouns=['monkey','falcon'],\n)\n\nprint(f'{base.generate_rnd_phone()=}')\nprint(f'{base.generate_num_phoneme(3, 2)}')\nprint(f'{base.generate_rnd_stem(3)}')\nprint (f'{base.pos_dict=}')\nprint (f'{base.declenations_dict=}')\nprint (f'{base.nouns_dict=}')\nprint (f'{base.plu_markers_dict=}')\nprint (f'{base.inf_markers_dict=}')\nprint (f'{base.tense_markers_dict=}')\nprint (f'{base.mood_markers_dict=}')\nprint (f'{base.aspects_dict=}')\n\n\n\n#export_lemmas(lexicon)\n# f = open('dict.csv','wb')\n# w = csv.DictWriter(f,mydict.keys())\n# w.writerow(mydict)\n# f.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbase.generate_rnd_phone()='da'\nfi\nmikadu\nbase.pos_dict={'': 'noun', 'ba': 'verb'}\nbase.declenations_dict={'': 'nominative'}\nbase.nouns_dict={'gokici': 'monkey', 'hacoba': 'falcon', 'gagama': 'puma', 'diqefo': 'conda', 'cujepa': 'tilapia', 'behihi': 'banana', 'cocaqi': 'kiwi', 'hopunu': 'coconut', 'nuqima': 'pear', 'kifida': 'river', 'fopuji': 'mountain', 'fojodu': 'ocean', 'kucuja': 'lake', 'lubulu': 'forest', 'qihide': 'clearing', 'bugaja': 'valley', 'bumimi': 'one', 'kofige': 'two', 'nuneka': 'many'}\nbase.plu_markers_dict={'': 'singular', 'ba': 'plural'}\nbase.inf_markers_dict={'': '1ps', 'ba': '2ps', 'ca': '3ps', 'da': '1pp', 'fa': '2pp', 'ga': '3pp'}\nbase.tense_markers_dict={'': 'past', 'ba': 'present', 'ca': 'future'}\nbase.mood_markers_dict={'': 'indicative', 'ba': 'subjunctive', 'ca': 'imperative', 'da': 'conditional', 'fa': 'optative', 'ga': 'jussive', 'ha': 'interrogative', 'ja': 'exclamatory'}\nbase.aspects_dict={'': 'perfective', 'ba': 'imperfective', 'ca': 'progressive', 'da': 'habitual', 'fa': 'frequentative', 'ga': 'iterative'}\n```\n:::\n:::\n\n\n## Syntax\n\na fixed template syntax to go with the morphology that can be generalized to a rule based grammar.\n\nchildren start with one word sentence then learn two sentences and then three word sentences and so on.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}