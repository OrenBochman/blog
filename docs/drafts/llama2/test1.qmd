
convert this transcript into markdown - putting it in a source cell so I can copy it:
1. try to keep the original language and style but correct any transcription errors.
2. split it into paragraphs separated by two new lines

1. try to keep the original language and style
2. split it into paragraphs separated by two new lines
3. Highlight the key sentences in each paragraph  key point using [sentnce]{.mark}
4. Mark using bold: keywords, terms and definitions using **text**
5. italics for indicating other named entities



```
let's annotate all entities using [text]{.ent_class} and extraneous short phrases or interjection with .edit_remove

<style>
.edit_remove { text-decoration: line-through; color: #95a5a6; } /* Grey and strike through for removal */
.ent_person { color: #e74c3c; } /* Red for people */
.ent_organization { color: #3498db; } /* Blue for organizations */
.ent_model { color: #9b59b6; } /* Purple for models */
.ent_event { color: #2ecc71; } /* Green for events */
.ent_subject { color: #f1c40f; } /* Yellow for subjects */
.ent_other { color: #95a5a6; } /* Grey for other entities */
</style>
```


WEBVTT

1
00:00:00.000 --> 00:00:04.579
[sound]. Hi, in this lecture we're gonna
step back a little bit, and we're gonna

2
00:00:04.579 --> 00:00:09.393
think about, how do we model people, Cuz a
lot of models are gonna be concerning us

3
00:00:09.393 --> 00:00:14.207
are models of, you know people and groups
of people like firms and governments and

4
00:00:14.207 --> 00:00:18.962
organizations. So, if you wanna make good
models of those things, then you've gotta

5
00:00:18.962 --> 00:00:23.603
have good models of the parts, good models
of the people. Okay. Modeling people is

6
00:00:23.603 --> 00:00:27.777
tricky. [inaudible]. Physicist Marie
Gelmont once famously said, imagine how

7
00:00:27.777 --> 00:00:32.169
difficult physics would be. If electrons
could think [laugh] so what did he mean

8
00:00:32.169 --> 00:00:36.608
human? What he meant was that you know if
you take an electron or a carbon atom or

9
00:00:36.608 --> 00:00:41.155
even a water molecule it doesn't think it
doesn't try to make sense of the world it

10
00:00:41.155 --> 00:00:45.378
doesn't have any goals or objectives or
anything like that no beliefs so it's

11
00:00:45.378 --> 00:00:49.871
pretty straight forward to model how those
things function when you look at people,

12
00:00:49.871 --> 00:00:54.094
people are much more complicated right?
We're purposeful, we've got goals we've

13
00:00:54.094 --> 00:00:58.425
got objectives we've got things we want to
do, we've got belief structures, we're

14
00:00:58.425 --> 00:01:03.465
messy. And because of that you just don't
quite know how we're going to behave. Now

15
00:01:03.465 --> 00:01:08.304
on top of that we're diverse, right? We
want different things. We have different

16
00:01:08.304 --> 00:01:13.454
goals and objectives. So this combination
of sort of purposeful, thinking actors who

17
00:01:13.454 --> 00:01:18.541
are different means that it can be really
hard to understand what they do and how

18
00:01:18.541 --> 00:01:22.865
they act. So how do we do it? Well, we're
gonna talk about three basic frameworks.

19
00:01:22.865 --> 00:01:27.275
The first framework is called the rational
actor model. Now in this framework, what

20
00:01:27.275 --> 00:01:31.579
you do is you assume that people optimize.
Now, this is unrealistic, and I'll talk

21
00:01:31.579 --> 00:01:35.828
about this in the next lecture in some
detail, but it's a good benchmark. So one

22
00:01:35.828 --> 00:01:40.292
way to think about it is just to assume,
let's just assume people have some sort of

23
00:01:40.292 --> 00:01:44.642
goal, and they optimize their goal, okay?
Second thing. We can assume what we call a

24
00:01:44.642 --> 00:01:49.084
behavioral model. Now here we [inaudible]
sort of gather up all sorts of data about

25
00:01:49.084 --> 00:01:53.580
how real people actually do make decisions
and choices and act, and then what we try

26
00:01:53.580 --> 00:01:58.023
to do is model people as close as possible
to how real people behave. Now of course

27
00:01:58.023 --> 00:02:01.984
you can make it too complicated. We sort
of try and include the one or two

28
00:02:01.984 --> 00:02:06.320
different things from perfect rationality,
like the biases that people might have.

29
00:02:06.320 --> 00:02:10.078
Right. And the third thing that we'll look
at, is even simpler. Which are sort of,

30
00:02:10.078 --> 00:02:13.836
ruled based models. So here what we're
going to do is instead of digging really

31
00:02:13.836 --> 00:02:17.737
deep into psychology we're just going to
assume that people follow rules, and then

32
00:02:17.737 --> 00:02:21.581
see how those things add up. Now, what
we're gonna see is in some cases which

33
00:02:21.581 --> 00:02:25.872
should be three things we assume matters a
lot and in other cases which should be

34
00:02:25.872 --> 00:02:29.849
three things we assume doesn't matters
very much at all. And so, how we level

35
00:02:29.849 --> 00:02:34.245
people and whether it is so important that
we get it exactly right is going to be a

36
00:02:34.245 --> 00:02:38.274
function of the particular model we are
playing with. Look at that little bit

37
00:02:38.274 --> 00:02:42.513
later on [inaudible]. First let me a lil
bit more back. So, how does that rational

38
00:02:42.513 --> 00:02:46.490
active model work? In rational active
model you assume there is an objective

39
00:02:46.490 --> 00:02:50.520
function. It'll be a is a mathematical
function you know someone is trying to.

40
00:02:50.520 --> 00:02:54.186
Maximize, right. So that could be if
you're a person, maximizing happiness or

41
00:02:54.186 --> 00:02:58.000
utility. If you're a firm it could be
maximizing profits or market share. Or if

42
00:02:58.000 --> 00:03:01.863
you're a gov, if you're running for office
it could be maximizing the number of

43
00:03:01.863 --> 00:03:05.970
votes. Then what you assume is that people
optimize. Given your objective what you do

44
00:03:05.970 --> 00:03:09.783
is you do the optimal thing. So you take
the choice that makes you as happy as

45
00:03:09.783 --> 00:03:13.499
possible. You make the, put a couple of
decisions, get used to those votes, and

46
00:03:13.499 --> 00:03:17.188
you produce the product that makes you the
most money. Let me, let me be more

47
00:03:17.188 --> 00:03:20.572
specific. So suppose you're in a
[inaudible] you see something. How many

48
00:03:20.572 --> 00:03:24.242
hours should someone choose to work? Well,
what'd you do is you'd write down a

49
00:03:24.242 --> 00:03:27.673
function, utility function. Say the
utility depends on consumption and on

50
00:03:27.673 --> 00:03:31.105
leisure. And you might assume it's like
this one. It's the square root of

51
00:03:31.105 --> 00:03:34.653
consumption times the square root of
leisure. Now why would you assume that?

52
00:03:34.653 --> 00:03:38.114
Well, the square root function, right,
starts out up and then sort of slowly

53
00:03:38.114 --> 00:03:42.043
falls off. So then so that means the first
bit of consumption's really good but then

54
00:03:42.043 --> 00:03:45.878
it becomes worth less. In the first bit of
leisure's really good, by the first hour

55
00:03:45.878 --> 00:03:49.666
of vacation's great, but by a week you're
sorta ready to get back to work. So that

56
00:03:49.666 --> 00:03:53.361
also falls off. They've got what they call
diminishing returns. So this function

57
00:03:53.361 --> 00:03:57.103
sorta says consumption's good but becomes
less good the more you have. Leisure's

58
00:03:57.103 --> 00:04:00.798
good but it becomes less good the more you
have. Well, this is your function, and

59
00:04:00.798 --> 00:04:04.353
then what you do is you just choose
consumption of leisure depending on how

60
00:04:04.353 --> 00:04:08.050
much they cost, to maximize that. Now that
may not be exactly what you, you may not

61
00:04:08.050 --> 00:04:11.778
be sitting at home writing down functions
and you know, solving these equations for

62
00:04:11.778 --> 00:04:15.371
what to do, but economists sort of assume
that you do that, its as if you do that.

63
00:04:15.371 --> 00:04:18.740
Right? You come close enough to doing
that, that this is a [inaudible] model.

64
00:04:18.740 --> 00:04:23.079
Now the rational act comes under lot of
criticism and particularly like according

65
00:04:23.079 --> 00:04:27.313
to. You know? Just basic data. Right? So,
there's a movement in the economics called

66
00:04:27.313 --> 00:04:31.441
behavior of revolution that, this is also
being going on in psychology for, for

67
00:04:31.441 --> 00:04:35.516
about a 100 years. Aiming that if you
[inaudible] servicing people rational, A

68
00:04:35.516 --> 00:04:39.855
rational ?cause you can look and see what
people do. And if you observe what people

69
00:04:39.855 --> 00:04:43.771
do, you'll find out that they're not
rational. And they are not rational in

70
00:04:43.771 --> 00:04:47.973
systematic ways. Recently, this whole
research pattern is been. Really propped

71
00:04:47.973 --> 00:04:52.136
up through evidence in neuroscience.
[inaudible] You can actually look at the

72
00:04:52.136 --> 00:04:56.515
structure of the brain, look at how people
think in particular situations, and you

73
00:04:56.515 --> 00:05:00.602
can see in fact, why they are thinking. In
ways that the rational actor model would

74
00:05:00.602 --> 00:05:04.206
consider to be irrational. So, those are
sort of two benchmarks. On the one hand,

75
00:05:04.206 --> 00:05:07.809
you can assume people are rational. On the
other hand, you can assume that, well,

76
00:05:07.809 --> 00:05:11.598
people sorta do what people do. Now this
other thing, this people doing what people

77
00:05:11.598 --> 00:05:15.063
do is gonna be a lot messier. [inaudible]
there's a third way, and the third

78
00:05:15.063 --> 00:05:18.416
approach comes from, you know, again.
Social scientist, but also from some

79
00:05:18.416 --> 00:05:21.936
computer scientist, and even some
psychologist, and that is to assume that

80
00:05:21.936 --> 00:05:25.650
people follow rules. It's more of a
Shelling model, like we didn't have a very

81
00:05:25.650 --> 00:05:29.508
elaborate model of how people behaved
there, all we did was we just assumed that

82
00:05:29.508 --> 00:05:33.366
people moved out of a neighborhood if, you
know the neighborhood became too much

83
00:05:33.366 --> 00:05:37.366
unlike them. So this is just a simple
rule, and if that simple rule. Is close to

84
00:05:37.366 --> 00:05:42.273
what people do, that might be sufficient.
To work in the model. Okay, so what do we

85
00:05:42.273 --> 00:05:46.368
got? We got these three basic frameworks,
people optimize, people are sort of

86
00:05:46.368 --> 00:05:50.572
behavioral, they do what people do and
people follow rules. Each of those in a

87
00:05:50.572 --> 00:05:54.776
given situation will give a sort of
slightly different predictions about how

88
00:05:54.776 --> 00:05:59.143
people behave. Right? And when we start
aggregating them and having them interact

89
00:05:59.143 --> 00:06:03.565
we can get very different conclusions. In
some of the other cases where we really

90
00:06:03.565 --> 00:06:07.623
don't see that much of a difference
depending on what we assume. But what we

91
00:06:07.623 --> 00:06:11.054
want to do now, these next couple
lectures, is we just wanna sort of think

92
00:06:11.054 --> 00:06:15.048
through the logic of what a rational actor
model means. Visit some of the biases that

93
00:06:15.048 --> 00:06:18.573
we've seen, there's that, you know,
psychologists have seen when they look at

94
00:06:18.573 --> 00:06:22.426
how people actually behave. And then think
through sort of what a rule based model

95
00:06:22.426 --> 00:06:26.233
might look like. And we'll conclude by
sort of comparing all these in a couple of

96
00:06:26.233 --> 00:06:30.163
settings and see when it doesn't matter
and when it really does matter. Okay.

97
00:06:30.163 --> 00:06:31.350
Thank you.

