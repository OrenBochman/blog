WEBVTT

1
00:00:00.000 --> 00:00:04.136
In this lecture. I'm gonna talk about our
first approach to modeling people. And

2
00:00:04.136 --> 00:00:07.797
this is known as the rational actor model.
Now the rational actor model assumes that

3
00:00:07.797 --> 00:00:11.023
people are, well, rational. We make
optimal choices. And it comes under a lot

4
00:00:11.023 --> 00:00:14.423
of criticism, increasingly so, because
people are dissatisfied with some of the

5
00:00:14.423 --> 00:00:17.910
results it produces. Nevertheless, I'm
gonna argue this is a really useful way to

6
00:00:17.910 --> 00:00:21.266
think about people as, especially when
you're constructing models. So what I'm

7
00:00:21.266 --> 00:00:24.623
gonna do in this lecture is define, I'm
gonna describe how it works. I'm gonna

8
00:00:24.623 --> 00:00:28.328
describe the rational actor model first in
the context of decisions, and then I'll do

9
00:00:28.328 --> 00:00:31.641
it in the context of games, which are
strategic interactions where my choice

10
00:00:31.641 --> 00:00:35.128
depends on your choice. After I do that
I'll talk about, you know, give an example

11
00:00:35.128 --> 00:00:39.048
where. Sort of breaks down. Let's talk
about sort of why I think the [inaudible]

12
00:00:39.048 --> 00:00:42.857
is a really useful thing to have in your
pocket. [inaudible] a useful way to

13
00:00:42.857 --> 00:00:47.251
analyze situations. Okay? So let's get
started. So how does the [inaudible] work?

14
00:00:47.251 --> 00:00:51.335
Well what you do is assume that people
have some sort of objective. That

15
00:00:51.335 --> 00:00:55.929
objective could be you know anyone of a
variety of things, but there is some goal

16
00:00:55.929 --> 00:01:00.353
or purpose that somebody has, or group
has, or firm has. Given that objective we

17
00:01:00.353 --> 00:01:05.061
assume that you make optimal choices, that
you optimize. Again strong assumption, but

18
00:01:05.061 --> 00:01:09.655
that what the approach assumes. There is
an objective and [inaudible]. So how does

19
00:01:09.655 --> 00:01:13.795
that work? Well let?s suppose it's a firm.
If you?re a firm you might want to

20
00:01:13.795 --> 00:01:17.901
maximize profits. Or he might wanna
maximize profit share, market share I'm

21
00:01:17.901 --> 00:01:22.365
sorry right. Or, he might wanna maximize
total revenue, those are all things that a

22
00:01:22.365 --> 00:01:26.280
firm might wanna do. If that?s your
objective. What we assume in the rational

23
00:01:26.280 --> 00:01:30.080
reactor model is that you do that, you
make the choice that maximizes that goal.

24
00:01:30.080 --> 00:01:33.903
Now, if your person you might care about
maximizing your own utility. Making

25
00:01:33.903 --> 00:01:37.828
yourself as happy as possible. Or if
you're altruistic, you might not care not

26
00:01:37.828 --> 00:01:41.702
only about yourself, but other people.
Getting the presumption though, is that

27
00:01:41.702 --> 00:01:45.423
whatever your objective is. That you make
optimal choices, to satisfy that

28
00:01:45.423 --> 00:01:48.941
objective. To do as well, as you can
possibly do. If you're a political

29
00:01:48.941 --> 00:01:53.172
candidate, right. What you might do is you
make [inaudible] as getting many votes as

30
00:01:53.172 --> 00:01:56.944
possible. So that's your goal. That's your
objective function. Get votes, and

31
00:01:56.944 --> 00:02:00.767
[inaudible] assumes that you take the
action. Make the choice, to get you as

32
00:02:00.767 --> 00:02:05.058
many votes as possible. Okay. So where can
you apply this? Where can you apply the

33
00:02:05.058 --> 00:02:09.252
rationale [inaudible]? Well, let's take
the simple case of a firm. And what they

34
00:02:09.252 --> 00:02:13.310
wanna do is they wanna you know maximize
let's say revenue instead of profits. And

35
00:02:13.310 --> 00:02:17.318
suppose that their revenue, we can write
it as this way where it gets price times

36
00:02:17.318 --> 00:02:21.127
the quantity. That's how much revenue
we're gonna get. And let's suppose if we

37
00:02:21.127 --> 00:02:25.086
let the quantity be Q that the price will
be 50 minus Q. Now why would this make

38
00:02:25.086 --> 00:02:29.143
sense? Well, if the Q is ten and if I only
produce ten of these things then there's

39
00:02:29.143 --> 00:02:32.988
not many to go around and maybe I can
charge $40 a piece for them. Right? But,

40
00:02:32.988 --> 00:02:37.516
so the price would equal 40. But if I
produce more of these, if I produce

41
00:02:37.516 --> 00:02:42.483
twenty, then there's more to go around,
and that's gonna cause the price to fall,

42
00:02:42.483 --> 00:02:47.388
and the price to fall to 30. So in this
first case, I'd get a revenue of 400. In

43
00:02:47.388 --> 00:02:52.481
the second case, I'd get a revenue of 600.
So the question is, what cue do I choose

44
00:02:52.481 --> 00:02:57.260
to maximize my total revenue? And if you
think of this, my total revenue is Q.

45
00:02:57.260 --> 00:03:03.392
Times 50 minus Q. So the optimal thing to
do is going to be, to have those two

46
00:03:03.392 --> 00:03:09.296
numbers be equal. So [inaudible] equals
25. So I get 25 times 25. Which is 625. So

47
00:03:09.296 --> 00:03:13.782
my optimal choice is q equals 25. So the
rational actor model would assume is, the

48
00:03:13.782 --> 00:03:18.268
firm wants to maximize revenue. That's the
objective function. And then given that,

49
00:03:18.268 --> 00:03:22.754
he wants to choose a quantity that will do
so, so he chooses q equals 25. So where

50
00:03:22.754 --> 00:03:26.907
can you apply this? You can apply this
just about anywhere. So you can think

51
00:03:26.907 --> 00:03:31.394
about, if I'm making investment decisions,
I get some objective and that may be to

52
00:03:31.394 --> 00:03:35.880
maximize the value of my portfolio, or to
give me some sort of nest egg to retire

53
00:03:35.880 --> 00:03:39.754
on. And so I'm gonna make choices that
maximize that. I want to think of my

54
00:03:39.754 --> 00:03:43.146
purchases, right. Or your purchases. Like,
when you go to the grocery store, or

55
00:03:43.146 --> 00:03:46.854
you're thinking about buying furniture for
your house. You could assume you've got

56
00:03:46.854 --> 00:03:50.217
some objective function. And you make
choices, right. That are optimal. Given

57
00:03:50.217 --> 00:03:54.003
that objective. Even if like education
level, you'd say how many years of school

58
00:03:54.003 --> 00:03:57.693
should I get, take, right. Should I just
get a bachelor's degree? Should I get a

59
00:03:57.693 --> 00:04:01.144
master's? Should I get a PhD? Well, you
could assume like, you've got some

60
00:04:01.144 --> 00:04:04.738
objective function which could be you
maybe care about income, you may care

61
00:04:04.738 --> 00:04:08.428
about what sort of life you lead. Is it
life of the mind, is it physical labor?

62
00:04:08.428 --> 00:04:12.217
And you choose how much education to get
given your objective. Now you can even

63
00:04:12.217 --> 00:04:16.065
apply this [inaudible] to how do I vote?
Right, cuz my objective could be for, you

64
00:04:16.065 --> 00:04:20.057
know, some policies to be implemented. So
if I look at the candidates and figure out

65
00:04:20.057 --> 00:04:23.808
which candidate is likely to vote, you
know, implement policies that I want. Now

66
00:04:23.808 --> 00:04:27.704
you also probably wanna figure out is that
candidate likely to win. I don't wanna

67
00:04:27.704 --> 00:04:31.311
vote for someone that espouses my
preferences but has no chance of winning.

68
00:04:31.311 --> 00:04:35.159
So, what I do is choose the candidate who
is likely to win, or most likely to win,

69
00:04:35.159 --> 00:04:38.910
and also takes the positions like the
positions that I prefer. Here's an issue

70
00:04:38.910 --> 00:04:43.220
though that I want to bring up, when we
assume rationality. Rationality, people

71
00:04:43.220 --> 00:04:47.857
often think that means selfish. That's not
true. So let me give you an example.

72
00:04:47.857 --> 00:04:51.655
Suppose I'm walking down the street and I
find a $100. Well, and I'm walking with a

73
00:04:51.655 --> 00:04:56.405
friend, so there's me. And there's my
friend. So one possibility's I can just

74
00:04:56.405 --> 00:05:01.126
say, you know what. This is so cool. I
just found $100.00. I can put it in my

75
00:05:01.126 --> 00:05:06.054
pocket, and give my friend nothing. That
would be rational, if my objective

76
00:05:06.054 --> 00:05:10.979
function was just me. If I could
[inaudible] about myself. But. It could be

77
00:05:10.979 --> 00:05:14.873
this is a really good friend and I care a
lot about my friend. And so when I found

78
00:05:14.873 --> 00:05:18.530
100 bucks, I say hey wait, whoa this is
great, I just found 100 bucks. So I walk

79
00:05:18.530 --> 00:05:22.377
into the nearest store and say can you
give me two 50s. And I give one of the 50s

80
00:05:22.377 --> 00:05:26.413
to my friend because I care a lot about my
friend. So there's nothing intrinsic about

81
00:05:26.413 --> 00:05:29.880
the rationality assumption that assumes
selfishness. So again, selfishness

82
00:05:29.880 --> 00:05:33.717
[inaudible] my objective function is me.
This is how we put it in the framework.

83
00:05:33.717 --> 00:05:37.343
All I care about is me, my happiness, my
income, my wealth, right. Altruistic

84
00:05:37.343 --> 00:05:41.515
preferences would be that my objective is
that I care about other people as well. So

85
00:05:41.515 --> 00:05:45.638
I care about, not only about the happiness
of myself, but I care about the happiness

86
00:05:45.638 --> 00:05:49.413
of others. And I can do all this same
mathematics, right. So here's an example

87
00:05:49.413 --> 00:05:53.387
just like the price quantity example
involving an altruistic person. So suppose

88
00:05:53.387 --> 00:05:57.510
I've got someone that's got an income of
$40,000 and they've got to decide how much

89
00:05:57.510 --> 00:06:01.583
they consume and how much do they donate.
And their objective function is just the

90
00:06:01.583 --> 00:06:05.379
square root. Of their consumption times
the square root of their donations. And

91
00:06:05.379 --> 00:06:09.256
they want to think about how much do I
donate, and how much do I consume if this

92
00:06:09.256 --> 00:06:13.223
is my goal? Well this is just a
mathematical problem, right? So, my

93
00:06:13.223 --> 00:06:18.146
donation is $40,000 minus whatever I
consume. So this is just the square root

94
00:06:18.146 --> 00:06:23.146
of C. Times the square root of 40 minus C.
I can bring everything under the square

95
00:06:23.146 --> 00:06:28.479
root sum. And get the square root of C
times 40 minus C. Where now looking at it

96
00:06:28.479 --> 00:06:33.766
this way realize I wanna make C times 40
minus C as big as possible and the way I

97
00:06:33.766 --> 00:06:39.100
do that is gonna chose C equals twenty so
that D equals twenty. So the optimal thing

98
00:06:39.100 --> 00:06:44.496
to do here is to consume twenty and donate
twenty to split my income halfway between

99
00:06:44.496 --> 00:06:49.639
consumption and donation. That's rational.
It's also incredibly altruistic. I could

100
00:06:49.639 --> 00:06:54.655
be irrational altruistic and possibly
consume less or more than this, right? And

101
00:06:54.655 --> 00:06:59.646
I could also be irrational and selfish.
But the point is that rationality. Right.

102
00:06:59.646 --> 00:07:04.456
This in no way assumes selfishness. You
can be rational and altruistic, you can be

103
00:07:04.456 --> 00:07:08.858
irrational and altruistic. Now, I wanna
move on to something that's sort of

104
00:07:08.858 --> 00:07:13.324
complicated. That is, I wanna make a
distinction between this decision and a

105
00:07:13.324 --> 00:07:17.910
game. So the previous example, that was a
decision. I had to decide how much to

106
00:07:17.910 --> 00:07:21.959
consume, how much to donate. And a
decision, my payoff, what I get, only

107
00:07:21.959 --> 00:07:27.557
depends on what I do. In a game, my pay
off depends on what other people do. This

108
00:07:27.557 --> 00:07:31.469
is where it gets tricky. Cuz for me to
decide what I'm gonna do in a game,

109
00:07:31.469 --> 00:07:35.916
depends on what I think the other person's
gonna do. So therefore I need a model of

110
00:07:35.916 --> 00:07:40.416
what I think the other person's gonna do.
And oftentimes a really good model to have

111
00:07:40.416 --> 00:07:44.917
is to assume the other person is rational.
And that's a lot of how game theory works.

112
00:07:44.917 --> 00:07:49.257
A lot of game theory assumes the other
person is rational and that allows you to

113
00:07:49.257 --> 00:07:53.489
figure out what you're gonna do. So here's
an example. Let's suppose there's two

114
00:07:53.489 --> 00:07:57.988
people. Let's call these people person
one. In person too, and this is what's

115
00:07:57.988 --> 00:08:02.775
called a normal form game. And this is a
game path. I'll explain this in a second.

116
00:08:02.775 --> 00:08:07.411
So person one can decide whether to stay
home or go into the city on a Saturday, as

117
00:08:07.411 --> 00:08:11.991
can person two. If person one stays home
and person two stays home, person one gets

118
00:08:11.991 --> 00:08:16.459
a payoff of one. If person one stays home
and person two goes to the city, person

119
00:08:16.459 --> 00:08:20.871
one still gets a payoff of one. So person
one, if they stay home, their payoff is

120
00:08:20.871 --> 00:08:25.284
one. It's also if they go to the city,
person one's payoff is just two. So one if

121
00:08:25.284 --> 00:08:29.702
they stay home, two if they go to the
city. Person two is a more complicated

122
00:08:29.702 --> 00:08:34.703
person. [laugh], person two, if they stay
home, their path is also one. But if they

123
00:08:34.703 --> 00:08:39.863
go to the city, their pay off depends on
what person one does. So if person two

124
00:08:39.863 --> 00:08:44.890
goes to the city, and person one stays
home, person two gets a payoff of zero.

125
00:08:44.890 --> 00:08:48.680
Because person two is lonely. It's no fun
to go to the city alone, at least for

126
00:08:48.680 --> 00:08:52.470
person two. But if person two goes to the
city, and person one goes to the city,

127
00:08:52.470 --> 00:08:56.197
person two gets path of four. Look at
person two's choice here. This is hard.

128
00:08:56.197 --> 00:09:00.328
Person two is trying to say do I go home,
do I stay home or go to the city. Well, if

129
00:09:00.328 --> 00:09:04.307
person one is going to stay home then I
should stay home because one is bigger

130
00:09:04.307 --> 00:09:08.438
than zero. But if person one goes to the
city then I should go to the city because

131
00:09:08.438 --> 00:09:12.418
four is bigger than one. So it would be
really fun to go to the city. Person two

132
00:09:12.418 --> 00:09:16.455
thinks it would be great to go to the city
with my friend. So, for person two to

133
00:09:16.455 --> 00:09:20.492
figure out what to do, person two has to
know what person one is going to do.

134
00:09:20.492 --> 00:09:24.848
Here's where an assumption of rationality
is really useful, cuz if person one says,

135
00:09:24.848 --> 00:09:28.938
I have no idea what person, person two
says, I have no idea what person one is

136
00:09:28.938 --> 00:09:33.347
going to do. I'm clueless. Then person two
can't figure out what to do. But if person

137
00:09:33.347 --> 00:09:37.597
two says, I think person one is rational,
then person two would say, well look, hmm,

138
00:09:37.597 --> 00:09:41.687
if person one is rational, if they go to
the city, they get a payoff of two, if

139
00:09:41.687 --> 00:09:45.990
they stay home, they get a payoff of one,
so I bet they're going to go to the city.

140
00:09:45.990 --> 00:09:50.378
So therefore, person two thinks person 1's
rational, person two thinks person 1's

141
00:09:50.378 --> 00:09:54.766
going to the city, therefore person two
goes to the city, and they get this great

142
00:09:54.766 --> 00:09:58.647
payoff. Okay so that's where when you
think about decisions you have to make in

143
00:09:58.647 --> 00:10:02.679
the real world you often have to have some
model of what other people do, and often

144
00:10:02.679 --> 00:10:06.274
times a decent model is to assume the
other person is rational, right, that

145
00:10:06.274 --> 00:10:09.966
they're going to do the rational thing.
Let's do another example. That was an

146
00:10:09.966 --> 00:10:13.610
example of what we call a normal form
game. This is an extinction, extensive

147
00:10:13.610 --> 00:10:17.448
form game. Now an extensive form game,
these are sometimes called game trees, and

148
00:10:17.448 --> 00:10:20.920
you sort of draw the action, action
sequentially. So here is, is a green

149
00:10:20.920 --> 00:10:25.068
person, right, and a blue person. So the
green person's gonna go first and they're

150
00:10:25.068 --> 00:10:29.164
gonna say "Do I go this way? And if I do,
we both get payoffs of zero, we're going

151
00:10:29.164 --> 00:10:33.087
to move down here. If we move down here
the blue person gets to move." So the

152
00:10:33.087 --> 00:10:37.681
green person's gotta decide hmmm. What do
I think the blue person's gonna do? Well,

153
00:10:37.681 --> 00:10:42.332
the green person passes it down to the
blue person and he looks and says the blue

154
00:10:42.332 --> 00:10:46.643
person can move over here, and the blue
person will get two and I'll get two.

155
00:10:46.643 --> 00:10:51.068
Where the blue person can go straight
down, and the blue person will get three,

156
00:10:51.068 --> 00:10:55.322
and I'll get minus three. So if the green
person assumes the blue person is

157
00:10:55.322 --> 00:10:59.520
rational, the green person is gonna say,
well look, three is bigger than two.

158
00:10:59.520 --> 00:11:03.982
Right. And so the blue person's going to
move down here. Well then, the green

159
00:11:03.982 --> 00:11:08.801
person is going to say, well even though I
could get two, two, I'm not going to get

160
00:11:08.801 --> 00:11:13.217
minus three, so I'm going to move over
here. So again here by the green person

161
00:11:13.217 --> 00:11:17.589
making a rationality assumption on the
part of the blue person the green person

162
00:11:17.589 --> 00:11:22.200
can figure out what to do. Okay, so when
would we see rationality? Rationality

163
00:11:22.200 --> 00:11:27.561
seems like a strong assumption. First case
is when the stakes are really large. So if

164
00:11:27.561 --> 00:11:32.283
you think about it, like, if you're just
in, you know, buying lunch somewhere,

165
00:11:32.283 --> 00:11:36.977
maybe you just follow some rule of thumb.
Right, or if you?re trying to decide you

166
00:11:36.977 --> 00:11:41.570
know exactly how many bagels to buy at the
bagel store, you know maybe you just sort

167
00:11:41.570 --> 00:11:46.108
of pick a dozen or something. But if you
think about buying a house or buying a car

168
00:11:46.108 --> 00:11:50.373
or deciding where to go to college or
deciding whether to go to college, those

169
00:11:50.373 --> 00:11:54.801
are large stake decisions and in those
situations it?s probably the case that you

170
00:11:54.801 --> 00:11:58.930
come fairly close to being rational. 'Kay,
when else? When it's repeated. So there's

171
00:11:58.930 --> 00:12:02.759
been a lot of experiments on whether or
not people are rational, what we often

172
00:12:02.759 --> 00:12:06.539
find is the first time somebody does
something, especially like, remember that

173
00:12:06.539 --> 00:12:10.466
mounty hall problem with the three tours
we did? First time people do stuff they

174
00:12:10.466 --> 00:12:14.296
typically, they often don't get it right,
right? But the more and more you do it,

175
00:12:14.296 --> 00:12:18.184
people tend to learn and we get closer and
closer, right? To being optimal. Third

176
00:12:18.184 --> 00:12:22.037
case. When you have groups of people
making decisions. Now groups can get led

177
00:12:22.037 --> 00:12:26.093
astray and you can get groupthink and
terrible choices and escalation of biases

178
00:12:26.093 --> 00:12:29.998
and that sort of thing. But, typically if
you bring in more people, you're less

179
00:12:29.998 --> 00:12:34.003
likely to make an irrational decision.
That's why, when we're making large-stake

180
00:12:34.003 --> 00:12:37.958
choices, right [laugh], we often go and
ask friends and family and other people

181
00:12:37.958 --> 00:12:41.761
who respect, so that we're not making
these decisions alone, so there's some

182
00:12:41.761 --> 00:12:45.834
sort of group of us making the decisions.
And then last case is, I mean, one reason

183
00:12:45.834 --> 00:12:49.974
we make [inaudible] choices is [inaudible]
choices are easy to make. And if someday

184
00:12:49.974 --> 00:12:53.764
says would you rather have $twenty or
$ten, we'd choose twenty. If someone says

185
00:12:53.764 --> 00:12:57.604
would you rather do less work or more
work, we'd typically say I prefer to do

186
00:12:57.604 --> 00:13:01.594
less work. So why then if rationality is
often too complicated and why if we just

187
00:13:01.594 --> 00:13:05.584
think about people don't do ration, act
rationally, why make the assumption? So my

188
00:13:05.584 --> 00:13:09.573
advisor, one of my advisors was Roger
Miason who won a Nobel Prize in the theory

189
00:13:09.573 --> 00:13:13.563
of mechan, called mechanism design, which
is a, sort of a branch of game theory, in

190
00:13:13.563 --> 00:13:17.449
a way. And Roger makes this. Follow a
compelling argument. That rational

191
00:13:17.449 --> 00:13:21.523
behavior is an incredibly important
benchmark, it's probably the most

192
00:13:21.523 --> 00:13:26.188
important benchmark is you think about
modeling people, why?  Well, first off, it's

193
00:13:26.188 --> 00:13:29.727
unique. Most of the time, not always,
but most of the time it's gonna be unique.

194
00:13:29.727 --> 00:13:33.078
So think about that case of the firm
deciding how much quantity to produce to

195
00:13:33.078 --> 00:13:36.472
maximize revenue. Or think of the person
trying to decide how much to donate to

196
00:13:36.472 --> 00:13:39.849
charity. There's a unique answer, so it
gives you this. Definitely testable

197
00:13:39.849 --> 00:13:43.973
amount, [inaudible] say this is what
rational behavior is. Okay, second thing

198
00:13:43.973 --> 00:13:48.043
something that's easy to solve for. So
even though [inaudible] is hard and

199
00:13:48.043 --> 00:13:52.112
practice. We're writing all these
mathematical equations okay, so we've got

200
00:13:52.112 --> 00:13:56.676
this function that looks like this. It's
often very easy to use mathematics to find

201
00:13:56.676 --> 00:14:01.240
the actual point. To find, you know within
the [inaudible] model. What someone should

202
00:14:01.240 --> 00:14:05.965
do. So let's contrast this with Irrational
behaviors. Suppose I write down some model

203
00:14:05.965 --> 00:14:10.275
and say, people are irrational. Well, I've
got two problems. One is its not unique.

204
00:14:10.275 --> 00:14:14.477
Right? There could be 1,000 ways to be
irrational. So I have no real prediction

205
00:14:14.477 --> 00:14:18.680
coming from the model. The second thing
is, it may be really hard to figure out.

206
00:14:18.680 --> 00:14:22.590
What exactly is it that this person's
gonna do in this context if I start taking

207
00:14:22.590 --> 00:14:26.258
in all these sort of psychological
influences and contextual influences and

208
00:14:26.258 --> 00:14:30.169
that sort of stuff? It's often just easier
to say, here's their objective function,

209
00:14:30.169 --> 00:14:34.031
let's just assume they optimize. Another
point, another reason it's an incredibly

210
00:14:34.031 --> 00:14:37.941
important benchmark. People learn, and we
talked about these experiments that over

211
00:14:37.941 --> 00:14:41.465
time people get things right. Well, if
over time you're moving towards the

212
00:14:41.465 --> 00:14:45.375
rationality assumption then maybe the
rationality assumption's sort of not a bad

213
00:14:45.375 --> 00:14:49.044
place to sorta start, and then you can
sorta say this is where we expect the

214
00:14:49.044 --> 00:14:52.473
system to go over time. Right? And then
last. In can be the case that, even if

215
00:14:52.473 --> 00:14:55.858
people make mistakes, if there's no bias
on way one way or the other in terms of

216
00:14:55.858 --> 00:14:59.158
the mistakes, those mistakes my darn well
cancel out. And what you're left with,

217
00:14:59.158 --> 00:15:02.712
then, is something that looks pretty close
to rational behavior. So some people could

218
00:15:02.712 --> 00:15:05.843
spend too much, some people would spend
too middle, little. And therefore, on

219
00:15:05.843 --> 00:15:09.725
average, you get something that looks
close to rational. Okay, so what have we

220
00:15:09.725 --> 00:15:14.514
seen? What we've seen is this is that
rational behavior. Works from the

221
00:15:14.514 --> 00:15:18.324
following set of assumptions, you assume
there's some sort of objective function.

222
00:15:18.324 --> 00:15:22.229
And then you assume people optimize, given
that objective. This could be firms, this

223
00:15:22.229 --> 00:15:25.801
could be people, whatever you want it to
be. Now, strong assumption yes, but a

224
00:15:25.801 --> 00:15:29.468
really powerful benchmark. Now one of the
things we found, from doing a lot of

225
00:15:29.468 --> 00:15:33.183
experiments we , Psychologists,
Economists, all sorts of people,

226
00:15:33.183 --> 00:15:36.993
Scientist. What we found is, there are
places where people sort of systematically

227
00:15:36.993 --> 00:15:40.660
deviate from rationality. That's what
we're going to look at next, we're gonna

228
00:15:40.660 --> 00:15:44.089
get some specific biases where the
rationality assumption, sort of seams

229
00:15:44.089 --> 00:15:47.669
consistently not to hold. Now there's
going to be cases where it does hold

230
00:15:47.669 --> 00:15:51.715
[inaudible] where it consistently doesn't
hold. Nevertheless, it still can be useful

231
00:15:51.715 --> 00:15:55.421
even if you think it's not consistently
going to hold. To think through your

232
00:15:55.421 --> 00:15:59.370
model, assuming rational behavior to get
that sort of benchmark, to see what would

233
00:15:59.370 --> 00:16:03.411
rational people do. That way when you're
actually looking at the evidence. You can

234
00:16:03.411 --> 00:16:09.660
see exactly how far from rational people
how we are behaving. Okay, thank you.