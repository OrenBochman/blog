WEBVTT

1
00:00:00.000 --> 00:00:04.757
Hi, in the previous lecture I talked about
the rational actor model. In the rational

2
00:00:04.757 --> 00:00:09.228
actor model individuals had objective
functions and they made optimal choices,

3
00:00:09.228 --> 00:00:13.642
made optimal decisions given those
objectives. In this lecture I want to talk

4
00:00:13.642 --> 00:00:18.399
about something called behavioral models.
Now behavioral models are critical of the

5
00:00:18.399 --> 00:00:22.870
rational actor assumption and they're
critical for two reasons. One is, there's

6
00:00:22.870 --> 00:00:27.398
a lot of observations, there's a lot of
data, from experiments in the laboratory

7
00:00:27.398 --> 00:00:32.156
and from just looking at the real world,
that people seem to systematically deviate

8
00:00:32.156 --> 00:00:36.382
from optimal choices. It's also the case
as we sort of understand how the brain

9
00:00:36.382 --> 00:00:40.394
works, the more deeply, that there's
evidence from neurology that, just the way

10
00:00:40.394 --> 00:00:44.510
our brain is structured, the way we encode
and represent information, the way we

11
00:00:44.510 --> 00:00:48.835
think, causes us to systematically differ
from what the rational actor model would

12
00:00:48.835 --> 00:00:52.830
assume. So what I'm going to do in this
lecture is I obviously can't give a full

13
00:00:52.830 --> 00:00:56.858
accounting of the behavioral revolution
within economics or of all of psychology.

14
00:00:56.858 --> 00:01:00.936
So what I'm going to do is just hit some
high points. I'm going to talk about four

15
00:01:00.936 --> 00:01:04.964
well-documented biases and talk about
their implications for when we think about

16
00:01:04.964 --> 00:01:08.694
modeling people. Before I get there
though, I just want to give a little bit

17
00:01:08.694 --> 00:01:12.474
of background. So Daniel Kahneman is a
psychologist and he won a Nobel Prize

18
00:01:12.474 --> 00:01:16.502
actually in economics for his work on how
people systematically differ from the

19
00:01:16.502 --> 00:01:20.381
rationality assumption. So he's got a
recent book out called Thinking Fast and

20
00:01:20.381 --> 00:01:24.461
Slow. And in this book he makes the
following point. You can think of the

21
00:01:24.461 --> 00:01:29.207
brain as having two sorts of processes. A
slower process that's a little bit closer

22
00:01:29.207 --> 00:01:33.610
to rational that processes information,
and then fast processes that are more

23
00:01:33.610 --> 00:01:37.956
likely to work based on emotion or just
based on quick clues. So as a result,

24
00:01:37.956 --> 00:01:42.416
these fast processes may make us biased,
right, in ways that the rational actor

25
00:01:42.416 --> 00:01:45.902
model assumes that we're not, right? The
rational argument assumes that we sort of

26
00:01:45.902 --> 00:01:49.045
think slowly and carefully about
everything. But Kahneman argues that we

27
00:01:49.045 --> 00:01:52.523
think both fast and slow and as a result
we make some mistakes. Now there's another

28
00:01:52.523 --> 00:01:55.707
book by Cass Sustien and Richard
Thaler who then would argue that

29
00:01:55.707 --> 00:01:59.143
these biases have real implications for
policy. And that's one of the things we're

30
00:01:59.143 --> 00:02:02.663
going to talk about. It's one thing to say
people make mistakes. Remember in the last

31
00:02:02.663 --> 00:02:05.680
lecture I said, well some people make
mistakes high and some people make

32
00:02:05.680 --> 00:02:08.822
mistakes low, those are just going to
cancel out and there's nothing we can

33
00:02:08.822 --> 00:02:11.923
really do about it. What they argue is
that these mistakes, because they're

34
00:02:11.923 --> 00:02:15.300
systematic had implications for how we
construct policy. And we'll talk about

35
00:02:15.300 --> 00:02:18.882
that as well. Okay, so what are we going
to do? Well, we're going to do four

36
00:02:18.882 --> 00:02:22.318
examples. We're going to take four
particular types of biases that are

37
00:02:22.318 --> 00:02:25.802
well-documented. The first one is
something called 'prospect theory' and

38
00:02:25.802 --> 00:02:29.826
this is the idea that we look at gains and
losses a little bit differently. Second

39
00:02:29.826 --> 00:02:33.555
one is something called 'hyperbolic
discounting'. This deals with how much we

40
00:02:33.555 --> 00:02:37.580
discount the future and how that changes
depending on how far in the future it is.

41
00:02:37.580 --> 00:02:42.054
Third, there's something called the status
quo bias. There's a tendency to sorta just

42
00:02:42.054 --> 00:02:46.421
stick to what we're currently doing and
not make changes. And this is one that has

43
00:02:46.421 --> 00:02:50.895
big implications for policy. And then the
last one is something called the base rate

44
00:02:50.895 --> 00:02:54.730
bias. And that is that we can just be
influenced by what we're currently

45
00:02:54.730 --> 00:02:58.479
thinking. So what I wanna do is just show
you some examples of each of these. Maybe

46
00:02:58.479 --> 00:03:02.196
talk a little bit about some evidence from
experiments in the real world that, that

47
00:03:02.196 --> 00:03:05.913
you know, suggest that these things really
are biases. And then I'll be a little bit

48
00:03:05.913 --> 00:03:09.540
critical about this whole approach at the
end and sort of push back a little bit.

49
00:03:09.540 --> 00:03:13.031
Okay? Alright. Let's get started. So
prospect theory. Suppose I say to you,

50
00:03:13.031 --> 00:03:17.859
okay, you've got two options. Option A is
I can give you 400 bucks for sure. Here's

51
00:03:17.859 --> 00:03:22.031
$400 right now. Option B, 50 percent of
the time I'll give you $1,000, I'll flip a

52
00:03:22.031 --> 00:03:26.502
coin. Comes up heads, you get 1,000 bucks,
but 50 percent of the time when I flip the

53
00:03:26.502 --> 00:03:31.270
coin I get a tails, I'll give you nothing.
So you go to decide, 400 bucks for sure,

54
00:03:31.270 --> 00:03:36.101
or this 50-50 proposition. So if you give
this to people, a lot of people choose

55
00:03:36.101 --> 00:03:40.825
"A". A lot of people say "look, I'll take
the 400 bucks." Now suppose I ramp this

56
00:03:40.825 --> 00:03:44.999
up. Suppose I said $400,000,000.00 for
sure, for a 50/50 chance at,

57
00:03:44.999 --> 00:03:49.891
$1,000,000,000.00 or nothing. And almost
everyone would choose A. Right? So, as

58
00:03:49.891 --> 00:03:54.867
amounts get larger, people tend to be what
we call risk adverse. In gains. But,

59
00:03:54.867 --> 00:04:00.139
here's what Kahneman showed. This is what
prospect theory is. Suppose it's a loss.

60
00:04:00.139 --> 00:04:05.335
Suppose I say okay. Option A. Is I'm just
going to take $400.00 from you. Or, we can

61
00:04:05.335 --> 00:04:09.542
flip a coin and half the time I'll take
$1000 from you which is even more, and

62
00:04:09.542 --> 00:04:13.854
half the time I'll take nothing. Well it
turns out, people are actually more likely

63
00:04:13.854 --> 00:04:18.060
to choose B in this setting because we're
risk loving over losses. So the

64
00:04:18.060 --> 00:04:22.319
risk averse over gain and risk loving over
losses, and that's different than what

65
00:04:22.319 --> 00:04:26.105
you'd get from a rational actor
assumption. So this is just a systematic

66
00:04:26.105 --> 00:04:30.207
deviation, and this explains why people
take gambles maybe that they shouldn't

67
00:04:30.207 --> 00:04:35.044
take. Okay, that's one. Let's go to the
next one. Hyperbolic discounting. Suppose I

68
00:04:35.044 --> 00:04:40.006
say to you, here's option A. I'll give you
1,000 bucks right now, or wait until

69
00:04:40.006 --> 00:04:45.096
tomorrow, and I'll give you 1,005 dollars.
What do you take? Well, a lot of people

70
00:04:45.096 --> 00:04:50.186
are gonna say you know what? Just give me
the 1,000 bucks today. Most people say,

71
00:04:50.186 --> 00:04:55.142
I'd rather get the $1,000 right now than
wait a day to just get five dollars more. Now.

72
00:04:55.142 --> 00:05:00.009
Suppose I say to you okay. Here's option
A. I'll give you $1,000.00 a year from

73
00:05:00.009 --> 00:05:04.749
today, or I'll give you $1,005.00 a year
and a day from today. Well, here most

74
00:05:04.749 --> 00:05:09.586
people say, well look. We're waiting a
year anyway. What's one more day? I'll

75
00:05:09.586 --> 00:05:14.467
take option "B". Well if you write down a
rational actor model where I'm trying

76
00:05:14.467 --> 00:05:18.713
to maximize wealth and I've got some
discount rate or something like that, if I

77
00:05:18.713 --> 00:05:22.960
in this case choose A then in this case I
should also choose A, but most people

78
00:05:22.960 --> 00:05:27.231
don't do that and the reason why is we
discount the near future. >> Right? A lot

79
00:05:27.231 --> 00:05:31.833
more than we discount that same amount of
time in the far future. So this is called

80
00:05:31.833 --> 00:05:35.880
hyperbolic discounting, where immediate
gratification matters a lot to us

81
00:05:35.880 --> 00:05:40.538
and so therefore we discount
short periods of time from right now a lot

82
00:05:40.538 --> 00:05:45.029
more than we do short periods of time down
the road. So this has implications and

83
00:05:45.029 --> 00:05:49.298
it has what is called sort of the "chocolate
cake" implication. So suppose I say, you

84
00:05:49.298 --> 00:05:53.734
know, I want to be fit. I want to stay in
great shape, I want to be healthy. And so

85
00:05:53.734 --> 00:05:57.370
someone said to me, "okay, you know... " A
week from now, would you want to have

86
00:05:57.370 --> 00:06:00.652
chocolate cake with your desert or not.
You know for desert or not. Would, would

87
00:06:00.652 --> 00:06:03.682
you forgo the chocolate cake. I'll say
absolutely, I'm going to forgo the

88
00:06:03.682 --> 00:06:07.048
chocolate cake because the thing is, I
really want to get in shape. But the thing

89
00:06:07.048 --> 00:06:10.592
is if I'm sitting at dinner and someone
put the chocolate cake in front of me Even

90
00:06:10.592 --> 00:06:14.485
though I want to lose weight. Even though
like I really want to be fit. The

91
00:06:14.485 --> 00:06:18.584
chocolate cake's right there. I can't put
it off. Because there's just, there's,

92
00:06:18.584 --> 00:06:22.546
let's see, it's, it's just right there in
front of me and so if you say in

93
00:06:22.546 --> 00:06:26.563
Kahneman's language, I am thinking fast,
right. I don't have a long drawn up

94
00:06:26.563 --> 00:06:30.851
thinking that allows me to make the
rational choice and so what's gonna happen

95
00:06:30.851 --> 00:06:35.410
is I am gonna choose the cake now and make
what would be a sub optimal choice. Third

96
00:06:35.410 --> 00:06:39.807
something called the status quo bias. So
suppose you got the phone, you go to work

97
00:06:39.807 --> 00:06:44.312
and it says, check this box, you can just
check to contribute to the pension fund or

98
00:06:44.312 --> 00:06:49.096
not. And so I'm sitting there thinking
"well, you know if I check here that means

99
00:06:49.096 --> 00:06:54.067
its going to take money out of my, You
know, salary out of my paychecks here, I'm

100
00:06:54.067 --> 00:06:58.595
not gonna check it. Now, alternatively,
what my firm could do is it could say,

101
00:06:58.595 --> 00:07:03.546
well, check box to not contribute to the
pension fund, so this is called a negative

102
00:07:03.546 --> 00:07:08.074
check off. So if I check here, then they
won't contribute. Otherwise, they will.

103
00:07:08.074 --> 00:07:12.251
Well, what happens here is, again, I
think. From a status quo standpoint, oh I'm

104
00:07:12.251 --> 00:07:16.629
already contributing. No, I don't want to
pull that money out. So it sort of seems a

105
00:07:16.629 --> 00:07:20.525
little like the prospect theory thing,
right? So what happens here is most

106
00:07:20.525 --> 00:07:24.636
people, in this case, wont check the box.
Right, most people won't check up here,

107
00:07:24.636 --> 00:07:28.853
and most people won't check down there.
Now how do we know this? We know this by

108
00:07:28.853 --> 00:07:33.124
looking at organ donations. So in England,
if you want to donate an organ, it says,

109
00:07:33.124 --> 00:07:37.569
they say this, check this box to donate
the organ. And you know how many people

110
00:07:37.569 --> 00:07:42.238
check the box? 25%. If you look to the
rest of Europe, there's a little thing

111
00:07:42.238 --> 00:07:47.222
that says, checkbox, to not donate your
organ. And you know how many people check

112
00:07:47.222 --> 00:07:52.080
the box? Ten%. So in the countries in
Europe in which you have to check the box

113
00:07:52.080 --> 00:07:56.434
to not donate your organ, 90 percent of
people donate their organs. In England

114
00:07:56.434 --> 00:08:01.229
where you have to check the box donate,
donate, to donate your organs, only 25

115
00:08:01.229 --> 00:08:05.519
percent of people check the box. So,
extremely large status quo bias. And that

116
00:08:05.519 --> 00:08:11.005
again is a deviation from rationality.
Okay, last one. What I want you to do, is

117
00:08:11.005 --> 00:08:15.626
I want you to think of the year, that you
think. This is a box made sometime during

118
00:08:15.626 --> 00:08:19.660
this century, the last century. I'm sorry.
in the 1900's. I want you to guess the year

119
00:08:19.660 --> 00:08:24.557
that this box was made, and I want you to
write it down. Okay. You've got it written

120
00:08:24.557 --> 00:08:29.036
down? Now what I want you to do is I want
you to guess how much this box costs. So I

121
00:08:29.036 --> 00:08:33.516
want you to guess, how much you think this
box is worth, if you were to you know, you

122
00:08:33.516 --> 00:08:37.689
know, buy it on the web. [inaudible]. How
much do you think the box is worth? So

123
00:08:37.689 --> 00:08:42.468
this is a bias called the base rate bias.
Look at the two numbers you wrote down.

124
00:08:42.468 --> 00:08:47.545
The first one is the year you thought it
was, made. So suppose I thought this thing

125
00:08:47.545 --> 00:08:52.086
was made in 1960. Then, I want you to
think, what price did you write down? And

126
00:08:52.086 --> 00:08:56.865
maybe you wrote down 63 dollars, right?
Those two numbers tend to be fairly close.

127
00:08:56.865 --> 00:09:01.823
So what the base rate bias is, if you get
people thinking about one number, and then

128
00:09:01.823 --> 00:09:06.721
you ask them something else, that second
number will tend to be close to the first

129
00:09:06.721 --> 00:09:11.185
number. So what you get, right, is this
bias that makes no sense at all. So you

130
00:09:11.185 --> 00:09:15.582
can do this, you can ask people, like,
think of a phone, think of the last two

131
00:09:15.582 --> 00:09:20.272
digits of someone's phone number, and then
price the box. And what you'll find is

132
00:09:20.272 --> 00:09:25.021
their prices are fairly close to the last
two digits of the phone number. So this

133
00:09:25.021 --> 00:09:29.770
base rate bias is just a clear deviation
from rational behavior, optimal behavior.

134
00:09:29.770 --> 00:09:33.815
Okay, so we've looked at these four
things. Prospect theory, hyperbolic

135
00:09:33.815 --> 00:09:38.311
discounting, status quo bias, base rate
bias. They're all well documented. So

136
00:09:38.311 --> 00:09:43.374
they're all well documented deviations
from rational behavior. So, there's a ton

137
00:09:43.374 --> 00:09:47.785
of these, right? So if you go and look out
in the web you'll see a list of hundreds

138
00:09:47.785 --> 00:09:52.035
of hundreds of biases, right that have
been found in the laboratory. Now there's

139
00:09:52.035 --> 00:09:55.908
people who are critical of this, right?
And so one of the acronyms that I

140
00:09:55.908 --> 00:09:59.952
sometimes hear in the psychology
department is weird. And if they say that

141
00:09:59.952 --> 00:10:04.381
the results are weird. Now, what do they
mean? Weird stands for Western Educated

142
00:10:04.381 --> 00:10:08.697
Industrialized Rich Developed countries.
So most of these biases, even though

143
00:10:08.697 --> 00:10:13.296
there's lots of biases, most of them have been
found on experiments with people like me. Western

144
00:10:13.296 --> 00:10:17.839
rich industrialized people from western
rich industrialized countries. So one of

145
00:10:17.839 --> 00:10:22.381
the things that we're trying to figure
out, is, how many of these biases hold true

146
00:10:22.381 --> 00:10:26.924
across different populations and different
cultures? Some of them do, and some of

147
00:10:26.924 --> 00:10:31.169
them probably won't. Now there's another
issue with these, right, you know there's

148
00:10:31.169 --> 00:10:35.211
lots of these biases. People tend to
learn, and again remember we talked about

149
00:10:35.211 --> 00:10:39.149
if the stakes are large, right, maybe
they'll learn the way out of the box. So,

150
00:10:39.149 --> 00:10:43.088
the Monty Hall problem, are they picking
the door which the prize was? People

151
00:10:43.088 --> 00:10:47.182
suffer from a bias there right, a status quo
bias. They stick to their door, but after

152
00:10:47.182 --> 00:10:51.379
they play it enough it goes away. So one
of the questions is, right, how strong are

153
00:10:51.379 --> 00:10:55.305
these biases through repeated interaction, and do
they go away? Last point about this

154
00:10:55.305 --> 00:10:59.266
behavior [inaudible], when you think about
modeling you may say boy, you know what,

155
00:10:59.266 --> 00:11:02.885
it's right, I think people suffer from
hyperbolic discounting. I think that

156
00:11:02.885 --> 00:11:06.748
there's prospect [inaudible] if there's a
base rate bias. I want to include all

157
00:11:06.748 --> 00:11:10.708
these biases in my model. Computationally
that can be a really difficult thing to

158
00:11:10.708 --> 00:11:14.327
do. So one reason why when you look at
[inaudible], we're going to be using

159
00:11:14.327 --> 00:11:18.288
simple rules or maybe assuming people
optimize even though we know people suffer

160
00:11:18.288 --> 00:11:22.102
from biases. It's that it can just be
computationally hard. It can be difficult,

161
00:11:22.102 --> 00:11:25.818
right, to write down a model where you
include these biases. Nevertheless, the

162
00:11:25.818 --> 00:11:29.674
biases are there. And when you think about
a model of people, we don't necessarily

163
00:11:29.674 --> 00:11:33.360
want to peo, assume the people optimize.
Nor as we look next, we want to see the

164
00:11:33.360 --> 00:11:37.093
people necessarily just follow simple
rules. Instead, we may want to assume that

165
00:11:37.093 --> 00:11:40.968
people are these complicated messy things
with these biases. It's just that can be

166
00:11:40.968 --> 00:11:45.731
sometimes hard to do unless elegant. Okay,
so how do we think about this, how do we

167
00:11:45.731 --> 00:11:50.359
think? Here's I think one way, is that if
I'm running down a model. It may not be a

168
00:11:50.359 --> 00:11:54.634
bad way to start, by sort of assuming
people make optimum choices given some

169
00:11:54.634 --> 00:11:58.743
simple objective function. Then I want to
ask myself. Okay, what biases are out

170
00:11:58.743 --> 00:12:02.247
there? What is the list of documented
biases? Given this list of documented

171
00:12:02.247 --> 00:12:06.035
biases, do I think any of these are going
to come into play? How large do I think

172
00:12:06.035 --> 00:12:09.728
they'd be and how relevant are they for
this particular case? I might also say

173
00:12:09.728 --> 00:12:13.563
boy, maybe I should do some experiments,
right. Maybe I should go look out there in

174
00:12:13.563 --> 00:12:17.588
the world and see are people behaving the
way that my rational actor model seems to be

175
00:12:17.588 --> 00:12:20.902
behaving. And if not, right, if, if
there's reason to believe that one of

176
00:12:20.902 --> 00:12:24.785
these biases is kicking in then I want to
include the bias. But because there's so

177
00:12:24.785 --> 00:12:28.667
many of them, I can't include them all. So
I want to think about which ones of them

178
00:12:28.667 --> 00:12:32.540
are most relevant. So what you see a lot
of times, in models that try to explain

179
00:12:32.540 --> 00:12:36.451
behavior is they'll sort of be, what I
call sort of, rational minus, or rationale

180
00:12:36.451 --> 00:12:40.411
plus. Right? So they're sort of, a little
bit less than rationale behavior, because

181
00:12:40.411 --> 00:12:45.012
what they've done is take rationality, and
added in a bias. And that's one way

182
00:12:45.012 --> 00:12:49.645
to write down useful models of
individuals. Okay, thanks.