WEBVTT

1
00:00:00.000 --> 00:00:03.784
Hi. We now turn to our third way to think
about modeling individuals, and that's

2
00:00:03.784 --> 00:00:07.473
rule-based behavior. So in rule-based
behavior what you do is you just assume

3
00:00:07.473 --> 00:00:11.162
that people follow some rule. And when
you're constructing your model you say,

4
00:00:11.162 --> 00:00:14.946
here's the rule that people follow. So the
Schelling model for instance, right? We

5
00:00:14.946 --> 00:00:18.540
just assume that people moved if the
percentage of the people like them in

6
00:00:18.540 --> 00:00:22.180
their neighborhood fell below some
threshold. That's a rule. Population below

7
00:00:22.180 --> 00:00:25.726
threshold, you move. Standing ovation
model. I have the percentage of people

8
00:00:25.726 --> 00:00:29.278
standing is above the threshold, you stand
up. That's another rule. So we can

9
00:00:29.278 --> 00:00:32.788
contrast rule based behaviour with the
rational behavior. Now remember rational

10
00:00:32.788 --> 00:00:36.035
behavior we assume people have an
objective function. And then we optimize

11
00:00:36.035 --> 00:00:39.501
with respect to that objective
function. We can also contrast it with

12
00:00:39.501 --> 00:00:42.873
behavioral models, where in behavior models
we assume that people have these sort of

13
00:00:42.923 --> 00:00:46.346
biases, that they suffer from. Now of
course, a bias could be a rule. And

14
00:00:46.346 --> 00:00:49.943
rational behavior can even be a rule. But
when we think of [rule based behavior. We

15
00:00:49.943 --> 00:00:53.410
really think of is you know, writing out a
rule. And assuming people follow them.

16
00:00:53.410 --> 00:00:59.222
Only four different types of rule based
behavior in a way. Right? So we look at

17
00:00:59.222 --> 00:01:03.671
fixed. Rule based behavior where you just
assume you follow the same rule every day,

18
00:01:03.675 --> 00:01:07.733
right. And we can think about adaptive
rule based behavior where you'd change

19
00:01:07.733 --> 00:01:11.950
your rule depending on sort of what's
happening. So we can apply these two types

20
00:01:11.950 --> 00:01:15.850
of rules, fixed and adaptive, in two
different contexts. So one context is a

21
00:01:15.850 --> 00:01:20.171
decision context, right. And in a decision
context, my payoff only depends on what I

22
00:01:20.171 --> 00:01:24.440
do. So the only person materially affected
by my choice is me. And so I can have a

23
00:01:24.440 --> 00:01:28.657
fixed rule that I apply in a decision
context or I could constantly be learning.

24
00:01:28.657 --> 00:01:32.537
I could be adapting my rule over time. The
same, second thing we'll consider is

25
00:01:32.537 --> 00:01:36.011
gains, right? So in a game that's a
strategic context. Now my payoff depends

26
00:01:36.011 --> 00:01:39.860
on what other people do. And if my payoff
depends on what other people do it could

27
00:01:39.860 --> 00:01:43.757
be that I don't want to use a fixed rule
and I wanna be a little bit more adaptive.

28
00:01:43.757 --> 00:01:47.465
But if I found works well almost all the
time like in a bargaining section, in a

29
00:01:47.465 --> 00:01:51.080
bargaining situation I may decide to just
to say I'm gonna bid ten percent less than

30
00:01:51.080 --> 00:01:54.882
my value. It could be the easier to follow
that fixed rule because you just think

31
00:01:54.882 --> 00:01:58.828
look it worked for me in the past I'm just
gonna stick with it. So what we're gonna

32
00:01:58.828 --> 00:02:02.780
do in this lecture is just walk through
all four of these cases. That's the goal.

33
00:02:02.780 --> 00:02:06.732
We're gonna walk through all four and just
sorta get some understanding of what

34
00:02:06.732 --> 00:02:10.586
modeling rule based behavior looks like.
So let's get started. Let's start with

35
00:02:10.586 --> 00:02:14.340
fixed decision rules. What would be an
example of a fixed decision rule? Well,

36
00:02:14.340 --> 00:02:18.293
one fixed decision rules that's actually
used quite often is random choice. I've

37
00:02:18.293 --> 00:02:22.196
done a model and I just assume that people
choose randomly. Now why would I do

38
00:02:22.196 --> 00:02:26.148
that? Well, the reason I do that is the
same reason that Myerson gave for, you

39
00:02:26.148 --> 00:02:29.293
know, choosing. Rational behavior.
Remember, rational behavior's a really

40
00:02:29.293 --> 00:02:32.447
good benchmark. It sort of says, what
would super smart people do in this

41
00:02:32.447 --> 00:02:35.908
situation? But we could also take the
opposite benchmark, and assume that people

42
00:02:35.908 --> 00:02:39.326
make completely random choices. And then
we could compare the two. We could say,

43
00:02:39.326 --> 00:02:42.918
well, how does optimal choice differ from
random choice in terms of how the system

44
00:02:42.918 --> 00:02:46.292
behaves? And so, by comparing those two
sort of extreme points, we get a little

45
00:02:46.292 --> 00:02:49.654
bit more understanding of what might
happen or what could happen. Now, that

46
00:02:49.654 --> 00:02:54.116
sort of, not, you know, we think of it a
fixed, normally we think of a fixed rule

47
00:02:54.116 --> 00:02:58.406
it'd be something like, take the most
direct route. So, for instance, supposing

48
00:02:58.406 --> 00:03:03.039
I'm in a city and I'm right here. And I'm
gonna head towards, I wanna head to this

49
00:03:03.039 --> 00:03:07.443
museum up here. So here's this museum,
cool, and we're next to it. And I come to

50
00:03:07.443 --> 00:03:12.191
this intersection and there's a road going
this way, and there's a road heading off

51
00:03:12.191 --> 00:03:16.534
at an angle. Well if I chose the mes, most
direct route. Rule. If it's [inaudible] I

52
00:03:16.534 --> 00:03:20.344
would choose this one. Right? I would head
in this direction. Now it could have been

53
00:03:20.344 --> 00:03:23.922
though, that if I had gone this way, I
would have found a diagonal street that

54
00:03:23.922 --> 00:03:27.640
headed directly to the museum. And it
could be that this street goes up here and

55
00:03:27.640 --> 00:03:31.264
dead ends, and I get stuck. So, following
the most direct route, locally can be a

56
00:03:31.264 --> 00:03:35.107
good rule to follow, but it might be that
there's something better to do. And the

57
00:03:35.107 --> 00:03:40.263
reason I give this example is because when
we think of fixed rules they may not be

58
00:03:40.263 --> 00:03:45.171
optimal. Right, they may be good, but they
may not be optimal. So let?s think about

59
00:03:45.171 --> 00:03:50.327
fixed strategies. Let's compare strategies
to decisions. Remember in the strategy, my

60
00:03:50.327 --> 00:03:55.360
payoff depends on what other people do, so
now a strategy could be something like

61
00:03:55.360 --> 00:03:59.263
divide evenly. So let's suppose I'm in a
bargaining situation, right? So we're

62
00:03:59.263 --> 00:04:03.079
thinking about, you know, thinking about
splitting an asset with someone. So for

63
00:04:03.079 --> 00:04:06.748
example, suppose that there's some land
that, you know, my brother and I have

64
00:04:06.748 --> 00:04:10.564
inherited. And we've gotta decide, okay,
so much do we get? There's 80 acres. Well,

65
00:04:10.564 --> 00:04:14.135
what we could say is, well, let's just
divide it evenly, let's each take 40

66
00:04:14.135 --> 00:04:18.049
acres. That would be a strategy that I
could use in a game, and it can be a fixed

67
00:04:18.049 --> 00:04:21.505
strategy. Now, we can have more
sophisticated strategies. So a strategy

68
00:04:21.505 --> 00:04:25.274
that's described a lot is a strategy
called tit for tat. So in tit for tat,

69
00:04:25.274 --> 00:04:29.345
you can imagine, let's suppose I can be
nice or I can be mean. And suppose I start

70
00:04:29.345 --> 00:04:33.366
out in tit for tat by being nice. And I'll
continue to be nice as long as you're

71
00:04:33.366 --> 00:04:37.336
nice. But if you're ever mean, then I'll
be mean. But then, as soon as you're nice

72
00:04:37.336 --> 00:04:41.407
again, I'll be nice. I'm just gonna sort
of do unto you what you do to me, but I'll

73
00:04:41.407 --> 00:04:45.528
start out by being nice. Now, the reason I
wanna explain tit for tat is because you

74
00:04:45.528 --> 00:04:49.498
can, I'll show you how you can sort of
encode this in the context of a model. So

75
00:04:49.498 --> 00:04:53.390
you can use something called a Moore
machine. And so this one machine works as

76
00:04:53.390 --> 00:04:57.364
follows. I'm gonna put a little star here.
You see, I'm gonna start out by being

77
00:04:57.364 --> 00:05:01.591
nice. Now what this green arrow says, this
sort of describes what the other person's

78
00:05:01.591 --> 00:05:05.768
doing. So I'm gonna stay nice unless the
other person's mean. If the other person's

79
00:05:05.768 --> 00:05:09.894
mean, then I'm gonna come over here and be
mean. And I'm gonna stay mean until the

80
00:05:09.894 --> 00:05:13.664
other person's nice. And if the other
person's nice, I'm gonna go back over

81
00:05:13.664 --> 00:05:17.841
here. So these arrows tell me, if I change
my state from the nice state to the mean

82
00:05:17.841 --> 00:05:21.712
state. So what I can construct is by
writing on this little sort of computer

83
00:05:21.712 --> 00:05:25.730
program, I can take a behavior like tit
for that, and embed it in a model. Now we

84
00:05:25.730 --> 00:05:29.631
can also embed another behavior. This
would be a model of behavior called grim

85
00:05:29.631 --> 00:05:33.433
trigger. Right, so here again I'm gonna
start off by being nice, and here I'm

86
00:05:33.433 --> 00:05:37.384
gonna stay nice until you do something
mean, represented by the green arrow. And

87
00:05:37.384 --> 00:05:41.436
then if you do something mean, I go over
here. But notice there's no arrows coming

88
00:05:41.436 --> 00:05:45.337
out of the mean box, right? This little
mean circle, I just stay mean. So this is

89
00:05:45.337 --> 00:05:49.239
a strategy where I start out nice, but if
you're ever mean to me, that's it. And

90
00:05:49.239 --> 00:05:54.027
this is formally called. Grim. [sound].
Trigger. And the reason it's called mean

91
00:05:54.027 --> 00:05:58.147
trigger is that it, it's pretty grim,
right. If you're ever mean to me, that's

92
00:05:58.147 --> 00:06:02.652
it, I'm mean forever. And the reason it's
called trigger is because all it takes is

93
00:06:02.652 --> 00:06:06.938
one mean action on your part and that
triggers my perpetual meanness. Again so,

94
00:06:06.938 --> 00:06:11.442
what I'm showing you here is how you can
write like just a simple diagram, right, a

95
00:06:11.442 --> 00:06:15.758
computer model to represent rule based
behavior. So those are fixed rules, right?

96
00:06:15.758 --> 00:06:20.144
Now let's go to adaptive rules. What would
an adaptive rule be? Well, let's suppose

97
00:06:20.144 --> 00:06:24.150
that you're, you know, making, like I like
to make chocolate chip cookies,

98
00:06:24.150 --> 00:06:28.590
[inaudible] I make oatmeal chocolate chip
cookies every week. And you're trying to

99
00:06:28.590 --> 00:06:32.814
figure out, okay, how do I make really
good oatmeal chocolate chip cookies? Well,

100
00:06:32.814 --> 00:06:37.145
one approach that people sometimes use is
called a gradient-based method. And so

101
00:06:37.145 --> 00:06:41.477
what a gradient-based method is, is you
keep sort of trying things in directions

102
00:06:41.477 --> 00:06:45.700
that are working. So suppose I keep adding
honey, suppose I start out by adding

103
00:06:45.700 --> 00:06:51.136
one-fourth cup of honey. Right and it
turns out that it's pretty, the cookies

104
00:06:51.136 --> 00:06:57.182
are pretty good. So then I add one more,
one tablespoon more, so it's one more

105
00:06:57.182 --> 00:07:01.548
table spoon. So this is plus. One
tablespoon and one more tablespoon and

106
00:07:01.548 --> 00:07:05.675
it's even better so then I may add another
tablespoon so then I may might add plus

107
00:07:05.675 --> 00:07:09.504
two tablespoons, right and so on. So
gradient based would say if I find a hill

108
00:07:09.504 --> 00:07:13.631
I keep trying to climb it, so that would
be adaptive in a sense and instead of just

109
00:07:13.631 --> 00:07:17.012
following a fixed rule I keep
experimenting and trying to find, find

110
00:07:17.012 --> 00:07:20.841
things that are better. Another type of
experimental rule is let's go back to

111
00:07:20.841 --> 00:07:24.721
remember we talked about random behavior.
It could be that I also tried something

112
00:07:24.721 --> 00:07:28.220
completely random. Like I might throw
wheat germ in, or I might throw raisins

113
00:07:28.220 --> 00:07:31.765
in, or I might throw walnuts in, or I
might do something, just completely crazy.

114
00:07:31.765 --> 00:07:35.495
That's an approach. I just random search.
Again it's adaptive. Instead of following

115
00:07:35.495 --> 00:07:39.270
a fixed rule. Right? I keep changing what
I'm doing trying to find something better.

116
00:07:39.270 --> 00:07:43.294
Trying to make a better decision. Now
where we see the adaptation a lot more, is

117
00:07:43.294 --> 00:07:47.541
in the context of strategy. The reason
that adaptation, and adaptive rules make more

118
00:07:47.541 --> 00:07:51.788
sense in the context of strategies, is
often the other person is changing their

119
00:07:51.788 --> 00:07:56.034
behavior to take advantage of me, so I
change my behavior to respond to them. So,

120
00:07:56.034 --> 00:08:00.227
in the context of a game it's often
important that you use an adaptive rule. So

121
00:08:00.227 --> 00:08:04.365
what would an adaptive rule be? So, one
would be best response. What does that

122
00:08:04.365 --> 00:08:08.829
mean? So, let's suppose that we have some
sort of strategic situation, and the other

123
00:08:08.829 --> 00:08:12.980
person is taking some action. Well what I
could do, I could then. Think like a

124
00:08:12.980 --> 00:08:17.550
rational choice person. I could say "What's
the best possible response I can do". Given

125
00:08:17.550 --> 00:08:21.358
what the other person's doing, and I'm
gonna do that, okay? So that's a rule I

126
00:08:21.358 --> 00:08:25.366
could follow. Because best respond to what
the other, whatever the other person's

127
00:08:25.366 --> 00:08:29.173
doing, alright? What's another thing I
could do? Well, mimicry. The only think I

128
00:08:29.173 --> 00:08:33.131
could do in a game, is, I might not be
able to figure out at all what to do. So I

129
00:08:33.131 --> 00:08:36.939
could just copy other people that are
doing well. So I might look around and

130
00:08:36.939 --> 00:08:40.415
think. I'm not sure it's, you know, how to
dress. Or I'm not sure how to behave in

131
00:08:40.415 --> 00:08:43.938
this situation. I'm not sure how to bid in
this auction. Well what I could do is I

132
00:08:43.938 --> 00:08:47.374
could watch, what are other people doing?
If you just copy what they do. So those

133
00:08:47.374 --> 00:08:50.810
are two really standard ways. One is just
[inaudible] best response. [inaudible],

134
00:08:50.810 --> 00:08:54.159
think of the game, think of the situation,
and then figure out, what's the best

135
00:08:54.159 --> 00:08:57.518
possible think I could do. Right? But
that's going to be adaptive, because if

136
00:08:57.518 --> 00:09:01.065
the other people change what they're
doing, the best possible thing to do next

137
00:09:01.065 --> 00:09:04.612
period could change. Another thing you
could do is just mimicry, to look around

138
00:09:04.612 --> 00:09:08.249
to see who's doing well, and I could copy
people who are doing well. Couple of

139
00:09:08.249 --> 00:09:12.669
observations about rule base B. First,
sometimes optimal rules, are simple. So,

140
00:09:12.669 --> 00:09:17.325
sometimes you might have a situation like
this, where I'm trying to maximize my

141
00:09:17.325 --> 00:09:21.863
happiness. My happiness depends on two
things. Chocolate and movies. And here's

142
00:09:21.863 --> 00:09:25.241
my. Happiness function, right? It's the
square root of chocolate times the square

143
00:09:25.241 --> 00:09:28.737
root of movies. And there's some price for
chocolate and some price of movies and I

144
00:09:28.737 --> 00:09:31.895
could sit down and think okay, what's the
optimal thing to do here? And if I

145
00:09:31.895 --> 00:09:35.139
actually solve for the optimal thing to
do, it turns out I should spend equal

146
00:09:35.139 --> 00:09:38.382
amounts of money on chocolate and on
movies [laugh]. And so, in some cases, the

147
00:09:38.382 --> 00:09:41.836
optimal thing to do is to follow a rule,
and so assuming rule-based behavior isn't

148
00:09:41.836 --> 00:09:45.247
that crazy of a thing to do. You can just
say, let's just assume people spend half

149
00:09:45.247 --> 00:09:48.533
their money on one thing and half their
money on another thing. If I'm writing

150
00:09:48.533 --> 00:09:51.776
down a model of the macro-economy, I might
say, let's suppose that people spend

151
00:09:51.776 --> 00:09:55.205
twenty percent of their money on food. 30
percent on transportation, 40 percent on

152
00:09:55.205 --> 00:09:59.387
housing and ten percent on entertainment.
Well, that may, you may say, well that's

153
00:09:59.387 --> 00:10:03.902
sort of a crazy fixed rule... Well it's
not, because it's actually wrote down some

154
00:10:03.902 --> 00:10:08.583
elaborate utility function and have people
optimize, they do something that's fairly

155
00:10:08.583 --> 00:10:12.707
similar to that. But another thing though
[laugh], simple rules can also be

156
00:10:12.707 --> 00:10:17.277
exploited. Let me give an example. Suppose
my rule is in bargaining, that instead of

157
00:10:17.277 --> 00:10:21.718
demanding half, or asking for half, I say
look initially I want 60%. And then what I

158
00:10:21.718 --> 00:10:25.452
do is I say, okay. Well. Okay. Next
[inaudible]. I'll say fifteen [inaudible].

159
00:10:25.452 --> 00:10:29.408
And I keep going down by one percent each
period. Think about what somebody else

160
00:10:29.408 --> 00:10:34.205
could do if they're bargaining against me.
When they, when I say 60, they say well,

161
00:10:34.205 --> 00:10:39.061
you know, how about 58. And then I could
say well okay, I'll follow my rule and I'm

162
00:10:39.061 --> 00:10:43.618
going to say 59. And they could say well,
okay, 57. And I could say 58 and they

163
00:10:43.618 --> 00:10:48.654
could walk me all the way down until I was
getting nothing because they just walked

164
00:10:48.654 --> 00:10:53.750
me down from 60 to 59 to 58 to 57 to 56 to
55 to so on. So a simple rule can often be

165
00:10:53.750 --> 00:10:57.454
exploited. So let's think and let's give a
quick summary. Why do, why model people

166
00:10:57.454 --> 00:11:01.001
using rules. Well the first one is it's
something really easy to model, right. You

167
00:11:01.001 --> 00:11:04.682
could start to say here's my model. Let's
just assume like in the Schelling model or

168
00:11:04.682 --> 00:11:08.097
in the standing ovation model, let's just
assume people follow the rules. It's

169
00:11:08.097 --> 00:11:11.689
computationally really easy to do and it's
easy to think through what's going to

170
00:11:11.689 --> 00:11:15.237
happen. Second reason, you can capture the
main effects. Often if you sit down and

171
00:11:15.237 --> 00:11:18.607
think how would someone behave in this
situation or how do I behave in this

172
00:11:18.607 --> 00:11:21.977
situation, you think it's a rule and
that's the, you write down that rule and

173
00:11:21.977 --> 00:11:25.660
you're going to capture the main things
that are going on. Third thing now. Often

174
00:11:25.660 --> 00:11:29.699
it's gonna be kinda ad hoc. Right, so one
of the problems in writing these things

175
00:11:29.699 --> 00:11:33.586
down is not everybody's the same as you.
So remember when we talked about the

176
00:11:33.586 --> 00:11:37.625
difficulty involving people? One is that
people differ, and because people differ

177
00:11:37.625 --> 00:11:41.361
so much, any one rule you write down may
be somewhat ad hoc, and it may not

178
00:11:41.361 --> 00:11:45.601
represent what's really going on. And then
the last thing which we just talked about

179
00:11:45.601 --> 00:11:49.438
is sometimes you see rules can be
exploited. And so if you think about a

180
00:11:49.438 --> 00:11:53.275
strategic situation, writing down a fixed
rule can be problematic because if

181
00:11:53.275 --> 00:11:57.263
everybody followed that rule, other people
would really take advantage of it. So

182
00:11:57.263 --> 00:12:01.000
where are we? We've talked about rational
models of behavior where people optimize.

183
00:12:01.000 --> 00:12:04.617
We've talked about more behavioral models
based on observation and even some, you

184
00:12:04.617 --> 00:12:08.188
know, recent neural signs. We now talked
about rule based models. What we do is we

185
00:12:08.188 --> 00:12:11.894
sort of write down what people do but, as
a rule, and then just ask sort of how does

186
00:12:11.894 --> 00:12:15.332
that rule aggregate. Depending on the
situation, any one of these three things

187
00:12:15.332 --> 00:12:18.993
might make perfect sense or maybe you want
to do all three to try to get a deeper

188
00:12:18.993 --> 00:12:22.520
understanding of what's going on. What
we're going to do in the next lecture is

189
00:12:22.520 --> 00:12:26.047
we're going to talk about how in some
cases it's not going to matter very much

190
00:12:26.047 --> 00:12:29.441
what behavioral rule we write down. And in
other cases it could matter a lot.

191
00:12:29.441 --> 00:12:30.602
Alright, let's get started.