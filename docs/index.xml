<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Oren Bochman&#39;s Blog</title>
<link>https://orenbochman.github.io/</link>
<atom:link href="https://orenbochman.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Personal website, portfolio and blog</description>
<generator>quarto-1.5.54</generator>
<lastBuildDate>Sun, 13 Oct 2024 21:00:00 GMT</lastBuildDate>
<item>
  <title>Compositionality in Lewis signaling games and MARL transfer learning.</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/what-is-composition.html</link>
  <description><![CDATA[ 





<section id="tldr-compositionality---a-guide-to-the-perplexed" class="level2">
<h2 class="anchored" data-anchor-id="tldr-compositionality---a-guide-to-the-perplexed">TL;DR: Compositionality - A guide to the perplexed</h2>
<p><mark><strong>Compositionality</strong> means different things to different people in different contexts, which is irksome to the student, renders researches prone to vagaries and foist unexpected complications onto partitions</mark>. Although I’m no Maimonides, I will assay to identify the different meanings; delineate thier contextual boundaries; and to establish a hierarchy relating such different facets of Compositionality.</p>
<p>We start considering Lewis signaling games, first with atomic signals and later with compound ones, I then consider transfer across tasks in Multi Objective Multi Agent Reinforcement MO-MARL learning and also try to abstract these into a formalize mathematical form.</p>
</section>
<section id="motivation-1---synthetic-and-emergent-languages" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="motivation-1---synthetic-and-emergent-languages">Motivation 1 - Synthetic and Emergent languages</h2>
<p>When dealing with emergent languages, I believe that complex signaling systems that are more faithful representations of real world states are superior to ones that just map them arbitrarily. I say complex because the simple signaling systems are highly symmetrical but when we use aggregate signals we immediately get systems that are have different levels of desireable properties. The signaling systems one ends up with may be path dependent, and may not be the most optimal for the task at hand.</p>
<p>But what does it mean to be faithful? <sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;this is not related to the mathematical notion of a group action discussed below, It just means that the signal is a good representation of the state.</p></div></div><p>We can most easily understand it using an example. If the state contain some space time description, a signaling system that has a rule about describing space and similar rule about time will require learning two rules and a much smaller lexicon rather than leaning no rules and a massive lexicon for each of the space time combinations deemed pertinent to communication. We also see how compositionality is intimately tied to learnability and faithfulness. In a language that is compositional we can allocate more of the lexicon to atomic and semantically orthogonal concepts and use the rules to create a whole space of possible meaning.</p>
<p>Why is this a challenge? In the real world states are complex and there are many facets to being faithful. For example, the state might be a picture of a cat. The signaling system might have to learn to describe the color of the cat, the shape of the cat, the size of the cat, the position of the cat, the orientation of the cat, the texture of the cat, the breed of the cat, the mood of the cat, the age of the cat, the health. The list goes on and on. So as a language grows we shift from a simple rules to capture parts of the state into more abstract system that can capture all the many facets of the state with the added constraint that this abstract system must be easy to learn via this same idea of composition. <sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;One would imagine that given a flexible template for a complex signaling game the constraint on learnability would select for more compositional languages. In reality there are many impediments to learning such a systems so there are no guarantees that using various constraints will lead to a paragon of complex signaling to emerge - we will more likely see something odd and obscure that is very hard to interpret. I’d like to point out that natural languages are rather hard for machines to learn and for most humans even more so.</p></div></div><section id="desiderata-fulfilled-by-the-original-lewis-signaling-game" class="level3">
<h3 class="anchored" data-anchor-id="desiderata-fulfilled-by-the-original-lewis-signaling-game">Desiderata fulfilled by the original Lewis signaling game</h3>
<p>Natural language temper a using faithful one to one mapping with of state to signals with abstraction that are easier to learn by being general.</p>
<ul>
<li>When learning using a Lewis signaling game, agents begin with a very simple semantic setup</li>
<li>there is a list of states and we want to be able to name them.</li>
<li>Agents learn a mapping not unlike a <strong>lexicon</strong> which list the different meaning of a token.</li>
<li>A good lexicon also lists things like prefixes, suffixes and collocations which are compositional elements of language.</li>
<li>A <strong>thesaurus</strong> list synonyms which are also compositional elements of language. Lewis games can also capture synonymy by having multiple signals for the same state we can call these partial pooling states equilibria - separating equilibria as they do not require receiver to guess the state from the signal. While synonyms are clearly inefficient in a a signaling system, when adding a time dimensions synonyms for common ideas can diverge into more nuanced states as we learn more facets of the partial state they correspond. We can think of this also as <img src="https://latex.codecogs.com/png.latex?X+w_1%20a,%20X+w_2%20b%20...%20X+n"> where X is the common state and a, b, … n are the different semantics atoms but the weights w_i start as 0 and slowly increase thus diverging into more meaningful versions.</li>
<li>Lewis games can also capture homonymy by having multiple states for the same signal this is called a partial pooling equilibrium. These are useful if we consider them as the most informative partial signals that can be sent. (This may sound a bit of a stretch but it is best way I found to think about it.)</li>
</ul>
<p>We can see how Lewis game can capture at least three structural properties of language. In the literature the focus has been on signaling systems which are one to one mappings between states and signals this corresponds to a large part of language which is unambiguous and has a list like structure I discussed above. However we can now see that algorithms that could be designed to target a broader set of equilibria that facilitate use of synonyms and homonyms. This is a more complex signaling system that is more like a thesaurus and a dictionary combined.</p>
</section>
</section>
<section id="motivation-2---transfer-learning-in-rl" class="level2">
<h2 class="anchored" data-anchor-id="motivation-2---transfer-learning-in-rl">Motivation 2 - Transfer learning in RL</h2>
<p>Some modern RL algorithms are fantastic for finding optimal or superhuman level policies for a single task. However, when we want to learn a new task we often have to start from scratch. This is because the policy is a complex function that is hard to decompose into simpler functions that can be reused. This is a problem of compositionality. If we could decompose the policy into simpler functions we could reuse the simpler functions and learn the new task faster. This is the idea behind transfer learning.</p>
<p>Recent research into using LLMs with RL agents indicates that with an expressive enough language and the kind of common sense knowledge captured in such a language agents may have enough structure to represent thier task in terms of an abstraction that is sufficiently amenable to transfer skills between task and may significantly reduce the amount of learning required to learn a new task. So if a language is a compositional representation of the world and the rewards can also be expressed as compositional functions of the state components then agents may be able to leverage these structures.</p>
<p>Also learning language in the microcosm of other games framing the lewis signaling might be key to exploring this duality of RL algorithms that learn abstract representations along with transferable RL skills.</p>
</section>
<section id="games-and-constraints" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="games-and-constraints">Games and constraints</h2>
<p>Besides the lewis signaling game there are:</p>
<ol type="1">
<li>the <strong>Lewis reconstruction game</strong> - where the receiver needs to reconstruct the state (an image) using the signal and there is a reconstruction loss. The agents get a shared reward but it is not 0 or 1 but a continuous value. (Deep learning practitioners likes continuous rewards since they can be differentiated and plugged into the backpropagation algorithm.)</li>
<li>the <strong>Lewis referential game</strong> AKA <strong>the classification game</strong>. The receiver needs to pick the correct state from a set of distractions and one or more good image. This is easier than the original game as there are fewer states to choose from. However selecting the state requires learning an image classifier or even a clip classifier and this is a harder task then just learning a mapping from states to signals. (It requires more steps to train if we start from scratch and learn one example at a time as we do in the lewis game. In this game i think if the distractions come from a GAN there would be better opportunities for compositionality to emerge.)</li>
<li>The set refernce game <span class="citation" data-cites="mu2022emergentcommunicationgeneralizations">(Mu and Goodman 2022)</span> in which states are sets of images that need a rule</li>
<li>The concept game <span class="citation" data-cites="mu2022emergentcommunicationgeneralizations">(Mu and Goodman 2022)</span></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-mu2022emergentcommunicationgeneralizations" class="csl-entry">
Mu, Jesse, and Noah Goodman. 2022. <span>“Emergent Communication of Generalizations.”</span> <a href="https://arxiv.org/abs/2106.02668">https://arxiv.org/abs/2106.02668</a>.
</div></div><p>Note: in both these task there are usually two modalities. Perception with multiple modalities may be key to developing the discriminative ability to learn to ignore distractions and focus on the most salient parts of the state. Each modality has its own distractions and noise. This places the actors language expressive enough to be general purpose. On the other hand the real world is four dimensional. A large parts of languages like tenses and part of the case systems are about capturing these. Anyhow if we can get an adversarial setup ideally the adversary can learn to generate distraction in all modalities.</p>
<p>A four dimensional world is a world where the state is a sequence of three dimensional attributes that evolve. Some are salient to one task others to another and most are distractions and should be ignored. Also in this kind of a game agents can more readily learn to distinguish between cheap talk and informative signals. This is because distractions are not just random noise but are generated by a model that is trying to fool the receiver.</p>
</section>
<section id="constraints" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="constraints">Constraints</h2>
<p>I am kind of biased that by adding constraints, preferably encoded as incentives some undesireable outcomes can be avoided.</p>
<ol type="1">
<li><strong>Laziness</strong> the loss of a complex lewis game should penalize agents for long messages and reward them for short ones. see also <span class="citation" data-cites="rita2020lazimpa">(Rita, Chaabouni, and Dupoux 2020)</span> where this is called lazy communication.</li>
<li><strong>Impulsivity</strong> the loss of a complex lewis game should reward early actions i.e.&nbsp;impulsiveness if it results in a correct action. see also <span class="citation" data-cites="rita2020lazimpa">(Rita, Chaabouni, and Dupoux 2020)</span> where this is called a impulsiveness.</li>
<li>I think that these could happen in a frame game of predation which mutiplies the Lewis game outcomes with a bonus and a malleus or in which each atomic signal sent carries a risk of sudden death.</li>
<li><strong>Communication bottleneck</strong> see <span class="citation" data-cites="kirby2001spontaneous">(Kirby 2001)</span> - complex signals would need to arise if agents have to use a restricted communication channel. I think of this as a shannon noisy channel and can only send a short sequence of drawn from a limited set of symbols. <sup>3</sup></li>
<li><strong>Errors Correction</strong> if there are errors then agents will benefit from being able to correct them. Injecting errors into the signals should incentivize agents to learn redundent a more complex signaling system that can detect and correct errors. This together with the previous item forms the notion shannon game, operating as a frame game for the lewis game.</li>
<li><strong>under-parametrization</strong> <span class="citation" data-cites="kottur2017natural">(Kottur et al. 2017)</span>, <span class="citation" data-cites="galke2022emergent">(Galke, Ram, and Raviv 2022)</span></li>
<li><strong>population dynamics</strong> <span class="citation" data-cites="chaabouni2022emergent">(Chaabouni et al. 2022)</span>, <span class="citation" data-cites="rita2022role">(Rita et al. 2022)</span></li>
<li><strong>memory restriction</strong> <span class="citation" data-cites="cogswell2019emergence">(Cogswell et al. 2019)</span>, <span class="citation" data-cites="cornish2017sequence">(Cornish et al. 2017)</span></li>
<li><strong>partial observability</strong> agents only see a fraction of the states at training time perhaps one or two combinations of each partial state. They need to be able to use language to coordiante the full state by pooling thier partial observations. This is what we generally mean about generalization.<sup>4</sup></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-rita2020lazimpa" class="csl-entry">
Rita, Mathieu, Rahma Chaabouni, and Emmanuel Dupoux. 2020. <span>“" LazImpa": Lazy and Impatient Neural Agents Learn to Communicate Efficiently.”</span> <em>arXiv Preprint arXiv:2010.01878</em>.
</div><div id="fn3"><p><sup>3</sup>&nbsp;This together with the previous constraints should encourage agents to learn to do source coding on the signals.</p></div><div id="ref-kottur2017natural" class="csl-entry">
Kottur, Satwik, José MF Moura, Stefan Lee, and Dhruv Batra. 2017. <span>“Natural Language Does Not Emerge’naturally’in Multi-Agent Dialog.”</span> <em>arXiv Preprint arXiv:1706.08502</em>.
</div><div id="ref-galke2022emergent" class="csl-entry">
Galke, Lukas, Yoav Ram, and Limor Raviv. 2022. <span>“Emergent Communication for Understanding Human Language Evolution: What’s Missing?”</span> <em>arXiv Preprint arXiv:2204.10590</em>.
</div><div id="ref-chaabouni2022emergent" class="csl-entry">
Chaabouni, Rahma, Florian Strub, Florent Altché, Eugene Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazaridou, and Bilal Piot. 2022. <span>“Emergent Communication at Scale.”</span> In <em>International Conference on Learning Representations</em>.
</div><div id="ref-rita2022role" class="csl-entry">
Rita, Mathieu, Florian Strub, Jean-Bastien Grill, Olivier Pietquin, and Emmanuel Dupoux. 2022. <span>“On the Role of Population Heterogeneity in Emergent Communication.”</span> <em>arXiv Preprint arXiv:2204.12982</em>.
</div><div id="ref-cogswell2019emergence" class="csl-entry">
Cogswell, Michael, Jiasen Lu, Stefan Lee, Devi Parikh, and Dhruv Batra. 2019. <span>“Emergence of Compositional Language with Deep Generational Transmission.”</span> <em>arXiv Preprint arXiv:1904.09067</em>.
</div><div id="ref-cornish2017sequence" class="csl-entry">
Cornish, Hannah, Rick Dale, Simon Kirby, and Morten H Christiansen. 2017. <span>“Sequence Memory Constraints Give Rise to Language-Like Structure Through Iterated Learning.”</span> <em>PloS One</em> 12 (1): e0168532.
</div><div id="fn4"><p><sup>4</sup>&nbsp;The greater the signaling systems the more challanging to learn from a few examples as agents are trying to learn a grammar a lexicon and many distributions to more effectively decode messages.</p></div></div></section>
<section id="functors-abstractions-of-function-composition" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="functors-abstractions-of-function-composition">Functors abstractions of function composition</h2>
<p>In mathematics composition of function is one of their most fundamental and useful properties. When we think about compositionality in natural language and in machine learning we are really trying to impose some version of this idea into the problem and this is a point we almost always forget. But since mathematics is where this ideas are formalized, mathematics is where the some of the best ideas are likely to be found.</p>
<p>In mathematics one is often more interested in functions that preserve a structure which are called morphisms. and the abstraction of this idea is <a href="https://en.wikipedia.org/wiki/Functor">functor</a> in <a href="https://en.wikipedia.org/wiki/Category_theory">category theory</a>)</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Commutative_diagram_for_morphism.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="functor"><img src="https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/Commutative_diagram_for_morphism.svg" class="img-fluid figure-img" alt="functor"></a></p>
<figcaption>functor</figcaption>
</figure>
</div></div><p>However lewis games do not require us to only use simple symbols. Agents can play the game with more complex signals and states. This is where the notion of compositionality becomes more interstsing. We can think of the lewis game as a function from states to signals</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Formal_language">formal languages</a> deal with transformation of one set of symbols to another set of symbols. This lets us rewrite a message from basic symbols into one with more complex symbols and allows us to use numbers to represent the different languages in the <a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy">chomsky hierarchy</a>. This is probably not the first thing that people in this field consider. However work starting with the simple formalism of Lewis game quickly raises the questions of how can we get language in which subsequent signals can used to break the symmetry leading to ambiguities associated with partial pooling equilibria. This is it worth noting as it might be the necessary abstraction to properly state the the problem.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Chomsky-hierarchy.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="The Chomsky hierarchy expresses greater expressivity."><img src="https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/Chomsky-hierarchy.svg" class="img-fluid figure-img" alt="The Chomsky hierarchy expresses greater expressivity."></a></p>
<figcaption>The Chomsky hierarchy expresses greater expressivity.</figcaption>
</figure>
</div></div><ul>
<li>games have a tree representation called <a href="https://en.wikipedia.org/wiki/Extensive-form_game">extensive form</a>.
<ul>
<li>We can graft to the lewis game tree additional trees states before and after and thus get game with equilibria that are more in line with various notions of compositionality and other properties of natural languages.</li>
<li>If this seems extreme it is worth noting most of the time we are not interested in a coordination task but some other framing task in which coordination is a means to an end. If this task can be learned by MARL then we already have this kind of extended tree with an embedded lewis game tree. It is essential that some kind of harmony is maintained between the parts or the equilibira may not be part of the biggger game. e.g.&nbsp;lewis games are cooperative games where the agents are trying to coordinate on a single equilibrium. If the framing game is a zero sum game it then it may eliminate the incentive to coordinate encoded into the payoffs of the lewis game. I don’t mean to say you cant have a game with incentives to cooperate and to compete but that when you do its a subtle ballance to maintain both without breaking either.</li>
<li></li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Syntax_tree.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="syntax tree"><img src="https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/Syntax_tree.svg" class="img-fluid figure-img" alt="syntax tree"></a></p>
<figcaption>syntax tree</figcaption>
</figure>
</div></div><ul>
<li>signals can be aggregated using different ways and it is hard to generelize from conjunction, to recursive structures.</li>
</ul>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>The syntax of English, for example, is clearly compositional—that is, the meaning of a sentence is some function of the meanings of the parts of that sentence. — <span class="citation" data-cites="kirby2001spontaneous">(Kirby 2001)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-kirby2001spontaneous" class="csl-entry">
Kirby, Simon. 2001. <span>“Spontaneous Evolution of Linguistic Structure-an Iterated Learning Model of the Emergence of Regularity and Irregularity.”</span> <em>IEEE Transactions on Evolutionary Computation</em> 5 (2): 102–10.
</div></div></div>
<ul>
<li></li>
</ul>
<style>
.gold { color: gold; }
.red { color: red; }
.green { color: green; }
.blue { color: blue; }
.purple { color: purple; }
</style>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 14%">
<col style="width: 13%">
<col style="width: 64%">
</colgroup>
<thead>
<tr class="header">
<th>Root (Gold)</th>
<th>Tense (Red)</th>
<th>Person &amp; Number (Green)</th>
<th>Group Action</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="gold">áll</span></td>
<td><span class="red">Present</span></td>
<td><span class="green">1st person singular</span></td>
<td><span class="gold">áll</span><span class="green">ok</span></td>
</tr>
<tr class="even">
<td><span class="gold">áll</span></td>
<td><span class="red">Present</span></td>
<td><span class="green">1st person plural</span></td>
<td><span class="gold">áll</span><span class="green">unk</span></td>
</tr>
<tr class="odd">
<td><span class="gold">áll</span></td>
<td><span class="red">Present</span></td>
<td><span class="green">2nd person singular</span></td>
<td><span class="gold">áll</span><span class="green">sz</span></td>
</tr>
<tr class="even">
<td><span class="gold">áll</span></td>
<td><span class="red">Present</span></td>
<td><span class="green">2nd person plural</span></td>
<td><span class="gold">áll</span><span class="green">tok</span></td>
</tr>
<tr class="odd">
<td><span class="gold">áll</span></td>
<td><span class="red">Present</span></td>
<td><span class="green">3rd person singular</span></td>
<td><span class="gold">áll</span></td>
</tr>
<tr class="even">
<td><span class="gold">áll</span></td>
<td><span class="red">Present</span></td>
<td><span class="green">3rd person plural</span></td>
<td><span class="gold">áll</span><span class="green">nak</span></td>
</tr>
<tr class="odd">
<td><span class="gold">áll</span></td>
<td><span class="red">Past</span></td>
<td><span class="green">1st person singular</span></td>
<td><span class="gold">áll</span><span class="red">t</span><span class="green">am</span></td>
</tr>
<tr class="even">
<td><span class="gold">áll</span></td>
<td><span class="red">Past</span></td>
<td><span class="green">1st person plural</span></td>
<td><span class="gold">áll</span><span class="red">t</span><span class="green">unk</span></td>
</tr>
<tr class="odd">
<td><span class="gold">áll</span></td>
<td><span class="red">Past</span></td>
<td><span class="green">3rd person singular</span></td>
<td><span class="gold">áll</span><span class="red">t</span></td>
</tr>
<tr class="even">
<td><span class="gold">áll</span></td>
<td><span class="red">Past</span></td>
<td><span class="green">3rd person plural</span></td>
<td><span class="gold">áll</span><span class="red">t</span><span class="green">ak</span></td>
</tr>
<tr class="odd">
<td><span class="gold">áll</span></td>
<td><span class="red">Future</span></td>
<td><span class="green">1st person singular</span></td>
<td><span class="red">fog</span><span class="green">ok</span> <span class="gold">áll</span><span class="green">ni</span></td>
</tr>
<tr class="even">
<td><span class="gold">áll</span></td>
<td><span class="red">Future</span></td>
<td><span class="green">1st person plural</span></td>
<td><span class="red">fog</span><span class="green">unk</span> <span class="gold">áll</span><span class="green">ni</span></td>
</tr>
<tr class="odd">
<td><span class="gold">áll</span></td>
<td><span class="red">Future</span></td>
<td><span class="green">2nd person singular</span></td>
<td><span class="red">fog</span><span class="green">sz</span> <span class="gold">áll</span><span class="green">ni</span></td>
</tr>
<tr class="even">
<td><span class="gold">áll</span></td>
<td><span class="red">Future</span></td>
<td><span class="green">2nd person plural</span></td>
<td><span class="red">fog</span><span class="green">tok</span> <span class="gold">áll</span><span class="green">ni</span></td>
</tr>
<tr class="odd">
<td><span class="gold">áll</span></td>
<td><span class="red">Future</span></td>
<td><span class="green">3rd person singular</span></td>
<td><span class="red">fog</span> <span class="gold">áll</span><span class="green">ni</span></td>
</tr>
<tr class="even">
<td><span class="gold">áll</span></td>
<td><span class="red">Future</span></td>
<td><span class="green">3rd person plural</span></td>
<td><span class="red">fog</span><span class="green">nak</span> <span class="gold">áll</span><span class="green">ni</span></td>
</tr>
</tbody>
</table>
<ul>
<li><p>this is considered a regular verb in hungarian</p></li>
<li><p>i have ommitted many of the forms of the verb for simplicity</p></li>
<li><p>we can ser that person and number have an entangled representation.</p></li>
<li><p>present tense is unmarked</p></li>
<li><p>past tense is is an infix</p></li>
<li><p>future tense has its own template with a auxilary verb and a infinitive</p></li>
<li><p>there is another point I’d like to make and it has to do with making agent able to communicate with humans.</p></li>
<li><p>if the agent’s language has the same group actions (homomorphism) to express structural semantics of nouns, verbs, pronouns etc. It should be much easier to then learn to converse in the homomorphic human language. The task boils down to learning functors that map agentic-roots to natural roots and agentic rules to natural rules. The agentic language might be highly regular with a single verb template and to use hungarian it might need to learn 60+ verb templates. But this is much easier I think then learning hungarian from scratch.</p></li>
<li><p>in reality learning a few extra rules might faccilitate (e.g.&nbsp;phonemic adjustments and vowel harmony) being able to communicae with hungarian verbs.</p></li>
</ul>
<p>Note though that we are no longer talking about learning hungarian but some underlying structure that is shared between hungarian and the agents language.</p>
<p>This idea of learning a underlying structure and a surface structure is one that can be further abstracted. We can have a morphological level a syntactical level and a logical level all disentandled and seperatable or we can have them all sitting in one level and possibly entagled.</p>
<p>Entaglement can arrise from … cancatenation, from coding the most common segements into shorter segments, erosion, and swapping to help with difficult phone sequences.</p>
<p>THis suggests that we might have a sequence-bag or soft-sequence agregator - a conventino that has a prefered order but is indifferent to change in the order so long as semantics are preserved.</p>
<p>also word order</p>
<p>şehirlileştiremediklerimizdensiniz mean “you are one of those that we could not make into a city dweller” in archaic turkish. The word order is the opposite of English or Hebrew, this is because Turkish is a VO and English is an OV language. The word order is a surface structure that is not important to the meaning of the sentence. <span class="citation" data-cites="deutscher2006unfolding">(Deutscher 2006)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-deutscher2006unfolding" class="csl-entry">
Deutscher, G. 2006. <em>The Unfolding of Language: An Evolutionary Tour of Mankind’s Greatest Invention</em>. Henry Holt; Company. <a href="https://books.google.co.il/books?id=maz9oLIKZKkC">https://books.google.co.il/books?id=maz9oLIKZKkC</a>.
</div></div></section>
<section id="an-evolving-desiderata-of-emergent-languages" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="an-evolving-desiderata-of-emergent-languages">An evolving desiderata of Emergent Languages</h2>
<ol type="1">
<li><p>mappings between states and signals</p>
<ol type="1">
<li>morphosyntax mappings preserve partial states (Homomorphism of normal subgroups)</li>
<li>mappings preserve semantic topologies (if a is close to b then f(a) should be close to f(b))<br>
</li>
</ol></li>
<li><p>Stability</p>
<ol type="1">
<li>Regualrity is stable (mappings used in syntax and morphology are stable over time)</li>
<li>Irregularity is stable (mappings used in irregular verbs and nouns are also stable over time) In english we maintain many irregular borrowings from other languages and thus preserve thier regularity - making such exceptions easier to learn too.</li>
</ol></li>
<li><p>Compositionality</p></li>
<li><p>Brevity (source coding)</p></li>
<li><p>Self correcting (channel coding to detect errors and correction them through redundancies like agreement, vowel harmony, etc.)</p></li>
<li><p>Learnability - how many things need to be coordinated; the complexity of the strucures, the <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoffding bound</a> on rates of learning distribution when there are errors. The <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a> for multiple learners.<sup>5</sup></p></li>
<li><p>Stable irregularities</p></li>
<li><p>zipfian distributions - the most common signals should be the shortest and the least common the longest. This is a form of source coding and would arrise naturally from huffman coding, except that this isn’t practical for several reasons. It could also arise out of laziness in the sender</p></li>
<li><p>Faithfulness</p></li>
<li><p>Distributional stability</p></li>
<li><p>Decidebility - easy to disambiguate ambiguous signals from thier context</p></li>
<li><p>Expressivity - the ability to express a wide range of concepts</p></li>
<li><p>Generalization - learning the language is possible from just a few signal state pairs.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;multiple learners has similar logic as a multiple hypothesis testing problem, for each learner postulating different signaling system with each failure or success in a Lewis game. More so when learners get to observe each other’s actions and rewards.</p></div></div><p>Some metrics</p>
<ol type="1">
<li>Compositionality
<ol type="1">
<li>Topographic similarity</li>
<li></li>
</ol></li>
<li>Source coding
<ol type="1">
<li>Compression ratio</li>
<li>Entropy</li>
<li>Mutual information</li>
</ol></li>
<li>Error detection and correction
<ol type="1">
<li>Error rate</li>
<li>Hamming distance</li>
</ol></li>
<li>Learnability
<ol type="1">
<li>Number of examples to learn a new word</li>
<li>Number of examples to learn a new rule</li>
</ol></li>
</ol>
<p>Another random thoguht or two:</p>
<section id="vo-and-ov-via-symmetry-breaking" class="level3">
<h3 class="anchored" data-anchor-id="vo-and-ov-via-symmetry-breaking">VO and OV via symmetry breaking</h3>
<p>If we use Huffman coding like process to organize the order of the morphological and syntactical elements (effectively making the fixing the on avarge most surprising partial signals before the next on average most surprising ones) we should have emergent languages that are rather similar and fairly easy to learn Like Turkish and Japanese. However there is at the start the question of how to apply aggregations. If action is first we get V O languages if it is second we get OV languages. I think that V carries more entropy in Predation and Resource gathering games so that VO should be more pravelent. However once this decision is made most partical algorithms will not be able to reverse it.</p>
</section>
<section id="vowel-harmony" class="level3">
<h3 class="anchored" data-anchor-id="vowel-harmony">Vowel Harmony</h3>
<p>if agents backprogogate with topographic similarity in mind and the basic signals (phonemes) are endowed with a similarity they may end up with systems with vowel harmony and alternation of consonants to capture sets normal subgroups with greater similarity.</p>
<p>if these regular configuration also lead to better channel coding the benefits should persist.</p>
</section>
</section>
<section id="compositionality-in-lewis-signaling-games" class="level2">
<h2 class="anchored" data-anchor-id="compositionality-in-lewis-signaling-games">Compositionality in Lewis signaling games</h2>
<p>So here is a sketch idea for an algorithm for learning a compsitinal language in a lewis game.</p>
<p>We need a language designer. This is can be the sender, the reciever or implicit. Without loss of generality we can assume that the sender is the language designer.</p>
<p>THe language designer needs 1. to a ‘semantic’ metric to decide when two state are close or distant. 2. a way to deconstruct states into atomic orthogonal/independent parts. I am thinking of normal subgroups.</p>
<p>Note that we can define the metric on the parts and aggregate them to get the metric on the whole. This is a form of compositionality.</p>
<p>More abstractly we can just say that the state is an algebric topological group.</p>
<p>So the language designer can use a template with n part (on for each of the subgroups) Idealy ordered with by the decresing size to prefix code the substates. If they there are duplicate sizes this will yeild multiple equilibria to be selected via sponatneous syemtry breaking.</p>
<p>The designer now can allocate the states to one of the points in the topology. By picking the system with loweset overall distances we get a regular compositional language.</p>
<p>Since there are many ways to do this the designer needs to coordiante with the reciever. However since there is graear regularity they onely need to coordinate a minimal set with each atomic substate apearing once.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Compositionality in {Lewis} Signaling Games and {MARL}
    Transfer Learning.},
  date = {2024-10-14},
  url = {https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/what-is-composition.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Compositionality in Lewis Signaling Games
and MARL Transfer Learning.”</span> October 14, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/what-is-composition.html">https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/what-is-composition.html</a>.
</div></div></section></div> ]]></description>
  <category>compositionality</category>
  <category>emergent languages</category>
  <category>reinforcement learning</category>
  <category>transfer learning</category>
  <category>information theory</category>
  <category>linguistics</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/what-is-composition.html</guid>
  <pubDate>Sun, 13 Oct 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>emergent communications</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-05-01-signals/shanon-game.html</link>
  <description><![CDATA[ 





<p>it seems that we might want to look at the emergent communications by considering 1. a Lewis signaling games to model coordination tasks for a basic communication system 2. a Shannon game to model the communication of information between agents in which the learn a shared communication protocol potentially using error detection and correction and corection. 3. a Chomsky game to model development of a shared grammar for complex signals.</p>
<section id="shannon-game" class="level2">
<h2 class="anchored" data-anchor-id="shannon-game">Shannon Game</h2>
<p>Shanon games are about emergence of randomized communication protocols. A randomised communication protocol is a probability distribution over the set of possible deterministic communication protocols.</p>
<p>We can model any deterministic communication protocol as a pair of decision rees, one for the sender and one for the receiver. The sender’s decision tree maps each possible message to a signal, and the receiver’s decision tree maps each possible signal to a message.</p>
<p>messages that the sender can send. The sender samples a message from this distribution and sends it to the receiver. The receiver then uses a decoding function to map the received message back to the original signal. The goal of the game is for the sender and receiver to coordinate on a communication protocol that maximizes their payoff, which is typically based on the accuracy of message transmission and reception. It is a protocol that uses randomness to encode and decode messages. This randomness can be used to introduce redundancy in the message, which can help in error detection and correction.</p>
<div id="f4dc8e19" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> CommunicationAgent:</span>
<span id="cb1-4">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, num_strategies):</span>
<span id="cb1-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_strategies <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> num_strategies</span>
<span id="cb1-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((num_strategies, num_strategies))</span>
<span id="cb1-7">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb1-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.discount_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span></span>
<span id="cb1-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.epsilon <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb1-10">    </span>
<span id="cb1-11">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> choose_strategy(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb1-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> np.random.rand() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.epsilon:</span>
<span id="cb1-13">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.random.randint(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_strategies)</span>
<span id="cb1-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-15">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.argmax(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb1-16">    </span>
<span id="cb1-17">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update_q_values(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, sender_strategy, receiver_strategy, reward):</span>
<span id="cb1-18">        max_future_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table[receiver_strategy])</span>
<span id="cb1-19">        current_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table[sender_strategy, receiver_strategy]</span>
<span id="cb1-20">        new_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.discount_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> max_future_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_q)</span>
<span id="cb1-21">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table[sender_strategy, receiver_strategy] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> new_q</span>
<span id="cb1-22"></span>
<span id="cb1-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simulation parameters</span></span>
<span id="cb1-24">num_strategies <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb1-25">num_iterations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span></span>
<span id="cb1-26"></span>
<span id="cb1-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize agents</span></span>
<span id="cb1-28">alice <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CommunicationAgent(num_strategies)</span>
<span id="cb1-29">bob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CommunicationAgent(num_strategies)</span>
<span id="cb1-30"></span>
<span id="cb1-31"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_iterations):</span>
<span id="cb1-32">    sender_strategy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> alice.choose_strategy()</span>
<span id="cb1-33">    receiver_strategy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bob.choose_strategy()</span>
<span id="cb1-34">    </span>
<span id="cb1-35">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simulate message transmission and reception with noise</span></span>
<span id="cb1-36">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This is a placeholder for actual encoding/decoding logic</span></span>
<span id="cb1-37">    success <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.rand() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Assume 80% chance of success</span></span>
<span id="cb1-38">    </span>
<span id="cb1-39">    reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> success <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-40">    alice.update_q_values(sender_strategy, receiver_strategy, reward)</span>
<span id="cb1-41">    bob.update_q_values(receiver_strategy, sender_strategy, reward)</span>
<span id="cb1-42"></span>
<span id="cb1-43"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Alice's Q-Table:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, alice.q_table)</span>
<span id="cb1-44"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bob's Q-Table:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, bob.q_table)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Alice's Q-Table:
 [[ 1.1870612   0.          0.          0.1         0.        ]
 [ 1.56945522  1.54181517  0.97338873  0.82764688  1.00508109]
 [ 0.63863811 -0.02636762  0.          0.19736455  0.1607126 ]
 [ 0.56019999  0.          0.          0.          0.        ]
 [ 0.80594973  0.25923685  0.14809569  0.          0.        ]]
Bob's Q-Table:
 [[ 1.50847687  1.20172521  0.6687741   0.64135768  0.55690049]
 [ 0.          0.7344698  -0.0829      0.          0.15625959]
 [ 0.          0.94187947  0.          0.          0.15028825]
 [ 0.15690016  0.91314851  0.22940322  0.          0.        ]
 [ 0.          0.89534698  0.14117867  0.          0.        ]]</code></pre>
</div>
</div>
<p>This example illustrates a basic game-theoretic approach where the sender and receiver iteratively learn better strategies for encoding and decoding messages over a noisy channel. The reinforcement learning framework allows both parties to adapt and improve their protocols, enhancing the reliability of communication over time. This model can be extended and refined to include more sophisticated encoding/decoding techniques and more complex noise models.</p>
<div id="538b0ef5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mesa <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Agent, Model</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mesa.time <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RandomActivation</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mesa.datacollection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DataCollector</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb3-5"></span>
<span id="cb3-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> hamming_distance(a, b):</span>
<span id="cb3-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> b) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(a)</span>
<span id="cb3-8"></span>
<span id="cb3-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Sender(Agent):</span>
<span id="cb3-10">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, unique_id, model):</span>
<span id="cb3-11">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(unique_id, model)</span>
<span id="cb3-12">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.protocol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.random_protocol()</span>
<span id="cb3-13">    </span>
<span id="cb3-14">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> random_protocol(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-15">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define a random protocol for encoding</span></span>
<span id="cb3-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> msg: msg  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Identity for simplicity</span></span>
<span id="cb3-17">    </span>
<span id="cb3-18">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-19">        message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randint(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.message_length)</span>
<span id="cb3-20">        encoded_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.protocol(message)</span>
<span id="cb3-21">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.sent_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encoded_message</span>
<span id="cb3-22"></span>
<span id="cb3-23"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Receiver(Agent):</span>
<span id="cb3-24">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, unique_id, model):</span>
<span id="cb3-25">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(unique_id, model)</span>
<span id="cb3-26">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.protocol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.random_protocol()</span>
<span id="cb3-27">    </span>
<span id="cb3-28">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> random_protocol(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-29">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define a random protocol for decoding</span></span>
<span id="cb3-30">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> msg: msg  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Identity for simplicity</span></span>
<span id="cb3-31">    </span>
<span id="cb3-32">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-33">        noisy_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.sent_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span> np.random.binomial(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.error_rate, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.message_length)</span>
<span id="cb3-34">        recovered_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.protocol(noisy_message)</span>
<span id="cb3-35">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.recovered_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> recovered_message</span>
<span id="cb3-36">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.evaluate_performance()</span>
<span id="cb3-37">    </span>
<span id="cb3-38">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> evaluate_performance(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-39">        original_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.original_message</span>
<span id="cb3-40">        recovered_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.recovered_message</span>
<span id="cb3-41">        distance <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> hamming_distance(original_message, recovered_message)</span>
<span id="cb3-42">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.payoff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.recovery_payoff(distance)</span>
<span id="cb3-43">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.payoff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.length_payoff(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(recovered_message))</span>
<span id="cb3-44">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.payoff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.early_recovery_payoff(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.current_step)</span>
<span id="cb3-45">    </span>
<span id="cb3-46"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> NoisyChannelModel(Model):</span>
<span id="cb3-47">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, message_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, error_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, max_steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>):</span>
<span id="cb3-48">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb3-49">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.message_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> message_length</span>
<span id="cb3-50">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.error_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> error_rate</span>
<span id="cb3-51">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb3-52">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> max_steps</span>
<span id="cb3-53">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.payoff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb3-54">        </span>
<span id="cb3-55">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomActivation(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb3-56">        </span>
<span id="cb3-57">        sender <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Sender(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb3-58">        receiver <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Receiver(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb3-59">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule.add(sender)</span>
<span id="cb3-60">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule.add(receiver)</span>
<span id="cb3-61">        </span>
<span id="cb3-62">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.original_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randint(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.message_length)</span>
<span id="cb3-63">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.sent_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-64">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.recovered_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-65">        </span>
<span id="cb3-66">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.datacollector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataCollector(</span>
<span id="cb3-67">            model_reporters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Payoff"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"payoff"</span>}</span>
<span id="cb3-68">        )</span>
<span id="cb3-69">    </span>
<span id="cb3-70">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> recovery_payoff(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, distance):</span>
<span id="cb3-71">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> distance</span>
<span id="cb3-72">    </span>
<span id="cb3-73">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> length_payoff(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, length):</span>
<span id="cb3-74">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> length</span>
<span id="cb3-75">    </span>
<span id="cb3-76">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> early_recovery_payoff(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, step):</span>
<span id="cb3-77">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> step) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_steps</span>
<span id="cb3-78">    </span>
<span id="cb3-79">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-80">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-81">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule.step()</span>
<span id="cb3-82">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.datacollector.collect(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb3-83">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_steps:</span>
<span id="cb3-84">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.running <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb3-85"></span>
<span id="cb3-86"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example of running the model</span></span>
<span id="cb3-87">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> NoisyChannelModel()</span>
<span id="cb3-88"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> model.running:</span>
<span id="cb3-89">    model.step()</span>
<span id="cb3-90"></span>
<span id="cb3-91"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Retrieve results</span></span>
<span id="cb3-92">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.datacollector.get_model_vars_dataframe()</span>
<span id="cb3-93"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(results)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    Payoff
0     1.39
1     2.97
2     4.54
3     6.10
4     7.55
..     ...
95  105.44
96  106.07
97  106.89
98  107.40
99  107.90

[100 rows x 1 columns]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/.local/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:

The AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.
We would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919
</code></pre>
</div>
</div>
<p>so this is a variant that uses a noisy channel model to simulate the transmission of messages between a sender and receiver. The agents have protocols for encoding and decoding messages, and the model tracks the performance of the communication system based on the accuracy of message recovery, message length, and early recovery. This example demonstrates how to model and analyze the performance of communication systems in the presence of noise and other challenges.</p>
<p>What we don’t have is a way to pick different protocols or to improve them over time.</p>
<p>I would break this down into a few steps: 1. identify the environmental factors that would encourage the agents to evolve diverse and efficient transmission protocols. a. noisy channels b. limited bandwidth c.&nbsp;limited computational resources d.&nbsp;time constraints e. risks of predation.</p>
<ol start="2" type="1">
<li>allow agents randomly generate candidate protocols and evaluate their performance.</li>
</ol>
<p>def random_protocol(): # Define a random protocol for encoding/decoding return lambda msg: np.random.randint(0, 2, len(msg))</p>
</section>
<section id="which-would-be-used-as-follows" class="level1">
<h1>which would be used as follows</h1>
<p>class Sender(Agent): def <strong>init</strong>(self, unique_id, model): super().__init__(unique_id, model) self.protocol = random_protocol()</p>
<pre><code>def step(self):
    message = np.random.randint(0, 2, self.model.message_length)
    encoded_message = self.protocol(message)
    self.model.sent_message = encoded_message</code></pre>
<p>This could be done by introducing reinforcement learning techniques to allow the agents to adapt and learn better encoding/decoding strategies based on feedback from the environment. This would enable the agents to optimize their protocols for improved communication performance in noisy channels.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Emergent Complex Communications Protocols},
  date = {2024-10-12},
  url = {https://orenbochman.github.io/posts/2024/2024-05-01-signals/shanon-game.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Emergent Complex Communications
Protocols.”</span> October 12, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-05-01-signals/shanon-game.html">https://orenbochman.github.io/posts/2024/2024-05-01-signals/shanon-game.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-05-01-signals/shanon-game.html</guid>
  <pubDate>Sat, 12 Oct 2024 15:19:59 GMT</pubDate>
</item>
<item>
  <title>Bayesian Gaussian mixture model</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-mixture-model.html</link>
  <description><![CDATA[ 





<p>this is a chart from https://en.wikipedia.org/wiki/File:Bayesian-gaussian-mixture-vb.svg by https://en.wikipedia.org/wiki/User:Benwing</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="tikz-mixture-model_files/figure-html/mixture-model-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Bayesian Gaussian mixture model"><img src="https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-mixture-model_files/figure-html/mixture-model-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" alt="Bayesian Gaussian mixture model"></a></p>
</figure>
</div>
<figcaption>Bayesian Gaussian mixture model</figcaption>
</figure>
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Bayesian {Gaussian} Mixture Model},
  date = {2024-10-12},
  url = {https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-mixture-model.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Bayesian Gaussian Mixture Model.”</span>
October 12, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-mixture-model.html">https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-mixture-model.html</a>.
</div></div></section></div> ]]></description>
  <category>tikz</category>
  <category>Bayesian Statistics</category>
  <category>mixture models</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-mixture-model.html</guid>
  <pubDate>Sat, 12 Oct 2024 14:35:31 GMT</pubDate>
</item>
<item>
  <title>Complete pooling</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-complete-pooling.html</link>
  <description><![CDATA[ 





<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="tikz-complete-pooling_files/figure-html/complete-pooling-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Complete pooling"><img src="https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-complete-pooling_files/figure-html/complete-pooling-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" alt="Complete pooling"></a></p>
</figure>
</div>
<figcaption>Complete pooling</figcaption>
</figure>
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Complete Pooling},
  date = {2024-10-12},
  url = {https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-complete-pooling.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Complete Pooling.”</span> October 12, 2024.
<a href="https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-complete-pooling.html">https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-complete-pooling.html</a>.
</div></div></section></div> ]]></description>
  <category>tikz</category>
  <category>game theory</category>
  <category>signaling games</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-complete-pooling.html</guid>
  <pubDate>Sat, 12 Oct 2024 14:34:49 GMT</pubDate>
</item>
<item>
  <title>Rethinking Signaling systems via the lens of compositionality</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/lewis.html</link>
  <description><![CDATA[ 





<p>I was introduced to the subject of language evolution by Brian Skryms in his book “Signals: Evolution, Learning, and Information” where he discusses the evolution of signaling systems and the emergence of language. In it he discusses the role of compositionality in the emergence of language and how it is a key feature of human language. Signals provides a coherent yet multifaceted views of the problem - philosophy, signaling system creation and assimilation via evolution or reinforcement learning. Skryms also considers Logic and complex signaling systems. Yet a unifying theme for this work is a reductionist view of the problem and his attempt to reduce the problem to a model that follows closely the Lewis Signaling Game.</p>
<p>I like this reductionist approach but I like to also to turn it on its head. By looking at how the problem takes form in more challenging and realistic settings can often uncover the true nature of the problem. Since language emergence is so open ended one might also use it to consider how it empowers agents to coordinate on better decision in ever more challenging problems and settings.</p>
<p>I first became frustrated with complex signaling systems when I read the chapters in Signals and realized that unlike the other chapters Skryms had not summarized how researchers in the field had come up with a definitive solution to the problem. I reread it a couple of times and finally realized that although he made some very interesting claims this topic was still unresolved. There are many interesting results but there are at least as many open questions.</p>
<p>The second time I became frustrated was when I tried to convert the simple signaling RL games into complex ones. Just the material in the book had versions with multiple agents signaling in parallel, one agent signaling without sequence, and agents signaling in sequence. The book also hints at cases where agents may make mistakes and that this is important for the evolution of signaling systems.</p>
<p>I also was coming across more and more research that isn’t covered in the book that looks at morphology and syntax in the emergence of language. Further more people were using deep learning to overcome the lewis signaling game inability of of arriving decoders for complex signals.</p>
<p>At this point I realized that there might be three problems that are being conflated in nature and that we might want to consider them separately as well as together.</p>
<ol type="1">
<li>the coordination problem - how agents learn a common convention for signaling and what is the most effective form of the solution.</li>
<li>the serialization problem - how the medium will e.g.&nbsp;a noisy channel can introduce additional desireable contraints like shorter signals, saliency, early decoding, (compression, error detection and correction, easy decoding, signal distributions, ). This problem is one which is solved by a descion tree. But the different options for the settings will lead to different optimal solutions. These are hidden by the symmetric form of the rewards in the lewis game.</li>
<li>the signal composition problem - given a simple signaling systems and a encoder decoder for the channel how can we add aggregation to the signaling system to make it more efficient. (more expressive, easier to learn, easier to extend, more robust to different errors.)</li>
</ol>
<p>This might help answer questions like - why does english use just 39-44 phonems instead of the full we have a languages making a full use of human phonemes (600 consonants and 200 vowels) ?</p>
<p>What became apparent to me is that the nature of a complex signaling system, depends very much on the game being played by the agents.</p>
<p>Metrics:</p>
<ul>
<li><p>Total number of signals</p></li>
<li><p>Minimal set of signals needed to learn the signalling system with n-learners with full observability of signal, action and reward by all learners.</p></li>
<li><p>How long to learn saliency (the distribution states of the world) of signals perhaps adjusted by risk (the distribution of malleuses for wrong action in the each state of the world)</p></li>
<li><p>How long to coordinate on a basic system with N states and N actions</p></li>
<li><p>How to learn to coordinate on a huffman cannonical code to optimize a signaling system</p></li>
<li><p>Learning and Coordinating via templates for complex signals</p>
<ul>
<li>Degree of morphology</li>
<li>Degree of syntax</li>
<li>Degree of contextual meaning</li>
<li>Degree of coordination (and aggreement in templates) and its error correction capacity)</li>
</ul></li>
<li><p>Message entropy</p></li>
<li><p>Robustness to error in sender, receiver, and channel</p></li>
<li><p>Mean</p></li>
<li><p>Regarding complex signaling systems he points out a couple of ideas:</p>
<ul>
<li>Complex signals might be composed by simple signals from multiple senders.
<ul>
<li>The reciever needs to both decode and aggregate the simple signals to infer the state of the world encoded in the complex signal.</li>
<li>This is particularly interesting and less artificial once consider realize it leads to a partially observed markov decision process (PMDP).
<ul>
<li>senders have partial observability of the state of the world and</li>
<li>recievers need to reconstruct the state by aggregating partial messages</li>
</ul></li>
<li>If we might also give the agents types and make the game a bayesian game.
<ul>
<li>Types are
<ul>
<li>Knights - with messages that are always true as well as thier atoms</li>
<li>Knaves - with messages that are always false.</li>
<li>Normals - with messages that are sometimes true and sometimes false.</li>
<li>Insane - who think that thier messages are always true but are actually always false.</li>
<li>etc</li>
</ul></li>
</ul></li>
</ul></li>
<li>Complex signals might be composed by multiple simple signals from a single sender.
<ul>
<li>The complex signal is a bag of signals (i.e.&nbsp;aggregation is not unordered - buy via a conjunction of signals i.e.&nbsp;A and B = B and A).</li>
<li>The complex signal is ordered sequence of signals sequence of signals i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?(A,B)%20%5Cneq%20(B,A)"> .</li>
<li>Sequences of sequences can capture morphology.</li>
<li>Natural language adds the notion of recursion - which in terms of mathematically boils down to a a partial ordering of simple signals to form complex signals.</li>
</ul></li>
</ul></li>
<li><p>There is a natural tendency to think about the Chomsky hierarchy of languages at this point.</p></li>
<li><p>Also once there are sequences of signals we will naturally consider ideas from information theory.</p>
<ul>
<li>Entropy of a signal</li>
<li>Error detection</li>
<li>Error correction</li>
<li>Source coding (compression)</li>
<li>Easy decoding of messages</li>
</ul></li>
<li><p>Errors are stated as important in the evolution of signaling systems in the paper of Nowak and Krakauer (1999).</p>
<ul>
<li>we</li>
</ul></li>
<li><p>Compression and easy decoding are also important too but this came up later when people noticed that thier agents were learning very inefficient signaling systems (with very long signals)</p>
<ul>
<li>this suggests that we add a parameter to the game to penalize long signals.</li>
<li>and to reward early decoding of the signal.</li>
</ul></li>
<li><p>Logic is also discussed in the</p></li>
</ul>
<p>has an extensive bibliography and I have been following up on some of the references.</p>
<p>This is a quick summary of a talk by Marco Baroni on the topic of compositionality in language. In it he outlines some of his work and his collegues/students work on the topic and the conclusions he has drawn from it.</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Rethinking {Signaling} Systems via the Lens of
    Compositionality},
  date = {2024-10-12},
  url = {https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/lewis.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Rethinking Signaling Systems via the Lens of
Compositionality .”</span> October 12, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/lewis.html">https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/lewis.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/lewis.html</guid>
  <pubDate>Sat, 12 Oct 2024 14:29:38 GMT</pubDate>
</item>
<item>
  <title>lewis game</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-lewis-signaling-games.html</link>
  <description><![CDATA[ 





<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="tikz-lewis-signaling-games_files/figure-html/complete-pooling-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Extensive form 3 x 3 Lewis signaling game"><img src="https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-lewis-signaling-games_files/figure-html/complete-pooling-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" alt="Extensive form 3 x 3 Lewis signaling game"></a></p>
</figure>
</div>
<figcaption>Extensive form 3 x 3 Lewis signaling game</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="tikz-lewis-signaling-games_files/figure-html/tree-lewis-game-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Extensive form 2x2 Lewis signaling game"><img src="https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-lewis-signaling-games_files/figure-html/tree-lewis-game-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" alt="Extensive form 2x2 Lewis signaling game"></a></p>
</figure>
</div>
<figcaption>Extensive form 2x2 Lewis signaling game</figcaption>
</figure>
</div>
</div>
</div>
<p>Here the two information sets linking s_1 instances and s_2 instances indicate that the receiver cannot distinguish between the two states.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="tikz-lewis-signaling-games_files/figure-html/tree-lewis-game-take2-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Extensive form 2x2 Lewis signaling game"><img src="https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-lewis-signaling-games_files/figure-html/tree-lewis-game-take2-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" alt="Extensive form 2x2 Lewis signaling game"></a></p>
</figure>
</div>
<figcaption>Extensive form 2x2 Lewis signaling game</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="tikz-lewis-signaling-games_files/figure-html/tree-lewis-game-3by3-1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Extensive form 3x3 Lewis signaling game"><img src="https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-lewis-signaling-games_files/figure-html/tree-lewis-game-3by3-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%" alt="Extensive form 3x3 Lewis signaling game"></a></p>
</figure>
</div>
<figcaption>Extensive form 3x3 Lewis signaling game</figcaption>
</figure>
</div>
</div>
</div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Lewis Game},
  date = {2024-10-11},
  url = {https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-lewis-signaling-games.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Lewis Game.”</span> October 11, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-lewis-signaling-games.html">https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-lewis-signaling-games.html</a>.
</div></div></section></div> ]]></description>
  <category>tikz</category>
  <category>game theory</category>
  <category>signaling games</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-05-02-signaling-games-tikz/tikz-lewis-signaling-games.html</guid>
  <pubDate>Fri, 11 Oct 2024 12:52:23 GMT</pubDate>
</item>
<item>
  <title>Four ways to a signaling system</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-05-01-signals/lewis-games-in-different-settings.html</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>I consider a number of different settings for simlpe an complex signaling games.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./mixture-tikz.tex" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="tikz"><embed src="./mixture-tikz.tex" class="center img-fluid"></a></p>
<figcaption>tikz</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./lewis-sig-tikz.tex" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="lewis signaling game"><embed src="./lewis-sig-tikz.tex" class="center img-fluid"></a></p>
<figcaption>lewis signaling game</figcaption>
</figure>
</div>
<section id="introduction" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>In The book signals <span class="citation" data-cites="skyrms2010signals">(Skyrms 2010)</span> the author, Skryms, discusses how Lewis challenged the skepticism of his advisor Quine regarding the meaning and convention may arise via an arbitrary mechanism like symmetry breaking.</p>
<div class="no-row-height column-margin column-container"></div><p>When I considered solving some additional issues surrounding the fundamentals of signaling systems I realized that I had a few different scenarios in mind and that writing them down with some semblance of formalism might be helpful. It turns out that indeed this turns out to be a stepping stone towards developing an optimal algorithms for learning signaling system in different rl settings.</p>
<p>Let’s face it under different settings the task of acquiring a signaling system can be easier or harder. In <span class="citation" data-cites="skyrms2010signals">(Skyrms 2010)</span> the author points out that at symmetry breaking all the different signaling systems that could be learned are equivalent. However if there is an asymmetry in the form of a non-uniform distribution of states or different signaling risks then we we might prefer some signaling systems over others and there might even be a unique optimal signaling system. Furthermore like in reality one would expect that with time distributions of states might change and the optimal signaling system might change as well.</p>
<div class="no-row-height column-margin column-container"><div id="ref-skyrms2010signals" class="csl-entry">
Skyrms, Brian. 2010. <span>“<span class="nocase">14512 Complex Signals and Compositionality</span>.”</span> In <em><span class="nocase">Signals: Evolution, Learning, and Information</span></em>. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780199580828.003.0013">https://doi.org/10.1093/acprof:oso/9780199580828.003.0013</a>.
</div></div></section>
</section>
<section id="the-evolution-of-signaling-systems" class="level2">
<h2 class="anchored" data-anchor-id="the-evolution-of-signaling-systems">The evolution of signaling systems</h2>
<p>In this section I want to address some of the questions that drive my research on signaling systems.</p>
<section id="when-do-we-expect-signaling-systems-to-evolve" class="level3">
<h3 class="anchored" data-anchor-id="when-do-we-expect-signaling-systems-to-evolve">When do we expect signaling systems to evolve?</h3>
<p>When agents fitness is increasingly predicated on coordination or communication they will get a benefit for evolving signaling systems. I.e. a evolutionary pressure to communicate will lead to the evolution of signaling systems.</p>
</section>
<section id="what-are-the-main-desiderata-for-signaling-systems" class="level3">
<h3 class="anchored" data-anchor-id="what-are-the-main-desiderata-for-signaling-systems">What are the main desiderata for signaling systems?</h3>
<p>Here are some of the main desiderata for signaling systems:</p>
<ul>
<li><strong>Efficiency</strong> - the signaling system should be as short as possible.</li>
<li><strong>Salience</strong> - the signaling system should be most salient for the distribution of states.</li>
<li><strong>Cost</strong> - the signaling system should be as cheap as possible to learn and use.</li>
<li><strong>Robustness</strong> - the signaling system should be robust to noise and deception.</li>
<li><strong>Adaptability</strong> - the signaling system should be able to adapt to changes in the distribution of states.</li>
<li><strong>Compositionality</strong> - the signaling system should be able to be combined with other RL activities to form
<ul>
<li>more complex signaling system.</li>
<li>more complex policies.</li>
</ul></li>
</ul>
<p>This is most clearly illustrated in:</p>
<ul>
<li>The <strong>predation scenario</strong> where
<ul>
<li>Agent’s short term survival is predicated on their ability to respond to signals indicating the presence of predators by take the appropriate precautions. Of course signals need a source.</li>
<li>Agents can send a signals for the state they perceive or to stay mute.</li>
<li>Agents can repeat signals they receive or stay mute.</li>
<li>As predation increases, selection pressure may induce signaling systems to evolve.</li>
</ul></li>
<li>The <strong>Dowery/Courtship scenario</strong> where:
<ul>
<li>The game can be cooperative or competitive.
<ul>
<li>In the competitive case only the fittest agents get a mate.</li>
<li>In the cooperative case all agents get to mate but some will mate more often, or with more desirable mates.<br>
</li>
</ul></li>
<li>Agent must collect resources (e.g.&nbsp;a bill of goods for a dowery) before they can reproducing from a changing landscape.</li>
<li>Only the top n dowries will generate an offspring. (bills of goods slowly perish but the size and diversity of is important).</li>
<li>Alternatively only the agent that is the the best at courtship n times can generate an offspring. (this time there are smaller bills of good that quickly perish)</li>
<li>Resources are plentiful but evanescent.</li>
<li>Agent that can signal would be able to collect a dowery faster and increase thier fitness.</li>
<li>As competition increases benefits signaling systems should evolve.</li>
<li>This is interesting as the exploration/exploitation dilemma caps the rate at which agents can reproduce. Yet signaling will allow agents to over come this cap.</li>
<li>This is also a case where agents may get a benefit from sending false signals if the receiver is a serious contender. So that the receiver will waste time and resources.</li>
<li>The agents must learn to discriminate To handle deception agents may also develop a model of the mind of the sender to predict the likelihood of deception. They may also want to tally if the sender has been deceptive in the past.</li>
<li>Or</li>
</ul></li>
<li>The <strong>Knights &amp; Knaves</strong> scenario where:
<ul>
<li>Agents need to:
<ol type="1">
<li>Classify agent by type. (knight or knave, monkey, insane, etc.) to interpret the semantics of their signals.</li>
<li>Assemble the state from messages with different semantics to recover the state of the world.</li>
</ol></li>
<li>This scenario does assumes the agents have an underlying motivation to learn to signal.</li>
<li>And now add a selection pressure on the evolution of basic logic and semantics.</li>
</ul></li>
</ul>
<p>Agents that communicate can spend less time exploring and more time exploiting. . In this case the agents will evolve a signaling system that is most salient for the distribution of states. This is the most likely scenario for the evolution of signaling systems. The reason why agents might want to learn a signaling system is to maximize their fitness</p>
<ul>
<li>What are the main parameters that affect the learning of signaling systems?
<ul>
<li>state distribution (these are the states of the world and signaling is used to share these states with others to maximize fitness - the expected progeny)</li>
<li>saliency distribution (weights for states ranking thier risk)</li>
<li>voracity of senders.</li>
<li>cost of signaling (risk of predation).</li>
</ul></li>
<li>What are the different settings for learning signaling systems?</li>
</ul>
<p>Some other questions within these contexts might be:</p>
<ul>
<li>What are the number of signaling systems for a given number of states and actions?</li>
<li>What are the number of pooling equilibria for a given number of states and actions?
<ul>
<li>Let’s break these down by the degeneracy of the pooling equilibrium. This might suggest the minimal number of signals needed in an experiment to learn the signaling system. It might also suggest the thresholds of success for optimal signaling systems in different settings.</li>
</ul></li>
<li>Can we estimate the regret for different RL algorithms ?
<ul>
<li>What is the expected signaling success for each of the above?</li>
<li>What is the expected and the mean number of steps to acquire a signaling system for a given number of states and actions under different settings?</li>
</ul></li>
<li>How does having more senders or receivers affect the above?
<ul>
<li>What is the complexity of n-agents to come up with a common signaling system?
<ul>
<li>under full communication</li>
<li>under partial communication</li>
</ul></li>
</ul></li>
<li>How does locality affect the time to a universal signaling systems?
<ul>
<li>if there is full observability</li>
<li>if communications are one to one</li>
<li>if communication are different neighborhood, Von Neuman, Moore, hexagonal, other lattices, chains, rings, random graphs. (need to use optimal dynamics)</li>
</ul></li>
</ul>
<p>Another question that like a lemma on time needed for an agent to become experienced enough to setup an optimal signaling system?</p>
<ul>
<li><p>Given distribution S of states with k states and some the rarest state <img src="https://latex.codecogs.com/png.latex?s'"> having probability <img src="https://latex.codecogs.com/png.latex?p(s')%20=%20%5Calpha"> what is the expected number of observations needed for agents to approximate the distribution of states to within some credible interval <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%3C%5Calpha">?</p></li>
<li><p>Note while there is no lower bound on alpha the upper bound is <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%201/k"> for a uniform distribution of states. I think this is the Bayesian version of an empirical distribution. This would be a waiting time for becoming experienced.</p></li>
<li><p>After this waiting time a steady state distribution should be known to all agents.</p></li>
</ul>
<p>Under partial observability the agents need to cooperate to learn the signaling system in a distributed manner. If the agents are on a grid or on a graph what are the bounds on coordination time for learning the signaling system - using a gossip protocol - i.e.&nbsp;each agent can only communicate with its neighbors - using a broadcast protocol - i.e.&nbsp;each agent can communicate with all other agents - using a random walk protocol - i.e.&nbsp;each agent can communicate with a random agent - using a central coordinator - i.e.&nbsp;each agent can communicate with a central coordinator - using an ambassador - i.e.&nbsp;each agent can communicate with an ambassador who can communicate with many other agents per Ramzey’s theory</p>
<p>While reviewing a paper of this subject I had realized that there are a number of hypothetical scenarios for signaling systems to arise.</p>
<p>In RL we have different setting for learning optimal strategies. Some of theres different scenarios can be framed in this form.</p>
<p>I wanted to list them here so I can reference them later</p>
<p>But thinking as I list these I notice that some provide an easy solutions to problems that others don’t.</p>
<p>One point of interest. If the agents are concerned with picking the right action for each state, they should collapse any states which share the same optimal action into a single signal. This will reduce the number of signals they must be learned and reduce the overall message length and cost of signaling. So in reality we should not be overly concerned with the number of actions exceeding the number of states.</p>
<p>When there are not enough signals agent need to learn to aggregate signals.</p>
</section>
</section>
<section id="the-oracle-of-saliency" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-oracle-of-saliency">1. The Oracle of Saliency</h2>
<p>In many situation having some shared</p>
<p>There is a distribution of the states of the word known to all players.</p>
<ul>
<li>In the easiest case each state has a different probability of occurring. -It is easiest because all players can infer a <code>canonical signal system</code> from such a distribution of states.
<ul>
<li>They order states and corresponding actions in decreasing expected value. The canonical system is the one mapping between the states and the actions.</li>
<li>Thus the salience distribution breaks the symmetry of all viable signaling systems and leaves just one option.<sup>1</sup></li>
</ul></li>
<li>In each subsequently harder case there are two or more states with equal probability of occurring. These probabilistic symmetry of these states cannot be broken as before and require the use of coordination. The coordinators can break the symmetry by trial and error when that state arises. Once all the symmetries have been coordinated the players can infer the rest via the canonical signal system from the distribution of states.</li>
<li>In the worst case all states have equal probability of occurring. This is the hardest case because after each state signal pair the problem is still maximally symmetric. The players need to solve this by using trial and error.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;This is notion of a most salient mapping acts as an optimal policy for agents who need to quickly avoid the long run costs of a non salient signaling system</p></div></div><section id="learning-the-saliency-distribution." class="level3">
<h3 class="anchored" data-anchor-id="learning-the-saliency-distribution.">Learning the Saliency distribution.</h3>
<p>Another point is to consider that if agents just observe states long enough they should eventually learn to approximate the state distribution. How long would this take ?</p>
<p>If there least common state has probability <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and the agents want to know the distribution with confidence <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> they would need, according to Hoeffding’s Inequality</p>
<p><img src="https://latex.codecogs.com/png.latex?K%5Cge%5Cfrac%7Blog(2/%5Cepsilon)%7D%7B2%5Calpha%5E2%7D%20%5Cqquad%20%5Ctext%7B(samples%20to%20learn%20S)%7D"></p>
<p>also recall that although there is no lower bound on <img src="https://latex.codecogs.com/png.latex?%5Calpha"> when <img src="https://latex.codecogs.com/png.latex?S%5Csim%20Uniform%5BN%5D"> the upper bound is <img src="https://latex.codecogs.com/png.latex?1/N"></p>
<p><img src="https://latex.codecogs.com/png.latex?K%5Cge%5Cfrac%7BN%5E2log(2/%5Cepsilon)%7D%7B2%7D%20%5Cqquad%20%5Ctext%7B(samples%20to%20learn%20uniform%20S)%7D"></p>
<div id="upper_bound_estimation" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> math</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Given values</span></span>
<span id="cb1-4">K <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># states</span></span>
<span id="cb1-5">epsilon <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.34</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># confidence</span></span>
<span id="cb1-6"></span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate time to learn the saliency distribution </span></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># N using the formula N &gt;= (K^2 * log(2 / epsilon)) / 2</span></span>
<span id="cb1-10">N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (K<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> math.log(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> epsilon)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-11"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Expected time </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(N)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> to learn a </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>K<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> state distribution with confidence </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epsilon<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)  </span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Expected time to learn a signaling system with N states</span></span>
<span id="cb1-14"></span>
<span id="cb1-15">T <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> K <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> math.log(K)</span>
<span id="cb1-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Expected time </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(T)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> to learn a </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>K<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> signaling system  '</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Expected time 56 to learn a 8 state distribution with confidence 0.34
Expected time 16 to learn a 8 signaling system  </code></pre>
</div>
</div>
</section>
</section>
<section id="hoppes-urn" class="level2">
<h2 class="anchored" data-anchor-id="hoppes-urn">Hoppe’s Urn</h2>
<p>Another way to make learning easier is to always have just one action in context when we need to learn. This allows the receiver to learn the signal system in a single step. It might work with a student learning to signal and act in tandem.</p>
<p>In this case urn used in learning have an Hoppe urn with a black stone indicating that a new state action pair is being learned. If the receiver learns the new signal action pair, the agents keep track of it otherwise the new signal and action are discarded.</p>
<p>Note that if the there is only one new state and action a suitable algorithm can learn it immediately. IF there is an exploration - this may cause an error.</p>
<p>We retain this mechanism and might use it for expanding a signaling systems incrementally in the presence of new data.</p>
<p>Note: if there are saliency distributions is being used a new signal would be the last signal in the saliency distribution or in the last group. Over time signals that are not in use might be discarded if thier saliency is bellow the minimum saliency threshold.</p>
</section>
<section id="the-gurus-prior" class="level2">
<h2 class="anchored" data-anchor-id="the-gurus-prior">The Guru’s Prior</h2>
<p>The Sender is a privileged elder who knows the distribution of the states, the associated risk and cost of signaling to sender and receiver and figures our the optimal signaling systems. As such he selects a specific signaling system. This means that students need to coordinate to this system.</p>
<ul>
<li>This means that whenever the state <img src="https://latex.codecogs.com/png.latex?s_i"> arises we will get signal <img src="https://latex.codecogs.com/png.latex?sig_i=Send(s_i)"> rather then some random signal. This means that the student for a mistake the <em>receiver</em> can use a negative reinforcement for <img src="https://latex.codecogs.com/png.latex?%3Csig_i,action_j%3E"> is the return is 0. This should allow the receiver to narrow down the actions chosen for the next time we he gets that signal.</li>
</ul>
<p>This is second hardest learning scenario but also most realistic. We don’t want to have to learn a new language for every person we meet.</p>
<p>What could happen - the distribution of states could evolve over time.</p>
</section>
<section id="the-prophets-prior" class="level2">
<h2 class="anchored" data-anchor-id="the-prophets-prior">The prophet’s prior</h2>
<p>The sender knows the distribution of the states and how it evolves over time. He choses the currently optimal signaling system. The receivers must learn the signaling system but once a change in the state distribution is observed they will switch to the the new optimal signaling system.</p>
<p>Imagine a world with many predators troubling the signaler. To avoid becoming prey agents must send a risky signals to their neighbors. They should use the signaling with the least expected cost. This cost combines the predator risk and its frequency. Signals can be 1 or 0. 1 is risky and 0 is safe. As frequency of the predators change the optimal signaling system will change as well.</p>
</section>
<section id="the-gurus-posterior" class="level2">
<h2 class="anchored" data-anchor-id="the-gurus-posterior">The Gurus’ Posterior</h2>
<p>Here there are multiple gurus with knowledge of different distribution. Can they coordinate on the most salient signaling system with respect to thier common knowledge ?</p>
<p>This should be the signaling system that is most salient for a mixture distribution with weight <img src="https://latex.codecogs.com/png.latex?w_i"> for each guru.</p>
<p>Lets perhaps assume that there are a very large N and a cutoff <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> probability for which the gurus won’t bother to include rare sates.</p>
<p>In the second setting two or more students must come up with any signaling systems as fast as possible.</p>
</section>
<section id="babylon-consensus" class="level2">
<h2 class="anchored" data-anchor-id="babylon-consensus">Babylon Consensus</h2>
<p>Multiple senders and receivers take shelter in common ground and need to arrive at a common signaling system.</p>
<ol type="1">
<li>They can want to learn the least costly signaling system in terms of learning.</li>
<li>They want to learn the most salient signaling system in terms of the distribution of states.
<ol start="3" type="1">
<li>There is an agent who knows the current distribution of states and the optimal signaling system.</li>
<li>There isn’t such an agent but the senders want to use a</li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cost of learning a second dialect
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>for each agent and for each signal that is different from the target signalaling system add a cost of 1.</li>
</ol>
<p><span id="eq-cost"><img src="https://latex.codecogs.com/png.latex?%0AC%20=%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Csum_%7Bj=1%7D%5E%7BM%7D%20%5Cdelta_%7Bij%7D%20%5C%5C%0A%5Ctag%7B1%7D"></span></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cdelta_%7Bij%7D"> is 1 if the signal <img src="https://latex.codecogs.com/png.latex?j"> is different from the target signal for state <img src="https://latex.codecogs.com/png.latex?i"> and 0 otherwise.</p>
</div>
</div>
</section>
<section id="pomdp" class="level2">
<h2 class="anchored" data-anchor-id="pomdp">POMDP</h2>
<p>In this settings one or multiple senders only a partial state.</p>
<p>Again we consider a hypothetical case where the state describe predators and that it can be partitioned into disjoint parts like &lt;type, proximity&gt; or &lt;type, proximity, number&gt; or &lt;type, proximity, number, direction&gt;. This partioning is also at the basis of compositionality in signaling systems.</p>
<p>Skyryms first considers three different settings.</p>
<ol type="1">
<li><strong>observation one of mutually exclusive partition:</strong> the case where each sender views one part of the partitioned state.</li>
<li><strong>observation of all mutually exclusive partition</strong> the case where senders see all the parts of the state but don’t have a mechanism in place to coordinate who sends which part of the state.</li>
<li><strong>observations of all mutually exclusive partition with coordination</strong> the case where one sender see all the parts of the state but lacks symbols to send the full state and needs to send each part. He must send the parts one at a time resulting in a sequence of signals.</li>
</ol>
<p>In the first settings the receiver somehow knows that he should first aggregate the signals using a logical and then decode the state.</p>
<p>In the first settings</p>
<p>where the agent again observe the full state but don’t have a a coordination mechanism for picking differnt parts of the message.</p>
<p>They send a partial signal to the receiver who must infer the state and take the appropriate action. The receiver must</p>
<ol type="1">
<li>aggregate the messages</li>
<li>infer the state</li>
<li>take the appropriate action</li>
</ol>
<p>note:</p>
<p>In the first case so long as each part of the state is a unique signal the state can be infered by the reciever using conjunction. The second case if more problematic and shows us a new way that some signaling systems can be better then others.</p>
<p>part the agent can’t infer the state better then chance. However reinforcement of random partition the senders can learn to send they both need to learn a decorelated partition for each state the state and send different parts of the state. The issues is if the semantics are composeable.</p>
<ul>
<li>An issue here is that there is no guarantte that the senders will send the same part of the state at each turn. If the aggregation rules is conjunction, i.e.&nbsp;logical and, then the receiver will be able to decode the state so long as he gets all the pieces.</li>
</ul>
</section>
<section id="bayesian-adversarial-signaling" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-adversarial-signaling">Bayesian Adversarial Signaling</h2>
<p>There are multiple senders and each state is known to more than one sender. Each sender has a voracity parameter <img src="https://latex.codecogs.com/png.latex?%5Cnu">, this is the probability that they send a faithful signal. At one extreme senders make small mistakes and at the other they are completely deceptive. At the extreme the agents have types (like knights and knaves) and the receivers must learn to classify the agents by type and then learn the signaling system. Agents need to learn a</p>
</section>
<section id="babbling-bayesian-babies" class="level2">
<h2 class="anchored" data-anchor-id="babbling-bayesian-babies">Babbling Bayesian Babies</h2>
<p>Babies in the babbling stage of language development are learning to signal. They are sending all possible phonemes and the parents and thier parents either respond or talk to each other. The babies are collecting the feedback and reinforecing both poitively and negatively until they only use the phonemes that are in the language of thier parents. They start with over 300 phonemes and end up with 40-50.</p>
<p>In this scenario the sender operates at random. Both the sender and the receiver must observe the rewards and reinfoce state signal action triplets.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Four Ways to a Signaling System},
  date = {2024-10-10},
  url = {https://orenbochman.github.io/posts/2024/2024-05-01-signals/lewis-games-in-different-settings.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Four Ways to a Signaling System.”</span>
October 10, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-05-01-signals/lewis-games-in-different-settings.html">https://orenbochman.github.io/posts/2024/2024-05-01-signals/lewis-games-in-different-settings.html</a>.
</div></div></section></div> ]]></description>
  <category>signaling systems</category>
  <category>lewis signaling game</category>
  <category>reinforcement learning</category>
  <category>bayesian games</category>
  <category>information theory</category>
  <category>game theory</category>
  <category>bayesian reinforcement learning</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-05-01-signals/lewis-games-in-different-settings.html</guid>
  <pubDate>Thu, 10 Oct 2024 11:04:27 GMT</pubDate>
</item>
<item>
  <title>ad hoc complex signaling systems</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-05-01-signals/complex-signals.html</link>
  <description><![CDATA[ 





<p>Rather them consider how complex signaling systems evolve from a lewis signaling game plus some modifications it might be worth while to better understand some complex signaling systems.</p>
<p>Essentially One would equip the agents with a set of complex signals and see if they can acquire more powerful signaling system to communicate more effectively.</p>
<p>This should allow us to quantify:</p>
<ol type="1">
<li>the expressivity of different features of complex signaling systems.</li>
<li>the complexities of learning</li>
<li>the complexities of avoiding deception…</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What is compositionality in signaling systems?
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Given a rudimentary signaling system how can we use it to construct and learn a more complex signaling systems?</li>
<li>Once we have this two step process we can then consider the complex signaling system as a single unit and see if it can be learned directly ?</li>
<li>After we have done it a few times we can we generalize the process to signaling systems with desiderata similar to natural languages?</li>
<li>Can we specialize signaling systems to operate with specific RL tasks</li>
<li>Can we use signaling systems as a symbolic abstraction of the environment and thus transfer learning from one environment to another?</li>
</ol>
</div>
</div>
<section id="logical-aggregation" class="level2">
<h2 class="anchored" data-anchor-id="logical-aggregation">Logical Aggregation</h2>
</section>
<section id="operators" class="level2">
<h2 class="anchored" data-anchor-id="operators">operators</h2>
</section>
<section id="learning-to-negate" class="level2">
<h2 class="anchored" data-anchor-id="learning-to-negate">Learning to negate:</h2>
<p>I suppose there are many ways to learn to negate. Let’s consider two</p>
<ul>
<li>in English. We use the word ‘Not’.</li>
<li>in logic we use the symbol <img src="https://latex.codecogs.com/png.latex?%5Cneg">.</li>
<li>in python we use the keyword ‘not’</li>
<li>in hungarian we use the word ‘nem’</li>
</ul>
<p>Not in all three cases a unitary operator that takes a single argument and returns the opposite of that argument.</p>
<p>We can use it to map the next signal to some other unique signal. This is how a unitary prefix operator works. For us though not means something more than some other signal it means all the other options. Not red means all the other colors, not cat means all the other animals. So the semantics we would like to capture requires that there are categories of signals and that the negation operator maps to the rest of the category. This is a handful. Also note that the categories may be defined as partial pooling equilibria.</p>
<p>let’s imagine that a group of Marmoset monkeys need to signal predators. The state space describes the predators are based on a product of the following features:</p>
<p>temporal : imminent, near, medium, distant type: cat, snake, pirana, eagle direction_theta: 0 1 2 3 position_phi: 0 1 2 3 number: 1, 2, 3, more</p>
<p>yes they use solid coordinates to describe to location of the predators.</p>
<p>this gives us 4^4 = 256 states.</p>
<p>that’s a lot of signals. but a complex signaling system could be able to communicate about all of them.</p>
<p>If the monkeys use a template with 4 parts to communicate about the predators then they can use just four signals.</p>
<p>also the 4 signals share common semantics of increasing values. for the animals the threat level might be used to name them …</p>
<ul>
<li><p>states <img src="https://latex.codecogs.com/png.latex?St_0:St_%7B2M%7D"></p></li>
<li><p>lew_primitives = <img src="https://latex.codecogs.com/png.latex?Sig_0:Sig_%7B2N%7D"> indicating 0…n and nor 0 … not n.</p></li>
<li><p>neg_primitives = <img src="https://latex.codecogs.com/png.latex?NOT,%20sig_0:sig_%7BN%7D"></p></li>
<li><p>prefix coding negation = &lt;NOT, neg_primitives&gt; = Sig_{n+N}</p></li>
<li><p>suffix coding negation = &lt;neg_primitives, NOT&gt;</p></li>
<li><p>prefix protocol</p></li>
<li><p>In this case we don’t have a clear benefit of suffix and prefix. but later we will see how prefix coding is a fit for the desiderata of complex signaling systems.</p></li>
<li><p>let’s consider a 2 state with negation.</p></li>
<li><p>in the lewis game we have 2 signals 0 and 1.</p></li>
<li><p>in the negation_system,</p></li>
<li><p>The semantics of negation (its meaning) can be defined as we are use to i.e.&nbsp;no 1 mean 0 and no 0 means 1. But in this case we don’t get any benefit from the negation, we just get a system with longer signals. we can interpret it as a trick we learn to double the number of symbols we can use.</p></li>
</ul>
<p>now consider a 4 symbol system with negation.</p>
<ul>
<li>A conjunctive signaling system</li>
<li>A disjunctive signaling system</li>
<li>A signaling system with conjunctions and disjunctions</li>
<li>Signaling with Run-length encoding</li>
<li>Signaling with Prefix-codes</li>
</ul>
</section>
<section id="morphology" class="level2">
<h2 class="anchored" data-anchor-id="morphology">Morphology</h2>
<ul>
<li>A signaling system with a morphological template</li>
</ul>
</section>
<section id="synatax" class="level2">
<h2 class="anchored" data-anchor-id="synatax">Synatax</h2>
<ul>
<li>A signaling system with a syntactic template</li>
<li>Signaling system with a multiple templates</li>
<li>Signaling system with a multiple templates</li>
</ul>
</section>
<section id="sequence-aggregation" class="level2">
<h2 class="anchored" data-anchor-id="sequence-aggregation">Sequence Aggregation</h2>
<ul>
<li>A Sequential signaling system with n signals</li>
<li>A matrix signaling system</li>
<li>Template signaling system</li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Ad Hoc Complex Signaling Systems},
  date = {2024-10-09},
  url = {https://orenbochman.github.io/posts/2024/2024-05-01-signals/complex-signals.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Ad Hoc Complex Signaling Systems.”</span>
October 9, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-05-01-signals/complex-signals.html">https://orenbochman.github.io/posts/2024/2024-05-01-signals/complex-signals.html</a>.
</div></div></section></div> ]]></description>
  <category>signaling games</category>
  <category>complex signaling systems</category>
  <category>compositionality</category>
  <category>communication protocols</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-05-01-signals/complex-signals.html</guid>
  <pubDate>Wed, 09 Oct 2024 07:09:31 GMT</pubDate>
</item>
<item>
  <title>Evolution of Coding in Singaling Systems</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-05-01-signals/evolution-of-coding.html</link>
  <description><![CDATA[ 





<!-- Move to review section -->
<ul>
<li><span class="citation" data-cites="sutton2018reinforcement">(Sutton and Barto 2018)</span></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-sutton2018reinforcement" class="csl-entry">
Sutton, Richard S., and Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. The MIT Press. <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.
</div></div><section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>This paper considers a setting for the evolution of a complex signaling systems. As I have already implemented such a system based on Skyrym’s work I was interested in the results. I think this is one of the papers discussed in the book by Skyrms. The main contribution is an extension of the Lewis signaling game of multiple senders that send partial binary signals to the receiver. The receiver aggregates these into a sequence ordered by sender. A complex signaling system is thus learned in which senders spontaneously learn to send a specific partial state.</p>
<p>What may seem surprising is that the senders learn to coordinate what part of the state to send even though they start out picking an independent signals.</p>
<p>My analysis shows that the agents are just undergoing a sequence of spontaneous symmetry breaking events which pick out a suitable equilibrium.</p>
<p>Criticism:</p>
<ul>
<li>The algorithms used in the paper do not learn to categorically exclude pooling equilibria.</li>
<li>Convergence is very slow, data efficiency is low</li>
<li>When the initial distribution of signals is not uniform the agents do worse rather than better.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My Research questions:
</div>
</div>
<div class="callout-body-container callout-body">
<p>Two approches to the signaling system</p>
<ol start="2" type="1">
<li>sender and receiver create the system spontaneously.</li>
<li>sender envisions a signaling system, and receiver needs to learn it. There is just one correct signaling system.</li>
<li>there are many systems and agents coordinate via a beliefs to exclude non-optimal systems.
<ul>
<li>this is better because it can handle the case where two groups of agents have learned similar systems and the best common system should emerege.</li>
<li>case when the sender dies by predation and is replaced by a new sender.</li>
</ul></li>
<li>how do we learn to coordinate without pooling equilibria?
<ul>
<li>hint: excluding already solved state action pairs.</li>
</ul></li>
<li>how fast should a data efficient algorithm converge to a signaling system?
<ul>
<li>hint: like a coupon collector problem a sum of negative binomials gives <img src="https://latex.codecogs.com/png.latex?O(n%20log%20n)"></li>
</ul></li>
<li>how can do better?
<ul>
<li>as succsses are very rare early on using negative rewards for failure can speed up learning.</li>
<li>using multiple learners can further improve convergence.</li>
<li>using source coding should help if the states are not uniformly distributed</li>
</ul></li>
<li>how can we learn the most salient signaling system for an uneven distribution of states possibly changing over time?
<ul>
<li>hint: if the urn model is directly learning the mapping this may be hard</li>
<li>hint: consider an urn scheme that votes via a belief on the most salient signaling system.
<ul>
<li>this would let the agents switch between signaling systems as the distribution of states changes. What is the overhead?</li>
</ul></li>
</ul></li>
<li>The book by Skryms suggests that different types of aggregations may lead to different signaling systems. He point out that conjunction is less powerful than concatenation.
<ul>
<li>I Found that implementing different aggregation schemes can be challenging - particularly if one has not read this paper! In fact it is hard enough to come up with variants for hard coded aggregation schemes for just the two cases above.
<ul>
<li>hint: using a FSM requries coordinating on some matrix of transitions.</li>
</ul></li>
<li>If one thinks of the desiderata for coding schemes - they should be easy to learn, easy to extend, easy to decode, and robust to errors. The easy to decode is also disrable here. I think that aggregation schemes are not very different from coding schemes once we have the ability to handle a sequence of signals.</li>
<li>Another complexity is that we could consider different settings like in RL. Here if we follow the book, we have
<ul>
<li>single sender single receiver (Lewis game)</li>
<li>multiple senders each with a disjoint partially state and a single receiver (leads to conjunctive aggregations to recover the state)</li>
<li>constrained binary signal and single receiver (full observability solves the sender decorolated coordination problem - that senders send different parts of the state)</li>
<li>multiple senders with a fully observed state</li>
</ul></li>
<li>In reality there is a whole zoo of aggregation schemes that are in use in natural languages.
<ul>
<li>template based aggregation for template based grammar and morphology (sequence of t signals)</li>
<li>tree based aggregation leading to recursive structures.</li>
<li>conjunctive aggregation for logical inference.
<ul>
<li>hint operators with noop symbol for closing implied bracket</li>
<li>sequence with nary operator + &gt; to close the context.
<ul>
<li>and ( A, B or C , D ) =&gt; and_n A or_n B C &gt; D &gt;</li>
</ul></li>
<li>encoding treess elegently using nary prefix operators
<ul>
<li>and ( A, B or C , D ) =&gt; and_3 A or2 B C D</li>
</ul></li>
<li>operators take arity as thier first argument like
<ul>
<li>and ( A, B or C , D ) =&gt; and 3 A or 2 B C D this can implement run length encoding too using</li>
<li>RLENC arity repetition input+ 0 1 2 3</li>
<li>and (aaabb aaabb aaabb) =&gt; RLENC 3 3 RLENC 3 3 a RLENC 3 2 b</li>
</ul></li>
</ul></li>
</ul></li>
<li>The issue here is that the aggregation scheme is not learned but fixed. If we were able to also learn an agreagation scheme we might understand how morphology, syntax and many other features of language emerge from a simple aggregation rule.</li>
<li>How can we learn the most efficient aggregation for a signaling system?</li>
<li>hint: using a template for the aggregation may be more efficient than learning the aggregation from scratch.</li>
</ul></li>
</ol>
</div>
</div>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Signaling games with reinforcement learning have been used to model the evolution of term languages (<span class="citation" data-cites="lewis1969convention">(Lewis 1969)</span>; <span class="citation" data-cites="skyrms2010signals">(Skyrms 2010)</span>). In this article, syntactic games, extensions of David Lewis’s original sender–receiver game, are used to illustrate how a language that exploits available syntactic structure might evolve to code for states of the world. The evolution of a language occurs in the context of available vocabulary and syntax—the role played by each component is compared in the context of simple reinforcement learning</p>
<p>– <span class="citation" data-cites="barrett2009evolution">(Barrett 2009)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-lewis1969convention" class="csl-entry">
Lewis, David Kellogg. 1969. <em>Convention: A Philosophical Study</em>. Cambridge, MA, USA: Wiley-Blackwell.
</div><div id="ref-skyrms2010signals" class="csl-entry">
Skyrms, Brian. 2010. <span>“<span class="nocase">14512 Complex Signals and Compositionality</span>.”</span> In <em><span class="nocase">Signals: Evolution, Learning, and Information</span></em>. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780199580828.003.0013">https://doi.org/10.1093/acprof:oso/9780199580828.003.0013</a>.
</div><div id="ref-barrett2009evolution" class="csl-entry">
Barrett, Jeffrey A. 2009. <span>“The Evolution of Coding in Signaling Games.”</span> <em>Theory and Decision</em> 67 (2): 223–37. <a href="https://doi.org/10.1007/s11238-007-9064-0">https://doi.org/10.1007/s11238-007-9064-0</a>.
</div></div></div>
<p>The paper starts with the standard Lewis signaling game a metric for signaling success a few learning algorithms</p>
<ul class="page-columns page-full">
<li><p>Urn Model after Richard Herstein’s matching law and with Rewards := {Succ: +1, Fail:0}</p></li>
<li><p>Urn Model after Richard Herstein’s matching law and with Rewards := {Succ: +2, Fail:-1}</p></li>
<li class="page-columns page-full"><p>Bereby-Meyer and Erev 1998 “On learning to become a successful loser”</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bereby-Meyer and Erev - “On Learning To Become a Successful Loser”
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this <span class="citation" data-cites="Bereby1998Loser">(Bereby-Meyer and Erev 1998)</span> paper the authors consider different abstraction of losses in repeated choice tasks. They use a probability learning task. (basically to estimate <img src="https://latex.codecogs.com/png.latex?p%5Cneq0.5"> in a Bernulli trial). They found that by adding constants to the payoff matrix enhanced the learning process and more significantly that learning in the loss domain was faster then in the gain domain.^[there are many games where lossess are much more common, also we might not be able to get a signal from a mistake…. ]</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-Bereby1998Loser" class="csl-entry">
Bereby-Meyer, Yoella, and Ido Erev. 1998. <span>“On Learning to Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain.”</span> <em>Journal of Mathematical Psychology</em> 42 (2): 266–86. https://doi.org/<a href="https://doi.org/10.1006/jmps.1998.1214">https://doi.org/10.1006/jmps.1998.1214</a>.
</div></div><p>Bereby-Meyer and Erev</p></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Evolution of {Coding} in {Singaling} {Systems}},
  date = {2024-10-06},
  url = {https://orenbochman.github.io/posts/2024/2024-05-01-signals/evolution-of-coding.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Evolution of Coding in Singaling
Systems.”</span> October 6, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-05-01-signals/evolution-of-coding.html">https://orenbochman.github.io/posts/2024/2024-05-01-signals/evolution-of-coding.html</a>.
</div></div></section></div> ]]></description>
  <category>signaling systems</category>
  <category>evolution of language</category>
  <category>evolutionary game theory</category>
  <category>lewis signaling game</category>
  <category>reinforcement learning</category>
  <category>bayesian games</category>
  <category>information theory</category>
  <category>game theory</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-05-01-signals/evolution-of-coding.html</guid>
  <pubDate>Sun, 06 Oct 2024 11:47:41 GMT</pubDate>
</item>
<item>
  <title></title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-10-01-alignment.html</link>
  <description><![CDATA[ 





<p>Recently I’ve been revisiting an old idea I had about creating a topological model of alignment. This was based on work on Wikipedia where many hints are available (Internal links, External links, Crosslanguage links, Wikidata, Citation, templates etc.) And where text are often translated by more often missing or different. Over time I became aware of more resources that could be used, movie subtitles, books translations, parallel corpora. However news articles and blogs are often not parallel but can contain lots of similar information.</p>
<p>If we look at the twitter feeds we can see that people often tweet the same news in different languages. This is a good source of parallel data. We can probably run this through a translation model and then use the output to learn alignment and segmentation. It should be even more usefull if we capture sequences of tweets that are about the same news item. In this case we might look at aligning or predicting emojis.</p>
<p>Ideally one would like to do unsupervised learning of alignment and segmentation. By simply deleting parts of one documents and then trying to predict the missing parts using the other document. The model would be able to learn to do this better by learning to segment and align the documents.</p>
<p>Another interesting idea is to learn ancillary representation for alignment and segmentation for each language. This is an idea i got from my work on language evolution. Instead of trying to learn the whole grammar we might try to model the most common short constructs in each language. With a suitable loss function we might might find a pragmatic representation that is useful for alignment and segmentation for a language pair. Ofcourse such representations would be useful for other tasks as well.</p>
<p>This might be much easier if we provide decent sized chunks for training. We might also first use very similar documents (from a parallel corpus) and later move to new articles or papers that are more loosely related.</p>
<p>Segmentation and Alignment are two related tasks that are often done together and in this abstract view more widely applicable than just in translation e.g.&nbsp;DNA and time series. However this post will focus primarily on translation.</p>
<p>I guess the algorithm should need to:</p>
<p>find a segment in the source, and decide if</p>
<ol type="1">
<li>there is a similar segment in the other text.</li>
<li>there are multiple segments that match. (due to mophology, metaphor, or lack of a specific word in the target language)</li>
<li>the segment is missing in the other text.</li>
<li>a conflicting segment is present in the other text.</li>
<li>if the segment is a non text segment (markup, templates, images, tables, etc.)</li>
<li>if the segment is a named entity or a place name that requires transliteration or lookup in a ‘knowledge base’</li>
</ol>
<p>The original idea was to use these hints to learn to align the documents at a rough level by providing a rough topology for each document. The open sets would be mappable to each other. They could then be concatenated to learn Latent Semantic Alignment or Latent Dirichlet Allocation.</p>
<p>Toppologies can then be refined by using cross language word models on the ssegements deemed to be similar.</p>
<p>One tool that might be available today is to use cross language word embeddings. These should allow to align the documents at a much finer level.</p>
<p>Word embeddings will often not be available for all words such as names, places, etc. This is where the hints come in. A second tool that can help here is a to lern translitiration models.</p>
<p>A second notion is to develop phrase embeddings. These could be used to better handle one to many mappings that arise from the differences in morphology between languages.</p>
<p>A second idea is that once we have alignments we can learn pooling priors for different constructs and achieve better defaults for translation.</p>
<p>The Phrase embeddings might have have combine a simple structural representation and a semantic representation. The structural representation would be used to align the phrases and the semantic representation would be used to align the words within the phrases. The semantic representation would be grounded in the same high dimensional semantic space as the word embeddings.</p>
<section id="bitext-and-alignment" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="bitext-and-alignment">BITEXT AND ALIGNMENT</h2>
<div class="page-columns page-full"><p> A bitext <img src="https://latex.codecogs.com/png.latex?B%20=%20(Bsrc%20,%20Btrg%20)"> is a pair of texts <img src="https://latex.codecogs.com/png.latex?B_%7Bsrc%7D"> and <img src="https://latex.codecogs.com/png.latex?B_%7Btrg%7D"> that correspond to each other.</p><div class="no-row-height column-margin column-container"><span class="">bitext</span></div></div>
<p><img src="https://latex.codecogs.com/png.latex?B_%7Bsrc%7D%20=%20(s_1%20,%20...,%20s_N%20)%20and%20B_%7Btrg%7D%20=%20(t_1%20,%20..,%20t_M%20)"></p>
<div class="page-columns page-full"><p>Empty elements  can be added to the source and target sentences to allow for empty alignments corresponding to deletions/insertions.</p><div class="no-row-height column-margin column-container"><span class="">Empty elements</span></div></div>
<p><img src="https://latex.codecogs.com/png.latex?(p%20%7C%7C%20r)%20=%20(s_%7Bx1%7D%20,%20..,%20s_%7BxI%7D%20)%7C%7C(t_%7By1%7D%20,%20..,%20t_%7ByJ%7D%20)"> with <img src="https://latex.codecogs.com/png.latex?1%20%E2%89%A4%20x_i%20%E2%89%A4%20N"> for all <img src="https://latex.codecogs.com/png.latex?i%20=%201..I"> and <img src="https://latex.codecogs.com/png.latex?1%20%E2%89%A4%20y_j%20%E2%89%A4%20M"> for all <img src="https://latex.codecogs.com/png.latex?j%20=%201..J"></p>
<p>An alignment A is then the set of bisegments for the entire bitext.</p>
<p>This should be a bijection, but it is not always the case.</p>
<div class="page-columns page-full"><p> bitext links <img src="https://latex.codecogs.com/png.latex?L%20=%20l_1%20,%20..,%20l_K"> which describe such mappings between elements <img src="https://latex.codecogs.com/png.latex?s_x"> and <img src="https://latex.codecogs.com/png.latex?s_y"> : <img src="https://latex.codecogs.com/png.latex?l_k%20=%20(x,%20y)"> with <img src="https://latex.codecogs.com/png.latex?1%20%E2%89%A4%20x%20%E2%89%A4%20N"> and <img src="https://latex.codecogs.com/png.latex?1%20%E2%89%A4%20y%20%E2%89%A4%20M"> for all <img src="https://latex.codecogs.com/png.latex?k%20=%201..K">. The set of links can also be referred to as a bitext map that aligns bitext positions with each other. Such a bitext map can then be used to induce an alignment A in the original sense</p><div class="no-row-height column-margin column-container"><span class="">bitext links</span></div></div>
<p>Extracting bisegments from this bitext map can be seen as the task of merging text elements in such a way that the resulting segments can be mapped one-to-one without violating any connection.</p>
<dl>
<dt>Text linking</dt>
<dd>
Find all connections between text elements from the source and the target text according to some constraints and conditions which together describe the correspondence relation of the two texts. The link structure is called a bitext map and may be used to extract bisegments.
</dd>
<dt>Bisegmentation</dt>
<dd>
Find source and target text segmentations such that there is a one-to-one mapping between corresponding segments
</dd>
</dl>
</section>
<section id="segmentation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="segmentation">Segmentation</h2>
<div class="page-columns page-full"><p> is the task of dividing a text into segments. Segmentation can be done at different levels of granularity, such as word, phrase, sentence, paragraph, or document level.</p><div class="no-row-height column-margin column-container"><span class="">Segmentation</span></div></div>
<p>For alignment, to successfully align two texts, the segments should be of the same granularity.</p>
<p>It is often fustrating to align hebrew texts with its rich morphology to english because one hebrew words frequently matches to several english words. Annotators will then segment the hebrew words with one letter in some segments, which may correspond to a english word e.g.&nbsp;a particle</p>
<p>different granularity of segmentation are:</p>
<ul>
<li>morpheme (sub-word semantic segmentation)</li>
<li>character segmentation</li>
<li>word segmentation</li>
<li>token segmentation</li>
<li>lemma segmentation (token clusters)</li>
<li>n-gram segmentation</li>
<li>phrase segmentation</li>
<li>sentence segmentation</li>
<li>paragraph segmentation</li>
<li>syntactic constituent segmentation</li>
</ul>
<p>Basic entropy/statistical tools should be useful here to identify and learn good segmentation for the different languages and possibly how to align them. I.e. where morpheme boundries lie and where clause/phrase boundries lie.</p>
<p>This is where another idea comes in, Some advanced TS models can model local behavior as well as long term behavior in a single model.</p>
<p>look into:</p>
<ul>
<li><a href="https://www.cl.uzh.ch/en/research-groups/texttechnologies/research/corpus-linguistics/paralleltreebanks/smultron.html">SMULTRON: A Multilingual Translation Memory System</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.3115/1220575.1220587">A Maximum Entropy Word Aligner for Arabic-English Machine Translation</a></li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  date = {2024-10-01},
  url = {https://orenbochman.github.io/posts/2024/2024-10-01-alignment.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. October 1, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-10-01-alignment.html">https://orenbochman.github.io/posts/2024/2024-10-01-alignment.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-10-01-alignment.html</guid>
  <pubDate>Tue, 01 Oct 2024 10:02:48 GMT</pubDate>
</item>
<item>
  <title>deduction evaluation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html</link>
  <description><![CDATA[ 





<p>goal create a deduction data-set for evaluating reasoning capabilities of a man and machine.</p>
<p>tasks:</p>
<ol type="1">
<li>learning graph based representation of arguments from a text</li>
<li>generating a text version of such a graph</li>
<li>identig roles of relations in the graph such as</li>
</ol>
<ul>
<li><p>subject, predicate, copula, quantity, quality, distribution, figure, mood, opposition, conversion</p></li>
<li><p>common sense knowledge, counterfactuals, hypotheticals, conditionals, causality, modality, necessity, possibility, probability, uncertainty, vagueness, ambiguity, contradiction, paradox, tautology, fallacy, sophism, enthymeme, analogy, dilemma, aporia, syllogism, enthymeme, paradox, proposition, argument, inference, deduction, induction, abduction.</p></li>
<li><p>term, proposition, argument, inference, fallacy, tautology, contradiction, paradox, syllogism, enthymeme, sophism, paradox, aporia, dilemma, analogy, deduction, induction, abduction aporias, finding dilemmas</p></li>
<li><p>removing the ambiguity from a text by constructing a graph then rewriting the text to be more precise.</p></li>
<li><p>graph of categories (perhaps drawn from wikidata, or extracted from a text by an LLM)</p></li>
<li><p>statements can be formed genereated from the graph using LLM (large language model)</p></li>
<li><p>we might prefer to genereate statements these using unification with spacy operating on the graph</p></li>
<li><p>use the square of opposition to formulate statements and inferences from the graph</p></li>
<li><p>generate graded deductions based on sylogisms</p>
<ul>
<li>tautologies</li>
<li>falaices with type of fallacy</li>
<li>inferences with type of inference</li>
</ul></li>
<li><p>each sylogism will need a template with designated parts of speech for each term in the sylogism</p></li>
<li><p>the arguments should be composable so that</p></li>
</ul>
<blockquote class="blockquote">
<p>All men are mortal. Socrates is a man. Therefore, Socrates is mortal.[2]</p>
</blockquote>
<p>P belongs to S P is predicated of S P is said of S</p>
<p>There are four different types of categorical sentences: universal affirmative (A), universal negative (E), particular affirmative (I) and particular negative (O).</p>
<p>A - A belongs to every B E - A belongs to no B I - A belongs to some B O - A does not belong to some B</p>
<p>a = belongs to every e = belongs to no i = belongs to some o = does not belong to some</p>
<p>Categorical sentences may then be abbreviated as follows:</p>
<p>AaB = A belongs to every B (Every B is A) AeB = A belongs to no B (No B is A) AiB = A belongs to some B (Some B is A) AoB = A does not belong to some B (Some B is not A)</p>
<p>the ten terms or parts of speech in a categorical sentence, drawn from the Organon are :</p>
<ol type="1">
<li>Subject</li>
<li>Predicate</li>
<li>Copula</li>
<li>Quantity</li>
<li>Quality</li>
<li>Distribution</li>
<li>Figure</li>
<li>Mood</li>
<li>Opposition</li>
<li>Conversion</li>
</ol>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Deduction Evaluation},
  date = {2024-09-09},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Deduction Evaluation.”</span> September 9,
2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html</guid>
  <pubDate>Mon, 09 Sep 2024 19:36:40 GMT</pubDate>
</item>
<item>
  <title>logic puzzles</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-06-11/logic puzzles.html</link>
  <description><![CDATA[ 





<p>Solve this logic puzzle step by step:</p>
<p>A man was looking at a portrait. Someone asked him, “Whose picture are you looking at?” He replied: “Brothers and sisters have I none, but this man’s father is my father’s son.” Whose picture was the man looking at?</p>
<p>A man was looking at a portrait. Someone asked him, “Whose picture are you looking at?” He replied: “Brothers and sisters have I none, but this man’s son is my father’s son.” Whose picture was the man looking at?</p>
<p>three dozen grey socks and 36 pink socks are lying in a drawer in a dark room. What is the minimum number of socks I must take out of the drawer which will guarantee that I have at least two socks of the same color?</p>
<p>A certain snail takes an hour and a half to crawl clockwise around a certain racetrack, yet when he crawls counter­ clockwise around that same racetrack it takes him only ninety minutes. Why this discrepancy?</p>
<p>If an airplane crashes right on the border of the United States and Canada, in which country would you bury the survivors?</p>
<p>A certain street contains 200 buildings. A sign-maker is called to number the houses from 7 to 207. He has to order numerals to do the job. Without using pencil and paper, can you figure out in your head how many 9’ s he will need?</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Logic Puzzles},
  date = {2024-09-09},
  url = {https://orenbochman.github.io/posts/2024/2024-06-11/logic puzzles.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Logic Puzzles.”</span> September 9, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-06-11/logic puzzles.html">https://orenbochman.github.io/posts/2024/2024-06-11/logic
puzzles.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-06-11/logic puzzles.html</guid>
  <pubDate>Sun, 08 Sep 2024 23:40:40 GMT</pubDate>
</item>
<item>
  <title>replay buffer questions</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html</link>
  <description><![CDATA[ 





<section id="replay-buffer" class="level2">
<h2 class="anchored" data-anchor-id="replay-buffer">Replay Buffer</h2>
<ol type="1">
<li>for continuous environment we should think about <strong>coverage</strong>.</li>
</ol>
<ul>
<li>given a paramertrization of the value function, for a level of generalization/discrimination we get an induced set of features. Is some set of experiences sufficent to do prediction or control.</li>
<li>if we have an estimate of the coverage can we use it to place a bound on the error of the value function.</li>
<li>can we do better if we also have an estimate <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)"> of the importance/long term probability of the states ?</li>
</ul>
<ol start="2" type="1">
<li>Traces present a highly correlated view of the state space.</li>
</ol>
<ul>
<li>How much do we need to wory about this.</li>
</ul>
<ol type="1">
<li>does replay buffer violate markov state.?</li>
</ol>
<ul>
<li>according to <a href="https://www.linkedin.com/in/shirli-di-castro/">Shirli Di-Castro Shashua</a>
<ul>
<li><a href="https://proceedings.mlr.press/v162/di-castro22a/di-castro22a.pdf">Analysis of Stochastic Processes through Replay Buffers</a></li>
<li><a href="https://arxiv.org/abs/2110.00445">Sim and Real: Better Together</a></li>
<li>the storage operation preserves the markov property</li>
<li>the sampling operation preserves the markov property</li>
<li>the mean operation om the replay buffer violates the markov property…</li>
</ul></li>
</ul>
<ol start="2" type="1">
<li>can reduce correlation between samples ?</li>
<li>can we be more stategic about what we keep in the RB</li>
</ol>
<ul>
<li>say we have a key using a <img src="https://latex.codecogs.com/png.latex?hash%5B%5Cdelta(state),%20action%5D"> neighbourhood
<ul>
<li>we can use the key to decide if to insert/replace the current buffer</li>
<li>we can use it to decide what to discard</li>
</ul></li>
<li>we can use the buffer to estimate mu(s)
<ul>
<li>might also have more info like states we did not insert or deleted.</li>
<li>if we also have mu(mu) - the state importance to decide what to keep</li>
</ul></li>
<li>do we prefer complete recent traces or many partial traces.</li>
</ul>
<ol start="4" type="1">
<li>Can we use options/skills to orgenize the buffer more effectively ?</li>
</ol>
<ul>
<li><p>we should aim to keep full options traces in the buffer</p></li>
<li><p>keep traces in &amp; out or options.</p></li>
<li><p>before and after the options.</p></li>
</ul>
<p>Think of the four room environment - there are different options to get from one room to another. they are composable. Once we have good coverage entry into the op</p>
</section>
<section id="ergodicity" class="level2">
<h2 class="anchored" data-anchor-id="ergodicity">Ergodicity</h2>
<ol type="1">
<li>in an environment is a maze and I have a one way door dividing the left side from the right parts of the maze. is this environment ergodic ?</li>
<li>If not how come we can still learn the optimal policy ?</li>
</ol>
<p>interchip dotan castro - sim to real</p>
</section>
<section id="replay-buffers--" class="level2">
<h2 class="anchored" data-anchor-id="replay-buffers--">Replay buffers -</h2>
<ul>
<li>storing sequence of states</li>
<li>State action state</li>
</ul>
<p>PMDPs</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Replay Buffer Questions},
  date = {2024-09-04},
  url = {https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Replay Buffer Questions.”</span> September
4, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html">https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html</guid>
  <pubDate>Wed, 04 Sep 2024 19:17:32 GMT</pubDate>
</item>
<item>
  <title>Fine-tune llm for Style and Grammar advice.</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html</link>
  <description><![CDATA[ 





<section id="fine-tuning-llm-for-readability" class="level1">
<h1>Fine tuning LLM for Readability</h1>
<p>Question 1: LLM are amazing - can I fine tune a state of the art LLM like Lama 3.1 with what I consider to be very high quality writing from my private library content to create a writing assistant?</p>
<p>Can I do it in a way that is aware of the writer’s style and the text domain other high level features so that the prompt can be used to condition the output this way.</p>
<p>Can I test it on wikipedia articles and see if it can improve the readability of existing and new articles?</p>
<section id="learning-to-write-well" class="level2">
<h2 class="anchored" data-anchor-id="learning-to-write-well">Learning to write well</h2>
<p>learning to write from:</p>
<ul>
<li>“Best American Science Writing” series</li>
<li>Bast papers - sourced from leading conferences</li>
<li>Authors
<ul>
<li>Oliver Sacks,</li>
<li>Natalie Angier,</li>
<li>Alan Lightman,</li>
<li>Sylvia Nasar,</li>
<li>Matt Ridley
<ul>
<li>Genome</li>
<li>Red Queen</li>
<li>Viral</li>
</ul></li>
<li>Steven Pinker
<ul>
<li>How the Mind Works</li>
<li>The Blank Slate: The Modern Denial of Human Nature</li>
<li>The Language Instinct: How the Mind Creates Language</li>
<li>The Secret Life of Verbs</li>
<li>Words and Rules</li>
<li>Hotheads</li>
</ul></li>
<li>Richard Dawkins
<ul>
<li>Selfish gene</li>
<li></li>
</ul></li>
</ul></li>
<li>Great Explianers
<ul>
<li>Richard Feynman
<ul>
<li>lectures on physics</li>
<li>QED</li>
<li>The pleasure of finding things out</li>
<li>The character of physical law</li>
<li>Surely you’re joking</li>
<li>What do you care about what other people think</li>
<li>Six easy pieces</li>
</ul></li>
<li>Levitt and dunbar
<ul>
<li>Freakonomics</li>
</ul></li>
<li>Michel Foucout
<ul>
<li>Discipline and Punish</li>
<li>The Birth of the Clinic</li>
<li>The Order of Things</li>
<li>The Archaeology of Knowledge</li>
<li>Madness and Civilization<br>
</li>
</ul></li>
<li>Leonard Susskind
<ul>
<li>Theoretical Minimum</li>
</ul></li>
<li>Richard Hawkins</li>
<li>Jared Diamond
<ul>
<li>Guns, Germs, and Steel</li>
<li>Collapse: How Societies Choose to Fail or Succeed</li>
<li>The World Until Yesterday: What Can We Learn from Traditional Societies?</li>
<li>The Invisible Hands: Top Hedge Fund Traders on Bubbles, Crashes, and Real Money</li>
<li>etc</li>
</ul></li>
<li>C.S. Lewis
<ul>
<li>A Grief Observed</li>
<li>The Problem of Pain</li>
<li>The Screwtape Letters</li>
<li>The Great Divorce</li>
<li>Mere Christianity</li>
<li>A Preface to Paradise Lost</li>
</ul></li>
<li>Eric Metaxas
<ul>
<li>Martin Luther</li>
<li>Bonhoffer</li>
<li>Discussing Mere Chritianity</li>
</ul></li>
<li>Yuval Noah Harari
<ul>
<li>Sapiens - A brief history of mankind</li>
<li>Homo Deus</li>
<li>21 Lessons for the 21st Century Audiobook</li>
</ul></li>
<li>Primo Levi
<ul>
<li>The Periodic Kingdom</li>
</ul></li>
<li>Mcluhan Marshall
<ul>
<li>The Medium Is The Massage</li>
</ul></li>
<li>Empire of the Summer moon</li>
<li>Hidden Figures</li>
<li>Art of War</li>
<li>Book of five rings</li>
<li>Adam Smith</li>
<li>The pencil</li>
<li>Dan Ariely</li>
<li>Chris Anderson
<ul>
<li>The long tail</li>
</ul></li>
<li><h2 id="plato" class="anchored">Plato</h2></li>
<li><h2 id="aristotle" class="anchored">Aristotle</h2></li>
<li>Machiavelli
<ul>
<li>The prince</li>
</ul></li>
<li>James Surowiecki
<ul>
<li>The Wisdom of Crowds</li>
</ul></li>
<li>Robert A. Caro
<ul>
<li>The Power Broker: Robert Moses and the Fall of New York</li>
<li>Working</li>
<li>Master of the Senate</li>
</ul></li>
<li>Stephen Jay Gould</li>
<li>Ian Ayres
<ul>
<li>Super Crunchers -</li>
</ul></li>
<li>Giles Milton
<ul>
<li>Nathaniel’s Nutmeg</li>
<li>D-Day The Soldiers’ Story</li>
<li>When Hitler Took Cocaine and Lenin Lost His Brain</li>
<li>Fascinating Footnotes From History</li>
<li>Churchill’s Ministry of Ungentlemanly Warfare</li>
<li>Wolfram The Boy Who Went to War</li>
<li>The Extraordinary Story of Thomas Pellow and Islam’s One Million White Slaves</li>
<li>The Stalin Affair: The Impossible Alliance that Won the War</li>
<li>Samurai William: The Englishman Who Opened Japan</li>
<li>Edward Trencom’s Nose</li>
<li>Russian Roulette - A Deadly Game: How British Spies Thwarted Lenin’s Global Plot</li>
</ul></li>
</ul></li>
</ul>
<p>etc</p>
<p>the ideas here are :</p>
<ol type="1">
<li>the primary text</li>
<li>the wikipedia article on</li>
<li>summaries</li>
</ol>
<p>where we want to focus on the primary text but and also to highlight its structure</p>
<p>The primary text has lots of words but one top level structure a few chapter level structure many paragraph level structure</p>
<p>idealy we want to learn structures:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bparagraph%7D%20%5Cto%20%20%5Ccdots%20%5Cto%20%20%5Ctext%7Btop%20level%7D%0A"> and idealy we would like to learn to research</p>
<p>i.e.&nbsp;source the ‘facts’ from reliable sources which we cite inline.</p>
<p>teach an LLM to rewrite text with high fidelity yet increase thier readability.</p>
<p>high quaity data sets:</p>
<ol type="1">
<li>wikipedia v.s. higher quality e.g.&nbsp;britanica, or others</li>
</ol>
<ul>
<li>check this isn’t the best of wikipedia</li>
<li>check the citations (is there a significant overlap we are in the ballpark)</li>
</ul>
<ol start="2" type="1">
<li></li>
</ol>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Fine-Tune Llm for {Style} and {Grammar} Advice.},
  date = {2024-09-04},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Fine-Tune Llm for Style and Grammar
Advice.”</span> September 4, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html</guid>
  <pubDate>Wed, 04 Sep 2024 14:17:38 GMT</pubDate>
</item>
<item>
  <title>LLM and the missing link</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html</link>
  <description><![CDATA[ 





<p>The missing link is my name for a set of agents that should be able to edit wikipedia or at least to significantly reduce the effort needed to contribute to wikipedia.</p>
<ul>
<li>Wikipedia has a number of task and challenges</li>
<li>Wikipedia also offers unique opportunities for learning not available elsewhere (edit histories, talk pages, etc.)</li>
</ul>
<p>tasks:</p>
<ol type="1">
<li>Wikification - use entropy maximize the entropy and mutual information of the wiki - i.e.&nbsp;choose links to other articles that are most likely to be clicked on rather than the the most most famous or like USA - which contributes no information to the reader.</li>
<li>inlining citations</li>
<li>adding missing references</li>
<li>adding missing sections across languages</li>
<li>Improving readability
<ul>
<li>most wikipedia articles are poorly written when compared with the best science writing in the world.</li>
</ul></li>
<li>Addressing biases and COI issues. [^we nay need to train the LLM on material that does not include wikipedia or to create a version that can separate wikipedia and non wikipedia material possibly using CLIP?]
<ul>
<li>with the advent of LLM we can now collect all the material in an articles Sources and use it to rewrite a more complete article and perhaps one with fewer biases.<sup>1</sup> Further more it is fairly easy to source additional material from the web and other sources and thus again allowing a second view of the the articles point of view.</li>
</ul></li>
<li>Addressing vandalism and spam - this can be learned across articles</li>
<li>Extracting wikidata from articles again this can be learned across many articles by mapping the article to the wikidata entries of the primary and secondary entities.</li>
<li>Replace low register terms with high register terms - with an eye to improving readability. One hopes that the higher register terms are more precise and less ambiguous.</li>
<li>Replace highly ambiguous terms with less ambiguous terms. The same perhaps for sentences.</li>
<li>Make use of other media - diagrams, maths, code, images, videos, maps and so on should be more than referenced in the text.</li>
</ol>


<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;LLM inherit and amplify biases from thier training material, so this aspect is an area of active research and may require some creativity</p></div></div>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {LLM and the Missing Link},
  date = {2024-09-04},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“LLM and the Missing Link.”</span> September
4, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html</guid>
  <pubDate>Wed, 04 Sep 2024 13:29:44 GMT</pubDate>
</item>
<item>
  <title>NLP with RL</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>supervised and unsupervised models are great for most NLP tasks, yet both approaches have their limitations. Supervised models require labeled data, which is usually in short supply, while unsupervised models often lack the precision needed for many applications.</p>
<ul>
<li>Large language models like GPT-3 can provide us with a way to genereate text on demand. They are weak when generating from sparse data (Hellucination) and can be biased.</li>
</ul>
<p>Perhaps RL which is able to learn from mistakes, as well as use supervised as well as unsupervised learning as representations can perhaps bridge the gaps that LLM cannot adress so well.</p>
<p>The main challanges in RL however are difficulty in Trasfer learning, or generalizing between similar and or related tasks. I believe that within the NLP domain transfer learning seems to be a bit easier than in other domains, as the representations learned from one task can be used in another task.</p>
<ul>
<li>NLP is a domain in which skills learned in one task may be transferrable into other task.</li>
<li>Multigoal learning can be used to solve this issue.</li>
<li>Meta learning can be used to learn from multiple tasks and generalize to new tasks.</li>
<li>Curriculum learning can be applied to:
<ul>
<li>identify weaknesses in the model</li>
<li>collect more samples to adress weakness</li>
<li>collect appropriate samples to correct for biases that emerge in the model.</li>
</ul></li>
<li>Evolving language from scratch using lewis siggins games and thier extentions can also be viewed as as from of Meta learning.</li>
<li>Can we define a abstract hamiltonian that can be used for energy/entropy based generation of text using the hamiltonian of the promprts/context
<ul>
<li><p>Can we define a minimalist grammar using this abstract formalism.</p></li>
<li><p>Can we define a resonant solution multiple hamiltonians that interact on differernt levels.</p></li>
<li><p>Can we make this something that is a good fit for multi-headed attention perhaps analagous to how the finit state machine morphology was simplified by understanding that the FSM can be represented as a regular expression and that the generations we bounded by the lexicon. In other words can we create a hamiltonian that introduced contraints on the generation of text using different heads of the transformer model and thereby places bounds on the computational complexity of the model?</p></li>
</ul></li>
</ul>
<p>Rewards for different task can be defined in different ways, and the reward function can be used to guide the model to learn the task. At the start the reward function seems to be the greatest unknown. I believe that this will be the most interesting part of the project and perhaps a driver for innovation.</p>
<ol type="1">
<li>Tasks and components</li>
</ol>
<ul>
<li>Creating a minimalist language model that allows for pretraining and fine tuning with small payloads of data generated by the RL agents.</li>
<li>Augmenting the LLM with sophisticated embeddings that are most amenable for transfer learning.</li>
<li></li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {NLP with {RL}},
  date = {2024-09-03},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“NLP with RL.”</span> September 3, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html</guid>
  <pubDate>Tue, 03 Sep 2024 14:44:12 GMT</pubDate>
</item>
<item>
  <title>LLM the good the bad and the ugly</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/</link>
  <description><![CDATA[ 





<section id="language-models" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="language-models">Language Models</h2>
<p>Early when I saw the first text by gpt2 I was intrigued that some of the researchers that did not get access to the early model and had to re-create the model based on just the paper reported that that thier model had ‘probabilities’ of generating all those texts given the prompt.</p>
<p>This seems to be a rather <em>weak claim</em> - after all a million blindfolded monkeys banging on type writers would have some probability of generating those texts.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/512px-Chimpanzee_seated_at_typewriter.jpg?20120413234640" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="One of a million IID monkeys at a typewriter   credit"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/512px-Chimpanzee_seated_at_typewriter.jpg?20120413234640" class="img-fluid quarto-figure quarto-figure-left figure-img" width="250" alt="One of a million IID monkeys at a typewriter   credit"></a></p>
</figure>
</div>
<figcaption>One of a million <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">IID</a> monkeys at a typewriter <br> <a href="https://commons.wikimedia.org/wiki/File:Chimpanzee_seated_at_typewriter.jpg">credit</a></figcaption>
</figure>
</div>
</div></div><p>One point to make is that the monkeys might have a higher probability of generating the text then the researcher’s model - but that is a different story.</p>
<section id="the-origen-of-seq2seq-models" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-origen-of-seq2seq-models">The origen of seq2seq models</h3>
<p>If you learned the pre LLM language modeling you would be familiar with <a href="https://en.wikipedia.org/wiki/N-gram">N-grams</a> you would be better equipped to be critical of LLMs. [N-grams]<sup>1</sup> More generally skip grams allow to model n-grams with gaps i.e.&nbsp;there are some tokens that are not specified or skipped. Another generalization was the introduction of a N-grams n representing unknown tokens.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;is an ordered sequence of tokens. It could be words, characters, or unicode code point.</p></div></div><p>The N-grams abstraction allowed for development of probabilistic models that are the basis of LLMs. However these older models had one significant limitation - they could only model a fixed number of tokens. This is because the number of possible N-grams grows exponentially with the number of tokens.</p>
<p>Related v.s. Similar</p>
</section>
</section>
<section id="the-good" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-good">The good</h2>
<p>There are a few powerful ideas in this approach.</p>
<ol type="1">
<li>Capturing co-occurrence statistics.
<ul>
<li>The word ‘the’ is followed by a noun 50% of the time.</li>
<li>Collocations may have gaps and this is where</li>
<li>Skip grams generalize N-grams by allowing for gaps in the sequence.</li>
</ul></li>
<li>Learning from positive examples is the forte of classical language models. You can learn the regular parts of a language using such a probabilistic quite fast. Unfortunately most languages are far from regular.</li>
<li>Generalizing using <a href="https://en.wikipedia.org/wiki/Smoothing">smoothing</a> - When the frequency of N-grams for some words is low enough we don’t expect to see them in a corpus of some given size. You just can’t fit all the N-grams of a given size in a corpus of a given size. but we can use shorter N-grams to estimate the probability of longer N-grams. (And this is how language models can be used to generate text). We call this process smoothing as conceptually we are filling holes in the longer N-grams probability distribution by moving some of the mass from related N-grams - to look more like the distribution of formed by combining shorter N-grams.</li>
<li>Learning negative examples. With enough data we may be infer that the absence of certain trigrams in the distribution where the associated bigrams are common isn’t due to chance but due some excluding factor. They might be linguistic or perhaps censoring. Regardless to detect get to a certain confidence level say 95% we need to see lots of bigrams and no trigram. Note though that we may have some broken english or some clumsy constructions that are in our corpus - they tend to muddy the waters and render these negative examples particularly challenging to infer. In fact it is generally easier to learn more by increasing the size of the corpus and learning more from rarer positive examples and this is what LLM do. not just from the corpus but from the language. However just as children learning to generelize have to be taught that the plural of goose is geese and not gooses, learning from positive -</li>
</ol>
<p>The problems with ngrams is that once n gets big enough and the corpus doesnt scale with it ngrams learn to model the corpus rather than the language. This is because as the ngram gets longer around the central word eventuall the contexts is specific enough that there is only one matching next ngram to for the given context - so the next word is certain.</p>
<blockquote class="blockquote">
<p>The egg hit the wall and <strong>it</strong> broke.</p>
</blockquote>
<ul>
<li>It must be the case that the egg hit the wall and it broke right.</li>
<li>Unless we are in feudal japan where internal walls are made to a large extent from rice paper on a frames.<sup>2</sup></li>
<li>We could also be dealing with a decorative egg and a glass wall.</li>
<li>Or we could be dealing with a metaphorical egg and a metaphorical wall.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;some gifted Samorai would need to catch the egg after it broke the wall to avoid <code>they broke</code></p></div></div><p>If the first scenario is correct 99.999% of the time why do we need to consider the other scenarios? The answer is best considered as a black swan problem. If we only consider the most likely scenarios we will be unprepared for the unlikely ones which could be catastrophic.</p>
<p>This suggests perhaps that while LLM should be great for learning a lexicon, a grammar, and some common sense knowledge - three very challanging tasks they are inadquate for making infrences about the world where different types of precise reasoning is required.</p>
</section>
<section id="the-bad" class="level2">
<h2 class="anchored" data-anchor-id="the-bad">the bad</h2>
<ul>
<li>the black swan problem</li>
<li>tokenization</li>
</ul>
</section>
<section id="the-sad" class="level2">
<h2 class="anchored" data-anchor-id="the-sad">the sad</h2>
<ul>
<li>context windows</li>
</ul>
</section>
<section id="the-ugly---where-are-llms-no-good" class="level2">
<h2 class="anchored" data-anchor-id="the-ugly---where-are-llms-no-good">the ugly - Where are LLMs no good?</h2>
<p>Let’s consider an analogy from physics. Classical physics is great for predicting phenomena at macro scales but quantum mechanics is required for the micro scale.</p>
<p>Physicist like to think that quantum physics should converge to classical physics at the macro scale but this is not always the case. There are phenomena that are only explained by quantum mechanics. We may soon discover more phenomena like superconductivity, quantum computers and quantum cryptography manifsting in our macro world</p>
<p>Fooled by randomness….</p>
<p>In the case of LLM there is the effect of stochasticity which is built into the models. We don’t care about this aspect so long as the model gives us good replies. But all replies are inherently stochastic. While humans might express an utternce in many ways they should be able to agree on its meaning, the facts, the options, the reasoning and so on. Neural netowrks are universal function approximators and in the case of LLMs the are approximate the LM from above which are stochastic all the way down - there is no agreement excepts on the most basic probabilities. The nlp researcher can only say that an utterance is likely to be generated by the model - with some probability. Any counter claim also has some probability.The probabilities in these cases are far more dramaicaly affected by the utterance length, word choices, grammarticality, common sense knowledge then factuallity, structured knowledge</p>
<p>This is a problem because we are used to deterministic replies from humans. We are used to deterministic replies from classical language models. We are used to deterministic replies from classical AI systems.</p>
<ul>
<li>hellucinations
<ul>
<li>where there is sparse data or the data used in training isn’t representative of the query we cannot expect the model to perform well.</li>
<li>even where there is good data - if the queries are subtle enough the stochastic nature of the model will manifest.</li>
</ul></li>
<li>prompt engineering</li>
</ul>
</section>
<section id="the-ugly---through-the-hole-in-the-coin" class="level2">
<h2 class="anchored" data-anchor-id="the-ugly---through-the-hole-in-the-coin">the ugly - through the hole in the coin</h2>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {LLM the Good the Bad and the Ugly},
  date = {2024-09-02},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“LLM the Good the Bad and the Ugly.”</span>
September 2, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/</guid>
  <pubDate>Mon, 02 Sep 2024 18:16:45 GMT</pubDate>
</item>
<item>
  <title>Is compositionality overrated? The view from language emergence</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/summary.html</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In 2020 Marco Baroni gave a talk where in which he discussed the role of compositionality in neural networks and its implications for language emergence. He also presents recent work by his group at Facebook on language emergence in deep networks, where two or more networks are trained with a communication channel to solve a task jointly. Baroni argues that compositionality is not a necessary condition for good generalization in neural networks and suggests that focusing on enhancing generalization directly may be more beneficial than worrying about the compositionality of emergent neural network languages.</p>
<p>This talk doesn’t provide us with an aristotelian definition of compositionality but a pragmatic one that Baroni used in investigation.</p>
<p>The notion of language emergence in deep networks is a fascinating yet also vague. With these caveats in mind there are a number of results that Baroni presents that are worth considering.</p>
<p>If I was initially critical of this talk and speaker I soon came to realize that this is just the kind of message that simulates thought and discussion. It is a good talk and I would recommend it to anyone interested in the topic of language emergence in deep networks. <span class="emoji" data-emoji="thumbsup">👍</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My Insights
</div>
</div>
<div class="callout-body-container callout-body">
<p>My work on this subject shows that by adding compositionality to the lewis signaling game not only significantly increases the ratio of messages to simple signals but also renders the overall systems easier to master by making it more systematic and predictable. This is greatly affected by form of aggregation used for signal composition. e.g.&nbsp;using a simple template can create a morphology that is automatic and therefore easy to master.</p>
<p>For example if we add a parameter to the game to penalize long signals and reward early decoding of the signal we can get a more efficient signaling system. This allows agents to learn a larger signaling system faster. It also has more subtle effects- perhaps the most important one is that the pooling equilibria in the lewis game which are far more common then the non-pooling equilibria which we call a signaling system can be allow us to learn more efficient signaling systems if we reinterpret these partial pooling equilibria as leading to categories of signals which are further refined within the complex signal but can lead to an early decoding of the signal.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Questions
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this subject we need to ask ourselves what are the main research questions and what are the main definitions we can make.</p>
<ul>
<li>What is language emergence?</li>
<li>What is compositionality</li>
<li>How are Neural networks used here
<ul>
<li>how were they set up?</li>
<li>what was the task loss function?</li>
</ul></li>
<li>what is generalization in this task?</li>
</ul>
<p>The big question is will learning an emergent language aid agents to achieve better generalization? - There is also talk about compositionality in RL where transfer learning is challenging. - Hinton discussed the idea of capsules in neural networks as a way to encode compositional structure in neural networks.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
slide 16
</div>
</div>
<div class="callout-body-container callout-body">
<p>in page 16 of the slides we see that agents are using a learned signaling system to very effectively communicate about noise patches.</p>
<p>skyryms has a couple of papers that can explain this using the idea of a template - this is where agents evolve a signaling systems under some regime (environment with a distribution of states assigned by nature) and under some new regime they can repurpose the old signaling system instead of learning a new signaling system from scratch.</p>
</div>
</div>
<ul>
<li>does this mean they have not learned to signal - clearly not</li>
<li>does this mean their language is wierd - nope they language is just a lexicon with a 1:1 mapping between what I call the frame game.</li>
<li>so WTF ?
<ul>
<li><p>Most people using the lewis referential game don’t really under stand the lewis game or game theory. e.g.&nbsp;deep RL algs only consider a single player game.</p></li>
<li><p>Lewis game has four types of equilibria. There are N! The signaling systems giving the best payoffs, there are lots of suboptimal with homonyms called partial pooling that give lower payoffs. There are a N complete pooling solutions that lead to almost 0 solutions. and there are also an infinite number mixed equilibria that are a mixture distribution over the other solutions.</p></li>
<li><p>Learning a good equilibrium is easy if the framing game has high fidelity. If the framing game can lead to mistakes in identifying th correct state then the RL algorithms may take longer to converge or may converge to a bad equilibrium.</p></li>
<li><p>In most MARL cases researcher we don’t care about coordination sub-task by itself but about some other task.</p>
<ul>
<li>Lets call the lewis game I call the <strong>coordination task</strong>
<ul>
<li>The Lewis game has a structure that leads to 1:1 mappings. If you want other mappings to be learned you need to do something else.</li>
<li>Let’s call the other task I call the <strong>frame task</strong> and I tacitly assume it has a reward structure.
<ul>
<li>The Framing game should not violate the payoff structure of the lewis game. I.e. lewis coordination games encode the incentive of agents too cooperate on coordination. If the symmetry of the payoffs is broken they will try to coordinate on different equilibria (c.f. battle of the sexes and the expected payoffs drop from 1 to 2/n!) and if payoffs structure is further eroded coordination the incentives for cooperation evaporate and we never learn a language.</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>recall that rationality in game theory assumes that players’ decisions are the result of maximizing their own selfish payoff functions conditional on their beliefs about the other players’ optimal behavior. In evolutionary game theory we can restate it as the decisions that will result in the greatest expected progeny (aka fitness) again given the state and other agent’s optimal behavior.</p></li>
<li><p>There are many ways rational agents can to learn to coordinate:</p>
<ul>
<li>If each agent has some shared knowledge that allows them to enumerate each state in the same order then they can use it to infer the same signaling system immediately.<br>
</li>
<li>if there is only one new action pair introduced then coordination only takes n-1 steps</li>
<li>if they both know the distribution of states, and no two states are as likely, agents can infer an signaling system.
<ul>
<li>this implies that if they can observe the state long enough they can suddenly signal with high fidelity.</li>
</ul></li>
<li>the sender can dictate a dictate a system that he knows and the receiver can learn from him or</li>
<li>the receiver can dictate a system that he knows and the sender can learn from him</li>
<li>they can also learn a system stochastically by trying signals and actions.</li>
</ul></li>
<li><p>Thus is they both have a pertained CLIP classifier, or they both learn the frame task using come classifier with shared weights they can simply send it random noise and many times and learn an empirical distribution of its categories which correspond to the state. They should have a very good signaling system at their disposal…</p></li>
<li><p>unless you realty know what you are doing you won’t get extra structure from the lewis game.</p></li>
<li></li>
<li><p>in this case the frame game learning to classify an image into thier lexicon and</p></li>
</ul></li>
<li>the lewis game is to send and recover the state i.e.&nbsp;pick the image with the same label</li>
</ul>
<p>If both agents share the same classifier the frame game is a no-brainer for this task.</p>
<p>In the lewis game agents need to send N signals and recover the state from N possible options in the referential game they need to guess one of four. If the classifier is the same then there is a very good chance that the distractors are different from the signal so the agents</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
slide 26 - 37 naive compositionality.
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section explains naive compositionality.</p>
<ul>
<li>“A L M” = Blue Banana</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Representation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>[ALM] : Blue Banana</td>
<td>Not compositional</td>
</tr>
<tr class="even">
<td>[A L] : Blue, [M] :Banana</td>
<td>Compositional</td>
</tr>
<tr class="odd">
<td>[AL] : Blue, [LM] :Banana</td>
<td>less compositional, but entangled representation.</td>
</tr>
</tbody>
</table>
<ul>
<li>The idea is that tokenizing a sequential signals should yield independently semantics. A simillar notion is that when messages are aggregated like sets, that certain subsets have this property. (the definition is vague and I would call it entangled with the aggregation)</li>
</ul>
<p>What we would like are</p>
<ol type="1">
<li>a definition of compositionality that takes a general notion of aggregation not just set and sequence. e.g.&nbsp;
<ul>
<li>prefix, suffix, v: e.g.&nbsp;[verb-prefix slot] and [slot, verb-inflection] n: e.g.&nbsp;[noun-prefix slot] and [slot, noun-inflection] adj: e.g.&nbsp;[adj-prefix slot] and [slot, adj-inflection] adv: e.g.&nbsp;[adv-prefix slot] and [slot, adv-inflection]</li>
<li>one template, [v s o]</li>
<li>many templates:
<ul>
<li>n2: [adj n] | n - is a noun phrase</li>
<li>v2: [v adv] | v - is a verb phrase</li>
<li>s1: [v] - e.g.&nbsp;run</li>
<li>s2: [v n2] - e.g.&nbsp;hit intransitive</li>
<li>s3: [v1|v,n2,n2] where v is a verb or a v2 , s and o are nouns either n or n2</li>
<li>s: s2 | s3 | v is a sentence</li>
<li>AND [s, s] - a conjunction</li>
<li>OR [s, s] - a disjunction</li>
<li>Not [S]</li>
<li>(templates can be just a prefix taking a fixed number of arguments).</li>
<li>templates prefix might be omitted if it can be inferred from the context.</li>
</ul></li>
<li>two level templates (a morphological agregation a syntax aggregation)
<ul>
<li>in a marked template we can discard the morphological markers</li>
<li>there may even be a unmarked morphological form with a null prefix</li>
<li>the template marker can be ommited if it can be inferred from the context…</li>
</ul></li>
<li>recursive templates.</li>
</ul></li>
<li>a definition of compositionality that plays well with embeddings
<ul>
<li>simple lewis games don’t have embeddings</li>
<li>complex</li>
</ul></li>
<li>a definition of compositionality adresses context sensitive grammars</li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>naive compositionality is indeed naieve. It does not capture basic aspects of what I considers to be compositionality.</p>
<p>note that a sequence like:</p>
<blockquote class="blockquote">
<p>OR ANDN A B C ] Z</p>
</blockquote>
<p>where OR Takes two arguments and ANDN takes different numbers of arguments, until it seees a closing bracket. This isn’t naively compositional as there are nested segments with different semantics. Also the ]</p>
<p>the above is a rather efficent way to encode complex signals</p>
<blockquote class="blockquote">
<p>A B C</p>
</blockquote>
<p>where A B and C are dense disjunctional embeddings (Or aggregation within the signal) and And to aggregate between signals. and that State1:3 are the most common perdation warning states out of 100 states.</p>
<p>A = [State 1, State 2, State 3]</p>
<p>B = [State 1, Not State2, State 2]</p>
<p>C = [State 1, State 2, Not State 3]</p>
<p>these might be considered less compositional as they are entangled</p>
<p>however they have the advantage allowing early partial decoding.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled page-columns page-full">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
slides 38-54
</div>
</div>
<div class="callout-body-container callout-body page-columns page-full">
<p>This section of the talk goes over the key results in <span class="citation" data-cites="chaabouni-etal-2020-compositionality">(Chaabouni et al. 2020)</span> in Baroni’s group at Facebook AI Research. I haven’t read the paper at great depth.</p>
<p>At this point I don’t think that the main results are particularly surprising not that the claims made are applicable to other framing games.</p>
<blockquote class="blockquote">
<p>First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts.</p>
</blockquote>
<p>I believe that the right settings we may see compositionality emerge in small Lewis games whose states are endowed with a structure amenable to compositionality, and that by restricting the signal will drive this capability.</p>
<blockquote class="blockquote">
<p>Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize</p>
</blockquote>
<p>I believe that once a complex signaling system is learned it can be repurposed to solve other tasks. So that a signaling system learned that is highly composable is likely to generalize to other domains. I think that with some time this can be demonstrated as it requires lots of tinkering with the lewis game to get it to support such structures as well as new RL algorithms that can learn these structures quickly.</p>
<p>This might also be a apples and oranges comparison as I may be thinking about the notions of generalization and compositionality in a different way than the authors of the paper and do not even see these as the top priorities in developing better signaling systems. (I think that they should be salient, learnable and transferable to other domains and translatable into a human readable forms.) Furthermore I don’t think the metrics used for compatibility are particularly useful. In liu of better ones is I listed all three metrics even though only the positional one is discussed in the talk. To come up with good metrics of novel ideas requires developing good intuition. I don’t have that yet for complex signaling systems.</p>
<p>Off hand my best guess has to do with integrating the lewis game with a contrastive loss that operates on structural components. Keeping members with similar structures close and members with different structures far apart. This may also be in line with the notion of Topographic similarity shown below</p>
<p>A second direction that seems to be promising is in <span class="citation" data-cites="mu2022emergentcommunicationgeneralizations">(Mu and Goodman 2022)</span> is to use a framing games in which agents refernce sets of signals, or reference so called concepts which are more like equivilence classes over states.</p>
<p>A third direction comes from <span class="citation" data-cites="rita2022emergentcommunicationgeneralizationoverfitting">(Rita et al. 2022)</span> where the authors consider how signaling systems that fail to generalize are overfitting and that this can be mitigated by decomposing the a co-adaptation loss and a information loss.</p>
<blockquote class="blockquote">
<p>Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners,</p>
</blockquote>
<p>Simple signaling systems require a large lexicon to be learned if there are many states. Complex signaling systems can reduce learning if the lexicon by allowing agents to use aggregates of simple symbols AKA a morphology. A systematic aggregation means you can learn a combinatorial explosion of possible signals. So it is no surprise that compositionality can makes signaling systems easier to learn. However this is only really helpful if the structure of the signaling systems is semantically a good match to the states… If it is not then the learner is just gets a massive list of words but needs to learn what each means.</p>
<p>For example many languages have a gender system that likely originated from the group of nouns that were used to describe people. However it is perpetuated into almost all nouns and ends up an arbitrary and a confusing system that makes learning the language harder. Bantu languages like Ganda have 10 nouns classes c.f. <a href="https://en.wikipedia.org/wiki/List_of_languages_by_type_of_grammatical_genders">Languages by type of grammatical genders</a> - clearly this is reuse of a template but not one that is the easiest to learn as now there are 7x arbitrary distinctions that need to be learned.</p>
<p>in <span class="citation" data-cites="chaabouni-etal-2020-compositionality">(Chaabouni et al. 2020)</span> the authors define the following metrics for compositionality:</p>
<p>there are a three metrics:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Topographic Similarity
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Given these two lists, the topographic similarity is defined as their negative Spearman ρ correlation (since we are correlating distances with similarities, negative values of correlation indicate topographic similarity of the two spaces). <mark>Intuitively, if similar objects share much of the message structure (e.g., common prefixes or suffixes), and dissimilar objects have little common structure in their respective messages, then the topographic similarity should be high</mark>, the highest possible value being 1. – <span class="citation" data-cites="lazaridou2018emergencelinguisticcommunicationreferential">(Lazaridou et al. 2018)</span></p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathit%7Btopsim%7D=%5Crho%5Cleft(%5Cleft%5C%7Bd%5Cleft(%5Cmathbf%7Bx%7D%5E%7B(i)%7D,%20%5Cmathbf%7Bx%7D%5E%7B(j)%7D%5Cright),%20d%5Cleft(%5Cmathbf%7Bm%7D%5E%7B(i)%7D,%20%5Cmathbf%7Bm%7D%5E%7B(j)%7D%5Cright)%5Cright%5C%7D_%7Bi,%20j=1%7D%5E%7Bn%7D%5Cright)%0A"></p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-lazaridou2018emergencelinguisticcommunicationreferential" class="csl-entry">
Lazaridou, Angeliki, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. 2018. <span>“Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input.”</span> <a href="https://arxiv.org/abs/1804.03984">https://arxiv.org/abs/1804.03984</a>.
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Positional Disentanglement
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p><strong>positional disentanglement</strong> (posdis) metric measures whether symbols in specific positions tend to univocally refer to the values of a specific attribute. This order-dependent strategy is commonly encountered in natural language structures (and it is a pre-condition for sophisticated syntactic structures to emerge) – <span class="citation" data-cites="chaabouni-etal-2020-compositionality">(Chaabouni et al. 2020)</span></p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathit%7Bposdis%7D=%5Cfrac%7B1%7D%7Bc_%7Blen%7D%7D%20%5Csum_%7Bj=1%7D%5E%7Bc_%7Blen%7D%7D%20%5Cfrac%7B%5Cmathcal%7BI%7D(s_j,a%5Ej_1)-%5Cmathcal%7BI%7D(s_j,a%5Ej_2)%7D%7B%5Cmathcal%7BH%7D(s_j)%7D%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?s_j"> the j<sup>th</sup> symbol of a message and</li>
<li><img src="https://latex.codecogs.com/png.latex?a%5Ej_1"> the attribute that has the highest mutual information with <img src="https://latex.codecogs.com/png.latex?s_j%20:%20a%5Ej_1%20=%20arg%20max_a%20%5Cmathcal%7BI%7D(s_j%20;%20a)"></li>
<li><img src="https://latex.codecogs.com/png.latex?a%5Ej_2"> the attribute that has the second highest mutual information with <img src="https://latex.codecogs.com/png.latex?s_j%20:%20a%5Ej_2%20=%20arg%20max_%7Ba%20%5Cneq%20a%5Ej_1%7D%20%5Cmathcal%7BI%7D(s_j%20;%20a)"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BH%7D(s_j)"> the entropy of j-th position (used as a normalizing term)</li>
</ul>
<p>positions with zero entropy are ignored in the computation.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bag-of-symbols Disentanglement
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Posdis assumes that a language uses positional information to disambiguate symbols. However, we can easily imagine a language where symbols univocally refer to distinct input elements independently of where they occur, making order irrelevant.3 Hence, we also introduce <strong>bag-of-symbols disentanglement</strong> (bosdis). The latter maintains the requirement for symbols to univocally refer to distinct meanings, but captures the intuition of a permutation-invariant language, where only symbol counts are informative – <span class="citation" data-cites="chaabouni-etal-2020-compositionality">(Chaabouni et al. 2020)</span></p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathit%7Bbodis%7D=%5Cfrac%7B1%7D%7Bc_%7Bvoc%7D%7D%20%5Csum_%7Bj=1%7D%5E%7Bc_%7Bvoc%7D%7D%20%5Cfrac%7B%5Cmathcal%7BI%7D(n_j,a%5Ej_1)-%5Cmathcal%7BI%7D(n_j,a%5Ej_2)%7D%7B%5Cmathcal%7BH%7D(n_j)%7D%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?n_j"> a counter of the j-th symbol in a message</li>
</ul>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-chaabouni-etal-2020-compositionality" class="csl-entry">
Chaabouni, Rahma, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni. 2020. <span>“Compositionality and Generalization in Emergent Languages.”</span> In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, edited by Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, 4427–42. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.407">https://doi.org/10.18653/v1/2020.acl-main.407</a>.
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definitions
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Simple Compositionality - The idea that the meaning of a complex signal is an aggregation the meanings of its parts.</li>
<li>Non compositionality - When each symbols or sequence is mapped to a full state.</li>
<li>Naïve compositionality - the meaning of a bag of atomic symbols or sequence.</li>
<li>Entanglement -</li>
</ul>
</div>
</div>
<ul>
<li>By the end of the talk he more or less concludes that the compositionality of emergent neural network languages is not a necessary condition for good generalization.</li>
</ul>
<p>, and there is no reason to expect deep networks to find compositional languages more “natural” than highly entangled ones. Baroni concludes that if fast generalization is the goal, we should focus on enhancing this property without worrying about the compositionality of emergent neural network languages.</p>
<section id="blurb-from-the-talk" class="level2">
<h2 class="anchored" data-anchor-id="blurb-from-the-talk">Blurb from the talk</h2>
<p>Compositionality is the property whereby linguistic expressions that denote new composite meanings are derived by a rule-based combination of expressions denoting their parts. Linguists agree that compositionality plays a central role in natural language, accounting for its ability to express an infinite number of ideas by finite means.</p>
<p>“Deep” neural networks, for all their impressive achievements, often fail to quickly generalize to unseen examples, even when the latter display a predictable composite structure with respect to examples the network is already familiar with. This has led to interest in the topic of compositionality in neural networks: can deep networks parse language compositionally? how can we make them more sensitive to compositional structure? what does “compositionality” even mean in the context of deep learning?</p>
<p>I would like to address some of these questions in the context of recent work on language emergence in deep networks, in which we train two or more networks endowed with a communication channel to solve a task jointly, and study the communication code they develop. I will try to be precise about what “compositionality” mean in this context, and I will report the results of proof-of-concept and larger-scale experiments suggesting that (non-circular) compositionality is not a necessary condition for good generalization (of the kind illustrated in the figure). Moreover, I will show that often there is no reason to expect deep networks to find compositional languages more “natural” than highly entangled ones. I will conclude by suggesting that, if fast generalization is what we care about, we might as well focus directly on enhancing this property, without worrying about the compositionality of emergent neural network languages.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p>https://cs.stanford.edu/people/karpathy/cnnembed/,</p></li>
<li><p>https://www.inverse.com/article/12664-google-s-alphago-supercomputer-wins-second-go-match-vs-lee-sedol</p></li>
<li><p>https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643</p></li>
<li><p>Lazaridou et al.&nbsp;ICLR 2017 <a href="https://arxiv.org/abs/1612.07182">Multi-Agent Cooperation and the Emergence of (Natural) Language</a></p></li>
<li><p><span class="citation" data-cites="bouchacourt2018agents">Bouchacourt and Baroni (2018)</span> <a href="https://arxiv.org/pdf/1808.10696v2">How agents see things: On visual representations in an emergent language game</a></p></li>
</ul>
<p>Are emergent languages compositional?</p>
<ul>
<li>Andreas ICLR 2019,</li>
<li>Choi et al ICLR 2018,</li>
<li>Havrylov &amp; Titov NIPS 2017,</li>
<li>Kottur et al EMNLP 2017,</li>
<li>Mordatch &amp; Abbeel AAAI 2018,</li>
<li>Resnick et al AAMAS 2020</li>
</ul>
<p>A compositional language is one where it is easy to read out which parts of a linguistic expression refer to which components of the input</p>
<dl>
<dt>Naïve compositionality</dt>
<dd>
a language is naïvely compositional if the atomic symbols in its expressions refer to single input elements, independently of either input or linguistic context
</dd>
</dl>
<ul>
<li>Chaabouni, Kharitonov et al.&nbsp;ACL 2020 <a href="https://aclanthology.org/2020.acl-main.407/">Compositionality and Generalization In Emergent Languages</a></li>
</ul>
<p>Quantifying (one type of) naïve compositionality</p>
<p><strong>Positional disentanglement</strong> measures strong form of naïve compositionality: to what extent do symbols in a certain position univocally refer to different values of the same attribute</p>
<p>note - the paper has two other measures</p>
<p>Do emergent languages support generalization?</p>
<p>Is compositionality needed for generalization?</p>
<ul>
<li>kind of obvious, particularly if parameters are shared not but is should help</li>
</ul>
<p>Lazaridou et al ICLR 2018</p>
<ul>
<li><p><a href="https://github.com/facebookresearch/EGG">EGG: Emergence of lanGuage in Games</a></p></li>
<li><p>Kharitonov and Baroni: Emergent Language Generalization and Acquisition Speed are not Tied to Compositionality <a href="https://arxiv.org/abs/2004.03420">Emergent Language Generalization and Acquisition Speed are not tied to Compositionality</a></p></li>
</ul>
</section>
<section id="video" class="level2">
<h2 class="anchored" data-anchor-id="video">Video</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/mi1q3Fbm9zg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="slides-and-a-paper" class="level2">
<h2 class="anchored" data-anchor-id="slides-and-a-paper">Slides &amp; and a paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="slides"><embed src="./slides.pdf" class="col-page" width="1000" height="800"></a></p>
<figcaption>slides</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Linguistic generalization and compositionality in modern artificial neural networks"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Linguistic generalization and compositionality in modern artificial neural networks</figcaption>
</figure>
</div>


</section>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-bouchacourt2018agents" class="csl-entry">
Bouchacourt, Diane, and Marco Baroni. 2018. <span>“How Agents See Things: On Visual Representations in an Emergent Language Game.”</span> <em>arXiv Preprint arXiv:1808.10696</em>.
</div></div><div class="no-row-height column-margin column-container"></div><div class="no-row-height column-margin column-container"><div id="ref-rita2022emergentcommunicationgeneralizationoverfitting" class="csl-entry">
Rita, Mathieu, Corentin Tallec, Paul Michel, Jean-Bastien Grill, Olivier Pietquin, Emmanuel Dupoux, and Florian Strub. 2022. <span>“Emergent Communication: Generalization and Overfitting in Lewis Games.”</span> <a href="https://arxiv.org/abs/2209.15342">https://arxiv.org/abs/2209.15342</a>.
</div></div><div class="no-row-height column-margin column-container"><div id="ref-mu2022emergentcommunicationgeneralizations" class="csl-entry">
Mu, Jesse, and Noah Goodman. 2022. <span>“Emergent Communication of Generalizations.”</span> <a href="https://arxiv.org/abs/2106.02668">https://arxiv.org/abs/2106.02668</a>.
</div></div><div class="no-row-height column-margin column-container"></div>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Is Compositionality Overrated? {The} View from Language
    Emergence},
  date = {2024-09-01},
  url = {https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/summary.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Is Compositionality Overrated? The View from
Language Emergence.”</span> September 1, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/summary.html">https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/summary.html</a>.
</div></div></section></div> ]]></description>
  <category>draft</category>
  <category>review</category>
  <category>compositionality</category>
  <category>neural networks</category>
  <category>signaling systems</category>
  <category>language evolution</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-10-10-marco-baoni-composionality/summary.html</guid>
  <pubDate>Sat, 31 Aug 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Six quick tips to improve modeling</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-08-26-six-quick-tips/</link>
  <description><![CDATA[ 





<p>The following tips are from <span class="citation" data-cites="GelmanHill2007Regression">(Gelman and Hill 2007)</span> Data Analysis Using Regression and Multilevel/Hierarchical Models by <a href="http://www.stat.columbia.edu/~gelman/">Andrew Gelman</a> and <a href="https://steinhardt.nyu.edu/people/jennifer-hill">Jennifer Hill</a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-GelmanHill2007Regression" class="csl-entry">
Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Vol. Analytical methods for social research. New York: Cambridge University Press.
</div></div><ol type="1">
<li>Fit many models:
<ul>
<li>Start with a baseline</li>
<li>Add more effects</li>
<li>Add interactions</li>
<li>Add non-linearities</li>
<li>Add more levels</li>
<li>Use different priors</li>
<li>Use different algorithms (each has its own inductive biases)</li>
</ul></li>
<li>Do a little work to make your computations faster and more reliable
<ol type="1">
<li>data subsetting is faster than full data set</li>
<li>redundant parameterization (e.g.&nbsp;re-centering and scaling) add some parameters but don’ really change the model yet make it more computationally stable by improving the geometry the sampler has to work with. 3 fake data and predictive simulations help understand if the problems we are facing are due to the model or the data. Fake data creates a version of the data for which we know what to expect.</li>
</ol></li>
<li>Graphing the relevant
<ul>
<li>Graphing the data is fine</li>
<li>Graphing the model is more informative (regression lines and curves)</li>
<li>I like to plot lots of graphs like regression diagnostics, residuals, and posterior predictive checks. But Gelman and Hill suggest that these are not relevant and warn that one should be prepared to explain any graph you show. Some of these diagnostics graphs are kind of hard to explain. e.g.&nbsp;Cook’s distance v.s. leverage plot.</li>
</ul></li>
<li>Transformations
<ul>
<li>Logarithms of all-positive variables (primarily because this leads to multiplicative models on the original scale, which often makes sense)</li>
<li>Standardizing based on the scale or potential range of the data (so that coefficients can be more directly interpreted and scaled); an alternative is to present coefficients in scaled and unscaled forms</li>
<li>Transforming before multilevel modeling (thus attempting to make coefficients more comparable, thus allowing more effective second-level regressions, which in turn improve partial pooling). There is some risk in transformations for real world models.</li>
<li>Can we be certain the predictions are still valid on the original scale?</li>
<li>What happen if new data comes in and the Z-transformation we used is no longer valid?</li>
<li>What if we are working with elasticity (percent change over percent change in response) - does the transformation still make sense?</li>
</ul></li>
<li>Consider all coefficients as potentially varying
<ul>
<li>Practical concerns sometimes limit the feasible complexity of a model</li>
<li>Ideally we would like to have a model that is as complex as the data might be in reality we need a good fit and the ability to understand the model.</li>
</ul></li>
<li>Estimate causal inferences in a targeted way, not as a byproduct of a large regression
<ul>
<li>Least square regression does not care about causality.</li>
<li>If you do you need to go beyond, e.g.&nbsp;sketch the structural model, identify the causal effect and the roles of the confounders. Then use regression to estimate the effects.</li>
</ul></li>
</ol>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Six Quick Tips to Improve Modeling},
  date = {2024-08-26},
  url = {https://orenbochman.github.io/posts/2024/2024-08-26-six-quick-tips/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Six Quick Tips to Improve Modeling.”</span>
August 26, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-08-26-six-quick-tips/">https://orenbochman.github.io/posts/2024/2024-08-26-six-quick-tips/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-08-26-six-quick-tips/</guid>
  <pubDate>Sun, 25 Aug 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Stumpy</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/</link>
  <description><![CDATA[ 





<section id="stumpy---time-series-analysis" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="stumpy---time-series-analysis">Stumpy - Time Series Analysis</h2>
<p>Stumpy <span class="citation" data-cites="law2019stumpy">(Law 2019)</span> is a powerful and scalable library for computing a matrix profile which can be used for a variety of time series data mining tasks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-law2019stumpy" class="csl-entry">
Law, Sean M. 2019. <span>“<span class="nocase">STUMPY: A Powerful and Scalable Python Library for Time Series Data Mining</span>.”</span> <em><span>The Journal of Open Source Software</span></em> 4 (39): 1504.
</div><div id="ref-sean2021online" class="csl-entry">
———. 2021. <span>“Modern Time Series Analysis with STUMPY.”</span> <a href="https://youtu.be/XKNdXN-Jfmo" class="uri">https://youtu.be/XKNdXN-Jfmo</a>.
</div></div><p>This post is based on <span class="citation" data-cites="sean2021online">(Law 2021)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TLDR <span class="emoji" data-emoji="peanuts">🥜</span>
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Are you <strong>Stomped</strong> on time series analysis, <strong>Stumpy</strong> may be the library for you. <span class="emoji" data-emoji="grinning">😀</span></li>
<li>Stumpy is a FOSS library with scalable algorithms for computing a <code>matrix profile</code> which can be used for a variety of time series data mining tasks.</li>
<li>It is easy to use and has a simple API that makes it easy to get started with time series analysis.</li>
</ul>
</div>
</div>
<section id="introduction" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>. It is easy to use and has a simple API that makes it easy to get started with time series analysis.</p>
<p>Under the hood Stumpy calculates the pairwise euclidean distance between two subsequences of a time series. By considering all possible subsequences of a time series of a fixed length, we get a <code>distance matrix</code> which is a matrix of distances between subsequences. However the matrix profile is too slow to compute using brute force and is also memory intensive to store. The research behind Stumpy uses a number of optimizations to make the computation of the <code>matrix profile</code> which is list the nearest neighbors for each subsequence in a time series. This structure is <img src="https://latex.codecogs.com/png.latex?O(n)"> in space and <img src="https://latex.codecogs.com/png.latex?O(n%5E2)"> time complexity.</p>
<p>Note: before computing the matrix profile, Stumpy first normalises the subsequences to have zero mean and unit variance. This is done to ensure that the distance between subsequences is meaningful and that the matrix profile is accurate. <sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;This is a common step in time series analysis and is done to remove any trends or seasonality in the data. It can also be disabled if needed.</p></div></div><p>why do we need to compute the matrix profile? The matrix profile is a powerful tool for time series analysis that can be used to identify patterns in the data.</p>
<blockquote class="blockquote">
<p>Given the <a href="https://www.cs.ucr.edu/%7Eeamonn/MatrixProfile.html">matrix profile</a>, most time series data mining tasks are trivial or easy to solve in a few lines of code. - Emonnn Keogh</p>
</blockquote>
<p>For example, motif discovery, discord discovery, semantic segmentation, and shapelet discovery can all be solved using the matrix profile.</p>
<p>By comparing the distance between subsequences, Stumpy can identify patterns in the data such as motifs, discords, chains, and other patterns. and then uses the matrix profile to identify motifs, discords, chains, and other patterns in the data.</p>
</section>
<section id="motivation" class="level3">
<h3 class="anchored" data-anchor-id="motivation">Motivation</h3>
<p>You get tasked with analyzing a time series data set there are many (10000+) points what do you do?</p>
<p>Magic Spells for Time Series Analysis:</p>
<ul>
<li>Visualizations
<ul>
<li>great for small data sets</li>
<li>not suitable with more than 1000 data points</li>
</ul></li>
<li>Statistics
<ul>
<li>traditional stats are not as meaningful for time series data</li>
<li>with many points we tend to care more about subsequences than points.</li>
</ul></li>
<li>ARIMA (AutoRegressive Integrated Moving Average)
<ul>
<li>autoregressive model assuming some kind of repeating patterns</li>
</ul></li>
<li>Anomaly Detection
<ul>
<li>assuming that we can agree what is normal</li>
</ul></li>
<li>ML (Predictive Modeling)
<ul>
<li>assuming that we can predict the future</li>
<li>using features to predict values based on a projection or interpolation.</li>
</ul></li>
<li>Forecasting
<ul>
<li>Using trends, historiacal and statistical data.</li>
<li>forecasting is for longer term frames</li>
</ul></li>
<li>Clustering
<ul>
<li>grouping similar parts of a time series together</li>
<li>grouping similar time series together</li>
</ul></li>
<li>Dynamic Time Warping
<ul>
<li>comparing time series that are not aligned</li>
<li>comparing time series that are not the same length</li>
<li>assuming we know enough about the time series to do this.</li>
</ul></li>
<li>Change Detection
<ul>
<li>detecting when a time series changes regimes.</li>
</ul></li>
</ul>
</section>
<section id="stumpys-approach" class="level3">
<h3 class="anchored" data-anchor-id="stumpys-approach">Stumpy’s Approach</h3>
<p>Stumpy tries to answer two questions:</p>
<ol type="1">
<li>Do any subsequences appear more than once in a time series?</li>
<li>If they are such subsequences, what are they and where do they appear?</li>
</ol>
<p>the design goals seem to be</p>
<ul>
<li>be easy to interpret</li>
<li>use/data agnostic</li>
<li>no prior knowledge of the data</li>
<li>parameter free</li>
</ul>
</section>
<section id="features" class="level3">
<h3 class="anchored" data-anchor-id="features">Features</h3>
<p>Stumpy has a number of features that make it a powerful tool for time series analysis. Some of the key features of Stumpy include:</p>
<ul>
<li>High level features:
<ul>
<li>Motif Discovery</li>
<li>Discord Detection</li>
<li>TS Chains</li>
<li>Semantic Segmentation</li>
<li>Shapelet &amp; Snippets</li>
<li>MPdist Clustering</li>
<li>Multi-dimensional TS</li>
</ul></li>
<li>Low level features:
<ul>
<li>Fast and memory efficient computation of matrix profiles.</li>
<li>Support for both univariate and multivariate time series data.</li>
<li>Support for both fixed-length and variable-length time series data.</li>
<li>Support for both Euclidean and DTW distance measures.</li>
<li>Support for both exact and approximate matrix profile computation.</li>
<li>Support for both CPU and GPU computation.</li>
<li>Support for both online and offline matrix profile computation.</li>
</ul></li>
</ul>
</section>
<section id="big-idea" class="level3">
<h3 class="anchored" data-anchor-id="big-idea">Big idea</h3>
</section>
<section id="novelty" class="level3">
<h3 class="anchored" data-anchor-id="novelty">Novelty</h3>
</section>
<section id="papers" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="papers">Papers</h3>
<p>Stumpy is based on a number of research papers that have been published in the field of time series data mining. Some of the key papers that have inspired Stumpy include:</p>
<ul>
<li><span class="citation" data-cites="Yeh2016MatrixProfileI">(Yeh et al. 2016)</span></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-Yeh2016MatrixProfileI" class="csl-entry">
Yeh, Chin-Chia Michael, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva, Abdullah Mueen, and Eamonn Keogh. 2016. <span>“Matrix Profile i: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets.”</span> In <em>2016 IEEE 16th International Conference on Data Mining (ICDM)</em>, 1317–22. <a href="https://doi.org/10.1109/ICDM.2016.0179">https://doi.org/10.1109/ICDM.2016.0179</a>.
</div></div><p>Zhu, Yan, et al.&nbsp;(2016) Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred Million Barrier for Time Series Motifs and Joins. ICDM:739-748.</p>
<p>Yeh, Chin-Chia Michael, et al.&nbsp;(2017) Matrix Profile VI: Meaningful Multidimensional Motif Discovery. ICDM:565-574.</p>
<p>Zhu, Yan, et al.&nbsp;(2017) Matrix Profile VII: Time Series Chains: A New Primitive for Time Series Data Mining. ICDM:695-704.</p>
<p>Gharghabi, Shaghayegh, et al.&nbsp;(2017) Matrix Profile VIII: Domain Agnostic Online Semantic Segmentation at Superhuman Performance Levels. ICDM:117-126.</p>
<p>Zhu, Yan, et al.&nbsp;(2017) Exploiting a Novel Algorithm and GPUs to Break the Ten Quadrillion Pairwise Comparisons Barrier for Time Series Motifs and Joins. KAIS:203-236.</p>
<p>Zhu, Yan, et al.&nbsp;(2018) Matrix Profile XI: SCRIMP++: Time Series Motif Discovery at Interactive Speeds. ICDM:837-846.</p>
<p>Yeh, Chin-Chia Michael, et al.&nbsp;(2018) Time Series Joins, Motifs, Discords and Shapelets: a Unifying View that Exploits the Matrix Profile. Data Min Knowl Disc:83-123.</p>
<p>Gharghabi, Shaghayegh, et al.&nbsp;(2018) “Matrix Profile XII: MPdist: A Novel Time Series Distance Measure to Allow Data Mining in More Challenging Scenarios.” ICDM:965-970.</p>
<p>Zimmerman, Zachary, et al.&nbsp;(2019) Matrix Profile XIV: Scaling Time Series Motif Discovery with GPUs to Break a Quintillion Pairwise Comparisons a Day and Beyond. SoCC ’19:74-86.</p>
<p>Akbarinia, Reza, and Betrand Cloez. (2019) Efficient Matrix Profile Computation Using Different Distance Functions. arXiv:1901.05708.</p>
<p>Kamgar, Kaveh, et al.&nbsp;(2019) Matrix Profile XV: Exploiting Time Series Consensus Motifs to Find Structure in Time Series Sets. ICDM:1156-1161.</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Stumpy},
  date = {2024-08-08},
  url = {https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Stumpy.”</span> August 8, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/">https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/</guid>
  <pubDate>Wed, 07 Aug 2024 21:00:00 GMT</pubDate>
</item>
</channel>
</rss>
