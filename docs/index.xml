<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Oren Bochman&#39;s Blog</title>
<link>https://orenbochman.github.io/</link>
<atom:link href="https://orenbochman.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Personal website, portfolio and blog</description>
<generator>quarto-1.5.54</generator>
<lastBuildDate>Wed, 25 Sep 2024 21:00:00 GMT</lastBuildDate>
<item>
  <title>Six quick tips to improve modeling</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-26-six-quick-tips/</link>
  <description><![CDATA[ 





<p>The following tips are from Data Analysis Using Regression and Multilevel/Hierarchical Models by Andrew Gelman and Jennifer Hill.</p>
<ol type="1">
<li>Fit many models</li>
<li>Do a little work to make your computations faster and more reliable 2.1 data subsetting is faster than full data set 2.2 redundent parameterizations (recentering and scaling) add some parameters but don’ realy change the model yet make it more computationally stable by improving the geometry the sampler has to work with. 2.3 fake data and predictive simulations help understand if the problems we are facing are due to the model or the data. Fake data creates a version of the data for which we know what to expect.</li>
<li>Graphing the relevant
<ul>
<li>Graphing the data is fine</li>
<li>Graphing the model is more informative (regression lines and curves)</li>
<li>I like to plot lots of graphs like regression diagnostics, residuals, and posterior predictive checks. But Gelman and Hill suggest that these are not relevant and warn that one should be prepared to explain any graph you show. Some of these diagnostics graphs are kind of hard to explain. e.g.&nbsp;Cook’s distance v.s. leverage plot.</li>
</ul></li>
<li>Transformations
<ul>
<li>Logarithms of all-positive variables (primarily because this leads to multiplicative models on the original scale, which often makes sense)</li>
<li>Standardizing based on the scale or potential range of the data (so that coefficients can be more directly interpreted and scaled); an alternative is to present coefficients in scaled and unscaled forms</li>
<li>Transforming before multilevel modeling (thus attempting to make coefficients more comparable, thus allowing more effective second-level regressions, which in turn improve partial pooling). There is some risk in transformations for real world models.</li>
<li>Can we be certain the predictions are still valid on the original scale?</li>
<li>What happen if new data comes in and the Z-transformation we used is no longer valid?</li>
<li>What if we are working with elasticity (percent change over percent change in response) - does the transformation still make sense?</li>
</ul></li>
<li>Consider all coefficients as potentially varying
<ul>
<li>Practical concerns sometimes limit the feasible complexity of a model</li>
<li>Idealy we would like to have a model that is as complex as the data might be in reality we need a good fit and the ability to understand the model.</li>
</ul></li>
<li>Estimate causal inferences in a targeted way, not as a byproduct of a large regression
<ul>
<li>Least square regression does not care about causality.</li>
<li>If you do you need to go beyond, e.g.&nbsp;sketch the structural model, identify the causal effect and the roles of the confounders. Then use regression to estimate the effects.</li>
</ul></li>
</ol>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Six Quick Tips to Improve Modeling},
  date = {2024-09-26},
  url = {https://orenbochman.github.io/posts/2024/2024-09-26-six-quick-tips/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Six Quick Tips to Improve Modeling.”</span>
September 26, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-26-six-quick-tips/">https://orenbochman.github.io/posts/2024/2024-09-26-six-quick-tips/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-26-six-quick-tips/</guid>
  <pubDate>Wed, 25 Sep 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>deduction evaluation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html</link>
  <description><![CDATA[ 





<p>goal create a deduction data-set for evaluating reasoning capabilities of a man and machine.</p>
<p>tasks:</p>
<ol type="1">
<li>learning graph based representation of arguments from a text</li>
<li>generating a text version of such a graph</li>
<li>identig roles of relations in the graph such as</li>
</ol>
<ul>
<li><p>subject, predicate, copula, quantity, quality, distribution, figure, mood, opposition, conversion</p></li>
<li><p>common sense knowledge, counterfactuals, hypotheticals, conditionals, causality, modality, necessity, possibility, probability, uncertainty, vagueness, ambiguity, contradiction, paradox, tautology, fallacy, sophism, enthymeme, analogy, dilemma, aporia, syllogism, enthymeme, paradox, proposition, argument, inference, deduction, induction, abduction.</p></li>
<li><p>term, proposition, argument, inference, fallacy, tautology, contradiction, paradox, syllogism, enthymeme, sophism, paradox, aporia, dilemma, analogy, deduction, induction, abduction aporias, finding dilemmas</p></li>
<li><p>removing the ambiguity from a text by constructing a graph then rewriting the text to be more precise.</p></li>
<li><p>graph of categories (perhaps drawn from wikidata, or extracted from a text by an LLM)</p></li>
<li><p>statements can be formed genereated from the graph using LLM (large language model)</p></li>
<li><p>we might prefer to genereate statements these using unification with spacy operating on the graph</p></li>
<li><p>use the square of opposition to formulate statements and inferences from the graph</p></li>
<li><p>generate graded deductions based on sylogisms</p>
<ul>
<li>tautologies</li>
<li>falaices with type of fallacy</li>
<li>inferences with type of inference</li>
</ul></li>
<li><p>each sylogism will need a template with designated parts of speech for each term in the sylogism</p></li>
<li><p>the arguments should be composable so that</p></li>
</ul>
<blockquote class="blockquote">
<p>All men are mortal. Socrates is a man. Therefore, Socrates is mortal.[2]</p>
</blockquote>
<p>P belongs to S P is predicated of S P is said of S</p>
<p>There are four different types of categorical sentences: universal affirmative (A), universal negative (E), particular affirmative (I) and particular negative (O).</p>
<p>A - A belongs to every B E - A belongs to no B I - A belongs to some B O - A does not belong to some B</p>
<p>a = belongs to every e = belongs to no i = belongs to some o = does not belong to some</p>
<p>Categorical sentences may then be abbreviated as follows:</p>
<p>AaB = A belongs to every B (Every B is A) AeB = A belongs to no B (No B is A) AiB = A belongs to some B (Some B is A) AoB = A does not belong to some B (Some B is not A)</p>
<p>the ten terms or parts of speech in a categorical sentence, drawn from the Organon are :</p>
<ol type="1">
<li>Subject</li>
<li>Predicate</li>
<li>Copula</li>
<li>Quantity</li>
<li>Quality</li>
<li>Distribution</li>
<li>Figure</li>
<li>Mood</li>
<li>Opposition</li>
<li>Conversion</li>
</ol>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Deduction Evaluation},
  date = {2024-09-09},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Deduction Evaluation.”</span> September 9,
2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/deduction.html</guid>
  <pubDate>Mon, 09 Sep 2024 19:36:40 GMT</pubDate>
</item>
<item>
  <title>Rethinking Signaling systems via the lens of compositionality</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/marco-baoni-composionality/lewis.html</link>
  <description><![CDATA[ 





<p>I was introduced to the subject of language evolution by Brian Skryms in his book “Signals: Evolution, Learning, and Information” where he discusses the evolution of signaling systems and the emergence of language. In it he discusses the role of compositionality in the emergence of language and how it is a key feature of human language. Signals provides a coherent yet multifaceted views of the problem - philosophy, signaling system creation and assimilation via evolution or reinforcement learning. Skryms also considers Logic and complex signaling systems. Yet a unifying theme for this work is a reductionist view of the problem and his attempt to reduce the problem to a model that follows closely the Lewis Signaling Game.</p>
<p>I like this reductionist approach but I like to also to turn it on its head. By looking at how the problem takes form in more challenging and realistic settings can often uncover the true nature of the problem. Since language emergence is so open ended one might also use it to consider how it empowers agents to coordinate on better decision in ever more challenging problems and settings.</p>
<p>I first became frustrated with complex signaling systems when I read the chapters in Signals and realized that unlike the other chapters Skryms had not summarized how researchers in the field had come up with a definitive solution to the problem. I reread it a couple of times and finally realized that although he made some very interesting claims this topic was still unresolved. There are many interesting results but there are at least as many open questions.</p>
<p>The second time I became frustrated was when I tried to convert the simple signaling RL games into complex ones. Just the material in the book had versions with multiple agents signaling in parallel, one agent signaling without sequence, and agents signaling in sequence. The book also hints at cases where agents may make mistakes and that this is important for the evolution of signaling systems.</p>
<p>I also was coming across more and more research that isn’t covered in the book that looks at morphology and syntax in the emergence of language. Further more people were using deep learning to overcome the lewis signaling game inability of of arriving decoders for complex signals.</p>
<p>At this point I realized that there might be three problems that are being conflated in nature and that we might want to consider them separately as well as together.</p>
<ol type="1">
<li>the coordination problem - how agents learn a common convention for signaling and what is the most effective form of the solution.</li>
<li>the serialization problem - how the medium will e.g.&nbsp;a noisy channel can introduce additional desireable contraints like shorter signals, saliency, early decoding, (compression, error detection and correction, easy decoding, signal distributions, ). This problem is one which is solved by a descion tree. But the different options for the settings will lead to different optimal solutions. These are hidden by the symmetric form of the rewards in the lewis game.</li>
<li>the signal composition problem - given a simple signaling systems and a encoder decoder for the channel how can we add aggregation to the signaling system to make it more efficient. (more expressive, easier to learn, easier to extend, more robust to different errors.)</li>
</ol>
<p>This might help answer questions like - why does english use just 39-44 phonems instead of the full we have a languages making a full use of human phonemes (600 consonants and 200 vowels) ?</p>
<p>What became apparent to me is that the nature of a complex signaling system, depends very much on the game being played by the agents.</p>
<p>Metrics:</p>
<ul>
<li><p>Total number of signals</p></li>
<li><p>Minimal set of signals needed to learn the signalling system with n-learners with full observability of signal, action and reward by all learners.</p></li>
<li><p>How long to learn saliency (the distribution states of the world) of signals perhaps adjusted by risk (the distribution of malleuses for wrong action in the each state of the world)</p></li>
<li><p>How long to coordinate on a basic system with N states and N actions</p></li>
<li><p>How to learn to coordinate on a huffman cannonical code to optimize a signaling system</p></li>
<li><p>Learning and Coordinating via templates for complex signals</p>
<ul>
<li>Degree of morphology</li>
<li>Degree of syntax</li>
<li>Degree of contextual meaning</li>
<li>Degree of coordination (and aggreement in templates) and its error correction capacity)</li>
</ul></li>
<li><p>Message entropy</p></li>
<li><p>Robustness to error in sender, receiver, and channel</p></li>
<li><p>Mean</p></li>
<li><p>Regarding complex signaling systems he points out a couple of ideas:</p>
<ul>
<li>Complex signals might be composed by simple signals from multiple senders.
<ul>
<li>The reciever needs to both decode and aggregate the simple signals to infer the state of the world encoded in the complex signal.</li>
<li>This is particularly interesting and less artificial once consider realise it leads to a partially observed markov descion process (PMDP)
<ul>
<li>senders have partial observability of the state of the world and</li>
<li>recievers need to reconstruct the state by aggregatig partial messages</li>
</ul></li>
<li>If we might also give the agents types and make the game a bayesian game.
<ul>
<li>Types are
<ul>
<li>Knights - with messages that are always true as well as thier atoms</li>
<li>Knaves - with messages that are always false.</li>
<li>Normals - with messages that are sometimes true and sometimes false.</li>
<li>Insane - who think that thier messages are always true but are actually always false.</li>
<li>etc</li>
</ul></li>
</ul></li>
</ul></li>
<li>Complex signals might be composed by multiple simple signals from a single sender.
<ul>
<li>The complex signal is a bag of signals (i.e.&nbsp;aggregation is not unordered - buy via a conjunction of signals i.e.&nbsp;A and B = B and A).</li>
<li>The complex signal is ordered sequence of signals sequence of signals i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?(A,B)%20%5Cneq%20(B,A)"> .</li>
<li>Sequences of sequences can capture morphology.</li>
<li>Natural language adds the notion of recursion - which in terms of mathematically boils down to a a partial ordering of simple signals to form complex signals.</li>
</ul></li>
</ul></li>
<li><p>There is a natural tendency to think about the Chomsky hierarchy of languages at this point.</p></li>
<li><p>Also once there are sequences of signals we will naturally consider ideas from information theory.</p>
<ul>
<li>Entropy of a signal</li>
<li>Error detection</li>
<li>Error correction</li>
<li>Source coding (compression)</li>
<li>Easy decoding of messages</li>
</ul></li>
<li><p>Errors are stated as important in the evolution of signaling systems in the paper of Nowak and Krakauer (1999).</p>
<ul>
<li>we</li>
</ul></li>
<li><p>Compression and easy decoding are also important too but this came up later when people noticed that thier agents were learning very inefficient signaling systems (with very long signals)</p>
<ul>
<li>this suggests that we add a parameter to the game to penalize long signals.</li>
<li>and to reward early decoding of the signal.</li>
</ul></li>
<li><p>Logic is also discussed in the</p></li>
</ul>
<p>has an extensive bibliography and I have been following up on some of the references.</p>
<p>This is a quick summary of a talk by Marco Baroni on the topic of compositionality in language. In it he outlines some of his work and his collegues/students work on the topic and the conclusions he has drawn from it.</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Rethinking {Signaling} Systems via the Lens of
    Compositionality},
  date = {2024-09-09},
  url = {https://orenbochman.github.io/posts/2024/marco-baoni-composionality/lewis.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Rethinking Signaling Systems via the Lens of
Compositionality .”</span> September 9, 2024. <a href="https://orenbochman.github.io/posts/2024/marco-baoni-composionality/lewis.html">https://orenbochman.github.io/posts/2024/marco-baoni-composionality/lewis.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/marco-baoni-composionality/lewis.html</guid>
  <pubDate>Mon, 09 Sep 2024 12:25:48 GMT</pubDate>
</item>
<item>
  <title>logic puzzles</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-06-11/logic puzzles.html</link>
  <description><![CDATA[ 





<p>Solve this logic puzzle step by step:</p>
<p>A man was looking at a portrait. Someone asked him, “Whose picture are you looking at?” He replied: “Brothers and sisters have I none, but this man’s father is my father’s son.” Whose picture was the man looking at?</p>
<p>A man was looking at a portrait. Someone asked him, “Whose picture are you looking at?” He replied: “Brothers and sisters have I none, but this man’s son is my father’s son.” Whose picture was the man looking at?</p>
<p>three dozen grey socks and 36 pink socks are lying in a drawer in a dark room. What is the minimum number of socks I must take out of the drawer which will guarantee that I have at least two socks of the same color?</p>
<p>A certain snail takes an hour and a half to crawl clockwise around a certain racetrack, yet when he crawls counter­ clockwise around that same racetrack it takes him only ninety minutes. Why this discrepancy?</p>
<p>If an airplane crashes right on the border of the United States and Canada, in which country would you bury the survivors?</p>
<p>A certain street contains 200 buildings. A sign-maker is called to number the houses from 7 to 207. He has to order numerals to do the job. Without using pencil and paper, can you figure out in your head how many 9’ s he will need?</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Logic Puzzles},
  date = {2024-09-09},
  url = {https://orenbochman.github.io/posts/2024/2024-06-11/logic puzzles.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Logic Puzzles.”</span> September 9, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-06-11/logic puzzles.html">https://orenbochman.github.io/posts/2024/2024-06-11/logic
puzzles.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-06-11/logic puzzles.html</guid>
  <pubDate>Sun, 08 Sep 2024 23:40:40 GMT</pubDate>
</item>
<item>
  <title>replay buffer questions</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html</link>
  <description><![CDATA[ 





<section id="replay-buffer" class="level2">
<h2 class="anchored" data-anchor-id="replay-buffer">Replay Buffer</h2>
<ol type="1">
<li>for continuous environment we should think about <strong>coverage</strong>.</li>
</ol>
<ul>
<li>given a paramertrization of the value function, for a level of generalization/discrimination we get an induced set of features. Is some set of experiences sufficent to do prediction or control.</li>
<li>if we have an estimate of the coverage can we use it to place a bound on the error of the value function.</li>
<li>can we do better if we also have an estimate <img src="https://latex.codecogs.com/png.latex?%5Cmu(s)"> of the importance/long term probability of the states ?</li>
</ul>
<ol start="2" type="1">
<li>Traces present a highly correlated view of the state space.</li>
</ol>
<ul>
<li>How much do we need to wory about this.</li>
</ul>
<ol type="1">
<li>does replay buffer violate markov state.?</li>
</ol>
<ul>
<li>according to <a href="https://www.linkedin.com/in/shirli-di-castro/">Shirli Di-Castro Shashua</a>
<ul>
<li><a href="https://proceedings.mlr.press/v162/di-castro22a/di-castro22a.pdf">Analysis of Stochastic Processes through Replay Buffers</a></li>
<li><a href="https://arxiv.org/abs/2110.00445">Sim and Real: Better Together</a></li>
<li>the storage operation preserves the markov property</li>
<li>the sampling operation preserves the markov property</li>
<li>the mean operation om the replay buffer violates the markov property…</li>
</ul></li>
</ul>
<ol start="2" type="1">
<li>can reduce correlation between samples ?</li>
<li>can we be more stategic about what we keep in the RB</li>
</ol>
<ul>
<li>say we have a key using a <img src="https://latex.codecogs.com/png.latex?hash%5B%5Cdelta(state),%20action%5D"> neighbourhood
<ul>
<li>we can use the key to decide if to insert/replace the current buffer</li>
<li>we can use it to decide what to discard</li>
</ul></li>
<li>we can use the buffer to estimate mu(s)
<ul>
<li>might also have more info like states we did not insert or deleted.</li>
<li>if we also have mu(mu) - the state importance to decide what to keep</li>
</ul></li>
<li>do we prefer complete recent traces or many partial traces.</li>
</ul>
<ol start="4" type="1">
<li>Can we use options/skills to orgenize the buffer more effectively ?</li>
</ol>
<ul>
<li><p>we should aim to keep full options traces in the buffer</p></li>
<li><p>keep traces in &amp; out or options.</p></li>
<li><p>before and after the options.</p></li>
</ul>
<p>Think of the four room environment - there are different options to get from one room to another. they are composable. Once we have good coverage entry into the op</p>
</section>
<section id="ergodicity" class="level2">
<h2 class="anchored" data-anchor-id="ergodicity">Ergodicity</h2>
<ol type="1">
<li>in an environment is a maze and I have a one way door dividing the left side from the right parts of the maze. is this environment ergodic ?</li>
<li>If not how come we can still learn the optimal policy ?</li>
</ol>
<p>interchip dotan castro - sim to real</p>
</section>
<section id="replay-buffers--" class="level2">
<h2 class="anchored" data-anchor-id="replay-buffers--">Replay buffers -</h2>
<ul>
<li>storing sequence of states</li>
<li>State action state</li>
</ul>
<p>PMDPs</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Replay Buffer Questions},
  date = {2024-09-04},
  url = {https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Replay Buffer Questions.”</span> September
4, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html">https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html</guid>
  <pubDate>Wed, 04 Sep 2024 19:17:32 GMT</pubDate>
</item>
<item>
  <title>Fine-tune llm for Style and Grammar advice.</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html</link>
  <description><![CDATA[ 





<section id="fine-tuning-llm-for-readability" class="level1">
<h1>Fine tuning LLM for Readability</h1>
<p>Question 1: LLM are amazing - can I fine tune a state of the art LLM like Lama 3.1 with what I consider to be very high quality writing from my private library content to create a writing assistant?</p>
<p>Can I do it in a way that is aware of the writer’s style and the text domain other high level features so that the prompt can be used to condition the output this way.</p>
<p>Can I test it on wikipedia articles and see if it can improve the readability of existing and new articles?</p>
<section id="learning-to-write-well" class="level2">
<h2 class="anchored" data-anchor-id="learning-to-write-well">Learning to write well</h2>
<p>learning to write from:</p>
<ul>
<li>“Best American Science Writing” series</li>
<li>Bast papers - sourced from leading conferences</li>
<li>Authors
<ul>
<li>Oliver Sacks,</li>
<li>Natalie Angier,</li>
<li>Alan Lightman,</li>
<li>Sylvia Nasar,</li>
<li>Matt Ridley
<ul>
<li>Genome</li>
<li>Red Queen</li>
<li>Viral</li>
</ul></li>
<li>Steven Pinker
<ul>
<li>How the Mind Works</li>
<li>The Blank Slate: The Modern Denial of Human Nature</li>
<li>The Language Instinct: How the Mind Creates Language</li>
<li>The Secret Life of Verbs</li>
<li>Words and Rules</li>
<li>Hotheads</li>
</ul></li>
<li>Richard Dawkins
<ul>
<li>Selfish gene</li>
<li></li>
</ul></li>
</ul></li>
<li>Great Explianers
<ul>
<li>Richard Feynman
<ul>
<li>lectures on physics</li>
<li>QED</li>
<li>The pleasure of finding things out</li>
<li>The character of physical law</li>
<li>Surely you’re joking</li>
<li>What do you care about what other people think</li>
<li>Six easy pieces</li>
</ul></li>
<li>Levitt and dunbar
<ul>
<li>Freakonomics</li>
</ul></li>
<li>Michel Foucout
<ul>
<li>Discipline and Punish</li>
<li>The Birth of the Clinic</li>
<li>The Order of Things</li>
<li>The Archaeology of Knowledge</li>
<li>Madness and Civilization<br>
</li>
</ul></li>
<li>Leonard Susskind
<ul>
<li>Theoretical Minimum</li>
</ul></li>
<li>Richard Hawkins</li>
<li>Jared Diamond
<ul>
<li>Guns, Germs, and Steel</li>
<li>Collapse: How Societies Choose to Fail or Succeed</li>
<li>The World Until Yesterday: What Can We Learn from Traditional Societies?</li>
<li>The Invisible Hands: Top Hedge Fund Traders on Bubbles, Crashes, and Real Money</li>
<li>etc</li>
</ul></li>
<li>C.S. Lewis
<ul>
<li>A Grief Observed</li>
<li>The Problem of Pain</li>
<li>The Screwtape Letters</li>
<li>The Great Divorce</li>
<li>Mere Christianity</li>
<li>A Preface to Paradise Lost</li>
</ul></li>
<li>Eric Metaxas
<ul>
<li>Martin Luther</li>
<li>Bonhoffer</li>
<li>Discussing Mere Chritianity</li>
</ul></li>
<li>Yuval Noah Harari
<ul>
<li>Sapiens - A brief history of mankind</li>
<li>Homo Deus</li>
<li>21 Lessons for the 21st Century Audiobook</li>
</ul></li>
<li>Primo Levi
<ul>
<li>The Periodic Kingdom</li>
</ul></li>
<li>Mcluhan Marshall
<ul>
<li>The Medium Is The Massage</li>
</ul></li>
<li>Empire of the Summer moon</li>
<li>Hidden Figures</li>
<li>Art of War</li>
<li>Book of five rings</li>
<li>Adam Smith</li>
<li>The pencil</li>
<li>Dan Ariely</li>
<li>Chris Anderson
<ul>
<li>The long tail</li>
</ul></li>
<li><h2 id="plato" class="anchored">Plato</h2></li>
<li><h2 id="aristotle" class="anchored">Aristotle</h2></li>
<li>Machiavelli
<ul>
<li>The prince</li>
</ul></li>
<li>James Surowiecki
<ul>
<li>The Wisdom of Crowds</li>
</ul></li>
<li>Robert A. Caro
<ul>
<li>The Power Broker: Robert Moses and the Fall of New York</li>
<li>Working</li>
<li>Master of the Senate</li>
</ul></li>
<li>Stephen Jay Gould</li>
<li>Ian Ayres
<ul>
<li>Super Crunchers -</li>
</ul></li>
<li>Giles Milton
<ul>
<li>Nathaniel’s Nutmeg</li>
<li>D-Day The Soldiers’ Story</li>
<li>When Hitler Took Cocaine and Lenin Lost His Brain</li>
<li>Fascinating Footnotes From History</li>
<li>Churchill’s Ministry of Ungentlemanly Warfare</li>
<li>Wolfram The Boy Who Went to War</li>
<li>The Extraordinary Story of Thomas Pellow and Islam’s One Million White Slaves</li>
<li>The Stalin Affair: The Impossible Alliance that Won the War</li>
<li>Samurai William: The Englishman Who Opened Japan</li>
<li>Edward Trencom’s Nose</li>
<li>Russian Roulette - A Deadly Game: How British Spies Thwarted Lenin’s Global Plot</li>
</ul></li>
</ul></li>
</ul>
<p>etc</p>
<p>the ideas here are :</p>
<ol type="1">
<li>the primary text</li>
<li>the wikipedia article on</li>
<li>summaries</li>
</ol>
<p>where we want to focus on the primary text but and also to highlight its structure</p>
<p>The primary text has lots of words but one top level structure a few chapter level structure many paragraph level structure</p>
<p>idealy we want to learn structures:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bparagraph%7D%20%5Cto%20%20%5Ccdots%20%5Cto%20%20%5Ctext%7Btop%20level%7D%0A"> and idealy we would like to learn to research</p>
<p>i.e.&nbsp;source the ‘facts’ from reliable sources which we cite inline.</p>
<p>teach an LLM to rewrite text with high fidelity yet increase thier readability.</p>
<p>high quaity data sets:</p>
<ol type="1">
<li>wikipedia v.s. higher quality e.g.&nbsp;britanica, or others</li>
</ol>
<ul>
<li>check this isn’t the best of wikipedia</li>
<li>check the citations (is there a significant overlap we are in the ballpark)</li>
</ul>
<ol start="2" type="1">
<li></li>
</ol>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Fine-Tune Llm for {Style} and {Grammar} Advice.},
  date = {2024-09-04},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Fine-Tune Llm for Style and Grammar
Advice.”</span> September 4, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/readability.html</guid>
  <pubDate>Wed, 04 Sep 2024 14:17:38 GMT</pubDate>
</item>
<item>
  <title>LLM and the missing link</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html</link>
  <description><![CDATA[ 





<p>The missing link is my name for a set of agents that should be able to edit wikipedia or at least to significantly reduce the effort needed to contribute to wikipedia.</p>
<ul>
<li>Wikipedia has a number of task and challenges</li>
<li>Wikipedia also offers unique opportunities for learning not available elsewhere (edit histories, talk pages, etc.)</li>
</ul>
<p>tasks:</p>
<ol type="1">
<li>Wikification - use entropy maximize the entropy and mutual information of the wiki - i.e.&nbsp;choose links to other articles that are most likely to be clicked on rather than the the most most famous or like USA - which contributes no information to the reader.</li>
<li>inlining citations</li>
<li>adding missing references</li>
<li>adding missing sections across languages</li>
<li>Improving readability
<ul>
<li>most wikipedia articles are poorly written when compared with the best science writing in the world.</li>
</ul></li>
<li>Addressing biases and COI issues. [^we nay need to train the LLM on material that does not include wikipedia or to create a version that can separate wikipedia and non wikipedia material possibly using CLIP?]
<ul>
<li>with the advent of LLM we can now collect all the material in an articles Sources and use it to rewrite a more complete article and perhaps one with fewer biases.<sup>1</sup> Further more it is fairly easy to source additional material from the web and other sources and thus again allowing a second view of the the articles point of view.</li>
</ul></li>
<li>Addressing vandalism and spam - this can be learned across articles</li>
<li>Extracting wikidata from articles again this can be learned across many articles by mapping the article to the wikidata entries of the primary and secondary entities.</li>
<li>Replace low register terms with high register terms - with an eye to improving readability. One hopes that the higher register terms are more precise and less ambiguous.</li>
<li>Replace highly ambiguous terms with less ambiguous terms. The same perhaps for sentences.</li>
<li>Make use of other media - diagrams, maths, code, images, videos, maps and so on should be more than referenced in the text.</li>
</ol>


<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;LLM inherit and amplify biases from thier training material, so this aspect is an area of active research and may require some creativity</p></div></div>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {LLM and the Missing Link},
  date = {2024-09-04},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“LLM and the Missing Link.”</span> September
4, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/the-missing-link.html</guid>
  <pubDate>Wed, 04 Sep 2024 13:29:44 GMT</pubDate>
</item>
<item>
  <title>NLP with RL</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>supervised and unsupervised models are great for most NLP tasks, yet both approaches have their limitations. Supervised models require labeled data, which is usually in short supply, while unsupervised models often lack the precision needed for many applications.</p>
<ul>
<li>Large language models like GPT-3 can provide us with a way to genereate text on demand. They are weak when generating from sparse data (Hellucination) and can be biased.</li>
</ul>
<p>Perhaps RL which is able to learn from mistakes, as well as use supervised as well as unsupervised learning as representations can perhaps bridge the gaps that LLM cannot adress so well.</p>
<p>The main challanges in RL however are difficulty in Trasfer learning, or generalizing between similar and or related tasks. I believe that within the NLP domain transfer learning seems to be a bit easier than in other domains, as the representations learned from one task can be used in another task.</p>
<ul>
<li>NLP is a domain in which skills learned in one task may be transferrable into other task.</li>
<li>Multigoal learning can be used to solve this issue.</li>
<li>Meta learning can be used to learn from multiple tasks and generalize to new tasks.</li>
<li>Curriculum learning can be applied to:
<ul>
<li>identify weaknesses in the model</li>
<li>collect more samples to adress weakness</li>
<li>collect appropriate samples to correct for biases that emerge in the model.</li>
</ul></li>
<li>Evolving language from scratch using lewis siggins games and thier extentions can also be viewed as as from of Meta learning.</li>
<li>Can we define a abstract hamiltonian that can be used for energy/entropy based generation of text using the hamiltonian of the promprts/context
<ul>
<li><p>Can we define a minimalist grammar using this abstract formalism.</p></li>
<li><p>Can we define a resonant solution multiple hamiltonians that interact on differernt levels.</p></li>
<li><p>Can we make this something that is a good fit for multi-headed attention perhaps analagous to how the finit state machine morphology was simplified by understanding that the FSM can be represented as a regular expression and that the generations we bounded by the lexicon. In other words can we create a hamiltonian that introduced contraints on the generation of text using different heads of the transformer model and thereby places bounds on the computational complexity of the model?</p></li>
</ul></li>
</ul>
<p>Rewards for different task can be defined in different ways, and the reward function can be used to guide the model to learn the task. At the start the reward function seems to be the greatest unknown. I believe that this will be the most interesting part of the project and perhaps a driver for innovation.</p>
<ol type="1">
<li>Tasks and components</li>
</ol>
<ul>
<li>Creating a minimalist language model that allows for pretraining and fine tuning with small payloads of data generated by the RL agents.</li>
<li>Augmenting the LLM with sophisticated embeddings that are most amenable for transfer learning.</li>
<li></li>
</ul>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {NLP with {RL}},
  date = {2024-09-03},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“NLP with RL.”</span> September 3, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/rl.html</guid>
  <pubDate>Tue, 03 Sep 2024 14:44:12 GMT</pubDate>
</item>
<item>
  <title>LLM the good the bad and the ugly</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/</link>
  <description><![CDATA[ 





<section id="language-models" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="language-models">Language Models</h2>
<p>Early when I saw the first text by gpt2 I was intrigued that some of the researchers that did not get access to the early model and had to re-create the model based on just the paper reported that that thier model had ‘probabilities’ of generating all those texts given the prompt.</p>
<p>This seems to be a rather <em>weak claim</em> - after all a million blindfolded monkeys banging on type writers would have some probability of generating those texts.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><a href="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/512px-Chimpanzee_seated_at_typewriter.jpg?20120413234640" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="One of a million IID monkeys at a typewriter   credit"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/512px-Chimpanzee_seated_at_typewriter.jpg?20120413234640" class="img-fluid quarto-figure quarto-figure-left figure-img" width="250" alt="One of a million IID monkeys at a typewriter   credit"></a></p>
</figure>
</div>
<figcaption>One of a million <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">IID</a> monkeys at a typewriter <br> <a href="https://commons.wikimedia.org/wiki/File:Chimpanzee_seated_at_typewriter.jpg">credit</a></figcaption>
</figure>
</div>
</div></div><p>One point to make is that the monkeys might have a higher probability of generating the text then the researcher’s model - but that is a different story.</p>
<section id="the-origen-of-seq2seq-models" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-origen-of-seq2seq-models">The origen of seq2seq models</h3>
<p>If you learned the pre LLM language modeling you would be familiar with <a href="https://en.wikipedia.org/wiki/N-gram">N-grams</a> you would be better equipped to be critical of LLMs. [N-grams]<sup>1</sup> More generally skip grams allow to model n-grams with gaps i.e.&nbsp;there are some tokens that are not specified or skipped. Another generalization was the introduction of a N-grams n representing unknown tokens.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;is an ordered sequence of tokens. It could be words, characters, or unicode code point.</p></div></div><p>The N-grams abstraction allowed for development of probabilistic models that are the basis of LLMs. However these older models had one significant limitation - they could only model a fixed number of tokens. This is because the number of possible N-grams grows exponentially with the number of tokens.</p>
<p>Related v.s. Similar</p>
</section>
</section>
<section id="the-good" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-good">The good</h2>
<p>There are a few powerful ideas in this approach.</p>
<ol type="1">
<li>Capturing co-occurrence statistics.
<ul>
<li>The word ‘the’ is followed by a noun 50% of the time.</li>
<li>Collocations may have gaps and this is where</li>
<li>Skip grams generalize N-grams by allowing for gaps in the sequence.</li>
</ul></li>
<li>Learning from positive examples is the forte of classical language models. You can learn the regular parts of a language using such a probabilistic quite fast. Unfortunately most languages are far from regular.</li>
<li>Generalizing using <a href="https://en.wikipedia.org/wiki/Smoothing">smoothing</a> - When the frequency of N-grams for some words is low enough we don’t expect to see them in a corpus of some given size. You just can’t fit all the N-grams of a given size in a corpus of a given size. but we can use shorter N-grams to estimate the probability of longer N-grams. (And this is how language models can be used to generate text). We call this process smoothing as conceptually we are filling holes in the longer N-grams probability distribution by moving some of the mass from related N-grams - to look more like the distribution of formed by combining shorter N-grams.</li>
<li>Learning negative examples. With enough data we may be infer that the absence of certain trigrams in the distribution where the associated bigrams are common isn’t due to chance but due some excluding factor. They might be linguistic or perhaps censoring. Regardless to detect get to a certain confidence level say 95% we need to see lots of bigrams and no trigram. Note though that we may have some broken english or some clumsy constructions that are in our corpus - they tend to muddy the waters and render these negative examples particularly challenging to infer. In fact it is generally easier to learn more by increasing the size of the corpus and learning more from rarer positive examples and this is what LLM do. not just from the corpus but from the language. However just as children learning to generelize have to be taught that the plural of goose is geese and not gooses, learning from positive -</li>
</ol>
<p>The problems with ngrams is that once n gets big enough and the corpus doesnt scale with it ngrams learn to model the corpus rather than the language. This is because as the ngram gets longer around the central word eventuall the contexts is specific enough that there is only one matching next ngram to for the given context - so the next word is certain.</p>
<blockquote class="blockquote">
<p>The egg hit the wall and <strong>it</strong> broke.</p>
</blockquote>
<ul>
<li>It must be the case that the egg hit the wall and it broke right.</li>
<li>Unless we are in feudal japan where internal walls are made to a large extent from rice paper on a frames.<sup>2</sup></li>
<li>We could also be dealing with a decorative egg and a glass wall.</li>
<li>Or we could be dealing with a metaphorical egg and a metaphorical wall.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;some gifted Samorai would need to catch the egg after it broke the wall to avoid <code>they broke</code></p></div></div><p>If the first scenario is correct 99.999% of the time why do we need to consider the other scenarios? The answer is best considered as a black swan problem. If we only consider the most likely scenarios we will be unprepared for the unlikely ones which could be catastrophic.</p>
<p>This suggests perhaps that while LLM should be great for learning a lexicon, a grammar, and some common sense knowledge - three very challanging tasks they are inadquate for making infrences about the world where different types of precise reasoning is required.</p>
</section>
<section id="the-bad" class="level2">
<h2 class="anchored" data-anchor-id="the-bad">the bad</h2>
<ul>
<li>the black swan problem</li>
<li>tokenization</li>
</ul>
</section>
<section id="the-sad" class="level2">
<h2 class="anchored" data-anchor-id="the-sad">the sad</h2>
<ul>
<li>context windows</li>
</ul>
</section>
<section id="the-ugly---where-are-llms-no-good" class="level2">
<h2 class="anchored" data-anchor-id="the-ugly---where-are-llms-no-good">the ugly - Where are LLMs no good?</h2>
<p>Let’s consider an analogy from physics. Classical physics is great for predicting phenomena at macro scales but quantum mechanics is required for the micro scale.</p>
<p>Physicist like to think that quantum physics should converge to classical physics at the macro scale but this is not always the case. There are phenomena that are only explained by quantum mechanics. We may soon discover more phenomena like superconductivity, quantum computers and quantum cryptography manifsting in our macro world</p>
<p>Fooled by randomness….</p>
<p>In the case of LLM there is the effect of stochasticity which is built into the models. We don’t care about this aspect so long as the model gives us good replies. But all replies are inherently stochastic. While humans might express an utternce in many ways they should be able to agree on its meaning, the facts, the options, the reasoning and so on. Neural netowrks are universal function approximators and in the case of LLMs the are approximate the LM from above which are stochastic all the way down - there is no agreement excepts on the most basic probabilities. The nlp researcher can only say that an utterance is likely to be generated by the model - with some probability. Any counter claim also has some probability.The probabilities in these cases are far more dramaicaly affected by the utterance length, word choices, grammarticality, common sense knowledge then factuallity, structured knowledge</p>
<p>This is a problem because we are used to deterministic replies from humans. We are used to deterministic replies from classical language models. We are used to deterministic replies from classical AI systems.</p>
<ul>
<li>hellucinations
<ul>
<li>where there is sparse data or the data used in training isn’t representative of the query we cannot expect the model to perform well.</li>
<li>even where there is good data - if the queries are subtle enough the stochastic nature of the model will manifest.</li>
</ul></li>
<li>prompt engineering</li>
</ul>
</section>
<section id="the-ugly---through-the-hole-in-the-coin" class="level2">
<h2 class="anchored" data-anchor-id="the-ugly---through-the-hole-in-the-coin">the ugly - through the hole in the coin</h2>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {LLM the Good the Bad and the Ugly},
  date = {2024-09-02},
  url = {https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“LLM the Good the Bad and the Ugly.”</span>
September 2, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/">https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-09-30-LLMs/</guid>
  <pubDate>Mon, 02 Sep 2024 18:16:45 GMT</pubDate>
</item>
<item>
  <title>Is compositionality overrated? The view from language emergence</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/marco-baoni-composionality/summary.html</link>
  <description><![CDATA[ 





<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In this talk, Marco Baroni discusses the role of compositionality in neural networks and its implications for language emergence. He presents recent work on language emergence in deep networks, where two or more networks are trained with a communication channel to solve a task jointly. Baroni argues that compositionality is not a necessary condition for good generalization in neural networks and suggests that focusing on enhancing generalization directly may be more beneficial than worrying about the compositionality of emergent neural network languages.</p>
<p>It turns out that compositionality means different things to different people in different contexts. This talk doesn’t provide us with an aristotelian definition of compositionality but a pragmatic one that Baroni used in investigation. The notion of language emergence in deep networks is a fascinating yet also vague. With these caveats in mind there are a numbeor of reuslts that Baroni presents that are worth considering.</p>
<p>If I was initially critical of this talk and speaker I soon came to realize that this is just the kind of message that simulates thought and discussion. It is a good talk and I would recommend it to anyone interested in the topic of language emergence in deep networks. <span class="emoji" data-emoji="thumbsup">👍</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My Insights
</div>
</div>
<div class="callout-body-container callout-body">
<p>My work on this subject shows that by adding compositionality to the lewis signaling game not only significantly increases the ratio of messages to simple signals but also renders the overall systems easier to master by making it more sytematic and predictable. This is greatly affected by form of aggregtion used for signal composition. e.g.&nbsp;using a simple template can create a morphology that is automatic and therefore easy to master.</p>
<p>For example if we add a parameter to the game to penalize long signals and reward early decoding of the signal we can get a more efficient signaling system. This allows agents to learn a larger signaling system faster. It also has more subtle effects- perhaps the most important one is that the pooling equilibria in the lewis game which are far more common then the non-pooling equilibria which we call a signaling system can be allow us to learn more efficient signaling systems if we reinterpret these partial pooling equilibria as leading to categories of signals which are further refined within the complex signal but can lead to an early decoding of the signal.</p>
</div>
</div>
</section>
<section id="questions" class="level2 callout-tim">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<p>In this subject we need to ask ourselves what are the main reseach questions and what are the main definitions we can make.</p>
<ul>
<li>what is language emergence</li>
<li>what is compositionality</li>
<li>How are Neural networks used here
<ul>
<li>how were they set up?</li>
<li>what was the task loss function?</li>
</ul></li>
<li>what is generalization in this task?</li>
</ul>
<p>The big question is will learning an emergent language aid agents to achive better generalization? - There is also talk about compositionality in RL where transfer learning is challenging. - Hinton discussed the idea of capsules in neural networks as a way to encode compositional structure in neural networks.</p>
</section>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definitions
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Simple Compositionality - The idea that the meaning of a complex signal is a function of the meanings of its parts.</li>
<li>Non compositionality - The idea that the meaning of a complex signal is not a function of the meanings of its parts.</li>
<li>Entaglement</li>
</ul>
</div>
</div>
<ul>
<li>By the end of the talk he more or less concludes that the compositionality of emergent neural network languages is not a necessary condition for good generalization.</li>
</ul>
<p>, and there is no reason to expect deep networks to find compositional languages more “natural” than highly entangled ones. Baroni concludes that if fast generalization is the goal, we should focus on enhancing this property without worrying about the compositionality of emergent neural network languages.</p>
<section id="blurb-from-the-talk" class="level2">
<h2 class="anchored" data-anchor-id="blurb-from-the-talk">Blurb from the talk</h2>
<p>Compositionality is the property whereby linguistic expressions that denote new composite meanings are derived by a rule-based combination of expressions denoting their parts. Linguists agree that compositionality plays a central role in natural language, accounting for its ability to express an infinite number of ideas by finite means.</p>
<p>“Deep” neural networks, for all their impressive achievements, often fail to quickly generalize to unseen examples, even when the latter display a predictable composite structure with respect to examples the network is already familiar with. This has led to interest in the topic of compositionality in neural networks: can deep networks parse language compositionally? how can we make them more sensitive to compositional structure? what does “compositionality” even mean in the context of deep learning?</p>
<p>I would like to address some of these questions in the context of recent work on language emergence in deep networks, in which we train two or more networks endowed with a communication channel to solve a task jointly, and study the communication code they develop. I will try to be precise about what “compositionality” mean in this context, and I will report the results of proof-of-concept and larger-scale experiments suggesting that (non-circular) compositionality is not a necessary condition for good generalization (of the kind illustrated in the figure). Moreover, I will show that often there is no reason to expect deep networks to find compositional languages more “natural” than highly entangled ones. I will conclude by suggesting that, if fast generalization is what we care about, we might as well focus directly on enhancing this property, without worrying about the compositionality of emergent neural network languages.</p>
</section>
<section id="references" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p>https://cs.stanford.edu/people/karpathy/cnnembed/,</p></li>
<li><p>https://www.inverse.com/article/12664-google-s-alphago-supercomputer-wins-second-go-match-vs-lee-sedol</p></li>
<li><p>https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643</p></li>
<li><p>Lazaridou et al.&nbsp;ICLR 2017 <a href="https://arxiv.org/abs/1612.07182">Multi-Agent Cooperation and the Emergence of (Natural) Language</a></p></li>
<li><p><span class="citation" data-cites="bouchacourt2018agents">Bouchacourt and Baroni (2018)</span> <a href="https://arxiv.org/pdf/1808.10696v2">How agents see things: On visual representations in an emergent language game</a></p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-bouchacourt2018agents" class="csl-entry">
Bouchacourt, Diane, and Marco Baroni. 2018. <span>“How Agents See Things: On Visual Representations in an Emergent Language Game.”</span> <em>arXiv Preprint arXiv:1808.10696</em>.
</div></div><p>Are emergent languages compositional?</p>
<ul>
<li>Andreas ICLR 2019,</li>
<li>Choi et al ICLR 2018,</li>
<li>Havrylov &amp; Titov NIPS 2017,</li>
<li>Kottur et al EMNLP 2017,</li>
<li>Mordatch &amp; Abbeel AAAI 2018,</li>
<li>Resnick et al AAMAS 2020</li>
</ul>
<p>A compositional language is one where it is easy to read out which parts of a linguistic expression refer to which components of the input</p>
<dl>
<dt>Naïve compositionality</dt>
<dd>
a language is naïvely compositional if the atomic symbols in its expressions refer to single input elements, independently of either input or linguistic context
</dd>
</dl>
<ul>
<li>Chaabouni, Kharitonov et al.&nbsp;ACL 2020 <a href="https://aclanthology.org/2020.acl-main.407/">Compositionality and Generalization In Emergent Languages</a></li>
</ul>
<p>Quantifying (one type of) naïve compositionality</p>
<p><strong>Positional disentanglement</strong> measures strong form of naïve compositionality: to what extent do symbols in a certain position univocally refer to different values of the same attribute</p>
<p>note - the paper has two other measures</p>
<p>Do emergent languages support generalization?</p>
<p>Is compositionality needed for generalization?</p>
<ul>
<li>kind of obvious, particularly if parameters are shared not but is should help</li>
</ul>
<p>Lazaridou et al ICLR 2018</p>
<ul>
<li><p><a href="https://github.com/facebookresearch/EGG">EGG: Emergence of lanGuage in Games</a></p></li>
<li><p>Kharitonov and Baroni: Emergent Language Generalization and Acquisition Speed are not Tied to Compositionality <a href="https://arxiv.org/abs/2004.03420">Emergent Language Generalization and Acquisition Speed are not tied to Compositionality</a></p></li>
</ul>
</section>
<section id="video" class="level2">
<h2 class="anchored" data-anchor-id="video">Video</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/mi1q3Fbm9zg" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="slides-and-a-paper" class="level2">
<h2 class="anchored" data-anchor-id="slides-and-a-paper">Slides &amp; and a paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./slides.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="slides"><embed src="./slides.pdf" class="col-page" width="1000" height="800"></a></p>
<figcaption>slides</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Linguistic generalization and compositionality in modern artificial neural networks"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Linguistic generalization and compositionality in modern artificial neural networks</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Is Compositionality Overrated? {The} View from Language
    Emergence},
  date = {2024-09-01},
  url = {https://orenbochman.github.io/posts/2024/marco-baoni-composionality/summary.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Is Compositionality Overrated? The View from
Language Emergence.”</span> September 1, 2024. <a href="https://orenbochman.github.io/posts/2024/marco-baoni-composionality/summary.html">https://orenbochman.github.io/posts/2024/marco-baoni-composionality/summary.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/marco-baoni-composionality/summary.html</guid>
  <pubDate>Sat, 31 Aug 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Stumpy</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/</link>
  <description><![CDATA[ 





<section id="stumpy---time-series-analysis" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="stumpy---time-series-analysis">Stumpy - Time Series Analysis</h2>
<p>Stumpy <span class="citation" data-cites="law2019stumpy">(Law 2019)</span> is a powerful and scalable library for computing a matrix profile which can be used for a variety of time series data mining tasks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-law2019stumpy" class="csl-entry">
Law, Sean M. 2019. <span>“<span class="nocase">STUMPY: A Powerful and Scalable Python Library for Time Series Data Mining</span>.”</span> <em><span>The Journal of Open Source Software</span></em> 4 (39): 1504.
</div><div id="ref-sean2021online" class="csl-entry">
———. 2021. <span>“Modern Time Series Analysis with STUMPY.”</span> <a href="https://youtu.be/XKNdXN-Jfmo" class="uri">https://youtu.be/XKNdXN-Jfmo</a>.
</div></div><p>This post is based on <span class="citation" data-cites="sean2021online">(Law 2021)</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TLDR <span class="emoji" data-emoji="peanuts">🥜</span>
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Are you <strong>Stomped</strong> on time series analysis, <strong>Stumpy</strong> may be the library for you. <span class="emoji" data-emoji="grinning">😀</span></li>
<li>Stumpy is a FOSS library with scalable algorithms for computing a <code>matrix profile</code> which can be used for a variety of time series data mining tasks.</li>
<li>It is easy to use and has a simple API that makes it easy to get started with time series analysis.</li>
</ul>
</div>
</div>
<section id="introduction" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>. It is easy to use and has a simple API that makes it easy to get started with time series analysis.</p>
<p>Under the hood Stumpy calculates the pairwise euclidean distance between two subsequences of a time series. By considering all possible subsequences of a time series of a fixed length, we get a <code>distance matrix</code> which is a matrix of distances between subsequences. However the matrix profile is too slow to compute using brute force and is also memory intensive to store. The research behind Stumpy uses a number of optimizations to make the computation of the <code>matrix profile</code> which is list the nearest neighbors for each subsequence in a time series. This structure is <img src="https://latex.codecogs.com/png.latex?O(n)"> in space and <img src="https://latex.codecogs.com/png.latex?O(n%5E2)"> time complexity.</p>
<p>Note: before computing the matrix profile, Stumpy first normalises the subsequences to have zero mean and unit variance. This is done to ensure that the distance between subsequences is meaningful and that the matrix profile is accurate. <sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;This is a common step in time series analysis and is done to remove any trends or seasonality in the data. It can also be disabled if needed.</p></div></div><p>why do we need to compute the matrix profile? The matrix profile is a powerful tool for time series analysis that can be used to identify patterns in the data.</p>
<blockquote class="blockquote">
<p>Given the <a href="https://www.cs.ucr.edu/%7Eeamonn/MatrixProfile.html">matrix profile</a>, most time series data mining tasks are trivial or easy to solve in a few lines of code. - Emonnn Keogh</p>
</blockquote>
<p>For example, motif discovery, discord discovery, semantic segmentation, and shapelet discovery can all be solved using the matrix profile.</p>
<p>By comparing the distance between subsequences, Stumpy can identify patterns in the data such as motifs, discords, chains, and other patterns. and then uses the matrix profile to identify motifs, discords, chains, and other patterns in the data.</p>
</section>
<section id="motivation" class="level3">
<h3 class="anchored" data-anchor-id="motivation">Motivation</h3>
<p>You get tasked with analyzing a time series data set there are many (10000+) points what do you do?</p>
<p>Magic Spells for Time Series Analysis:</p>
<ul>
<li>Visualizations
<ul>
<li>great for small data sets</li>
<li>not suitable with more than 1000 data points</li>
</ul></li>
<li>Statistics
<ul>
<li>traditional stats are not as meaningful for time series data</li>
<li>with many points we tend to care more about subsequences than points.</li>
</ul></li>
<li>ARIMA (AutoRegressive Integrated Moving Average)
<ul>
<li>autoregressive model assuming some kind of repeating patterns</li>
</ul></li>
<li>Anomaly Detection
<ul>
<li>assuming that we can agree what is normal</li>
</ul></li>
<li>ML (Predictive Modeling)
<ul>
<li>assuming that we can predict the future</li>
<li>using features to predict values based on a projection or interpolation.</li>
</ul></li>
<li>Forecasting
<ul>
<li>Using trends, historiacal and statistical data.</li>
<li>forecasting is for longer term frames</li>
</ul></li>
<li>Clustering
<ul>
<li>grouping similar parts of a time series together</li>
<li>grouping similar time series together</li>
</ul></li>
<li>Dynamic Time Warping
<ul>
<li>comparing time series that are not aligned</li>
<li>comparing time series that are not the same length</li>
<li>assuming we know enough about the time series to do this.</li>
</ul></li>
<li>Change Detection
<ul>
<li>detecting when a time series changes regimes.</li>
</ul></li>
</ul>
</section>
<section id="stumpys-approach" class="level3">
<h3 class="anchored" data-anchor-id="stumpys-approach">Stumpy’s Approach</h3>
<p>Stumpy tries to answer two questions:</p>
<ol type="1">
<li>Do any subsequences appear more than once in a time series?</li>
<li>If they are such subsequences, what are they and where do they appear?</li>
</ol>
<p>the design goals seem to be</p>
<ul>
<li>be easy to interpret</li>
<li>use/data agnostic</li>
<li>no prior knowledge of the data</li>
<li>parameter free</li>
</ul>
</section>
<section id="features" class="level3">
<h3 class="anchored" data-anchor-id="features">Features</h3>
<p>Stumpy has a number of features that make it a powerful tool for time series analysis. Some of the key features of Stumpy include:</p>
<ul>
<li>High level features:
<ul>
<li>Motif Discovery</li>
<li>Discord Detection</li>
<li>TS Chains</li>
<li>Semantic Segmentation</li>
<li>Shapelet &amp; Snippets</li>
<li>MPdist Clustering</li>
<li>Multi-dimensional TS</li>
</ul></li>
<li>Low level features:
<ul>
<li>Fast and memory efficient computation of matrix profiles.</li>
<li>Support for both univariate and multivariate time series data.</li>
<li>Support for both fixed-length and variable-length time series data.</li>
<li>Support for both Euclidean and DTW distance measures.</li>
<li>Support for both exact and approximate matrix profile computation.</li>
<li>Support for both CPU and GPU computation.</li>
<li>Support for both online and offline matrix profile computation.</li>
</ul></li>
</ul>
</section>
<section id="big-idea" class="level3">
<h3 class="anchored" data-anchor-id="big-idea">Big idea</h3>
</section>
<section id="novelty" class="level3">
<h3 class="anchored" data-anchor-id="novelty">Novelty</h3>
</section>
<section id="papers" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="papers">Papers</h3>
<p>Stumpy is based on a number of research papers that have been published in the field of time series data mining. Some of the key papers that have inspired Stumpy include:</p>
<ul>
<li><span class="citation" data-cites="Yeh2016MatrixProfileI">(Yeh et al. 2016)</span></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-Yeh2016MatrixProfileI" class="csl-entry">
Yeh, Chin-Chia Michael, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva, Abdullah Mueen, and Eamonn Keogh. 2016. <span>“Matrix Profile i: All Pairs Similarity Joins for Time Series: A Unifying View That Includes Motifs, Discords and Shapelets.”</span> In <em>2016 IEEE 16th International Conference on Data Mining (ICDM)</em>, 1317–22. <a href="https://doi.org/10.1109/ICDM.2016.0179">https://doi.org/10.1109/ICDM.2016.0179</a>.
</div></div><p>Zhu, Yan, et al.&nbsp;(2016) Matrix Profile II: Exploiting a Novel Algorithm and GPUs to Break the One Hundred Million Barrier for Time Series Motifs and Joins. ICDM:739-748.</p>
<p>Yeh, Chin-Chia Michael, et al.&nbsp;(2017) Matrix Profile VI: Meaningful Multidimensional Motif Discovery. ICDM:565-574.</p>
<p>Zhu, Yan, et al.&nbsp;(2017) Matrix Profile VII: Time Series Chains: A New Primitive for Time Series Data Mining. ICDM:695-704.</p>
<p>Gharghabi, Shaghayegh, et al.&nbsp;(2017) Matrix Profile VIII: Domain Agnostic Online Semantic Segmentation at Superhuman Performance Levels. ICDM:117-126.</p>
<p>Zhu, Yan, et al.&nbsp;(2017) Exploiting a Novel Algorithm and GPUs to Break the Ten Quadrillion Pairwise Comparisons Barrier for Time Series Motifs and Joins. KAIS:203-236.</p>
<p>Zhu, Yan, et al.&nbsp;(2018) Matrix Profile XI: SCRIMP++: Time Series Motif Discovery at Interactive Speeds. ICDM:837-846.</p>
<p>Yeh, Chin-Chia Michael, et al.&nbsp;(2018) Time Series Joins, Motifs, Discords and Shapelets: a Unifying View that Exploits the Matrix Profile. Data Min Knowl Disc:83-123.</p>
<p>Gharghabi, Shaghayegh, et al.&nbsp;(2018) “Matrix Profile XII: MPdist: A Novel Time Series Distance Measure to Allow Data Mining in More Challenging Scenarios.” ICDM:965-970.</p>
<p>Zimmerman, Zachary, et al.&nbsp;(2019) Matrix Profile XIV: Scaling Time Series Motif Discovery with GPUs to Break a Quintillion Pairwise Comparisons a Day and Beyond. SoCC ’19:74-86.</p>
<p>Akbarinia, Reza, and Betrand Cloez. (2019) Efficient Matrix Profile Computation Using Different Distance Functions. arXiv:1901.05708.</p>
<p>Kamgar, Kaveh, et al.&nbsp;(2019) Matrix Profile XV: Exploiting Time Series Consensus Motifs to Find Structure in Time Series Sets. ICDM:1156-1161.</p>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Stumpy},
  date = {2024-08-08},
  url = {https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Stumpy.”</span> August 8, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/">https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-08-08-TS-with-Stumpy/</guid>
  <pubDate>Wed, 07 Aug 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>two ideas on generelization</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html</link>
  <description><![CDATA[ 





<section id="feature-selection-problem" class="level2">
<h2 class="anchored" data-anchor-id="feature-selection-problem">Feature selection problem</h2>
<p>the first idea comes from looking at regression and hyperparameter optimization tasks</p>
<p>First we want to investigate the idea of finding the best values of individual parameters or hyperparameters. Next if these are second order effects corresponding to the interaction of two or more parameters, we will want to investigate if there are pairs of hyper/parameters that are more important than others.</p>
<ul>
<li>one way to orgenize this search might be characterised as <strong>a reductionist search</strong>, where we start with a simple model and add complexity as needed.</li>
<li>another way to orgenize this search might be characterised as <strong>a covariance matrix bingo</strong>, where we concieve of all the possible interactions reperesented by the covariance matrix, in reality the covariance matrix is a symmetric and sparse, we may discover that certain features are correlated with each other, or better yet collinear, and we may want to remove these features and retain perhaps better <strong>orthonormal features</strong>. In other words we might be able to plan our model selection by looking at how the covariance matrix evolves as we add features to the model. We will end up with a subset of the matrix similar to a bingo card, where we wish to get to this wininning combination of features faster than the other players.</li>
</ul>
<p>A second reality of this issues adressable by RL is that high order effects tend to be increasingly sparse so that if we have a hint of that certain slots in the covariance matrix are non zero we should defintely explore those efects more than others. This infact suggests an improvement to baysian search.</p>
<p>This aspect of feature selection is not partularly exciting, but suggests a more abstract way of thinking about the problem. This leads to a second perhaps more powerful idea.</p>
</section>
<section id="localised-subspace-embedding" class="level2">
<h2 class="anchored" data-anchor-id="localised-subspace-embedding">Localised subspace embedding</h2>
<p>The idea of embedings which allow us to use a distributed representation instead of a one hot encoding is very powerful. In one sense it is the opposite of the reductionist notion mentioned above. Instead of trying to find the best features we are trying to find the best representation of the features. However if one is familiar with PCA or SVD one knows that the best representation is often some linear combination of the actual features we observe. A reductionist might view these a generlized coordinates that are more useful for inverstigating the feature space.</p>
<p>If our features are built from such embeddings, we learn weights that correspond to the importance of the features in the model. This representation is still subject to the bias variance tradeoff and we generaly have many parameters in Neural Networks. One way to view this is to try and orgeinze the model so that it can use emebeddings of subspaces - corresponding to features that are balance generlization and discrimination.</p>
<p>A second point is that in different contexts we might need to use different features. This is much easier to see in RL and NLP. Building a embedding that is localised to a just some features and some observations/states might allow the model to get good generalization then by considering all the features at once. This is an analogue of the idea of factoring a distribution into a product of marginals, particularly in the case of a much larger bayesian network. In this case though we might be talking about using two such factorizations, with some discriminator selecting the observations that are used in different contexts.</p>
<p>We might think of a neural network as evolving system of that learns to bifurcate the distributed representation of the features of the data set in the input into any number of<br>
smaller and more localised subspaces. The more Localised subspaces are more likely to be linearly separable and thus easier to learn.</p>
<p>However all this happens by breaking symmetries using the random aspects of the learning algorithm. Minibatches present many random samples which carry differnt payloads of information. Certain such payloads may reinforce the current network weights, while the next may require a bifurcation of the representation into two to minimize the loss. Another might require many bifurcations and may not lead to any new bifurcations or reinforcements. Drop out breaks symmetries by shutting down parts of the network temporarily.</p>
<p>On other problems with generalization in RL and in NLP might be resolved using local subspace embedding of the state space. These are features that conflates states that are similar and distinguishes states that are different. By avoiding a full embedding we reduce the variance of the function approximation and thus improve generalization.</p>
</section>
<section id="question" class="level2">
<h2 class="anchored" data-anchor-id="question">Question</h2>
<ul>
<li>How can we encourage the NN model to learn localised subspaces?</li>
<li>Howe can query the model to learn/interpret the localised subspaces?</li>
<li>If the model learns a powerfull feature how can we give access to it to other parts of the model? (this would reduce learning time and increase generalization)
<ul>
<li>all discrminators following the features will be have one use of the feature.</li>
<li>could we let all other nodes in the network have a residual connection to the feature?</li>
</ul></li>
<li>if we wanted to further refine a representation of a localised subspace of verbs to intransitive verbs how would we do that?
<ul>
<li>we would like to lean a discriminator that can tell the difference between transitive and intransitive verbs and then use it to gate the input to the verb feature.</li>
<li>however we might prefer to do better than that and learn a better representation of the<br>
transitive and intransitive verbs. This would need learning different weights for the different verbs. This means we want to bifurcate the verb fearture subnetwork into two replicates but add the discriminator as a gate to the input of the two subnetworks.</li>
<li>another point worth considering is that once we have learned a good representation of both<br>
the transitive and intransitive verbs we can use these as features in the next layers of the network. We sould be able to combine them to get a better representation than just the original verb.</li>
<li>I recon this happens many times in LLMs. What we might want is to have some way for the model to attend to all the subspaces it has learned and use them to guide its learning.</li>
<li>The challange seems to be in identification of the subspaces. We may be using differnt basis for each subspace etc which may lead to difficulty in reusing them.</li>
</ul></li>
</ul>
</section>
<section id="attention-heads" class="level2">
<h2 class="anchored" data-anchor-id="attention-heads">Attention heads</h2>
<p>It seems thogh that attention heads are a good way to orgenize and increase the diversity of the localised subspaces. This is because the attention heads can be trained to focus on different parts of the input and thus can be used to create a localised subspace embedding. This is a very powerful idea and is the basis of the transformer architecture.</p>
</section>
<section id="residual-rerouting" class="level2">
<h2 class="anchored" data-anchor-id="residual-rerouting">Residual rerouting</h2>
</section>
<section id="guided-bifuration-along-side-the-spontaneous-symetry-breaking" class="level2">
<h2 class="anchored" data-anchor-id="guided-bifuration-along-side-the-spontaneous-symetry-breaking">Guided bifuration along side the spontaneous symetry breaking</h2>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Two Ideas on Generelization},
  date = {2024-07-01},
  url = {https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Two Ideas on Generelization.”</span> July 1,
2024. <a href="https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html">https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html</guid>
  <pubDate>Sun, 30 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>mesa &amp; rl</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-06-25-mesa-rl/mesa-rl.html</link>
  <description><![CDATA[ 





<section id="trajectory-trace-replay---based-on-mesa-caching." class="level2">
<h2 class="anchored" data-anchor-id="trajectory-trace-replay---based-on-mesa-caching.">Trajectory trace &amp; replay - based on <code>Mesa</code> caching.</h2>
<section id="tldr" class="level3">
<h3 class="anchored" data-anchor-id="tldr">TLDR:</h3>
<p>Ifs we can store state, action, reward trajectories (to a file) this would facilitate</p>
<ol type="1">
<li>Off-policy sample based learning</li>
<li>model based learning</li>
<li>deep RL learning of NN in certain modern algs.</li>
</ol>
<p>RL Recap for ABM:</p>
<ol type="1">
<li>in RL there is a Markov Decision Process MDP that genereates a sequence</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0As_0,%20a_0,%20r_0,%20s_1,%20a_1,%20r_1%20%5Cldots%0A"></p>
<p>s is the state. a the action . r the reward.</p>
<p>the goal of RL is to learn an optimal policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> in the sense of generating the maximum expected total rewards. by picking the the best action at each step.</p>
<p>note: the reward is for getting to the next state s’ using the action a.</p>
<ol start="2" type="1">
<li>In model based RL agents learn a model of the environment to facilitate many quick planning steps between the more expensive/risky interactions with the environment.</li>
</ol>
<p>the model is typically two functions</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?P(s'%7Cs,a,)"> - the Markov chain transitions denoted as T</li>
<li><img src="https://latex.codecogs.com/png.latex?R(r%7Cs,a,s')"> - the reward for the above transition</li>
</ul>
<ol start="3" type="1">
<li>in deep RL agents use ML techinques to approximate with a NeuralNet</li>
</ol>
<p>the action value function <img src="https://latex.codecogs.com/png.latex?Q_%5Cpi(s,a)"> the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> or the model.</p>
<ol start="3" type="1">
<li>off policy learning.</li>
</ol>
<p>A more general setting in RL has 2 policies</p>
<ul>
<li>the behavioral policy - which determines the actions of the agents
<ul>
<li>Deterministic random policy - all possible actions have equal probability</li>
<li>Epsilon soft - all possible actions have at least epsilon probability</li>
</ul></li>
<li>the target policy - which is a better
<ul>
<li>pi star - the optimal policy.</li>
</ul></li>
</ul>
<p>the point:</p>
<p>if we generate a trace of S,A,R … from an ABM model and the ABM (plus some techincal caveats - it is an MDP, the ABM behaviour is ergodic etc) Then rl agents can use these traces to train a more optimal agent using off policy sample based learning methods like:</p>
<ul>
<li><p>first visit MC with importance sampling</p></li>
<li><p>any visit MC with importance sampling</p></li>
<li><p>Q-learning,</p></li>
<li><p>Expected Sarsa</p></li>
<li><p><code>Dyna-Q+</code> vcan learn a model and use it for efficent planning.</p>
<ul>
<li>e.g.&nbsp;how to navigate in a maze which changes over time.</li>
</ul></li>
</ul>
</section>
<section id="the-api" class="level3">
<h3 class="anchored" data-anchor-id="the-api">The API</h3>
</section>
<section id="model" class="level3">
<h3 class="anchored" data-anchor-id="model">Model</h3>
<ol type="1">
<li>I don’t think its worth converting all ABM model to RL</li>
<li>many ABM don’t have rewards structure.</li>
<li>some don’t have much states</li>
<li>doing this would mean being creative</li>
</ol>
<p>Say we have a forest fire sim which we could extend as follows</p>
<p>add two new agents:</p>
<pre><code>- fire starter - total rewards trees burnt 
    - can light a fire
- fire fighter - total trees remaining.
    - can cut/move a tree </code></pre>
<ul>
<li><p>fire fighter can cut down k trees before the start of the simulation</p></li>
<li><p>fire starter can light a fire in some location x</p></li>
<li><p>fire fighter can cut down k more trees</p>
<ul>
<li>e.g.&nbsp;can we cut down some trees to stop a forest fire from spreading.</li>
</ul></li>
</ul>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Mesa \&amp; Rl},
  date = {2024-06-25},
  url = {https://orenbochman.github.io/posts/2024/2024-06-25-mesa-rl/mesa-rl.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Mesa &amp; Rl.”</span> June 25, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-06-25-mesa-rl/mesa-rl.html">https://orenbochman.github.io/posts/2024/2024-06-25-mesa-rl/mesa-rl.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-06-25-mesa-rl/mesa-rl.html</guid>
  <pubDate>Mon, 24 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>zero inflated data</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-06-23-zero-inflated-data/2024-06-23-zero-inflated-data.html</link>
  <description><![CDATA[ 





<p>Zero inflated data is a type of data that has a large number of zero values.</p>
<p>This type of data is common in many fields, including biology, economics, and social sciences.</p>
<p>In this post, we will explore how to analyze zero inflated data.</p>
<p>Imagine you are creating a regression model to predict the sales for a product as a function of price.</p>
<p>You have a dataset that contains the sales data for the product, as well as the price of the product.</p>
<p>Looking at the data you notice</p>
<ul>
<li>as price increases the number of sales decreases.</li>
<li>there are many days where no sales are made.</li>
<li>the regression line does not fit the data well.</li>
</ul>
<p>when you try to dive deeper into the data you notice that</p>
<ol type="1">
<li>some products when out of stock cannot be bought until the next shipment arrives.</li>
<li>occasionaly the IT system fails and no sales are recorded.</li>
<li>When acounting for 1,2 demand is stochastic. there is always a chance that no sales are made on a certain day.</li>
<li>For high value products, if we are more expensive than the competition by some threshold, chances of zero sales are high.</li>
<li>Sales get canceled but it can also be credited as a negative sale on some other day.</li>
<li>Sales are seasonal, some days for some products are far more likely to have zero sales than others.</li>
<li>There was a marketing campaign that increased sales for a certain period of time but the budget run out and sales dropped to zero.</li>
<li>There was a well marketed promotion that made clients buy more than they needed at a low price and then they did not buy for a long time.</li>
<li>Traffic to our store for some products is mainly from a recommendation engine and that engine may stop recommending our product or drop our product’s ranking leading to zero sales.</li>
<li>When some products might get bad reviews on the site and sales might drop to zero.</li>
</ol>
<ul>
<li>While we can track some of these factors, we cannot track all of them. Also ther effects can be specific to a certain product or a certain day.</li>
<li>In the end though we cannot match cause to effect, we can only speculate why we get zero sales on some days.</li>
<li>However regardless of the root cause, we probably want our model to have a good fit to the real functional relationship between price and sales.</li>
<li>We might also want to be able to predict the stochatic nature of the demand. (i.e.&nbsp;the probability of zero sales on a certain day assuming a stochastic model of demand)</li>
<li>We might also want to know how much more zeros we are getting than we would expect from a simple model of demand.</li>
</ul>
<p>Some approaches to model zero inflated data are:</p>
<ol type="1">
<li>Zero inflated Poisson regression</li>
<li>Zero inflated negative binomial regression</li>
<li>Zero inflated generalized linear models</li>
<li>Zero inflated mixed models</li>
<li>Zero inflated hurdle models</li>
</ol>
<section id="zero-inflated-poisson-regression" class="level2">
<h2 class="anchored" data-anchor-id="zero-inflated-poisson-regression">Zero inflated Poisson regression</h2>
<p>The zero inflated Poisson regression model is a type of regression model that is used to analyze zero inflated data.</p>
<p>This model assumes that the data is generated by a Poisson distribution, but that there are two processes that can generate zeros:</p>
<ol type="1">
<li>A process that generates zeros with a certain probability, even if the Poisson process generates a non-zero value.</li>
<li>A Poisson process that generates zeros with a certain probability.</li>
</ol>
<p>The model is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_i%20=%20%5Cbegin%7Bcases%7D%0A0%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%20%5Cpi_i%20%5C%5C%0APoisson(%5Clambda_i)%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%201%20-%20%5Cpi_i%0A%5Cend%7Bcases%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Y_i"> is the observed value, <img src="https://latex.codecogs.com/png.latex?%5Cpi_i"> is the probability of a zero value, and <img src="https://latex.codecogs.com/png.latex?%5Clambda_i"> is the mean of the Poisson distribution.</p>
<p>The model can be estimated using maximum likelihood estimation.</p>
</section>
<section id="zero-inflated-negative-binomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="zero-inflated-negative-binomial-regression">Zero inflated negative binomial regression</h2>
<p>The zero inflated negative binomial regression model is a type of regression model that is used to analyze zero inflated data.</p>
<p>This model assumes that the data is generated by a negative binomial distribution, but that there are two processes that can generate zeros:</p>
<ol type="1">
<li>A process that generates zeros with a certain probability, even if the negative binomial process generates a non-zero value.</li>
<li>A negative binomial process that generates zeros with a certain probability.</li>
</ol>
<p>The model is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_i%20=%20%5Cbegin%7Bcases%7D%0A0%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%20%5Cpi_i%20%5C%5C%0ANegBin(%5Cmu_i,%20%5Ctheta)%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%201%20-%20%5Cpi_i%0A%5Cend%7Bcases%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Y_i"> is the observed value, <img src="https://latex.codecogs.com/png.latex?%5Cpi_i"> is the probability of a zero value, <img src="https://latex.codecogs.com/png.latex?%5Cmu_i"> is the mean of the negative binomial distribution, and <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the dispersion parameter.</p>
<p>The model can be estimated using maximum likelihood estimation.</p>
</section>
<section id="zero-inflated-generalized-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="zero-inflated-generalized-linear-models">Zero inflated generalized linear models</h2>
<p>The zero inflated generalized linear model is a type of regression model that is used to analyze zero inflated data.</p>
<p>This model is a generalization of the zero inflated Poisson and zero inflated negative binomial regression models.</p>
<p>The model is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_i%20=%20%5Cbegin%7Bcases%7D%0A0%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%20%5Cpi_i%20%5C%5C%0AGLM(%5Ceta_i)%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%201%20-%20%5Cpi_i%0A%5Cend%7Bcases%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Y_i"> is the observed value, <img src="https://latex.codecogs.com/png.latex?%5Cpi_i"> is the probability of a zero value, and <img src="https://latex.codecogs.com/png.latex?%5Ceta_i"> is the linear predictor.</p>
<p>The model can be estimated using maximum likelihood estimation.</p>
</section>
<section id="zero-inflated-mixed-models" class="level2">
<h2 class="anchored" data-anchor-id="zero-inflated-mixed-models">Zero inflated mixed models</h2>
<p>The zero inflated mixed model is a type of regression model that is used to analyze zero inflated data.</p>
<p>This model is a generalization of the zero inflated Poisson and zero inflated negative binomial regression models.</p>
<p>The model is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_i%20=%20%5Cbegin%7Bcases%7D%0A0%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%20%5Cpi_i%20%5C%5C%0AMixed(%5Ceta_i)%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%201%20-%20%5Cpi_i%0A%5Cend%7Bcases%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Y_i"> is the observed value, <img src="https://latex.codecogs.com/png.latex?%5Cpi_i"> is the probability of a zero value, and <img src="https://latex.codecogs.com/png.latex?%5Ceta_i"> is the linear predictor.</p>
<p>The model can be estimated using maximum likelihood estimation.</p>
</section>
<section id="zero-inflated-hurdle-models" class="level2">
<h2 class="anchored" data-anchor-id="zero-inflated-hurdle-models">Zero inflated hurdle models</h2>
<p>The zero inflated hurdle model is a type of regression model that is used to analyze zero inflated data.</p>
<p>This model is a generalization of the zero inflated Poisson and zero inflated negative binomial regression models.</p>
<p>The model is defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_i%20=%20%5Cbegin%7Bcases%7D%0A0%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%20%5Cpi_i%20%5C%5C%0AHurdle(%5Ceta_i)%20&amp;%20%5Ctext%7Bwith%20probability%20%7D%201%20-%20%5Cpi_i%0A%5Cend%7Bcases%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Y_i"> is the observed value, <img src="https://latex.codecogs.com/png.latex?%5Cpi_i"> is the probability of a zero value, and <img src="https://latex.codecogs.com/png.latex?%5Ceta_i"> is the linear predictor.</p>
<p>The model can be estimated using maximum likelihood estimation.</p>
<p>In conclusion, zero inflated data is a type of data that has a large number of zero values.</p>
<p>There are several approaches to model zero inflated data, including zero inflated Poisson regression, zero inflated negative binomial regression, zero inflated generalized linear models, zero inflated mixed models, and zero inflated hurdle models.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Zero Inflated Data},
  date = {2024-06-23},
  url = {https://orenbochman.github.io//posts/2024/2024-06-23-zero-inflated-data/2024-06-23-zero-inflated-data.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Zero Inflated Data.”</span> June 23, 2024.
<a href="https://orenbochman.github.io//posts/2024/2024-06-23-zero-inflated-data/2024-06-23-zero-inflated-data.html">https://orenbochman.github.io//posts/2024/2024-06-23-zero-inflated-data/2024-06-23-zero-inflated-data.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-06-23-zero-inflated-data/2024-06-23-zero-inflated-data.html</guid>
  <pubDate>Sat, 22 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>readings in rl</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-06-18-readings-in-rl/</link>
  <description><![CDATA[ 





<p>A list of RL-papers and resources that I have read or plan to read.</p>
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="../../../posts/2023/2023-06-01-Synthesis-and-Stabilization/2023-06-01-Synthesis-and-Stabilization.html">Synthesis-and-Stabilization</a></label></li>
</ul>
<p>I think that RL is fascinating and I have been reading a lot about it recently.</p>
<ul>
<li>Recently I took a number of courses in RL in the Coursera platform.</li>
<li>The teachers were Martha and Adam White and the courses used Barto and Sutton’s book, which is a classic.
<ul>
<li>Read a large part of the book and still plan to read some more.</li>
</ul></li>
<li>RL is currently a rapidly evolving field many modern algorithm are not covered in the book.
<ul>
<li>The basic building blocks however are mostly the same and the course taught each in its simplest setting, which is the best way to understand them, according to Adam White.</li>
<li>Later many of the ideas are combined in new and often more challenging problem settings.</li>
</ul></li>
<li>I have been reading papers and watching videos on the internet to try and keep up with the field.</li>
</ul>
<p>My approach to the book is is reductionist. I try and break down the algorithms into their innovating parts and then try and understand the parts. This seems a good fit to the book and courses that I took.</p>
<p>However, in papers the authors are trying to present their work in the best light. They typically present lost of references to other work, which seem like new ideas to all but the most expert reader. They also present lots of ideas, which are not actually realized in the paper. The reality is that most papers in the end are about one new innovation.</p>
<p>I recently made a time line of the RL and bandit algorithms that I had learned. This was an interesting exercise and I got to see that indeed one a new idea is introduced it is quickly picked up by other researchers who try to apply it to other problems, other algorithms, or different settings. There are many setting in RL and as Adam White explains in “Fundamentals of Reinforcement Learning” it is best to understand new ideas in the simplest setting possible.</p>
<p>The main thrusts of RL seem to be</p>
<ol type="1">
<li>Finding formulations of problems that can be applied to many different settings</li>
<li>Parametrizing these as differnt trade-offs for the algorithm</li>
<li>gamma - discounting factor</li>
<li>epsilon - to control exploration</li>
<li>alpha - learning rate (but adaptive schedules are better)</li>
<li>lambda - eligibility trace ??</li>
<li>n-step - how many steps to back up</li>
<li>Aggregation type in the update rule (e.g.&nbsp;sum, max, average, weighted average,minimax, moving average)</li>
<li>First/any-visit - for the Monte-Carlo methods</li>
<li>Behavioral policy (exploratory) target policy (exploitative) - for off policy learning</li>
<li>Weighted/Unweighted/doubly robust/ for importance sampling</li>
<li>Finding algorithms that can be applied to many different settings</li>
</ol>
<p>However a second circle of ideas is covered in the course and book. These too are worth listing here. If only to create a checklist of things to consider when implementing an RL system or coming up with a new algorithm.</p>
<ol type="1">
<li><strong>Intrinsic motivation</strong> in RL agents</li>
</ol>
<ul>
<li>Use of <strong>Utility functions</strong> as motivation for agents based on preferences</li>
<li>Use of <strong>Curiosity</strong> as motivation for agents based on novelty</li>
<li>Use of <strong>Empowerment</strong> as motivation for agents based on control</li>
<li>Use of <strong>Causal Influence</strong> in constructing intrinsic social motivation</li>
</ul>
<ol start="3" type="1">
<li>The reward hypothesis - that all goals can be described by the maximization of the expected reward
<ul>
<li>Alternatively the idea that the agent might benefit from multiple reward signals.</li>
<li>Can I get a cookie cutter function to
<ul>
<li>combine multiple reward signals
<ul>
<li>to a scalar reward signal</li>
<li>to a tensor reward signal with an attention mechanism that can retrieve the most relevant reward signal</li>
<li>to a pareto optimal signal</li>
</ul></li>
<li>learn using multiple reward signals ?</li>
</ul></li>
</ul></li>
<li>The notion of accelerating learning by prioritizing the updates based in planning.</li>
</ol>
<ul>
<li>can we do prioritized sweeps in non deterministic environments ?</li>
<li>what about continuous state spaces ?</li>
</ul>
<ol start="5" type="1">
<li>The idea of using a model to simulate the environment and improve the policy.</li>
<li>The idea of doubly robust estimators for correct importance sampling of small trajectory samlples in off policy learning. This could be the cornerstone of few shot learning algorithms.</li>
<li><strong>Covariate Shift</strong> - the idea that the distribution of states changes as the policy changes.
<ul>
<li>This is a problem for off policy learning.</li>
<li>This may also be an issue in actor critic methods where the policy is updated based on the value function.</li>
</ul></li>
</ol>
<p>Hinton’s in his course and in <span class="citation" data-cites="Nowlan1992SimplifyingNN">(Nowlan and Hinton 1992)</span> at <a href="https://www.cs.utoronto.ca/~hinton/absps/sunspots.pdf">paper</a> describes his approach where by initilizing neural networks with zero values this in effect creates a network that that can grow in capacity as it learns. This can continue until all the weights are non-zero.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Nowlan1992SimplifyingNN" class="csl-entry">
Nowlan, S., and Geoffrey E. Hinton. 1992. <span>“Simplifying Neural Networks by Soft Weight-Sharing.”</span> <em>Neural Computation</em> 4: 473–93. <a href="https://www.cs.utoronto.ca/~hinton/absps/sunspots.pdf">https://www.cs.utoronto.ca/~hinton/absps/sunspots.pdf</a>.
</div></div><p>Two other ideas - weight sharing and sparsity inducing penalties and constraints can also be used to control the capacity of the network.</p>
<p>This idea is also of some interest to RL where different function approximation methods use neural networks to approximate value functions, V,Q or the policy pi. Modle based methods also use neural networks to approximate the model of the environment which typicaly is a state transition function M(s’|a,s) and a reward function R(s’|a,s) though a four part dynamics model can also be used p(s’,r|a,s).</p>
<p>A central issue is that that when approximating these on continuous state the functions treat regions of the state space as equivalent. This can creates generalization when states are very similar (same state, highly correlated or have high mutual information) But so long as the function approximator fails to distinguish between states that are very different there will be a problem. The above ideas on controlling the capacity of the network could be used to allow the capacity of the network to grow as the agent learns thus perhaps allowing coarse features to be learned first and then more fine grained features to be learned later.</p>
<p>We see that this dependence between states is a breaking point to translating algorithms for tabular methods to setting with function approximation.</p>
<p>the V,Q functions are approximated by neural networks. In the course Adam White describes how the capacity of the network. In the course Adam White describes how the capacity of the network can be controlled by the number of hidden units in the network.</p>
<p>He also discuses how we can slow this down by using a sparsity inducing penalty. These are L1 and L2 regularizations terms on the weights.</p>
<p>As I recall weights and gradients in neural networks get smaller as the network gets deeper. This is because the gradients are multiplied by the weights in the backpropagation algorithm. This can lead to the vanishing gradient problem. One way to solve this is to use a sparsity inducing penalty on the weights. This can be done by adding a term to the loss function that encourages the weights to be zero. This is a form of regularization that is different from the L1 and L2 regularization that we have seen before.</p>
<p>But today I was thinking that we might use a per layers multiplier on the weights so as to rescale the weights and their gradients. This could improve numerical stability and help to avoid the vanishing gradient problem.</p>
<p>encourage them to be zero. This is a form of regularization that is different from</p>
<ul>
<li>From <a href="https://katefvision.github.io/">Deep Reinforcement Learning and Control Spring 2017, CMU 10703</a></li>
</ul>
<p>there are about 42 papers that are covered in the course.</p>
<ul>
<li><p>some I have already read</p></li>
<li><p>others seen to be on robotics</p></li>
<li><p>others are on deep learning but may be dated by now.</p></li>
<li><p>is there a more recent version of the course ?</p></li>
<li><p><a href="https://cmudeeprl.github.io/403website_s24/lectures/">10-403 Deep RL</a> - Spring 2024 seems less daunting and covers more algorithms I found missing from the Coursera specialization.</p>
<ul>
<li>Natural Policy Gradient - PG</li>
<li>Proximal Policy Optimization - PPO</li>
<li>Trust Region Policy Optimization - TRPO <span class="citation" data-cites="Schulman2015TrustRP">(Schulman et al. 2015)</span> <a href="https://arxiv.org/abs/1502.05477">paper</a></li>
</ul></li>
<li><p>OpenAI Gym tutorial <a href="https://colab.research.google.com/drive/1PDdfwG1cZB6YXYsqkask6iDw3_XoYHTR">notebook</a></p></li>
<li><p><label><input type="checkbox" checked=""><span class="citation" data-cites="Russo2017ATO">(Russo et al. 2017)</span> <a href="https://arxiv.org/abs/1707.02038">A Tutorial on Thompson Sampling</a></label></p></li>
<li><p><label><input type="checkbox"><a href="https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf">The Development of Embodied Cognition: Six Lessons from Babies</a></label></p></li>
<li><p><label><input type="checkbox" checked=""><a href="https://www.ted.com/talks/daniel_wolpert_the_real_reason_for_brains/transcript?language=en">The real reason for brains</a></label></p></li>
<li><p><label><input type="checkbox"><a href="https://arxiv.org/abs/1703.03864">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></label></p></li>
<li><p><label><input type="checkbox"><a href="https://arxiv.org/pdf/1604.00772">The CMA Evolution Strategy: A Tutorial</a></label></p></li>
<li><p><label><input type="checkbox"><a href="https://arxiv.org/abs/1407.3501">Robots that can adapt like animals</a> So here are some algorithms I read about and papers to cover.</label></p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-Schulman2015TrustRP" class="csl-entry">
Schulman, John, S. Levine, P. Abbeel, Michael I. Jordan, and Philipp Moritz. 2015. <span>“Trust Region Policy Optimization.”</span> <em>ArXiv</em> abs/1502.05477.
</div><div id="ref-Russo2017ATO" class="csl-entry">
Russo, Daniel, Benjamin Van Roy, Abbas Kazerouni, and Ian Osband. 2017. <span>“A Tutorial on Thompson Sampling.”</span> <em>ArXiv</em> abs/1707.02038.
</div></div><p>Some papers</p>
<ul>
<li><a href="https://www.deepstack.ai">DeepStack</a>: Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker</li>
<li>AlphaGoZero</li>
<li>AlphaMuZero</li>
<li>AlphaStar</li>
</ul>
<section id="making-algrothims-that-can-transfer-to-different-problems" class="level2">
<h2 class="anchored" data-anchor-id="making-algrothims-that-can-transfer-to-different-problems">Making algrothims that can transfer to different problems</h2>
<ul>
<li><p>IMPALA algorithm</p></li>
<li><p>Using Impala with Pixels</p></li>
<li><p><a href="https://arxiv.org/abs/1710.02298">Rainbow - Combining Improvements in Deep Reinforcement Learning</a> Combine the following DQN variants to make Rainbow variant.</p>
<ol type="1">
<li>DDQN fix bias in DQN by decoupling selection and evaluation of the bootstrap action</li>
<li>Prioritized experience replay - improves data efficiency, by replaying more often transitions from which there is more to learn.</li>
<li>The dueling network architecture - helps to generalize across actions by separately representing state values and action advantages</li>
<li>Learning from multi-step bootstrap targets A3C - shifts the bias-variance trade off and helps to propagate newly observed rewards faster to earlier visited states.</li>
<li>Distributional RL - learns a distribution over returns rather than the expected return. This can help to capture the uncertainty in the value estimates and can lead to better policies.</li>
<li>Noisy DQN uses stochastic network layers for exploration.</li>
<li>Distributional Q-learning - learns a categorical distribution of discounted returns, instead of estimating the mean</li>
</ol></li>
<li><p>PopArt - Normalizing for multiple goals with multiple time scales</p></li>
<li><p>R2D2 - Recurrent Experience Replay in Distributed Reinforcement Learning</p></li>
<li><p>PAIRED - <a href="https://arxiv.org/abs/1906.02994">Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design</a></p></li>
<li><p><a href="https://natashajaques.ai/publication/social-influence-as-intrinsic-motivation-for-multiagent-deep-reinforce/">social influence as intrinsic motivation for multiagent deep reinforce</a></p></li>
<li><p>https://openai.com/index/meta-learning-for-wrestling/</p></li>
<li><p>JaxMARL: Multi-Agent RL Environments in JAX</p></li>
<li><p>https://ai.stanford.edu/users/nir/Papers/DFR1.pdf</p></li>
</ul>
<p>some ideas</p>
<ul class="task-list">
<li><label><input type="checkbox"><a href="https://arxiv.org/pdf/1604.00772">The CMA Evolution Strategy: A Tutorial</a></label></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Readings in Rl},
  date = {2024-06-18},
  url = {https://orenbochman.github.io//posts/2024/2024-06-18-readings-in-rl},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Readings in Rl.”</span> June 18, 2024. <a href="https://orenbochman.github.io//posts/2024/2024-06-18-readings-in-rl">https://orenbochman.github.io//posts/2024/2024-06-18-readings-in-rl</a>.
</div></div></section></div> ]]></description>
  <category>rl</category>
  <category>reinforcement learning</category>
  <category>papers</category>
  <category>notes</category>
  <category>reading</category>
  <category>rl-papers</category>
  <category>rl-algorithms</category>
  <category>rl-resources</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-06-18-readings-in-rl/</guid>
  <pubDate>Mon, 17 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Hyperparameter Optimization</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-06-13-hyper/2024-06-13-hyper.html</link>
  <description><![CDATA[ 





<section id="hyperparameter-optimization" class="level1">
<h1>Hyperparameter Optimization</h1>
<section id="best-model" class="level2">
<h2 class="anchored" data-anchor-id="best-model">best model</h2>
<p>one the idea is for a set of Hyperparameters to find the best one.</p>
<ul>
<li>start with random search.</li>
<li>for k in 1 to |hyper|:</li>
<li>look for k hyperparameters which lead to correlated results.</li>
<li>try to find the ranges where the correlation is valid</li>
<li>use the correlation to search for the best joint hyperparameters.</li>
</ul>
<p>basically, we are trying to infer if <img src="https://latex.codecogs.com/png.latex?loss(a_1,a_2)"> is correlated with <img src="https://latex.codecogs.com/png.latex?a_1"> and <img src="https://latex.codecogs.com/png.latex?a_2">. each time we find such a relationship we can limit the search space to a subspace of the hyperparameter space. For a + correlation, we know that by increasing <img src="https://latex.codecogs.com/png.latex?a_1"> and <img src="https://latex.codecogs.com/png.latex?a_2"> we get a better result. For a - correlation, we know that by decreasing <img src="https://latex.codecogs.com/png.latex?a_1"> and increasing <img src="https://latex.codecogs.com/png.latex?a_2"> we get a better result.</p>
<p>While we test these we should look for other correlations between hyperparameters. either with a_1 or a_2 or with within some other subset of hyperparameters.</p>
</section>
<section id="fastest-model" class="level2">
<h2 class="anchored" data-anchor-id="fastest-model">fastest model</h2>
<p>the above is a good idea but we might do better if we can train faster and reject hyperparameters which are slow learners.</p>
<p>another idea is to not find the best model first but the fasterst learner.</p>
<p>we could do this iteratively by training an epoch per model and then selecting the models which converges the fastest. we can then pick the neighbours of the fastest model and train them for another epoch. as we gain confidence in the fastest learner we can use it to guide the search for subspace reduction by also looking for corelations between hyperparameters per the previous section.</p>
<p>for example we may find that for - a big learning rate we learn faster - a big batch size we learn faster - a big number of layers we learn slower</p>
<ol type="1">
<li>we can use early stopping to save costs once the fast models overfit.</li>
<li>we can use the best weights we got to starting future epocs for future hyperparameters settings.</li>
</ol>
</section>
<section id="adaptive-learning-schedules" class="level2">
<h2 class="anchored" data-anchor-id="adaptive-learning-schedules">adaptive learning schedules</h2>
<p>we could probably do better if we start with a fast learner then as it converges switch to a better but slower learner.</p>
<p>this may allow us to train the best model faster.</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Hyperparameter {Optimization}},
  date = {2024-06-13},
  url = {https://orenbochman.github.io//posts/2024/2024-06-13-hyper/2024-06-13-hyper.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Hyperparameter Optimization.”</span> June
13, 2024. <a href="https://orenbochman.github.io//posts/2024/2024-06-13-hyper/2024-06-13-hyper.html">https://orenbochman.github.io//posts/2024/2024-06-13-hyper/2024-06-13-hyper.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-06-13-hyper/2024-06-13-hyper.html</guid>
  <pubDate>Wed, 12 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Generally Capable Agents Emerge from Open-Ended Play</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-06-10-review-generally-capable-agents-emerge-from-open-ended-play/</link>
  <description><![CDATA[ 





<section id="first-impressions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="first-impressions">First impressions</h2>
<p>The paper does not present a breakthrough like alpha go zero etc. But it shows very high level of creativity and innovation. I am still a new comer to RL and this paper has opened my eyes to how little of the field I have seen. There are lots of buzz words, references to other papers and concepts that I am not familiar with. Also this paper is visually stunning. The authors have put a lot of energy into creating an aesthetically pleasing project and they have gone to some length to explain what would otherwise might be a very challenging evaluation process.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/1yVoNAMLIy0" title="Max Jaderberg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ol type="1">
<li>to what extent can agents learn to solve certain types of problems like solving a maze. I might call these learning tactical solutions</li>
<li>to what extent can agents compress this tactical knowledge into a heuristic that might end up as much more general.</li>
<li>to what extent can RL agents learn representations of the environment that allow them to reuse tactics and heuristics across different problems instead of having to discover them anew each time.</li>
<li>when sparse rewards or no rewards are given can agents learn use their capabilities to model the environment</li>
<li>generally capable agents should be able to handle the many different RL problem settings that are out there:</li>
</ol>
<ul>
<li>single state, multi-state, continuous state,</li>
<li>tabular, continuous,</li>
<li>finite state space, infinite state space,</li>
<li>episodic, continuing, i.e.&nbsp;finite horizon, infinite horizon,</li>
<li>single agent, multi-agent,</li>
<li>online, offline,</li>
<li>model based, model free,</li>
<li>known dynamics, unknown dynamics,</li>
<li>sparse rewards, dense rewards,</li>
<li>on-policy, off-policy,</li>
<li>discounted, undiscounted rewards,</li>
<li>single goal, multi-goal,</li>
<li>deterministic, stochastic,</li>
<li>stationary, non-stationary,</li>
<li>specific constraints, in reality there are also variations of these settings not all of these are dichotomies.</li>
</ul>
<ol start="6" type="1">
<li>The paper mentions priors work on social dilemmas - another dimension that seems to be related is how well can agents learn to solve simple game theoretic scenarios like the prisoner’s dilemma or colonel Blotto and then to transfer the knowledge to more complex games. The same idea might be applied to problems based in economic models.</li>
</ol>
<p>here are some of the concepts that I am not familiar with:</p>
<ul>
<li><a href="https://deepmind.google/discover/blog/population-based-training-of-neural-networks/">Population based training</a> a technique used to optimize a series of NN at the same time.
<ul>
<li>can this be useful in RL where an agent might need to learn multiple NNs to solve a problem.
<ul>
<li>the transition model P(s’ | s, a) model</li>
<li>the reward model R(r | s, a) model</li>
<li>for representing the model (this is the four part dynamic function for <img src="https://latex.codecogs.com/png.latex?f(s',r%7Cs,a)"></li>
<li>value functions
<ul>
<li>the value function, <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi_%7B%5Cstar%7D%20%7D%20(s)"></li>
<li>the action value function <img src="https://latex.codecogs.com/png.latex?q_%7B%5Cpi_%7B%5Cstar%7D%20%7D(s,a)"></li>
</ul></li>
<li>the advantage function <img src="https://latex.codecogs.com/png.latex?A_%7B%5Cpi_%7B%5Cstar%7D%20%7D(s,a)=%20Q_%7B%5Cpi_%7B%5Cstar%7D%20%7D(s,a)%20-%20V_%7B%5Cpi_%7B%5Cstar%7D%20%7D(s)"></li>
<li>the the policy <img src="https://latex.codecogs.com/png.latex?pi_%5Cstar%20(s)"></li>
</ul></li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Abr</th>
<th>Q-Fn<br>
<img src="https://latex.codecogs.com/png.latex?Q(s,a)"></th>
<th>V-Fn<br>
<img src="https://latex.codecogs.com/png.latex?V(s)"></th>
<th>Policy<br>
<img src="https://latex.codecogs.com/png.latex?%CF%80(a%E2%88%A3s)"></th>
<th>Advantage<br>
<img src="https://latex.codecogs.com/png.latex?A(s,a)"></th>
<th>Transitions<br>
<img src="https://latex.codecogs.com/png.latex?P(s%E2%80%B2%E2%88%A3s,a)"></th>
<th>Reward <img src="https://latex.codecogs.com/png.latex?R(s,a)"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deep Q-Network<br>
<span class="citation" data-cites="Mnih2015HumanlevelCT">(Mnih et al. 2015)</span></td>
<td>(DQN)</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td><p>Double DQN</p>
<p><span class="citation" data-cites="Hasselt2015DeepRL">(Hasselt, Guez, and Silver 2015)</span></p></td>
<td>(DDQN)</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Dueling DQN<br>
<span class="citation" data-cites="Wang2015DuelingNA">(Wang et al. 2015)</span></td>
<td></td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td>Deep Deterministic Policy Gradients<br>
<span class="citation" data-cites="Lillicrap2015ContinuousCW">(Lillicrap et al. 2015)</span></td>
<td>(DDPG)</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td><p>Twin Delayed DDPG</p>
<p><span class="citation" data-cites="Fujimoto2018AddressingFA">(Fujimoto, Hoof, and Meger 2018)</span></p></td>
<td>(TD3)</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td><p>Soft Actor-Critic</p>
<p><span class="citation" data-cites="Haarnoja2018SoftAO">(Haarnoja et al. 2018)</span></p></td>
<td>(SAC)</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td><p>Proximal Policy Optimization</p>
<p><span class="citation" data-cites="Schulman2017ProximalPO">(Schulman et al. 2017)</span></p></td>
<td>(PPO)</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td><p>Trust Region Policy Optimization</p>
<p><span class="citation" data-cites="Schulman2015TrustRP">(Schulman et al. 2015)</span></p></td>
<td>(TRPO)</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Advantage Actor-Critic<br>
<span class="citation" data-cites="Mnih2016AsynchronousMF">(Mnih et al. 2016)</span></td>
<td>(A2C/A3C)</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td>Model-Based DQN<br>
<span class="citation" data-cites="Feinberg2018ModelBasedVE">(Feinberg et al. 2018)</span></td>
<td>(M-DQN)</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Model-Based PPO<br>
<span class="citation" data-cites="Clavera2018ModelBasedRL">(Clavera et al. 2018)</span></td>
<td>(M-PPO)</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><p>AlphaGo</p>
<p><span class="citation" data-cites="Silver2016MasteringTG">(Silver et al. 2016)</span></p></td>
<td></td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td><p>AlphaGo Zero</p>
<p><span class="citation" data-cites="Silver2017MasteringTG">(Silver et al. 2017)</span></p></td>
<td></td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><p>AlphaZero</p>
<p><span class="citation" data-cites="Silver2018AGR">(Silver et al. 2018)</span></p></td>
<td></td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
</tbody>
</table>
<div class="no-row-height column-margin column-container"><div id="ref-Mnih2015HumanlevelCT" class="csl-entry">
Mnih, Volodymyr, K. Kavukcuoglu, David Silver, Andrei A. Rusu, J. Veness, Marc G. Bellemare, Alex Graves, et al. 2015. <span>“Human-Level Control Through Deep Reinforcement Learning.”</span> <em>Nature</em> 518: 529–33.
</div><div id="ref-Hasselt2015DeepRL" class="csl-entry">
Hasselt, H. V., A. Guez, and David Silver. 2015. <span>“Deep Reinforcement Learning with Double q-Learning,”</span> 2094–2100.
</div><div id="ref-Wang2015DuelingNA" class="csl-entry">
Wang, Ziyun, T. Schaul, Matteo Hessel, H. V. Hasselt, Marc Lanctot, and Nando de Freitas. 2015. <span>“Dueling Network Architectures for Deep Reinforcement Learning,”</span> 1995–2003.
</div><div id="ref-Lillicrap2015ContinuousCW" class="csl-entry">
Lillicrap, T., Jonathan J. Hunt, A. Pritzel, N. Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. <span>“Continuous Control with Deep Reinforcement Learning.”</span> <em>CoRR</em> abs/1509.02971.
</div><div id="ref-Fujimoto2018AddressingFA" class="csl-entry">
Fujimoto, Scott, H. V. Hoof, and D. Meger. 2018. <span>“Addressing Function Approximation Error in Actor-Critic Methods,”</span> 1582–91.
</div><div id="ref-Haarnoja2018SoftAO" class="csl-entry">
Haarnoja, Tuomas, Aurick Zhou, P. Abbeel, and S. Levine. 2018. <span>“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.”</span> <em>ArXiv</em> abs/1801.01290.
</div><div id="ref-Schulman2017ProximalPO" class="csl-entry">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal Policy Optimization Algorithms.”</span> <em>ArXiv</em> abs/1707.06347.
</div><div id="ref-Schulman2015TrustRP" class="csl-entry">
Schulman, John, S. Levine, P. Abbeel, Michael I. Jordan, and Philipp Moritz. 2015. <span>“Trust Region Policy Optimization.”</span> <em>ArXiv</em> abs/1502.05477.
</div><div id="ref-Mnih2016AsynchronousMF" class="csl-entry">
Mnih, Volodymyr, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, T. Lillicrap, Tim Harley, David Silver, and K. Kavukcuoglu. 2016. <span>“Asynchronous Methods for Deep Reinforcement Learning,”</span> 1928–37.
</div><div id="ref-Feinberg2018ModelBasedVE" class="csl-entry">
Feinberg, Vladimir, Alvin Wan, I. Stoica, Michael I. Jordan, Joseph E. Gonzalez, and S. Levine. 2018. <span>“Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning.”</span> <em>ArXiv</em> abs/1803.00101.
</div><div id="ref-Clavera2018ModelBasedRL" class="csl-entry">
Clavera, I., Jonas Rothfuss, John Schulman, Yasuhiro Fujita, T. Asfour, and P. Abbeel. 2018. <span>“Model-Based Reinforcement Learning via Meta-Policy Optimization,”</span> 617–29.
</div><div id="ref-Silver2016MasteringTG" class="csl-entry">
Silver, David, Aja Huang, Chris J. Maddison, A. Guez, L. Sifre, George van den Driessche, Julian Schrittwieser, et al. 2016. <span>“Mastering the Game of Go with Deep Neural Networks and Tree Search.”</span> <em>Nature</em> 529: 484–89.
</div><div id="ref-Silver2017MasteringTG" class="csl-entry">
Silver, David, Julian Schrittwieser, K. Simonyan, Ioannis Antonoglou, Aja Huang, A. Guez, T. Hubert, et al. 2017. <span>“Mastering the Game of Go Without Human Knowledge.”</span> <em>Nature</em> 550: 354–59.
</div><div id="ref-Silver2018AGR" class="csl-entry">
Silver, David, T. Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, A. Guez, Marc Lanctot, et al. 2018. <span>“A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play.”</span> <em>Science</em> 362: 1140–44.
</div></div></section>
<section id="summary" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>The paper <span class="citation" data-cites="Team2021OpenEndedLL">(Team et al. 2021)</span> explores the idea that generally capable agents can emerge from open-ended play, similar to how human children learn and develop through play. The goal is to create agents that exhibit broad competencies and adaptability without being explicitly trained for specific tasks. In other words in the typical RL settings agents are trained to perform specific tasks and can learn solution to general problems. However they are very poor at generalizing these solutions to slightly different versions of the same problem. The authors seek to develop agents that can not only learn to solve a wide range of tasks but can also generalize and transfer their solutions to new problems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Team2021OpenEndedLL" class="csl-entry">
Team, Open-Ended Learning, Adam Stooke, Anuj Mahajan, C. Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, et al. 2021. <span>“Open-Ended Learning Leads to Generally Capable Agents.”</span> <em>ArXiv</em> abs/2107.12808.
</div><div id="ref-impala2018" class="csl-entry">
Espeholt, Lasse, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, et al. 2018. <span>“IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.”</span> In <em>Proceedings of the International Conference on Machine Learning (ICML)</em>.
</div><div id="ref-Hessel2018MultitaskDR" class="csl-entry">
Hessel, Matteo, Hubert Soyer, L. Espeholt, Wojciech M. Czarnecki, Simon Schmitt, and H. V. Hasselt. 2018. <span>“Multi-Task Deep Reinforcement Learning with PopArt.”</span> <em>ArXiv</em> abs/1809.04474.
</div></div><p>This is not the first time that this idea has been explored. Prior work includes the development of agents that can learn to play a variety of video games without explicit training on each game. Other work like <span class="citation" data-cites="impala2018">(Espeholt et al. 2018)</span> and <span class="citation" data-cites="Hessel2018MultitaskDR">(Hessel et al. 2018)</span> at deep mind have already shown how this can be done. However, the authors of this paper take this idea further by developing agents that can learn to solve a wider range of tasks.</p>
<p>In this paper they develop environments that co-evolve with the agents. The environment increase in difficulty as the agents learn to solve them. The agents are equipped with intrinsic motivation mechanisms, such as curiosity and novelty-seeking behaviors, to drive exploration. A variety of tasks and challenges are presented dynamically, promoting continuous learning and adaptation.</p>
<p>But the real question is to what degree does this approach create agents that can learn to solve a wide range of tasks and can generalize their solutions to new problems. There seems to be three parts</p>
</section>
<section id="open-ended-learning" class="level2">
<h2 class="anchored" data-anchor-id="open-ended-learning">Open-Ended Learning:</h2>
<dl>
<dt>Open-Ended Learning</dt>
<dd>
<p>Open-ended learning refers to an unsupervised, exploratory process where agents interact with their environment without predefined goals. This approach contrasts with traditional reinforcement learning, which focuses on optimizing performance for specific tasks.</p>
</dd>
</dl>
<p>Have the the authors really provided environment with not predefined goals or just many many goals. There are games in game theory where the player</p>
<p>is given incomplete information - you don’t get told the reward or the rules and need to figure out an optimal strategy without them.</p>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology:</h2>
<p>The authors design an environment that encourages diverse interactions and challenges. Agents are equipped with intrinsic motivation mechanisms, such as curiosity and novelty-seeking behaviors, to drive exploration. A variety of tasks and challenges are presented dynamically, promoting continuous learning and adaptation.</p>
<p>As the authors point out - making a maze larger don’t necessarily make it more difficult once the agents have learned to solved a few mazes. The authors have to carefully design the environment to ensure that the agents are always challenged but not overwhelmed.</p>
<p>I have three criticism of the methodology:</p>
<ol type="1">
<li>Although this paper is about RL there is more than a fair share of evolutionary algorithms. It isn’t clear to what extent the agents are learning through RL and to what extent they are evolving through evolutionary algorithms. I don’t dislike this idea but it seems to muddy the waters regarding how well this research might be applied to create generally capable RL agents in the real world.</li>
<li>Some of the environment used in testing are hand crafted while the bulk of the environments are procedurally generated.</li>
<li>The claim about these these hand crafted test environments being unlike other environments that are procedurally generated is not very convincing.</li>
</ol>
<p>How is skill acquisition tracked?</p>
<p>Intrinsic Motivations are based on curiosity and novelty seeking behaviors. However I think that for some environment/problems intrinsic (motivation) could emerge from a dynamic of the environment itself. In some way this intrinsic motivation reflects the agents ability to model the environment and to predict the consequences of its actions.</p>
<p>For example in an agent can reproduce under some selection pressure it should acquire a relevant fitness intrinsic (expected progeny). If it needs to solve different mazes it should need an exploration intrinsic. If it needs to maximize harvesting of resources it should learn some utility function intrinsic. For a social dilemma it might learn some social utility function intrinsic. However in this case this is an intrinsic that need to be learned by all agents Even if all the agents learn it there is are possibility that the agents will not cooperate. We might look to game theory, mechanism design to see if agents can learn self encouraging mechanisms to cooperate and so on. Can they learn to signal or coordinate behavior to activate the social utility function intrinsic. Can they plan to change roles in sequential games with memory and without.</p>
<p>a environment itself. For example in the case of the maze the agent might be intrinsically motivated to explore the maze because it is the only way to find the reward. In this case the environment itself is providing the intrinsic motivation.</p>
<p>A more interesting approach would be to track the agents ability to solve a wide range of tasks and to generalize their solutions to new problems. This would be a more direct measure of the agents general capabilities.</p>
<p>The paper and website show how the internal state of the agent is visualized over the course of play. This seems to be a hearmap with different possible goals.</p>
</section>
<section id="results-and-findings" class="level2">
<h2 class="anchored" data-anchor-id="results-and-findings">Results and Findings:</h2>
<p>Agents developed through open-ended play demonstrate a wide range of capabilities, such as problem-solving, tool use, and social interaction. These agents outperform those trained with traditional task-specific reinforcement learning in terms of adaptability and generalization. Emergent behaviors and skills are observed, highlighting the potential of open-ended play in fostering general intelligence.</p>
</section>
<section id="implications-for-ai-development" class="level2">
<h2 class="anchored" data-anchor-id="implications-for-ai-development">Implications for AI Development:</h2>
<p>The findings suggest that fostering environments that encourage open-ended play can lead to the development of more robust and versatile AI agents. This approach could be pivotal in advancing AI towards general intelligence, where agents can perform well across a wide range of tasks without explicit training for each.</p>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions:</h2>
<p>Further research is needed to understand the mechanisms underlying the success of open-ended play. Scaling up the complexity of environments and intrinsic motivation systems could lead to even more capable agents. Exploring the integration of open-ended play with other AI paradigms might enhance the development of general AI.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Generally {Capable} {Agents} {Emerge} from {Open-Ended}
    {Play}},
  date = {2024-06-10},
  url = {https://orenbochman.github.io//posts/2024/2024-06-10-review-generally-capable-agents-emerge-from-open-ended-play},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Generally Capable Agents Emerge from
Open-Ended Play.”</span> June 10, 2024. <a href="https://orenbochman.github.io//posts/2024/2024-06-10-review-generally-capable-agents-emerge-from-open-ended-play">https://orenbochman.github.io//posts/2024/2024-06-10-review-generally-capable-agents-emerge-from-open-ended-play</a>.
</div></div></section></div> ]]></description>
  <category>paper review</category>
  <category>multi-agent reinforcement learning</category>
  <category>sequential social dilemmas</category>
  <category>sequential social dilemmas</category>
  <category>cooperation</category>
  <category>Markov games</category>
  <category>agent-based social simulation</category>
  <category>non-cooperative games</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-06-10-review-generally-capable-agents-emerge-from-open-ended-play/</guid>
  <pubDate>Sun, 09 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Multi-agent Reinforcement Learning in Sequential Social Dilemmas</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-06-10-review-MARL-in-sequential-social-dilemmas/</link>
  <description><![CDATA[ 





<blockquote class="blockquote">
<p>Matrix games like Prisoner’s Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Qnetwork, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.” – <span class="citation" data-cites="Leibo2017marl">(<strong>Leibo2017marl?</strong>)</span> abstract</p>
</blockquote>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Initially at least it seems to me that the “sequential social dilemma” is just an analog of an iterated prisoner’s dilemma game. This game considered the basis of all other social dilemmas has been treated at great length in <span class="citation" data-cites="Axelrod1980">(<strong>Axelrod1980?</strong>)</span> and <span class="citation" data-cites="Axelrod1997">(<strong>Axelrod1997?</strong>)</span> with the famous Axelrod tournaments. Some interesting results include the fact that the best strategy in the tournament was the simplest. Population dynamics are such that in population of agents with one strategy, the introduction of new agents with a different strategy can gain a foothold and cause the dominant strategy in the population to switch to the new strategy. This is known as the “evolution of cooperation”.</p>
<p>This work led to a growing interest in the study of cooperation in non-cooperative games with about 300 new papers published on the topic in the 10 years following the publication.</p>
<p>Later work looked at the robustness of the winning strategies in the Axelrod tournaments, see <span class="citation" data-cites="Nowak2004">(<strong>Nowak2004?</strong>)</span>.</p>
</div>
</div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In <span class="citation" data-cites="Leibo2017marl">(<strong>Leibo2017marl?</strong>)</span>, the authors introduce a new class of social dilemmas, called sequential social dilemmas, which are inspired by the classic matrix game social dilemmas like the Prisoner’s Dilemma.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Multi-Agent {Reinforcement} {Learning} in {Sequential}
    {Social} {Dilemmas}},
  date = {2024-06-10},
  url = {https://orenbochman.github.io/posts/2024/2024-06-10-review-MARL-in-sequential-social-dilemmas/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Multi-Agent Reinforcement Learning in
Sequential Social Dilemmas.”</span> June 10, 2024. <a href="https://orenbochman.github.io/posts/2024/2024-06-10-review-MARL-in-sequential-social-dilemmas/">https://orenbochman.github.io/posts/2024/2024-06-10-review-MARL-in-sequential-social-dilemmas/</a>.
</div></div></section></div> ]]></description>
  <category>paper review</category>
  <category>multi-agent reinforcement learning</category>
  <category>sequential social dilemmas</category>
  <category>sequential social dilemmas</category>
  <category>cooperation</category>
  <category>Markov games</category>
  <category>agent-based social simulation</category>
  <category>non-cooperative games</category>
  <guid>https://orenbochman.github.io/posts/2024/2024-06-10-review-MARL-in-sequential-social-dilemmas/</guid>
  <pubDate>Sun, 09 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Semantic Kernel</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2004-06-03-sk-repl/semantic-kernel/2024-04-03-sk-repl.html</link>
  <description><![CDATA[ 





<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> semantic_kernel <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sk</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> semantic_kernel.connectors.ai.open_ai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AzureChatCompletion, OpenAIChatCompletion</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> semantic_kernel.ai.open_ai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sk_oai</span>
<span id="cb1-4"></span>
<span id="cb1-5"></span>
<span id="cb1-6">kernel <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sk.Kernel()</span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#useAzureOpenAI = False</span></span>
<span id="cb1-9"></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># if useAzureOpenAI:</span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()</span></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#     kernel.add_text_completion_service("azureopenai", AzureChatCompletion(deployment, endpoint, api_key))</span></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># else:</span></span>
<span id="cb1-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#api_key, org_id = sk.openai_settings_from_dot_env()</span></span>
<span id="cb1-15">api_key , org_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sk_oai.openai_settings_from_dot_env()</span>
<span id="cb1-16">kernel.add_text_completion_service(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"openai"</span>, OpenAIChatCompletion(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-3.5-turbo-0301"</span>, api_key, org_id))</span>
<span id="cb1-17"></span>
<span id="cb1-18"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"You made a kernel!"</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> semantic_kernel <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sk</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> semantic_kernel.connectors.ai.hugging_face <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> HuggingFaceTextCompletion</span>
<span id="cb2-3"></span>
<span id="cb2-4">kernel <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sk.Kernel()</span>
<span id="cb2-5"></span>
<span id="cb2-6">kernel.add_text_completion_service(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"huggingface"</span>, HuggingFaceTextCompletion(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt2"</span>, task<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text-generation"</span>))</span>
<span id="cb2-7"></span>
<span id="cb2-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"You made an open source kernel using an open source AI model!"</span>)</span></code></pre></div>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Semantic {Kernel}},
  date = {2024-06-08},
  url = {https://orenbochman.github.io//posts/2024/2004-06-03-sk-repl/semantic-kernel/2024-04-03-sk-repl.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Semantic Kernel.”</span> June 8, 2024. <a href="https://orenbochman.github.io//posts/2024/2004-06-03-sk-repl/semantic-kernel/2024-04-03-sk-repl.html">https://orenbochman.github.io//posts/2024/2004-06-03-sk-repl/semantic-kernel/2024-04-03-sk-repl.html</a>.
</div></div></section></div> ]]></description>
  <category>ai</category>
  <category>nlp</category>
  <guid>https://orenbochman.github.io/posts/2024/2004-06-03-sk-repl/semantic-kernel/2024-04-03-sk-repl.html</guid>
  <pubDate>Fri, 07 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>emergent communications</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2024/2024-05-01-signals/shanon-game.html</link>
  <description><![CDATA[ 





<p>it seems that we might want to look at the emergent communications by considering 1. a Lewis signaling games to model coordination tasks for a basic communication system 2. a Shannon game to model the communication of information between agents in which the learn a shared communication protocol potentially using error detection and correction and corection. 3. a Chomsky game to model development of a shared grammar for complex signals.</p>
<section id="shannon-game" class="level2">
<h2 class="anchored" data-anchor-id="shannon-game">Shannon Game</h2>
<p>Shanon games are about emergence of randomized communication protocols. A randomised communication protocol is a probability distribution over the set of possible deterministic communication protocols.</p>
<p>We can model any deterministic communication protocol as a pair of decision rees, one for the sender and one for the receiver. The sender’s decision tree maps each possible message to a signal, and the receiver’s decision tree maps each possible signal to a message.</p>
<p>messages that the sender can send. The sender samples a message from this distribution and sends it to the receiver. The receiver then uses a decoding function to map the received message back to the original signal. The goal of the game is for the sender and receiver to coordinate on a communication protocol that maximizes their payoff, which is typically based on the accuracy of message transmission and reception. It is a protocol that uses randomness to encode and decode messages. This randomness can be used to introduce redundancy in the message, which can help in error detection and correction.</p>
<div id="f4dc8e19" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> CommunicationAgent:</span>
<span id="cb1-4">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, num_strategies):</span>
<span id="cb1-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_strategies <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> num_strategies</span>
<span id="cb1-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((num_strategies, num_strategies))</span>
<span id="cb1-7">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb1-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.discount_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span></span>
<span id="cb1-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.epsilon <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb1-10">    </span>
<span id="cb1-11">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> choose_strategy(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb1-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> np.random.rand() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.epsilon:</span>
<span id="cb1-13">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.random.randint(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.num_strategies)</span>
<span id="cb1-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-15">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.argmax(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb1-16">    </span>
<span id="cb1-17">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> update_q_values(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, sender_strategy, receiver_strategy, reward):</span>
<span id="cb1-18">        max_future_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table[receiver_strategy])</span>
<span id="cb1-19">        current_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table[sender_strategy, receiver_strategy]</span>
<span id="cb1-20">        new_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.discount_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> max_future_q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_q)</span>
<span id="cb1-21">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.q_table[sender_strategy, receiver_strategy] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> new_q</span>
<span id="cb1-22"></span>
<span id="cb1-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simulation parameters</span></span>
<span id="cb1-24">num_strategies <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb1-25">num_iterations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span></span>
<span id="cb1-26"></span>
<span id="cb1-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize agents</span></span>
<span id="cb1-28">alice <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CommunicationAgent(num_strategies)</span>
<span id="cb1-29">bob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CommunicationAgent(num_strategies)</span>
<span id="cb1-30"></span>
<span id="cb1-31"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_iterations):</span>
<span id="cb1-32">    sender_strategy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> alice.choose_strategy()</span>
<span id="cb1-33">    receiver_strategy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bob.choose_strategy()</span>
<span id="cb1-34">    </span>
<span id="cb1-35">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simulate message transmission and reception with noise</span></span>
<span id="cb1-36">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This is a placeholder for actual encoding/decoding logic</span></span>
<span id="cb1-37">    success <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.rand() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Assume 80% chance of success</span></span>
<span id="cb1-38">    </span>
<span id="cb1-39">    reward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> success <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-40">    alice.update_q_values(sender_strategy, receiver_strategy, reward)</span>
<span id="cb1-41">    bob.update_q_values(receiver_strategy, sender_strategy, reward)</span>
<span id="cb1-42"></span>
<span id="cb1-43"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Alice's Q-Table:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, alice.q_table)</span>
<span id="cb1-44"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bob's Q-Table:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, bob.q_table)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Alice's Q-Table:
 [[ 1.1870612   0.          0.          0.1         0.        ]
 [ 1.56945522  1.54181517  0.97338873  0.82764688  1.00508109]
 [ 0.63863811 -0.02636762  0.          0.19736455  0.1607126 ]
 [ 0.56019999  0.          0.          0.          0.        ]
 [ 0.80594973  0.25923685  0.14809569  0.          0.        ]]
Bob's Q-Table:
 [[ 1.50847687  1.20172521  0.6687741   0.64135768  0.55690049]
 [ 0.          0.7344698  -0.0829      0.          0.15625959]
 [ 0.          0.94187947  0.          0.          0.15028825]
 [ 0.15690016  0.91314851  0.22940322  0.          0.        ]
 [ 0.          0.89534698  0.14117867  0.          0.        ]]</code></pre>
</div>
</div>
<p>This example illustrates a basic game-theoretic approach where the sender and receiver iteratively learn better strategies for encoding and decoding messages over a noisy channel. The reinforcement learning framework allows both parties to adapt and improve their protocols, enhancing the reliability of communication over time. This model can be extended and refined to include more sophisticated encoding/decoding techniques and more complex noise models.</p>
<div id="538b0ef5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mesa <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Agent, Model</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mesa.time <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RandomActivation</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mesa.datacollection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DataCollector</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb3-5"></span>
<span id="cb3-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> hamming_distance(a, b):</span>
<span id="cb3-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> b) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(a)</span>
<span id="cb3-8"></span>
<span id="cb3-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Sender(Agent):</span>
<span id="cb3-10">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, unique_id, model):</span>
<span id="cb3-11">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(unique_id, model)</span>
<span id="cb3-12">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.protocol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.random_protocol()</span>
<span id="cb3-13">    </span>
<span id="cb3-14">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> random_protocol(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-15">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define a random protocol for encoding</span></span>
<span id="cb3-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> msg: msg  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Identity for simplicity</span></span>
<span id="cb3-17">    </span>
<span id="cb3-18">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-19">        message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randint(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.message_length)</span>
<span id="cb3-20">        encoded_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.protocol(message)</span>
<span id="cb3-21">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.sent_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encoded_message</span>
<span id="cb3-22"></span>
<span id="cb3-23"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Receiver(Agent):</span>
<span id="cb3-24">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, unique_id, model):</span>
<span id="cb3-25">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(unique_id, model)</span>
<span id="cb3-26">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.protocol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.random_protocol()</span>
<span id="cb3-27">    </span>
<span id="cb3-28">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> random_protocol(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-29">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define a random protocol for decoding</span></span>
<span id="cb3-30">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> msg: msg  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Identity for simplicity</span></span>
<span id="cb3-31">    </span>
<span id="cb3-32">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-33">        noisy_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.sent_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span> np.random.binomial(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.error_rate, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.message_length)</span>
<span id="cb3-34">        recovered_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.protocol(noisy_message)</span>
<span id="cb3-35">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.recovered_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> recovered_message</span>
<span id="cb3-36">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.evaluate_performance()</span>
<span id="cb3-37">    </span>
<span id="cb3-38">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> evaluate_performance(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-39">        original_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.original_message</span>
<span id="cb3-40">        recovered_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.recovered_message</span>
<span id="cb3-41">        distance <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> hamming_distance(original_message, recovered_message)</span>
<span id="cb3-42">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.payoff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.recovery_payoff(distance)</span>
<span id="cb3-43">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.payoff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.length_payoff(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(recovered_message))</span>
<span id="cb3-44">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.payoff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.early_recovery_payoff(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model.current_step)</span>
<span id="cb3-45">    </span>
<span id="cb3-46"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> NoisyChannelModel(Model):</span>
<span id="cb3-47">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, message_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, error_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, max_steps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>):</span>
<span id="cb3-48">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb3-49">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.message_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> message_length</span>
<span id="cb3-50">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.error_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> error_rate</span>
<span id="cb3-51">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb3-52">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> max_steps</span>
<span id="cb3-53">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.payoff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb3-54">        </span>
<span id="cb3-55">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomActivation(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb3-56">        </span>
<span id="cb3-57">        sender <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Sender(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb3-58">        receiver <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Receiver(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb3-59">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule.add(sender)</span>
<span id="cb3-60">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule.add(receiver)</span>
<span id="cb3-61">        </span>
<span id="cb3-62">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.original_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randint(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.message_length)</span>
<span id="cb3-63">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.sent_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-64">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.recovered_message <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb3-65">        </span>
<span id="cb3-66">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.datacollector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataCollector(</span>
<span id="cb3-67">            model_reporters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Payoff"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"payoff"</span>}</span>
<span id="cb3-68">        )</span>
<span id="cb3-69">    </span>
<span id="cb3-70">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> recovery_payoff(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, distance):</span>
<span id="cb3-71">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> distance</span>
<span id="cb3-72">    </span>
<span id="cb3-73">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> length_payoff(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, length):</span>
<span id="cb3-74">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> length</span>
<span id="cb3-75">    </span>
<span id="cb3-76">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> early_recovery_payoff(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, step):</span>
<span id="cb3-77">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> step) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_steps</span>
<span id="cb3-78">    </span>
<span id="cb3-79">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> step(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb3-80">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-81">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.schedule.step()</span>
<span id="cb3-82">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.datacollector.collect(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>)</span>
<span id="cb3-83">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.current_step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_steps:</span>
<span id="cb3-84">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.running <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb3-85"></span>
<span id="cb3-86"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example of running the model</span></span>
<span id="cb3-87">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> NoisyChannelModel()</span>
<span id="cb3-88"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> model.running:</span>
<span id="cb3-89">    model.step()</span>
<span id="cb3-90"></span>
<span id="cb3-91"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Retrieve results</span></span>
<span id="cb3-92">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.datacollector.get_model_vars_dataframe()</span>
<span id="cb3-93"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(results)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    Payoff
0     1.39
1     2.97
2     4.54
3     6.10
4     7.55
..     ...
95  105.44
96  106.07
97  106.89
98  107.40
99  107.90

[100 rows x 1 columns]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/.local/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:

The AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.
We would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919
</code></pre>
</div>
</div>
<p>so this is a variant that uses a noisy channel model to simulate the transmission of messages between a sender and receiver. The agents have protocols for encoding and decoding messages, and the model tracks the performance of the communication system based on the accuracy of message recovery, message length, and early recovery. This example demonstrates how to model and analyze the performance of communication systems in the presence of noise and other challenges.</p>
<p>What we don’t have is a way to pick different protocols or to improve them over time.</p>
<p>I would break this down into a few steps: 1. identify the environmental factors that would encourage the agents to evolve diverse and efficient transmission protocols. a. noisy channels b. limited bandwidth c.&nbsp;limited computational resources d.&nbsp;time constraints e. risks of predation.</p>
<ol start="2" type="1">
<li>allow agents randomly generate candidate protocols and evaluate their performance.</li>
</ol>
<p>def random_protocol(): # Define a random protocol for encoding/decoding return lambda msg: np.random.randint(0, 2, len(msg))</p>
</section>
<section id="which-would-be-used-as-follows" class="level1">
<h1>which would be used as follows</h1>
<p>class Sender(Agent): def <strong>init</strong>(self, unique_id, model): super().__init__(unique_id, model) self.protocol = random_protocol()</p>
<pre><code>def step(self):
    message = np.random.randint(0, 2, self.model.message_length)
    encoded_message = self.protocol(message)
    self.model.sent_message = encoded_message</code></pre>
<p>This could be done by introducing reinforcement learning techniques to allow the agents to adapt and learn better encoding/decoding strategies based on feedback from the environment. This would enable the agents to optimize their protocols for improved communication performance in noisy channels.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Emergent Communications},
  date = {2024-06-01},
  url = {https://orenbochman.github.io//posts/2024/2024-05-01-signals/shanon-game.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Emergent Communications.”</span> June 1,
2024. <a href="https://orenbochman.github.io//posts/2024/2024-05-01-signals/shanon-game.html">https://orenbochman.github.io//posts/2024/2024-05-01-signals/shanon-game.html</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2024/2024-05-01-signals/shanon-game.html</guid>
  <pubDate>Sat, 01 Jun 2024 06:11:36 GMT</pubDate>
</item>
</channel>
</rss>
