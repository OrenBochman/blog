<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Oren Bochman&#39;s Blog</title>
<link>https://orenbochman.github.io/reviews.html</link>
<atom:link href="https://orenbochman.github.io/reviews.xml" rel="self" type="application/rss+xml"/>
<description>Personal website, portfolio and blog</description>
<generator>quarto-1.6.39</generator>
<lastBuildDate>Wed, 12 Mar 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>üó£Ô∏è Talking to Neighbors: Evolution of Regional Meaning in Communication Games</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2005/zolman-talking-to-neighbours/</link>
  <description><![CDATA[ 





<!-- LEDE Try to provide some personal context for reviewing this paper -->
<p>I came across this paper in the bibliography section of signals. Based on the titles of his work <a href="https://www.kevinzollman.com/about-kevin-zollman.html">Kevin J. S. Zollman</a> seems not only a solid researcher but also interested in many of the questions I find fascinating.</p>
<p>I decided to review this one as it investigates how augmenting the Lewis signaling alters its stability.</p>
<p>Stability is not the first concept that comes to mind when thinking about game theory. I am usually interested in understanding what behaviors are possible in a game and how changing incentives might alter these. (Not all games have equilibria.)</p>
<p>Languages are known to change over time. when it comes to Agent based modeling time can pass much faster then in the real world. We can actually see the evolution a dynamic system like a languages in real time. If the language is hard to interpret we may not be happy if it also keeps changing. The stability of equilibria may also be of value to an algorithm designer looking for unique behaviors that exists in unique equilibria. If these are hard to get by chance then we may need better algorithms to find/learn them.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Spatial Structure and Communication in Game Theory
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Emergent Languages In a Nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="Emergent Languages In a Nutshell"></a></p>
<figcaption>Emergent Languages In a Nutshell</figcaption>
</figure>
</div>
<p>The main research questions being explored in this paper are:</p>
<ul>
<li><strong>What is the effect of adding spatial structure to communication games?</strong>
<ul>
<li>looks at adding spatial structure to the <strong>Lewis Signaling</strong> game and a modified <strong>Stag Hunt</strong> game.</li>
</ul></li>
<li><strong>What happens when both communication and spatial arrangement are combined in these models?</strong>
<ul>
<li>The author highlight that these two modifications have largely been studied individually, and their paper aims to study the combined models. This leads to the question of whether combining them yields new insights.</li>
</ul></li>
<li><strong>Does increasing the complexity of the model by adding spatial structure and/or communication add explanatory value for social cooperation?</strong>
<ul>
<li>The author questions whether more complex models behave analogously to simpler ones or if new features emerge that can explain other types of social behavior. More specifically he want to determine if these richer models provide equally good or better explanations for the evolution of social behavior than prior models.</li>
</ul></li>
<li><strong>Does increasing the realism of the model by adding these complexities generally assist in the evolution of cooperative behavior?</strong>
<ul>
<li>The authors explicitly aim to assess the ‚Äúgeneral lesson‚Äù that the probability of cooperation increases as models become more realistic. They also explore whether this holds true when communication and spatial structure are combined, acknowledging the possibility that these modifications might interfere with each other. The paper investigates two types of cooperation in this context: achieving the Pareto optimum payoff and the achievement of meaning.</li>
</ul></li>
<li><strong>How does the addition of spatial structure affect the emergence and nature of meaning in signals?</strong>
<ul>
<li>The paper analyzes the status of meaning of signals in both the Sender-Receiver game and the Stag Hunt game in the spatial context. They specifically examine if signals have meaning (provide more information than prior beliefs) and how this meaning evolves in a spatially structured population, leading to the concept of ‚Äúregional meaning‚Äù. They compare the nature of meaning in the spatial model to that observed in non-spatial models using replicator dynamics.</li>
</ul></li>
</ul>
<p>In essence, the overarching research goal is to understand how making game-theoretic models more realistic by incorporating spatial interaction and communication influences the evolution of cooperation and the emergence of meaning.</p>
</div>
</div>
<p>Here is a light hearted Deep Dive into the paper:</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
<section id="abstract" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>In seeking to explain the evolution of social cooperation, many scholars are using increasingly complex game-theoretic models. These complexities often model readily observable features of human and animal populations. In the case of previous games analyzed in the literature, these modifications have had radical effects on the stability and efficiency properties of the models. We will analyze the effect of adding spatial structure to two communication games: the Lewis Sender-Receiver game and a modified Stag Hunt game. For the Stag Hunt, we find that the results depart strikingly from previous models. In all cases, the departures increase the explanatory value of the models for social phenomenon. ‚Äî <span class="citation" data-cites="Zollman_2005">(Zollman 2005)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-Zollman_2005" class="csl-entry">
Zollman, Kevin J. S. 2005. <span>‚ÄúTalking to Neighbors: The Evolution of Regional Meaning.‚Äù</span> <em>Philosophy of Science</em> 72 (1): 69‚Äì85. <a href="https://doi.org/10.1086/428390">https://doi.org/10.1086/428390</a>.
</div></div></div>
</section>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This paper has a number of <strong>big words</strong> that seem technical and may seem impediments to understanding it. Here are some key definitions to help us navigate the content:</p>
<dl>
<dt>Replicator Dynamics</dt>
<dd>
A mathematical model describing how the frequencies of different strategies in a population change over time based on their relative payoffs.
</dd>
<dt>Nash Equilibrium</dt>
<dd>
A state in a game where no player can improve their payoff by unilaterally changing their strategy, assuming the other players‚Äô strategies remain the same.
</dd>
<dt>Payoff Dominant Equilibrium</dt>
<dd>
A Nash equilibrium where all players receive higher payoffs compared to any other equilibrium.
</dd>
</dl>
<p>Babbling Equilibrium: An equilibrium in a communication game where signals convey no information about the state of the world or the sender‚Äôs intentions.</p>
<dl>
<dt>Evolutionarily Stable Strategy (ESS)</dt>
<dd>
A strategy that, if adopted by a population, cannot be invaded by any alternative strategy under the replicator dynamics.
</dd>
<dt>Spatial Structure</dt>
<dd>
The arrangement of individuals in a population, often modeled as a grid or network, influencing who interacts with whom.
</dd>
<dt>Imitate-the-Best Dynamics</dt>
<dd>
A strategy updating rule in spatial models where individuals adopt the strategy of their most successful neighbor.
</dd>
<dt>Secret Handshake</dt>
<dd>
A signal used by a subpopulation to coordinate their actions and achieve cooperative outcomes by identifying each other.
</dd>
<dt>Polymorphic Equilibrium</dt>
<dd>
A stable state in a population where multiple strategies coexist at specific frequencies.
</dd>
<dt>Regional Meaning</dt>
<dd>
The phenomenon where a signal‚Äôs meaning becomes localized to a specific area within a population due to spatial clustering of signaling systems.
</dd>
<dt>Aumann Stag Hunt</dt>
<dd>
A variation of the Stag Hunt game where Hare Hunters would prefer their partner to hunt Stag, even though they choose to hunt Hare themselves.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<p>Here is an outline summary of the paper ‚ÄúTalking to Neighbors: The Evolution of Regional Meaning‚Äù:</p>
<ol type="1">
<li><strong>Introduction</strong>
<ul>
<li>The paper investigates the evolution of social cooperation using game-theoretic models by adding complexities that model observable features of human and animal populations.</li>
<li>Traditional equilibrium analysis is often inadequate to explain observed social practices, leading to the increasing use of dynamic models like replicator dynamics and myopic best response.</li>
<li>The paper focuses on the effect of adding <strong>spatial structure</strong> and <strong>communication</strong> to two communication games: the Lewis Sender-Receiver game and a modified Stag Hunt game.</li>
<li>Previous literature suggests that increasing the reality of models by adding communication or spatial arrangement often helps cooperation, but this may be an oversimplification as new pitfalls can be introduced.</li>
<li>Studying the combined effects of communication and spatial arrangement can reveal if increased model complexity adds explanatory value and test the general lesson that realism enhances cooperation.</li>
<li>The paper examines two types of cooperation: achieving mutually beneficial outcomes and the emergence of meaning in signals.</li>
</ul></li>
<li><strong>The Sender-Receiver Game</strong>
<ul>
<li>David Lewis proposed that meaning can be explained by communicators using strategies in repeated cooperation games.</li>
<li>A simple Sender-Receiver game with two states, acts, and signals has two high-payoff equilibria and babbling equilibria where signals lack meaning.</li>
<li>Standard equilibrium analysis requires high cognitive capabilities, which may not be realistic for humans or animals.</li>
<li>While factors like natural salience and focal points have been suggested for the emergence of signaling systems, they have limitations in explaining all cases, especially in creatures with low cognitive capacities.</li>
<li>Blume et al.&nbsp;(1998) found that humans can converge to a signaling system even with meaningless signals.</li>
<li>Skyrms (1996) used evolutionary game theory with replicator dynamics to show that signaling systems can evolve from various starting points and are evolutionarily stable.</li>
<li>To address concerns about the assumptions of replicator dynamics, the paper introduces a spatial model with 10,000 players on a torus, each interacting with eight neighbors and updating strategies by imitating more successful neighbors.</li>
<li>Simulations of this spatial model show the emergence of both possible signaling systems, leading to a neutrally stable state unlike the unstable equilibrium in Skyrms‚Äôs model.</li>
<li>A small proportion of starting points in the spatial model can lead to populations of babblers, which can be invaded by signalers.</li>
<li>Similar results of coexisting conventions in spatial populations have been found with different dynamics like myopic best-reply (Berninghaus and Schwalbe, 1996) and in slightly different games (Grim et al., 2001).</li>
<li>In the spatial model, signals acquire <strong>regional meaning</strong>, where the signal provides perfect information about the state of the world if the receiver knows the region of the sender.</li>
<li>Replacing replicator dynamics with a spatial model strengthens the explanation for the evolution of meaning and explains the emergence of different signaling systems in different locations.</li>
</ul></li>
<li><strong>The Stag Hunt</strong>
<ul>
<li>The Stag Hunt game models situations where individuals must risk cooperation for a higher payoff. It has two Nash equilibria: all hunt stag (payoff dominant) and all hunt hare (less risky).</li>
<li>Replicator dynamics analysis suggests that stag hunting takes over only if initially present in more than three-quarters of the population.</li>
<li>The paper examines the Stag Hunt with communication and spatial structure, both individually and combined.</li>
<li><strong>Stag Hunt with Communication:</strong> Aumann (1990) raised concerns about costless preplay communication, arguing signals might be meaningless. However, Skyrms (2002) found that signals can significantly promote cooperative (stag hunting) behavior using replicator dynamics.</li>
<li><strong>Spatial Stag Hunt:</strong> Ellison (1993) found that with best-reply dynamics on a circle, hare hunting usually dominates. Lee and Valentinyi (2000) found a similar result on a two-dimensional lattice with best-reply dynamics and no mutations. Skyrms (2004) found that with imitate-the-best dynamics on a two-dimensional lattice, stag hunting prevails in 99% of cases. On a circle with imitate-the-best, neither equilibrium is contagious without a larger imitation neighborhood.</li>
<li><strong>Spatial Stag Hunt with Communication:</strong> The paper uses the same spatial structure and imitate-the-best dynamics as in the Sender-Receiver game, with strategies represented by a 3-tuple (Signal, Act if signal 1, Act if signal 2).</li>
<li>Simulations show that almost all populations evolve to a state with six of the eight strategies coexisting, and everyone hunting stag. This stability relies on inertia in the model (players only switch if a neighbor does better).</li>
<li>Even with relaxed inertia or variations in payoffs and mutation rates, stag hunting remains prevalent. Aumann Stag Hunts take longer to stabilize but still mostly result in stag hunting.</li>
<li>Populations of Hare Hunters with an unused signal can be invaded by two neighboring Nationalists using a ‚Äúsecret handshake‚Äù. This invasion is possible under certain payoff conditions or with a larger imitation neighborhood.</li>
<li>The polymorphic equilibrium of Individualists found in Skyrms‚Äôs replicator dynamic model can be invaded by Stag Hunters in the spatial model.</li>
<li>Signals in the spatial Stag Hunt with communication contribute beyond spatial structure alone. Invading an all-hare-hunting population requires only two non-simultaneous mutations compared to at least six in the spatial Stag Hunt without communication.</li>
<li>Signals in the combined model acquire more global meaning, indicating a higher probability of a sender being a Stag Hunter or Nationalist, and provide evidence of a player‚Äôs propensity to cooperate with those sending the same signal. They also exhibit regional meaning.</li>
</ul></li>
<li><strong>Conclusion</strong>
<ul>
<li>The combination of signaling and spatial arrangement leads to new results not observed in simpler models. For the Sender-Receiver game, it explains the emergence of different signaling systems within a population. For the Stag Hunt, it results in a radically different population structure.</li>
<li>The general lesson that increasing model complexity helps cooperation is <strong>partially confirmed</strong>.
<ul>
<li>Achieving the Pareto optimum payoff (highest mutual benefit) is generally enhanced in both games with increased complexity.</li>
<li>Population-wide meaning is harmed by spatial structure but a weaker form of <strong>regional meaning</strong> emerges.</li>
</ul></li>
<li>Overall, increasing the reality of the model assists the evolution of cooperative behavior in the Sender-Receiver and Stag Hunt games, suggesting simpler models do not fully capture the dynamics of cooperation and spatial structure.</li>
</ul></li>
</ol>
</section>
<section id="the-review" class="level2">
<h2 class="anchored" data-anchor-id="the-review">The Review</h2>
<p>So adding spatial structure to the Lewis signaling game and the Stag Hunt game has some interesting effects. The paper shows that the spatial structure can lead to the emergence of different signaling systems within a population. This is a significant departure from the traditional equilibrium analysis of these games.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {üó£Ô∏è {Talking} to {Neighbors:} {Evolution} of {Regional}
    {Meaning} in {Communication} {Games}},
  date = {2025-03-13},
  url = {https://orenbochman.github.io/reviews/2005/zolman-talking-to-neighbours/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>‚Äúüó£Ô∏è Talking to Neighbors: Evolution of
Regional Meaning in Communication Games.‚Äù</span> March 13, 2025. <a href="https://orenbochman.github.io/reviews/2005/zolman-talking-to-neighbours/">https://orenbochman.github.io/reviews/2005/zolman-talking-to-neighbours/</a>.
</div></div></section></div> ]]></description>
  <category>draft</category>
  <category>review</category>
  <guid>https://orenbochman.github.io/reviews/2005/zolman-talking-to-neighbours/</guid>
  <pubDate>Wed, 12 Mar 2025 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2005/zolman-talking-to-neighbours/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>On Learning To Become a Successful Loser</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/</link>
  <description><![CDATA[ 





<section id="on-learning-to-become-a-successful-loser-a-comparison-of-alternative-abstractions-of-learning-processes-in-the-loss-domain" class="level1 page-columns page-full">
<h1>On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain</h1>
<p>I tracked this paper due to it being highlighted in <span class="citation" data-cites="Skyrms2010signals">(Skyrms 2010)</span> as the source of a model that learns a signaling systems faster. I got me started with the loss domain. I was eventually able to find how to speed up learning by in the Lewis signaling game by considering the much more likely mistakes. Once I got on this algorithm I was thinking that using this idea for Bayesian updating of beliefs. This eventually led me to a second algorithm that was able to rapidly adjust a belief regarding the the state of the world in lewis signaling game with changing distributions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Skyrms2010signals" class="csl-entry">
Skyrms, Brian. 2010. <span>‚Äú<span class="nocase">Signals: Evolution, Learning, and Information</span>.‚Äù</span> In. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780199580828.003.0013">https://doi.org/10.1093/acprof:oso/9780199580828.003.0013</a>.
</div></div><div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="On Learning to become a successful loser in a nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="On Learning to become a successful loser in a nutshell"></a></p>
<figcaption>On Learning to become a successful loser in a nutshell</figcaption>
</figure>
</div>
<p>Besides having this amazing title this research paper compares five mathematical models that predict student behavior in repeated decision-making tasks involving gains and losses. <mark>The core issue is how to accurately represent the effect of losses on learning, as observed deviations from expected utility theory exist</mark> They conducted an experiment and find that learning in the loss domain can be faster than in the gain domain. <mark>The main results suggest that adding a constant to the payoff matrix can accelerate the learning process</mark>, supporting the adjustable reference point (ARP) abstraction of the effect of losses proposed by <span class="citation" data-cites="Roth1995Learning">(Roth and Erev 1995)</span>.</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<div class="no-row-height column-margin column-container"></div><section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>One of the main difficulties in the development of descriptive models of learning in repeated choice tasks involves the abstraction of the effect of losses. The present paper explains this difficulty, summarizes its common solutions, and presents an experiment that was designed to compare the descriptive power of the specific quantifications of these solutions proposed in recent research. The experiment utilized a probability learning task. In each of the experiment‚Äôs 500 trials participants were asked to predict the appearance of one of two colors. The probabilities of appearance of the colors were different but fixed during the entire experiment. The experimental manipulation involved an addition of a constant to the payoffs. The results demonstrate that learning in the loss domain can be faster than learning in the gain domain; adding a constant to the payoff matrix can affect the learning process. These results are consistent with by <span class="citation" data-cites="Roth1995Learning">(Roth and Erev 1995)</span> adjustable reference point abstraction of the effect of losses, and violate all other models</p>
<p>‚Äî <span class="citation" data-cites="bereby1998learning">(Bereby-Meyer and Erev 1998)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-Roth1995Learning" class="csl-entry">
Roth, Alvin, and Ido Erev. 1995. <span>‚ÄúLearning in Extensive-Form Games: Experimental Data and Simple Dynamic Models in the Intermediate Term.‚Äù</span> <em>Games and Economic Behavior</em> 8 (1): 164‚Äì212. <a href="https://EconPapers.repec.org/RePEc:eee:gamebe:v:8:y:1995:i:1:p:164-212">https://EconPapers.repec.org/RePEc:eee:gamebe:v:8:y:1995:i:1:p:164-212</a>.
</div><div id="ref-bereby1998learning" class="csl-entry">
Bereby-Meyer, Yoella, and Ido Erev. 1998. <span>‚ÄúOn Learning to Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain.‚Äù</span> <em>Journal of Mathematical Psychology</em> 42 (2-3): 266‚Äì86.
</div></div></div>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline:</h2>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>Highlights the difficulty in developing descriptive models of learning in repeated choice tasks that involve potential losses.</li>
<li>Presents the main goal of the paper: to compare the descriptive power of five distinct solutions to this difficulty and to identify a robust approximation of learning in simple decision tasks.</li>
</ul>
</section>
<section id="the-challenge-and-alternative-solutions" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-challenge-and-alternative-solutions">The Challenge and Alternative Solutions</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="table-01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Models and Solutions"><img src="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/table-01.png" class="img-fluid figure-img" alt="Models and Solutions"></a></p>
<figcaption>Models and Solutions</figcaption>
</figure>
</div></div><ul>
<li>Discusses the difficulty in abstracting the effect of losses given the approximately linear relationship between choice probabilities and the ratio of accumulated payoffs observed in the gain domain.</li>
<li>Introduces probability learning tasks used to derive and compare the predictions of different models.</li>
<li>Presents five alternative solutions to the problem:
<ul>
<li>Low Reference Point (LRP):
<ul>
<li>Transforms objective payoffs into non-negative rewards by subtracting the worst possible outcome</li>
</ul></li>
<li>Adjustable Reference Point and Truncation (ARP)
<ul>
<li>Uses an evolving reference point to distinguish gains and losses and truncates negative values to ensure positive propensities</li>
</ul></li>
<li>Exponential Response Rule (EDS, EFP, EWA)
<ul>
<li>Applies an exponential function to propensities, eliminating the need for handling negative values directly. Examples include Exponential Discounted Sum (EDS), Exponential Fictitious Play (EFP), and Experience Weighted Attractions (EWA) models</li>
</ul></li>
<li>Cumulative Normal Response Rule (CNFP)
<ul>
<li>Uses a cumulative normal distribution to model the relationship between payoffs and propensities- Employs the cumulative normal distribution function to map propensities (which can be negative) to choice probabilities (which are always between 0 and 1). The CNFP model exemplifies this.</li>
</ul></li>
<li>Relative Reinforcement solutions (CLO)
<ul>
<li>Uses outcome-specific parameters to determine the impact of different outcomes on choice probabilities. The Cardinal Linear Operator (CLO) model demonstrates this.</li>
</ul></li>
</ul></li>
<li>Describes the specific implementations of these solutions through different models, including their assumptions and parameterizations.</li>
</ul>
</section>
<section id="experiment" class="level3">
<h3 class="anchored" data-anchor-id="experiment">Experiment</h3>
<ul>
<li>Describes the experimental method, including the participants, the procedure, and the payoff structure of the probability learning task across three conditions.</li>
</ul>
</section>
<section id="results" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="results">Results</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="table-01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Models and Solutions"><img src="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/table-01.png" class="img-fluid figure-img" alt="Models and Solutions"></a></p>
<figcaption>Models and Solutions</figcaption>
</figure>
</div></div><ul>
<li>Presents the aggregated experimental results, showing a significant effect of the reward condition on the proportion of optimal choices.</li>
<li>Compares the quantitative predictive and descriptive power of the models using correlation and mean squared deviation (MSD) measures.</li>
<li>Discusses the between-subject variability observed in the data and the limitations of the models in capturing this variability.</li>
<li>Conducts a model-based analysis to evaluate the robustness of the condition effect.</li>
<li>Performs a sensitivity analysis to assess the robustness of the ARP model‚Äôs predictions to changes in parameter values.</li>
</ul>
</section>
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">Discussion</h3>
<ul>
<li>Discusses the main finding that the addition of constants to payoffs affects the speed of learning, highlighting the role of the distinction between gains and losses.</li>
<li>Notes the advantage of the ARP model in capturing the observed results and acknowledges the potential validity of other solutions under specific assumptions or parameterizations.</li>
<li>Addresses the generality of the findings by discussing:
<ul>
<li>Settings where the ARP model‚Äôs predictions are consistent with previous research (probability learning, signal detection).</li>
<li>Settings where the model might fail (learning among only positive outcomes, influence of other players‚Äô payoffs).</li>
</ul></li>
</ul>
</section>
<section id="conclusions" class="level3">
<h3 class="anchored" data-anchor-id="conclusions">Conclusions</h3>
<ul>
<li>Concludes that human learning is affected by the distinction between gains and losses.</li>
<li>Emphasizes that modeling this distinction, particularly through the adjustable reference point approach, improves the descriptive power of adaptive learning models.</li>
<li>Acknowledges the need for further research to refine the quantification of the reference point for a more accurate and generalizable model.</li>
</ul>
</section>
</section>
<section id="key-takeaways" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ul>
<li>In most RL settings rewards are sparse. One way to speed up learning is to try and increase our reward signal.</li>
<li>This is the basis for seeling out to decompose the reward signal into an internal motivation for the agent and an external motivation for the problem designer.</li>
<li>Another approach though is to consider the loss domain. If we can get signals out of losses we can speed up learning and RL agents are slow learners - especially deep RL agents.</li>
<li>A third approach that I was able to make use of is to use both the loss domain and the gain domain to update beliefs about the possible states of the world. This was allowed me to speed up Bayesian learning algorithm for a coordination task.</li>
</ul>
<p>Besides this the paper has a lot of possible options for potential update rules to get this potential speed up.</p>
<ol type="1">
<li>How does adding a constant to all payoffs in a decision task affect learning, and which model best explains this effect?</li>
</ol>
<p>One of the results I learned is that adding a constant to the payoff matrix doesn‚Äôt change it. In fact linear transformations of the payoff matrix don‚Äôt change the outcomes. In policy gradient methods this we call this trick learning with baselines. What we see is that it doesn‚Äôt bias the estimator but can drastically reduce the variance of the estimator. And this variance is the noise that slows down learning by the agent. So adding a constant can surprisingly impact learning speed. The ARP model uniquely predicts this: subtracting a constant to introduce losses speeds learning compared to a purely gain-based scenario. This highlights the psychological impact of the gain-loss framing.</p>
<p>Another insight I had about this is while trying to abstract the RL algorithms. Was that under some conditions we can convert the reward function into a distance metric. Having a metric makes navigation states space much simpler. I really can‚Äôt think of a better feature.</p>
<ol start="2" type="1">
<li>What are the limitations of the ARP model?</li>
</ol>
<ul>
<li>The ARP model, with its current parameters, assumes an initial reference point of zero and a slow adjustment process. This might not hold when:
<ul>
<li>All options are positive: The model would predict slow learning even when clear differences exist.</li>
<li>Social comparison exists: People may adjust their reference point based on other players‚Äô payoffs, a factor not currently incorporated in the model.</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li>How would you define the loss domain and the gain domain ?</li>
</ol>
<ul>
<li><p>The gain domain is when choice probabilities are approximately linearly related to the ratio of accumulated reinforcement.</p></li>
<li><p>The loss domain is when negative payoffs are possible.</p></li>
<li><p>In the gain domain, the probabilities of choosing an alternative match the ratio of accumulated reinforcement, meaning that individuals are more likely to choose options that have yielded higher rewards in the past.</p></li>
<li><p>Descriptive models have to assume that choice probabilities are determined by a function of the accumulated reinforcements, which must have strictly positive values. However, this presents a problem when losses are possible because negative payoffs can result in negative values for the function.</p></li>
</ul>
<ol start="4" type="1">
<li>In the paper the autors mention the value function from prospect theory c.f. <span class="citation" data-cites="kahneman1979econ">(Kahneman 1979)</span>. How does this relate to the ARP model?</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-kahneman1979econ" class="csl-entry">
Kahneman, Daniel. 1979. <span>‚ÄúEcon Ometrica i Ci.‚Äù</span> <em>Econometrica</em> 47 (2): 263‚Äì91.
</div></div><p>The authors state that models that use solutions other than the adjustable reference point can account for the results of the study under the assumption that the model‚Äôs parameters can be affected by the payoffs. One way to account for this is to use reinforcement functions with the characteristics of Prospect Theory‚Äôs value function. Prospect theory, developed by Kahneman and Tversky, suggests that individuals make decisions based on the potential value of losses and gains rather than the final outcome, and that losses have a greater impact on individuals than gains do. This relates to the ARP model because it also assumes that reinforcements are evaluated relative to a reference point, meaning outcomes above the reference point are perceived as gains (reinforcements) and outcomes below the reference point are perceived as losses (punishments).</p>
<ol start="5" type="1">
<li>Is there a formal definition of this prospect theoretic value function?</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="value-function.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="a value function"><img src="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/value-function.png" class="img-fluid figure-img" alt="a value function"></a></p>
<figcaption>a value function</figcaption>
</figure>
</div></div><p>A key element of this theory is the value function, which exhibits these characteristics: - It‚Äôs defined on deviations from a reference point. - It‚Äôs generally concave for gains and convex for losses. - It‚Äôs steeper for losses than for gains, meaning an equivalent loss has a greater psychological impact than the corresponding gain.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {On {Learning} {To} {Become} a {Successful} {Loser}},
  date = {2025-01-02},
  url = {https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>‚ÄúOn Learning To Become a Successful
Loser.‚Äù</span> January 2, 2025. <a href="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/">https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/</a>.
</div></div></section></div> ]]></description>
  <category>RL</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/</guid>
  <pubDate>Wed, 01 Jan 2025 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2018/Lazaridou2018Emergence/</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div id="fig-iclr-2018" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iclr-2018-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<iframe src="https://www.facebook.com/plugins/video.php?height=300&amp;href=https%3A%2F%2Fwww.facebook.com%2Ficlr.cc%2Fvideos%2F2125495797479475%2F&amp;show_text=false&amp;width=500&amp;t=4701" width="500" height="300" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowfullscreen="true" allow="autoplay; clipboard-write; encrypted-media; picture-in-picture; web-share">
</iframe>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iclr-2018-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: ICLR Presentation by Angeliki Lazaridou on Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input
</figcaption>
</figure>
</div><div id="fig-iclr-2018" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iclr-2018-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/GnBdK61cBUY" title="DeepMind" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iclr-2018-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Talk by Hugh Perkins on this paper Titled ‚ÄúDeepMind‚Äôs emergent communication using pixel input‚Äù
</figcaption>
</figure>
</div><div id="fig-towards-multi-agent-emergent-communications" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-towards-multi-agent-emergent-communications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/FgN01DCHfjU" title="Towards Multi-agent Emergent Communication | Angeliki Lazaridou " frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-towards-multi-agent-emergent-communications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Talk titled ‚ÄòTowards Multi-agent Emergent Communication‚Äô by Angeliki Lazaridou at Imperial College London in the ICARL Seminar Series - 2022 Spring
</figcaption>
</figure>
</div></div>

<p>This is the paper that Marco Baroni used to explain the emergence of languages in his talk ‚ÄúIs Composonality over rated?‚Äù.</p>
<p>In <span class="citation" data-cites="lazaridou2018emergence">(Lazaridou et al. 2018)</span> the authors look emergence of language using a deep reinforcement learning approach. They train reinforcement-learning neural network agents on referential communication games. They extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. They find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.</p>
<div class="no-row-height column-margin column-container"></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR: Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Emergence of Linguistic Communication in a nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="Emergence of Linguistic Communication in a nutshell"></a></p>
<figcaption>Emergence of Linguistic Communication in a nutshell</figcaption>
</figure>
</div>
<p>The goal of the paper is <mark>to investigate the properties of communication protocols that emerge when reinforcement learning agents are trained on referential communication games</mark>. The study aims to explore how agents learn to communicate in scenarios with structured and disentangled input data, as well as in more challenging scenarios with raw pixel input, resembling the complexity of real-world environments.</p>
<p>The training of agents just described was successful and the researchers found that <mark>agents can produce structured and compositional communication protocols when presented with disentangled inputs, but <strong>struggle to do so when presented with entangled raw pixel input</strong></mark>. The emergent protocols were found to be <strong>unstable</strong> and highly grounded in the specific game situation, leading to specialized ad-hoc naming conventions.</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by us ing contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured</p>
<p>‚Äî <span class="citation" data-cites="lazaridou2018emergence">(Lazaridou et al. 2018)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-lazaridou2018emergence" class="csl-entry">
Lazaridou, Angeliki, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. 2018. <span>‚ÄúEmergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input.‚Äù</span> <a href="https://arxiv.org/abs/1804.03984">https://arxiv.org/abs/1804.03984</a>.
</div></div></div>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<p>So I went through the paper and outlined most of the methodolgy etc. Its a bit long but I think it is worth it. Here is a quick outline. I might come back and add more material later. But I think the video above though not dierectly on this paper is a good explainer to understand this more easily.</p>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>Explores the emergence of linguistic communication through referential games with symbolic and pixel inputs.</li>
<li>Motivated by understanding the role of environmental conditions on emergent communication.</li>
<li>Introduces the use of deep reinforcement learning agents to scale up traditional studies of language emergence.</li>
</ul>
</section>
<section id="referential-games-framework" class="level3">
<h3 class="anchored" data-anchor-id="referential-games-framework">Referential Games Framework</h3>
<ul>
<li>Based on multi-agent cooperative reinforcement learning, inspired by the Lewis signaling game.</li>
<li>Involves a speaker communicating a target object to a listener, who identifies it among distractors.</li>
<li>Differentiates between symbolic data (structured and disentangled) and pixel data (entangled).</li>
</ul>
</section>
<section id="study-1-referential-game-with-symbolic-data" class="level3">
<h3 class="anchored" data-anchor-id="study-1-referential-game-with-symbolic-data">Study 1: Referential Game with Symbolic Data</h3>
<ul>
<li>Uses disentangled input from the Visual Attributes for Concepts Dataset.</li>
<li>Demonstrates that agents can learn compositional protocols when input is structured.</li>
<li>Explores the effects of message length, showing improved communicative success and reduced ambiguity with longer messages.</li>
<li>Investigates how context-dependent distractors impact language emergence and object confusability.</li>
</ul>
</section>
<section id="study-2-referential-game-with-raw-pixel-data" class="level3">
<h3 class="anchored" data-anchor-id="study-2-referential-game-with-raw-pixel-data">Study 2: Referential Game with Raw Pixel Data</h3>
<ul>
<li>Employs synthetic scenes of geometric objects generated using the MuJoCo engine.</li>
<li>Agents learn to process raw pixel input without pre-training, achieving significant communicative success.</li>
<li>Highlights environmental pressures‚Äô role in shaping emergent protocols, leading to overfitting and ad-hoc conventions.</li>
</ul>
</section>
<section id="structural-properties-of-emergent-protocols" class="level3">
<h3 class="anchored" data-anchor-id="structural-properties-of-emergent-protocols">Structural Properties of Emergent Protocols</h3>
<ul>
<li>Examines the topographic similarity metric, correlating object similarity with message similarity.</li>
<li>Observes compositional signals in structured environments but instability and environmental overfitting with pixel input.</li>
</ul>
</section>
<section id="probe-models" class="level3">
<h3 class="anchored" data-anchor-id="probe-models">Probe Models</h3>
<ul>
<li>Analyzes the speaker‚Äôs visual representations using linear classifiers.</li>
<li>Finds that disentanglement is necessary for encoding object properties and effective communication.</li>
</ul>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<ul>
<li>Demonstrates that structured input aids compositionality, while raw pixel input challenges protocol stability.</li>
<li>Highlights the scalability of emergent communication studies with realistic data and deep learning techniques.</li>
<li>Suggests future work to mitigate overfitting and promote generalization across diverse environments.</li>
</ul>
</section>
</section>
<section id="comments" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="comments">Comments</h2>
<p>What are the main research questions of the paper?</p>
<ol type="1">
<li>How do environmental or pre-linguistic conditions affect the nature of the communication protocol that an agent learns?</li>
<li>Can reinforcement learning agents successfully communicate when presented with raw pixel input, in addition to symbolic and highly structured input data?</li>
<li>How does the degree of structure in input data influence the nature of emergent communication protocols, particularly concerning the hypothesis that structured compositional language emerges when agents perceive the world as structured?</li>
</ol>
<p>Looking over this paper I did not see any outrageous claims not much that I thought wrong. Although Lazaridou has a number of criticism on research in this area this paper seems sound work.</p>
<section id="the-referential-game" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-referential-game">The referential game</h3>

<div class="no-row-height column-margin column-container"><div id="fig-01" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="p3-referntial-game.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="The referential game"><img src="https://orenbochman.github.io/reviews/2018/Lazaridou2018Emergence/p3-referntial-game.png" class="img-fluid figure-img"></a></p>
<figcaption>The referential game</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>First, a speaker is presented with a target object (highlighted as CAR in the symbolic example on the left, and highlighted as the far right im- age in the pixel example on the right). Then, by making use of an alphabet consisting of primitive discrete symbols (‚Äú22‚Äù, ‚Äú10‚Äù, ‚Äú0‚Äù,‚Äú2‚Äù), the speaker constructs a message describing that object (‚Äú22 2 0‚Äù). <strong>We will refer to the set of all distinct messages generated by the speaker as their lexicon or protocol</strong>. Finally, the listener is presented with the target and a set of destructor objects, and‚Äîby making use of the speaker‚Äôs message‚Äîhas to identify the target object from the set of candidate objects. Communicative success is defined as the correct identification of the target by the listening agent</p>
</blockquote>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-01-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4
</figcaption>
</figure>
</div></div><p>Although the referential game isn‚Äôt a novelty and the authors give a number of prior works that use it, I do suspect that using the referential game has some possible pitfalls. Let‚Äôs consider for a second how the referential game differs from the vanilla Lewis Signaling game and if these differences should be significant.</p>
<p>In a vanilla Lewis signaling game the sender encodes the pre-linguistic object into a message and the receiver has to pick one state from all states. In this game a good sender should be able to pick a unique message per state (assuming there are sufficient<sup>1</sup> signals and it does not make use of homonyms).</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;for a simple system one per state is enough. For complex signaling systems this depends on how the atomic signals are aggreaged into complex ones. If the are assembled with replacement into a sequence of length k there are <img src="https://latex.codecogs.com/png.latex?%7CS%7C%5Ek"> complex symbols possible. If additional structure is imposed there may be less possible states. If partial sequences are allowed we may have almost twice as many states.</p></div></div><p>The receiver needs to match the signal with a state. It can pick one from the undecoded states. This is initially a task with en expectation of 1/|S|. Once it solves a messages it should eliminate its states thus increase its expectation of success.</p>
<p>In the referential games I abstract to a two round extensive form game. In the first round the sender and receiver play a classification game. Sender looks at the pre-linguistic object and classifies it. It then encodes it into a sequence of symbols. The encoder has an error rate and should perform poorly as it has no pretraining.</p>
<p>In the referential game we can imagine two rounds. In the first round the agent</p>
<p>Let‚Äôs assume that the sender encodes each input into a unique message or at least unique up to</p>
<p>If there are S states the</p>
<p>In the referential game the receiver need to solve a multiple choice question with one answer and several distractors by decoding the message from the sender.</p>
<p>The researchers call the language that emerges a lexicon or a protocol rather than a language or a signaling system.</p>
<ol type="1">
<li>To call it a lexicon is tantamount to admitting there is no grammar and that the agents are using a simple lewis signaling game. One in which they coordinate a single symbol with each pre-linguistic item or class.</li>
</ol>
<p>A complex signaling system</p>
<p>One term I don‚Äôt know if i like is pre-linguistic concepts, usually we call this as the states. However I think that this term isn‚Äôt bad at all. It suggests that we arn‚Äôt looking just at states but at an item we want to talk about. This makes more sense particularly when we think about bitmaps of states - they are less like states.</p>
<p>One more point is that by adding the vision learning we are adding a second game. Call it a classification game. The agent needs to succeed at classification game otherwise they are just guessing. It worth while to consider though that just guessing with a good memory is enough to develop a signaling system.</p>
<p>There is a massive asymmetry between the sender and the receiver that is not extant in the original game. The sender can learn all images via a ground truth while the receiver can only learn about the correct ones.</p>
<p>So that as a framing game this needs to be reconsidered. What I mean is that the sender‚Äôs vision should be evaluated compared in a scale between an agent with a perfect vision and perfect blindness as baselines. And the same for receiver.</p>
<p>The vision capability should be factored in to the evaluation of the agent‚Äôs learning of the signaling system.</p>
<p>The paper does have many interesting ideas and shows methods, for achieving them. In a number of areas I think one could do better, but I doubt the results should be very different.</p>
<p>One area that seems wort further investigation is CONCEPTUAL ALIGNMENT in appendix A. This seems to be related to semantic grounding ‚Äî getting the agents language concepts/semantics to align with the world or with a second set of semantics like say a human language.</p>
<p>What they consider here is much more specific - does the visual capacity learned by the agents provide them with a disentangled view of the world that is in line with the compositional structure of the state space they are observing (called pre-linguistic concepts).</p>
<p>It seems that either the methodology is inadequate or that there is a problem with alignment.</p>
<p>What might be done -</p>
<ol type="1">
<li>consider a hierarchial model that learns just this types of relationship.</li>
<li>think more on this comparing the vision capabilities of the agent is truly fascinating.</li>
<li>It seems the crux of the matter is if the softmax layers diverge for classifying the atomic concepts and the compositional concepts?</li>
</ol>
<p>If they do we might consider that the agents vision are not seeing things in the same way. But consider that the sender always knows the state the receivers might not know the state most of the time. So thier vision might be less developed</p>
</section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Emergence of Linguistic communication"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Emergence of Linguistic communication</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Emergence of {Linguistic} {Communication} from {Referential}
    {Games} with {Symbolic} and {Pixel} {Input}},
  date = {2025-01-01},
  url = {https://orenbochman.github.io/reviews/2018/Lazaridou2018Emergence/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>‚ÄúEmergence of Linguistic Communication from
Referential Games with Symbolic and Pixel Input.‚Äù</span> January 1,
2025. <a href="https://orenbochman.github.io/reviews/2018/Lazaridou2018Emergence/">https://orenbochman.github.io/reviews/2018/Lazaridou2018Emergence/</a>.
</div></div></section></div> ]]></description>
  <category>review</category>
  <category>compositionality</category>
  <category>neural networks</category>
  <category>signaling systems</category>
  <category>language evolution</category>
  <category>complex signaling system</category>
  <guid>https://orenbochman.github.io/reviews/2018/Lazaridou2018Emergence/</guid>
  <pubDate>Tue, 31 Dec 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2018/Lazaridou2018Emergence/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Linguistic generalization and compositionality in modern artificial neural networks</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2019/baroni-linguistic-generalization/</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Linguistic generalization and compositionality in a nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="Linguistic generalization and compositionality in a nutshell"></a></p>
<figcaption>Linguistic generalization and compositionality in a nutshell</figcaption>
</figure>
</div>
<p>In <span class="citation" data-cites="baroni2020linguistic">(Baroni 2020)</span> the author discusses the role of deep artificial neural networks in natural language processing tasks and their generalization abilities. The paper reviews the innovations characterizing modern deep language processing networks, such as large training datasets, gated recurrent networks, encoder-decoder architectures, and attention mechanisms. It then delves into studies examining the generalization abilities of deep networks, particularly in the context of grammatical generalization and compositional tasks. The paper presents empirical evidence suggesting that deep networks are capable of subtle grammar-dependent generalizations but do not rely on systematic compositional rules. It discusses the implications of these findings for linguistics and cognitive science, highlighting the need for better analytical tools to understand the mechanisms underlying the intriguing behavior of deep networks in language processing tasks.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"></div><section id="abstract" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>In the last decade, deep artificial neural networks have achieved astounding performance in many natural language processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: Are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar dependent generalizations, but also that they do not rely on systematic compositional rules. I argue that the intriguing behavior of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language.</p>
<p>‚Äî <span class="citation" data-cites="baroni2020linguistic">(Baroni 2020)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-baroni2020linguistic" class="csl-entry">
Baroni, Marco. 2020. <span>‚ÄúLinguistic Generalization and Compositionality in Modern Artificial Neural Networks.‚Äù</span> <em>Philosophical Transactions of the Royal Society B</em> 375 (1791): 20190307.
</div></div></div>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">1. Introduction</h3>
<ul>
<li>Mentions the history of neural networks as tools for modeling cognitive phenomena and their recent success as machine learning algorithms, particularly in natural language processing.</li>
<li>Highlights the need to understand how neural networks achieve their language skills, given their impressive performance in tasks such as machine translation.</li>
<li>Presents the argument for studying deep networks from a comparative psychology perspective, to gain insights into the nature of linguistic tasks and how computational devices solve them.</li>
<li>Discusses the concept of linguistic productivity, its relation to compositionality, and the debate on whether neural networks possess this ability, focusing on the intriguing generalization patterns observed in deep networks.</li>
</ul>
</section>
<section id="modern-deep-networks-for-language-processing-what-has-changed" class="level3">
<h3 class="anchored" data-anchor-id="modern-deep-networks-for-language-processing-what-has-changed">2. Modern Deep Networks for Language Processing: What Has Changed</h3>
<ul>
<li>Discusses the role of large training datasets in enhancing neural network performance, enabling complex, multi-layer architecture training, and facilitating language modeling as a general-purpose approach.</li>
<li>Describes gated recurrent networks, such as LSTMs and GRUs, with their information flow regulation mechanisms (gates) enhancing control and performance.</li>
<li>Explains the innovation of encoder-decoder architectures, decoupling input and output processing for effective handling of sequence-to-sequence tasks, commonly used in machine translation.</li>
<li>Presents the concept of attention mechanisms, enabling dynamic information access from past states, further improving the performance of encoder-decoder architectures, particularly in models relying heavily on attention over recurrent connections.</li>
</ul>
</section>
<section id="colorless-green-grammatical-generalization-in-deep-networks" class="level3">
<h3 class="anchored" data-anchor-id="colorless-green-grammatical-generalization-in-deep-networks">3. Colorless Green Grammatical Generalization in Deep Networks</h3>
<ul>
<li>Discusses the generalization abilities of modern language-processing neural networks, particularly in the context of machine translation, and the ongoing research into understanding the basis of this performance (shallow heuristics vs.&nbsp;grammar-based generalizations).</li>
<li>Presents Gulordava et al.‚Äôs study testing grammatical generalization in recurrent networks using controlled, nonsensical sentences designed to assess long-distance number agreement, showing above-chance performance across multiple languages.</li>
<li>Highlights Lakretz et al.‚Äôs ablation and connectivity studies of the Gulordava network, revealing specialized units sensitive to long-distance number information and connected to sub-networks sensitive to syntactic constituency, suggesting genuine grammatical processing mechanisms.</li>
<li>Notes that while networks demonstrate grammatical productivity in these studies, further research is needed to determine if they possess a rule-based system akin to traditional linguistic theory.</li>
</ul>
</section>
<section id="compositional-generalization-can-deep-networks-dax-twice" class="level3">
<h3 class="anchored" data-anchor-id="compositional-generalization-can-deep-networks-dax-twice">4. Compositional Generalization: Can Deep Networks Dax Twice?</h3>
<ul>
<li>Introduces the SCAN benchmark, a miniature language designed to test compositional abilities in sequence-processing networks, where networks learn to map commands to action sequences.</li>
<li>Explains the random split in SCAN, testing generic productivity, and the jump split, testing systematic compositionality by introducing a novel verb (‚Äújump‚Äù) only in isolation during training and evaluating its use in composite commands during testing.</li>
<li>Describes the around-right split, controlling for distributional biases in the jump split, where networks are trained on commands excluding the combination of ‚Äúaround‚Äù and ‚Äúright‚Äù but with sufficient evidence of their individual behavior.</li>
<li>Presents results showing recurrent networks excel in the random split but fail in the jump and around-right splits, while a convolutional network with heavy attention achieves partial success, suggesting a non-systematic form of generalization.</li>
</ul>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">5. Conclusion</h3>
<ul>
<li>Summarizes the surprising empirical evidence suggesting modern deep networks are proficient in language processing without demonstrably possessing compositional rules.</li>
<li>Discusses the need for better analytical tools to understand the mechanisms underlying this dissociation between productive grammatical competence and systematic compositionality.</li>
<li>Highlights the importance of exploring whether enhancing compositionality in neural networks can lead to improved adaptability and learning speed, potentially addressing their current limitations in tasks like machine reading and natural language inference.</li>
<li>Notes the potential for comparative studies between neural networks and human language, offering insights into the less systematic and fuzzy aspects of linguistic productivity, ultimately contributing to a more comprehensive understanding of human language.</li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Linguistic generalization and compositionality in modern artificial neural networks"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Linguistic generalization and compositionality in modern artificial neural networks</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Linguistic Generalization and Compositionality in Modern
    Artificial Neural Networks},
  date = {2025-01-01},
  url = {https://orenbochman.github.io/reviews/2019/baroni-linguistic-generalization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>‚ÄúLinguistic Generalization and
Compositionality in Modern Artificial Neural Networks.‚Äù</span> January
1, 2025. <a href="https://orenbochman.github.io/reviews/2019/baroni-linguistic-generalization/">https://orenbochman.github.io/reviews/2019/baroni-linguistic-generalization/</a>.
</div></div></section></div> ]]></description>
  <category>review, compositionality neural networks signaling systems language evolution</category>
  <guid>https://orenbochman.github.io/reviews/2019/baroni-linguistic-generalization/</guid>
  <pubDate>Tue, 31 Dec 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2019/baroni-linguistic-generalization/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Compositionality and Generalization in Emergent Languages</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2020/compositionality-and-generalization/</link>
  <description><![CDATA[ 





<section id="review-of-compositionality-and-generalization-in-emergent-languages" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review-of-compositionality-and-generalization-in-emergent-languages">Review of ‚ÄúCompositionality and Generalization in Emergent Languages‚Äù</h2>
<p>Very exciting - this is a paper with a lot of interesting ideas. It comes with a a lot of code in the form of a library called EGG as well as many JuPyteR notebooks. There is also a video of the talk at NeurIPS 2020.</p>
<p>In <span class="citation" data-cites="chaabouni-etal-2020-compositionality">(Chaabouni et al. 2020)</span> the authors look at ideas from representation learning and apply them to emergent languages in deep networks. THey come up with a number of results.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages <mark>inspired by disentanglement in representation learning</mark>, we establish three main results.</p>
<p>First, <mark>given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts.</mark></p>
<p>Second, there is <mark>no correlation between the degree of compositionality of an emergent language and its ability to generalize</mark>.</p>
<p>Third, <mark>while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission</mark>: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive</p>
<p>‚Äî <span class="citation" data-cites="chaabouni-etal-2020-compositionality">(Chaabouni et al. 2020)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<p>Here is the outline of the paper:</p>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>Describes a variant of Lewis signaling game used to study the emergence of reference to composite concepts in deep multi-agent simulations.</li>
<li>Discusses two specific and intuitive compositionality strategies that capture common compositional structures in natural languages.<br>
</li>
<li>Introduces two new compositionality measures, positional disentanglement (posdis) and bag-of-symbols disentanglement (bosdis), inspired by work on disentanglement in representation learning.</li>
</ul>
</section>
<section id="measurements" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="measurements">Measurements</h3>
<ul>
<li>Describes the commonly used <strong>topographic similarity</strong> (topsim) metric.</li>
<li>Introduces and defines two new measures of compositionality:
<ul>
<li>posdis - <strong>positional disentanglement</strong> and</li>
<li>bosdis - <strong>bag-of-symbols disentanglement</strong>.</li>
</ul></li>
<li>Explains how the new measures are similar to the <strong>Information Gap disentanglement measure</strong> used in representation learning.</li>
<li>Illustrates the behavior of the three compositionality metrics on three miniature languages in the Appendix.</li>
</ul>
<div id="note-topsim" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Topographic Similarity
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Given these two lists, the topographic similarity is defined as their negative Spearman œÅ correlation (since we are correlating distances with similarities, negative values of correlation indicate topographic similarity of the two spaces). <mark>Intuitively, if similar objects share much of the message structure (e.g., common prefixes or suffixes), and dissimilar objects have little common structure in their respective messages, then the topographic similarity should be high</mark>, the highest possible value being 1 ‚Äì <span class="citation" data-cites="lazaridou2018emergence">(Lazaridou et al. 2018)</span></p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathit%7Btopsim%7D=%5Crho%5Cleft(%5Cleft%5C%7Bd%5Cleft(%5Cmathbf%7Bx%7D%5E%7B(i)%7D,%20%5Cmathbf%7Bx%7D%5E%7B(j)%7D%5Cright),%20d%5Cleft(%5Cmathbf%7Bm%7D%5E%7B(i)%7D,%20%5Cmathbf%7Bm%7D%5E%7B(j)%7D%5Cright)%5Cright%5C%7D_%7Bi,%20j=1%7D%5E%7Bn%7D%5Cright)%0A"></p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-lazaridou2018emergence" class="csl-entry">
Lazaridou, Angeliki, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. 2018. <span>‚ÄúEmergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input.‚Äù</span> <a href="https://arxiv.org/abs/1804.03984">https://arxiv.org/abs/1804.03984</a>.
</div></div><div id="note-posdis" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Positional Disentanglement
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p><strong>positional disentanglement</strong> (posdis) metric measures whether symbols in specific positions tend to univocally refer to the values of a specific attribute. This order-dependent strategy is commonly encountered in natural language structures (and it is a pre-condition for sophisticated syntactic structures to emerge) ‚Äì <span class="citation" data-cites="chaabouni-etal-2020-compositionality">(Chaabouni et al. 2020)</span></p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathit%7Bposdis%7D=%5Cfrac%7B1%7D%7Bc_%7Blen%7D%7D%20%5Csum_%7Bj=1%7D%5E%7Bc_%7Blen%7D%7D%20%5Cfrac%7B%5Cmathcal%7BI%7D(s_j,a%5Ej_1)-%5Cmathcal%7BI%7D(s_j,a%5Ej_2)%7D%7B%5Cmathcal%7BH%7D(s_j)%7D%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?s_j"> the j<sup>th</sup> symbol of a message and</li>
<li><img src="https://latex.codecogs.com/png.latex?a%5Ej_1"> the attribute that has the highest mutual information with <img src="https://latex.codecogs.com/png.latex?s_j%20:%20a%5Ej_1%20=%20arg%20max_a%20%5Cmathcal%7BI%7D(s_j%20;%20a)"></li>
<li><img src="https://latex.codecogs.com/png.latex?a%5Ej_2"> the attribute that has the second highest mutual information with <img src="https://latex.codecogs.com/png.latex?s_j%20:%20a%5Ej_2%20=%20arg%20max_%7Ba%20%5Cneq%20a%5Ej_1%7D%20%5Cmathcal%7BI%7D(s_j%20;%20a)"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BH%7D(s_j)"> the entropy of j-th position (used as a normalizing term)</li>
</ul>
<p>positions with zero entropy are ignored in the computation.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"></div><div id="note-bosdis" class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bag-of-symbols Disentanglement
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Posdis assumes that a language uses positional information to disambiguate symbols. However, we can easily imagine a language where symbols univocally refer to distinct input elements independently of where they occur, making order irrelevant.3 Hence, we also introduce <strong>bag-of-symbols disentanglement</strong> (bosdis). The latter maintains the requirement for symbols to univocally refer to distinct meanings, but captures the intuition of a permutation-invariant language, where only symbol counts are informative ‚Äì <span class="citation" data-cites="chaabouni-etal-2020-compositionality">(Chaabouni et al. 2020)</span></p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathit%7Bbodis%7D=%5Cfrac%7B1%7D%7Bc_%7Bvoc%7D%7D%20%5Csum_%7Bj=1%7D%5E%7Bc_%7Bvoc%7D%7D%20%5Cfrac%7B%5Cmathcal%7BI%7D(n_j,a%5Ej_1)-%5Cmathcal%7BI%7D(n_j,a%5Ej_2)%7D%7B%5Cmathcal%7BH%7D(n_j)%7D%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?n_j"> a counter of the j-th symbol in a message</li>
</ul>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-chaabouni-etal-2020-compositionality" class="csl-entry">
Chaabouni, Rahma, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni. 2020. <span>‚ÄúCompositionality and Generalization in Emergent Languages.‚Äù</span> In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, edited by Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, 4427‚Äì42. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.407">https://doi.org/10.18653/v1/2020.acl-main.407</a>.
</div></div></section>
<section id="generalization-emerges-naturally-if-the-input-space-is-large" class="level3">
<h3 class="anchored" data-anchor-id="generalization-emerges-naturally-if-the-input-space-is-large">Generalization Emerges ‚ÄúNaturally‚Äù if the Input Space is Large</h3>
<ul>
<li>Presents an experiment showing that emergent languages are able to generalize to unseen combinations as long as input size is sufficiently large.</li>
<li>Discusses how the results challenge claims in the recent literature that deep networks fail to generalize.</li>
<li>Notes that the minimum channel capacity required for the emergence of a generalizing language is significantly larger than the minimum channel capacity required for a perfectly compositional language.</li>
<li>Presents additional experiments in the Appendix analyzing the effects of agent capacity and input density on generalization.</li>
</ul>
</section>
<section id="generalization-does-not-require-compositionality" class="level3">
<h3 class="anchored" data-anchor-id="generalization-does-not-require-compositionality">Generalization Does Not Require Compositionality</h3>
<ul>
<li>Presents results showing that there is no correlation between compositionality and generalization ability.</li>
<li>Analyzes the language of a specific run with near-perfect generalization accuracy and medium posdis score.</li>
<li>Discusses how the analyzed language uses a ‚Äúleaky disentanglement‚Äù strategy where two positions largely specialize as predictors of two attributes, respectively, but a third more entangled position is still necessary for perfect communication.</li>
<li>Briefly analyzes in the Appendix a language with near-perfect generalization accuracy and very low posdis score.</li>
</ul>
</section>
<section id="compositionality-and-ease-of-transmission" class="level3">
<h3 class="anchored" data-anchor-id="compositionality-and-ease-of-transmission">Compositionality and Ease of Transmission</h3>
<ul>
<li>Discusses the hypothesis that compositional languages are easier to decode and transmit to new learners.</li>
<li>Presents an experiment where new Receivers are trained on frozen Senders that achieved a high level of generalization accuracy.</li>
<li>Finds that learning speed and generalization accuracy of new Receivers are strongly positively correlated with degree of compositionality.</li>
<li>Mentions further experiments in the Appendix that replicate the ease-of-transmission analysis across various channel capacities.</li>
</ul>
</section>
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">Discussion</h3>
<ul>
<li>Summarizes the main findings of the paper, highlighting the results that challenge common assumptions in the emergent language literature.</li>
<li>Relates the findings to the ongoing debate on the origins of compositionality in natural language.</li>
<li>Discusses the potential benefits of compositionality for developing languages that are quickly usable by wide communities of artificial agents.</li>
<li>Highlights the connection between compositionality and disentanglement in representation learning.</li>
</ul>
</section>
</section>
<section id="my-thoughts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="my-thoughts">My Thoughts</h2>
<p>We have three main results:</p>
<ol type="1">
<li><strong>Generalization Emerges ‚ÄúNaturally‚Äù if the Input Space is Large</strong></li>
<li><strong>No Correlation Between Compositionality and Generalization</strong></li>
<li><strong>Compositionality increases Ease of Transmission</strong></li>
</ol>
<p>From a simple analysis of the lewis signaling game. The cost of coordination for an non compositional language is exponential in the number of states. For a compositional language the cost can reduced an exponential of the size of the lexicon (number of atomic signals) plus some constant factor for learning the grammar if it is some small set of aggregation rules. (Usually one rule is sufficient to support compositionality).</p>
<p>The lewis signaling game is more or less guaranteed to converge to some a signaling system<sup>1</sup> with the suitable algorithm. Without a good algorithm the game is more likely to converge to a partial pooling equilibrium where payoffs are less than 1 due to one side or both being unable to conflate different states.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;one that has an expected payoff of 1</p></div></div><p>What this means is that all things being equal the compositional language will emerge much sooner the the non compositional language given the opportunity to do so. So why don‚Äôt we see this.</p>
<ol type="1">
<li>When the number of states isn‚Äôt larger than the number of basic signals it is easier to learn a non compositional language. So we should put a cap on the number of basic signals.</li>
<li>The multiplier for learning grammar may be big especially if we use a neural network, more so if the grammar is complicates. So if we don‚Äôt see compositionality perhaps we need to make the grammar simpler or the state space bigger.</li>
<li>This is theoretical - perhaps since we use a neural network rather then a tabular RL solution we need lots of data to learn anything. As we go though the epoch there may be enough rounds in the lewis game for use to establish a convention without the need for compositionality.</li>
<li>Ok Let‚Äôs say we have a fast learning algorithm and a big but manageable input space. Do we get compositionality. The answer is not necessarily.</li>
</ol>
<p>Let dive deeper into this last point:</p>
<p>Despite what Chat GPT will tell you if you ask the lewis signaling game only outputs simple languages. No complex signals, no grammar, no recursion and no compositionality. It has many possible equilibria and none correspond to a language complex signals or grammar.</p>
<p>For grammar and complex signals to emerge you need to tweak the lewis signaling game. <span class="citation" data-cites="skyrms2010signals">(Skyrms 2010)</span> reports on a couple of papers which produce complex signals. Most of the interesting work came out after the book. Regardless in the papers the agents were given a simple aggregation rule to follow. The conjunction leads to a bag of words. The concatenation leads to sequences. But what they don‚Äôt seem to stress is that for a complex signals we want a state that decomposes into a way we can match in our aggregation rule. Think group homomorphism. And there may be multiple decompositions so think normal subgroups.</p>
<div class="no-row-height column-margin column-container"><div id="ref-skyrms2010signals" class="csl-entry">
Skyrms, Brian. 2010. <span>‚Äú<span class="nocase">14512 Complex Signals and Compositionality</span>.‚Äù</span> In <em><span class="nocase">Signals: Evolution, Learning, and Information</span></em>. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780199580828.003.0013">https://doi.org/10.1093/acprof:oso/9780199580828.003.0013</a>.
</div></div><p>There isn‚Äôt a how to guide to get the agents to use some arbitrary grammar. (Not AFAIK). There are a bunch of books and many papers but they don‚Äôt seem to have a the kind of recipes that are needed here. In my view most of these books look for the answers based on what they know rather what they need to know. They may have fascinating points but lead to greater confusion rather then more clarity.</p>
<p>One abstraction I came across is that the notion of a grammar is essentially a decision tree mapping the signals back into the state. Decision trees sounds simple enough but this tree is not given but needs to be learned by trial and error. Signals due to successes are sparse. There might be a setting in which a sender can construct the tree and then the receiver just needs to learn it. But it requires the sender to have access to the distribution of states and sub states. This distribution can be used to come up with a tree that is optimal for a given set of signals. If the sender and receiver don‚Äôt have access to this distribution the can learn it. But my work indicates that to learn the distribution to a high degree of accuracy requires more turns then my lewis signaling algorithm does. At least for simple signaling systems.</p>
<p>For a simple signaling system I developed two algorithms. THe first learned a signaling system. The second first enumerated all signaling systems of a certain size and then selected one from those it believed were optimal. Each new state would reduce the belief until the sender and receiver had a common belief. This may not scale but can adapt to new distributions seamlessly. I thought about a similar approach for working with complex grammars. But In this case I did not have an efficient way to enumerate all possible grammars. However there seems to be a way to do this. Instead of considering all decision trees, we can instead consider just huffman trees. These means that the sender and receiver use the lewis signaling game to learn a shared huffman tree. The outcome is that the tree should compress the state space. The only problem is that such a grammar is not likely to be compositional and would be very difficult to learn for humans.</p>
<p>So what we need is for the agents to learn the tree interactively. Two approaches come to mind and these are 1. huffman coding - which builds the tree but doesn‚Äôt update it to account for distributional shits. 2. Vitter algorithm for adaptive huffman coding - which updates the tree as new states are seen. THis is 3. adaptive arithmetic coding - which is a generalization of adaptive huffman coding.</p>
<p>One point to consider is that such a grammar is likely to provide a good compression of the state space. This is due to the these algorithms also being compression algorithms.</p>
<p>I would imagine that the output of such a grammar to be a binary sequence. This suggest that this would lead to a entangled representation with no discernable compositional structure.</p>
<p>Now there are reputedly many languages with very simple grammars. But the ones we are familiar with are not simple. They also have large lexicons. We need to put that aside and look for ways to work with simple grammars. It is quite possible to come up with two or three rules that can generate both morphology and a recursive syntax. It might be possible with one rule.</p>
<p>Ok lets briefly consider the other two points.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Compositionality and Generalization in Emergent Languages"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>Compositionality and Generalization in Emergent Languages</figcaption>
</figure>
</div>
<p>there is also a video at</p>
<p><a href="https://slideslive.com/38928781/compositionality-and-generalization-in-emergent-languages">video</a></p>
</section>
<section id="the-code" class="level2">
<h2 class="anchored" data-anchor-id="the-code">The code</h2>
<p>and code at</p>
<p><a href="https://paperswithcode.com/paper/compositionality-and-generalization-in">papers with code</a></p>
<p>and even the colab link to</p>
<p><a href="https://colab.research.google.com/github/facebookresearch/EGG/blob/main/tutorials/EGG%20walkthrough%20with%20a%20MNIST%20autoencoder.ipynb">EGG walkthrough</a></p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Compositionality and {Generalization} in {Emergent}
    {Languages}},
  date = {2025-01-01},
  url = {https://orenbochman.github.io/reviews/2020/compositionality-and-generalization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>‚ÄúCompositionality and Generalization in
Emergent Languages.‚Äù</span> January 1, 2025. <a href="https://orenbochman.github.io/reviews/2020/compositionality-and-generalization/">https://orenbochman.github.io/reviews/2020/compositionality-and-generalization/</a>.
</div></div></section></div> ]]></description>
  <category>review</category>
  <category>compositionality</category>
  <category>neural networks</category>
  <category>signaling systems</category>
  <category>language evolution</category>
  <guid>https://orenbochman.github.io/reviews/2020/compositionality-and-generalization/</guid>
  <pubDate>Tue, 31 Dec 2024 22:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2020/compositionality-and-generalization/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Emergent Communication of Generalizations</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>I think this is an amazing paper. I read it critically and made copious notes to see what I could learn from it. The paper point out some limitations of Lewis referential games^[In this game the sender sees images, it needs to classify them into some representation and sends a message. The reciever gets the same or similar images + distractors, it needs to run a classifier and needs to select the correct one. Learning an image classifier per agent is expensive and requires access to both an an image classification is likely shared. This however presents a problem‚Ä¶. It allows the agents? ] and suggest a couple of extentions that can over come these limitations. There is</p>
<p><span class="citation" data-cites="mu2022emergentcomms">(Mu and Goodman 2021)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-mu2022emergentcomms" class="csl-entry">
Mu, Jesse, and Noah D. Goodman. 2021. <span>‚ÄúEmergent Communication of Generalizations.‚Äù</span> <em>CoRR</em> abs/2106.02668. <a href="https://arxiv.org/abs/2106.02668">https://arxiv.org/abs/2106.02668</a>.
</div></div></section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>To build agents that can collaborate effectively with others, recent research has trained artificial agents to communicate with each other in Lewis-style referential games. However, this often leads to successful but uninterpretable communication. We argue that this is due to the game objective: communicating about a single object in a shared visual context is prone to overfitting and does not encourage language useful beyond concrete reference. In contrast, human language conveys a rich variety of abstract ideas. To promote such skills, we propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. We find that these games greatly improve systematicity and interpretability of the learned languages, according to several metrics in the literature. Finally, we propose a method for identifying logical operations embedded in the emergent languages by learning an approximate compositional reconstruction of the language.</p>
</section>
<section id="the-video" class="level2">
<h2 class="anchored" data-anchor-id="the-video">The Video</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/LVW_t7p42X0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<p>Here is the paper with my annotation and highlights.</p>
</section>
<section id="annotations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="annotations">Annotations</h2>

<div class="no-row-height column-margin column-container"><div class="">
<blockquote class="blockquote">
<p>To promote such skills, propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. <sup>1</sup></p>
<div id="fn1"><p><sup>1</sup>&nbsp;Significant modification the game: tweak payoffs, assign categories to symbols and allow sending of categories.</p></div></blockquote>
<blockquote class="blockquote">
<p>We argue that the reference games typically used in these studies are ill-suited to drive linguistic <em>systematicity</em> for two reasons <sup>2</sup></p>
<div id="fn2"><p><sup>2</sup>&nbsp;The best the original Lewis signaling game can do is establish a one to one convention between a sender‚Äôs siganal of states and reciever action per states. This is just a coordination part of communication.</p></div></blockquote>
<blockquote class="blockquote">
<p>These tasks are more difficult <sup>3</sup></p>
<div id="fn3"><p><sup>3</sup>&nbsp;adding categories can result in a combinatorial increase the total messages. So that the agents need to coordinate on one of many more equilibria. Also you now want the agents to learn a much narrower subset of those possible equilibrium i.e.&nbsp;those that are are faithfull to certain structures in of the states. This is essentially a new problem which could be embodies as a second step after the Lewis game. There is no guarantee in general that such a structure exists. And as the authors suggest other structures are not considered</p></div></blockquote>
<blockquote class="blockquote">
<p>In the <strong>set reference</strong> (setref) game, a teacher must communicate to a student not just a single object, but rather a group of objects belonging to a concept <sup>4</sup></p>
<div id="fn4"><p><sup>4</sup>&nbsp;this is an interesting game - and also similar to <span class="cn">add reference</span> one modifies the game to refer to set of states by adding and, or and not operators giving an agent an basic reasoning ability. The new signaling systems allows specifying many more states. This can be useful in many applications. Of course learning to send additional operators becomes trivial conceptually if not in the practical sense</p></div></blockquote>
<blockquote class="blockquote">
<p>These tasks are more difficult than traditional reference games <sup>5</sup></p>
<div id="fn5"><p><sup>5</sup>&nbsp;a ‚Äúconcept‚Äù like a red triangle is a specific type of a set. so this should be a easier task than the set reference. The difficulty seems to be not in the language or the categories but in the added classification of varied visual representation of seagulls</p></div></blockquote>
</div></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf#page=1" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf#page=1" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
<hr>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf#page=2" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="paper"><embed src="./paper.pdf#page=2" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Emergent {Communication} of {Generalizations}},
  date = {2024-10-09},
  url = {https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúEmergent Communication of
Generalizations.‚Äù</span> October 9, 2024. <a href="https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/">https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/</a>.
</div></div></section></div> ]]></description>
  <category>signaling games</category>
  <category>stub</category>
  <guid>https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/</guid>
  <pubDate>Tue, 08 Oct 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Evolutionary dynamics of Lewis signaling games: signaling systems vs.¬†partial pooling</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p><span class="citation" data-cites="huttegger2010evolutionary">(Huttegger et al. 2010)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-huttegger2010evolutionary" class="csl-entry">
Huttegger, Simon M, Brian Skyrms, Rory Smead, and Kevin JS Zollman. 2010. <span>‚ÄúEvolutionary Dynamics of Lewis Signaling Games: Signaling Systems Vs. Partial Pooling.‚Äù</span> <em>Synthese</em> 172: 177‚Äì91.
</div></div></section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>Abstract Transfer of information between senders and receivers, of one kind or another, is essential to all life. David Lewis introduced a game theoretic model of the simplest case, where one sender and one receiver have pure common interest. How hard or easy is it for evolution to achieve information transfer in Lewis signaling?. The answers involve surprising subtleties. We discuss some if these in terms of evolutionary dynamics in both finite and infinite populations, with and without mutation.</p>
</section>
<section id="this-paper" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="this-paper">This paper</h2>
<p>I‚Äôve not looked too deeply into this paper as it needs a deep dive into evolutionary dynamics which I‚Äôve yet to study in depth. My interests are RL and complex signaling systems. Some of the results in evolutionary dynamics can be directly applied to reinforcement learning.</p>
<p>The paper looks at moran processes and mutations. This is interesting in the open ended setting of language evolution.</p>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Because this is a partnership game, average payoff is a <strong>Lyapunov function</strong> <sup>1</sup> for the system [in fact, it is even a potential function; see <span class="citation" data-cites="hofbauer1998evolutionary">(Hofbauer and Sigmund 1998)</span> ].</p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Lyapunov functions</strong> are scalar functions that may be used to prove the stability of an equilibrium of an ODE</p></div><div id="ref-hofbauer1998evolutionary" class="csl-entry">
Hofbauer, Josef, and Karl Sigmund. 1998. <em>Evolutionary Games and Population Dynamics</em>. Cambridge university press.
</div></div></div>
<p>The look at Lyapunov functions and the replicator dynamics. This is interesting in the context stability of equilibria in signaling games.</p>
<blockquote class="blockquote">
<p>raises the same questions in the context of evolution in finite populations with fixed population size (via the Moran process with and without mutation)</p>
</blockquote>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Evolutionary Dynamics of {Lewis} Signaling Games: Signaling
    Systems Vs. Partial Pooling},
  date = {2024-10-08},
  url = {https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúEvolutionary Dynamics of Lewis Signaling
Games: Signaling Systems Vs. Partial Pooling.‚Äù</span> October 8, 2024.
<a href="https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/">https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/</a>.
</div></div></section></div> ]]></description>
  <category>signaling games</category>
  <category>evolutionary dynamics</category>
  <category>complex signaling system</category>
  <category>stub</category>
  <guid>https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/</guid>
  <pubDate>Mon, 07 Oct 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Why Overfitting Isn‚Äôt Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2020/Why Overfitting Isn‚Äôt Always Bad/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>This paper, ‚ÄúWhy Overfitting Isn‚Äôt Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries,‚Äù challenges the traditional view that overfitting is inherently detrimental when developing cross-lingual word embeddings (CLWE)<sup>1</sup> and that the evaluation of CLWE using Bilingual Lexicon Induction (BLI) is flawed.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;see my note below regarding this point.</p></div></div><div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cross Language Word Embeddings (CLWE) in a nutshell
</div>
</div>
<div class="callout-body-container callout-body">
<p>Since I have not looked into CLWE before I add this outline on how CLWE are being learned from in the video lined below.</p>
<ol type="1">
<li>embeddings are learned for each language</li>
<li>a Bilingual dictionary provides a mapping from word pairs which is used to tweak the embeddings so they align across languages.</li>
</ol>
</div>
</div>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting the training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.computational resources to train.</p>
<p>‚Äî <span class="citation" data-cites="Zhang2020XLingEmbedd">(Zhang et al. 2020)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-Zhang2020XLingEmbedd" class="csl-entry">
Zhang, Mozhi, Yoshinari Fujinuma, Michael J. Paul, and Jordan Boyd-Graber. 2020. <span>‚ÄúWhy Overfitting Isn‚Äôt Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries.‚Äù</span> In <em>Association for Computational Linguistics</em>. The Cyberverse Simulacrum of Seattle. <a href="http://umiacs.umd.edu/~jbg//docs/2020_acl_refine.pdf">http://umiacs.umd.edu/~jbg//docs/2020_acl_refine.pdf</a>.
</div></div></div>
</section>
<section id="key-points" class="level2">
<h2 class="anchored" data-anchor-id="key-points">Key Points:</h2>
<ol type="1">
<li><strong>Traditional Evaluation of CLWE</strong>:
<ul>
<li>Cross-lingual word embeddings (CLWE) typically aim to map words from different languages into a shared vector space.</li>
<li>They are commonly evaluated using Bilingual Lexicon Induction (BLI), which tests the model‚Äôs ability to translate words based on a set of test words.</li>
</ul></li>
<li><strong>Underfitting in Projection-Based CLWE</strong>:
<ul>
<li>Current CLWE methods, particularly linear projection-based ones, underfit the training dictionary. This means they don‚Äôt perfectly align all translation pairs in the training data.</li>
<li>The paper argues that this underfitting, while beneficial for BLI test accuracy, can hinder performance on downstream tasks where words from the training dictionary play a critical role.</li>
</ul></li>
<li><strong>Retrofitting Approach</strong>:
<ul>
<li>The authors propose retrofitting as a post-processing step to bring training translation pairs closer in the embedding space, essentially overfitting the training dictionary.</li>
<li>This retrofitting involves modifying the embeddings to minimize distances between training translation pairs while retaining as much of the original structure as possible.</li>
</ul></li>
<li>Retrofitting to Synthetic Dictionaries:
<ul>
<li>In addition to the training dictionary, the paper introduces retrofitting to a synthetic dictionary induced from the original CLWE using the Cross-Domain Similarity Local Scaling (CSLS) heuristic.</li>
<li>This helps balance the need to fit the training dictionary while maintaining some generalization capability for unseen words.</li>
</ul></li>
</ol>
</section>
<section id="experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results:</h2>
<ol type="1">
<li><strong>BLI Accuracy</strong>:
<ul>
<li>Retrofitting the embeddings to the training dictionary results in perfect alignment of training pairs, leading to decreased BLI test accuracy since the embeddings overfit the training data.</li>
<li>However, retrofitting to a synthetic dictionary can achieve a balance, improving BLI test accuracy somewhat while still fitting the training data better.</li>
</ul></li>
<li><strong>Downstream Tasks</strong>:
<ul>
<li>The authors evaluates the retrofitted embeddings on two downstream tasks: <strong>document classification</strong> and <strong>dependency parsing</strong>.</li>
<li>Despite lower BLI test accuracy, retrofitted embeddings often lead to improved performance on these tasks. This underscores the importance of fully exploiting the training dictionary for downstream performance.</li>
</ul></li>
</ol>
</section>
<section id="main-contributions" class="level2">
<h2 class="anchored" data-anchor-id="main-contributions">Main Contributions:</h2>
<ol type="1">
<li><strong>Challenge to BLI as a Sole Metric</strong>:
<ul>
<li>The authors argues that BLI accuracy does not always correlate with downstream task performance, revealing the limitations of relying solely on BLI for evaluating CLWE.</li>
</ul></li>
<li><strong>Retrofitting as a Beneficial Overfitting</strong>:
<ul>
<li>It shows that overfitting to the training dictionary through retrofitting can be beneficial, enhancing the performance of downstream tasks even if it harms BLI test accuracy.</li>
</ul></li>
<li>Synthetic Dictionary for Balance:
<ul>
<li>Introducing the use of a synthetic dictionary for retrofitting provides a middle ground, balancing the need to fit the training dictionary while retaining some generalization ability.</li>
</ul></li>
</ol>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion:</h2>
<p>The authors suggest that the overemphasis on BLI as an evaluation metric for CLWE should be reconsidered, advocating instead for a focus on downstream tasks that better reflect the utility of the embeddings. They propose that future work might explore more complex non-linear projections to better fit the dictionary without compromising on generalization.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion:</h2>
<p>The paper‚Äôs main takeaway is that overfitting to the training dictionary via retrofitting is not inherently harmful. In fact, it can lead to better performance on downstream NLP tasks. This insight invites a reconsideration of the evaluation metrics for cross-lingual word embeddings and opens the door for future work on more sophisticated retrofitting and projection methods.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
overfitting
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>It is not surprising that the authors ‚Äòdiscovered‚Äô that fitting the model on <code>test</code> + <code>train</code> + <code>validation</code> gives better results then fitting on <code>train</code> part only. After all the best way to reduce overfitting is to give it more data.</li>
<li>This is a common practice in machine learning to train on the full dataset once we have used to get the best results when the model is deployed to production.</li>
<li>Bayesian LOO Cross validation also allows one to measure the generalization error of the model without having to make the sacrifice of a train/test split.</li>
<li>Considering that the dataset is a dictionary means there is probably little noise to overfit on.</li>
</ul>
<p>On the other hand their criticism of BLI as a metric is valid. There are lots of bad metrics and it is more so when we wish to use it to estimate performance on a different task. In RL one uses importance sampling to make the correction between on policy and off policy - perhaps this is something to look into, though it would be easier to do this if this was a RL problem rather than supervised ML.</p>
<p>It is a common practice in machine learning to use a proxy metric to evaluate the model. The problem is that the proxy metric is not always a good indicator of the model‚Äôs performance on the real task. This is why it is important to evaluate the model on the real task as well.</p>
</div>
</div>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also</h2>
<ul>
<li><a href="http://users.umiacs.umd.edu/~jbg/docs/2020_acl_refine.pdf">Paper</a></li>
<li><a href="https://www.youtube.com/watch?v=yVN47wGkCko">Video</a></li>
<li><a href="https://github.com/zhangmozhi/retrofit_clwe">Code</a></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Why {Overfitting} {Isn‚Äôt} {Always} {Bad:} {Retrofitting}
    {Cross-Lingual} {Word} {Embeddings} to {Dictionaries}},
  date = {2024-06-11},
  url = {https://orenbochman.github.io/reviews/2020/Why Overfitting Isn‚Äôt Always Bad/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúWhy Overfitting Isn‚Äôt Always Bad:
Retrofitting Cross-Lingual Word Embeddings to Dictionaries.‚Äù</span> June
11, 2024. <a href="https://orenbochman.github.io/reviews/2020/Why Overfitting Isn‚Äôt Always Bad/">https://orenbochman.github.io/reviews/2020/Why
Overfitting Isn‚Äôt Always Bad/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2020/Why Overfitting Isn‚Äôt Always Bad/</guid>
  <pubDate>Mon, 10 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>The Evolution of Coding in signaling games</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2007/evolution_of_coding/</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR: The Evolution of Coding in signaling games
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="The Evolution of Coding In a Nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="The Evolution of Coding In a Nutshell"></a></p>
<figcaption>The Evolution of Coding In a Nutshell</figcaption>
</figure>
</div>
<p>This paper considers a setting for the evolution of a complex signaling systems</p>
<ul>
<li>in this case social evolution or reinforcement learning. As I have already implemented such a system based on Skyrym‚Äôs work I was interested in the results. I think this is one of the papers discussed in the book by Skyrms. The main contribution is an extension of the Lewis signaling game of multiple senders that send partial binary signals to the receiver. The receiver aggregates these into a sequence ordered by sender. A complex signaling system is thus learned in which senders spontaneously learn to send a specific partial state.</li>
</ul>
<p>It may seem surprising that the senders learn to coordinate what part of the state to send even though they start out by picking the signals to send independently of each other.</p>
</div>
</div>
<section id="abstract" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Signaling games with reinforcement learning have been used to model the evolution of term languages (<span class="citation" data-cites="lewis1969convention">(Lewis 1969)</span>; <span class="citation" data-cites="skyrms2010signals">(Skyrms 2010)</span>). In this article, syntactic games, extensions of David Lewis‚Äôs original sender‚Äìreceiver game, are used to illustrate how a language that exploits available syntactic structure might evolve to code for states of the world. The evolution of a language occurs in the context of available vocabulary and syntax‚Äîthe role played by each component is compared in the context of simple reinforcement learning</p>
<p>‚Äì <span class="citation" data-cites="barrett2009evolution">(Barrett 2009)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-lewis1969convention" class="csl-entry">
Lewis, David Kellogg. 1969. <em>Convention: A Philosophical Study</em>. Cambridge, MA, USA: Wiley-Blackwell.
</div><div id="ref-barrett2009evolution" class="csl-entry">
Barrett, Jeffrey A. 2009. <span>‚ÄúThe Evolution of Coding in Signaling Games.‚Äù</span> <em>Theory and Decision</em> 67 (2): 223‚Äì37. <a href="https://doi.org/10.1007/s11238-007-9064-0">https://doi.org/10.1007/s11238-007-9064-0</a>.
</div></div></div>
</section>
<section id="the-review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-review">The Review</h2>
<p>First the Lewis signaling game is explained - the game is not presented in terms of game theory e.g.&nbsp;it‚Äôs extensive form but using a state transition diagram.</p>

<div class="no-row-height column-margin column-container"><div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_1.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;1: Lewis signaling game"><img src="https://orenbochman.github.io/reviews/2007/evolution_of_coding/fig_1.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Lewis signaling game
</figcaption>
</figure>
</div></div><p>Later lewis signaling variant with two senders is introduced</p>

<div class="no-row-height column-margin column-container"><div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_2.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;2: two sender Lewis signaling game"><img src="https://orenbochman.github.io/reviews/2007/evolution_of_coding/fig_2.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: two sender Lewis signaling game
</figcaption>
</figure>
</div></div><p>The paper starts with the standard Lewis signaling game a metric for signaling success a few learning algorithms</p>
<ul>
<li>Urn Model after Richard Herrnstein‚Äôs matching law and with Rewards := {Succ: +1, Fail:0}</li>
<li>Urn Model after Richard Herrnstein‚Äôs matching law and with Rewards := {Succ: +2, Fail:-1} AKA ARP model</li>
<li>Bereby-Meyer and Erev 1998 ‚ÄúOn learning to become a successful loser‚Äù</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Better RL Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Herrnstein‚Äôs matching law <span class="citation" data-cites="Herrnstein1961Reinforcement">(Herrnstein 1961)</span> is not a well suited to learning in the signaling system. It is not sample efficient (e.g.&nbsp;it does not learn from making mistakes thus most of the samples are discarded) nor can it escape form the numerous attraction basins of the (partial) pooling equilibria which represent local maxima of expected returns to locate the much rarer global maxima of the separating equilibria that we call signaling systems.</p>
<p>Thus the authors looked for algorithms that could converge faster and also overcome the attraction basins due to local minima of the partial pooling equilibria. Some alternatives listed by Skyrms in <span class="citation" data-cites="skyrms2010signals">(Skyrms 2010)</span> are:</p>
<ul>
<li>Thorndike‚Äôs law of effect <span class="citation" data-cites="Thorndike1927LawOfEffect">(Thorndike 1927)</span></li>
<li>Richard Herrnstein‚Äôs matching law <span class="citation" data-cites="Herrnstein1961Reinforcement">(Herrnstein 1961)</span></li>
<li>Roth-Erev learning algorithm <span class="citation" data-cites="RothErev1995Learning">(Roth and Erev 1995)</span></li>
<li>Bush‚ÄìMosteller Reinforcement <span class="citation" data-cites="bush1953stochastic">(Bush and Mosteller 1953)</span> Has been shown to correspond the replicator dynamics in the limit of small learning rates.</li>
<li>Yoella Bereby-Meyer and Ido Erev ARP model <span class="citation" data-cites="Bereby1998Loser">(Bereby-Meyer and Erev 1998)</span></li>
</ul>
<p>This paper looks at a variant of the Roth-Erev learning which was published in <span class="citation" data-cites="Bereby1998Loser">(Bereby-Meyer and Erev 1998)</span> by Bereby-Meyer and Erev titled ‚ÄúOn Learning To Become a Successful Loser‚Äù. Here the authors consider different abstraction of losses in repeated choice tasks. They use a probability learning task. (basically to estimate <img src="https://latex.codecogs.com/png.latex?p%5Cneq0.5"> in a Bernulli trial). They found that by adding constants to the payoff matrix enhanced the learning process and more significantly that learning in the loss domain was faster then in the gain domain.<sup>1</sup></p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;there are many games where lossess are much more common, also we might not be able to get a signal from a mistake‚Ä¶. </p></div></div><div class="no-row-height column-margin column-container"><div id="ref-Bereby1998Loser" class="csl-entry">
Bereby-Meyer, Yoella, and Ido Erev. 1998. <span>‚ÄúOn Learning to Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain.‚Äù</span> <em>Journal of Mathematical Psychology</em> 42 (2): 266‚Äì86. https://doi.org/<a href="https://doi.org/10.1006/jmps.1998.1214">https://doi.org/10.1006/jmps.1998.1214</a>.
</div></div><div class="no-row-height column-margin column-container"></div><div class="no-row-height column-margin column-container"><div id="ref-bush1953stochastic" class="csl-entry">
Bush, Robert R., and Frederick Mosteller. 1953. <span>‚Äú<span class="nocase">A Stochastic Model with Applications to Learning</span>.‚Äù</span> <em>The Annals of Mathematical Statistics</em> 24 (4): 559‚Äì85. <a href="https://doi.org/10.1214/aoms/1177728914">https://doi.org/10.1214/aoms/1177728914</a>.
</div></div><div class="no-row-height column-margin column-container"><div id="ref-RothErev1995Learning" class="csl-entry">
Roth, Alvin E., and Ido Erev. 1995. <span>‚Äú<span class="nocase">Learning in extensive-form games: Experimental data and simple dynamic models in the intermediate term</span>.‚Äù</span> <em>Games and Economic Behavior</em> 8 (1): 164‚Äì212. <a href="https://ideas.repec.org/a/eee/gamebe/v8y1995i1p164-212.html">https://ideas.repec.org/a/eee/gamebe/v8y1995i1p164-212.html</a>.
</div></div><div class="no-row-height column-margin column-container"><div id="ref-Herrnstein1961Reinforcement" class="csl-entry">
Herrnstein, R. J. 1961. <span>‚ÄúRELATIVE AND ABSOLUTE STRENGTH OF RESPONSE AS a FUNCTION OF FREQUENCY OF REINFORCEMENT.‚Äù</span> <em>Journal of the Experimental Analysis of Behavior</em> 4 (3): 267‚Äì72. https://doi.org/<a href="https://doi.org/10.1901/jeab.1961.4-267">https://doi.org/10.1901/jeab.1961.4-267</a>.
</div></div><div class="no-row-height column-margin column-container"><div id="ref-Thorndike1927LawOfEffect" class="csl-entry">
Thorndike, Edward L. 1927. <span>‚ÄúThe Law of Effect.‚Äù</span> <em>The American Journal of Psychology</em> 39 (1/4): 212‚Äì22. <a href="http://www.jstor.org/stable/1415413">http://www.jstor.org/stable/1415413</a>.
</div></div><div class="no-row-height column-margin column-container"></div><div class="no-row-height column-margin column-container"></div><pre><code>Bereby-Meyer and Erev</code></pre>
</section>
<section id="the-paper" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>To my considerable surprise, Barrett found that Roth‚ÄìErev reinforcement learners reliably learned to optimally categorize and signal. ‚Äî <span class="citation" data-cites="skyrms2010signals">(Skyrms 2010, 140)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-skyrms2010signals" class="csl-entry">
Skyrms, Brian. 2010. <span>‚Äú<span class="nocase">14512 Complex Signals and Compositionality</span>.‚Äù</span> In <em><span class="nocase">Signals: Evolution, Learning, and Information</span></em>. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780199580828.003.0013">https://doi.org/10.1093/acprof:oso/9780199580828.003.0013</a>.
</div></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Blind Watchmaker in the Lewis Signaling Game
</div>
</div>
<div class="callout-body-container callout-body">
<p>In his popular science Classic ‚ÄúThe blind watchmaker‚Äù <span class="citation" data-cites="dawkins1986the-blind-watch">(Dawkins 1986)</span>, the author, Richard Dawkins, suggest the notion of a blind watchmaker. The idea is that the blind watchmaker does not have to know how to make a watch rather he has to be able to learn by by trial and error how to make better watches. Given the millions of years, evolution‚Äôs blind watch maker seems to design wonders with much greater complexity than any watch.</p>
<p>The way I understand this ability to learn to coordinate is</p>
<ol type="1">
<li>The senders don‚Äôt really learn to coordinate - we have a blind watchmaker at work here‚Ä¶.</li>
<li>Initially Senders don‚Äôt really have any meaning in the Lewis signaling game. Only when the environment assign a positive reward (which is a spontaneous symmetry breaking event) due to the a receiver taking the correct action. At this point the senders are bound to send the same signals they each sent before. That is the convention they are learning.</li>
<li>Assuming they won‚Äôt use the same signal combinations again for a different state, this is repeated until all the states and signals are learned.</li>
<li>There is no reason for the senders to send the same part of the state - they could alternate.</li>
</ol>
<p>2 signaling systems for a 4 by 4 game</p>
<table class="caption-top table">
<caption>a SS with correlated senders and sub-state</caption>
<thead>
<tr class="header">
<th>state</th>
<th>sender 1</th>
<th>sender 2</th>
<th>receiver</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0,0</td>
<td>0</td>
<td>0</td>
<td>0,0</td>
</tr>
<tr class="even">
<td>0,1</td>
<td>0</td>
<td>1</td>
<td>0,1</td>
</tr>
<tr class="odd">
<td>1,0</td>
<td>1</td>
<td>0</td>
<td>1,1</td>
</tr>
<tr class="even">
<td>1,1</td>
<td>1</td>
<td>1</td>
<td>1,0</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>a SS with un-correlated senders and sub-state</caption>
<thead>
<tr class="header">
<th>state</th>
<th>sender 1</th>
<th>sender 2</th>
<th>receiver</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0,1</td>
<td>1</td>
<td>1</td>
<td>0,1</td>
</tr>
<tr class="even">
<td>1,0</td>
<td>0</td>
<td>0</td>
<td>1,1</td>
</tr>
<tr class="odd">
<td>0,0</td>
<td>0</td>
<td>1</td>
<td>0,0</td>
</tr>
<tr class="even">
<td>1,1</td>
<td>1</td>
<td>0</td>
<td>1,0</td>
</tr>
</tbody>
</table>
<p>For sender 1 and sender 2 any mapping of the signals to state that is one to one works.</p>
<p>However in terms of learnability a system that has a correlated sender sub-state assignment is easier to learn. Is easier to learn because the receiver can learn the mapping of the signals to the state in a single step.</p>
<p>To sum up: senders are assigned a one to one mapping spontaneously whenever the receiver takes the correct action. If they are de correlated with each other or correlated with a sub-state is not important.</p>
<p>However we can see that an arbitrary assignment of the signals to the state is harder to learn. This is because the receiver has to learn the full mapping of the signals to the state. Whereas in the correlated case the receiver can reuse the mapping to generalize.</p>
<p>Note: here we assume that the agents consider the signaling system as mappings rather than a dictionary.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-dawkins1986the-blind-watch" class="csl-entry">
Dawkins, R. 1986. <em>The Blind Watchmaker</em>. Longman Scientific; Technical.
</div></div><p>Criticism:</p>
<ol type="1">
<li>The new game is not considered from a game theoretic point of view.
<ul>
<li>The extensive form should be made clear.</li>
<li>We don‚Äôt have just a new sender - we have seperate information sets for each sender. In RL the senders and the recievers all have different partial information corersponding to a highly localized view of the world.</li>
<li>Senders don‚Äôt see each others signals - as if they must signal at the same time.</li>
<li>We don‚Äôt know if it has similar equilibria as the lewis signaling game.</li>
<li>We don‚Äôt now if there are new kinds of equilibria.</li>
<li>We don‚Äôt know how they are distributed neumerically and this has bearing on the many tables shown in the paper. i.e.&nbsp;do the different empirical rates of success reflect what is expected in term of the relative numbers of equilibria in the game.</li>
</ul></li>
<li>The algorithms used in the paper do not learn to categorically exclude pooling equilibria. So they are slow to converge. Nor do they learn to exclude already solved state action pairs. This makes them slow to converge.</li>
<li>When the initial distribution of signals is not uniform the agents should do better not worse as they should learn a coding that is more efficient. This notion of coding is mentioned in the paper‚Äôs title but we don‚Äôt get a satisfying result. Perhaps there is a better way to learn efficient coding for a signaling system.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
My Research questions:
</div>
</div>
<div class="callout-body-container callout-body">
<p>Two approaches to the signaling system</p>
<ol start="2" type="1">
<li>sender and receiver create the system spontaneously.</li>
<li>sender envisions a signaling system, and receiver needs to learn it. There is just one correct signaling system.</li>
<li>there are many systems and agents coordinate via a beliefs to exclude non-optimal systems.
<ul>
<li>this is better because it can handle the case where two groups of agents have learned similar systems and the best common system should emerege.</li>
<li>case when the sender dies by predation and is replaced by a new sender.</li>
</ul></li>
<li>how do we learn to coordinate without pooling equilibria?
<ul>
<li>hint: excluding already solved state action pairs.</li>
</ul></li>
<li>how fast should a data efficient algorithm converge to a signaling system?
<ul>
<li>hint: like a coupon collector problem a sum of negative binomials gives <img src="https://latex.codecogs.com/png.latex?O(n%20log%20n)"></li>
</ul></li>
<li>how can do better?
<ul>
<li>as succsses are very rare early on using negative rewards for failure can speed up learning.</li>
<li>using multiple learners can further improve convergence.</li>
<li>using source coding should help if the states are not uniformly distributed</li>
</ul></li>
<li>how can we learn the most salient signaling system for an uneven distribution of states possibly changing over time?
<ul>
<li>hint: if the urn model is directly learning the mapping this may be hard</li>
<li>hint: consider an urn scheme that votes via a belief on the most salient signaling system.
<ul>
<li>this would let the agents switch between signaling systems as the distribution of states changes. What is the overhead?</li>
</ul></li>
</ul></li>
<li>The book by Skyrms suggests that different types of aggregations may lead to different signaling systems. He point out that conjunction is less powerful than concatenation.
<ul>
<li>I Found that implementing different aggregation schemes can be challenging - particularly if one has not read this paper! In fact it is hard enough to come up with variants for hard coded aggregation schemes for just the two cases above.
<ul>
<li>hint: using a FSM requries coordinating on some matrix of transitions.</li>
</ul></li>
<li>If one thinks of the desiderata for coding schemes - they should be easy to learn, easy to extend, easy to decode, and robust to errors. The easy to decode is also disrable here. I think that aggregation schemes are not very different from coding schemes once we have the ability to handle a sequence of signals.</li>
<li>Another complexity is that we could consider different settings like in RL. Here if we follow the book, we have
<ul>
<li>single sender single receiver (Lewis game)</li>
<li>multiple senders each with a disjoint partially state and a single receiver (leads to conjunctive aggregations to recover the state)</li>
<li>constrained binary signal and single receiver (full observability solves the sender decorolated coordination problem - that senders send different parts of the state)</li>
<li>multiple senders with a fully observed state</li>
</ul></li>
<li>In reality there is a whole zoo of aggregation schemes that are in use in natural languages.
<ul>
<li>template based aggregation for template based grammar and morphology (sequence of t signals)</li>
<li>tree based aggregation leading to recursive structures.</li>
<li>conjunctive aggregation for logical inference.
<ul>
<li>hint operators with noop symbol for closing implied bracket</li>
<li>sequence with nary operator + &gt; to close the context.
<ul>
<li>and ( A, B or C , D ) =&gt; and_n A or_n B C &gt; D &gt;</li>
</ul></li>
<li>encoding treess elegently using nary prefix operators
<ul>
<li>and ( A, B or C , D ) =&gt; and_3 A or2 B C D</li>
</ul></li>
<li>operators take arity as thier first argument like
<ul>
<li>and ( A, B or C , D ) =&gt; and 3 A or 2 B C D this can implement run length encoding too using</li>
<li>RLENC arity repetition input+ 0 1 2 3</li>
<li>and (aaabb aaabb aaabb) =&gt; RLENC 3 3 RLENC 3 3 a RLENC 3 2 b</li>
</ul></li>
</ul></li>
</ul></li>
<li>The issue here is that the aggregation scheme is not learned but fixed. If we were able to also learn an agreagation scheme we might understand how morphology, syntax and many other features of language emerge from a simple aggregation rule.</li>
<li>How can we learn the most efficient aggregation for a signaling system?</li>
<li>hint: using a template for the aggregation may be more efficient than learning the aggregation from scratch.</li>
</ul></li>
</ol>
</div>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {The {Evolution} of {Coding} in Signaling Games},
  date = {2024-06-10},
  url = {https://orenbochman.github.io/reviews/2007/evolution_of_coding/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúThe Evolution of Coding in Signaling
Games.‚Äù</span> June 10, 2024. <a href="https://orenbochman.github.io/reviews/2007/evolution_of_coding/">https://orenbochman.github.io/reviews/2007/evolution_of_coding/</a>.
</div></div></section></div> ]]></description>
  <category>review</category>
  <category>reinforcement learning</category>
  <category>signaling games</category>
  <category>complex signaling system</category>
  <guid>https://orenbochman.github.io/reviews/2007/evolution_of_coding/</guid>
  <pubDate>Sun, 09 Jun 2024 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2007/evolution_of_coding/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Ô∏èüëÆ Multi-agent Reinforcement Learning in Sequential Social Dilemmas</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2017/MARL-in-sequential-social-dilemmas/</link>
  <description><![CDATA[ 





<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Matrix games like Prisoner‚Äôs Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Qnetwork, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.‚Äù ‚Äì <span class="citation" data-cites="leibo2017multi">(Leibo et al. 2017)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Initially at least it seems to me that the ‚Äúsequential social dilemma‚Äù is just an analog of an iterated prisoner‚Äôs dilemma game. This game considered the basis of all other social dilemmas has been treated at great length in <span class="citation" data-cites="axelrod2009evolution">(Axelrod 2009)</span> and <span class="citation" data-cites="axelrod1997complexity">(Axelrod 1997)</span> with the famous Axelrod tournaments. Some interesting results include the fact that the best strategy in the tournament was the simplest. Population dynamics are such that in population of agents with one strategy, the introduction of new agents with a different strategy can gain a foothold and cause the dominant strategy in the population to switch to the new strategy. This is known as the ‚Äúevolution of cooperation‚Äù.</p>
<p>This work led to a growing interest in the study of cooperation in non-cooperative games with about 300 new papers published on the topic in the 10 years following the publication.</p>
<p>Later work looked at the robustness of the winning strategies in the Axelrod tournaments, see <span class="citation" data-cites="nowak2004emergence">(Nowak et al. 2004)</span>.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-nowak2004emergence" class="csl-entry">
Nowak, Martin A, Akira Sasaki, Christine Taylor, and Drew Fudenberg. 2004. <span>‚ÄúEmergence of Cooperation and Evolutionary Stability in Finite Populations.‚Äù</span> <em>Nature</em> 428 (6983): 646‚Äì50.
</div></div><div class="no-row-height column-margin column-container"><div id="ref-axelrod1997complexity" class="csl-entry">
Axelrod, R. 1997. <em>The Complexity of Cooperation: Agent-Based Models of Competition and Collaboration</em>. Princeton Studies in Complexity. Princeton University Press. <a href="https://books.google.co.il/books?id=J0dgRGMdjmQC">https://books.google.co.il/books?id=J0dgRGMdjmQC</a>.
</div></div><div class="no-row-height column-margin column-container"><div id="ref-axelrod2009evolution" class="csl-entry">
‚Äî‚Äî‚Äî. 2009. <em>The Evolution of Cooperation: Revised Edition</em>. Basic Books. <a href="https://books.google.co.il/books?id=SwU4DgAAQBAJ">https://books.google.co.il/books?id=SwU4DgAAQBAJ</a>.
</div></div></section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ol type="1">
<li><p>Introduction</p>
<ul>
<li><p>Discusses the tension in social dilemmas between individual rationality and collective rationality.</p></li>
<li><p>Presents three canonical examples of matrix game social dilemmas: Prisoner‚Äôs Dilemma, Chicken, and Stag Hunt.</p></li>
<li><p>Discusses limitations of the matrix game social dilemma (MGSD) formalism.</p></li>
<li><p>Proposes a Sequential Social Dilemma (SSD) model.</p></li>
</ul></li>
<li><p>Definitions and Notation</p>
<ul>
<li><p>Defines SSDs as general-sum Markov games where policies, rather than actions, are classified as cooperation or defection.</p></li>
<li><p>Presents formal definitions of Markov games and SSDs, including the concept of empirical payoff matrices.</p></li>
</ul></li>
<li><p>Learning Algorithms</p>
<ol start="3" type="1">
<li>Discusses previous work on multi-agent learning in Markov games, taking a descriptive rather than a prescriptive view.</li>
<li>Describes the use of deep Q-networks (DQN) for learning in SSDs, assuming independent learners that treat each other as part of the environment.</li>
</ol></li>
<li><p>Simulation Methods</p>
<ul>
<li><p>Describes the implementation of Gathering and Wolfpack games in a 2D grid-world environment.</p></li>
<li><p>Specifies state representation, observation function, action space, reward structure, episode length, and DQN architecture.</p></li>
</ul></li>
<li><p>Results</p>
<ul>
<li>Presents three experiments exploring the effects of environmental and agent parameters on emergent social outcomes.</li>
</ul>
<ol type="1">
<li>Experiment 1: Gathering
<ul>
<li><p>Describes the Gathering game, where players collect apples and can tag each other with a beam.</p></li>
<li><p>Discusses the influence of apple abundance (Napple) and conflict cost (Ntagged) on the emergence of aggressive (beam-use) behavior.</p></li>
<li><p>Shows that empirical game-theoretic analysis reveals Prisoner‚Äôs Dilemma-type payoffs in most cases where social dilemma conditions are met.</p></li>
</ul></li>
<li>Experiment 2: Wolfpack
<ul>
<li><p>Describes the Wolfpack game, where two wolves cooperate to hunt prey and receive higher rewards for joint captures.</p></li>
<li><p>Investigates the impact of group capture bonus (rteam/rlone) and capture radius on the level of cooperation (wolves per capture).</p></li>
<li><p>Demonstrates that empirical payoff matrices in Wolfpack can exhibit Chicken, Stag Hunt, and Prisoner‚Äôs Dilemma characteristics.</p></li>
</ul></li>
<li>Experiment 3: Agent Parameters Influencing the Emergence of Defection
<ul>
<li><p>Explores the effects of agent parameters (discount factor, batch size, and network size) on the emergence of defection in Gathering and Wolfpack.</p></li>
<li><p>Shows that agents with higher discount factors are more likely to defect, while larger batch sizes promote cooperation.</p></li>
<li><p>Highlights the opposite effects of network size on defection: increasing in Gathering and decreasing in Wolfpack, explained by the complexity of learning different policies.</p></li>
</ul></li>
</ol></li>
<li><p>Discussion</p>
<ul>
<li><p>Discusses the differences in learning complexity for cooperative and defecting policies in Gathering and Wolfpack.</p></li>
<li><p>Argues that SSD models provide a richer framework for understanding social dilemmas compared to MGSDs.</p></li>
<li><p>Proposes several important learning-related phenomena that are not captured by MGSD models.</p></li>
<li><p>Suggests broader applications of SSDs to study real-world social dilemmas, including policy interventions and resource sustainability.</p></li>
</ul></li>
</ol>
</section>
<section id="introduction" class="level1 page-columns page-full">
<h1>Introduction</h1>
<p>In <span class="citation" data-cites="leibo2017multi">(Leibo et al. 2017)</span>, the authors introduce a new class of social dilemmas, called sequential social dilemmas, which are inspired by the classic matrix game social dilemmas like the Prisoner‚Äôs Dilemma.</p>


<div class="no-row-height column-margin column-container"><div id="ref-leibo2017multi" class="csl-entry">
Leibo, Joel Z, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. 2017. <span>‚ÄúMulti-Agent Reinforcement Learning in Sequential Social Dilemmas.‚Äù</span> <em>arXiv Preprint arXiv:1702.03037</em>.
</div></div>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Ô∏èüëÆ {Multi-agent} {Reinforcement} {Learning} in {Sequential}
    {Social} {Dilemmas}},
  date = {2024-06-10},
  url = {https://orenbochman.github.io/reviews/2017/MARL-in-sequential-social-dilemmas/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúÔ∏èüëÆ Multi-Agent Reinforcement Learning in
Sequential Social Dilemmas.‚Äù</span> June 10, 2024. <a href="https://orenbochman.github.io/reviews/2017/MARL-in-sequential-social-dilemmas/">https://orenbochman.github.io/reviews/2017/MARL-in-sequential-social-dilemmas/</a>.
</div></div></section></div> ]]></description>
  <category>paper review</category>
  <category>multi-agent reinforcement learning</category>
  <category>sequential social dilemmas</category>
  <category>sequential social dilemmas</category>
  <category>cooperation</category>
  <category>Markov games</category>
  <category>agent-based social simulation</category>
  <category>non-cooperative games</category>
  <guid>https://orenbochman.github.io/reviews/2017/MARL-in-sequential-social-dilemmas/</guid>
  <pubDate>Sun, 09 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Generally Capable Agents Emerge from Open-Ended Play</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2021/generally-capable-agents-emerge-from-open-ended-play/</link>
  <description><![CDATA[ 





<section id="first-impressions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="first-impressions">First impressions</h2>
<p>The paper does not present a breakthrough like alpha go zero etc. But it shows very high level of creativity and innovation. I am still a new comer to RL and this paper has opened my eyes to how little of the field I have seen. There are lots of buzz words, references to other papers and concepts that I am not familiar with. Also this paper is visually stunning. The authors have put a lot of energy into creating an aesthetically pleasing project and they have gone to some length to explain what would otherwise might be a very challenging evaluation process.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/1yVoNAMLIy0" title="Max Jaderberg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<ol type="1">
<li>to what extent can agents learn to solve certain types of problems like solving a maze. I might call these learning tactical solutions</li>
<li>to what extent can agents compress this tactical knowledge into a heuristic that might end up as much more general.</li>
<li>to what extent can RL agents learn representations of the environment that allow them to reuse tactics and heuristics across different problems instead of having to discover them anew each time.</li>
<li>when sparse rewards or no rewards are given can agents learn use their capabilities to model the environment</li>
<li>generally capable agents should be able to handle the many different RL problem settings that are out there:</li>
</ol>
<ul>
<li>single state, multi-state, continuous state,</li>
<li>tabular, continuous,</li>
<li>finite state space, infinite state space,</li>
<li>episodic, continuing, i.e.&nbsp;finite horizon, infinite horizon,</li>
<li>single agent, multi-agent,</li>
<li>online, offline,</li>
<li>model based, model free,</li>
<li>known dynamics, unknown dynamics,</li>
<li>sparse rewards, dense rewards,</li>
<li>on-policy, off-policy,</li>
<li>discounted, undiscounted rewards,</li>
<li>single goal, multi-goal,</li>
<li>deterministic, stochastic,</li>
<li>stationary, non-stationary,</li>
<li>specific constraints, in reality there are also variations of these settings not all of these are dichotomies.</li>
</ul>
<ol start="6" type="1">
<li>The paper mentions priors work on social dilemmas - another dimension that seems to be related is how well can agents learn to solve simple game theoretic scenarios like the prisoner‚Äôs dilemma or colonel Blotto and then to transfer the knowledge to more complex games. The same idea might be applied to problems based in economic models.</li>
</ol>
<p>here are some of the concepts that I am not familiar with:</p>
<ul>
<li><a href="https://deepmind.google/discover/blog/population-based-training-of-neural-networks/">Population based training</a> a technique used to optimize a series of NN at the same time.
<ul>
<li>can this be useful in RL where an agent might need to learn multiple NNs to solve a problem.
<ul>
<li>the transition model P(s‚Äô | s, a) model</li>
<li>the reward model R(r | s, a) model</li>
<li>for representing the model (this is the four part dynamic function for <img src="https://latex.codecogs.com/png.latex?f(s',r%7Cs,a)"></li>
<li>value functions
<ul>
<li>the value function, <img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi_%7B%5Cstar%7D%20%7D%20(s)"></li>
<li>the action value function <img src="https://latex.codecogs.com/png.latex?q_%7B%5Cpi_%7B%5Cstar%7D%20%7D(s,a)"></li>
</ul></li>
<li>the advantage function <img src="https://latex.codecogs.com/png.latex?A_%7B%5Cpi_%7B%5Cstar%7D%20%7D(s,a)=%20Q_%7B%5Cpi_%7B%5Cstar%7D%20%7D(s,a)%20-%20V_%7B%5Cpi_%7B%5Cstar%7D%20%7D(s)"></li>
<li>the the policy <img src="https://latex.codecogs.com/png.latex?pi_%5Cstar%20(s)"></li>
</ul></li>
</ul></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Abr</th>
<th>Q-Fn<br>
<img src="https://latex.codecogs.com/png.latex?Q(s,a)"></th>
<th>V-Fn<br>
<img src="https://latex.codecogs.com/png.latex?V(s)"></th>
<th>Policy<br>
<img src="https://latex.codecogs.com/png.latex?%CF%80(a%E2%88%A3s)"></th>
<th>Advantage<br>
<img src="https://latex.codecogs.com/png.latex?A(s,a)"></th>
<th>Transitions<br>
<img src="https://latex.codecogs.com/png.latex?P(s%E2%80%B2%E2%88%A3s,a)"></th>
<th>Reward <img src="https://latex.codecogs.com/png.latex?R(s,a)"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Deep Q-Network<br>
<span class="citation" data-cites="Mnih2015HumanlevelCT">(Mnih et al. 2015)</span></td>
<td>(DQN)</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td><p>Double DQN</p>
<p><span class="citation" data-cites="Hasselt2015DeepRL">(Hasselt, Guez, and Silver 2015)</span></p></td>
<td>(DDQN)</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Dueling DQN<br>
<span class="citation" data-cites="Wang2015DuelingNA">(Wang et al. 2015)</span></td>
<td></td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td>Deep Deterministic Policy Gradients<br>
<span class="citation" data-cites="Lillicrap2015ContinuousCW">(Lillicrap et al. 2015)</span></td>
<td>(DDPG)</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td><p>Twin Delayed DDPG</p>
<p><span class="citation" data-cites="Fujimoto2018AddressingFA">(Fujimoto, Hoof, and Meger 2018)</span></p></td>
<td>(TD3)</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td><p>Soft Actor-Critic</p>
<p><span class="citation" data-cites="Haarnoja2018SoftAO">(Haarnoja et al. 2018)</span></p></td>
<td>(SAC)</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td><p>Proximal Policy Optimization</p>
<p><span class="citation" data-cites="Schulman2017ProximalPO">(Schulman et al. 2017)</span></p></td>
<td>(PPO)</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td><p>Trust Region Policy Optimization</p>
<p><span class="citation" data-cites="Schulman2015TrustRP">(Schulman et al. 2015)</span></p></td>
<td>(TRPO)</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Advantage Actor-Critic<br>
<span class="citation" data-cites="Mnih2016AsynchronousMF">(Mnih et al. 2016)</span></td>
<td>(A2C/A3C)</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td>Model-Based DQN<br>
<span class="citation" data-cites="Feinberg2018ModelBasedVE">(Feinberg et al. 2018)</span></td>
<td>(M-DQN)</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Model-Based PPO<br>
<span class="citation" data-cites="Clavera2018ModelBasedRL">(Clavera et al. 2018)</span></td>
<td>(M-PPO)</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><p>AlphaGo</p>
<p><span class="citation" data-cites="Silver2016MasteringTG">(Silver et al. 2016)</span></p></td>
<td></td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td><p>AlphaGo Zero</p>
<p><span class="citation" data-cites="Silver2017MasteringTG">(Silver et al. 2017)</span></p></td>
<td></td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><p>AlphaZero</p>
<p><span class="citation" data-cites="Silver2018AGR">(Silver et al. 2018)</span></p></td>
<td></td>
<td>No</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
</tr>
</tbody>
</table>
<div class="no-row-height column-margin column-container"><div id="ref-Mnih2015HumanlevelCT" class="csl-entry">
Mnih, Volodymyr, K. Kavukcuoglu, David Silver, Andrei A. Rusu, J. Veness, Marc G. Bellemare, Alex Graves, et al. 2015. <span>‚ÄúHuman-Level Control Through Deep Reinforcement Learning.‚Äù</span> <em>Nature</em> 518: 529‚Äì33.
</div><div id="ref-Hasselt2015DeepRL" class="csl-entry">
Hasselt, H. V., A. Guez, and David Silver. 2015. <span>‚ÄúDeep Reinforcement Learning with Double q-Learning,‚Äù</span> 2094‚Äì2100.
</div><div id="ref-Wang2015DuelingNA" class="csl-entry">
Wang, Ziyun, T. Schaul, Matteo Hessel, H. V. Hasselt, Marc Lanctot, and Nando de Freitas. 2015. <span>‚ÄúDueling Network Architectures for Deep Reinforcement Learning,‚Äù</span> 1995‚Äì2003.
</div><div id="ref-Lillicrap2015ContinuousCW" class="csl-entry">
Lillicrap, T., Jonathan J. Hunt, A. Pritzel, N. Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. <span>‚ÄúContinuous Control with Deep Reinforcement Learning.‚Äù</span> <em>CoRR</em> abs/1509.02971.
</div><div id="ref-Fujimoto2018AddressingFA" class="csl-entry">
Fujimoto, Scott, H. V. Hoof, and D. Meger. 2018. <span>‚ÄúAddressing Function Approximation Error in Actor-Critic Methods,‚Äù</span> 1582‚Äì91.
</div><div id="ref-Haarnoja2018SoftAO" class="csl-entry">
Haarnoja, Tuomas, Aurick Zhou, P. Abbeel, and S. Levine. 2018. <span>‚ÄúSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.‚Äù</span> <em>ArXiv</em> abs/1801.01290.
</div><div id="ref-Schulman2017ProximalPO" class="csl-entry">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>‚ÄúProximal Policy Optimization Algorithms.‚Äù</span> <em>ArXiv</em> abs/1707.06347.
</div><div id="ref-Schulman2015TrustRP" class="csl-entry">
Schulman, John, S. Levine, P. Abbeel, Michael I. Jordan, and Philipp Moritz. 2015. <span>‚ÄúTrust Region Policy Optimization.‚Äù</span> <em>ArXiv</em> abs/1502.05477.
</div><div id="ref-Mnih2016AsynchronousMF" class="csl-entry">
Mnih, Volodymyr, Adri√† Puigdom√®nech Badia, Mehdi Mirza, Alex Graves, T. Lillicrap, Tim Harley, David Silver, and K. Kavukcuoglu. 2016. <span>‚ÄúAsynchronous Methods for Deep Reinforcement Learning,‚Äù</span> 1928‚Äì37.
</div><div id="ref-Feinberg2018ModelBasedVE" class="csl-entry">
Feinberg, Vladimir, Alvin Wan, I. Stoica, Michael I. Jordan, Joseph E. Gonzalez, and S. Levine. 2018. <span>‚ÄúModel-Based Value Estimation for Efficient Model-Free Reinforcement Learning.‚Äù</span> <em>ArXiv</em> abs/1803.00101.
</div><div id="ref-Clavera2018ModelBasedRL" class="csl-entry">
Clavera, I., Jonas Rothfuss, John Schulman, Yasuhiro Fujita, T. Asfour, and P. Abbeel. 2018. <span>‚ÄúModel-Based Reinforcement Learning via Meta-Policy Optimization,‚Äù</span> 617‚Äì29.
</div><div id="ref-Silver2016MasteringTG" class="csl-entry">
Silver, David, Aja Huang, Chris J. Maddison, A. Guez, L. Sifre, George van den Driessche, Julian Schrittwieser, et al. 2016. <span>‚ÄúMastering the Game of Go with Deep Neural Networks and Tree Search.‚Äù</span> <em>Nature</em> 529: 484‚Äì89.
</div><div id="ref-Silver2017MasteringTG" class="csl-entry">
Silver, David, Julian Schrittwieser, K. Simonyan, Ioannis Antonoglou, Aja Huang, A. Guez, T. Hubert, et al. 2017. <span>‚ÄúMastering the Game of Go Without Human Knowledge.‚Äù</span> <em>Nature</em> 550: 354‚Äì59.
</div><div id="ref-Silver2018AGR" class="csl-entry">
Silver, David, T. Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, A. Guez, Marc Lanctot, et al. 2018. <span>‚ÄúA General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play.‚Äù</span> <em>Science</em> 362: 1140‚Äì44.
</div></div></section>
<section id="summary" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>The paper <span class="citation" data-cites="Team2021OpenEndedLL">(Team et al. 2021)</span> explores the idea that generally capable agents can emerge from open-ended play, similar to how human children learn and develop through play. The goal is to create agents that exhibit broad competencies and adaptability without being explicitly trained for specific tasks. In other words in the typical RL settings agents are trained to perform specific tasks and can learn solution to general problems. However they are very poor at generalizing these solutions to slightly different versions of the same problem. The authors seek to develop agents that can not only learn to solve a wide range of tasks but can also generalize and transfer their solutions to new problems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Team2021OpenEndedLL" class="csl-entry">
Team, Open-Ended Learning, Adam Stooke, Anuj Mahajan, C. Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, et al. 2021. <span>‚ÄúOpen-Ended Learning Leads to Generally Capable Agents.‚Äù</span> <em>ArXiv</em> abs/2107.12808.
</div><div id="ref-impala2018" class="csl-entry">
Espeholt, Lasse, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, et al. 2018. <span>‚ÄúIMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.‚Äù</span> In <em>Proceedings of the International Conference on Machine Learning (ICML)</em>.
</div><div id="ref-Hessel2018MultitaskDR" class="csl-entry">
Hessel, Matteo, Hubert Soyer, L. Espeholt, Wojciech M. Czarnecki, Simon Schmitt, and H. V. Hasselt. 2018. <span>‚ÄúMulti-Task Deep Reinforcement Learning with PopArt.‚Äù</span> <em>ArXiv</em> abs/1809.04474.
</div></div><p>This is not the first time that this idea has been explored. Prior work includes the development of agents that can learn to play a variety of video games without explicit training on each game. Other work like <span class="citation" data-cites="impala2018">(Espeholt et al. 2018)</span> and <span class="citation" data-cites="Hessel2018MultitaskDR">(Hessel et al. 2018)</span> at deep mind have already shown how this can be done. However, the authors of this paper take this idea further by developing agents that can learn to solve a wider range of tasks.</p>
<p>In this paper they develop environments that co-evolve with the agents. The environment increase in difficulty as the agents learn to solve them. The agents are equipped with intrinsic motivation mechanisms, such as curiosity and novelty-seeking behaviors, to drive exploration. A variety of tasks and challenges are presented dynamically, promoting continuous learning and adaptation.</p>
<p>But the real question is to what degree does this approach create agents that can learn to solve a wide range of tasks and can generalize their solutions to new problems. There seems to be three parts</p>
</section>
<section id="open-ended-learning" class="level2">
<h2 class="anchored" data-anchor-id="open-ended-learning">Open-Ended Learning:</h2>
<dl>
<dt>Open-Ended Learning</dt>
<dd>
<p>Open-ended learning refers to an unsupervised, exploratory process where agents interact with their environment without predefined goals. This approach contrasts with traditional reinforcement learning, which focuses on optimizing performance for specific tasks.</p>
</dd>
</dl>
<p>Have the the authors really provided environment with not predefined goals or just many many goals. There are games in game theory where the player</p>
<p>is given incomplete information - you don‚Äôt get told the reward or the rules and need to figure out an optimal strategy without them.</p>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology:</h2>
<p>The authors design an environment that encourages diverse interactions and challenges. Agents are equipped with intrinsic motivation mechanisms, such as curiosity and novelty-seeking behaviors, to drive exploration. A variety of tasks and challenges are presented dynamically, promoting continuous learning and adaptation.</p>
<p>As the authors point out - making a maze larger don‚Äôt necessarily make it more difficult once the agents have learned to solved a few mazes. The authors have to carefully design the environment to ensure that the agents are always challenged but not overwhelmed.</p>
<p>I have three criticism of the methodology:</p>
<ol type="1">
<li>Although this paper is about RL there is more than a fair share of evolutionary algorithms. It isn‚Äôt clear to what extent the agents are learning through RL and to what extent they are evolving through evolutionary algorithms. I don‚Äôt dislike this idea but it seems to muddy the waters regarding how well this research might be applied to create generally capable RL agents in the real world.</li>
<li>Some of the environment used in testing are hand crafted while the bulk of the environments are procedurally generated.</li>
<li>The claim about these these hand crafted test environments being unlike other environments that are procedurally generated is not very convincing.</li>
</ol>
<p>How is skill acquisition tracked?</p>
<p>Intrinsic Motivations are based on curiosity and novelty seeking behaviors. However I think that for some environment/problems intrinsic (motivation) could emerge from a dynamic of the environment itself. In some way this intrinsic motivation reflects the agents ability to model the environment and to predict the consequences of its actions.</p>
<p>For example in an agent can reproduce under some selection pressure it should acquire a relevant fitness intrinsic (expected progeny). If it needs to solve different mazes it should need an exploration intrinsic. If it needs to maximize harvesting of resources it should learn some utility function intrinsic. For a social dilemma it might learn some social utility function intrinsic. However in this case this is an intrinsic that need to be learned by all agents Even if all the agents learn it there is are possibility that the agents will not cooperate. We might look to game theory, mechanism design to see if agents can learn self encouraging mechanisms to cooperate and so on. Can they learn to signal or coordinate behavior to activate the social utility function intrinsic. Can they plan to change roles in sequential games with memory and without.</p>
<p>a environment itself. For example in the case of the maze the agent might be intrinsically motivated to explore the maze because it is the only way to find the reward. In this case the environment itself is providing the intrinsic motivation.</p>
<p>A more interesting approach would be to track the agents ability to solve a wide range of tasks and to generalize their solutions to new problems. This would be a more direct measure of the agents general capabilities.</p>
<p>The paper and website show how the internal state of the agent is visualized over the course of play. This seems to be a hearmap with different possible goals.</p>
</section>
<section id="results-and-findings" class="level2">
<h2 class="anchored" data-anchor-id="results-and-findings">Results and Findings:</h2>
<p>Agents developed through open-ended play demonstrate a wide range of capabilities, such as problem-solving, tool use, and social interaction. These agents outperform those trained with traditional task-specific reinforcement learning in terms of adaptability and generalization. Emergent behaviors and skills are observed, highlighting the potential of open-ended play in fostering general intelligence.</p>
</section>
<section id="implications-for-ai-development" class="level2">
<h2 class="anchored" data-anchor-id="implications-for-ai-development">Implications for AI Development:</h2>
<p>The findings suggest that fostering environments that encourage open-ended play can lead to the development of more robust and versatile AI agents. This approach could be pivotal in advancing AI towards general intelligence, where agents can perform well across a wide range of tasks without explicit training for each.</p>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions:</h2>
<p>Further research is needed to understand the mechanisms underlying the success of open-ended play. Scaling up the complexity of environments and intrinsic motivation systems could lead to even more capable agents. Exploring the integration of open-ended play with other AI paradigms might enhance the development of general AI.</p>
<!-- ![paper](https://arxiv.org/pdf/2107.12808){.col-page width="800px" height="1000px"} -->



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Generally {Capable} {Agents} {Emerge} from {Open-Ended}
    {Play}},
  date = {2024-06-10},
  url = {https://orenbochman.github.io/reviews/2021/generally-capable-agents-emerge-from-open-ended-play/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>‚ÄúGenerally Capable Agents Emerge from
Open-Ended Play.‚Äù</span> June 10, 2024. <a href="https://orenbochman.github.io/reviews/2021/generally-capable-agents-emerge-from-open-ended-play/">https://orenbochman.github.io/reviews/2021/generally-capable-agents-emerge-from-open-ended-play/</a>.
</div></div></section></div> ]]></description>
  <category>paper review</category>
  <category>multi-agent reinforcement learning</category>
  <category>sequential social dilemmas</category>
  <category>sequential social dilemmas</category>
  <category>cooperation</category>
  <category>Markov games</category>
  <category>agent-based social simulation</category>
  <category>non-cooperative games</category>
  <guid>https://orenbochman.github.io/reviews/2021/generally-capable-agents-emerge-from-open-ended-play/</guid>
  <pubDate>Sun, 09 Jun 2024 21:00:00 GMT</pubDate>
</item>
<item>
  <title>sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2015/sense2vec/</link>
  <description><![CDATA[ 





<p><strong>Sense2Vec</strong> <span class="citation" data-cites="trask2015sense2vecfastaccurate">(Trask, Michalak, and Liu 2015)</span> is an interesting deep learning model based on word2vec that can learn more interesting and detailed word vectors from large corpora. Sense2Vec embeddings are for word senses rather than for tokens.</p>
<div class="no-row-height column-margin column-container"></div><p>The shortcoming of word2vec is that it only learns one vector per word, which is not enough to capture the multiple meanings of a word. Sense2Vec addresses this issue by learning multiple embeddings for each word based on supervised disambiguation. This allows a consuming NLP model to select a sense-disambiguated embedding quickly and accurately.</p>
<p>I thought that the next step would be to cluster these embeddings by contexts and thus arrive at a wordsense version - however it turns out this was computationally expensive and difficult to apply in a scalable fashion. The idea of this paper is to use a supervised approach to disambiguate the senses of words. This means that the words need to be tagged using thier part-of-speech (POS) tags or named entity resolution. This approach is faster and more accurate than the clustering approach. However there is a limitation that the supervised labels are required and there can be multiple wordsenses within a signle POS tag.</p>
<p>Abstract</p>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or ‚Äúsenses‚Äù. Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8% average error reduction in unlabeled attachment scores across 6 languages.</p>
<p>‚Äî <span class="citation" data-cites="trask2015sense2vecfastaccurate">(Trask, Michalak, and Liu 2015)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>The paper <span class="citation" data-cites="trask2015sense2vecfastaccurate">(Trask, Michalak, and Liu 2015)</span> the autors introduced a novel method for word sense disambiguation in neural word embeddings. Traditional word embedding techniques such as Word2Vec represent each word by a single vector, regardless of its multiple meanings or senses, which often leads to ambiguities. This challenge is known as the ‚Äúsuperposition‚Äù problem, where multiple meanings of a word are combined into a single vector, potentially leading to suboptimal performance in downstream NLP tasks. SENSE2VEC addresses this limitation by generating multiple embeddings for each word, effectively disambiguating different senses.</p>
<div class="no-row-height column-margin column-container"><div id="ref-trask2015sense2vecfastaccurate" class="csl-entry">
Trask, Andrew, Phil Michalak, and John Liu. 2015. <span>‚ÄúSense2vec - a Fast and Accurate Method for Word Sense Disambiguation in Neural Word Embeddings.‚Äù</span> <a href="https://arxiv.org/abs/1511.06388">https://arxiv.org/abs/1511.06388</a>.
</div></div></section>
<section id="motivation-and-related-work" class="level2">
<h2 class="anchored" data-anchor-id="motivation-and-related-work">Motivation and Related Work</h2>
<p>The motivation behind SENSE2VEC stems from the shortcomings of earlier models like Word2Vec and Wang2Vec. Word2Vec, while highly successful, does not consider the order of words in a sentence and struggles with polysemy‚Äîthe phenomenon where a single word can have multiple meanings. Wang2Vec improves upon Word2Vec by incorporating word order, making it more suitable for syntactic tasks, yet still relies on single embeddings per word, making it less effective for handling polysemic words.</p>
<p>Other approaches, such as multi-prototype vector-space models by Reisinger and Mooney (2010), attempt to tackle polysemy by clustering the contexts in which a word appears and generating different embeddings for each cluster. However, these methods often require unsupervised clustering, making the process computationally expensive and difficult to apply in a scalable fashion. SENSE2VEC circumvents these challenges by leveraging supervised learning with part-of-speech (POS) tagging or named entity resolution, reducing the computational overhead while providing more accurate disambiguation.</p>
</section>
<section id="the-sense2vec-model" class="level2">
<h2 class="anchored" data-anchor-id="the-sense2vec-model">The SENSE2VEC Model</h2>
<p>The core innovation of SENSE2VEC lies in its use of supervised labels, such as POS tags, to disambiguate the senses of words. Unlike previous models that rely on unsupervised clustering methods, SENSE2VEC uses these labels to generate separate embeddings for each sense of a word. For example, the word ‚Äúbank‚Äù can have distinct embeddings for its noun and verb forms.</p>
<p>The model can be trained using traditional methods like Continuous Bag of Words (CBOW) or Skip-gram, with a key difference: instead of predicting words given surrounding words, SENSE2VEC predicts word senses given surrounding senses. This approach leads to more accurate representations of polysemic words in context, reducing the negative impact of superposition.</p>
</section>
<section id="evaluation-and-results" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-and-results">Evaluation and Results</h2>
<p>The authors evaluated SENSE2VEC on various NLP tasks, including dependency parsing and named entity resolution (NER). Notably, the disambiguation achieved by SENSE2VEC led to significant improvements in performance. For example, in dependency parsing across six languages (Bulgarian, German, English, French, Italian, and Swedish), SENSE2VEC embeddings resulted in an average error reduction of over 8% compared to the Wang2Vec baseline.</p>
<p>The model also demonstrated its effectiveness in handling sentiment disambiguation, as shown in Table 5 of the paper, where the word ‚Äúbad‚Äù was successfully disambiguated into a negative and positive sentiment sense. The positive sense captured sarcastic uses of ‚Äúbad,‚Äù which is often interpreted as ‚Äúgood‚Äù in informal language, while the negative sense retained the more classical meaning of ‚Äúbad.‚Äù</p>
</section>
<section id="strengths-and-contributions" class="level2">
<h2 class="anchored" data-anchor-id="strengths-and-contributions">Strengths and Contributions</h2>
<p>SENSE2VEC‚Äôs strengths lie in its efficiency and accuracy. By utilizing supervised labels, the model eliminates the need for expensive clustering algorithms, making it both faster and easier to scale. The ability to disambiguate nuanced senses of words, such as sentiment and named entities, showcases the flexibility and robustness of the approach.</p>
<p>Additionally, the model‚Äôs performance improvements in downstream tasks like dependency parsing and NER demonstrate its practical applicability in real-world NLP systems. The fact that SENSE2VEC outperforms earlier models like Wang2Vec and other clustering-based approaches highlights its contribution to the field of word sense disambiguation.</p>
</section>
<section id="limitations-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-future-work">Limitations and Future Work</h2>
<p>One potential limitation of SENSE2VEC is its reliance on labeled data. While supervised learning offers many advantages in terms of accuracy, it also introduces a dependency on the availability of high-quality labels. For languages or domains where such labels are scarce, applying SENSE2VEC may be more challenging.</p>
<p>The authors acknowledge this limitation and suggest that future work could explore the use of other types of supervised labels or investigate ways to combine both supervised and unsupervised methods to further enhance word sense disambiguation.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Overall, SENSE2VEC presents a compelling and efficient solution to the problem of word sense disambiguation in neural word embeddings. By leveraging supervised NLP labels, the model significantly improves the accuracy of embeddings for polysemic words, leading to better performance in NLP tasks like dependency parsing and NER. Its contribution to the field is clear, and it paves the way for future advancements in sense-disambiguated word embeddings.</p>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also:</h2>
<ul>
<li><a href="https://arxiv.org/abs/1511.06388">paper</a></li>
<li><a href="https://github.com/explosion/sense2vec">code</a> by Explosion AI</li>
<li><a href="https://demos.explosion.ai/sense2vec">demo</a></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sense2vec <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Sense2Vec</span>
<span id="cb1-2"></span>
<span id="cb1-3">s2v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Sense2Vec().from_disk(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/path/to/s2v_reddit_2015_md"</span>)</span>
<span id="cb1-4">query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"natural_language_processing|NOUN"</span></span>
<span id="cb1-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> query <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> s2v</span>
<span id="cb1-6">vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v[query]</span>
<span id="cb1-7">freq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v.get_freq(query)</span>
<span id="cb1-8">most_similar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v.most_similar(query, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [('machine_learning|NOUN', 0.8986967),</span></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  ('computer_vision|NOUN', 0.8636297),</span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  ('deep_learning|NOUN', 0.8573361)]</span></span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sense2vec <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Sense2Vec</span>
<span id="cb2-2">s2v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Sense2Vec().from_disk(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./s2v_reddit_2015_md"</span>)</span>
<span id="cb2-3">vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"natural_language_processing|NOUN"</span>]</span>
<span id="cb2-4">most_similar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v.most_similar(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"duck|VERB"</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb2-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(most_similar)</span></code></pre></div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Sense2vec - {A} {Fast} and {Accurate} {Method} for {Word}
    {Sense} {Disambiguation} {In} {Neural} {Word} {Embeddings}},
  date = {2022-06-26},
  url = {https://orenbochman.github.io/reviews/2015/sense2vec/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>‚ÄúSense2vec - A Fast and Accurate Method for
Word Sense Disambiguation In Neural Word Embeddings.‚Äù</span> June 26,
2022. <a href="https://orenbochman.github.io/reviews/2015/sense2vec/">https://orenbochman.github.io/reviews/2015/sense2vec/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2015/sense2vec/</guid>
  <pubDate>Sat, 25 Jun 2022 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Variational Inference with Normalizing Flows</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="figure 1"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/fig_1.png" class="img-fluid figure-img" width="800" alt="figure 1"></a></p>
<figcaption>figure 1</figcaption>
</figure>
</div>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.</p>
<p>‚Äî <span class="citation" data-cites="rezende2016vinflows">(Rezende and Mohamed 2016)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-rezende2016vinflows" class="csl-entry">
Rezende, Danilo Jimenez, and Shakir Mohamed. 2016. <span>‚ÄúVariational Inference with Normalizing Flows.‚Äù</span> <a href="https://arxiv.org/abs/1505.05770">https://arxiv.org/abs/1505.05770</a>.
</div></div></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig_2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="figure 2"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/fig_2.png" class="col-page img-fluid figure-img" width="800" alt="figure 2"></a></p>
<figcaption>figure 2</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./alg_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="algorithm 1"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/alg_1.png" class="col-page img-fluid figure-img" width="800" alt="algorithm 1"></a></p>
<figcaption>algorithm 1</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig_3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="figure 3"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/fig_3.png" class="col-page img-fluid figure-img" width="800" alt="figure 3"></a></p>
<figcaption>figure 3</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./table_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="table 1"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/table_1.png" class="col-page img-fluid figure-img" width="800" alt="table 1"></a></p>
<figcaption>table 1</figcaption>
</figure>
</div>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Variational {Inference} with {Normalizing} {Flows}},
  date = {2022-06-26},
  url = {https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>‚ÄúVariational Inference with Normalizing
Flows.‚Äù</span> June 26, 2022. <a href="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/">https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/</a>.
</div></div></section></div> ]]></description>
  <category>variational inference</category>
  <category>normalizing flows</category>
  <category>probabilistic modeling</category>
  <category>stub</category>
  <guid>https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/</guid>
  <pubDate>Sat, 25 Jun 2022 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/1989/skeletonization/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>This Nips 1988 paper is about simplifying neural networks by removing redundant units. The authors‚Äô approach is systematically identifying and removing redundant or less-relevant units without losing functionality.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="figure 1"><img src="https://orenbochman.github.io/reviews/1989/skeletonization/fig_1.png" class="img-fluid figure-img" width="800" alt="figure 1"></a></p>
<figcaption>figure 1</figcaption>
</figure>
</div>
</section>
<section id="summary" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>In <span class="citation" data-cites="mozer1988skeletonization">(Mozer and Smolensky 1988)</span> the authors presents a novel approach to simplifying neural networks by systematically identifying and removing redundant or less-relevant units. The authors address a key challenge in connectionist models: understanding and optimizing the network‚Äôs behavior by trimming unnecessary components without losing functionality.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mozer1988skeletonization" class="csl-entry">
Mozer, Michael C, and Paul Smolensky. 1988. <span>‚ÄúSkeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment.‚Äù</span> <em>Advances in Neural Information Processing Systems</em> 1.
</div></div><p>The core idea of the paper is to iteratively train a network, compute a relevance metric for each input or hidden unit, and eliminate the units that are least relevant to the network‚Äôs performance. The skeletonization process offers several practical benefits:</p>
<ul>
<li><p><strong>Constraining Generalization</strong>: By reducing redundant units, skeletonization effectively limits the network‚Äôs complexity, which can enhance its generalization capability.</p></li>
<li><p><strong>Improving Learning Performance</strong>: Networks often require many hidden units to quickly learn a dataset. Skeletonization accelerates initial learning with excess units and trims unnecessary ones later, leading to better generalization without sacrificing learning speed.</p></li>
<li><p><strong>Simplifying Network Behavior</strong>: The authors argue that skeletonization can transform complex networks into simpler models, effectively capturing core decision-making rules.</p></li>
</ul>
<p>The technique contrasts with other approaches such as weight decay methods by opting for an all-or-none removal of units, motivated by the desire to identify the most critical components through explicit relevance metrics. These metrics are determined by computing the network‚Äôs error with and without specific units, using a time-averaged relevance estimate.</p>
</section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<ol type="1">
<li><p><strong>Relevance Metric and Error Propagation</strong>: Mozer and Smolensky introduce a <mark>relevance measure derived from the network‚Äôs error response to the removal of a unit</mark>. The metric can be computed via an error propagation procedure similar to backpropagation, making the approach computationally feasible for larger networks.</p></li>
<li><p><strong>Practical Application in Simple Examples</strong>:</p>
<ul>
<li>In the <strong>cue salience problem</strong>, the relevance metric highlights the most critical input unit, effectively eliminating irrelevant units.</li>
<li>In the <strong>rule-plus-exception problem</strong>, the method identifies the hidden unit responsible for most general cases, while distinguishing another unit that handles exceptions, reflecting the nuanced task structure.</li>
<li>The <strong>train problem</strong> demonstrates the technique‚Äôs ability to reduce inputs to a minimal set of features needed to differentiate categories.</li>
</ul></li>
<li><p><strong>Skeletonization in More Complex Tasks</strong>: Mozer and Smolensky apply skeletonization to more sophisticated problems like the <strong>four-bit multiplexor</strong> and the random mapping problem, showing that skeletonized networks can outperform standard ones in terms of both failure rate and learning speed.</p></li>
</ol>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Effective Reduction of Network Size</strong>: One of the most impressive outcomes is the ability of skeletonized networks to match or exceed the performance of larger networks with fewer units. The networks show resilience in maintaining their functionality even as units are removed.</p></li>
<li><p><strong>Improvement in Learning Time</strong>: The authors provide evidence that learning with an initially large network and then trimming it can lead to faster convergence than training a smaller network from the start. This result challenges conventional thinking that fewer hidden units should always be preferable from the outset.</p></li>
<li><p><strong>Rule Abstraction</strong>: The skeletonization process successfully identifies essential ‚Äúrules‚Äù that govern network behavior, making it easier to interpret a network‚Äôs decisions in a simplified and concise manner.</p></li>
</ul>
</section>
<section id="limitations-and-open-questions" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-open-questions">Limitations and Open Questions</h2>
<ul>
<li><p><strong>Predefined Trimming Limit</strong>: One limitation of the paper‚Äôs approach is the need to predefine how much of the network to trim. While the authors acknowledge that magnitudes of relevance values may offer insights, an automatic stopping criterion based on these values is not fully explored.</p></li>
<li><p><strong>Error Function Sensitivity</strong>: The paper highlights an issue with using quadratic error functions to compute relevance. The authors propose an alternative linear error function, but the sensitivity of results to different error metrics could benefit from further investigation.</p></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Mozer and Smolensky‚Äôs skeletonization technique represents a significant step toward optimizing neural networks by removing redundant units without losing core functionality. The method not only improves learning performance but also offers valuable insights into the internal workings of neural networks. By focusing on relevance metrics, the authors pave the way for more interpretable, efficient, and generalized neural models.</p>
<p>Overall, skeletonization remains an influential contribution to the study of neural network optimization, providing both theoretical insights and practical improvements in learning systems.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Skeletonization: {A} {Technique} for {Trimming} the {Fat}
    from a {Network} via {Relevance} {Assessment}},
  date = {2022-06-22},
  url = {https://orenbochman.github.io/reviews/1989/skeletonization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>‚ÄúSkeletonization: A Technique for Trimming
the Fat from a Network via Relevance Assessment.‚Äù</span> June 22, 2022.
<a href="https://orenbochman.github.io/reviews/1989/skeletonization/">https://orenbochman.github.io/reviews/1989/skeletonization/</a>.
</div></div></section></div> ]]></description>
  <category>neural networks</category>
  <category>pruning</category>
  <guid>https://orenbochman.github.io/reviews/1989/skeletonization/</guid>
  <pubDate>Tue, 21 Jun 2022 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/1989/skeletonization/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Simplifying Neural Networks by soft weight sharing</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/</link>
  <description><![CDATA[ 





<p>This paper was mentioned in Geoffrey Hinton‚Äôs <a href="https://www.coursera.org/learn/neural-networks-deep-learning/">Coursera course</a> as a way to simplify neural networks.</p>
<p>The main takeaway is that of modeling the loss using a mixture of Gaussians to cluster the weights and penalize the complexity of the model.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
To Dos
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><label><input type="checkbox">add categories</label></li>
<li><label><input type="checkbox">add a citation for the course.</label></li>
<li><label><input type="checkbox">It would be worth-while to look through what he says about this paper if only to ensure the main result is made to stand out from some of the others.</label></li>
<li><label><input type="checkbox">Do a from scratch implementation of the paper</label></li>
<li><label><input type="checkbox">Do a Python implementation of the paper.</label></li>
<li><label><input type="checkbox">Why isn‚Äôt this technique easier to implement in practice?</label></li>
</ol>
</div>
</div>
</div>
<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>The primary aim of the paper <span class="citation" data-cites="nowlan2018simplifying">(Nowlan and Hinton 1992)</span> is reducing the complexity of neural networks by employing a mixture of Gaussian priors to the weights, creating a ‚Äúsoft‚Äù weight-sharing mechanism. Instead of simply penalizing large weights (as in L2 regularization), this method clusters the weights, allowing some to stay close to zero and others to remain non-zero, depending on their usefulness. Soft weight sharing along with weight decay, improving generalization and making the model more interpretable.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p><mark>One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity</mark>. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple Gaussians. <mark>A set of weights is <em>simple</em> if the weights have high probability density under the mixture model</mark>. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms</p>
<p>‚Äì <span class="citation" data-cites="nowlan2018simplifying">(Nowlan and Hinton 1992)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Weight clustering - a perplexing idea &amp; Other quandaries
</div>
</div>
<div class="callout-body-container callout-body">
<p>This notion of clustering weights is odd to say the least as these are just numbers in a data structure. Viewed as a method to reduce the effective number of parameters in the model, it makes some convoluted sense. What this idea seems to boil down to is that we are prioritizing neural net architectures with some abstract symmetry in the weights and thus a lower capacity and thus less prone to overfitting.</p>
<ul>
<li>We shall shall soon see that the authors have attempted to motivate this idea in at least two ways:
<ol type="1">
<li><strong>Weight decay</strong> - the penalty is a function of the weights themselves based on <span class="citation" data-cites="plaut1986experiments">(Plaut, Nowlan, and Hinton 1986)</span></li>
<li><strong>A Bayesian perspective</strong> - is a negative log density of the weights under a Gaussian prior.</li>
</ol></li>
<li>it might also help if we learned that mixture models are often used to do clustering in unsupervised learning.</li>
</ul>
<p>A few quandaries then arise:</p>
<ol type="1">
<li>How can we figure for different layers having weights, gradients and learning rates being more correlated then between layers.</li>
<li>That there may be other structure so that the weights are not independent of each other.
<ol type="1">
<li>In classifiers the are continuous approximation of logic gates.</li>
<li>In regression settings their values approximate continuous variables ?</li>
</ol></li>
<li>In many networks most of the weights are in the last layer, so we can use a different penalty for the last layer.</li>
<li>Is there a way to impose an abstract symmetry on the weights of a neural network such that is commensurate with the problem?</li>
<li>Can we impose multiple such symmetries on the network to give it other advantages?
<ul>
<li>Invariance to certain transformations,</li>
<li>using it for initialization,</li>
<li>making the model more interpretable,</li>
<li>Once we learn these mixture distribution of weights, can we use its parameters in, batch normalization, layer norm and with other regularization techniques like dropout?</li>
</ul></li>
</ol>
</div>
</div>
<div class="no-row-height column-margin column-container"></div></section>
<section id="the-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-problem">The problem:</h2>
<p>This main problem in this paper is that of supervised ML</p>
<blockquote class="blockquote">
<p>How to train a model so it will generalize well on unseen data?</p>
</blockquote>
<p>In deep learning this problem is exacerbated by the fact that neural networks require fitting lots of parameters while the data for training is limited. This naturally leads to overfitting - memorizing the data and noise rather than learning the underlying data generating process.</p>
</section>
<section id="related-work" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="related-work">Related work</h2>
<p>How have others tried to solve this problem?</p>
<ul>
<li><strong>Weight Sharing</strong>: Reducing effective numbers of <em>free</em> parameters in a neural network by using a prior distribution over the weights. This is equivalent to adding a penalty term to the loss function. c.f. <span class="citation" data-cites="lang1990time">(Lang, Waibel, and Hinton 1990)</span></li>
<li><strong>Early Stopping</strong>: Does not restricts the parameters instead detecting the point when training and test scores diverge and stopping the training early. In reality weights are saved after each epoch and once we are certain that a divergence in say accuracy we restore the earlier weights c.f. <span class="citation" data-cites="morgan1989generalization">(Morgan and Bourlard 1989)</span>, <span class="citation" data-cites="weigend1990predicting">(Weigend, Huberman, and Rumelhart 1990)</span>.</li>
<li><strong>Pruning</strong>: Removing weight from the network <span class="citation" data-cites="mozer1989using">(Mozer and Smolensky 1989)</span>,
<ol type="1">
<li>keep track of the importance of each unit and drop the least important ones - could work well in RL where we keep track of the importance of each state/action/features we might also care more about prioritizing certain states and discarding others. c.f. <span class="citation" data-cites="mozer1989using">(Mozer and Smolensky 1989)</span></li>
<li>Use second order gradient information to estimate network sensitivity to weight changes and prune based on that. c.f. <span class="citation" data-cites="lecun-90b">(LeCun et al. 1990)</span></li>
</ol></li>
<li><strong>Penalty Term</strong>: Adding a term in the loss penelizing for the network‚Äôs complexity. c.f. <span class="citation" data-cites="mozer1989using">(Mozer and Smolensky 1989)</span> <span class="citation" data-cites="lecun-90b">(LeCun et al. 1990)</span><sup>1</sup>.
<ol type="1">
<li>Complexity can be approximated using sum of the squares of the weights.</li>
<li>Differentiating the sum of the squares of the weights leads to weight decay. <span class="citation" data-cites="plaut1986experiments">(Plaut, Nowlan, and Hinton 1986)</span></li>
</ol></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-lang1990time" class="csl-entry">
Lang, Kevin J, Alex H Waibel, and Geoffrey E Hinton. 1990. <span>‚ÄúA Time-Delay Neural Network Architecture for Isolated Word Recognition.‚Äù</span> <em>Neural Networks</em> 3 (1): 23‚Äì43.
</div><div id="ref-morgan1989generalization" class="csl-entry">
Morgan, Nelson, and Herv√© Bourlard. 1989. <span>‚ÄúGeneralization and Parameter Estimation in Feedforward Nets: Some Experiments.‚Äù</span> <em>Advances in Neural Information Processing Systems</em> 2.
</div><div id="ref-weigend1990predicting" class="csl-entry">
Weigend, Andreas S, Bernardo A Huberman, and David E Rumelhart. 1990. <span>‚ÄúPredicting the Future: A Connectionist Approach.‚Äù</span> <em>International Journal of Neural Systems</em> 1 (03): 193‚Äì209.
</div><div id="ref-mozer1989using" class="csl-entry">
Mozer, Michael C, and Paul Smolensky. 1989. <span>‚ÄúUsing Relevance to Reduce Network Size Automatically.‚Äù</span> <em>Connection Science</em> 1 (1): 3‚Äì16.
</div><div id="ref-lecun-90b" class="csl-entry">
LeCun, Yann, J. S. Denker, S. Solla, R. E. Howard, and L. D. Jackel. 1990. <span>‚ÄúOptimal Brain Damage.‚Äù</span> In <em>Advances in Neural Information Processing Systems (NIPS 1989)</em>, edited by David Touretzky. Vol. 2. Denver, CO: Morgan Kaufman. <a href="https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;in the paper this citation is ambiguous, but I think this is the correct one - based on the abstract</p></div><div id="ref-nowlan2018simplifying" class="csl-entry">
Nowlan, Steven J., and Geoffrey E. Hinton. 1992. <span>‚Äú<span class="nocase">Simplifying Neural Networks by Soft Weight-Sharing</span>.‚Äù</span> <em>Neural Computation</em> 4 (4): 473‚Äì93. <a href="https://doi.org/10.1162/neco.1992.4.4.473">https://doi.org/10.1162/neco.1992.4.4.473</a>.
</div></div><p>Just a few ideas from this paper: <span class="citation" data-cites="nowlan2018simplifying">(Nowlan and Hinton 1992)</span></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bcost%7D%20=%20%5Ctext%7Bdata-misfit%7D%20+%20%5Clambda%20%5Ctimes%20%5Ctext%7Bcomplexity%7D%20%5Cqquad%0A"></p>
<p>penalties:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi%7D%20w_i%5E2%20%5Cqquad%20%5Ctext%7B(L2%20penalty)%7D%0A"></p>
<p>the authors provide two ways to think about this penalty:</p>
<ol type="1">
<li><strong>Weight decay</strong> - the penalty is a function of the weights themselves based on <span class="citation" data-cites="plaut1986experiments">(Plaut, Nowlan, and Hinton 1986)</span></li>
<li><strong>a Bayesian perspective</strong> - is a negative log density of the weights under a Gaussian prior.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-plaut1986experiments" class="csl-entry">
Plaut, D. C., S. J. Nowlan, and G. E. Hinton. 1986. <span>‚ÄúExperiments on Learning by Back-Propagation.‚Äù</span> CMU-CS-86-126. Pittsburgh, PA: Carnegie‚ÄìMellon University. <a href="https://ni.cmu.edu/~plaut/papers/pdf/PlautNowlanHinton86TR.backprop.pdf">https://ni.cmu.edu/~plaut/papers/pdf/PlautNowlanHinton86TR.backprop.pdf</a>.
</div></div><p>The authors point out that the problem with L2 penalty term is that it prefers two weak interactions over a strong one.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft(%5Cfrac%7Bw%7D%7B2%7D%5Cright)%5E2+%5Cleft(%5Cfrac%7Bw%7D%7B2%7D%5Cright)%5E2%20%3C%20w%5E2%20+%200%20%5Cqquad%0A"></p>
<p>we want to keep larger weights and drop the smaller ones</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(w)%20=%20%5Cpi_n%20%5Cfrac%7B1%7D%7B2%20%5Cpi%5Csigma_n%7De%5E%7B-w%5E2/2%5Csigma%5E2_n%7D%20+%20%5Cpi_b%20%5Cfrac%7B1%7D%7B2%20%5Cpi%5Csigma_b%7De%5E%7B-w%5E2/2%5Csigma%5E2_b%7D%20%5Cqquad%0A"></p>
<p>This is equivalent to the L2 penalty. <span class="citation" data-cites="mackay1991bayesian">(MacKay 1991)</span> showed that we can do better with:</p>
<div class="no-row-height column-margin column-container"><div id="ref-mackay1991bayesian" class="csl-entry">
MacKay, David JC. 1991. <span>‚ÄúBayesian Modeling and Neural Networks.‚Äù</span> <em>PhD Thesis, Dept. Of Computation and Neural Systems, CalTech</em>. <a href="https://www.inference.org.uk/mackay/thesis.pdf">https://www.inference.org.uk/mackay/thesis.pdf</a>.
</div></div><p><img src="https://latex.codecogs.com/png.latex?%0Ap(w)%20=%5Csum_i%20%5Clambda_i%20%5Csum_%7Bj%7D%20w_j%5E2%20%5Cqquad%20%5Ctext%7B(L2%20penalty)%7D%0A"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?i"> is the layer and</li>
<li><img src="https://latex.codecogs.com/png.latex?j"> is the weight in that layer</li>
</ul>
<p>Different layers has a different penalty, which is equivalent to a Gaussian prior with different variances for each layer.<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;note: this answers the first of my question above.</p></div></div><p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi%7D%20%5Cdelta(w%7B_i%5Cne%200%7D)%20%5Cqquad%20%5Ctext%7B(L0%20penalty)%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(w)%20=%20%5Csum_%7Bi%7D%20%5Cdelta(w_i%20%5Cne%200)%20+%20%5Clambda%20%5Csum_%7Bi%7D%20w_i%5E2%0A"></p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li>Article <a href="https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/">using weight constraints to reduce generalization</a></li>
<li>The paper is available at https://www.cs.utoronto.ca/~hinton/absps/sunspots.pdf</li>
</ul>
</section>
<section id="an-after-thought" class="level2">
<h2 class="anchored" data-anchor-id="an-after-thought">An after thought</h2>
<p>Can we use a Bayesian RL to tune the hyper-parameters of model and dataset. We can perhaps create an RL alg that controls the many aspects of training of a model. It can explore/exploit different setups on subsets of the data. Find variants that converge faster and are more robust by adding constraints at different levels. It can identify problems in the datasets (possible bad labels etc) . Ensambles, mixtures of experts, different regularization strategies. Different Learning rates and schedules globaly or per layer.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Simplifying {Neural} {Networks} by Soft Weight Sharing},
  date = {2022-06-22},
  url = {https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>‚ÄúSimplifying Neural Networks by Soft Weight
Sharing.‚Äù</span> June 22, 2022. <a href="https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/">https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/</a>.
</div></div></section></div> ]]></description>
  <category>neural networks</category>
  <category>regularization</category>
  <category>stub</category>
  <guid>https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/</guid>
  <pubDate>Tue, 21 Jun 2022 21:00:00 GMT</pubDate>
</item>
<item>
  <title>VGGNet: Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2014/VGGNet/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In this paper <span class="citation" data-cites="simonyan2015deepconvolutionalnetworkslargescale">(Simonyan and Zisserman 2015)</span> the authors, Karen Simonyan and Andrew Zisserman from the Visual Geometry Group at Oxford, investigated the effect of increasing the convolutional network depth on the accuracy in the large-scale image recognition setting. The authors show that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers.</p>
<div class="no-row-height column-margin column-container"></div><ul>
<li><p>By using 3x3 convolution filters with stride of 1 instead of larger ones like 5x5 or 7x7 the authors were able to reduce the number of parameters in the network which allowed them to use deeper networks (16-19) layers with a similar capacity to earlier networks. This is possible as stack of three 3x3 convolutional layers has an effective receptive field of 7x7 with 81% fewer parameters than a single 7x7 convolutional layer. Once this was understood 3x3 became the standard convolutional filter size in modern CNN architectures.</p></li>
<li><p>The authors introduced a data augmentation method called ‚Äòimage jittering‚Äô which varying image scales.</p></li>
<li><p>The authors later tweaked their model further including using 1x1 convolutional layers and Local Response Normalization (LRN) which improved the performance of the model as well as Xaiver initialization. And they were able to achieve state-of-the-art results on the ImageNet dataset.</p></li>
<li><p>The authors released weights for <strong>VGG16</strong> and <strong>VGG19</strong> Called D and E in the table below which were the basis of their ImageNet Challenge 2014 submission. And it is is these two models that are most commonly used in practice as thier weight are available in the Keras library <span class="citation" data-cites="chollet2015keras">(Chollet et al. 2015)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-chollet2015keras" class="csl-entry">
Chollet, Fran√ßois et al. 2015. <span>‚ÄúKeras.‚Äù</span> <a href="https://keras.io" class="uri">https://keras.io</a>.
</div></div></section>
<section id="the-abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-abstract">The abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>In this work <mark>we investigate the effect of the convolutional network depth on its accuracy</mark> in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a <mark>significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers</mark>. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. ‚Äî <span class="citation" data-cites="simonyan2015deepconvolutionalnetworkslargescale">(Simonyan and Zisserman 2015)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-simonyan2015deepconvolutionalnetworkslargescale" class="csl-entry">
Simonyan, Karen, and Andrew Zisserman. 2015. <span>‚ÄúVery Deep Convolutional Networks for Large-Scale Image Recognition.‚Äù</span> <a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a>.
</div></div></div>
</section>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>The paper has a table with some network architectures and their performance on the ImageNet dataset. In many cases data scientist etc. like to copy the architectures of well known models and use them in their own work. So this paper is a good reference for giving a few more options for architectures to use.</p>
<p>The paper uses 3x3 convolution filters which is a common practice in modern CNN architectures.</p>
<blockquote class="blockquote">
<p>We use very small 3 √ó 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 √ó 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 √ó 5; three such layers have a 7 √ó 7 effective receptive field. <mark>So what have we gained by using, for instance, a stack of three 3 √ó 3 conv. layers instead of a single 7 √ó 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters</mark>: assuming that both the input and the output of a three-layer 3 √ó 3 convolution stack has C channels, the stack is parametrised by 3 (32C2) = 27C^2 weights; at the same time, a single 7 √ó 7 conv. layer would require 72C2 = 49C2 parameters, i.e.&nbsp;81% more. This can be seen as imposing a regularisation on the 7 √ó 7 conv. filters, forcing them to have a decomposition through the 3 √ó 3 filters (with non-linearity injected in between).</p>
</blockquote>
<p>The authors also reference 1 √ó 1 convolutions from [NiN] paper which also have large FC layers at the end.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./table1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="architecture"><img src="https://orenbochman.github.io/reviews/2014/VGGNet/table1.png" class="img-fluid figure-img" width="800" alt="architecture"></a></p>
<figcaption>architecture</figcaption>
</figure>
</div>
<p>Where: - A is 11 layered. - A-LRN is 11 layered but have Local Response Normalization. - B is 13 layered. - C is 16 layered but has 1x1 convolutional layers. - D is 16 layered but 1x1 convolutional layers in C are replaced with 3x3 convolutional layers. - E is 19 layered</p>
<p>Training</p>
<p>The result were state of the art but by 2018 <span class="citation" data-cites="DBLP:journals/corr/GoyalDGNWKTJH17">(Goyal et al. 2017)</span> it would be possible to train a ResNet-50 imagenet classifier in under an hour of compute with just using 256 GPUs. There is little novelty in the methods. The authors simply increased the depth of the network and increase the umber of parameters.(but they also used them more efficiently).</p>
<div class="no-row-height column-margin column-container"><div id="ref-DBLP:journals/corr/GoyalDGNWKTJH17" class="csl-entry">
Goyal, Priya, Piotr Doll√°r, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. <span>‚ÄúAccurate, Large Minibatch <span>SGD:</span> Training ImageNet in 1 Hour.‚Äù</span> <em>CoRR</em> abs/1706.02677. <a href="http://arxiv.org/abs/1706.02677">http://arxiv.org/abs/1706.02677</a>.
</div><div id="ref-BibEntry2024Sep" class="csl-entry">
Appalapuri, Prabhu. 2016. <span>‚Äú<span class="nocase">Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition</span>.‚Äù</span> <a href="https://github.com/Prabhu204/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition" class="uri">https://github.com/Prabhu204/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition</a>; GitHub.
</div></div><p>At <span class="citation" data-cites="BibEntry2024Sep">(Appalapuri 2016)</span> I found a Pytorch implementation of this paper.</p>
<p>Many People ask what is the difference between VGG16 and VGG19. The difference is that VGG19 has 3 more convolutional layers than VGG16. Since these extra convolutional layers are stacked after two other layers, the receptive field of VGG19 is larger than that of VGG16. Also the CNN also have a RELU so that the network also has increased discriminative power. This means that VGG19 can capture more complex patterns in the input image than VGG16. However, this comes at the cost of more parameters and more computation. In practice, VGG16 is often used because it is simpler and faster to train than VGG19.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./vgg16.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="VGG16"><img src="https://orenbochman.github.io/reviews/2014/VGGNet/vgg16.png" class="img-fluid figure-img" width="800" alt="VGG16"></a></p>
<figcaption>VGG16</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./vgg19.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="VGG19"><img src="https://orenbochman.github.io/reviews/2014/VGGNet/vgg19.png" class="img-fluid figure-img" width="800" alt="VGG19"></a></p>
<figcaption>VGG19</figcaption>
</figure>
</div>
<!--
::: callout-warning

:::: {.column-margin}

![](captain_abvious.jpg){width="250px"} 




@meme2013captainobvious

> More layers $\implies$ more parameters <br>
> more parameters $\implies$ more capacity <br>
> more capacity $\implies$ better fit. <br>
> BaZinga! üöÄ

::::

It seems that the authors were aware of methods for speeding up training but they did not bother with them. They as using 4 GPUs gave them the results in 2-3 weeks per model.

:::
-->
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<ul>
<li>The authors did not use any data augmentation methods like random cropping, flipping, etc. which are common in modern CNN architectures. They also did not use any regularization methods like dropout, L2 regularization, etc. which are also common in modern CNN architectures.</li>
<li>The networks are pretty massive and require a lot of GPU memory in inference.</li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://www.robots.ox.ac.uk/~vgg/research/very_deep/">home page</a></li>
<li>https://medium.com/<span class="citation" data-cites="siddheshb008/vgg-net-architecture-explained-71179310050f">(<strong>siddheshb008/vgg-net-architecture-explained-71179310050f?</strong>)</span></li>
<li>https://karan3-zoh.medium.com/paper-summary-very-deep-convolutional-networks-for-large-scale-image-recognition-e7437959d856</li>
<li>https://safakkbilici.github.io/summary-vggnet/</li>
<li>https://www.cs.toronto.edu/~frossard/post/vgg16/</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2015,
  author = {Bochman, Oren},
  title = {VGGNet: {Very} {Deep} {Convolutional} {Networks} for
    {Large-Scale} {Image} {Recognition}},
  date = {2015-12-10},
  url = {https://orenbochman.github.io/reviews/2014/VGGNet/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2015" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2015. <span>‚ÄúVGGNet: Very Deep Convolutional Networks for
Large-Scale Image Recognition.‚Äù</span> December 10, 2015. <a href="https://orenbochman.github.io/reviews/2014/VGGNet/">https://orenbochman.github.io/reviews/2014/VGGNet/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2014/VGGNet/</guid>
  <pubDate>Wed, 09 Dec 2015 22:00:00 GMT</pubDate>
</item>
<item>
  <title>üìñ All of Statistics</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2004/all-of-statisitcs/</link>
  <description><![CDATA[ 






<!-- VIDEOS GO HERE 

::: {.column-margin #fig-subtasks}


Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
:::

-->
<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/reviews/2004/all-of-statisitcs/cover.webp" class="nolightbox img-fluid figure-img" width="250"></p>
<figcaption>cover</figcaption>
</figure>
</div></div><blockquote class="blockquote">
<p>The are lies, damn lies, and <mark>statistics..</mark> - Mark Twain</p>
</blockquote>
<!-- LEDE personal context why I reviewed this source -->
<p>After I became a data scientist I wanted to squeeze out more insights and to developer better models from the data. I kept going back to my statistic and probability text book.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A horror story of statistics
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="coding-horror.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="a statistical horror story"><img src="https://orenbochman.github.io/reviews/2004/all-of-statisitcs/coding-horror.png" class="float-start img-fluid figure-img" width="250" alt="a statistical horror story"></a></p>
<figcaption>a statistical horror story</figcaption>
</figure>
</div>
<p>But that text book was all theorems and no ‚Ä¶ practical advice. The author had been a friendly professor. He told us that we should buy his book as it would cover the material in the course. I was broke but I scraped together the money to buy the book. About three weeks into the course he became the department head for CS and Mathematics and never taught us again. He assigned a visiting post doc to teaching us. She was smart, sweet and didn‚Äôt speak a word of English or Hebrew only French and the book was in hebrew. I got by reading the book and most of the class failed the first and second exam and had to retake it. I got a mediocre grade and had to do my military service. Years later I wanted to under stand why such a wonderfully printer book was so hard to read and looked at the single page bibliography. It was a very short list. The first title that seemed relevant was a graduate level text book which stated in the intro ‚ÄúThe aim of this book is to present a fundamental proof to the central limit theorems. The material is written so as to get the reader to the proof in the fastest way possible.‚Äù I looked at the table of contents and realized I had a trimmed translation of this text with a few appendixes for material from other courses. Ok there was some stats, the central limit theorem, the law of large numbers and definitions of expectation and variance as a plethora of other results scattered here and there.</p>
<p>The real disaster for me is that I desperately needed some statistics for my second and third year courses in Physics. But the course was a year too late. I eventually taught myself statistics, but I was forever feeling an Imposter in the field - even after tutoring friends and fellow student on the subject.</p>
<p>When I eventually took another class on statistics some years later. There ware no proofs and the teacher was an emeritus professor charged with calculating academic biblliometrics like h-scores for all the academics in the university. He was shocked every time he came up with a new concept I kept answering his questions without thinking ‚Äì while my classmates seemed to be having a hard time. I had developed an uncanny intuition for the subject‚Ä¶ But in reality I was just as shocked as he was.</p>
<p>So I keep the highlighting copy of the book around at least until I get my money‚Äôs worth :-) but I ended up taking courses on line to get to more advanced material.</p>
</div>
</div>
<p>Eventually I cam across this book which does what I had set out to do. Making an outline of the more practical theoretical results alongside the techniques of statistics, exercises giving some opportunities to practice them so you can really start using them on a daily basis! And last but not least code examples to use.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Too Long; Didn‚Äôt Read about Statistics <!-- Short Catchy title -->
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="All of Statistics in a nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="All of Statistics in a nutshell"></a></p>
<figcaption>All of Statistics in a nutshell</figcaption>
</figure>
</div>
<p>This is a no nonsense introduction to statistics.<br>
It isn‚Äôt a complete treatment as the title suggest. However when it came out it was more up to date then what you might expect to be taught in most statistics courses.</p>
<p>I recommend the chapters on inequalities and causality.</p>
</div>
</div>
<p>Here is a lighthearted Deep Dive into the book:</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This book uses lots of big terms so let‚Äôs break them down so we can understand them better</p>
<dl>
<dt>Sample Space (<img src="https://latex.codecogs.com/png.latex?%5COmega">)</dt>
<dd>
The set of all possible outcomes of an experiment.
</dd>
<dt>Outcome (<img src="https://latex.codecogs.com/png.latex?%5Comega">)</dt>
<dd>
A point or element in the sample space.
</dd>
<dt>Event (A)</dt>
<dd>
A subset of the sample space <img src="https://latex.codecogs.com/png.latex?%5COmega">.
</dd>
<dt>Complement of A (<img src="https://latex.codecogs.com/png.latex?A%5Ec">)</dt>
<dd>
The event that A does not occur.
</dd>
<dt>Union (<img src="https://latex.codecogs.com/png.latex?A%20%5Cbigcup%20B">)</dt>
<dd>
The event that A or B (or both) occur.
</dd>
<dt>Intersection (<img src="https://latex.codecogs.com/png.latex?A%20%5Cbigcap%20B">)</dt>
<dd>
The event that <mark>A and B both occur</mark>. Sometimes written as <img src="https://latex.codecogs.com/png.latex?AB"> or <img src="https://latex.codecogs.com/png.latex?(A,B)">. For a sequence of sets <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20%5Cldots">, <img src="https://latex.codecogs.com/png.latex?%5Cbigcap_%7Bi=1%7D%5E%7B%5Cinfty%7D%20A_i%20=%20%5C%7B%20%5Comega%20%5Cin%20%5COmega%20:%20%5Comega%20%5Cin%20A_i%20%5Ctext%7B%20for%20all%20%7D%20i%20%5C%7D">.
</dd>
<dt>Set Difference (<img src="https://latex.codecogs.com/png.latex?A%20-%20B">)</dt>
<dd>
The set of outcomes <img src="https://latex.codecogs.com/png.latex?%5Comega"> such that <img src="https://latex.codecogs.com/png.latex?%5Comega%20%5Cin%20A"> and <img src="https://latex.codecogs.com/png.latex?%5Comega%20%5Cnotin%20B">.
</dd>
<dt>Subset (<img src="https://latex.codecogs.com/png.latex?A%20%5Csubset%20B">)</dt>
<dd>
Every element of <img src="https://latex.codecogs.com/png.latex?A"> is also contained in <img src="https://latex.codecogs.com/png.latex?B">. Equivalently, <img src="https://latex.codecogs.com/png.latex?B%20%5Csupset%20A">.
</dd>
<dt><img src="https://latex.codecogs.com/png.latex?%7CA%7C"></dt>
<dd>
The <mark>number of elements in a finite set A</mark>.
</dd>
<dt>Probability (P)</dt>
<dd>
A <mark>probability measure defined on a <img src="https://latex.codecogs.com/png.latex?%5Csigma">-algebra</mark>.
</dd>
<dt><img src="https://latex.codecogs.com/png.latex?%5Csigma">-algebra (or <img src="https://latex.codecogs.com/png.latex?%5Csigma">-field) (A)</dt>
<dd>
A <mark>class of subsets of <img src="https://latex.codecogs.com/png.latex?%5COmega"></mark> that satisfies three conditions: (i) <img src="https://latex.codecogs.com/png.latex?%5Cemptyset%20%5Cin%20A">, (ii) if <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20%5Cldots,%20%5Cin%20A"> then <img src="https://latex.codecogs.com/png.latex?%5Cbigcup_%7Bi=1%7D%5E%7B%5Cinfty%7D%20A_i%20%5Cin%20A">, and (iii) <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20A"> implies that <img src="https://latex.codecogs.com/png.latex?A%5Ec%20%5Cin%20A">. The sets in <img src="https://latex.codecogs.com/png.latex?A"> are said to be <mark>measurable</mark>.
</dd>
<dt>Measurable Space (<img src="https://latex.codecogs.com/png.latex?%5COmega,%20A">)</dt>
<dd>
A sample space <img src="https://latex.codecogs.com/png.latex?%5COmega"> together with a <img src="https://latex.codecogs.com/png.latex?%5Csigma">-algebra <img src="https://latex.codecogs.com/png.latex?A">.
</dd>
<dt>Probability Space (<img src="https://latex.codecogs.com/png.latex?%5COmega,%20A,%20P">)</dt>
<dd>
A measurable space <img src="https://latex.codecogs.com/png.latex?(%5COmega,%20A)"> together with a probability measure <img src="https://latex.codecogs.com/png.latex?P"> defined on <img src="https://latex.codecogs.com/png.latex?A">.
</dd>
<dt>Borel <img src="https://latex.codecogs.com/png.latex?%5Csigma">-field</dt>
<dd>
The <mark>smallest <img src="https://latex.codecogs.com/png.latex?%5Csigma">-field that contains all the open subsets of the real line</mark>.
</dd>
<dt>Bayes‚Äô Theorem</dt>
<dd>
A theorem that relates the <mark>conditional and marginal probabilities of events</mark>. For a partition <img src="https://latex.codecogs.com/png.latex?A_1,%20%5Cldots,%20A_k"> of <img src="https://latex.codecogs.com/png.latex?%5COmega"> with <img src="https://latex.codecogs.com/png.latex?P(A_i)%20%3E%200"> and an event <img src="https://latex.codecogs.com/png.latex?B"> with <img src="https://latex.codecogs.com/png.latex?P(B)%20%3E%200">, <img src="https://latex.codecogs.com/png.latex?P(A_i%7CB)%20=%20%5Cfrac%7BP(B%7CA_i)P(A_i)%7D%7B%5Csum_%7Bj=1%7D%5E%7Bk%7D%20P(B%7CA_j)P(A_j)%7D">. <img src="https://latex.codecogs.com/png.latex?P(A_i)"> is the <mark>prior probability</mark>, and <img src="https://latex.codecogs.com/png.latex?P(A_i%7CB)"> is the <mark>posterior probability</mark>.
</dd>
<dt>Estimation (Statistics)</dt>
<dd>
<mark>Learning using data to estimate an unknown quantity</mark>. Refers to providing a single ‚Äúbest guess‚Äù of some quantity of interest (point estimation).
</dd>
<dt>Learning (Computer Science)</dt>
<dd>
Finding a good classifier or
</dd>
<dd>
using data to estimate an unknown quantity.
</dd>
<dt>Classification (Statistics)</dt>
<dd>
Supervised learning, predicting a discrete <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X">.
</dd>
<dt>Supervised Learning (Computer Science)</dt>
<dd>
Predicting a discrete <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X">.
</dd>
<dt>Data (Statistics)</dt>
<dd>
<mark>Training sample</mark> <img src="https://latex.codecogs.com/png.latex?(X_1,%20Y_1),%20%5Cldots,%20(X_n,%20Y_n)">.
</dd>
<dt>Training Sample (Computer Science)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?(X_1,%20Y_1),%20%5Cldots,%20(X_n,%20Y_n)">.
</dd>
<dt>Covariates (Statistics)</dt>
<dd>
The <img src="https://latex.codecogs.com/png.latex?X_i">‚Äôs.
</dd>
<dt>Features (Computer Science)</dt>
<dd>
The <img src="https://latex.codecogs.com/png.latex?X_i">‚Äôs.
</dd>
<dt>Classifier (Statistics)</dt>
<dd>
A <mark>hypothesis</mark>, a map from covariates to outcomes.
</dd>
<dt>Hypothesis (Computer Science)</dt>
<dd>
A <mark>map <img src="https://latex.codecogs.com/png.latex?h:%20X%20%5Crightarrow%20Y"></mark> (a classifier).
</dd>
<dt>Hypothesis (Statistics)</dt>
<dd>
A <mark>subset of a parameter space <img src="https://latex.codecogs.com/png.latex?%5CTheta"></mark>.
</dd>
<dt>Confidence Interval</dt>
<dd>
An <mark>interval that contains an unknown quantity with a given frequency</mark> Requires <img src="https://latex.codecogs.com/png.latex?P_%5Ctheta(%5Ctheta%20%5Cin%20C_n)%20%5Cgeq%201%20-%20%5Calpha"> for all <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cin%20%5CTheta">.
</dd>
<dt>Bayes Net</dt>
<dd>
A <mark>directed acyclic graph representing a multivariate distribution with given conditional independence relations</mark>.
</dd>
<dt>Bayesian Inference</dt>
<dd>
<mark>Statistical methods for using data to update beliefs</mark>.
</dd>
<dt>Frequentist Inference</dt>
<dd>
<mark>Statistical methods with guaranteed frequency behavior</mark>.
</dd>
<dt>Large Deviation Bounds</dt>
<dd>
<mark>Uniform bounds on the probability of errors</mark>.
</dd>
<dt>PAC Learning</dt>
<dd>
<mark>Probably Approximately Correct learning</mark>, related to uniform bounds on probability of errors.
</dd>
<dt>Point Estimation</dt>
<dd>
<mark>Providing a single ‚Äúbest guess‚Äù of some quantity of interest</mark>, such as a parameter, cdf <img src="https://latex.codecogs.com/png.latex?F">, pdf <img src="https://latex.codecogs.com/png.latex?f">, regression function <img src="https://latex.codecogs.com/png.latex?r">, or a prediction.
</dd>
<dt>Confidence Sets</dt>
<dd>
A set of values that is believed to contain the true value of a parameter with a certain probability.
</dd>
<dt>Hypothesis Testing</dt>
<dd>
A procedure to decide between two or more competing statements about a population.
</dd>
<dt>Parametric Model</dt>
<dd>
A statistical model where the <mark>set of possible distributions is indexed by a finite number of parameters</mark>.
</dd>
<dt>Nonparametric Model</dt>
<dd>
A statistical model where the <mark>set of possible distributions is not indexed by a finite number of parameters</mark>.
</dd>
<dt>CDF (Cumulative Distribution Function) (<img src="https://latex.codecogs.com/png.latex?F_X(x)">)</dt>
<dd>
For a random variable <img src="https://latex.codecogs.com/png.latex?X">, the probability that <img src="https://latex.codecogs.com/png.latex?X"> takes on a value less than or equal to <img src="https://latex.codecogs.com/png.latex?x"> i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?P(X%20%5Cleq%20x)">
</dd>
<dt>PDF (Probability Density Function) (<img src="https://latex.codecogs.com/png.latex?f_X(x)">)</dt>
<dd>
For continuous random variables, <img src="https://latex.codecogs.com/png.latex?P(a%20%5Cleq%20X%20%5Cleq%20b)%20=%20%5Cint_a%5Eb%20f_X(x)%20dx"> .
</dd>
<dt>IID (Independent and Identically Distributed) Samples</dt>
<dd>
A sequence of random variables <img src="https://latex.codecogs.com/png.latex?X_1,%20%5Cldots,%20X_n"> drawn from the same distribution and are independent of each other.
</dd>
<dt>Statistic</dt>
<dd>
A <mark>function <img src="https://latex.codecogs.com/png.latex?T(X_n)"> of the data</mark>.
</dd>
<dt>Sufficient Statistic</dt>
<dd>
A statistic that <mark>contains all the information in the data</mark> about the parameter of interest.
</dd>
<dt>MLE (Maximum Likelihood Estimator)</dt>
<dd>
An estimator that <mark>maximizes the likelihood function</mark>, which is the probability of observing the data given the parameters.
</dd>
<dt>Bayes Estimator</dt>
<dd>
An estimator that <mark>minimizes the Bayes risk</mark>.
</dd>
<dt>Wald Test</dt>
<dd>
A statistical test for <mark>hypothesis testing about parameters based on the asymptotic normality of estimators</mark>.
</dd>
<dt>p-value</dt>
<dd>
The <mark>probability of observing a test statistic as extreme as, or more extreme than, the one calculated from the sample data, assuming the null hypothesis is true</mark>.
</dd>
<dt>Bootstrap</dt>
<dd>
A <mark>resampling method used for statistical inference</mark>, such as estimating variance and constructing confidence intervals.
</dd>
<dt>Nonparametric Curve Estimation</dt>
<dd>
Methods for <mark>estimating functions (like density or regression functions) without assuming a specific parametric form</mark>.
</dd>
<dt>Classification Rule (Classifier)</dt>
<dd>
A rule or algorithm that <mark>assigns an object to one of several predefined categories based on its features</mark>.
</dd>
<dt>Bayes Classifier</dt>
<dd>
The <mark>classification rule that minimizes the probability of error</mark>. For binary classification (0 or 1), <img src="https://latex.codecogs.com/png.latex?h%5E*(x)%20=%201"> if <img src="https://latex.codecogs.com/png.latex?P(Y=1%7CX=x)%20%3E%20P(Y=0%7CX=x)">, and 0 otherwise. Equivalently, <img src="https://latex.codecogs.com/png.latex?h%5E*(x)%20=%201"> if <img src="https://latex.codecogs.com/png.latex?%5Cpi%20f_1(x)%20%3E%20(1-%5Cpi)%20f_0(x)">, and 0 otherwise.
</dd>
<dt>LDA (Linear Discriminant Analysis)</dt>
<dd>
A classification method where the <mark>decision boundary between classes is a linear function of the features</mark>.
</dd>
<dt>Logistic Regression</dt>
<dd>
A statistical model that uses a <mark>logistic function to model the probability of a binary outcome</mark>.
</dd>
<dt>Support Vector Machines (SVM)</dt>
<dd>
A class of <mark>linear classifiers that aim to find the hyperplane that maximizes the margin between two classes</mark>. Can be extended to non-linear classification using kernel functions.
</dd>
<dt>Kernelization</dt>
<dd>
A technique used in SVM and other methods to <mark>implicitly map data into a higher-dimensional space</mark> to allow for non-linear decision boundaries.
</dd>
<dt>Boosting</dt>
<dd>
An <mark>ensemble learning technique that combines multiple weak classifiers to create a strong classifier</mark>. Can be thought of as a bias reduction technique.
</dd>
<dt>Bagging</dt>
<dd>
An <mark>ensemble learning technique that reduces the variance of a classifier by averaging predictions from multiple classifiers trained on different subsets of the data</mark>.
</dd>
<dt>Markov Chain</dt>
<dd>
A <mark>sequence of random variables where the future state depends only on the current state</mark>, not on the sequence of events that preceded it.
</dd>
<dt>Stationary Distribution (<img src="https://latex.codecogs.com/png.latex?%5Cpi">)</dt>
<dd>
A probability distribution for a Markov chain that <mark>remains unchanged in time</mark>. <img src="https://latex.codecogs.com/png.latex?%5Cpi%20P%20=%20%5Cpi">, where <img src="https://latex.codecogs.com/png.latex?P"> is the transition matrix.
</dd>
<dt>Detailed Balance</dt>
<dd>
A condition for a Markov chain with stationary distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">: <img src="https://latex.codecogs.com/png.latex?%5Cpi_i%20p_%7Bij%7D%20=%20p_%7Bji%7D%20%5Cpi_j"> for all states <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?j">. <mark>Detailed balance guarantees that <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a stationary distribution</mark>.
</dd>
<dt>MCMC (Markov Chain Monte Carlo)</dt>
<dd>
A class of algorithms for <mark>sampling from a probability distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution</mark>. Examples include Metropolis-Hastings and Gibbs sampling.
</dd>
<dt>Graphical Models</dt>
<dd>
Statistical models that use <mark>graphs to represent the conditional dependence structure between random variables</mark>. Can be directed (DAGs) or undirected.
</dd>
<dt>DAG (Directed Acyclic Graph)</dt>
<dd>
A <mark>directed graph with no directed cycles</mark>, used to represent probabilistic relationships and conditional independencies.
</dd>
<dt>Undirected Graph</dt>
<dd>
A graph where the <mark>edges between nodes have no direction</mark>, used to represent symmetric relationships and conditional independencies.
</dd>
<dt>Conditional Independence</dt>
<dd>
A concept where the <mark>independence between two random variables holds given a third variable</mark>. Represented using notation like <img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y%20%7C%20Z">. In graphical models, this can be determined by d-separation in DAGs and graph separation in undirected graphs.
</dd>
<dt>Clique</dt>
<dd>
A <mark>set of variables in a graph that are all adjacent to each other</mark>.
</dd>
<dt>Potential</dt>
<dd>
Any <mark>positive function</mark> defined on a clique in an undirected graphical model.
</dd>
<dt>Log-Linear Models</dt>
<dd>
Statistical models used to <mark>analyze categorical data by modeling the logarithm of the expected frequencies as a linear combination of parameters</mark>, often related to fitting undirected graphical models.
</dd>
<dt>Nonparametric Regression</dt>
<dd>
Methods for <mark>estimating the relationship between a response variable and one or more predictor variables without assuming a specific parametric form for the regression function</mark>.
</dd>
<dt>Kernel Density Estimation</dt>
<dd>
A <mark>nonparametric method for estimating the probability density function of a random variable</mark>.
</dd>
<dt>Smoothing</dt>
<dd>
Techniques used to <mark>estimate an underlying function from noisy data</mark>, often used in nonparametric curve estimation.
</dd>
<dt>Orthogonal Functions</dt>
<dd>
A set of functions that are <mark>mutually orthogonal with respect to some inner product</mark>, used as basis functions in smoothing and density estimation.
</dd>
<dt>Wavelets</dt>
<dd>
A <mark>set of orthogonal functions that are localized in both time and frequency</mark>, used in smoothing, density estimation, and regression.
</dd>
<dt>Simulation</dt>
<dd>
Using computer-generated random numbers to <mark>approximate the properties of a statistical model or to sample from a complex distribution</mark>.
</dd>
<dt>Bayes Risk</dt>
<dd>
The <mark>expected value of the loss function when the parameter is considered a random variable with a prior distribution</mark>.
</dd>
<dt>Minimax Rule</dt>
<dd>
A decision rule that <mark>minimizes the maximum possible risk over all possible values of the parameter</mark>.
</dd>
<dt>Admissibility</dt>
<dd>
An estimator <img src="https://latex.codecogs.com/png.latex?%5Cdelta_1"> is admissible if it is not dominated by any other estimator <img src="https://latex.codecogs.com/png.latex?%5Cdelta_2">, meaning <img src="https://latex.codecogs.com/png.latex?R(%5Ctheta,%20%5Cdelta_2)%20%5Cleq%20R(%5Ctheta,%20%5Cdelta_1)"> for all <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and <img src="https://latex.codecogs.com/png.latex?R(%5Ctheta,%20%5Cdelta_2)%20%3C%20R(%5Ctheta,%20%5Cdelta_1)"> for at least one <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, where <img src="https://latex.codecogs.com/png.latex?R"> is the risk function.
</dd>
<dt>Causal Inference</dt>
<dd>
The process of <mark>drawing conclusions about causal relationships from data</mark>, often involving concepts like potential outcomes and directed acyclic graphs.
</dd>
<dt>Potential Outcomes (<img src="https://latex.codecogs.com/png.latex?C_0,%20C_1">)</dt>
<dd>
Random variables representing the <mark>outcome if a subject is not treated (<img src="https://latex.codecogs.com/png.latex?X=0">) and the outcome if the subject is treated (<img src="https://latex.codecogs.com/png.latex?X=1">)</mark>.
</dd>
<dt>Average Causal Effect</dt>
<dd>
The <mark>average difference in potential outcomes</mark>.
</dd>
<dt>Average Treatment Effect</dt>
<dd>
Another term for the <mark>average causal effect</mark>.
</dd>
<dt>d-separated</dt>
<dd>
In a directed acyclic graph, two nodes A and B are d-separated by a set of nodes C if all paths between A and B are blocked by C. A path is blocked if it contains a collider (V-structure) not in C and none of its descendants are in C, or a non-collider (chain or fork) that is in C.
</dd>
<dt>d-connected</dt>
<dd>
If two nodes are not d-separated.
</dd>
<dt>Markov Equivalent (DAGs)</dt>
<dd>
Two DAGs are Markov equivalent if and only if they have the same skeleton (undirected graph obtained by ignoring the directions of the edges) and the same unshielded colliders (V-structures where the parents are not directly connected).
</dd>
<dt>Pairwise Markov Graph</dt>
<dd>
An undirected graph where an edge is omitted between two variables if they are independent given all other variables.
</dd>
<dt>Clique (Maximal)</dt>
<dd>
A clique is maximal if it is not possible to include another variable and still be a clique.
</dd>
<dt>Hierarchical Model (Log-Linear)</dt>
<dd>
A log-linear model where if a higher-order interaction term is included, then all lower-order terms involving subsets of those variables are also included.
</dd>
<dt>Graphical Model (Log-Linear)</dt>
<dd>
A hierarchical log-linear model where the included interaction terms correspond to the cliques of an undirected graph, and the excluded terms correspond to conditional independencies implied by the graph.
</dd>
<dt>Generator (Log-Linear Models)</dt>
<dd>
A concise way to represent a hierarchical log-linear model by specifying the highest-order interaction terms to be included. All lower-order terms are then automatically included.
</dd>
<dt>Bias-Variance Tradeoff</dt>
<dd>
The property of a set of statistical models whereby models with a lower bias tend to have a higher variance, and vice versa. Important in model selection and curve estimation.
</dd>
<dt>Cross-Validation</dt>
<dd>
A model evaluation technique where the data is divided into subsets, and the model is trained on some subsets and evaluated on the remaining subsets to estimate its performance on unseen data.
</dd>
<dt>Error Rate (Classification)</dt>
<dd>
The proportion of misclassified instances.
</dd>
<dt>Bayes Rule (Classification)</dt>
<dd>
The classification rule that achieves the minimum possible error rate.
</dd>
<dt>Discriminant Function (LDA)</dt>
<dd>
A function that assigns a score to each class for a given observation, and the observation is classified to the class with the highest score.
</dd>
<dt>Decision Boundary</dt>
<dd>
The surface in the feature space that separates different classes according to a classification rule.
</dd>
<dt>Gini Index</dt>
<dd>
A measure of impurity used in decision tree algorithms.
</dd>
<dt>Hyperplane (SVM)</dt>
<dd>
A flat affine subspace of dimension <img src="https://latex.codecogs.com/png.latex?p-1"> in a <img src="https://latex.codecogs.com/png.latex?p">-dimensional space, used by linear classifiers to separate classes.
</dd>
<dt>Margin (SVM)</dt>
<dd>
The distance between the hyperplane and the closest data points from each class.
</dd>
<dt>Kernel (SVM)</dt>
<dd>
A function that defines a similarity measure between data points and allows SVMs to learn non-linear decision boundaries by implicitly mapping the data to a higher-dimensional space.
</dd>
<dt>Random Walk Metropolis-Hastings</dt>
<dd>
A specific type of Metropolis-Hastings algorithm where the proposal distribution is centered at the current state.
</dd>
<dt>Gibbs Sampling</dt>
<dd>
An MCMC algorithm where each variable is sampled conditionally on the current values of all other variables.
</dd>
<dt>Posterior Mean</dt>
<dd>
The mean of the posterior distribution in Bayesian inference.
</dd>
<dt>Posterior Interval</dt>
<dd>
An interval that contains the true value of a parameter with a certain probability according to the posterior distribution.
</dd>
<dt>Frequentist Approach</dt>
<dd>
Statistical methods that interpret probability as a long-run frequency of events.
</dd>
<dt>Maximum Likelihood (Frequentist)</dt>
<dd>
A method of estimating the parameters of a statistical model by finding the parameter values that maximize the likelihood of observing the data.
</dd>
</dl>
<section id="part-i-probability" class="level3">
<h3 class="anchored" data-anchor-id="part-i-probability">Part I: Probability</h3>
<p>Here is an outline of Chapter 1, ‚ÄúProbability,‚Äù based on the provided excerpts from ‚Äúbook.pdf,‚Äù with mathematical expressions surrounded by dollar signs:</p>
<ul>
<li><strong>1 Probability</strong>
<ul>
<li><strong>1.1 Introduction</strong>
<ul>
<li>Probability is defined as a <strong>mathematical language for quantifying uncertainty</strong>.</li>
<li>This chapter introduces the basic concepts underlying probability theory.</li>
<li>It begins with the <strong>sample space</strong>, which is the set of possible outcomes.</li>
</ul></li>
<li><strong>1.2 Sample Spaces and Events</strong>
<ul>
<li>The <strong>sample space <img src="https://latex.codecogs.com/png.latex?%5COmega"> is the set of possible outcomes of an experiment</strong>.</li>
<li>Points <img src="https://latex.codecogs.com/png.latex?%5Comega"> in <img src="https://latex.codecogs.com/png.latex?%5COmega"> are called <strong>sample outcomes, realizations, or elements</strong>.</li>
<li><strong>Subsets of <img src="https://latex.codecogs.com/png.latex?%5COmega"> are called Events</strong>.</li>
<li><strong>Example 1.1</strong>: Tossing a coin twice gives <img src="https://latex.codecogs.com/png.latex?%5COmega%20=%20%5C%7BHH,%20HT,%20TH,%20TT%5C%7D">. The event that the first toss is heads is <img src="https://latex.codecogs.com/png.latex?A%20=%20%5C%7BHH,%20HT%5C%7D">.</li>
</ul></li>
<li><strong>1.3 Probability</strong>
<ul>
<li>This section introduces the definition and axioms of probability.</li>
<li><strong>Definition 1.5</strong>: A <strong>probability measure</strong> <img src="https://latex.codecogs.com/png.latex?P"> on a sample space <img src="https://latex.codecogs.com/png.latex?%5COmega"> is a function <img src="https://latex.codecogs.com/png.latex?P:%20A%20%5Crightarrow%20%5B0,%201%5D"> that satisfies three axioms:
<ul>
<li><strong>Axiom 1</strong>: <img src="https://latex.codecogs.com/png.latex?P(A)%20%5Cgeq%200"> for all events <img src="https://latex.codecogs.com/png.latex?A">.</li>
<li><strong>Axiom 2</strong>: <img src="https://latex.codecogs.com/png.latex?P(%5COmega)%20=%201">.</li>
<li><strong>Axiom 3</strong>: If <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20%5Cldots"> are disjoint events, then <img src="https://latex.codecogs.com/png.latex?P(%5Cbigcup_%7Bi=1%7D%5E%7B%5Cinfty%7D%20A_i)%20=%20%5Csum_%7Bi=1%7D%5E%7B%5Cinfty%7D%20P(A_i)">.</li>
</ul></li>
</ul></li>
<li><strong>1.4 Probability on Finite Sample Spaces</strong>
<ul>
<li>This section discusses how to calculate probabilities when the sample space contains a finite number of equally likely outcomes.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5COmega%20=%20%5C%7B%20%5Comega_1,%20%5Cldots,%20%5Comega_n%20%5C%7D">, then <img src="https://latex.codecogs.com/png.latex?P(A)%20=%20%5Cfrac%7B%7CA%7C%7D%7B%7C%5COmega%7C%7D"> for any event <img src="https://latex.codecogs.com/png.latex?A%20%5Csubset%20%5COmega">.<br>
</li>
</ul></li>
<li><strong>1.5 Independent Events</strong>
<ul>
<li>This section defines independent events.</li>
</ul></li>
<li><strong>1.6 Conditional Probability</strong>
<ul>
<li>This section introduces the concept of conditional probability, <img src="https://latex.codecogs.com/png.latex?P(A%7CB)%20=%20%5Cfrac%7BP(A%20%5Ccap%20B)%7D%7BP(B)%7D"></li>
</ul></li>
<li><strong>1.7 Bayes‚Äô Theorem</strong>
<ul>
<li><strong>Theorem 1.17 (Bayes‚Äô Theorem)</strong> is presented: Let <img src="https://latex.codecogs.com/png.latex?A_1,%20%5Cldots,%20A_k"> be a partition of <img src="https://latex.codecogs.com/png.latex?%5COmega"> such that <img src="https://latex.codecogs.com/png.latex?P(A_i)%20%3E%200"> for each <img src="https://latex.codecogs.com/png.latex?i">. If <img src="https://latex.codecogs.com/png.latex?P(B)%20%3E%200">, then for each <img src="https://latex.codecogs.com/png.latex?i%20=%201,%20%5Cldots,%20k">, <img src="https://latex.codecogs.com/png.latex?P(A_i%7CB)%20=%20%5Cfrac%7BP(B%7CA_i)P(A_i)%7D%7B%5Csum_%7Bj=1%7D%5E%7Bk%7D%20P(B%7CA_j)P(A_j)%7D">.</li>
<li><strong>Remark 1.18</strong>: <img src="https://latex.codecogs.com/png.latex?P(A_i)"> is called the <strong>prior probability</strong> of <img src="https://latex.codecogs.com/png.latex?A_i">, and <img src="https://latex.codecogs.com/png.latex?P(A_i%7CB)"> is called the <strong>posterior probability</strong> of <img src="https://latex.codecogs.com/png.latex?A_i">.</li>
<li><strong>Example 1.19</strong>: An email categorization example is provided, calculating the probability that an email containing the word ‚Äúfree‚Äù is spam using Bayes‚Äô Theorem.</li>
</ul></li>
<li><strong>1.8 Bibliographic Remarks</strong>
<ul>
<li>Several books are mentioned as further reading:</li>
<li>DeGroot and Schervish (2002),</li>
<li>Grimmett and Stirzaker (1982),</li>
<li>Karr (1993),</li>
<li>Billingsley (1979), and</li>
<li>Breiman (1992).</li>
</ul></li>
</ul></li>
<li><strong>2 Random Variables</strong>
<ul>
<li><strong>2.1 Introduction</strong>
<ul>
<li>A <strong>random variable</strong> is a mapping that assigns a real number <img src="https://latex.codecogs.com/png.latex?X(%5Comega)"> to each outcome <img src="https://latex.codecogs.com/png.latex?%5Comega"> in the sample space.</li>
</ul></li>
<li><strong>2.2 Distribution Functions and Probability Functions</strong>
<ul>
<li>Covers the cumulative distribution function (CDF) <img src="https://latex.codecogs.com/png.latex?F_X(x)%20=%20P(X%20%5Cleq%20x)">.</li>
<li>Discusses probability functions <img src="https://latex.codecogs.com/png.latex?f_X(x)%20=%20P(X=x)"> for discrete random variables.</li>
<li>Example 2.6 provides a probability function for flipping a coin twice: <img src="https://latex.codecogs.com/png.latex?f_X(x)%20=%20%5Cbegin%7Bcases%7D%201/4%20&amp;%20x%20=%200%20%5C%5C%201/2%20&amp;%20x%20=%201%20%5C%5C%201/4%20&amp;%20x%20=%202%20%5C%5C%200%20&amp;%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D">.</li>
<li>Figure 2.2 illustrates this probability function.</li>
</ul></li>
<li><strong>2.3 Some Important Discrete Random Variables</strong>
<ul>
<li>Point Mass Distribution.</li>
<li>The Discrete Uniform Distribution.</li>
<li>The Bernoulli Distribution.</li>
<li>The Binomial Distribution.</li>
<li>The Geometric Distribution.</li>
<li>The Poisson Distribution.</li>
<li>The text mentions that for continuous random variables, it‚Äôs more convenient to define things in terms of the pdf, as the probability of a continuous random variable taking a specific value is zero.</li>
</ul></li>
<li><strong>2.4 Some Important Continuous Random Variables</strong>
<ul>
<li>This section details important continuous distributions.</li>
<li><strong>Normal Distribution</strong>: Denoted as <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20N(%5Cmu,%20%5Csigma%5E2)">, with density <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma%5E2%7D%7D%20e%5E%7B-%5Cfrac%7B(x-%5Cmu)%5E2%7D%7B2%5Csigma%5E2%7D%7D">.
<ul>
<li>Standard Normal distribution <img src="https://latex.codecogs.com/png.latex?Z%20%5Csim%20N(0,%201)"> has density <img src="https://latex.codecogs.com/png.latex?%5Cphi(z)%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%20e%5E%7B-z%5E2/2%7D"> and CDF <img src="https://latex.codecogs.com/png.latex?%5CPhi(z)%20=%20P(Z%20%5Cleq%20z)">.</li>
<li><img src="https://latex.codecogs.com/png.latex?P(a%20%3C%20X%20%3C%20b)%20=%20%5CPhi(%5Cfrac%7Bb%20-%20%5Cmu%7D%7B%5Csigma%7D)%20-%20%5CPhi(%5Cfrac%7Ba%20-%20%5Cmu%7D%7B%5Csigma%7D)"> if <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20N(%5Cmu,%20%5Csigma%5E2)">.</li>
<li>Figure 2.4 shows the density of a standard Normal.</li>
<li>Example 2.17: If <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20N(3,%205)">, find <img src="https://latex.codecogs.com/png.latex?P(X%20%3E%201)">.</li>
</ul></li>
<li><strong>Uniform Distribution</strong>: <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20Uniform(a,%20b)"> has density <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B1%7D%7Bb-a%7D"> for <img src="https://latex.codecogs.com/png.latex?a%20%3C%20x%20%3C%20b">, and <img src="https://latex.codecogs.com/png.latex?0"> otherwise.
<ul>
<li>Example 2.39 involves a Uniform(0, 1) distribution.</li>
</ul></li>
<li><strong>Exponential Distribution</strong>: <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20Exp(%5Cbeta)"> has density <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B1%7D%7B%5Cbeta%7D%20e%5E%7B-x/%5Cbeta%7D"> for <img src="https://latex.codecogs.com/png.latex?x%20%3E%200">, <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%3E%200">, and <img src="https://latex.codecogs.com/png.latex?0"> otherwise.</li>
</ul></li>
<li><strong>2.5 Multivariate Distributions</strong>
<ul>
<li>This section covers the joint distribution of two or more random variables.</li>
<li>The concept of a joint density <img src="https://latex.codecogs.com/png.latex?f(x,%20y)"> for continuous random variables <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> such that <img src="https://latex.codecogs.com/png.latex?P(a%20%3C%20X%20%3C%20b,%20c%20%3C%20Y%20%3C%20d)%20=%20%5Cint_a%5Eb%20%5Cint_c%5Ed%20f(x,%20y)%20dy%20dx"> will likely be introduced.</li>
<li>Marginal distributions can be obtained by integrating (or summing) the joint distribution over the other variables: <img src="https://latex.codecogs.com/png.latex?f_X(x)%20=%20%5Cint%20f(x,%20y)%20dy"> and <img src="https://latex.codecogs.com/png.latex?f_Y(y)%20=%20%5Cint%20f(x,%20y)%20dx">.</li>
<li>Example 2.27 is mentioned in Example 2.38, where the marginal density <img src="https://latex.codecogs.com/png.latex?f_Y(y)%20=%20y%20+%20(1/2)"> is used.</li>
<li>Example 2.38 involves finding a conditional probability <img src="https://latex.codecogs.com/png.latex?P(X%20%3C%201/4%20%7C%20Y%20=%201/3)"> using the conditional density <img src="https://latex.codecogs.com/png.latex?f_%7BX%7CY%7D(x%7Cy)%20=%20%5Cfrac%7Bf_%7BX,Y%7D(x,%20y)%7D%7Bf_Y(y)%7D">.</li>
</ul></li>
<li><strong>2.6 Independence</strong>
<ul>
<li>Two random variables <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are independent if their joint distribution is the product of their marginal distributions: <img src="https://latex.codecogs.com/png.latex?F_%7BX,Y%7D(x,%20y)%20=%20F_X(x)%20F_Y(y)"> or <img src="https://latex.codecogs.com/png.latex?f_%7BX,Y%7D(x,%20y)%20=%20f_X(x)%20f_Y(y)">.<br>
</li>
</ul></li>
<li><strong>2.7 Conditional Probability</strong>
<ul>
<li>Extends the concept of conditional probability from events to random variables, likely defining conditional distributions and densities.</li>
<li>Example 2.38 shows the calculation of conditional probability for continuous random variables.</li>
</ul></li>
<li><strong>2.8 Bayes‚Äô Theorem for Random Variables</strong>
<ul>
<li>Will likely present an extension of Bayes‚Äô Theorem to relate conditional densities: <img src="https://latex.codecogs.com/png.latex?f_%7BX%7CY%7D(x%7Cy)%20=%20%5Cfrac%7Bf_%7BY%7CX%7D(y%7Cx)%20f_X(x)%7D%7Bf_Y(y)%7D">.</li>
</ul></li>
<li><strong>2.9 Multivariate Distributions and iid Samples</strong>
<ul>
<li>Covers the concept of multiple random variables considered together.</li>
<li>Introduces the idea of independent and identically distributed (iid) samples <img src="https://latex.codecogs.com/png.latex?X_1,%20%5Cldots,%20X_n"> drawn from the same distribution.</li>
</ul></li>
<li><strong>2.10 Two Important Multivariate Distributions</strong>
<ul>
<li>Discusses specific multivariate distributions.</li>
<li><strong>Multinomial Distribution</strong>: Mentioned in Chapter 14 and likely introduced here.</li>
<li><strong>Multivariate Normal Distribution</strong>: Also mentioned in Chapter 14 and likely introduced here.</li>
</ul></li>
<li><strong>2.11 Transformations of Random Variables</strong>
<ul>
<li>Deals with finding the distribution of a function of one or more random variables (e.g., if <img src="https://latex.codecogs.com/png.latex?Y%20=%20g(X)">, how to find <img src="https://latex.codecogs.com/png.latex?F_Y"> or <img src="https://latex.codecogs.com/png.latex?f_Y">).</li>
</ul></li>
<li><strong>2.12 Transformations of Several Random Variables</strong>
<ul>
<li>Extends the transformation techniques to functions of multiple random variables.</li>
</ul></li>
</ul></li>
<li><strong>3 Expectation</strong>
<ul>
<li><strong>3.1 Expectation of a Random Variable</strong>
<ul>
<li>Introduces the concept of the expectation of a random variable.</li>
<li>The notation <img src="https://latex.codecogs.com/png.latex?%5Cint%20x%20dF(x)"> is used as a unifying notation for both discrete (<img src="https://latex.codecogs.com/png.latex?%5Csum_x%20xf(x)">) and continuous (<img src="https://latex.codecogs.com/png.latex?%5Cint%20xf(x)dx">) random variables.</li>
<li>It is noted that the precise meaning of <img src="https://latex.codecogs.com/png.latex?%5Cint%20x%20dF(x)"> is discussed in real analysis courses.</li>
<li>The expectation <img src="https://latex.codecogs.com/png.latex?E(X)"> is said to exist if <img src="https://latex.codecogs.com/png.latex?%5Cint%20%7Cx%7CdF_X(x)%20%3C%20%5Cinfty">. Otherwise, the expectation does not exist.</li>
</ul></li>
<li><strong>3.2 Properties of Expectations</strong>
<ul>
<li>This section will likely cover properties such as linearity of expectation, <img src="https://latex.codecogs.com/png.latex?E(aX%20+%20bY)%20=%20aE(X)%20+%20bE(Y)">, and other fundamental rules.</li>
</ul></li>
<li><strong>3.3 Variance and Covariance</strong>
<ul>
<li>Defines variance and covariance, likely with formulas such as <img src="https://latex.codecogs.com/png.latex?Var(X)%20=%20E%5B(X%20-%20E%5BX%5D)%5E2%5D"> and <img src="https://latex.codecogs.com/png.latex?Cov(X,%20Y)%20=%20E%5B(X%20-%20E%5BX%5D)(Y%20-%20E%5BY%5D)%5D">.</li>
</ul></li>
<li><strong>3.4 Expectation and Variance of Important Random Variables</strong>
<ul>
<li>This section will likely provide the formulas for the expectation and variance of common probability distributions like Poisson, Normal, and Gamma.</li>
</ul></li>
<li><strong>3.5 Conditional Expectation</strong>
<ul>
<li>Introduces the concept of conditional expectation, <img src="https://latex.codecogs.com/png.latex?E(X%7CY=y)"> and <img src="https://latex.codecogs.com/png.latex?E(X%7CY)">.</li>
</ul></li>
<li><strong>3.6 Moment Generating Functions</strong>
<ul>
<li>Covers moment generating functions (MGFs), <img src="https://latex.codecogs.com/png.latex?M_X(t)%20=%20E%5Be%5E%7BtX%7D%5D">, and their properties.</li>
</ul></li>
<li><strong>3.7 Appendix</strong>
<ul>
<li>This section might contain proofs or more technical details related to expectation, variance, covariance, conditional expectation, and moment generating functions.</li>
</ul></li>
</ul></li>
<li><strong>4 Inequalities</strong>
<ul>
<li><strong>4.1 Probability Inequalities</strong>
<ul>
<li>Introduces the concept that inequalities are useful for bounding quantities that might be hard to compute.</li>
<li>Mentions that inequalities will also be used in the theory of convergence (discussed in Chapter 5).</li>
<li><strong>Markov‚Äôs Inequality</strong>: Likely presents the inequality, which provides an upper bound on the probability that a non-negative random variable exceeds a certain value. It is referred to as ‚ÄúOur first inequality‚Äù.</li>
<li><strong>Chebyshev‚Äôs Inequality</strong>: Will likely be covered, providing a bound on the probability that a random variable deviates from its mean by more than a certain amount.</li>
<li><strong>Jensen‚Äôs Inequality</strong>: This inequality relates the value of a convex function of the expected value of a random variable to the expected value of the convex function of the random variable.</li>
<li><strong>Hoeffding‚Äôs Inequality</strong>: Mentioned as having a proof in the appendix. This inequality provides bounds on the probability that the sum of independent, bounded random variables deviates from its expected value.</li>
<li>Mill‚Äôs Inequality is mentioned in the index.</li>
</ul></li>
<li><strong>4.2 Inequalities For Expectations</strong>
<ul>
<li>This section will likely cover inequalities that provide bounds or relationships between the expectations of random variables or functions of random variables.</li>
</ul></li>
<li><strong>4.3 Bibliographic Remarks</strong>
<ul>
<li>This section will provide references to other resources for further reading on inequalities.</li>
</ul></li>
<li><strong>4.4 Appendix</strong>
<ul>
<li>Contains the <strong>Proof of Hoeffding‚Äôs Inequality</strong>.</li>
<li>Includes the <strong>Proof of Theorem 4.4</strong>, which uses Markov‚Äôs inequality. The theorem and its context are not explicitly detailed in the chapter outline, but the proof‚Äôs starting point is mentioned.</li>
</ul></li>
</ul></li>
<li><strong>5 Convergence of Random Variables</strong>
<ul>
<li><strong>5.1 Introduction</strong>
<ul>
<li>This section likely introduces the concept of convergence for sequences of random variables.</li>
</ul></li>
<li><strong>5.2 Types of Convergence</strong>
<ul>
<li>This section will detail different modes of convergence for random variables. These typically include:
<ul>
<li>Convergence in probability: <img src="https://latex.codecogs.com/png.latex?P(%7CX_n%20-%20X%7C%20%3E%20%5Cepsilon)%20%5Cto%200"> as <img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty"> for all <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%3E%200">.</li>
<li>Almost sure convergence (or convergence with probability 1): <img src="https://latex.codecogs.com/png.latex?P(%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20X_n%20=%20X)%20=%201">.</li>
<li>Convergence in <img src="https://latex.codecogs.com/png.latex?r">-th mean: <img src="https://latex.codecogs.com/png.latex?E(%7CX_n%20-%20X%7C%5Er)%20%5Cto%200"> as <img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty"> for some <img src="https://latex.codecogs.com/png.latex?r%20%3E%200"> (commonly <img src="https://latex.codecogs.com/png.latex?r=1"> for convergence in mean and <img src="https://latex.codecogs.com/png.latex?r=2"> for convergence in mean square).</li>
<li>Convergence in distribution (or weak convergence): <img src="https://latex.codecogs.com/png.latex?F_%7BX_n%7D(x)%20%5Cto%20F_X(x)"> as <img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty"> for all <img src="https://latex.codecogs.com/png.latex?x"> at which <img src="https://latex.codecogs.com/png.latex?F_X"> is continuous, where <img src="https://latex.codecogs.com/png.latex?F_%7BX_n%7D"> and <img src="https://latex.codecogs.com/png.latex?F_X"> are the cumulative distribution functions of <img src="https://latex.codecogs.com/png.latex?X_n"> and <img src="https://latex.codecogs.com/png.latex?X">, respectively.</li>
</ul></li>
</ul></li>
<li><strong>5.3 The Law of Large Numbers</strong>
<ul>
<li>This section will likely cover fundamental theorems related to the convergence of sample averages to the population mean. This usually includes:
<ul>
<li>The Weak Law of Large Numbers (WLLN): For a sequence of independent and identically distributed (iid) random variables <img src="https://latex.codecogs.com/png.latex?X_1,%20X_2,%20...,%20X_n"> with mean <img src="https://latex.codecogs.com/png.latex?%5Cmu">, the sample mean <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D_n%20=%20(1/n)%20%5Csum_%7Bi=1%7D%5En%20X_i"> converges in probability to <img src="https://latex.codecogs.com/png.latex?%5Cmu">.</li>
<li>The Strong Law of Large Numbers (SLLN): Under similar conditions, the sample mean <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D_n"> converges almost surely to <img src="https://latex.codecogs.com/png.latex?%5Cmu">.</li>
</ul></li>
</ul></li>
<li><strong>5.4 The Central Limit Theorem</strong>
<ul>
<li>This crucial theorem describes the limiting distribution of the standardized sample mean. It typically states that for a sequence of iid random variables <img src="https://latex.codecogs.com/png.latex?X_1,%20X_2,%20...,%20X_n"> with mean <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20%3E%200">, the standardized sample mean <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cbar%7BX%7D_n%20-%20%5Cmu%7D%7B%5Csigma%20/%20%5Csqrt%7Bn%7D%7D"> converges in distribution to a standard Normal distribution <img src="https://latex.codecogs.com/png.latex?N(0,%201)">.</li>
</ul></li>
<li><strong>5.5 The Delta Method</strong>
<ul>
<li>The delta method is a technique for approximating the distribution of a function of a random variable when the limiting distribution of the random variable is known (usually Normal). If <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bn%7D(X_n%20-%20%5Ctheta)%20%5Cxrightarrow%7Bd%7D%20N(0,%20%5Csigma%5E2)"> and <img src="https://latex.codecogs.com/png.latex?g"> is a differentiable function, then <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bn%7D(g(X_n)%20-%20g(%5Ctheta))%20%5Cxrightarrow%7Bd%7D%20N(0,%20(g'(%5Ctheta))%5E2%20%5Csigma%5E2)">. This is also mentioned in the context of parametric inference in Chapter 9.</li>
</ul></li>
<li><strong>5.6 Bibliographic Remarks</strong>
<ul>
<li>This section will provide references to other resources for further reading on the topic of convergence of random variables.</li>
</ul></li>
<li><strong>5.7 Appendix</strong>
<ul>
<li>This section might contain proofs or additional technical details related to the concepts discussed in the chapter.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="part-ii-statistical-inference" class="level3">
<h3 class="anchored" data-anchor-id="part-ii-statistical-inference">Part II: Statistical Inference</h3>
<ul>
<li><strong>6. Models, Statistical Inference and Learning</strong>
<ul>
<li><strong>6.1 Introduction</strong>
<ul>
<li>Discusses the increasing availability of data and the need for statistical tools to analyze it.</li>
<li>Highlights the importance of understanding basic probability and mathematical statistics for data analysis, even when using advanced tools.</li>
<li>Mentions the book‚Äôs aim to cover a broad range of topics quickly, including modern concepts often in follow-up courses.</li>
<li>States that the book is for graduate or advanced undergraduate students in computer science, mathematics, statistics, and related disciplines, assuming knowledge of calculus and some linear algebra.</li>
<li>Figure 1 illustrates the relationship between probability (data generating process to observed data) and inference/data mining (observed data back to understanding the process).</li>
</ul></li>
<li><strong>6.2 Parametric and Nonparametric Models</strong>
<ul>
<li>Introduces the concept of a <strong>statistical model</strong> as a set of probability distributions that we assume our data comes from.</li>
<li>Defines a <strong>parametric model</strong> as a model that is indexed by a finite-dimensional parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> belonging to a parameter space <img src="https://latex.codecogs.com/png.latex?%5CTheta">. It is written as <img src="https://latex.codecogs.com/png.latex?F%20=%20%5C%7Bf(x;%20%5Ctheta)%20:%20%5Ctheta%20%5Cin%20%5CTheta%5C%7D">.</li>
<li>Defines a <strong>nonparametric model</strong> as a model that is not indexed by a finite-dimensional parameter. The set of all continuous distribution functions is an example.</li>
<li>Provides an example of a parametric model: the set of all Normal distributions with mean <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">, where <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20(%5Cmu,%20%5Csigma%5E2)">.</li>
<li>Explains that in parametric inference, the goal is to estimate <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
<li>In nonparametric inference, the goal might be to estimate an unknown distribution function <img src="https://latex.codecogs.com/png.latex?F"> or a regression function <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20E(Y%7CX=x)"> without assuming a specific parametric form.</li>
<li>Mentions <strong>frequentist</strong> and <strong>Bayesian</strong> approaches to statistical inference, noting that both will be covered, starting with frequentist inference.</li>
<li>Introduces the notation <img src="https://latex.codecogs.com/png.latex?P_%5Ctheta(X%20%5Cin%20A)%20=%20%E2%88%AB_A%20f(x;%20%5Ctheta)dx"> and <img src="https://latex.codecogs.com/png.latex?E_%5Ctheta(r(X))%20=%20%E2%88%AB%20r(x)f(x;%20%5Ctheta)dx"> for parametric models, emphasizing that <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is fixed, not averaged over. <img src="https://latex.codecogs.com/png.latex?V_%5Ctheta"> is used for variance.</li>
</ul></li>
<li><strong>6.3 Fundamental Concepts in Inference</strong>
<ul>
<li>States that many inferential problems fall into three categories: <strong>estimation</strong>, <strong>confidence sets</strong>, or <strong>hypothesis testing</strong>.</li>
<li><strong>6.3.1 Point Estimation</strong>
<ul>
<li>Defines <strong>point estimation</strong> as providing a single ‚Äúbest guess‚Äù of a quantity of interest.</li>
<li>The quantity can be a parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, a cdf <img src="https://latex.codecogs.com/png.latex?F">, a pdf <img src="https://latex.codecogs.com/png.latex?f">, a regression function <img src="https://latex.codecogs.com/png.latex?r">, or a future prediction <img src="https://latex.codecogs.com/png.latex?Y">.</li>
</ul></li>
<li><strong>6.3.2 Confidence Sets</strong>
<ul>
<li>Explains that a <strong>confidence set</strong> <img src="https://latex.codecogs.com/png.latex?C_n"> is a random set constructed from data such that it contains the true value of the parameter with a specified probability.</li>
<li>For a parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, a <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> confidence set satisfies <img src="https://latex.codecogs.com/png.latex?P_%5Ctheta(%5Ctheta%20%5Cin%20C_n)%20%5Cge%201%20-%20%5Calpha"> for all <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cin%20%5CTheta">, where <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> is the <strong>coverage probability</strong>.</li>
<li>Discusses <strong>pointwise asymptotic confidence intervals</strong>, where <img src="https://latex.codecogs.com/png.latex?lim%20inf_%7Bn%20%5Cto%20%5Cinfty%7D%20P_%5Ctheta(%5Ctheta%20%5Cin%20C_n)%20%5Cge%201%20-%20%5Calpha"> for all <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cin%20%5CTheta">.</li>
<li>Also mentions <strong>uniform asymptotic confidence intervals</strong>, where <img src="https://latex.codecogs.com/png.latex?lim%20inf_%7Bn%20%5Cto%20%5Cinfty%7D%20inf_%7B%5Ctheta%20%5Cin%20%5CTheta%7D%20P_%5Ctheta(%5Ctheta%20%5Cin%20C_n)%20%5Cge%201%20-%20%5Calpha">.</li>
</ul></li>
<li><strong>6.3.3 Hypothesis Testing</strong>
<ul>
<li>Describes <strong>hypothesis testing</strong> as a procedure for deciding between two competing claims about a population, the <strong>null hypothesis</strong> (<img src="https://latex.codecogs.com/png.latex?H_0">) and the <strong>alternative hypothesis</strong> (<img src="https://latex.codecogs.com/png.latex?H_1">).</li>
<li>Provides an example of testing the fairness of a coin: <img src="https://latex.codecogs.com/png.latex?H_0%20:%20p%20=%201/2"> versus <img src="https://latex.codecogs.com/png.latex?H_1%20:%20p%20%5Cne%201/2">, where <img src="https://latex.codecogs.com/png.latex?p"> is the probability of heads.</li>
<li>Suggests using a <strong>test statistic</strong>, such as <img src="https://latex.codecogs.com/png.latex?T%20=%20%7C%5Chat%7Bp%7D_n%20-%20(1/2)%7C">, to make a decision, where <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D_n"> is the sample proportion of heads. The null hypothesis would be rejected if <img src="https://latex.codecogs.com/png.latex?T"> is large.</li>
</ul></li>
</ul></li>
<li><strong>6.4 Bibliographic Remarks</strong>
<ul>
<li>Lists several textbooks on statistical inference at elementary, intermediate, and advanced levels.</li>
</ul></li>
<li><strong>6.5 Appendix</strong>
<ul>
<li>Provides definitions for different types of confidence intervals: standard confidence interval, pointwise asymptotic confidence interval, and uniform asymptotic confidence interval.</li>
</ul></li>
</ul></li>
<li><strong>7 Nonparametric Estimation of a CDF and Statistical Functionals</strong>
<ul>
<li><strong>7.1 The Empirical Distribution Function</strong>
<ul>
<li>Introduces the empirical cumulative distribution function (ecdf) <img src="https://latex.codecogs.com/png.latex?F%CC%82_n(x)%20=%20(1/n)%20%E2%88%91_%7Bi=1%7D%5En%20I(X_i%20%E2%89%A4%20x)">.</li>
<li>Discusses the Glivenko-Cantelli theorem, which states that <img src="https://latex.codecogs.com/png.latex?sup_x%20%7CF%CC%82_n(x)%20%E2%88%92%20F(x)%7C%20%E2%86%92%200"> almost surely.</li>
<li>Mentions the Dvoretzky-Kiefer-Wolfowitz inequality, which provides a bound on the probability that the supremum distance between <img src="https://latex.codecogs.com/png.latex?F%CC%82_n(x)"> and <img src="https://latex.codecogs.com/png.latex?F(x)"> exceeds a certain value.</li>
<li><strong>Example (Nerve Data)</strong> shows the empirical cdf for waiting times between nerve pulses. It mentions estimating the fraction of waiting times between .4 and .6 seconds using <img src="https://latex.codecogs.com/png.latex?F%CC%82_n(.6)%20-%20F%CC%82_n(.4)">.</li>
<li>Figure 7.1 shows nerve data with the empirical distribution function and a 95 percent confidence band.</li>
</ul></li>
<li><strong>7.2 Statistical Functionals</strong>
<ul>
<li>Defines a statistical functional as a map <img src="https://latex.codecogs.com/png.latex?T(F)"> from a distribution function <img src="https://latex.codecogs.com/png.latex?F"> to a real number.</li>
<li>Explains the plug-in principle: estimate <img src="https://latex.codecogs.com/png.latex?T(F)"> by <img src="https://latex.codecogs.com/png.latex?T(F%CC%82_n)">.</li>
<li><strong>Example (The Mean)</strong> illustrates the plug-in estimate for the mean <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20E_F(X)%20=%20%E2%88%AB%20x%20dF(x)">, which is the sample mean <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D_n%20=%20%E2%88%AB%20x%20dF%CC%82_n(x)%20=%20(1/n)%20%E2%88%91_%7Bi=1%7D%5En%20X_i">.</li>
<li><strong>Example (The Variance)</strong> shows the plug-in estimate for the variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20=%20E_F(X%20%E2%88%92%20%5Cmu)%5E2">, which is the sample variance <img src="https://latex.codecogs.com/png.latex?S_n%5E2%20=%20(1/n)%20%E2%88%91_%7Bi=1%7D%5En%20(X_i%20%E2%88%92%20%5Cbar%7BX%7D_n)%5E2">.</li>
<li><strong>Example (The Median)</strong> defines the median <img src="https://latex.codecogs.com/png.latex?m"> as <img src="https://latex.codecogs.com/png.latex?F(m)%20=%201/2"> and the plug-in estimate as the sample median <img src="https://latex.codecogs.com/png.latex?F%CC%82_n%5E%7B-1%7D(1/2)">.</li>
<li><strong>Example (Trimmed Mean)</strong> defines the <img src="https://latex.codecogs.com/png.latex?%5Calpha">-trimmed mean and its plug-in estimate.</li>
<li>The <img src="https://latex.codecogs.com/png.latex?p">th sample quantile is defined as <img src="https://latex.codecogs.com/png.latex?T(F%CC%82_n)%20=%20F%CC%82_n%5E%7B-1%7D(p)">.</li>
</ul></li>
<li><strong>7.3 Standard Errors and Confidence Intervals</strong>
<ul>
<li>Discusses obtaining standard errors and confidence intervals for nonparametric estimates.</li>
<li>Mentions that formulas will be developed for parametric methods, but nonparametric settings require something else.</li>
<li>The next chapter will introduce the bootstrap for getting standard errors and confidence intervals.</li>
<li><strong>Example (Plasma Cholesterol)</strong> uses histograms to compare plasma cholesterol levels between patients with and without heart disease. It raises the question of whether the mean cholesterol is different in the two groups.</li>
</ul></li>
</ul></li>
<li><strong>8. The Bootstrap</strong>
<ul>
<li><strong>8.1 Simulation</strong></li>
<li><strong>8.2 Bootstrap Variance Estimation</strong></li>
<li><strong>8.3 Bootstrap Confidence Intervals</strong></li>
<li><strong>8.4 Bibliographic Remarks</strong></li>
<li><strong>8.5 Appendix</strong>
<ul>
<li>8.5.1 The Jackknife</li>
<li>8.5.2 Justification For The Percentile Interval
<ul>
<li>The justification involves a monotone transformation <img src="https://latex.codecogs.com/png.latex?U%20=%20m(T)"> such that <img src="https://latex.codecogs.com/png.latex?U%20%5Csim%20N(%5Cphi,%20c%5E2)"> where <img src="https://latex.codecogs.com/png.latex?%5Cphi%20=%20m(%5Ctheta)">.</li>
<li>It discusses the relationship between the sample quantiles of the bootstrap replicates <img src="https://latex.codecogs.com/png.latex?U%5E%7B*b%7D"> and the quantiles of the Normal distribution.</li>
<li>It shows that <img src="https://latex.codecogs.com/png.latex?P(%5Ctheta%5E*_%7B%5Calpha/2%7D%20%5Cle%20%5Ctheta%20%5Cle%20%5Ctheta%5E*_%7B1-%5Calpha/2%7D)%20=%201%20-%20%5Calpha"> under the assumption of an exact normalizing transformation.</li>
<li>It notes that exact normalizing transformations rarely exist but approximate ones might.</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>9 Parametric Inference</strong>
<ul>
<li><strong>9.1 Parameter of Interest</strong>
<ul>
<li>Introduction to the idea of a parameter of interest in a statistical model.</li>
</ul></li>
<li><strong>9.2 The Method of Moments</strong>
<ul>
<li>Explanation of the method of moments for estimating parameters.</li>
</ul></li>
<li><strong>9.3 Maximum Likelihood</strong>
<ul>
<li>Introduction to the principle of maximum likelihood estimation (MLE).</li>
</ul></li>
<li><strong>9.4 Properties of Maximum Likelihood Estimators</strong>
<ul>
<li>Discussion of properties of MLEs.</li>
</ul></li>
<li><strong>9.5 Consistency of Maximum Likelihood Estimators</strong>
<ul>
<li>Conditions under which MLEs are consistent.</li>
<li>Hint for proving consistency for Uniform distribution using <img src="https://latex.codecogs.com/png.latex?Y%20=%20%5Cmax%5C%7BX_1,%20%5Cldots%20,%20X_n%5C%7D"> and <img src="https://latex.codecogs.com/png.latex?P(Y%20%3C%20c)%20=%20P(X_1%20%3C%20c)%5Cldots%20P(X_n%20%3C%20c)">.</li>
</ul></li>
<li><strong>9.6 Equivariance of the MLE</strong>
<ul>
<li>Property that the MLE of a function of a parameter is the function of the mle of the parameter.</li>
</ul></li>
<li><strong>9.7 Asymptotic Normality</strong>
<ul>
<li>Large sample behavior of MLEs tending towards a Normal distribution.</li>
</ul></li>
<li><strong>9.8 Optimality</strong>
<ul>
<li>Discussion of the optimality properties of MLEs.</li>
</ul></li>
<li><strong>9.9 The Delta Method</strong>
<ul>
<li>Method for finding the asymptotic distribution of a function of an estimator if the estimator is asymptotically Normal.</li>
<li>Approximation <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bn%7D(%5Chat%7B%5Ctheta%7D_n%20-%20%5Ctau)%20%5Capprox%20%5Csqrt%7Bn%7D(%5C%5Chat%7B%5Ctheta%7D_n%20-%20%5Ctheta)g'(%5Ctheta)"> where <img src="https://latex.codecogs.com/png.latex?%5Ctau%20=%20g(%5Chat%7B%5Ctheta%7D)"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D_n%20=%20g(%5Chat%7B%5Ctau%7D_a%20n)">.</li>
<li>Resulting asymptotic distribution <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_n%20%5Capprox%20N(%5Ctau,%20se%5E2(%5Chat%7B%5Ctheta%7D_n))"> with <img src="https://latex.codecogs.com/png.latex?se%5E2(%5Chat%7B%5Ctheta%7D_n)%20=%20(g'(%5Ctheta))%5E2%20/%20(nI(%5Ctheta))">.</li>
</ul></li>
<li><strong>9.10 Multiparameter Models</strong>
<ul>
<li>Extension of parametric inference to models with multiple parameters.</li>
</ul></li>
<li><strong>9.11 The Parametric Bootstrap</strong>
<ul>
<li>Using the fitted parametric model to generate bootstrap samples and estimate the sampling distribution of an estimator.</li>
</ul></li>
<li><strong>9.12 Checking Assumptions</strong>
<ul>
<li>Methods for verifying the assumptions of the parametric model.</li>
</ul></li>
<li><strong>9.13 Appendix</strong>
<ul>
<li><strong>9.13.1 Score Function</strong>
<ul>
<li>Definition and Lemma 9.31: <img src="https://latex.codecogs.com/png.latex?E_%5Ctheta%20%5Bs(X;%20%5Ctheta)%5D%20=%200">.</li>
</ul></li>
<li><strong>9.13.2 Sufficiency</strong>
<ul>
<li>Definition of a statistic <img src="https://latex.codecogs.com/png.latex?T(X%5En)"> and a sufficient statistic containing all information in the data.</li>
<li>Formal definition requiring that the conditional distribution of the data given the sufficient statistic does not depend on the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
<li><img src="https://latex.codecogs.com/png.latex?f(x_n;%20%5Ctheta)%20=%20h_n(x_n)%20g(T(x_n);%20%5Ctheta)"> for some functions <img src="https://latex.codecogs.com/png.latex?h_n"> and <img src="https://latex.codecogs.com/png.latex?g">.</li>
</ul></li>
<li><strong>9.13.3 Information Inequality (Cram√©r-Rao Lower Bound)</strong> (Not explicitly covered in the excerpts)</li>
<li><strong>9.13.4 Computing Maximum Likelihood Estimates</strong>
<ul>
<li>Discussion of analytical and numerical methods (Newton-Raphson, EM algorithm) for finding MLEs.</li>
<li><mark>The method of moments estimator as a good starting value</mark> for numerical methods.</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>10 Hypothesis Testing and p-values</strong>
<ul>
<li><strong>10.1 The Wald Test</strong>
<ul>
<li>Introduction of the Wald test.</li>
<li>Wald test statistic (form not explicitly given in the excerpts).</li>
<li>Mentioned in the context of multiple regression output where the ‚Äút-value‚Äù is the Wald test statistic for testing <img src="https://latex.codecogs.com/png.latex?H_0:%20%5Cbeta_j%20=%200"> versus <img src="https://latex.codecogs.com/png.latex?H_1:%20%5Cbeta_j%20%5Cneq%200">.</li>
</ul></li>
<li><strong>10.2 p-values</strong>
<ul>
<li>Definition and interpretation of p-values.</li>
<li>Mentioned in the context of hypothesis tests.</li>
<li>Example 10.28 showing ordered p-values from 10 independent hypothesis tests.</li>
</ul></li>
<li><strong>10.3 The <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> Distribution</strong>
<ul>
<li>Introduction of the <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> distribution.</li>
</ul></li>
<li><strong>10.4 Pearson‚Äôs <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> Test For Multinomial Data</strong>
<ul>
<li>Description of Pearson‚Äôs <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> test for multinomial data.</li>
</ul></li>
<li><strong>10.5 The Permutation Test</strong>
<ul>
<li>Explanation of the permutation test.</li>
</ul></li>
<li><strong>10.6 The Likelihood Ratio Test</strong>
<ul>
<li>Description of the likelihood ratio test.</li>
</ul></li>
<li><strong>10.7 Multiple Testing</strong>
<ul>
<li>Discussion of the challenges in multiple hypothesis testing.</li>
<li>Example 10.28 with ordered p-values.</li>
<li>Illustration of the Benjamini-Hochberg (BH) procedure in Figure 10.6.</li>
<li>Mention of Bonferroni testing where the rejection criterion is <img src="https://latex.codecogs.com/png.latex?P_i%20%3C%20%5Calpha/m">.</li>
<li>The BH procedure rejects when <img src="https://latex.codecogs.com/png.latex?P_i%20%5Cle%20T">, where <img src="https://latex.codecogs.com/png.latex?T"> corresponds to the rightmost undercrossing in Figure 10.6.</li>
</ul></li>
<li><strong>10.8 Goodness-of-fit Tests</strong>
<ul>
<li>Overview of goodness-of-fit tests.</li>
</ul></li>
</ul></li>
<li><strong>11 Bayesian Inference</strong>
<ul>
<li><strong>11.1 Introduction</strong>
<ul>
<li>Mention of Bayesian methods treating the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> as a random variable.</li>
<li>Focus on making probability statements about <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> given the data.</li>
<li>Contrast with frequentist methods where <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is fixed and unknown.</li>
</ul></li>
<li><strong>11.2 Bayes Theorem for Parameters</strong>
<ul>
<li>Posterior density <img src="https://latex.codecogs.com/png.latex?f(%5Ctheta%7CX_n)%20=%20%5Cfrac%7BL(%5Ctheta)f(%5Ctheta)%7D%7Bc%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?L(%5Ctheta)"> is the likelihood function.</li>
<li><img src="https://latex.codecogs.com/png.latex?f(%5Ctheta)"> is the prior density.</li>
<li><img src="https://latex.codecogs.com/png.latex?c%20=%20%5Cint%20L(%5Ctheta)f(%5Ctheta)%20d%5Ctheta"> is the normalizing constant.</li>
</ul></li>
<li><strong>11.3 Posterior Distributions</strong>
<ul>
<li>Example 11.3: Beta prior and Binomial likelihood leading to a Beta posterior for <img src="https://latex.codecogs.com/png.latex?p">.</li>
<li>Steps for approximating the posterior for <img src="https://latex.codecogs.com/png.latex?%5Cpsi%20=%20%5Clog(P/(1-P))"> without calculus:
<ol type="1">
<li>Draw <img src="https://latex.codecogs.com/png.latex?P_1,%20%5Cldots,%20P_B%20%5Csim%20Beta(s+1,%20n-s+1)">.</li>
<li>Let <img src="https://latex.codecogs.com/png.latex?%5Cpsi_i%20=%20%5Clog(P_i/(1-P_i))"> for <img src="https://latex.codecogs.com/png.latex?i%20=%201,%20%5Cldots,%20B">.</li>
</ol></li>
</ul></li>
<li><strong>11.4 Point Estimation</strong>
<ul>
<li>The Bayes estimator is often the posterior mean <img src="https://latex.codecogs.com/png.latex?E(%5Ctheta%20%5Cmid%20X_n)%20=%20%5Cint%20%5Ctheta%20f(%5Ctheta%20%5Cmid%20X_n)%20d%5Ctheta">.</li>
</ul></li>
<li><strong>11.5 Credible Sets</strong>
<ul>
<li>Definition of a credible set <img src="https://latex.codecogs.com/png.latex?C_n"> such that <img src="https://latex.codecogs.com/png.latex?P(%5Ctheta%20%5Cin%20C_n%20%5Cmid%20X_n)%20=%201%20-%20%5Calpha">.</li>
<li>Bayesian intervals refer to degree-of-belief probabilities about <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
<li>Bayesian intervals do not generally trap the parameter <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> percent of the time, unlike frequentist confidence intervals.</li>
</ul></li>
<li><strong>11.6 Bayesian Testing</strong> (Mentioned in the index)</li>
<li><strong>11.7 Strengths and Weaknesses of Bayesian Methods</strong> (Mentioned in the index)</li>
<li><strong>11.8 Asymptotic Approximations</strong>
<ul>
<li>Posterior of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is approximately Normal with mean <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_n"> (MLE) and variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2_n%20=%20-1/%5Cmathcal%7Bl%7D''(%5Chat%7B%5Ctheta%7D_n)%20%5Capprox%20(nI(%5Chat%7B%5Ctheta%7D_n))%5E%7B-1%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7Bl%7D%20=%20%5Clog%20f(X%7C%5Ctheta)"> is the log-likelihood.</li>
<li><img src="https://latex.codecogs.com/png.latex?I(%5Ctheta)"> is the Fisher information.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csigma_n%20%5Capprox%20se(%5Chat%7B%5Ctheta%7D)">.</li>
</ul></li>
<li><strong>11.9 Hierarchical Bayes Models</strong> (Not explicitly covered in the excerpts)</li>
<li><strong>11.10 Empirical Bayes</strong> (Not explicitly covered in the excerpts)</li>
<li><strong>11.11 Computation</strong>
<ul>
<li>Mention of simulation being important for Bayesian computation (as elaborated in Chapter 24).</li>
</ul></li>
</ul></li>
<li><strong>12 Decision Theory</strong>
<ul>
<li><strong>12.1 Introduction</strong>
<ul>
<li>Mention of statistical inference as leading to an estimate or a decision.</li>
<li>Concept of a loss function <img src="https://latex.codecogs.com/png.latex?L(%5Ctheta,%20%5Chat%7B%5Ctheta%7D)"> quantifying the loss of estimating <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> by <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D">.</li>
<li>Risk function <img src="https://latex.codecogs.com/png.latex?R(%5Ctheta,%20%5Chat%7B%5Ctheta%7D)%20=%20E_%7B%5Ctheta%7D(L(%5Ctheta,%20%5Chat%7B%5Ctheta%7D))">.</li>
<li>Example 12.1 with squared error loss <img src="https://latex.codecogs.com/png.latex?L(%5Ctheta,%20%5Chat%7B%5Ctheta%7D)%20=%20(%5Ctheta%20-%20%5Chat%7B%5Ctheta%7D)%5E2"> and risk being the mean squared error (MSE).</li>
<li>Example 12.2 comparing two estimators for a Normal mean with squared error loss.</li>
<li>Example 12.3 with Bernoulli data and squared error loss for estimating <img src="https://latex.codecogs.com/png.latex?p">.</li>
</ul></li>
<li><strong>12.2 Bayes Rules</strong>
<ul>
<li>Introduction of Bayesian decision theory.</li>
<li>Prior distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta)"> for the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
<li>Posterior distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta%7Cx%5En)">.</li>
<li>Bayes risk <img src="https://latex.codecogs.com/png.latex?r(%5Cpi,%20%5Chat%7B%5Ctheta%7D)%20=%20E_%7B%5Ctheta%7Cx%5En%7D(L(%5Ctheta,%20%5Chat%7B%5Ctheta%7D))%20=%20%5Cint%20L(%5Ctheta,%20%5Chat%7B%5Ctheta%7D)%20%5Cpi(%5Ctheta%7Cx%5En)%20d%5Ctheta">.</li>
<li>Bayes estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_%7B%5Cpi%7D"> that minimizes the Bayes risk.</li>
<li>With squared error loss, the Bayes estimator is the posterior mean <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_%7B%5Cpi%7D%20=%20E(%5Ctheta%7Cx%5En)">.</li>
<li>Example 12.4 finding the Bayes estimator for the mean of a Normal with a Normal prior under squared error loss.</li>
</ul></li>
<li><strong>12.3 Admissibility</strong>
<ul>
<li>Definition of an estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_1"> dominating <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_2"> if <img src="https://latex.codecogs.com/png.latex?R(%5Ctheta,%20%5Chat%7B%5Ctheta%7D_1)%20%5Cle%20R(%5Ctheta,%20%5Chat%7B%5Ctheta%7D_2)"> for all <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, with strict inequality for some <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
<li>Definition of an admissible estimator as one that is not dominated by any other estimator.</li>
<li>Connection between Bayes estimators and admissibility (without full coverage).</li>
<li>Mention that Bayes estimators with constant risk are minimax.</li>
</ul></li>
<li><strong>12.4 Minimax Rules</strong>
<ul>
<li>Definition of the maximum risk <img src="https://latex.codecogs.com/png.latex?M(%5Chat%7B%5Ctheta%7D)%20=%20%5Csup_%7B%5Ctheta%7D%20R(%5Ctheta,%20%5Chat%7B%5Ctheta%7D)">.</li>
<li>Definition of a minimax estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_%7Bminimax%7D"> that minimizes the maximum risk.</li>
<li>Bayes estimators with a constant risk function are minimax.</li>
</ul></li>
<li><strong>12.5 Point Estimation: A Summary</strong>
<ul>
<li>Recap of frequentist (bias, standard error, MSE) and Bayesian (prior, posterior, Bayes estimator, Bayes risk) approaches to point estimation.</li>
</ul></li>
<li><strong>12.6 Hypothesis Testing</strong> (Not covered in the provided excerpts for this chapter)</li>
<li><strong>12.7 Relationship between Confidence Sets and Hypothesis Tests</strong> (Not covered in the provided excerpts for this chapter)</li>
<li><strong>12.8 Bibliographic Remarks</strong>
<ul>
<li>References to books on decision theory.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="part-iii-statistical-models-and-methods" class="level3">
<h3 class="anchored" data-anchor-id="part-iii-statistical-models-and-methods">Part III: Statistical Models and Methods**</h3>
<ul>
<li><strong>13 Linear and Logistic Regression</strong>
<ul>
<li><strong>13.1 Simple Linear Regression</strong>
<ul>
<li>Introduces the basic concepts of simple linear regression.</li>
<li>Example 13.2 shows a plot of log surface temperature versus log light intensity for stars with an estimated linear regression line.</li>
<li>The model involves an intercept (<img src="https://latex.codecogs.com/png.latex?%5Cbeta_0">) and a slope (<img src="https://latex.codecogs.com/png.latex?%5Cbeta_1">).</li>
<li>The fitted line is given by <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D(x)%20=%20%5Chat%7B%5Cbeta%7D_0%20+%20%5Chat%7B%5Cbeta%7D_1x">.</li>
</ul></li>
<li><strong>13.2 Least Squares and Maximum Likelihood</strong>
<ul>
<li>Discusses methods for estimating the parameters of the linear regression model, including least squares and maximum likelihood.</li>
<li>The residual sum of squares (RSS) is defined as <img src="https://latex.codecogs.com/png.latex?RSS(%5Cbeta)%20=%20(Y%20-%20X%5Cbeta)%5ET%20(Y%20-%20X%5Cbeta)">.</li>
<li>The least squares estimator for <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is given by <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%20=%20(X%5ETX)%5E%7B-1%7DX%5ETY">.</li>
</ul></li>
<li><strong>13.3 Properties of the Least Squares Estimators</strong>
<ul>
<li>Examines the statistical properties of the least squares estimators.</li>
</ul></li>
<li><strong>13.4 Prediction</strong>
<ul>
<li>Covers how to use the fitted regression model for prediction.</li>
<li>Predicted values or fitted values are <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D_i%20=%20%5Chat%7Br%7D(X_i)">.</li>
<li>Residuals are defined as the difference between the observed and predicted values.</li>
<li>Example 13.5 shows the least squares estimates for the star data.</li>
<li>The variance of a prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%5E*"> at a new point <img src="https://latex.codecogs.com/png.latex?x%5E*"> is given by <img src="https://latex.codecogs.com/png.latex?%5Cxi%5E2_n%20=%20V(%5Chat%7BY%7D%5E*)%20+%20%5Csigma%5E2">.</li>
</ul></li>
<li><strong>13.5 Multiple Regression</strong>
<ul>
<li>Extends the linear regression model to include multiple covariates.</li>
<li>Example 13.6 illustrates multiple regression with the 2001 Presidential Election data in Florida, predicting Buchanan‚Äôs votes based on Bush‚Äôs votes.</li>
<li>The output of a multiple regression program typically includes coefficient estimates, standard errors, t-values (Wald test statistics), and p-values.</li>
</ul></li>
<li><strong>13.6 Model Selection</strong>
<ul>
<li>Addresses the problem of choosing which covariates to include in a multiple regression model.</li>
<li>Smaller models are more parsimonious and might give better predictions than larger models.</li>
<li>Adding more variables decreases bias but increases variance, leading to a bias-variance tradeoff.</li>
<li>Underfitting (too few covariates) leads to high bias, while overfitting (too many covariates) leads to high variance.</li>
<li>Example 13.14 (not explicitly shown in the excerpts but mentioned) illustrates the problem of having many covariates.</li>
<li>AIC (Akaike Information Criterion) is introduced as an approximately unbiased estimate of a measure of prediction accuracy.</li>
<li>Theorem 13.18 states that <img src="https://latex.codecogs.com/png.latex?AIC(M_j)"> is an approximately unbiased estimate of <img src="https://latex.codecogs.com/png.latex?a(f,%20%5Chat%7Bf%7D)"> (a measure of distance between the true density <img src="https://latex.codecogs.com/png.latex?f"> and the estimated density <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D"> for model <img src="https://latex.codecogs.com/png.latex?M_j">).</li>
</ul></li>
<li><strong>13.7 Logistic Regression</strong>
<ul>
<li>Introduces logistic regression for predicting a binary outcome variable.</li>
<li>The model is <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20P(Y%20=%201%7CX%20=%20x)%20=%20%5Cfrac%7Be%5E%7B%5Cbeta_0%20+%20%5Csum_j%20%5Cbeta_j%20x_j%7D%7D%7B1%20+%20e%5E%7B%5Cbeta_0%20+%20%5Csum_j%20%5Cbeta_j%20x_j%7D%7D">.</li>
<li>The maximum likelihood estimate (MLE) <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D"> is obtained numerically.</li>
<li>Example 13.17 (referenced in Example 22.11) likely provides an example of logistic regression.</li>
</ul></li>
<li><strong>13.8 Bibliographic Remarks</strong>
<ul>
<li>Provides references for further reading on linear and logistic regression.</li>
</ul></li>
</ul></li>
<li><strong>14 Multivariate Models</strong>
<ul>
<li><strong>14.1 Random Vectors</strong>
<ul>
<li>Introduces the concept of <strong>random vectors</strong>.</li>
<li>Discusses notation involving vectors <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D,%20%5Cmathbf%7By%7D"> and matrices <img src="https://latex.codecogs.com/png.latex?A">.</li>
</ul></li>
<li><strong>14.2 Estimating the Correlation</strong>
<ul>
<li>Covers methods for <strong>estimating the correlation</strong> between multiple variables.<br>
</li>
<li>Errata mentions a correction for equation (14.6), where the numerator should be divided by <img src="https://latex.codecogs.com/png.latex?n-1">, likely related to the sample covariance or correlation.</li>
</ul></li>
<li><strong>14.3 Multivariate Normal</strong>
<ul>
<li>Discusses the <strong>Multivariate Normal distribution</strong>.</li>
<li>The index mentions ‚Äúpositive definite‚Äù and a matrix <img src="https://latex.codecogs.com/png.latex?%5CSigma"> on p.231, which is a key property of the covariance matrix in a Multivariate Normal distribution.</li>
<li>Example 12.9 shows the Bayes estimator for the mean of a Normal distribution with a Normal prior.</li>
</ul></li>
<li><strong>14.4 Multinomial</strong>
<ul>
<li>Covers the <strong>Multinomial distribution</strong>.</li>
<li>Errata corrects a phrase in Section 14.4, changing ‚Äú3. ‚Äòballs of the kth color‚Äô‚Äù to ‚Äú3. ‚Äòballs of the jth color‚Äô,‚Äù indicating a discussion of categories within the Multinomial distribution.</li>
<li>The index mentions Pearson‚Äôs <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> test on p.&nbsp;241, which is commonly used for analyzing Multinomial data.</li>
</ul></li>
</ul></li>
<li><strong>15 Contingency Tables</strong>
<ul>
<li>Introduces the analysis of <strong>multivariate discrete data</strong>.</li>
<li>The data can be represented as <strong>counts in a <img src="https://latex.codecogs.com/png.latex?r_1%20%5Ctimes%20r_2%20%5Ctimes%20%5Ccdots%20%5Ctimes%20r_m"> table</strong>.</li>
<li><strong>15.1 Definition. The odds ratio</strong> is defined to be <img src="https://latex.codecogs.com/png.latex?%5Cpsi%20=%20%5Cfrac%7Bp_%7B00%7Dp_%7B11%7D%7D%7Bp_%7B01%7Dp_%7B10%7D%7D"> (Equation 15.1).</li>
<li><strong>The log odds ratio</strong> is defined to be <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20=%20%5Clog(%5Cpsi)"> (Equation 15.2).</li>
<li><strong>15.2 Theorem. The following statements are equivalent:</strong>
<ul>
<li><ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?Y%20%5Cperp%20Z"></li>
</ol></li>
<li><ol start="2" type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cpsi%20=%201"></li>
</ol></li>
<li><ol start="3" type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma%20=%200"></li>
</ol></li>
<li><ol start="4" type="1">
<li>For <img src="https://latex.codecogs.com/png.latex?i,%20j%20%5Cin%20%5C%7B0,%201%5C%7D">, <img src="https://latex.codecogs.com/png.latex?p_%7Bij%7D%20=%20p_%7Bi%20%5Ccdot%7Dp_%7B%5Ccdot%20j%7D">.</li>
</ol></li>
</ul></li>
<li>Discusses <strong>testing for independence</strong> (<img src="https://latex.codecogs.com/png.latex?H_0:%20Y%20%5Cperp%20Z"> versus <img src="https://latex.codecogs.com/png.latex?H_1:%20Y%20%5Cnot%5Cperp%20Z">).</li>
<li>Covers <strong>Prospective Sampling (Cohort Sampling)</strong>, where exposed and unexposed groups are observed to count the number with disease in each group.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X_%7B01%7D%20%5Csim%20Binomial(X_%7B0%5Ccdot%7D,%20P(D%7CE%5Ec))"></li>
<li><img src="https://latex.codecogs.com/png.latex?X_%7B11%7D%20%5Csim%20Binomial(X_%7B1%5Ccdot%7D,%20P(D%7CE))">.</li>
<li>Estimating <img src="https://latex.codecogs.com/png.latex?P(D%7CE)"> and <img src="https://latex.codecogs.com/png.latex?P(D%7CE%5Ec)"> is possible, and <img src="https://latex.codecogs.com/png.latex?%5Cpsi"> can be estimated as a function of these probabilities.</li>
</ul></li>
</ul></li>
<li><strong>16 Causal Inference</strong>
<ul>
<li>Introduction
<ul>
<li>The statement ‚Äú<strong><img src="https://latex.codecogs.com/png.latex?X"> causes <img src="https://latex.codecogs.com/png.latex?Y"></strong>‚Äù roughly means that changing the value of <img src="https://latex.codecogs.com/png.latex?X"> will change the distribution of <img src="https://latex.codecogs.com/png.latex?Y">.</li>
<li>When <img src="https://latex.codecogs.com/png.latex?X"> causes <img src="https://latex.codecogs.com/png.latex?Y">, <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> will be associated, but association does not necessarily imply causation.</li>
<li>The chapter discusses two frameworks for discussing causation: <strong>counterfactual random variables</strong> and <strong>directed acyclic graphs (DAGs)</strong> (presented in the next chapter).</li>
</ul></li>
<li><strong>16.1 The Counterfactual Model</strong>
<ul>
<li>Introduces the counterfactual model, where <img src="https://latex.codecogs.com/png.latex?X"> is a binary treatment variable (<img src="https://latex.codecogs.com/png.latex?X=1"> for treated, <img src="https://latex.codecogs.com/png.latex?X=0"> for not treated).</li>
<li>Introduces <strong>potential outcomes</strong> <img src="https://latex.codecogs.com/png.latex?(C_0,%20C_1)">:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?C_0">: the outcome if the subject is not treated (<img src="https://latex.codecogs.com/png.latex?X=0">).</li>
<li><img src="https://latex.codecogs.com/png.latex?C_1">: the outcome if the subject is treated (<img src="https://latex.codecogs.com/png.latex?X=1">).</li>
</ul></li>
<li>The observed outcome <img src="https://latex.codecogs.com/png.latex?Y"> is related to the potential outcomes by:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Y%20=%20C_1"> if <img src="https://latex.codecogs.com/png.latex?X%20=%201">.</li>
<li><img src="https://latex.codecogs.com/png.latex?Y%20=%20C_0"> if <img src="https://latex.codecogs.com/png.latex?X%20=%200">.</li>
</ul></li>
<li>The <strong>causal effect</strong> of the treatment for a single subject is <img src="https://latex.codecogs.com/png.latex?C_1%20-%20C_0">.</li>
<li>The <strong>average causal effect (ACE)</strong> or <strong>average treatment effect (ATE)</strong> is <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20E(C_1%20-%20C_0)%20=%20E(C_1)%20-%20E(C_0)">.</li>
<li>It is impossible to observe both <img src="https://latex.codecogs.com/png.latex?C_0"> and <img src="https://latex.codecogs.com/png.latex?C_1"> for the same subject, leading to the ‚Äú<strong>fundamental problem of causal inference</strong>‚Äù.</li>
</ul></li>
<li><strong>16.2 Identifiability</strong>
<ul>
<li>Discusses conditions under which causal effects can be estimated from observed data.</li>
<li>If treatment <img src="https://latex.codecogs.com/png.latex?X"> is randomly assigned and independent of <img src="https://latex.codecogs.com/png.latex?(C_0,%20C_1)">, then:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?E(C_1)%20=%20E(Y%7CX=1)">.</li>
<li><img src="https://latex.codecogs.com/png.latex?E(C_0)%20=%20E(Y%7CX=0)">.</li>
<li>In this case, <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20E(Y%7CX=1)%20-%20E(Y%7CX=0)"> can be estimated.</li>
</ul></li>
<li>Example 16.2 (not fully described) is mentioned in an exercise.</li>
<li>Theorem 16.4 is mentioned in an exercise requiring proof.</li>
</ul></li>
<li><strong>16.3 Observational Studies</strong>
<ul>
<li>Discusses the challenges of estimating causal effects in observational studies where treatment assignment is not random.</li>
<li>In observational studies, <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?(C_0,%20C_1)"> may be dependent due to <strong>confounding variables</strong>.</li>
<li>Figure 17.11 illustrates observational studies with measured and unmeasured confounders, indicating the relevance of DAGs (from Chapter 17) to this topic.</li>
</ul></li>
<li><strong>16.4 Instrumental Variables</strong> (Not explicitly detailed in the excerpts but likely covered)</li>
</ul></li>
<li><strong>17 Directed Graphs and Conditional Independence</strong>
<ul>
<li><strong>17.1 Introduction</strong>
<ul>
<li>Introduces <strong>directed graphs</strong> consisting of nodes and arrows.</li>
<li>Graphs are useful for <strong>representing independence relations between variables</strong>.</li>
<li>They can also be used as an alternative to counterfactuals to <strong>represent causal relationships</strong>.</li>
<li>The term ‚ÄúBayesian network‚Äù is sometimes used for a directed graph with a probability distribution, but this is considered poor terminology.</li>
<li>Statistical inference for directed graphs can be performed.</li>
</ul></li>
<li><strong>17.2 Conditional Independence</strong>
<ul>
<li>Discusses the concept of <strong>conditional independence</strong>.</li>
</ul></li>
<li><strong>17.3 DAGs</strong>
<ul>
<li>Introduces <strong>Directed Acyclic Graphs (DAGs)</strong>.</li>
</ul></li>
<li><strong>17.4 Probability and DAGs</strong>
<ul>
<li>Explains the relationship between <strong>probability distributions and DAGs</strong>.</li>
<li>For a distribution consistent with a DAG, the probability function can be factored according to the graph structure, e.g., <img src="https://latex.codecogs.com/png.latex?f(x,%20y,%20z)%20=%20f(x)f(y%7Cx)f(z%7Cx,%20y)"> for the DAG <img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20Y%20%5Crightarrow%20Z">.</li>
</ul></li>
<li><strong>17.5 More Independence Relations</strong>
<ul>
<li>Explores further <strong>independence relations</strong> implied by DAGs.</li>
<li>Introduces the concept of an <strong>unshielded collider</strong>.</li>
</ul></li>
<li><strong>17.6 Estimation for DAGs</strong>
<ul>
<li>Briefly mentions the two main estimation questions for DAGs:
<ul>
<li>Estimating the distribution <img src="https://latex.codecogs.com/png.latex?f"> given a DAG <img src="https://latex.codecogs.com/png.latex?G"> and data.</li>
<li>Estimating the DAG <img src="https://latex.codecogs.com/png.latex?G"> given data.</li>
</ul></li>
<li>Notes that these are involved topics beyond the scope of the book.</li>
</ul></li>
<li><strong>17.7 Bibliographic Remarks</strong>
<ul>
<li>Provides references for further reading on DAGs and their applications. Mentions texts by Edwards (1995) and Jordan (2004), as well as the early work by Wright (1934) on causal DAGs and modern treatments by Spirtes et al.&nbsp;(2000) and Pearl (2000). Also notes discussions on the challenges of estimating causal structure from data by Robins et al.&nbsp;(2003).</li>
</ul></li>
<li><strong>17.8 Appendix</strong>
<ul>
<li><strong>Causation Revisited</strong>: Discusses causation using DAGs as an alternative to counterfactual random variables (from Chapter 16), noting that the two approaches are mathematically equivalent.</li>
<li>Introduces the idea of <strong>intervention</strong> in the context of DAGs.</li>
<li>Provides pseudocode for generating data from a distribution consistent with a DAG.</li>
<li>Illustrates different study designs (randomized study, observational study with measured confounders, observational study with unmeasured confounders) using DAGs in Figure 17.11.</li>
<li>Mentions a way to make the correspondence between the DAG approach and the counterfactual approach explicit using a variable <img src="https://latex.codecogs.com/png.latex?Z"> defined based on <img src="https://latex.codecogs.com/png.latex?(C_0,%20C_1)"> from Chapter 16.</li>
</ul></li>
<li><strong>Theorem 17.13</strong> states that two DAGs <img src="https://latex.codecogs.com/png.latex?G_1"> and <img src="https://latex.codecogs.com/png.latex?G_2"> are Markov equivalent if and only if (i) skeleton(<img src="https://latex.codecogs.com/png.latex?G_1">) = skeleton(<img src="https://latex.codecogs.com/png.latex?G_2">) and (ii) <img src="https://latex.codecogs.com/png.latex?G_1"> and <img src="https://latex.codecogs.com/png.latex?G_2"> have the same unshielded colliders.</li>
<li><strong>Example 17.14</strong> illustrates Markov equivalent DAGs.</li>
</ul></li>
<li><strong>18 Undirected Graphs</strong>
<ul>
<li><strong>18.1 Undirected Graphs</strong>
<ul>
<li>Definition of an <strong>undirected graph</strong> <img src="https://latex.codecogs.com/png.latex?G%20=%20(V,%20E)"> with a finite set of <strong>vertices</strong> <img src="https://latex.codecogs.com/png.latex?V"> and a set of <strong>edges</strong> <img src="https://latex.codecogs.com/png.latex?E"> (unordered pairs of vertices).</li>
<li>Vertices correspond to random variables (<img src="https://latex.codecogs.com/png.latex?X,%20Y,%20Z,%20%5Cldots">).</li>
<li>An edge <img src="https://latex.codecogs.com/png.latex?(X,%20Y)%20%5Cin%20E"> means <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are joined by an edge, written as <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20Y"> if <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are <strong>adjacent</strong>.</li>
<li>Definition of a <strong>path</strong> as a sequence <img src="https://latex.codecogs.com/png.latex?X_0,%20%5Cldots,%20X_n"> where <img src="https://latex.codecogs.com/png.latex?X_%7Bi-1%7D%20%5Csim%20X_i"> for each <img src="https://latex.codecogs.com/png.latex?i">.</li>
<li>Definition of a <strong>complete graph</strong> (an edge between every pair of vertices).</li>
<li>Definition of a <strong>subgraph</strong> (a subset <img src="https://latex.codecogs.com/png.latex?U%20%5Csubset%20V"> of vertices together with their edges).</li>
<li>Definition of <strong>separation</strong>: <img src="https://latex.codecogs.com/png.latex?C"> separates <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> (three distinct subsets of <img src="https://latex.codecogs.com/png.latex?V">) if every path from a variable in <img src="https://latex.codecogs.com/png.latex?A"> to a variable in <img src="https://latex.codecogs.com/png.latex?B"> intersects a variable in <img src="https://latex.codecogs.com/png.latex?C">.</li>
</ul></li>
<li><strong>18.2 Probability and Graphs</strong>
<ul>
<li>Constructing a graph based on conditional independence: no edge between <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> <img src="https://latex.codecogs.com/png.latex?%5Ciff"> <img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y%20%7C%20%5Ctext%7Brest%7D"> (where ‚Äúrest‚Äù refers to all other variables).</li>
<li>The resulting graph is called a <strong>pairwise Markov graph</strong>.</li>
<li><strong>Global Markov property</strong>: if <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> are separated by <img src="https://latex.codecogs.com/png.latex?C">, then <img src="https://latex.codecogs.com/png.latex?X_A%20%5Cperp%20X_B%20%7C%20X_C">.</li>
<li><strong>Theorem 18.3</strong>: The pairwise Markov property and the global Markov property are equivalent: <img src="https://latex.codecogs.com/png.latex?M_%7Bpair%7D(G)%20=%20M_%7Bglobal%7D(G)">.</li>
<li>Examples of independence relations implied by different graph structures (Figures 18.3, 18.4, 18.5, 18.6, 18.7, 18.8).</li>
<li><strong>Example 18.4</strong>: Figure 18.7 implies <img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y">, <img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Z">, and <img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20(Y,%20Z)">.</li>
<li><strong>Example 18.5</strong>: Figure 18.8 implies <img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20W%20%7C%20(Y,%20Z)"> and <img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Z%20%7C%20Y">.</li>
</ul></li>
<li><strong>18.3 Cliques and Potentials</strong>
<ul>
<li>Definition of a <strong>clique</strong>: a set of variables in a graph that are all adjacent to each other.</li>
<li>Definition of a <strong>maximal clique</strong>: a clique where it‚Äôs not possible to include another variable and still be a clique.</li>
<li>Definition of a <strong>potential</strong>: any positive function.</li>
<li>The probability function <img src="https://latex.codecogs.com/png.latex?f(x)"> for a distribution <img src="https://latex.codecogs.com/png.latex?P"> that is Markov to a graph <img src="https://latex.codecogs.com/png.latex?G"> can be written as <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B1%7D%7BZ%7D%20%5Cprod_%7BC%20%5Cin%20%5Cmathcal%7BC%7D%7D%20%5Cpsi_C(x_C)"> (Equation 18.1), where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D"> is the set of maximal cliques and <img src="https://latex.codecogs.com/png.latex?%5Cpsi_C(x_C)"> are positive potential functions. <img src="https://latex.codecogs.com/png.latex?Z%20=%20%5Csum_x%20%5Cprod_%7BC%20%5Cin%20%5Cmathcal%7BC%7D%7D%20%5Cpsi_C(x_C)"> is the normalization constant.</li>
<li><strong>Example 18.6</strong>: For the graph in Figure 18.1, maximal cliques are <img src="https://latex.codecogs.com/png.latex?%5C%7BX,%20Y%5C%7D"> and <img src="https://latex.codecogs.com/png.latex?%5C%7BY,%20Z%5C%7D">, so <img src="https://latex.codecogs.com/png.latex?f(x,%20y,%20z)%20%5Cpropto%20%5Cpsi_1(x,%20y)%20%5Cpsi_2(y,%20z)">.</li>
<li><strong>Example 18.7</strong>: Lists the maximal cliques for the graph in Figure 18.9 and gives the form of the probability function.</li>
</ul></li>
<li><strong>18.4 Fitting Graphs to Data</strong>
<ul>
<li>Briefly mentions the problem of finding a graphical model that fits a given data set.</li>
<li>In the discrete case, one way to fit a graph to data is to use a <strong>log-linear model</strong> (the subject of Chapter 19).</li>
</ul></li>
<li><strong>18.5 Bibliographic Remarks</strong>
<ul>
<li>References thorough treatments of undirected graphs, including books by Whittaker (1990) and Lauritzen (1996).</li>
</ul></li>
</ul></li>
<li><strong>19 Log-Linear Models</strong>
<ul>
<li>Introduction to <strong>log-linear models</strong> for modeling multivariate discrete data.</li>
<li>Strong connection between log-linear models and undirected graphs.</li>
<li><strong>19.1 The Log-Linear Model</strong>
<ul>
<li>Let <img src="https://latex.codecogs.com/png.latex?X%20=%20(X_1,%20%5Cldots,%20X_m)"> be a discrete random vector with probability function <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20P(X%20=%20x)%20=%20P(X_1%20=%20x_1,%20%5Cldots,%20X_m%20=%20x_m)">.</li>
<li><img src="https://latex.codecogs.com/png.latex?X_j"> takes <img src="https://latex.codecogs.com/png.latex?r_j"> values, assumed to be in <img src="https://latex.codecogs.com/png.latex?%5C%7B0,%201,%20%5Cldots,%20r_j%20-%201%5C%7D"> without loss of generality.</li>
<li>Data can be seen as a sample from a Multinomial distribution with <img src="https://latex.codecogs.com/png.latex?N%20=%20r_1%20%5Ctimes%20r_2%20%5Ctimes%20%5Ccdots%20%5Ctimes%20r_m"> categories.</li>
<li><img src="https://latex.codecogs.com/png.latex?p%20=%20(p_1,%20%5Cldots,%20p_N)"> denotes the multinomial parameter.</li>
<li>Definition of <img src="https://latex.codecogs.com/png.latex?x_A%20=%20(x_j%20:%20j%20%5Cin%20A)"> for a subset <img src="https://latex.codecogs.com/png.latex?A%20%5Csubset%20S%20=%20%5C%7B1,%20%5Cldots,%20m%5C%7D">.</li>
<li><strong>Theorem 19.1</strong>: The joint probability function <img src="https://latex.codecogs.com/png.latex?f(x)"> can be written as <img src="https://latex.codecogs.com/png.latex?%5Clog%20f(x)%20=%20%5Csum_%7BA%20%5Csubset%20S%7D%20%5Cpsi_A(x)"> (Equation 19.1), where <img src="https://latex.codecogs.com/png.latex?%5Cpsi_A(x)"> only depends on <img src="https://latex.codecogs.com/png.latex?x_A"> and <img src="https://latex.codecogs.com/png.latex?%5Cpsi_%5Cemptyset(x)"> is a constant.</li>
<li>The terms <img src="https://latex.codecogs.com/png.latex?%5Cpsi_A(x)"> are called the <strong>log-linear parameters</strong> or <strong>interactions</strong>.</li>
<li><strong>Example 19.2</strong>: Log-linear model for a Bernoulli random variable.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5C%7B0,%201%5C%7D"> with <img src="https://latex.codecogs.com/png.latex?P(X%20=%201)%20=%20p_1"> and <img src="https://latex.codecogs.com/png.latex?P(X%20=%200)%20=%20p_2%20=%201%20-%20p_1">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Clog%20f(x)%20=%20%5Cpsi_%5Cemptyset(x)%20+%20%5Cpsi_%7B%5C%7B1%5C%7D%7D(x)">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpsi_%5Cemptyset(x)%20=%20%5Clog(p_2)"> and <img src="https://latex.codecogs.com/png.latex?%5Cpsi_%7B%5C%7B1%5C%7D%7D(x)%20=%20x%20%5Clog(%5Cfrac%7Bp_1%7D%7Bp_2%7D)">.</li>
</ul></li>
<li><strong>Example 19.3</strong>: Log-linear model for <img src="https://latex.codecogs.com/png.latex?X%20=%20(X_1,%20X_2)"> where <img src="https://latex.codecogs.com/png.latex?X_1%20%5Cin%20%5C%7B0,%201%5C%7D"> and <img src="https://latex.codecogs.com/png.latex?X_2%20%5Cin%20%5C%7B0,%201,%202%5C%7D">, connected to a multinomial with 6 categories.</li>
</ul></li>
<li><strong>19.2 Graphical Log-Linear Models</strong>
<ul>
<li>Connection between log-linear models and undirected graphs: a log-linear model is graphical with respect to a graph <img src="https://latex.codecogs.com/png.latex?G"> if <img src="https://latex.codecogs.com/png.latex?%5Cpsi_A(x)%20=%200"> whenever the indices in <img src="https://latex.codecogs.com/png.latex?A"> form a set that is not a clique in <img src="https://latex.codecogs.com/png.latex?G">.</li>
<li><strong>Theorem 19.4</strong>: Let <img src="https://latex.codecogs.com/png.latex?G"> be an undirected graph with vertex set <img src="https://latex.codecogs.com/png.latex?S%20=%20%5C%7B1,%20%5Cldots,%20m%5C%7D">. A probability distribution <img src="https://latex.codecogs.com/png.latex?f"> factors according to <img src="https://latex.codecogs.com/png.latex?G"> (i.e., <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Cfrac%7B1%7D%7BZ%7D%20%5Cprod_%7BC%20%5Cin%20%5Cmathcal%7BC%7D%7D%20%5Cpsi_C(x_C)"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D"> is the set of maximal cliques) if and only if its log-linear expansion <img src="https://latex.codecogs.com/png.latex?%5Clog%20f(x)%20=%20%5Csum_%7BA%20%5Csubset%20S%7D%20%5Cpsi_A(x)"> satisfies <img src="https://latex.codecogs.com/png.latex?%5Cpsi_A%20=%200"> whenever <img src="https://latex.codecogs.com/png.latex?A"> is not a clique in <img src="https://latex.codecogs.com/png.latex?G">.</li>
<li><strong>Lemma 19.5</strong>: A partition <img src="https://latex.codecogs.com/png.latex?(X_a,%20X_b,%20X_c)"> satisfies <img src="https://latex.codecogs.com/png.latex?X_b%20%5Cperp%20X_c%20%7C%20X_a"> if and only if <img src="https://latex.codecogs.com/png.latex?f(x_a,%20x_b,%20x_c)%20=%20g(x_a,%20x_b)h(x_a,%20x_c)"> for some functions <img src="https://latex.codecogs.com/png.latex?g"> and <img src="https://latex.codecogs.com/png.latex?h">.</li>
<li>Proof of Theorem 19.4 using Lemma 19.5.</li>
<li>If a term can be added to the model without changing the graph, the model is not graphical.</li>
<li><strong>Example 19.6</strong>: Graphical log-linear model corresponding to a path graph.</li>
<li><strong>Example 19.7</strong>: Graphical log-linear model corresponding to a more complex graph (Figure 19.1).</li>
</ul></li>
<li><strong>19.3 Hierarchical Log-Linear Models</strong>
<ul>
<li>Definition of <strong>hierarchical log-linear models</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cpsi_A%20=%200"> and <img src="https://latex.codecogs.com/png.latex?A%20%5Csubset%20B"> implies that <img src="https://latex.codecogs.com/png.latex?%5Cpsi_B%20=%200">.</li>
<li><strong>Lemma 19.9</strong>: A graphical model is hierarchical, but the reverse is not necessarily true.</li>
<li><strong>Example 19.10</strong>: A hierarchical and graphical model.</li>
<li><strong>Example 19.11</strong>: A hierarchical but not graphical model (complete graph).</li>
<li><strong>Example 19.12</strong>: A model that is not hierarchical and therefore not graphical.</li>
</ul></li>
<li><strong>19.4 Model Generators</strong>
<ul>
<li>Hierarchical models can be written succinctly using <strong>generators</strong>.</li>
<li>Example: <img src="https://latex.codecogs.com/png.latex?M%20=%201.2%20+%201.3"> stands for <img src="https://latex.codecogs.com/png.latex?%5Clog%20f%20=%20%5Cpsi_%5Cemptyset%20+%20%5Cpsi_1%20+%20%5Cpsi_2%20+%20%5Cpsi_3%20+%20%5Cpsi_%7B12%7D%20+%20%5Cpsi_%7B13%7D">.</li>
<li>The generator <img src="https://latex.codecogs.com/png.latex?M%20=%201.2.3"> is the saturated model.</li>
</ul></li>
<li><strong>19.5 Fitting Log-Linear Models to Data</strong>
<ul>
<li>Given <img src="https://latex.codecogs.com/png.latex?n"> random vectors from a multinomial distribution, the log-likelihood function can be expressed in terms of the log-linear parameters <img src="https://latex.codecogs.com/png.latex?%5Cbeta">.</li>
<li>Maximum likelihood estimates (mle) <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D"> generally found numerically.</li>
<li>Fisher information matrix also found numerically, allowing estimation of standard errors.</li>
<li>Model selection problem: which <img src="https://latex.codecogs.com/png.latex?%5Cpsi"> terms to include, similar to variable selection in linear regression.</li>
</ul></li>
</ul></li>
<li><strong>20 Nonparametric Curve Estimation</strong>
<ul>
<li>Discussion of <strong>nonparametric estimation</strong> of probability density functions and regression functions (curve estimation or smoothing).</li>
<li>Contrast with estimating cumulative distribution functions, which can be done consistently without smoothness assumptions.</li>
<li>Need for <strong>smoothing</strong> due to bias-variance tradeoff.</li>
<li><strong>20.1 The Bias-Variance Tradeoff</strong>
<ul>
<li>Fundamental challenge in curve estimation: balancing <strong>bias</strong> and <strong>variance</strong>.</li>
<li>Oversmoothing leads to high bias and low variance.</li>
<li>Undersmoothing leads to low bias and high variance.</li>
</ul></li>
<li><strong>20.2 Histograms</strong>
<ul>
<li>Histogram as an example of a density estimator.</li>
<li>Dividing the real line into disjoint sets called <strong>bins</strong>.</li>
<li>Histogram estimator is a piecewise constant function with height proportional to the number of observations in each bin.</li>
<li><strong>Smoothing parameter</strong>: number of bins (or binwidth).</li>
<li>Formal definition for iid data on $$ with density <img src="https://latex.codecogs.com/png.latex?f">, using <img src="https://latex.codecogs.com/png.latex?m"> bins <img src="https://latex.codecogs.com/png.latex?B_j"> of width <img src="https://latex.codecogs.com/png.latex?h%20=%201/m"> (Equation 20.7).</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cnu_j">: number of observations in <img src="https://latex.codecogs.com/png.latex?B_j">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D_j%20=%20%5Cnu_j%20/%20n">.</li>
<li><img src="https://latex.codecogs.com/png.latex?p_j%20=%20%5Cint_%7BB_j%7D%20f(u)%20du">.</li>
<li><strong>Histogram estimator</strong>: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)%20=%20%5Chat%7Bp%7D_j%20/%20h"> for <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20B_j"> (Equation 20.8).</li>
<li><strong>Theorem 20.3</strong>: Mean and variance of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)">:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?E(%5Chat%7Bf%7D_n(x))%20=%20p_j%20/%20h"> (Equation 20.9).</li>
<li><img src="https://latex.codecogs.com/png.latex?V(%5Chat%7Bf%7D_n(x))%20=%20p_j(1%20-%20p_j)%20/%20(nh%5E2)"> (Equation 20.9).</li>
</ul></li>
<li>Integrated Squared Error (ISE) as a measure of risk: <img src="https://latex.codecogs.com/png.latex?R(f,%20%5Chat%7Bf%7D_n)%20=%20%5Cint%20(f(x)%20-%20%5Chat%7Bf%7D_n(x))%5E2%20dx"> (Equation 20.10).</li>
<li>Mean Integrated Squared Error (MISE): <img src="https://latex.codecogs.com/png.latex?E%5BR(f,%20%5Chat%7Bf%7D_n)%5D%20=%20%5Cint%20E%5B(f(x)%20-%20%5Chat%7Bf%7D_n(x))%5E2%5D%20dx%20=%20%5Cint%20Bias%5E2(%5Chat%7Bf%7D_n(x))%20dx%20+%20%5Cint%20Var(%5Chat%7Bf%7D_n(x))%20dx">.</li>
<li>Approximate MISE for histograms (Equation 20.11).</li>
<li>Optimal bandwidth <img src="https://latex.codecogs.com/png.latex?h_%7Bopt%7D%20%5Cpropto%20n%5E%7B-1/3%7D"> and optimal number of bins <img src="https://latex.codecogs.com/png.latex?m_%7Bopt%7D%20%5Cpropto%20n%5E%7B1/3%7D"> (Equation 20.12).</li>
<li>Cross-validation for choosing the number of bins.</li>
<li>Cross-validation score <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)"> (Equation 20.13).</li>
<li><strong>Theorem 20.7</strong>: Identity for <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)">:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Cfrac%7B2%7D%7B(n-1)h%7D%20-%20%5Cfrac%7Bn+1%7D%7Bh(n-1)%7D%20%5Csum_%7Bj=1%7D%5E%7Bm%7D%20%5Chat%7Bp%7D_j%5E2"> (Equation 20.14).</li>
</ul></li>
<li><strong>Example 20.8</strong>: Cross-validation for astronomy data.</li>
<li>Confidence bands for histograms.</li>
<li><strong>Theorem 20.10</strong>: Approximate <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> confidence band <img src="https://latex.codecogs.com/png.latex?(%5Chat%7Bl%7D_n(x),%20%5Chat%7Bu%7D_n(x))"> where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bl%7D_n(x)%20=%20(%5Cmax%5C%7B%5Csqrt%7B%5Chat%7Bf%7D_n(x)%7D%20-%20c,%200%20%5C%7D)%5E2"> (Equation 20.17).</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bu%7D_n(x)%20=%20(%5Csqrt%7B%5Chat%7Bf%7D_n(x)%7D%20+%20c)%5E2"> (Equation 20.17).</li>
<li><img src="https://latex.codecogs.com/png.latex?c%20=%20z_%7B%5Calpha/(2m)%7D%20%5Cfrac%7B%5Csqrt%7Bm%7D%7D%7B%5Csqrt%7Bn%7D%7D"> (Equation 20.18).</li>
</ul></li>
</ul></li>
<li><strong>20.3 Kernel Density Estimation</strong>
<ul>
<li>Kernel density estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)%20=%20%5Cfrac%7B1%7D%7Bnh%7D%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20K(%5Cfrac%7Bx%20-%20X_i%7D%7Bh%7D)"> (Equation 20.20).</li>
<li><img src="https://latex.codecogs.com/png.latex?K">: kernel function.</li>
<li><img src="https://latex.codecogs.com/png.latex?h">: bandwidth (smoothing parameter).</li>
<li><strong>Example 20.9</strong>: Gaussian kernel.</li>
<li>Expectation and variance of kernel density estimator (Equations 20.21 and 20.22).</li>
<li>Approximate bias and variance (Equations 20.23 and 20.24).</li>
<li>Approximate MISE for kernel density estimation (Equation 20.25).</li>
<li>Optimal bandwidth <img src="https://latex.codecogs.com/png.latex?h_%7Bopt%7D%20%5Cpropto%20n%5E%7B-1/5%7D"> (Equation 20.26).</li>
<li>Choice of kernel function is less critical than the choice of bandwidth.</li>
<li>Cross-validation for bandwidth selection.</li>
</ul></li>
<li><strong>20.4 Nonparametric Regression</strong>
<ul>
<li>Model: <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20r(x_i)%20+%20%5Cepsilon_i">, where <img src="https://latex.codecogs.com/png.latex?E(%5Cepsilon_i)%20=%200">.</li>
<li>Goal: estimate the regression function <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20E(Y%20%7C%20X%20=%20x)">.</li>
<li><strong>Nadaraya-Watson estimator</strong>: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D(x)%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20K(%5Cfrac%7Bx%20-%20x_i%7D%7Bh%7D)%20Y_i%7D%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20K(%5Cfrac%7Bx%20-%20x_i%7D%7Bh%7D)%7D"> (Equation 20.27).</li>
<li>Local constant estimator.</li>
<li>Local linear estimator (Equation 20.28).</li>
<li>Choice of bandwidth <img src="https://latex.codecogs.com/png.latex?h"> using cross-validation (Equation 20.29).</li>
<li>Risk function for regression (Equation 20.30).</li>
<li>Leave-one-out cross-validation score <img src="https://latex.codecogs.com/png.latex?%5Chat%7BV%7D(h)"> (Equation 20.31).</li>
<li>Confidence bands for nonparametric regression.</li>
<li>Estimating <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20=%20Var(%5Cepsilon_i)"> using differences of adjacent <img src="https://latex.codecogs.com/png.latex?Y_i">‚Äôs (Equation 20.36).</li>
<li>Approximate point-wise confidence interval (Equation 20.37).</li>
<li>Simultaneous confidence bands (Equation 20.38).</li>
<li><strong>Example 20.11</strong>: Nonparametric regression for galaxy data.</li>
</ul></li>
</ul></li>
<li><strong>21 Smoothing Using Orthogonal Functions</strong>
<ul>
<li>Introduction to nonparametric curve estimation using orthogonal functions.</li>
<li>Brief introduction to the theory of orthogonal functions.</li>
<li>Application to density estimation and regression.</li>
<li><strong>21.1 Orthogonal Functions and <img src="https://latex.codecogs.com/png.latex?L%5E2"> Spaces</strong>
<ul>
<li>Definition of <img src="https://latex.codecogs.com/png.latex?L%5E2(a,%20b)"> as the space of square integrable functions on <img src="https://latex.codecogs.com/png.latex?%5Ba,%20b%5D">, where <img src="https://latex.codecogs.com/png.latex?%5Cint_a%5Eb%20f(x)%5E2%20dx%20%3C%20%5Cinfty">.</li>
<li>Inner product of two functions <img src="https://latex.codecogs.com/png.latex?f,%20g%20%5Cin%20L%5E2(a,%20b)">: <img src="https://latex.codecogs.com/png.latex?%5Clangle%20f,%20g%20%5Crangle%20=%20%5Cint_a%5Eb%20f(x)g(x)%20dx">.</li>
<li>Norm of a function: <img src="https://latex.codecogs.com/png.latex?%7C%7Cf%7C%7C%20=%20%5Csqrt%7B%5Clangle%20f,%20f%20%5Crangle%7D%20=%20(%5Cint_a%5Eb%20f(x)%5E2%20dx)%5E%7B1/2%7D">.</li>
<li>Orthogonal functions: <img src="https://latex.codecogs.com/png.latex?%5Clangle%20%5Cphi_j,%20%5Cphi_k%20%5Crangle%20=%200"> for <img src="https://latex.codecogs.com/png.latex?j%20%5Cneq%20k">.</li>
<li>Orthonormal functions: <img src="https://latex.codecogs.com/png.latex?%5Clangle%20%5Cphi_j,%20%5Cphi_k%20%5Crangle%20=%20%5Cdelta_%7Bjk%7D"> (Kronecker delta, 1 if <img src="https://latex.codecogs.com/png.latex?j=k">, 0 otherwise).</li>
<li>Any <img src="https://latex.codecogs.com/png.latex?f%20%5Cin%20L%5E2(a,%20b)"> can be expanded as <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Csum_%7Bj=1%7D%5E%5Cinfty%20%5Cbeta_j%20%5Cphi_j(x)">, where <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%20=%20%5Clangle%20f,%20%5Cphi_j%20%5Crangle%20=%20%5Cint_a%5Eb%20f(x)%20%5Cphi_j(x)%20dx"> (Equation 21.3).</li>
<li>Parseval‚Äôs relation: <img src="https://latex.codecogs.com/png.latex?%7C%7Cf%7C%7C%5E2%20=%20%5Csum_%7Bj=1%7D%5E%5Cinfty%20%5Cbeta_j%5E2"> (Equation 21.6).</li>
<li>Example 21.1: Cosine basis on $$: <img src="https://latex.codecogs.com/png.latex?%5Cphi_0(x)%20=%201">, <img src="https://latex.codecogs.com/png.latex?%5Cphi_j(x)%20=%20%5Csqrt%7B2%7D%20%5Ccos(j%20%5Cpi%20x)"> for <img src="https://latex.codecogs.com/png.latex?j%20%5Cge%201">.</li>
<li>Example 21.2: Doppler function <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Csqrt%7Bx(1-x)%7D%20%5Csin(%5Cfrac%7B2.1%20%5Cpi%7D%7Bx%20+%200.05%7D)"> and its approximation using a cosine basis <img src="https://latex.codecogs.com/png.latex?f_J(x)%20=%20%5Csum_%7Bj=1%7D%5EJ%20%5Cbeta_j%20%5Cphi_j(x)">.</li>
<li>Example 21.3: Legendre polynomials <img src="https://latex.codecogs.com/png.latex?P_j(x)%20=%20%5Cfrac%7B1%7D%7B2%5Ej%20j!%7D%20%5Cfrac%7Bd%5Ej%7D%7Bdx%5Ej%7D%20(x%5E2%20-%201)%5Ej"> on <img src="https://latex.codecogs.com/png.latex?%5B-1,%201%5D">.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?f"> is smooth, coefficients <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j"> will be small for large <img src="https://latex.codecogs.com/png.latex?j">.</li>
</ul></li>
<li><strong>21.2 Density Estimation</strong>
<ul>
<li>Let <img src="https://latex.codecogs.com/png.latex?X_1,%20%5Cldots,%20X_n"> be iid from a distribution on $$ with density <img src="https://latex.codecogs.com/png.latex?f%20%5Cin%20L%5E2">.</li>
<li><img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Csum_%7Bj=0%7D%5E%5Cinfty%20%5Cbeta_j%20%5Cphi_j(x)"> with orthonormal basis <img src="https://latex.codecogs.com/png.latex?%5Cphi_1,%20%5Cphi_2,%20%5Cldots">.</li>
<li>Estimator for coefficients: <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_j%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20%5Cphi_j(X_i)"> (Equation 21.11).</li>
<li><strong>Theorem 21.4</strong>: Mean and variance of <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_j">:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?E(%5Chat%7B%5Cbeta%7D_j)%20=%20%5Cbeta_j"> (unbiased estimator).</li>
<li><img src="https://latex.codecogs.com/png.latex?Var(%5Chat%7B%5Cbeta%7D_j)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20(1%20-%20%5Cbeta_j%5E2)"> (Equation 21.12).</li>
</ul></li>
<li>Orthogonal series density estimator: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_J(x)%20=%20%5Csum_%7Bj=0%7D%5EJ%20%5Chat%7B%5Cbeta%7D_j%20%5Cphi_j(x)">, where <img src="https://latex.codecogs.com/png.latex?J"> is a truncation parameter.</li>
<li>Risk of the estimator (mean integrated squared error, MISE).</li>
<li><strong>Theorem 21.5</strong>: MISE(<img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_J">) = <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BJ+1%7D%7Bn%7D%20+%20%5Csum_%7Bj=J+1%7D%5E%5Cinfty%20%5Cbeta_j%5E2">.</li>
<li>First term is variance, second term is squared bias.</li>
<li>Optimal <img src="https://latex.codecogs.com/png.latex?J"> balances bias and variance.</li>
<li>Cross-validation for choosing <img src="https://latex.codecogs.com/png.latex?J">.</li>
</ul></li>
<li><strong>21.3 Regression</strong>
<ul>
<li>Regression model: <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20r(x_i)%20+%20%5Cepsilon_i">, <img src="https://latex.codecogs.com/png.latex?i%20=%201,%20%5Cldots,%20n">, with <img src="https://latex.codecogs.com/png.latex?E(%5Cepsilon_i)%20=%200">, <img src="https://latex.codecogs.com/png.latex?Var(%5Cepsilon_i)%20=%20%5Csigma%5E2">, and <img src="https://latex.codecogs.com/png.latex?x_i%20=%20i/n"> (initially).</li>
<li><img src="https://latex.codecogs.com/png.latex?r(x)%20=%20%5Csum_%7Bj=1%7D%5E%5Cinfty%20%5Cbeta_j%20%5Cphi_j(x)"> with <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%20=%20%5Cint_0%5E1%20r(x)%20%5Cphi_j(x)%20dx"> (Equation 21.22).</li>
<li>Estimator for coefficients: <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_j%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20Y_i%20%5Cphi_j(x_i)">, <img src="https://latex.codecogs.com/png.latex?j%20=%201,%202,%20%5Cldots"> (Equation 21.23).</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_j"> is approximately Normally distributed by the central limit theorem.</li>
<li>Orthogonal series regression estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_J(x)%20=%20%5Csum_%7Bj=1%7D%5EJ%20%5Chat%7B%5Cbeta%7D_j%20%5Cphi_j(x)">.</li>
<li>Estimating <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">: <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D%5E2%20=%20%5Cfrac%7Bn%7D%7Bk%7D%20%5Csum_%7Bj=n-k+1%7D%5En%20%5Chat%7B%5Cbeta%7D_j%5E2"> with <img src="https://latex.codecogs.com/png.latex?k%20%5Capprox%20n/4"> (Equation 21.28).</li>
<li>Risk estimation <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(J)%20=%20%5Cfrac%7BJ%20%5Chat%7B%5Csigma%7D%5E2%7D%7Bn%7D%20+%20%5Csum_%7Bj=J+1%7D%5En%20(%5Chat%7B%5Cbeta%7D_j%5E2%20-%20%5Cfrac%7B%5Chat%7B%5Csigma%7D%5E2%7D%7Bn%7D)_+"> (Equation 21.29), where <img src="https://latex.codecogs.com/png.latex?(a)_+%20=%20%5Cmax(a,%200)">.</li>
<li>Choose <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D"> to minimize <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(J)">.</li>
<li>Example 21.10: Doppler test function and its estimation using orthogonal series regression.</li>
</ul></li>
<li><strong>21.4 Wavelets</strong>
<ul>
<li>Wavelets as a basis designed for inhomogeneous functions.</li>
<li>Allows placing ‚Äúblips‚Äù in small regions without adding wiggles elsewhere.</li>
<li>Haar wavelets as an example.</li>
<li>Father wavelet (scaling function) <img src="https://latex.codecogs.com/png.latex?%5Cphi(x)%20=%201"> for <img src="https://latex.codecogs.com/png.latex?0%20%5Cle%20x%20%3C%201">, <img src="https://latex.codecogs.com/png.latex?0"> otherwise.</li>
<li>Mother wavelet <img src="https://latex.codecogs.com/png.latex?%5Cpsi(x)%20=%201"> for <img src="https://latex.codecogs.com/png.latex?0%20%5Cle%20x%20%3C%201/2">, <img src="https://latex.codecogs.com/png.latex?-1"> for <img src="https://latex.codecogs.com/png.latex?1/2%20%5Cle%20x%20%3C%201">, <img src="https://latex.codecogs.com/png.latex?0"> otherwise.</li>
<li>Rescaled and shifted wavelets: <img src="https://latex.codecogs.com/png.latex?%5Cpsi_%7Bj,k%7D(x)%20=%202%5E%7Bj/2%7D%20%5Cpsi(2%5Ej%20x%20-%20k)">.</li>
<li>Sets of rescaled and shifted mother wavelets at resolution <img src="https://latex.codecogs.com/png.latex?j">: <img src="https://latex.codecogs.com/png.latex?W_j%20=%20%5C%7B%5Cpsi_%7Bjk%7D,%20k%20=%200,%201,%20%5Cldots,%202%5Ej%20-%201%5C%7D">.</li>
<li><strong>Theorem 21.13</strong>: The set <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Cphi,%20W_0,%20W_1,%20W_2,%20%5Cldots%20%5C%7D"> is an orthonormal basis for <img src="https://latex.codecogs.com/png.latex?L%5E2(0,%201)">.</li>
<li>Expansion of <img src="https://latex.codecogs.com/png.latex?f%20%5Cin%20L%5E2(0,%201)">: <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Calpha%20%5Cphi(x)%20+%20%5Csum_%7Bj=0%7D%5E%5Cinfty%20%5Csum_%7Bk=0%7D%5E%7B2%5Ej%20-%201%7D%20%5Cbeta_%7Bj,k%7D%20%5Cpsi_%7Bj,k%7D(x)">.</li>
<li>Discrete Wavelet Transform (DWT) for Haar wavelets.</li>
<li>Wavelet shrinkage for denoising.</li>
<li>Universal thresholding for wavelet coefficients.</li>
<li>Example 21.14: Estimation of the Doppler function using Haar wavelets and universal thresholding.</li>
</ul></li>
<li><strong>21.5 Appendix</strong>
<ul>
<li>The DWT for Haar Wavelets algorithm.</li>
</ul></li>
</ul></li>
<li><strong>22 Classification</strong>
<ul>
<li><strong>22.1 Introduction</strong>
<ul>
<li>Definition of classification as supervised learning, predicting a discrete <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X">.</li>
<li>Terminology mapping from statistics to computer science:
<ul>
<li>classification <img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow"> supervised learning</li>
<li>predicting a discrete <img src="https://latex.codecogs.com/png.latex?Y"> from <img src="https://latex.codecogs.com/png.latex?X"> <img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow"></li>
<li>data training sample <img src="https://latex.codecogs.com/png.latex?(X_1,%20Y_1),%20%5Cldots,%20(X_n,%20Y_n)"> <img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow"></li>
<li>covariates <img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow"> features the <img src="https://latex.codecogs.com/png.latex?X_i">‚Äôs</li>
<li>classifier <img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow"> hypothesis map <img src="https://latex.codecogs.com/png.latex?h:%20%5Cmathcal%7BX%7D%20%5Crightarrow%20%5Cmathcal%7BY%7D"></li>
<li>estimation <img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow"> learning finding a good classifier.</li>
</ul></li>
</ul></li>
<li><strong>22.2 Error Rates and the Bayes Classifier</strong>
<ul>
<li>Goal: find a classification rule <img src="https://latex.codecogs.com/png.latex?h"> that makes accurate predictions.</li>
<li>Definitions:
<ul>
<li>Classification rule <img src="https://latex.codecogs.com/png.latex?h:%20%5Cmathcal%7BX%7D%20%5Crightarrow%20%5C%7B0,%201%5C%7D"> (for binary case).</li>
<li>Loss function <img src="https://latex.codecogs.com/png.latex?L(h)%20=%20P(h(X)%20%5Cneq%20Y)"> (probability of misclassification or error rate).</li>
<li>Observed error rate <img src="https://latex.codecogs.com/png.latex?%5Chat%7BL%7D_n(h)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20I(h(X_i)%20%5Cneq%20Y_i)">.</li>
<li>Regression function <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20E(Y%7CX%20=%20x)%20=%20P(Y%20=%201%7CX%20=%20x)"> (for <img src="https://latex.codecogs.com/png.latex?Y%20%5Cin%20%5C%7B0,%201%5C%7D">).</li>
</ul></li>
<li><strong>Bayes Classifier</strong> <img src="https://latex.codecogs.com/png.latex?h%5E*(x)%20=%20%5Cbegin%7Bcases%7D%201%20&amp;%20%5Ctext%7Bif%20%7D%20P(Y%20=%201%7CX%20=%20x)%20%3E%20P(Y%20=%200%7CX%20=%20x)%20%5C%5C%200%20&amp;%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D"> (Equation 22.5).</li>
<li>Alternative form of Bayes Classifier: <img src="https://latex.codecogs.com/png.latex?h%5E*(x)%20=%20%5Cbegin%7Bcases%7D%201%20&amp;%20%5Ctext%7Bif%20%7D%20%5Cpi%20f_1(x)%20%3E%20(1%20-%20%5Cpi)%20f_0(x)%20%5C%5C%200%20&amp;%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D"> (Equation 22.6), where <img src="https://latex.codecogs.com/png.latex?%5Cpi%20=%20P(Y%20=%201)"> and <img src="https://latex.codecogs.com/png.latex?f_k(x)"> is the density of <img src="https://latex.codecogs.com/png.latex?X"> given <img src="https://latex.codecogs.com/png.latex?Y%20=%20k">.</li>
<li><strong>Theorem 22.5</strong>: The Bayes rule is optimal, <img src="https://latex.codecogs.com/png.latex?L(h%5E*)%20%5Cle%20L(h)"> for any other classification rule <img src="https://latex.codecogs.com/png.latex?h">.</li>
<li>Three main approaches to approximate the Bayes rule when unknown quantities are involved:
<ul>
<li>Empirical Risk Minimization: Choose a set of classifiers <img src="https://latex.codecogs.com/png.latex?H"> and find <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bh%7D%20%5Cin%20H"> that minimizes some estimate of <img src="https://latex.codecogs.com/png.latex?L(h)">.</li>
<li>Regression: Find an estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D"> of the regression function <img src="https://latex.codecogs.com/png.latex?r"> and define <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bh%7D(x)%20=%20%5Cbegin%7Bcases%7D%201%20&amp;%20%5Ctext%7Bif%20%7D%20%5Chat%7Br%7D(x)%20%3E%201/2%20%5C%5C%200%20&amp;%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D">.</li>
<li>Density Estimation: Estimate <img src="https://latex.codecogs.com/png.latex?f_0"> and <img src="https://latex.codecogs.com/png.latex?f_1">, let <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cpi%7D%20=%20n%5E%7B-1%7D%20%5Csum_%7Bi=1%7D%5En%20Y_i">, define <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D(x)%20=%20%5Cfrac%7B%5Chat%7B%5Cpi%7D%20%5Chat%7Bf%7D_1(x)%7D%7B%5Chat%7B%5Cpi%7D%20%5Chat%7Bf%7D_1(x)%20+%20(1%20-%20%5Chat%7B%5Cpi%7D)%20%5Chat%7Bf%7D_0(x)%7D">, and use <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bh%7D(x)%20=%20%5Cbegin%7Bcases%7D%201%20&amp;%20%5Ctext%7Bif%20%7D%20%5Chat%7Br%7D(x)%20%3E%201/2%20%5C%5C%200%20&amp;%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D">.</li>
</ul></li>
<li><strong>Theorem 22.6</strong>: For <img src="https://latex.codecogs.com/png.latex?Y%20%5Cin%20%5Cmathcal%7BY%7D%20=%20%5C%7B1,%20%5Cldots,%20K%5C%7D">, the optimal rule is <img src="https://latex.codecogs.com/png.latex?h%5E*(x)%20=%20%5Carg%5Cmax_%7By%20%5Cin%20%5Cmathcal%7BY%7D%7D%20P(Y%20=%20y%7CX%20=%20x)">.</li>
</ul></li>
<li><strong>22.3 Gaussian and Linear Classifiers</strong>
<ul>
<li>Linear discriminant analysis (LDA) assuming <img src="https://latex.codecogs.com/png.latex?f_k(x)%20%5Csim%20N(%5Cmu_k,%20%5CSigma)"> (common covariance matrix).</li>
<li>Bayes rule becomes linear: <img src="https://latex.codecogs.com/png.latex?h(x)%20=%20%5Cbegin%7Bcases%7D%201%20&amp;%20%5Ctext%7Bif%20%7D%20w%5ET%20x%20%5Cge%20c%20%5C%5C%200%20&amp;%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?w%20=%20%5CSigma%5E%7B-1%7D(%5Cmu_1%20-%20%5Cmu_0)"> and <img src="https://latex.codecogs.com/png.latex?c%20=%20%5Cfrac%7B1%7D%7B2%7D%20(%5Cmu_1%20+%20%5Cmu_0)%5ET%20%5CSigma%5E%7B-1%7D%20(%5Cmu_1%20-%20%5Cmu_0)%20-%20%5Clog(%5Cfrac%7B%5Cpi_1%7D%7B%5Cpi_0%7D)">.</li>
<li>Estimating <img src="https://latex.codecogs.com/png.latex?%5Cmu_k"> with sample means <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D_k"> and <img src="https://latex.codecogs.com/png.latex?%5CSigma"> with pooled sample covariance <img src="https://latex.codecogs.com/png.latex?S_W">.</li>
<li>Fisher‚Äôs discriminant analysis: find a linear combination <img src="https://latex.codecogs.com/png.latex?U%20=%20w%5ET%20X"> such that the classes are well separated.</li>
<li>Optimizing <img src="https://latex.codecogs.com/png.latex?J(w)%20=%20%5Cfrac%7Bw%5ET%20B%20w%7D%7Bw%5ET%20W%20w%7D">, where <img src="https://latex.codecogs.com/png.latex?B"> is between-class scatter and <img src="https://latex.codecogs.com/png.latex?W"> is within-class scatter.</li>
<li>Solution <img src="https://latex.codecogs.com/png.latex?w%20%5Cpropto%20S_W%5E%7B-1%7D%20(%5Cbar%7BX%7D_1%20-%20%5Cbar%7BX%7D_0)">.</li>
<li>Fisher linear discriminant function <img src="https://latex.codecogs.com/png.latex?U%20=%20w%5ET%20X%20=%20(%5Cbar%7BX%7D_0%20-%20%5Cbar%7BX%7D_1)%5ET%20S_W%5E%7B-1%7D%20X">.</li>
<li>Midpoint <img src="https://latex.codecogs.com/png.latex?m%20=%20%5Cfrac%7B1%7D%7B2%7D%20(%5Cbar%7BX%7D_0%20+%20%5Cbar%7BX%7D_1)%5ET%20S_W%5E%7B-1%7D%20(%5Cbar%7BX%7D_0%20-%20%5Cbar%7BX%7D_1)">.</li>
<li>Fisher‚Äôs classification rule <img src="https://latex.codecogs.com/png.latex?h(x)%20=%20%5Cbegin%7Bcases%7D%200%20&amp;%20%5Ctext%7Bif%20%7D%20w%5ET%20x%20%5Cge%20m%20%5C%5C%201%20&amp;%20%5Ctext%7Bif%20%7D%20w%5ET%20x%20%3C%20m%20%5Cend%7Bcases%7D">.</li>
<li>Relationship with Bayes linear classifier when <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cpi%7D%20=%201/2">.</li>
</ul></li>
<li><strong>22.4 Linear Regression and Logistic Regression</strong>
<ul>
<li>Direct approach: estimate <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20E(Y%7CX%20=%20x)"> and classify using <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bh%7D(x)%20=%20%5Cbegin%7Bcases%7D%201%20&amp;%20%5Ctext%7Bif%20%7D%20%5Chat%7Br%7D(x)%20%3E%201/2%20%5C%5C%200%20&amp;%20%5Ctext%7Botherwise%7D%20%5Cend%7Bcases%7D">.</li>
<li>Linear regression model <img src="https://latex.codecogs.com/png.latex?Y%20=%20X%5Cbeta%20+%20%5Cepsilon">, with <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D%20=%20(X%5ET%20X)%5E%7B-1%7D%20X%5ET%20Y"> and predicted values <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%20=%20X%5Chat%7B%5Cbeta%7D">.</li>
<li>Logistic regression model <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20P(Y%20=%201%7CX%20=%20x)%20=%20%5Cfrac%7Be%5E%7B%5Cbeta_0%20+%20%5Csum_j%20%5Cbeta_j%20x_j%7D%7D%7B1%20+%20e%5E%7B%5Cbeta_0%20+%20%5Csum_j%20%5Cbeta_j%20x_j%7D%7D"> (Equation 22.24), with <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D"> obtained numerically.</li>
<li>Example 22.11: Heart disease data, error rate using logistic regression (.27) and linear regression (.26).</li>
<li>Fitting richer logistic regression models by adding interaction terms (Equation 22.25).</li>
<li>Bias-variance tradeoff in choosing the complexity of the model.</li>
</ul></li>
<li><strong>22.5 Relationship Between Logistic Regression and LDA</strong></li>
<li><strong>22.6 Density Estimation and Naive Bayes</strong>
<ul>
<li>Naive Bayes classifier assumes conditional independence of features given the class label: <img src="https://latex.codecogs.com/png.latex?f(x%7CY=k)%20=%20%5Cprod_%7Bj=1%7D%5Ed%20f_j(x_j%7CY=k)">.</li>
</ul></li>
<li><strong>22.7 Trees</strong>
<ul>
<li>Classification trees partition the feature space into rectangular regions.</li>
<li>Prediction is constant within each region.</li>
<li>Recursive partitioning based on impurity measures (e.g., misclassification error, Gini index, cross-entropy).</li>
<li>Stopping criteria to prevent overfitting.</li>
<li>Pruning to simplify the tree.</li>
<li>Example 22.13: Classification tree for heart disease data yielding a misclassification rate of .21. Tree using only tobacco and age has a misclassification rate of .29 (Figure 22.4).</li>
</ul></li>
<li><strong>22.8 Assessing Error Rates and Choosing a Good Classifier</strong>
<ul>
<li>Observed classification error can underestimate the true error rate.</li>
<li>Example 22.14: Sequence of logistic regression models for heart disease data, showing observed error decreasing with complexity, while cross-validation error shows a bias-variance tradeoff (Figure 22.5).</li>
<li><strong>Cross-Validation</strong>: Split data into training set <img src="https://latex.codecogs.com/png.latex?T"> and validation set <img src="https://latex.codecogs.com/png.latex?V">. Train classifier on <img src="https://latex.codecogs.com/png.latex?T">, estimate error on <img src="https://latex.codecogs.com/png.latex?V">. <img src="https://latex.codecogs.com/png.latex?k">-fold cross-validation.</li>
<li><strong>Probability Inequalities</strong>: Using theorems like Hoeffding‚Äôs inequality to bound the difference between observed error and true error.</li>
<li><strong>Theorem 22.16 (Uniform Convergence)</strong>: <img src="https://latex.codecogs.com/png.latex?P(%5Cmax_%7Bh%20%5Cin%20H%7D%20%7C%5Chat%7BL%7D_n(h)%20-%20L(h)%7C%20%3E%20%5Cepsilon)%20%5Cle%202me%5E%7B-2n%5Cepsilon%5E2%7D"> for a finite hypothesis space <img src="https://latex.codecogs.com/png.latex?H"> of size <img src="https://latex.codecogs.com/png.latex?m">.</li>
<li><strong>Theorem 22.17</strong>: <img src="https://latex.codecogs.com/png.latex?%5Chat%7BL%7D_n(%5Chat%7Bh%7D)%20%5Cpm%20%5Cepsilon"> is a <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> confidence interval for <img src="https://latex.codecogs.com/png.latex?L(%5Chat%7Bh%7D)">, with <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20=%20%5Csqrt%7B%5Cfrac%7B1%7D%7B2n%7D%20%5Clog(%5Cfrac%7B2m%7D%7B%5Calpha%7D)%7D">.</li>
</ul></li>
<li><strong>22.9 Support Vector Machines</strong>
<ul>
<li>Class of linear classifiers for binary <img src="https://latex.codecogs.com/png.latex?Y%20%5Cin%20%5C%7B-1,%20+1%5C%7D">.</li>
<li>Linear classifier <img src="https://latex.codecogs.com/png.latex?h(x)%20=%20%5Ctext%7Bsign%7D(H(x))">, where <img src="https://latex.codecogs.com/png.latex?H(x)%20=%20a_0%20+%20%5Csum_%7Bi=1%7D%5Ed%20a_i%20x_i"> and <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bsign%7D(z)%20=%20-1"> if <img src="https://latex.codecogs.com/png.latex?z%20%3C%200">, <img src="https://latex.codecogs.com/png.latex?0"> if <img src="https://latex.codecogs.com/png.latex?z%20=%200">, <img src="https://latex.codecogs.com/png.latex?1"> if <img src="https://latex.codecogs.com/png.latex?z%20%3E%200">.</li>
<li>Linearly separable data: exists a hyperplane perfectly separating the classes.</li>
<li><strong>Lemma 22.27</strong>: Data separable by some hyperplane iff exists <img src="https://latex.codecogs.com/png.latex?H(x)"> such that <img src="https://latex.codecogs.com/png.latex?Y_i%20H(x_i)%20%5Cge%201"> for all <img src="https://latex.codecogs.com/png.latex?i">.</li>
<li>Maximum margin hyperplane: separates classes and maximizes the distance to the closest point (margin).</li>
<li>Support vectors: points on the boundary of the margin.</li>
<li><strong>Theorem 22.28</strong>: The maximum margin hyperplane <img src="https://latex.codecogs.com/png.latex?%5Chat%7BH%7D(x)%20=%20%5Chat%7Ba%7D_0%20+%20%5Csum_%7Bi=1%7D%5Ed%20%5Chat%7Ba%7D_i%20x_i"> is the solution to a constrained optimization problem.</li>
</ul></li>
<li><strong>22.10 Kernelization</strong>
<ul>
<li>Mapping covariates to a higher-dimensional space <img src="https://latex.codecogs.com/png.latex?%5Cphi(x)"> to create non-linear classifiers in the original space while using linear classifiers in the higher space.</li>
<li>Example with an ellipse separable in a higher dimension.</li>
</ul></li>
<li><strong>22.11 Other Classifiers</strong></li>
</ul></li>
<li><strong>23 Probability Redux: Stochastic Processes</strong>
<ul>
<li><strong>23.1 Introduction</strong></li>
<li><strong>23.2 Markov Chains</strong>
<ul>
<li>Definition of a Markov chain.</li>
<li>Transition matrix <img src="https://latex.codecogs.com/png.latex?P(i,%20j)%20=%20P(X_%7Bn+1%7D%20=%20j%7CX_n%20=%20i)%20=%20p_%7Bij%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?n">-step transition probabilities <img src="https://latex.codecogs.com/png.latex?p_%7Bij%7D(n)%20=%20P(X_n%20=%20j%7CX_0%20=%20i)">.</li>
<li>Chapman-Kolmogorov equations.</li>
<li>Distribution at time <img src="https://latex.codecogs.com/png.latex?n">: <img src="https://latex.codecogs.com/png.latex?%5Cmu_n(j)%20=%20P(X_n%20=%20j)">.</li>
<li><strong>Lemma 23.10</strong>: The marginal probabilities are given by <img src="https://latex.codecogs.com/png.latex?%5Cmu_n%20=%20%5Cmu_0%20P%5En">.</li>
<li>Stationary distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"> where <img src="https://latex.codecogs.com/png.latex?%5Cpi%20P%20=%20%5Cpi">.</li>
<li>Ergodic Markov chain.</li>
<li><strong>Theorem 23.26</strong>: If <img src="https://latex.codecogs.com/png.latex?%5Cpi"> satisfies detailed balance (<img src="https://latex.codecogs.com/png.latex?%5Cpi_i%20p_%7Bij%7D%20=%20p_%7Bji%7D%20%5Cpi_j">), then <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a stationary distribution.</li>
<li>Example: Genotype of descendants as a Markov chain.</li>
</ul></li>
<li><strong>23.3 Poisson Processes</strong>
<ul>
<li>Definition of a Poisson process.</li>
<li>Waiting times in a Poisson process.</li>
</ul></li>
<li><strong>Summary of Terminology</strong>
<ul>
<li>Transition matrix: <img src="https://latex.codecogs.com/png.latex?P(i,%20j)%20=%20P(X_%7Bn+1%7D%20=%20j%7CX_n%20=%20i)%20=%20p_%7Bij%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?n">-step transition probability: <img src="https://latex.codecogs.com/png.latex?p_%7Bij%7D(n)%20=%20P(X_n%20=%20j%7CX_0%20=%20i)">.</li>
<li>Marginal probability at time <img src="https://latex.codecogs.com/png.latex?n">: <img src="https://latex.codecogs.com/png.latex?%5Cmu_n(j)%20=%20P(X_n%20=%20j)">.</li>
<li>Stationary distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">: <img src="https://latex.codecogs.com/png.latex?%5Cpi%20P%20=%20%5Cpi">.</li>
<li>Detailed balance: <img src="https://latex.codecogs.com/png.latex?%5Cpi_i%20p_%7Bij%7D%20=%20p_%7Bji%7D%20%5Cpi_j">.</li>
</ul></li>
<li><strong>Example (Markov chain Monte Carlo)</strong>: Brief description of MCMC, using a proposal distribution <img src="https://latex.codecogs.com/png.latex?W%20%5Csim%20N(X_i,%20b%5E2)"> and acceptance probability <img src="https://latex.codecogs.com/png.latex?r%20=%20%5Cmin%20%5C%7B%20%5Cfrac%7Bg(W)%7D%7Bg(X_i)%7D,%201%20%5C%7D"> to generate a Markov chain with stationary distribution <img src="https://latex.codecogs.com/png.latex?f(x)%20%5Cpropto%20g(x)">.</li>
</ul></li>
<li><strong>24 Simulation Methods</strong>
<ul>
<li><strong>24.1 Bayesian Inference Revisited</strong>
<ul>
<li>Mention of using simulation methods in Bayesian inference.</li>
</ul></li>
<li><strong>24.2 Basic Monte Carlo Integration</strong>
<ul>
<li>Estimating integrals using random sampling.</li>
<li>Estimate of <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20E(g(X))%20=%20%5Cint%20g(x)%20f(x)%20dx"> by <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_n%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20g(X_i)">, where <img src="https://latex.codecogs.com/png.latex?X_i%20%5Csim%20f">.</li>
<li>Central Limit Theorem for Monte Carlo integration: <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bn%7D(%5Chat%7B%5Ctheta%7D_n%20-%20%5Ctheta)%20/%20%5Csigma%20%5Cxrightarrow%7Bd%7D%20N(0,%201)">, where <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20=%20Var(g(X))">.</li>
</ul></li>
<li><strong>24.3 Importance Sampling</strong>
<ul>
<li>Reducing variance by sampling from an importance sampling distribution <img src="https://latex.codecogs.com/png.latex?h(x)">.</li>
<li>Estimator: <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_n%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20%5Cfrac%7Bg(X_i)%20f(X_i)%7D%7Bh(X_i)%7D">, where <img src="https://latex.codecogs.com/png.latex?X_i%20%5Csim%20h">.</li>
</ul></li>
<li><strong>24.4 MCMC Part I: The Metropolis‚ÄìHastings Algorithm</strong>
<ul>
<li>Introduction to Markov chain Monte Carlo (MCMC) for sampling from a target distribution <img src="https://latex.codecogs.com/png.latex?f(x)">.</li>
<li>Proposal distribution <img src="https://latex.codecogs.com/png.latex?q(y%7Cx)">.</li>
<li>Acceptance probability <img src="https://latex.codecogs.com/png.latex?r%20=%20%5Cmin%20%5C%7B%20%5Cfrac%7Bf(y)%20q(x%5Cmid%20y)%7D%7Bf(x)%20q(y%20%5Cmid%20x)%7D,%201%20%5C%7D">.</li>
<li>Algorithm:
<ul>
<li>Start with <img src="https://latex.codecogs.com/png.latex?X_0">.</li>
<li>For <img src="https://latex.codecogs.com/png.latex?n%20%5Cge%200">, given <img src="https://latex.codecogs.com/png.latex?X_n">, generate <img src="https://latex.codecogs.com/png.latex?Y%20%5Csim%20q(%5Ccdot%7CX_n)">.</li>
<li>Calculate acceptance probability <img src="https://latex.codecogs.com/png.latex?r">.</li>
<li>Set <img src="https://latex.codecogs.com/png.latex?X_%7Bn+1%7D%20=%20Y"> with probability <img src="https://latex.codecogs.com/png.latex?r">, and <img src="https://latex.codecogs.com/png.latex?X_%7Bn+1%7D%20=%20X_n"> with probability <img src="https://latex.codecogs.com/png.latex?1-r">.</li>
</ul></li>
</ul></li>
<li><strong>24.5 MCMC Part II: Different Flavors</strong>
<ul>
<li><strong>Gibbs Sampler</strong>: Sampling each variable from its full conditional distribution.
<ul>
<li>Example with a bivariate Normal distribution.</li>
<li>Example involving drawing <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Cpsi_i"> iteratively from their conditional distributions.</li>
<li><img src="https://latex.codecogs.com/png.latex?f(%5Cpsi_i%7Crest)%20%5Cpropto%20%5Cexp%20%5C%7B%20-%5Cfrac%7B1%7D%7B2%7D%20(%5Cpsi_i%20-%20%5Cmu)%5E2%20%5C%7D%20%5Cexp%20%5C%7B%20-%5Cfrac%7B1%7D%7B2%5Csigma%5E2_i%7D%20(Z_i%20-%20%5Cpsi_i)%5E2%20%5C%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpsi_i%7Crest%20%5Csim%20N(e_i,%20d%5E2_i)">, where <img src="https://latex.codecogs.com/png.latex?e_i%20=%20%5Cfrac%7BZ_i/%5Csigma%5E2_i%20+%20%5Cmu%7D%7B1%20+%201/%5Csigma%5E2_i%7D"> and <img src="https://latex.codecogs.com/png.latex?d%5E2_i%20=%20%5Cfrac%7B1%7D%7B1%20+%201/%5Csigma%5E2_i%7D">.</li>
</ul></li>
<li><strong>Metropolis within Gibbs</strong>: Combining Metropolis-Hastings steps within a Gibbs sampling framework.
<ul>
<li>Algorithm involving proposals and acceptance probabilities for <img src="https://latex.codecogs.com/png.latex?X_n"> and <img src="https://latex.codecogs.com/png.latex?Y_n">.</li>
<li>Acceptance for <img src="https://latex.codecogs.com/png.latex?X">: <img src="https://latex.codecogs.com/png.latex?r%20=%20%5Cmin%20%5C%7B%20%5Cfrac%7Bf(Z,%20Y_n)%20q(X_n%7CZ)%7D%7Bf(X_n,%20Y_n)%20q(Z%7CX_n)%7D,%201%20%5C%7D">.</li>
<li>Acceptance for <img src="https://latex.codecogs.com/png.latex?Y">: <img src="https://latex.codecogs.com/png.latex?r%20=%20%5Cmin%20%5C%7B%20%5Cfrac%7Bf(X_%7Bn+1%7D,%20Z)%20q%CC%83(Y_n%7CZ)%7D%7Bf(X_%7Bn+1%7D,%20Yn)%20q%CC%83(Z%7CY_n)%7D,%201%20%5C%7D">.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="frequently-asked-questions" class="level2">
<h2 class="anchored" data-anchor-id="frequently-asked-questions">Frequently Asked Questions</h2>
<ol type="1">
<li>What are probability inequalities and why are they important in statistics?</li>
</ol>
<p>Probability inequalities, as discussed in Chapter 4, provide bounds on the probability of certain events occurring. These inequalities, such as Markov‚Äôs inequality, Chebyshev‚Äôs inequality, and Jensen‚Äôs inequality, are crucial because they allow us to make statements about the likelihood of events even when the exact probability distribution is unknown or complex. They are fundamental tools for understanding the concentration of probability mass and for deriving other statistical results, particularly in the study of the behavior of random variables.</p>
<ol start="2" type="1">
<li>What are the different modes of convergence for random variables, as outlined in Chapter 5?</li>
</ol>
<p>Chapter 5 details several ways in which a sequence of random variables can converge to a limiting random variable or a constant. The main types of convergence discussed are:</p>
<p>Convergence in probability: The probability that the difference between the random variable and its limit exceeds any small positive number approaches zero as the sequence progresses. Almost sure convergence (or convergence with probability 1): The sequence of random variables converges pointwise to its limit for all outcomes except for a set of probability zero. Convergence in mean square: The expected squared difference between the random variable and its limit approaches zero. Convergence in distribution (or weak convergence): The cumulative distribution functions of the sequence of random variables converge pointwise to the cumulative distribution function of the limit at all points where the limit distribution function is continuous. These different modes of convergence have distinct implications and are used in various contexts within statistical theory.</p>
<ol start="3" type="1">
<li>What are the Law of Large Numbers and the Central Limit Theorem, and why are they central to statistical inference?</li>
</ol>
<p>The Law of Large Numbers (LLN), presented in Chapter 5, essentially states that the sample average of a large number of independent and identically distributed (i.i.d.) random variables will converge to the true population mean. This theorem provides the theoretical justification for using sample means to estimate population means.</p>
<p>The Central Limit Theorem (CLT), also in Chapter 5, is another cornerstone of statistics. It states that the sum (or average) of a large number of i.i.d. random variables will be approximately normally distributed, regardless of the shape of the original distribution, provided certain conditions are met. The CLT is vital because it allows us to make probabilistic statements and construct statistical inferences (like confidence intervals and hypothesis tests) about population parameters, even when the underlying distribution is not normal.</p>
<ol start="4" type="1">
<li>What are the primary goals of point estimation, confidence sets, and hypothesis testing, as introduced in Chapter 6?</li>
</ol>
<p>Chapter 6 lays the groundwork for statistical inference.</p>
<p>Point estimation aims to find a single ‚Äúbest‚Äù value to estimate an unknown population parameter based on observed sample data. Confidence sets (specifically confidence intervals in the one-dimensional case) provide a range of plausible values for an unknown population parameter, along with a measure of confidence that the true parameter lies within that range. Hypothesis testing is a formal procedure used to decide between two competing claims (hypotheses) about a population parameter, based on evidence from a sample. It involves formulating a null hypothesis and an alternative hypothesis, and then using data to determine whether there is enough evidence to reject the null hypothesis. 5. How can we estimate the cumulative distribution function (cdf) and what are statistical functionals, as discussed in Chapter 7?</p>
<p>Chapter 7 introduces methods for understanding the distribution of data. The empirical distribution function (EDF) is a non-parametric estimator of the true cdf, based directly on the observed sample. It assigns a probability of 1/n to each observed data point and is a step function that jumps at the values of the data points.</p>
<p>Statistical functionals are functions of the cdf. Many important statistical parameters and estimators can be expressed as functionals of the underlying distribution. Examples include the mean, variance, quantiles, and correlation. Studying statistical functionals provides a unified way to analyze the properties of various estimators.</p>
<ol start="6" type="1">
<li>What is the bootstrap method and how can it be used for variance estimation and constructing confidence intervals, as detailed in Chapter 8?</li>
</ol>
<p>The bootstrap is a powerful resampling technique, explained in Chapter 8, used to estimate the sampling distribution of an estimator. It works by repeatedly drawing random samples with replacement from the original data to create many ‚Äúbootstrap samples.‚Äù By calculating the estimator of interest for each bootstrap sample, we can approximate the variance of the estimator and construct bootstrap confidence intervals. This method is particularly useful when analytical methods for calculating variance or constructing confidence intervals are difficult or unavailable, especially for complex estimators or small sample sizes. The jackknife, a related technique for bias and variance estimation, is also mentioned in the appendix of this chapter.</p>
<ol start="7" type="1">
<li>What are the method of moments and maximum likelihood estimation, and what are some important properties of maximum likelihood estimators, as covered in Chapter 9?</li>
</ol>
<p>Chapter 9 focuses on parametric inference. The method of moments is a technique for estimating parameters of a probability distribution by equating sample moments (like the sample mean and sample variance) to their corresponding population moments (expressed as functions of the parameters) and solving for the parameters.</p>
<p>Maximum likelihood estimation (MLE) is another widely used method. It involves finding the parameter values that maximize the likelihood function, which represents the probability of observing the given sample data under different possible parameter values. MLEs often have desirable asymptotic properties, such as consistency (converging to the true parameter as the sample size increases) and asymptotic normality. The delta method, also mentioned, is a technique for approximating the distribution of a function of an estimator, often used in conjunction with MLEs.</p>
<ol start="8" type="1">
<li>What are the Wald test and p-values, as introduced in Chapter 10 in the context of hypothesis testing?</li>
</ol>
<p>Chapter 10 delves into hypothesis testing. The Wald test is a statistical test used to assess hypotheses about parameters in statistical models, often based on the maximum likelihood estimator. It compares the estimated parameter value to the hypothesized value, taking into account the estimated variance of the estimator.</p>
<p>A p-value is a measure of the statistical significance of an observed result in hypothesis testing. It is defined as the probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is true. A small p-value provides evidence against the null hypothesis, leading to its rejection if the p-value is below a predetermined significance level (alpha).</p>
</section>
<section id="the-review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-review">The Review</h2>
<!-- Beyond the outline -->
<p>Proin sodales neque erat, varius cursus diam tincidunt sit amet. Etiam scelerisque fringilla nisl eu venenatis. Donec sem ipsum, scelerisque ac venenatis quis, hendrerit vel mauris. Praesent semper erat sit amet purus condimentum, sit amet auctor mi feugiat. In hac habitasse platea dictumst. Nunc ac mauris in massa feugiat bibendum id in dui. Praesent accumsan urna at lacinia aliquet. Proin ultricies eu est quis pellentesque. In vel lorem at nisl rhoncus cursus eu quis mi. In eu rutrum ante, quis placerat justo. Etiam euismod nibh nibh, sed elementum nunc imperdiet in. Praesent gravida nunc vel odio lacinia, at tempus nisl placerat. Aenean id ipsum sed est sagittis hendrerit non in tortor.</p>
<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.</p>

<div class="no-row-height column-margin column-container"><div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_1.jpg" class="lightbox" data-gallery="figures" title="Figure&nbsp;1: figure 1"><img src="https://orenbochman.github.io/reviews/2004/all-of-statisitcs/fig_1.jpg" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: figure 1
</figcaption>
</figure>
</div></div><p>Etiam quis tortor luctus, pellentesque ante a, finibus dolor. Phasellus in nibh et magna pulvinar malesuada. Ut nisl ex, sagittis at sollicitudin et, sollicitudin id nunc. In id porta urna. Proin porta dolor dolor, vel dapibus nisi lacinia in. Pellentesque ante mauris, ornare non euismod a, fermentum ut sapien. Proin sed vehicula enim. Aliquam tortor odio, vestibulum vitae odio in, tempor molestie justo. Praesent maximus lacus nec leo maximus blandit.</p>
<p>Maecenas turpis velit, ultricies non elementum vel, luctus nec nunc. Nulla a diam interdum, faucibus sapien viverra, finibus metus. Donec non tortor diam. In ut elit aliquet, bibendum sem et, aliquam tortor. Donec congue, sem at rhoncus ultrices, nunc augue cursus erat, quis porttitor mauris libero ut ex. Nullam quis leo urna. Donec faucibus ligula eget pellentesque interdum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean rhoncus interdum erat ut ultricies. Aenean tempus ex non elit suscipit, quis dignissim enim efficitur. Proin laoreet enim massa, vitae laoreet nulla mollis quis.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {üìñ {All} of {Statistics}},
  url = {https://orenbochman.github.io/reviews/2004/all-of-statisitcs/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>‚Äúüìñ All of Statistics.‚Äù</span> <a href="https://orenbochman.github.io/reviews/2004/all-of-statisitcs/">https://orenbochman.github.io/reviews/2004/all-of-statisitcs/</a>.
</div></div></section></div> ]]></description>
  <category>draft</category>
  <category>review</category>
  <guid>https://orenbochman.github.io/reviews/2004/all-of-statisitcs/</guid>
  <pubDate>Fri, 14 Mar 2025 02:32:17 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/images/lit-review-cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>title</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/</link>
  <description><![CDATA[ 






<!-- VIDEOS GO HERE 

::: {.column-margin #fig-subtasks}


Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
:::

-->
<!-- A QUOTE by someone more famous than the author of the paper for context, add highlighting for emphasis, verse is a nice touch! 

> "The ideal market completely disregards those spikes‚Äîbut a realistic model cannot." [Mandelbrot highlights the inadequacy of models ignoring extreme price movements, emphasizing the need for a framework that can accommodate them.]{.mark}

-->
<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/cover.webp" class="nolightbox img-fluid figure-img" width="250"></p>
<figcaption>cover</figcaption>
</figure>
</div></div><p>This is the follow up book to All of Statistics by Wasserman. This one covers non-parametric methods. Though the first book also covered a number of non-parametric methods like wavelets, this book goes into more detail on the subject. It too is a bit dated as it was written before the deep learning revolution which falls under the non-parametric methods. However, it is still a good reference for the basics of non-parametric statistics.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR - Too Long; Didn‚Äôt Read about XXX <!-- Short Catchy title -->
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="XXX in a nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="XXX in a nutshell"></a></p>
<figcaption>XXX in a nutshell</figcaption>
</figure>
</div>
<p>Nullam dapibus cursus dolor sit amet consequat. Nulla facilisi. Curabitur vel nulla non magna lacinia tincidunt. Duis porttitor quam leo, et blandit velit efficitur ut. Etiam auctor tincidunt porttitor. Phasellus sed accumsan mi. Fusce ut erat dui. Suspendisse eu augue eget turpis condimentum finibus eu non lorem. Donec finibus eros eu ante condimentum, sed pharetra sapien sagittis. Phasellus non dolor ac ante mollis auctor nec et sapien. Pellentesque vulputate at nisi eu tincidunt. Vestibulum at dolor aliquam, hendrerit purus eu, eleifend massa. Morbi consectetur eros id tincidunt gravida. Fusce ut enim quis orci hendrerit lacinia sed vitae enim. <!-- SHORT & OPINIONATED--></p>
<!-- 1. What are the research questions? -->
<!-- 2. What are the main findings? -->
<!-- 3. In historical context why was this important? -->
</div>
</div>
<p>Here is a light hearted Deep Dive into the book:</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This book uses lots of big terms so let‚Äôs break them down so we can understand them better</p>
<dl>
<dt><img src="https://latex.codecogs.com/png.latex?%5Csigma">-field</dt>
<dd>
A class of events <img src="https://latex.codecogs.com/png.latex?A"> such that (i) <img src="https://latex.codecogs.com/png.latex?%5Cemptyset%20%5Cin%20A">, (ii) <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20A"> implies that <img src="https://latex.codecogs.com/png.latex?A%5Ec%20%5Cin%20A"> and (iii) <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20%5Cldots%20%5Cin%20A"> implies that <img src="https://latex.codecogs.com/png.latex?%5Cbigcup_%7Bi=1%7D%5E%5Cinfty%20A_i%20%5Cin%20A">.
</dd>
<dt>Probability measure</dt>
<dd>
A function <img src="https://latex.codecogs.com/png.latex?P"> defined on a <img src="https://latex.codecogs.com/png.latex?%5Csigma">-field <img src="https://latex.codecogs.com/png.latex?A"> such that <img src="https://latex.codecogs.com/png.latex?P(A)%20%5Cgeq%200"> for all <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20A">, <img src="https://latex.codecogs.com/png.latex?P(%5COmega)%20=%201"> and if <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20%5Cldots%20%5Cin%20A"> are disjoint then <img src="https://latex.codecogs.com/png.latex?P%5Cleft(%5Cbigcup_%7Bi=1%7D%5E%5Cinfty%20A_i%5Cright)%20=%20%5Csum_%7Bi=1%7D%5E%5Cinfty%20P(A_i)">.
</dd>
<dt>Probability space</dt>
<dd>
The triple <img src="https://latex.codecogs.com/png.latex?(%5COmega,%20A,%20P)"> where <img src="https://latex.codecogs.com/png.latex?%5COmega"> is the sample space, <img src="https://latex.codecogs.com/png.latex?A"> is a <img src="https://latex.codecogs.com/png.latex?%5Csigma">-field of events, and <img src="https://latex.codecogs.com/png.latex?P"> is a probability measure.
</dd>
<dt>Random variable</dt>
<dd>
A map <img src="https://latex.codecogs.com/png.latex?X%20:%20%5COmega%20%5Cto%20%5Cmathbb%7BR%7D"> such that, for every real <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Comega%20%5Cin%20%5COmega%20:%20X(%5Comega)%20%5Cleq%20x%5C%7D%20%5Cin%20A">.
</dd>
<dt>Mean (of a function <img src="https://latex.codecogs.com/png.latex?a">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?E(a(X))%20=%20%5Cint%20a(x)%20dF(x)%20%5Cequiv%20%5Cleft%5C%7B%20%5Cint%20a(x)%20f(x)%20dx%20%5Ctext%7B%20in%20the%20continuous%20case,%20%7D%20%5Csum_j%20a(x_j)%20f(x_j)%20%5Ctext%7B%20in%20the%20discrete%20case%7D%20%5Cright%5C%7D">.
</dd>
<dt>Variance (of a random variable <img src="https://latex.codecogs.com/png.latex?X">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?V(X)%20=%20E((X%20-%20E(X))%5E2)">.
</dd>
<dt>Mean squared error (mse)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bmse%7D%20=%20%5Ctext%7Bbias%7D%5E2(%5Chat%7B%5Ctheta%7D_n)%20+%20V(%5Chat%7B%5Ctheta%7D_n)">. Also defined as <img src="https://latex.codecogs.com/png.latex?R(f(x),%20%5Chat%7Bf%7D_n(x))%20=%20E%20(%20L(f(x),%20%5Chat%7Bf%7D_n(x))%20)">.
</dd>
<dt>Confidence set</dt>
<dd>
A set <img src="https://latex.codecogs.com/png.latex?C_n"> of possible values of a quantity of interest <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, which depends on the data <img src="https://latex.codecogs.com/png.latex?X_1,%20%5Cldots,%20X_n">.
</dd>
<dt>Plug-in estimator (of <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20T(F)">)</dt>
<dd>
Defined by <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_n%20=%20T(%5Chat%7BF%7D_n)">.
</dd>
<dt>Linear functional (of <img src="https://latex.codecogs.com/png.latex?F">)</dt>
<dd>
A functional of the form <img src="https://latex.codecogs.com/png.latex?%5Cint%20a(x)%20dF(x)">.
</dd>
<dt>Influence function (<img src="https://latex.codecogs.com/png.latex?L_F(x)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?L_F(x)%20=%20%5Clim_%7B%5Cepsilon%20%5Cto%200%7D%20%5Cfrac%7BT((1%20-%20%5Cepsilon)F%20+%20%5Cepsilon%20%5Cdelta_x)%20-%20T(F)%7D%7B%5Cepsilon%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Cdelta_x"> is a point mass at <img src="https://latex.codecogs.com/png.latex?x">.
</dd>
<dt>Empirical influence function (<img src="https://latex.codecogs.com/png.latex?%5Chat%7BL%7D(x)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BL%7D(x)%20=%20L_%7B%5Chat%7BF%7D_n%7D(x)">.
</dd>
<dt>Nonparametric delta method</dt>
<dd>
The approximation <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BT(%5Chat%7BF%7D_n)%20-%20T(F)%7D%7B%5Chat%7Bs%7D_e%7D%20%5Capprox%20N(0,%201)"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bs%7D_e%20=%20%5Chat%7B%5Ctau%7D/%5Csqrt%7Bn%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D%5E2%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20%5Chat%7BL%7D%5E2(X_i)">.
</dd>
<dt>Empirical probability distribution (<img src="https://latex.codecogs.com/png.latex?%5Chat%7BP%7D_n(A)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BP%7D_n(A)%20=%20%5Cfrac%7B%5Ctext%7Bnumber%20of%20%7D%20X_i%20%5Cin%20A%7D%7Bn%7D">.
</dd>
<dt>G√¢teaux derivative (of <img src="https://latex.codecogs.com/png.latex?T"> at <img src="https://latex.codecogs.com/png.latex?F"> in the direction <img src="https://latex.codecogs.com/png.latex?G">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?L_F(G)%20=%20%5Clim_%7B%5Cepsilon%20%5Cto%200%7D%20%5Cfrac%7BT((1%20-%20%5Cepsilon)F%20+%20%5Cepsilon%20G)%20-%20T(F)%7D%7B%5Cepsilon%7D">.
</dd>
<dt>Hadamard differentiable (at <img src="https://latex.codecogs.com/png.latex?F">)</dt>
<dd>
There exists a linear functional <img src="https://latex.codecogs.com/png.latex?L_F"> on <img src="https://latex.codecogs.com/png.latex?D"> such that for any <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_n%20%5Cto%200"> and <img src="https://latex.codecogs.com/png.latex?%5C%7BD,%20D_1,%20D_2,%20%5Cldots%5C%7D%20%5Csubset%20D"> such that <img src="https://latex.codecogs.com/png.latex?d(D_n,%20D)%20%5Cto%200"> and <img src="https://latex.codecogs.com/png.latex?F%20+%20%5Cepsilon_n%20D_n%20%5Cin%20%5Cmathcal%7BF%7D">, <img src="https://latex.codecogs.com/png.latex?%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20%5Cleft(%5Cfrac%7BT(F%20+%20%5Cepsilon_n%20D_n)%20-%20T(F)%7D%7B%5Cepsilon_n%7D%20-%20L_F(D_n)%5Cright)%20=%200">.
</dd>
<dt>Linear smoother (estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n"> of <img src="https://latex.codecogs.com/png.latex?r">)</dt>
<dd>
For each <img src="https://latex.codecogs.com/png.latex?x">, there exists a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bl%7D(x)%20=%20(l_1(x),%20%5Cldots,%20l_n(x))%5ET"> such that <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)%20=%20%5Csum_%7Bi=1%7D%5En%20l_i(x)%20Y_i">.
</dd>
<dt>Leave-one-out cross-validation score (<img src="https://latex.codecogs.com/png.latex?cv%20=%20%5Chat%7BR%7D(h)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(h)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(Y_i%20-%20%5Chat%7Br%7D%5E%7B(-i)%7D(x_i))%5E2"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D%5E%7B(-i)%7D"> is the estimator obtained by omitting the <img src="https://latex.codecogs.com/png.latex?i">th pair <img src="https://latex.codecogs.com/png.latex?(x_i,%20Y_i)">.
</dd>
<dt>Histogram estimator (<img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)%20=%20m%20%5Csum_%7Bj=1%7D%20%5Cfrac%7B%5Chat%7Bp%7D_j%7D%7Bh%7D%20I(x%20%5Cin%20B_j)"> where <img src="https://latex.codecogs.com/png.latex?h=1/m"> is the binwidth, <img src="https://latex.codecogs.com/png.latex?B_j"> are the bins, and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D_j"> is the proportion of observations in bin <img src="https://latex.codecogs.com/png.latex?B_j">.
</dd>
<dt>Cross-validation estimator of risk (<img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Cint%20%5Chat%7Bf%7D_n%5E2(x)%20dx%20-%20%5Cfrac%7B2%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20%5Chat%7Bf%7D_n%5E%7B(-i)%7D(X_i)">.
</dd>
<dt>Sobolev space of order <img src="https://latex.codecogs.com/png.latex?m"> (<img src="https://latex.codecogs.com/png.latex?W%5E%7B(m)%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5C%7B%20f%20%5Cin%20L_2(0,%201)%20:%20D%5Em%20f%20%5Cin%20L_2(0,%201)%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?D%5Em%20f"> is the <img src="https://latex.codecogs.com/png.latex?m">th weak derivative of <img src="https://latex.codecogs.com/png.latex?f">.
</dd>
<dt>Sobolev space of order <img src="https://latex.codecogs.com/png.latex?m"> and radius <img src="https://latex.codecogs.com/png.latex?c"> (<img src="https://latex.codecogs.com/png.latex?W%5E%7B(m,%20c)%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5C%7B%20f%20:%20f%20%5Cin%20W%5E%7B(m)%7D,%20%5C%7CD%5Em%20f%20%5C%7C_2%20%5Cleq%20c%5E2%20%5C%7D">.
</dd>
<dt>Periodic Sobolev class (<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BW%7D%5E%7B(m,%20c)%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5C%7B%20f%20%5Cin%20W%5E%7B(m,%20c)%7D%20:%20D%5Ej%20f(0)%20=%20D%5Ej%20f(1),%20j%20=%200,%20%5Cldots,%20m%20-%201%20%5C%7D">.
</dd>
<dt>Ellipsoid (<img src="https://latex.codecogs.com/png.latex?%5CTheta">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5C%7B%20%5Ctheta%20:%20%5Csum_%7Bj=1%7D%5E%5Cinfty%20a_j%5E2%20%5Ctheta_j%5E2%20%5Cleq%20c%5E2%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?a_j"> is a sequence of numbers such that <img src="https://latex.codecogs.com/png.latex?a_j%20%5Cto%20%5Cinfty"> as <img src="https://latex.codecogs.com/png.latex?j%20%5Cto%20%5Cinfty">.
</dd>
<dt>Minimax risk</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Cinf_%7B%5Chat%7B%5Ctheta%7D_n%7D%20%5Csup_%7B%5Ctheta%20%5Cin%20%5CTheta%7D%20E_%5Ctheta%20%5BL(%5Chat%7B%5Ctheta%7D_n,%20%5Ctheta)%5D"> where the infimum is over all estimators <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_n"> and the supremum is over a class of parameters <img src="https://latex.codecogs.com/png.latex?%5CTheta">.
</dd>
<dt>Linear shrinkage estimator (in Normal means problem)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D%20=%20bZ%20=%20(bZ_1,%20%5Cldots,%20b_n%20Z_n)">.
</dd>
<dt>Soft threshold estimator</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_i%20=%20%5Ctext%7Bsign%7D(Z_i)(%7CZ_i%7C%20-%20%5Clambda)_+">.
</dd>
<dt>Hard threshold estimator</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_i%20=%20Z_i%20I(%7CZ_i%7C%20%3E%20%5Clambda)">.
</dd>
<dt>Multiresolution analysis (MRA) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> (generated by <img src="https://latex.codecogs.com/png.latex?%5Cphi">)</dt>
<dd>
A sequence of closed subspaces <img src="https://latex.codecogs.com/png.latex?V_j%20%5Csubset%20L_2(%5Cmathbb%7BR%7D)">, <img src="https://latex.codecogs.com/png.latex?j%20%5Cin%20%5Cmathbb%7BZ%7D">, such that <img src="https://latex.codecogs.com/png.latex?V_j%20%5Csubset%20V_%7Bj+1%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbigcup_%7Bj%20%5Cin%20%5Cmathbb%7BZ%7D%7D%20V_j"> is dense in <img src="https://latex.codecogs.com/png.latex?L_2(%5Cmathbb%7BR%7D)">, <img src="https://latex.codecogs.com/png.latex?%5Cbigcap_%7Bj%20%5Cin%20%5Cmathbb%7BZ%7D%7D%20V_j%20=%20%5C%7B0%5C%7D">, <img src="https://latex.codecogs.com/png.latex?f(x)%20%5Cin%20V_j%20%5CLeftrightarrow%20f(2x)%20%5Cin%20V_%7Bj+1%7D">, and there exists a scaling function (father wavelet) <img src="https://latex.codecogs.com/png.latex?%5Cphi%20%5Cin%20V_0"> such that <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Cphi(x%20-%20k)%20:%20k%20%5Cin%20%5Cmathbb%7BZ%7D%5C%7D"> forms a Riesz basis for <img src="https://latex.codecogs.com/png.latex?V_0">.
</dd>
<dt>Father wavelet (scaling function) (<img src="https://latex.codecogs.com/png.latex?%5Cphi">)</dt>
<dd>
A function in <img src="https://latex.codecogs.com/png.latex?V_0"> that generates the basis for <img src="https://latex.codecogs.com/png.latex?V_0"> in a multiresolution analysis.
</dd>
<dt>Mother wavelet (<img src="https://latex.codecogs.com/png.latex?%5Cpsi">)</dt>
<dd>
A function such that <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Cpsi_%7Bjk%7D(x)%20=%202%5E%7Bj/2%7D%5Cpsi(2%5Ej%20x%20-%20k)%20:%20j,%20k%20%5Cin%20%5Cmathbb%7BZ%7D%5C%7D"> forms an orthonormal basis for <img src="https://latex.codecogs.com/png.latex?L_2(%5Cmathbb%7BR%7D)">.
</dd>
<dt>Wavelet regression</dt>
<dd>
A method to estimate a regression function by projecting the data onto a wavelet basis and then shrinking the wavelet coefficients.
</dd>
<dt>Thresholding</dt>
<dd>
A nonlinear shrinkage method used in wavelet regression where small wavelet coefficients are set to zero.
</dd>
<dt>Hard thresholding (estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7Bjk%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7Bjk%7D%20=%20%5Cleft%5C%7B%20%5Cbegin%7Barray%7D%7Bll%7D%200%20&amp;%20%5Ctext%7Bif%20%7D%20%7CD_%7Bjk%7D%7C%20%3C%20%5Clambda%20%5C%5C%20D_%7Bjk%7D%20&amp;%20%5Ctext%7Bif%20%7D%20%7CD_%7Bjk%7D%7C%20%5Cgeq%20%5Clambda%20%5Cend%7Barray%7D%20%5Cright.">.
</dd>
<dt>Soft thresholding (estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7Bjk%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7Bjk%7D%20=%20%5Ctext%7Bsign%7D(D_%7Bjk%7D)(%7CD_%7Bjk%7D%7C%20-%20%5Clambda)_+">.
</dd>
<dt>Universal threshold (<img src="https://latex.codecogs.com/png.latex?%5Clambda">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D%20%5Csqrt%7B%5Cfrac%7B2%20%5Clog%20n%7D%7Bn%7D%7D">.
</dd>
<dt>Besov space (<img src="https://latex.codecogs.com/png.latex?B_%7Bp,q%7D%5E%5Cxi">)</dt>
<dd>
A function space that generalizes Sobolev spaces and characterizes smoothness in terms of differences.
</dd>
<dt>Overcomplete dictionary</dt>
<dd>
A set of basis functions where the number of basis functions <img src="https://latex.codecogs.com/png.latex?m"> is greater than the number of observations <img src="https://latex.codecogs.com/png.latex?n">.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ol type="1">
<li><strong>Introduction</strong>
<ul>
<li>1.1 <strong>What Is Nonparametric Inference?</strong>
<ul>
<li>Basic idea: Use data to infer an unknown quantity with <strong>minimal assumptions</strong>.</li>
<li>Often involves <strong>infinite-dimensional statistical models</strong>.</li>
<li>The term ‚Äònonparametric inference‚Äô in this book refers to modern statistical methods that aim to keep the number of underlying assumptions as weak as possible.</li>
<li>Problems considered in the book include:
<ul>
<li>Estimating the <strong>distribution function (cdf)</strong>: Given an iid sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n%20%5Csim%20F">, estimate <img src="https://latex.codecogs.com/png.latex?F(x)%20=%20P(X%20%E2%89%A4%20x)">.</li>
<li>Estimating <strong>functionals</strong>: Given an iid sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n%20%5Csim%20F">, estimate a functional <img src="https://latex.codecogs.com/png.latex?T(F)"> such as the mean <img src="https://latex.codecogs.com/png.latex?T(F%20)%20=%20%E2%88%AB%20xdF%20(x)">.</li>
<li><strong>Density estimation</strong>: Given an iid sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n%20%5Csim%20F">, estimate the density <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20F%20%E2%80%B2(x)">.</li>
<li><strong>Nonparametric regression or curve estimation</strong>: Given <img src="https://latex.codecogs.com/png.latex?(X_1,%20Y_1),%20.%20.%20.%20,%20(X_n,%20Yn)"> estimate the regression function <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20E(Y%20%7CX%20=%20x)">.</li>
<li><strong>Normal means</strong>: Given <img src="https://latex.codecogs.com/png.latex?Y_i%20%5Csim%20N(%CE%B8_i,%20%CF%83%5E2)">, <img src="https://latex.codecogs.com/png.latex?i%20=%201,%20.%20.%20.%20,%20n">, estimate <img src="https://latex.codecogs.com/png.latex?%CE%B8%20=%20(%CE%B8_1,%20.%20.%20.%20,%20%CE%B8_n)">.</li>
</ul></li>
<li>Typically assumes the unknown quantity (distribution <img src="https://latex.codecogs.com/png.latex?F">, density <img src="https://latex.codecogs.com/png.latex?f">, or regression function <img src="https://latex.codecogs.com/png.latex?r">) lies in some <strong>large set <img src="https://latex.codecogs.com/png.latex?F"> called a statistical model</strong>. For example, when estimating a density <img src="https://latex.codecogs.com/png.latex?f">, we might assume <img src="https://latex.codecogs.com/png.latex?f%20%E2%88%88%20F%20=%20%5C%7B%20g%20:%20%E2%88%AB%20(g%E2%80%B2%E2%80%B2(x))%5E2dx%20%E2%89%A4%20c%5E2%20%5C%7D">.</li>
</ul></li>
<li>1.2 <strong>Notation and Background</strong>
<ul>
<li>Summary of some useful notation (see also Table 1.1).</li>
<li>Definition of <strong>mean of <img src="https://latex.codecogs.com/png.latex?a"></strong>: <img src="https://latex.codecogs.com/png.latex?E(a(X))%20=%20%E2%88%AB%20a(x)dF%20(x)%20%E2%89%A1%20%5C%7B%20%E2%88%AB%20a(x)f(x)dx"> continuous case, <img src="https://latex.codecogs.com/png.latex?%E2%88%91_j%20a(x_j)f(x_j)"> discrete case.</li>
<li>Definition of <strong>variance of <img src="https://latex.codecogs.com/png.latex?X"></strong>: <img src="https://latex.codecogs.com/png.latex?V(X)%20=%20E(X%20%E2%88%92%20E(X))%5E2">.</li>
<li>Brief review of probability.
<ul>
<li><strong>Sample space <img src="https://latex.codecogs.com/png.latex?%CE%A9"></strong>: The set of possible outcomes of an experiment.</li>
<li><strong>Events and <img src="https://latex.codecogs.com/png.latex?%CF%83">-field <img src="https://latex.codecogs.com/png.latex?A"></strong>: A class of events <img src="https://latex.codecogs.com/png.latex?A"> is a <img src="https://latex.codecogs.com/png.latex?%CF%83">-field if (i) <img src="https://latex.codecogs.com/png.latex?%E2%88%85%20%E2%88%88%20A">, (ii) <img src="https://latex.codecogs.com/png.latex?A%20%E2%88%88%20A"> implies that <img src="https://latex.codecogs.com/png.latex?A%5Ec%20%E2%88%88%20A"> and (iii) <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20.%20.%20.%20%E2%88%88%20A"> implies that <img src="https://latex.codecogs.com/png.latex?%E2%8B%83_%7Bi=1%7D%5E%E2%88%9E%20A_i%20%E2%88%88%20A">.</li>
<li><strong>Probability measure <img src="https://latex.codecogs.com/png.latex?P"></strong>: A function <img src="https://latex.codecogs.com/png.latex?P"> defined on a <img src="https://latex.codecogs.com/png.latex?%CF%83">-field <img src="https://latex.codecogs.com/png.latex?A"> such that <img src="https://latex.codecogs.com/png.latex?P(A)%20%E2%89%A5%200"> for all <img src="https://latex.codecogs.com/png.latex?A%20%E2%88%88%20A">, <img src="https://latex.codecogs.com/png.latex?P(%CE%A9)%20=%201"> and if <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20.%20.%20.%20%E2%88%88%20A"> are disjoint then <img src="https://latex.codecogs.com/png.latex?P(%E2%8B%83_%7Bi=1%7D%5E%E2%88%9E%20A_i)%20=%20%E2%88%91_%7Bi=1%7D%5E%E2%88%9E%20P(A_i)">.</li>
<li><strong>Probability space <img src="https://latex.codecogs.com/png.latex?(%CE%A9,%20A,%20P)"></strong>: The triple consisting of the sample space, the <img src="https://latex.codecogs.com/png.latex?%CF%83">-field, and the probability measure.</li>
<li><strong>Random variable <img src="https://latex.codecogs.com/png.latex?X%20:%20%CE%A9%20%E2%86%92%20R"></strong>: A map <img src="https://latex.codecogs.com/png.latex?X%20:%20%CE%A9%20%E2%86%92%20R"> such that, for every real <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?%5C%7B%CF%89%20%E2%88%88%20%CE%A9%20:%20X(%CF%89)%20%E2%89%A4%20x%5C%7D%20%E2%88%88%20A">.</li>
<li><strong>Mean squared error (mse)</strong>: <img src="https://latex.codecogs.com/png.latex?mse%20=%20bias%5E2(%CE%B8%CC%82_n)%20+%20V(%CE%B8%CC%82_n)"> (1.10).</li>
</ul></li>
</ul></li>
<li>1.3 <strong>Confidence Sets</strong>
<ul>
<li>Much of nonparametric inference is devoted to finding an estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%CE%B8%7D_n"> of some quantity of interest <img src="https://latex.codecogs.com/png.latex?%CE%B8"> and providing <strong>confidence sets</strong> for these quantities.</li>
<li>Let <img src="https://latex.codecogs.com/png.latex?F"> be a class of distribution functions and let <img src="https://latex.codecogs.com/png.latex?%CE%B8"> be some quantity of interest. Let <img src="https://latex.codecogs.com/png.latex?C_n"> be a set of possible values of <img src="https://latex.codecogs.com/png.latex?%CE%B8"> which depends on the data <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n">. The coverage of <img src="https://latex.codecogs.com/png.latex?C_n"> is <img src="https://latex.codecogs.com/png.latex?P_F(%CE%B8%20%E2%88%88%20C_n)">. <img src="https://latex.codecogs.com/png.latex?C_n"> is a <img src="https://latex.codecogs.com/png.latex?1%20%E2%88%92%20%CE%B1"> confidence set if <img src="https://latex.codecogs.com/png.latex?%5Cinf_%7BF%7D%20P_F(%CE%B8%20%E2%88%88%20C_n)%20%E2%89%A5%201%20%E2%88%92%20%CE%B1">.</li>
</ul></li>
<li>1.4 <strong>Useful Inequalities</strong>
<ul>
<li><strong>Jensen‚Äôs inequality</strong>: If <img src="https://latex.codecogs.com/png.latex?g"> is convex then <img src="https://latex.codecogs.com/png.latex?Eg(X)%20%E2%89%A5%20g(EX)"> (1.32). If <img src="https://latex.codecogs.com/png.latex?g"> is concave then <img src="https://latex.codecogs.com/png.latex?Eg(X)%20%E2%89%A4%20g(EX)"> (1.33).</li>
</ul></li>
<li>1.5 <strong>Bibliographic Remarks</strong>
<ul>
<li>References on probability inequalities and their use in statistics and pattern recognition include Devroye et al.&nbsp;(1996) and van der Vaart and Wellner (1996). To review basic probability and mathematical statistics, I recommend Casella and Berger (2002), van der Vaart (1998) and Wasserman (2004).</li>
</ul></li>
</ul></li>
<li><strong>Estimating the cdf and Statistical Functionals</strong>
<ul>
<li>2.1 <strong>The cdf</strong>
<ul>
<li>Definition of the empirical cumulative distribution function (cdf) <img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_n(x)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20I(X_i%20%5Cle%20x)">.</li>
<li>Glivenko-Cantelli Theorem: <img src="https://latex.codecogs.com/png.latex?%5CVert%20%5Chat%7BF%7D_n%20-%20F%20%5CVert_%5Cinfty%20%5Cstackrel%7Ba.s.%7D%7B%5Clongrightarrow%7D%200">.</li>
<li>Dvoretzky-Kiefer-Wolfowitz (DKW) inequality: <img src="https://latex.codecogs.com/png.latex?P(%5CVert%20%5Chat%7BF%7D_n%20-%20F%20%5CVert_%5Cinfty%20%3E%20%5Cepsilon)%20%5Cle%202e%5E%7B-2n%5Cepsilon%5E2%7D"> for all <img src="https://latex.codecogs.com/png.latex?F"> and all <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%3E%200">.</li>
<li>Construction of a nonparametric <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> confidence band for <img src="https://latex.codecogs.com/png.latex?F(x)"> using the DKW inequality: <img src="https://latex.codecogs.com/png.latex?(L(x),%20U(x))"> where <img src="https://latex.codecogs.com/png.latex?L(x)%20=%20%5Cmax%5C%7B%5Chat%7BF%7D_n(x)%20-%20%5Cepsilon_n,%200%5C%7D"> and <img src="https://latex.codecogs.com/png.latex?U(x)%20=%20%5Cmin%5C%7B%5Chat%7BF%7D_n(x)%20+%20%5Cepsilon_n,%201%5C%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_n%20=%20%5Csqrt%7B%5Cfrac%7B1%7D%7B2n%7D%20%5Clog(%5Cfrac%7B2%7D%7B%5Calpha%7D)%7D">.</li>
<li>Theorem 2.6 summarizes the construction of the confidence band.</li>
<li>Example 2.7 illustrates a 95 percent confidence band.</li>
</ul></li>
<li>2.2 <strong>Estimating Statistical Functionals</strong>
<ul>
<li>Introduction to statistical functionals <img src="https://latex.codecogs.com/png.latex?T(F)"> which map a distribution function <img src="https://latex.codecogs.com/png.latex?F"> to a number.</li>
<li>Plug-in estimator: Estimate <img src="https://latex.codecogs.com/png.latex?T(F)"> by <img src="https://latex.codecogs.com/png.latex?T(%5Chat%7BF%7D_n)">.</li>
<li>Example 2.32 (The mean): Plug-in estimator for the mean <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20%5Cint%20xdF(x)"> is <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D%20=%20%5Cint%20xd%5Chat%7BF%7D_n(x)%20=%20%5Cbar%7BX%7D_n">. Asymptotic nonparametric 95 percent confidence interval for <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D_n%20%5Cpm%202%20%5Chat%7Bse%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bse%7D%5E2%20=%20%5Chat%7B%5Csigma%7D%5E2/n">.</li>
<li>Statistical functionals in the form <img src="https://latex.codecogs.com/png.latex?T(F)%20=%20a(T_1(F),%20.%20.%20.%20,%20T_m(F))">.</li>
</ul></li>
<li>2.3 <strong>Influence Functions</strong>
<ul>
<li>Definition of the influence function <img src="https://latex.codecogs.com/png.latex?L(x)"> of a functional <img src="https://latex.codecogs.com/png.latex?T"> at <img src="https://latex.codecogs.com/png.latex?F">.</li>
<li><img src="https://latex.codecogs.com/png.latex?T((1%20-%20%5Cepsilon)F%20+%20%5Cepsilon%20%5Cdelta_x)%20%5Capprox%20T(F)%20+%20%5Cepsilon%20L(x)"> for small <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">, where <img src="https://latex.codecogs.com/png.latex?%5Cdelta_x"> is a point mass at <img src="https://latex.codecogs.com/png.latex?x">.</li>
<li>Asymptotic variance of the plug-in estimator <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bn%7D(T(%5Chat%7BF%7D_n)%20-%20T(F))%20%5Cstackrel%7Bd%7D%7B%5Clongrightarrow%7D%20N(0,%20%5Csigma%5E2)"> where <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20=%20V_F(L(X_1))">.</li>
<li>Estimated standard error <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bse%7D%20=%20(%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20%5Chat%7BL%7D(X_i)%5E2)%5E%7B1/2%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7BL%7D"> is the estimated influence function.</li>
<li>Pointwise asymptotic <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> confidence interval <img src="https://latex.codecogs.com/png.latex?T(%5Chat%7BF%7D_n)%20%5Cpm%20z_%7B1-%5Calpha/2%7D%20%5Chat%7Bse%7D">.</li>
</ul></li>
<li>2.4 <strong>Empirical Probability Distributions</strong>
<ul>
<li>The empirical probability distribution <img src="https://latex.codecogs.com/png.latex?P_n"> assigns mass <img src="https://latex.codecogs.com/png.latex?1/n"> to each observation <img src="https://latex.codecogs.com/png.latex?X_i">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_n(x)%20=%20P_n((-%5Cinfty,%20x%5D)">.</li>
</ul></li>
<li>2.5 <strong>Bibliographic Remarks</strong>
<ul>
<li>Provides references for further reading on empirical processes, statistical functionals, and influence functions.</li>
</ul></li>
<li>2.6 <strong>Appendix</strong>
<ul>
<li>(Content of the appendix is not detailed in the provided excerpts).</li>
</ul></li>
</ul></li>
<li><strong>The Bootstrap and the Jackknife</strong>
<ul>
<li>3.1 <strong>The Jackknife</strong>
<ul>
<li>Introduction to the jackknife as a method for estimating bias and variance.</li>
<li>Definition of the jackknife estimator <img src="https://latex.codecogs.com/png.latex?T_%7B(i)%7D">: the estimator computed by omitting the <img src="https://latex.codecogs.com/png.latex?i">-th observation.</li>
<li>Jackknife estimate of bias: <img src="https://latex.codecogs.com/png.latex?bias_%7Bjack%7D%20=%20(n-1)(%5Cbar%7BT%7D_%7B(.)%7D%20-%20T_n)">, where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BT%7D_%7B(.)%7D%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20T_%7B(i)%7D"> and <img src="https://latex.codecogs.com/png.latex?T_n%20=%20T(X_1,%20.%20.%20.%20,%20X_n)">.</li>
<li>Jackknife estimate of variance: <img src="https://latex.codecogs.com/png.latex?V_%7Bjack%7D%20=%20%5Cfrac%7Bn-1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(T_%7B(i)%7D%20-%20%5Cbar%7BT%7D_%7B(.)%7D)%5E2">.</li>
<li>Approximate <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> confidence interval using the jackknife: <img src="https://latex.codecogs.com/png.latex?T_n%20%5Cpm%20t_%7Bn-1,%201-%5Calpha/2%7D%20%5Csqrt%7BV_%7Bjack%7D%7D">.</li>
<li>Example 3.3 illustrating jackknife estimation of bias and standard error for the mean.</li>
<li>Example 3.5 showing the inconsistency of the jackknife variance estimator for the median.</li>
</ul></li>
<li>3.2 <strong>The Bootstrap</strong>
<ul>
<li>Introduction to the bootstrap as a general method for statistical inference based on resampling.</li>
<li>Nonparametric bootstrap procedure:
<ol type="1">
<li>Draw a bootstrap sample <img src="https://latex.codecogs.com/png.latex?X%5E*_1,%20.%20.%20.%20,%20X%5E*_n"> with replacement from the original data <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n">.</li>
<li>Compute the statistic of interest <img src="https://latex.codecogs.com/png.latex?T%5E*_n%20=%20T(X%5E*_1,%20.%20.%20.%20,%20X%5E*_n)">.</li>
<li>Repeat steps 1 and 2 <img src="https://latex.codecogs.com/png.latex?B"> times to get <img src="https://latex.codecogs.com/png.latex?T%5E*_%7Bn,1%7D,%20.%20.%20.%20,%20T%5E*_%7Bn,B%7D">.</li>
</ol></li>
<li>Bootstrap estimate of standard error: <img src="https://latex.codecogs.com/png.latex?se_%7Bboot%7D%20=%20%5Csqrt%7B%5Cfrac%7B1%7D%7BB%7D%20%5Csum_%7Bb=1%7D%5EB%20(T%5E*_%7Bn,b%7D%20-%20%5Cbar%7BT%7D%5E*_n)%5E2%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BT%7D%5E*_n%20=%20%5Cfrac%7B1%7D%7BB%7D%20%5Csum_%7Bb=1%7D%5EB%20T%5E*_%7Bn,b%7D">.</li>
<li>Example 3.12 provides pseudo-code for bootstrapping the median.</li>
</ul></li>
<li>3.3 <strong>Parametric Bootstrap</strong>
<ul>
<li>Procedure for parametric bootstrap:
<ol type="1">
<li>Assume a parametric model <img src="https://latex.codecogs.com/png.latex?F_%5Ctheta"> for the data.</li>
<li>Estimate the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> from the data to get <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D">.</li>
<li>Draw bootstrap samples from <img src="https://latex.codecogs.com/png.latex?F_%7B%5Chat%7B%5Ctheta%7D%7D">.</li>
<li>Compute the statistic of interest for each bootstrap sample.</li>
</ol></li>
<li>Useful when a good parametric model is known or can be assumed.</li>
</ul></li>
<li>3.4 <strong>Bootstrap Confidence Intervals</strong>
<ul>
<li>Introduction to different types of bootstrap confidence intervals.</li>
<li><strong>Normal approximation interval</strong>: <img src="https://latex.codecogs.com/png.latex?T_n%20%5Cpm%20z_%7B1-%5Calpha/2%7D%20se_%7Bboot%7D">.</li>
<li><strong>Percentile interval</strong>: <img src="https://latex.codecogs.com/png.latex?C_n%20=%20(T%5E*_%7B(B%5Calpha/2)%7D,%20T%5E*_%7B(B(1-%5Calpha/2))%7D)">, using the <img src="https://latex.codecogs.com/png.latex?%5Calpha/2"> and <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha/2"> quantiles of the bootstrap sample.</li>
<li><strong>Pivotal interval</strong>: Based on a pivotal quantity <img src="https://latex.codecogs.com/png.latex?R_n(X,%20%5Ctheta)"> whose distribution does not depend on <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Approximate the distribution of <img src="https://latex.codecogs.com/png.latex?R_n(X,%20%5Ctheta)"> by the distribution of <img src="https://latex.codecogs.com/png.latex?R%5E*_n(X%5E*,%20T_n)">. The <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> bootstrap pivotal interval is <img src="https://latex.codecogs.com/png.latex?(T_n%20-%20R%5E*_%7B1-%5Calpha/2%7D,%20T_n%20-%20R%5E*_%7B%5Calpha/2%7D)">, where <img src="https://latex.codecogs.com/png.latex?R%5E*_%5Cbeta"> is the <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> quantile of <img src="https://latex.codecogs.com/png.latex?R%5E*_n">.</li>
<li><strong>Bootstrap studentized pivotal interval</strong>: <img src="https://latex.codecogs.com/png.latex?(%5Chat%7B%5Ctheta%7D_n%20-%20z%5E*_%7B1-%5Calpha/2%7D%20%5Chat%7Bse%7D_%7Bboot%7D,%20%5Chat%7B%5Ctheta%7D_n%20-%20z%5E*_%7B%5Calpha/2%7D%20%5Chat%7Bse%7D_%7Bboot%7D)">, where <img src="https://latex.codecogs.com/png.latex?z%5E*_%5Cbeta"> is the <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> quantile of <img src="https://latex.codecogs.com/png.latex?Z%5E*_n,b%20=%20%5Cfrac%7BT%5E*_%7Bn,b%7D%20-%20T_n%7D%7B%5Chat%7Bse%7D%5E*_b%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bse%7D%5E*_b"> is an estimate of the standard error of <img src="https://latex.codecogs.com/png.latex?T%5E*_%7Bn,b%7D">.</li>
<li>Example 3.17 shows various bootstrap confidence intervals for skewness.</li>
</ul></li>
<li>3.5 <strong>Some Theory</strong>
<ul>
<li>Brief overview of theoretical aspects of the bootstrap.</li>
<li>Consistency of the bootstrap distribution.</li>
<li>Discussion of when the bootstrap works and when it might fail.</li>
</ul></li>
<li>3.6 <strong>Bibliographic Remarks</strong>
<ul>
<li>Provides references for further reading on the jackknife and bootstrap methods.</li>
</ul></li>
<li>3.7 <strong>Appendix</strong>
<ul>
<li>(Content of the appendix is not detailed in the provided excerpts).</li>
</ul></li>
</ul></li>
<li><strong>Smoothing: General Concepts</strong>
<ul>
<li>Introduction to the necessity of smoothing data to estimate curves like probability density functions (<img src="https://latex.codecogs.com/png.latex?f">) or regression functions (<img src="https://latex.codecogs.com/png.latex?r">).</li>
<li>Discussion of two main types of problems studied:
<ul>
<li><strong>Density estimation</strong>: Estimating the probability density function <img src="https://latex.codecogs.com/png.latex?f"> given a sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n%20%5Csim%20f">.</li>
<li><strong>Regression</strong>: Estimating the regression function <img src="https://latex.codecogs.com/png.latex?r"> given pairs <img src="https://latex.codecogs.com/png.latex?(x_1,%20Y_1),%20.%20.%20.%20,%20(x_n,%20Yn)"> where <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20r(x_i)%20+%20%5Cepsilon_i"> and <img src="https://latex.codecogs.com/png.latex?E(%5Cepsilon_i)%20=%200">.</li>
</ul></li>
<li>Example 4.3 (Density estimation) showing histograms of astronomy data with different amounts of smoothing.</li>
<li>Example 4.4 (Nonparametric regression) discussing the Cosmic Microwave Background (CMB) data.</li>
<li>Example 4.5 (Nonparametric regression) illustrating the LIDAR experiment data and regressograms.</li>
<li>Example 4.6 (Nonparametric regression) showing BPD (Bronchopulmonary Dysplasia) data and fits from different regression methods.</li>
<li>Example 4.7 (Nonparametric regression) presenting rock data and additive model fits.</li>
<li>4.1 <strong>The Bias‚ÄìVariance Tradeoff</strong>
<ul>
<li>Introduction to the concepts of bias and variance in the context of curve estimation.</li>
<li>Definition of <strong>risk or mean squared error (mse)</strong>: <img src="https://latex.codecogs.com/png.latex?mse%20=%20R(f(x),%20%5Chat%7Bf%7D_n(x))%20=%20E(L(f(x),%20%5Chat%7Bf%7D_n(x)))"> (4.9).</li>
<li>Relationship between risk, bias, and variance: <img src="https://latex.codecogs.com/png.latex?R(f(x),%20%5Chat%7Bf%7D_n(x))%20=%20bias_x%5E2%20+%20V_x"> (4.10) and <img src="https://latex.codecogs.com/png.latex?risk%20=%20mse%20=%20bias%5E2%20+%20variance"> (4.11).</li>
<li>Illustration of the bias-variance tradeoff where bias increases and variance decreases with more smoothing.</li>
<li>Example illustrating the mse of a histogram estimator: <img src="https://latex.codecogs.com/png.latex?mse%20%5Capprox%20Ah%5E4%20+%20B/(nh)"> (4.18).</li>
</ul></li>
<li>4.2 <strong>Kernels</strong>
<ul>
<li>Introduction to kernels as a smoothing tool.</li>
<li>Examples of common kernels: boxcar, Gaussian, Epanechnikov, and tricube.</li>
</ul></li>
<li>4.3 <strong>Which Loss Function?</strong>
<ul>
<li>Discussion of loss functions beyond squared error, such as <img src="https://latex.codecogs.com/png.latex?L_p"> loss and Kullback‚ÄìLeibler loss.</li>
<li>Reason for the continued popularity of <img src="https://latex.codecogs.com/png.latex?L_2"> (squared error) loss.</li>
</ul></li>
<li>4.4 <strong>Confidence Sets</strong>
<ul>
<li>Brief mention of the desire to provide confidence sets for estimated curves.</li>
</ul></li>
<li>4.5 <strong>The Curse of Dimensionality</strong>
<ul>
<li>Explanation of how the difficulty of nonparametric estimation increases with the dimensionality of the data.</li>
<li>Even with computational advancements, the statistical challenge of high-dimensional data remains, leading to large confidence intervals.</li>
</ul></li>
<li>4.6 <strong>Bibliographic Remarks</strong>
<ul>
<li>Listing of several key texts on smoothing methods.</li>
</ul></li>
</ul></li>
<li><strong>Nonparametric Regression</strong>
<ul>
<li>Introduction to nonparametric regression, also known as ‚Äúlearning a function‚Äù.</li>
<li>Given <img src="https://latex.codecogs.com/png.latex?n"> pairs of observations <img src="https://latex.codecogs.com/png.latex?(x_1,%20Y_1),%20.%20.%20.%20,%20(x_n,%20Yn)"> where <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20r(x_i)%20+%20%5Cepsilon_i"> and <img src="https://latex.codecogs.com/png.latex?E(%5Cepsilon_i)%20=%200">.</li>
<li>The goal is to estimate the regression function <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20E(Y%20%7CX%20=%20x)">.</li>
<li>Methods considered include local regression methods (kernel regression, local polynomial regression) and penalization methods (splines).</li>
<li>Chapter 8 and 9 will cover an approach based on orthogonal functions.</li>
<li>All estimators in this chapter are linear smoothers.</li>
<li>5.1 <strong>Review of Linear and Logistic Regression</strong>
<ul>
<li>Brief review of standard parametric regression techniques as a contrast to nonparametric methods.</li>
</ul></li>
<li>5.2 <strong>Linear Smoothers</strong>
<ul>
<li>Definition of a linear smoother: An estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n"> of <img src="https://latex.codecogs.com/png.latex?r"> is a linear smoother if <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)%20=%20%5Csum_%7Bi=1%7D%5En%20%5Cbeta_i(x)Y_i">.</li>
<li>Vector of fitted values <img src="https://latex.codecogs.com/png.latex?r%20=%20(%5Chat%7Br%7D_n(x_1),%20.%20.%20.%20,%20%5Chat%7Br%7D_n(x_n))%5ET">.</li>
<li>Relationship <img src="https://latex.codecogs.com/png.latex?r%20=%20LY"> where <img src="https://latex.codecogs.com/png.latex?L"> is the smoothing matrix.</li>
</ul></li>
<li>5.3 <strong>Choosing the Smoothing Parameter</strong>
<ul>
<li>The importance of selecting an appropriate smoothing parameter (e.g., bandwidth <img src="https://latex.codecogs.com/png.latex?h">).</li>
<li>Cross-validation as a common data-driven method for choosing the smoothing parameter.</li>
<li>The cross-validation score <img src="https://latex.codecogs.com/png.latex?cv%20=%20%5Chat%7BR%7D(h)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(Y_i%20-%20%5Chat%7Br%7D%5E%7B(-i)%7D(x_i))%5E2"> (5.30).</li>
<li>Definition of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D%5E%7B(-i)%7D(x)%20=%20%5Csum_%7Bj=1%7D%5En%20Y_j%20%5Cbeta_%7Bj,(-i)%7D(x)"> (5.31) with specific conditions on <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7Bj,(-i)%7D(x)"> (5.32).</li>
<li>Generalized cross-validation (GCV) as an alternative.</li>
</ul></li>
<li>5.4 <strong>Local Regression</strong>
<ul>
<li>Introduction to local regression techniques.</li>
<li><strong>Kernel regression (Nadaraya-Watson estimator)</strong>: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5En%20K(%5Cfrac%7Bx-x_i%7D%7Bh%7D)Y_i%7D%7B%5Csum_%7Bi=1%7D%5En%20K(%5Cfrac%7Bx-x_i%7D%7Bh%7D)%7D"> (5.38).</li>
<li><strong>Local polynomial regression</strong>: Fitting a local polynomial of degree <img src="https://latex.codecogs.com/png.latex?p"> in a neighborhood of <img src="https://latex.codecogs.com/png.latex?x">.</li>
<li>Example 5.54 illustrating local linear regression for the LIDAR data.</li>
<li>Theorem 5.60 stating that the local linear estimator has weights <img src="https://latex.codecogs.com/png.latex?%5Cbeta_i(x)%20=%20%5Cfrac%7BK_h(x-x_i)(S_2(x)%20-%20S_1(x)(x_i%20-%20x))%7D%7BS_0(x)S_2(x)%20-%20S_1(x)%5E2%7D"> where <img src="https://latex.codecogs.com/png.latex?S_j(x)%20=%20%5Csum_%7Bi=1%7D%5En%20K_h(x-x_i)(x_i%20-%20x)%5Ej">.</li>
</ul></li>
<li>5.5 <strong>Penalized Regression, Regularization and Splines</strong>
<ul>
<li>Introduction to penalized regression and regularization methods.</li>
<li><strong>Smoothing splines</strong>: Minimizing a penalized residual sum of squares <img src="https://latex.codecogs.com/png.latex?M(%5Clambda)%20=%20%5Csum_%7Bi=1%7D%5En%20(Y_i%20-%20r(x_i))%5E2%20+%20%5Clambda%20%5Cint%20(r''(x))%5E2%20dx"> (5.70, 5.71).</li>
<li>Theorem 5.73 stating that the minimizer is a natural cubic spline with knots at the data points.</li>
<li><strong>Basis functions for splines</strong>: Truncated power basis (Theorem 5.74) and B-spline basis (Theorem 5.76).</li>
</ul></li>
<li>5.6 <strong>Variance Estimation</strong>
<ul>
<li>Estimating the variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2(x)%20=%20V(%5Cepsilon%7CX=x)">.</li>
<li>Method based on squared residuals <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D%5E2(x)%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5En%20K_h(x-x_i)(Y_i%20-%20%5Chat%7Br%7D_n(x_i))%5E2%7D%7B%5Csum_%7Bi=1%7D%5En%20K_h(x-x_i)%7D"> (5.89).</li>
<li>Iterative procedure for estimating <img src="https://latex.codecogs.com/png.latex?r(x)"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2(x)">.</li>
<li>Example 5.96 showing variance estimation for the CMB data.</li>
</ul></li>
<li>5.7 <strong>Confidence Bands</strong>
<ul>
<li>Constructing confidence bands for the regression function <img src="https://latex.codecogs.com/png.latex?r(x)">.</li>
<li>Typically of the form <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)%20%5Cpm%20c%20%5C%20se(x)"> (5.97).</li>
<li>The bias problem: confidence bands are often for <img src="https://latex.codecogs.com/png.latex?r_n(x)%20=%20E(%5Chat%7Br%7D_n(x))"> not <img src="https://latex.codecogs.com/png.latex?r(x)">.</li>
<li>Constructing approximate pointwise confidence bands using the asymptotic normality of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)">.</li>
<li>Simultaneous confidence bands using <img src="https://latex.codecogs.com/png.latex?%5Ckappa_0"> and <img src="https://latex.codecogs.com/png.latex?c"> derived from the <img src="https://latex.codecogs.com/png.latex?L_%5Cinfty"> norm of a Gaussian process.</li>
<li>Approximate <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> simultaneous confidence band <img src="https://latex.codecogs.com/png.latex?I(x)%20=%20%5Chat%7Br%7D_n(x)%20%5Cpm%20c%20%5Chat%7B%5Csigma%7D(x)%20%7C%7C%5Cbeta(x)%7C%7C"> (5.104).</li>
<li>Example 5.105 showing simultaneous confidence bands for the CMB data.</li>
<li>Remark on adjusting for the uncertainty in the smoothing parameter choice using the Bonferroni inequality.</li>
<li>Remark on using bootstrap methods for confidence bands.</li>
</ul></li>
<li>5.8 <strong>Average Coverage</strong>
<ul>
<li>Discussion of confidence bands with average coverage instead of pointwise or simultaneous coverage.</li>
</ul></li>
<li>5.9 <strong>Summary of Linear Smoothing</strong>
<ul>
<li>Steps to construct the estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n"> and a confidence band.</li>
<li>Example 5.110 applying the summary to the LIDAR data.</li>
</ul></li>
<li>5.10 <strong>Local Likelihood and Exponential Families</strong>
<ul>
<li>Extending local methods to likelihood-based estimation for exponential families.</li>
<li>Local likelihood estimation for binary regression (logistic regression).</li>
<li>Example 5.119 showing local linear logistic regression.</li>
<li>Example 5.120 illustrating local likelihood for BPD data.</li>
</ul></li>
<li>5.11 <strong>Scale-Space Smoothing</strong>
<ul>
<li>An approach that examines <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_h(x)"> over a range of bandwidths <img src="https://latex.codecogs.com/png.latex?h">.</li>
<li>The scale-space surface <img src="https://latex.codecogs.com/png.latex?S%20=%20%5C%7B%20r_h(x),%20x%20%5Cin%20X%20,%20h%20%5Cin%20H%20%5C%7D">.</li>
<li>Method SiZer (significant zero crossings of derivatives).</li>
</ul></li>
<li>5.12 <strong>Multiple Regression</strong>
<ul>
<li>Nonparametric regression with multiple covariates <img src="https://latex.codecogs.com/png.latex?r(x_1,%20.%20.%20.%20,%20x_p)">.</li>
<li>Challenges due to the curse of dimensionality.</li>
<li><strong>Additive models</strong>: <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20%5Cmu%20+%20%5Csum_%7Bj=1%7D%5Ep%20r_j(x_j)">.</li>
<li>Backfitting algorithm for fitting additive models.</li>
<li>Example 5.126 applying additive models to rock data.</li>
<li><strong>Projection pursuit regression</strong>: <img src="https://latex.codecogs.com/png.latex?r(x_1,%20.%20.%20.%20,%20x_p)%20=%20%5Cmu%20+%20%5Csum_%7Bm=1%7D%5EM%20r_m(%5Calpha_m%5ET%20x)">.</li>
<li>Algorithm for projection pursuit regression.</li>
<li>Example 5.130 applying projection pursuit regression to rock data.</li>
<li><strong>Regression trees</strong>: Partitioning the covariate space into rectangles and fitting a constant in each.</li>
<li>Complexity parameter <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and cost-complexity pruning (5.132).</li>
<li>Example 5.133 showing a regression tree for rock data.</li>
<li><strong>Multivariate Adaptive Regression Splines (MARS)</strong>: Building a model from piecewise linear basis functions.</li>
<li><strong>Tensor Product Models</strong>: <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20%5Csum_%7Bm=1%7D%5EM%20%5Cbeta_m%20h_m(x)"> where <img src="https://latex.codecogs.com/png.latex?h_m"> are basis functions in a tensor product space (5.135).</li>
</ul></li>
<li>5.13 <strong>Other Issues</strong>
<ul>
<li><strong>Plug-In Bandwidths</strong>: Using formulas for asymptotically optimal bandwidths.</li>
<li><strong>Choice of Kernel</strong>: Impact of kernel shape.</li>
<li><strong>Boundary Effects</strong>: Problems near the boundaries of the data range.</li>
<li><strong>Varying Coefficient Models</strong>: <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20%5Csum_%7Bj=1%7D%5Ep%20%5Cbeta_j(x)%20x_j">.</li>
<li><strong>Quantile Regression</strong>: Modeling conditional quantiles of <img src="https://latex.codecogs.com/png.latex?Y"> given <img src="https://latex.codecogs.com/png.latex?X">.</li>
<li><strong>Derivative Estimation</strong>: Estimating derivatives of the regression function (5.143).</li>
</ul></li>
<li>5.14 <strong>Bibliographic Remarks</strong>
<ul>
<li>References for further reading on nonparametric regression.</li>
</ul></li>
<li>5.15 <strong>Appendix</strong>
<ul>
<li>(Content of the appendix is not detailed in the provided excerpts).</li>
</ul></li>
</ul></li>
<li><strong>Density Estimation</strong>
<ul>
<li>Introduction to the problem of estimating the probability density function <img src="https://latex.codecogs.com/png.latex?f(x)"> from a sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n">.</li>
<li>6.1 <strong>Cross-Validation</strong>
<ul>
<li>Using cross-validation to evaluate the quality of a density estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n"> with the risk <img src="https://latex.codecogs.com/png.latex?R%20=%20E(L)"> (integrated mean squared error).</li>
</ul></li>
<li>6.2 <strong>Histograms</strong>
<ul>
<li>Definition of a histogram estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)%20=%20m%20%5Csum_%7Bj=1%7D%5Em%20%5Chat%7Bp%7D_j%20I(x%20%5Cin%20B_j)"> (6.7), where <img src="https://latex.codecogs.com/png.latex?h%20=%201/m"> is the binwidth, <img src="https://latex.codecogs.com/png.latex?Y_j"> is the count in bin <img src="https://latex.codecogs.com/png.latex?B_j">, and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D_j%20=%20Y_j/n">.</li>
<li>Example 6.8 showing histograms of astronomy data with different numbers of bins (oversmoothing, undersmoothing, and cross-validation chosen).</li>
<li>Theorem 6.9 relating the expected value of the histogram estimator to the true density and its derivatives.</li>
<li>The risk (integrated squared error) of a histogram estimator <img src="https://latex.codecogs.com/png.latex?J(h)%20=%20E%20%5Cint%20(f%CC%82_n(x)%20%E2%88%92%20f(x))%5E2%20dx"> (6.12).</li>
<li>The leave-one-out cross-validation score for histograms <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Csum_%7Bi=1%7D%5En%20(%5Chat%7Bf%7D_n%5E%7B(-i)%7D(X_i))%5E2"> (6.14).</li>
<li>Theorem 6.15 giving a formula for the cross-validation score: <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Cfrac%7B2%7D%7Bh(n%20-%201)%7D%20-%20%5Cfrac%7Bn%20+%201%7D%7Bh(n%20-%201)%7D%20%5Csum_%7Bj=1%7D%5Em%20%5Chat%7Bp%7D_j%5E2"> (6.16).</li>
<li>Example 6.17 showing the application of cross-validation to choose the number of bins for the astronomy data.</li>
<li>Constructing confidence sets for <img src="https://latex.codecogs.com/png.latex?f"> at the resolution of the histogram.</li>
</ul></li>
<li>6.3 <strong>Kernel Density Estimation</strong>
<ul>
<li>The kernel density estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)%20=%20%5Cfrac%7B1%7D%7Bnh%7D%20%5Csum_%7Bi=1%7D%5En%20K(%5Cfrac%7Bx%20-%20X_i%7D%7Bh%7D)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20K_h(x%20-%20X_i)"> (6.23).</li>
<li>The mean integrated squared error (MISE) as a measure of performance (6.27).</li>
<li>Asymptotic MISE (AMISE) and optimal bandwidth (6.29, 6.31).</li>
<li>Rule-of-thumb bandwidth selection based on assuming a Normal distribution (6.33).</li>
<li>Leave-one-out cross-validation for kernel density estimation <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(%5Chat%7Bf%7D_n%5E%7B(-i)%7D(X_i))%5E2"> (6.34).</li>
<li>Theorem 6.35 giving a formula for the cross-validation score: <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Cfrac%7B1%7D%7Bn(n-1)h%7D%20%5Csum_%7Bi=1%7D%5En%20%5Csum_%7Bj=1%7D%5En%20K(%5Cfrac%7BX_i%20-%20X_j%7D%7Bh%7D)%20-%20%5Cfrac%7B2%7D%7Bn(n-1)h%7D%20%5Csum_%7Bi%20%5Cneq%20j%7D%20K(%5Cfrac%7BX_i%20-%20X_j%7D%7Bh%7D)">.</li>
<li>Example 6.36 applying kernel density estimation to the astronomy data and using cross-validation for bandwidth selection.</li>
<li>Confidence bands for kernel density estimators using the relationship with regression.</li>
</ul></li>
<li>6.4 <strong>Local Polynomials</strong>
<ul>
<li>Extending the idea of local fitting to density estimation.</li>
<li>Local polynomial density estimation.</li>
<li>Relation to kernel density estimation when the polynomial degree <img src="https://latex.codecogs.com/png.latex?p=0">.</li>
</ul></li>
<li>6.5 <strong>Multivariate Problems</strong>
<ul>
<li>Challenges of density estimation in higher dimensions (curse of dimensionality).</li>
</ul></li>
<li>6.6 <strong>Converting Density Estimation Into Regression</strong>
<ul>
<li>Transforming the density estimation problem into a regression problem by binning the data and using regression techniques on the counts.</li>
<li>Applying kernel regression to the square root of the counts.</li>
<li>Constructing confidence bands using regression techniques.</li>
<li>Example 6.51 applying this method to the Bart Simpson distribution.</li>
</ul></li>
<li>6.7 <strong>Bibliographic Remarks</strong>
<ul>
<li>Listing key references for density estimation.</li>
</ul></li>
<li>6.8 <strong>Appendix</strong>
<ul>
<li>Proof of Theorem 6.11 regarding the bias of the histogram estimator.</li>
</ul></li>
</ul></li>
<li><strong>Normal Means and Minimax Theory</strong>
<ul>
<li>Introduction to the Normal means problem and its role in unifying some nonparametric problems, serving as a basis for Chapters 8 and 9.</li>
<li>The chapter is noted as being more theoretical.</li>
<li>Recommendation for readers not interested in theoretical details to read sections 7.1, 7.2, and 7.3 and then skip to the next chapter.</li>
<li>7.1 <strong>The Normal Means Model</strong>
<ul>
<li>Defining the basic Normal means model: <img src="https://latex.codecogs.com/png.latex?Zi%20%5Csim%20N(%5Ctheta_i,%20%5Csigma%5E2_n)">, for <img src="https://latex.codecogs.com/png.latex?i%20=%201,%202,%20.%20.%20."> (7.2), where <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20(%5Ctheta_1,%20%5Ctheta_2,%20.%20.%20.)"> is the unknown parameter and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2_n"> is assumed known.</li>
<li>Practical note: In reality, <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2_n"> would often need to be estimated (as discussed in Chapter 5), which might affect the exact theoretical results.</li>
<li>Example 7.3 illustrating the model with a one-way analysis of variance setup: <img src="https://latex.codecogs.com/png.latex?X_%7Bij%7D%20=%20%5Ctheta_i%20+%20%5Csigma%20%5Cdelta_%7Bij%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Cdelta_%7Bij%7D"> are independent <img src="https://latex.codecogs.com/png.latex?N(0,1)"> random variables.</li>
</ul></li>
<li>7.2 <strong>Function Spaces</strong>
<ul>
<li>Introduction to function spaces relevant to the theoretical development.</li>
</ul></li>
<li>7.3 <strong>Connection to Regression and Density Estimation</strong>
<ul>
<li>Discussing how the Normal means problem relates to nonparametric regression and density estimation problems.</li>
</ul></li>
<li>7.4 <strong>Stein‚Äôs Unbiased Risk Estimator (sure)</strong>
<ul>
<li>Introducing Stein‚Äôs Unbiased Risk Estimator as a method for estimating the risk of an estimator.</li>
<li>Example showing the application of sure to the soft threshold estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_i%20=%20sign(Zi)(%7CZi%7C%20-%20%5Clambda)_+"> and providing the risk estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(Z)%20=%20%5Csum_%7Bi=1%7D%5En%20(%5Csigma%5E2%20-%202%5Csigma%5E2I(%7CZi%7C%20%5Cleq%20%5Clambda)%20+%20min(Z%5E2_i%20,%20%5Clambda%5E2)%20)"> (7.22).</li>
<li>Mentioning that sure is not appropriate for the hard threshold estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_i%20=%20Zi%20I(%7CZi%7C%20%3E%20%5Clambda)">.</li>
</ul></li>
<li>7.5 <strong>Minimax Risk and Pinsker‚Äôs Theorem</strong>
<ul>
<li>Discussing the concept of minimax risk.</li>
<li>Introducing Pinsker‚Äôs Theorem (Theorem 7.28) which gives an exact expression for the asymptotic minimax risk over an ellipsoid <img src="https://latex.codecogs.com/png.latex?%5CTheta(c%5E2)%20=%20%5C%7B%5Ctheta%20:%20%5Csum%20a_i%5E2%20%5Ctheta_i%5E2%20%5Cleq%20c%5E2%20%5C%7D">.</li>
</ul></li>
<li>7.6 <strong>Linear Shrinkage and the James‚ÄìStein Estimator</strong>
<ul>
<li>Returning to model (7.1) and exploring improvements over the MLE using linear estimators of the form <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D%20=%20bZ%20=%20(bZ_1,%20.%20.%20.%20,%20bZ_n)"> where <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20b%20%5Cleq%201">.</li>
<li>Defining the set of linear shrinkage estimators <img src="https://latex.codecogs.com/png.latex?L%20=%20%5C%7BbZ%20:%20b%20%5Cin%5C%7D">.</li>
<li>Presenting the James-Stein estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D%5E%7BJS%7D%20=%20(1%20-%20%5Cfrac%7B(n-2)%5Csigma%5E2_n%7D%7B%5C%7CZ%5C%7C%5E2%7D)_+%20Z"> (7.41) and noting it minimizes the estimated risk over <img src="https://latex.codecogs.com/png.latex?L">.</li>
<li>Mentioning a block estimation scheme by Cai et al.&nbsp;(2000) using the James-Stein estimator within blocks.</li>
<li>Theorem 7.50 (Cai, Low and Zhao, 2000) providing asymptotic optimality of their block estimator over Sobolev ellipsoids <img src="https://latex.codecogs.com/png.latex?%5CTheta(m,%20c)%20=%20%5C%7B%5Ctheta%20:%20%5Csum_%7Bi=1%7D%5E%5Cinfty%20a_i%5E2%20%5Ctheta%5E2_i%20%5Cleq%20c%5E2%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?a_%7B2i%7D%20=%20a_%7B2i+1%7D%20=%201%20+%20(2i%5Cpi)%5E%7B2m%7D">.</li>
</ul></li>
<li>7.7 <strong>Adaptive Estimation Over Sobolev Spaces</strong>
<ul>
<li>Discussing adaptive estimation methods for Sobolev spaces.</li>
</ul></li>
<li>7.8 <strong>Confidence Sets</strong>
<ul>
<li>Focusing on the construction of confidence sets <img src="https://latex.codecogs.com/png.latex?B_n%20%5Csubset%20R%5En"> for <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20(%5Ctheta_1,%20...,%20%5Ctheta_n)"> such that <img src="https://latex.codecogs.com/png.latex?%5Cinf_%7B%5Ctheta%20%5Cin%20R%5En%7D%20P_%5Ctheta(%5Ctheta%20%5Cin%20B_n)%20%5Cgeq%201%20-%20%5Calpha"> (7.51).</li>
<li>Describing different methods for constructing confidence sets.</li>
</ul></li>
<li>7.9 <strong>Optimality of Confidence Sets</strong>
<ul>
<li>Discussing the concept of optimal confidence sets.</li>
</ul></li>
<li>7.10 <strong>Random Radius Bands?</strong>
<ul>
<li>Questioning the use of random radius bands for confidence sets.</li>
</ul></li>
<li>7.11 <strong>Penalization, Oracles and Sparsity</strong>
<ul>
<li>Connecting penalization methods to oracle properties and the concept of sparsity in the parameter vector <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
<li>Mentioning the LASSO (Tibshirani (1996)) and basis pursuit (Chen et al.&nbsp;(1998)) in the context of criterion (7.89).</li>
<li>Relating soft thresholding to wavelet methods (Chapter 9).</li>
</ul></li>
<li>7.12 <strong>Bibliographic Remarks</strong>
<ul>
<li>Providing references for further reading on Normal means and minimax theory.</li>
</ul></li>
<li>7.13 <strong>Appendix</strong>
<ul>
<li>Containing technical proofs, including the proof of Theorem 7.28 (Pinsker‚Äôs Theorem).</li>
</ul></li>
</ul></li>
<li>Nonparametric Inference Using Orthogonal Functions</li>
</ol>
<ul>
<li>This chapter introduces the use of orthogonal functions (e.g., Fourier series, wavelets) for nonparametric inference in both regression and density estimation.</li>
<li>For nonparametric regression, the function is expanded in an orthonormal basis: - ‚Äúr(x) = ‚àë‚àû j=1 Œ∏jœÜj(x) (8.3)‚Äù.</li>
<li>The concept of a modulator is introduced for shrinking the coefficients:
<ul>
<li>‚ÄúA modulator is a vector b = (b1, . . . , bn) such that 0 ‚â§ bj ‚â§ 1, j = 1, . . . , n.&nbsp;A modulation estimator is an estimator of the form Œ∏ÃÇ = bZ = (b1Z1, b2Z2, . . . , bnZn). (8.8)‚Äù.</li>
</ul></li>
<li>Risk estimation and minimization over classes of modulators are discussed, connecting to the Pinsker bound.</li>
<li>The chapter also covers irregular designs and density estimation using orthogonal functions.</li>
</ul>
<ol start="9" type="1">
<li>Wavelets and Other Adaptive Methods</li>
</ol>
<ul>
<li>This chapter focuses on wavelets, a powerful tool for adaptive nonparametric estimation, particularly for functions with varying degrees of smoothness.</li>
<li>Haar wavelets are introduced as a simple example.</li>
<li>The construction of more general wavelets through multiresolution analysis (MRA) is outlined:</li>
<li>‚Äú9.11 Definition. Given a function œÜ, define V0, V1, . . . , as in (9.10). We say that œÜ generates a multiresolution analysis (MRA) of R if Vj ‚äÇ Vj+1, j ‚â• 0, (9.12) and ‚ãÉ j‚â•0 Vj is dense in L2(R). (9.13)‚Äù.</li>
<li>Wavelet regression techniques, including wavelet thresholding (VisuShrink and SureShrink), are discussed for denoising and estimating functions from noisy data. The oracle risk and the performance of thresholding estimators are mentioned.</li>
<li>Besov spaces are introduced as function spaces that are well-suited for characterizing the properties of functions that can be effectively estimated using wavelets.</li>
<li>The construction of confidence sets using wavelet methods is covered.</li>
<li>Other adaptive methods, such as the Intersection of Confidence Intervals (ICI) method by Goldenshluger and Nemirovski, are briefly introduced as alternatives to wavelet-based approaches for adapting to unknown smoothness.</li>
</ul>
<ol start="10" type="1">
<li>Other Topics</li>
</ol>
<ul>
<li>This final chapter covers a range of more advanced and specialized topics in nonparametric inference.</li>
<li>Measurement error in covariates and responses and its impact on estimation are discussed, along with potential correction methods. Inverse problems, where the goal is to infer an underlying function or parameter from indirect observations, are introduced.</li>
<li>Nonparametric Bayes, semiparametric inference, and issues with correlated errors are briefly mentioned.</li>
<li>The chapter touches upon classification, sieves, shape-restricted inference, and testing in a nonparametric framework.</li>
<li>Finally, computational issues relevant to implementing nonparametric methods are highlighted.</li>
</ul>
</section>
<section id="faq" class="level2">
<h2 class="anchored" data-anchor-id="faq">FAQ</h2>
<ol type="1">
<li>What is nonparametric inference and how does it differ from parametric inference?</li>
</ol>
<p>Nonparametric inference refers to statistical methods where the structure of the underlying model is not fully specified by a finite number of parameters. Instead of assuming a particular distribution (like a normal distribution) with fixed parameters, nonparametric methods make fewer assumptions about the data-generating process. They aim to estimate functions or distributions directly from the data. This contrasts with parametric inference, which relies on models defined by a fixed set of parameters. Nonparametric methods are more flexible and can capture complex relationships in the data, but they often require larger sample sizes and can be less efficient if a simple parametric model is indeed appropriate.</p>
<ol start="2" type="1">
<li>What are confidence sets and why are they important in nonparametric inference?</li>
</ol>
<p>Confidence sets are generalizations of confidence intervals to higher dimensions or to function spaces. Instead of providing a range of plausible values for a single parameter, a confidence set provides a set of plausible functions, distributions, or other statistical objects. They are crucial in nonparametric inference because the goal is often to estimate an entire function or distribution, and it‚Äôs important to quantify the uncertainty associated with this estimate. A confidence set provides a way to assess the range of plausible estimates consistent with the observed data at a certain confidence level.</p>
<ol start="3" type="1">
<li>What are the bootstrap and the jackknife, and how are they used in nonparametric inference?</li>
</ol>
<p>The bootstrap and the jackknife are resampling techniques used to estimate the properties of an estimator (like its variance or bias) or to construct confidence intervals without relying on strong parametric assumptions.</p>
<p>Bootstrap: This method involves repeatedly resampling with replacement from the original data to create many ‚Äúbootstrap samples.‚Äù The statistic of interest is calculated for each bootstrap sample, and the distribution of these statistics is used to approximate the sampling distribution of the original estimator. This can be used to estimate standard errors and construct bootstrap confidence intervals.</p>
<p>Jackknife: This technique involves systematically leaving out one observation at a time from the original data and calculating the statistic of interest for each of these ‚Äúleave-one-out‚Äù samples. These values are then used to estimate bias and variance of the estimator. Both methods are valuable in nonparametric settings where analytical derivations of standard errors or confidence intervals might be difficult or unreliable due to the complexity of the estimators or the lack of distributional assumptions.</p>
<ol start="4" type="1">
<li>What is the bias-variance tradeoff in smoothing, and how does it relate to the choice of smoothing parameters like bandwidth?</li>
</ol>
<p>Smoothing techniques, such as kernel density estimation and nonparametric regression, aim to uncover underlying patterns in noisy data by averaging or weighting nearby observations. The bias-variance tradeoff is a fundamental concept in these methods.</p>
<p>Bias: This refers to the error introduced by approximating a complex real-world problem (which may be nonlinear) by a simpler model (the smoothed estimate). If the smoothing is too strong (e.g., a large bandwidth), the estimate might be overly smooth and miss important details in the true underlying function, leading to high bias.</p>
<p>Variance: This refers to the variability of the estimator. If the smoothing is weak (e.g., a small bandwidth), the estimator might be too sensitive to the noise in the data, resulting in high variance. The choice of the smoothing parameter (like the bandwidth <img src="https://latex.codecogs.com/png.latex?h"> in kernel methods) controls this tradeoff. A larger bandwidth typically leads to a smoother estimate with lower variance but potentially higher bias. A smaller bandwidth leads to a more wiggly estimate with higher variance but potentially lower bias. The goal is to choose a bandwidth that balances bias and variance to minimize the overall risk (e.g., mean squared error).</p>
<ol start="5" type="1">
<li>What are linear smoothers in nonparametric regression, and what are some examples?</li>
</ol>
<p>A nonparametric regression estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)"> is called a linear smoother if it can be expressed as a linear combination of the response variables <img src="https://latex.codecogs.com/png.latex?Y_i">, i.e., <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7Dn(x)%20=%20%5Csum%7Bi=1%7D%5E%7Bn%7D%20l_i(x)%20Y_i">, where <img src="https://latex.codecogs.com/png.latex?l_i(x)"> are weights that depend on the predictor variables <img src="https://latex.codecogs.com/png.latex?x_i"> and the point of estimation <img src="https://latex.codecogs.com/png.latex?x">, but not on the <img src="https://latex.codecogs.com/png.latex?Y_i">‚Äôs. These weights essentially determine how much each observation <img src="https://latex.codecogs.com/png.latex?Y_i"> contributes to the estimate at <img src="https://latex.codecogs.com/png.latex?x">. The collection of these weights for all <img src="https://latex.codecogs.com/png.latex?x_i"> forms the smoothing matrix <img src="https://latex.codecogs.com/png.latex?L">, such that the vector of fitted values <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Chat%7Br%7D%7D%20=%20LY">.</p>
<p>Examples of linear smoothers discussed include:</p>
<p>Regressogram: This method divides the range of the predictor variable into bins and estimates the regression function within each bin by the average of the <img src="https://latex.codecogs.com/png.latex?Y_i">‚Äôs in that bin. The weights <img src="https://latex.codecogs.com/png.latex?l_i(x)"> are constant within each bin and zero outside.</p>
<p>Local Averages: This estimator averages the <img src="https://latex.codecogs.com/png.latex?Y_i">‚Äôs for <img src="https://latex.codecogs.com/png.latex?x_i"> values within a certain distance (defined by a bandwidth <img src="https://latex.codecogs.com/png.latex?h">) of the target point <img src="https://latex.codecogs.com/png.latex?x">. The weights <img src="https://latex.codecogs.com/png.latex?l_i(x)"> are equal for points within the window and zero outside.</p>
<p>Kernel Estimators: These methods use a kernel function <img src="https://latex.codecogs.com/png.latex?K"> and a bandwidth <img src="https://latex.codecogs.com/png.latex?h"> to weight the <img src="https://latex.codecogs.com/png.latex?Y_i">‚Äôs based on the distance of their corresponding <img src="https://latex.codecogs.com/png.latex?x_i">‚Äôs from the target <img src="https://latex.codecogs.com/png.latex?x">. The weights <img src="https://latex.codecogs.com/png.latex?l_i(x)"> are proportional to <img src="https://latex.codecogs.com/png.latex?K((x-x_i)/h)">.</p>
<p>Local Polynomials: These estimators fit a low-degree polynomial to the data within a local neighborhood of <img src="https://latex.codecogs.com/png.latex?x"> and use the value of the fitted polynomial at <img src="https://latex.codecogs.com/png.latex?x"> as the estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)">. The weights are determined by the polynomial fit.</p>
<p>Smoothing Splines: These methods find a smooth function that minimizes the residual sum of squares plus a penalty term that penalizes the roughness of the function. The resulting estimator is also a linear smoother.</p>
<ol start="6" type="1">
<li>How is the concept of minimaxity used in nonparametric inference, particularly in the context of normal means problems?</li>
</ol>
<p>Minimaxity in statistics refers to finding an estimator that performs optimally in the worst-case scenario over a given class of parameters or functions. In nonparametric inference, where the underlying function or parameter might belong to a large, infinite-dimensional space, minimax theory helps in understanding the fundamental limitations of estimation. It aims to find an estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D"> that minimizes the maximum risk <img src="https://latex.codecogs.com/png.latex?R(%5Ctheta,%20%5Chat%7B%5Ctheta%7D)"> over all <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> in a class <img src="https://latex.codecogs.com/png.latex?%5CTheta">, where the risk <img src="https://latex.codecogs.com/png.latex?R"> quantifies the error of the estimator (e.g., mean squared error).</p>
<p>In the context of normal means problems, which serve as a simplified yet insightful model for many nonparametric problems (like regression or density estimation in an orthonormal basis), minimax theory helps determine the best possible rate of convergence for the risk. For instance, Pinsker‚Äôs theorem provides the minimax mean squared error for estimating a function in a Sobolev ellipsoid, showing that the optimal rate depends on the smoothness of the function class.</p>
<p>Minimax theory also guides the development of estimators that achieve these optimal rates. Examples like shrinkage estimators (e.g., James-Stein estimator in the finite normal means problem, and wavelet thresholding in the sequence space setting related to function estimation) are often motivated by minimax considerations, aiming to reduce risk compared to simpler estimators like the MLE, especially when many parameters are involved and some might be zero or small.</p>
<ol start="7" type="1">
<li>What role do orthogonal functions and wavelets play in nonparametric inference? Orthogonal functions (like Fourier series, cosine basis) and wavelets provide flexible bases for representing functions and distributions in nonparametric inference.</li>
</ol>
<p>Orthogonal Functions: By expanding an unknown function <img src="https://latex.codecogs.com/png.latex?r(x)"> in an orthonormal basis <img src="https://latex.codecogs.com/png.latex?%7B%5Cphi_j(x)%7D">, we can represent it by its coefficients <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j%20=%20%5Cint%20r(x)%20%5Cphi_j(x)%20dx">. The data can then be projected onto these basis functions, yielding noisy estimates of these coefficients. Nonparametric inference then proceeds by appropriately shrinking or selecting these coefficients. This approach transforms the problem of estimating a function into a problem of estimating a sequence of coefficients, which can be analyzed using techniques similar to the normal means problem. Modulator estimators, which apply weights <img src="https://latex.codecogs.com/png.latex?b_j"> to the estimated coefficients <img src="https://latex.codecogs.com/png.latex?Z_j">, are a common approach. The choice of basis can be tailored to the expected properties of the function (e.g., smoothness, periodicity).</p>
<p>Wavelets: Wavelets are basis functions that are localized in both time (or space) and frequency. This localization property makes them particularly well-suited for analyzing functions with local features like sharp changes or discontinuities. Wavelet methods involve decomposing the data into wavelet coefficients, applying a thresholding rule to these coefficients (to remove noise and capture important features), and then reconstructing the function from the thresholded coefficients. Wavelet shrinkage estimators like VisuShrink and SureShrink are popular nonparametric regression and density estimation techniques known for their adaptivity to varying smoothness of the underlying function. They can achieve near-optimal performance over a wide range of function spaces.</p>
<p>Both orthogonal functions and wavelets allow for a sparse representation of many signals and functions, meaning that many of their coefficients are close to zero. This sparsity is exploited in nonparametric methods to achieve efficient estimation and adaptivity.</p>
<ol start="8" type="1">
<li>What are some of the challenges and extensions in nonparametric inference discussed in the text? The text touches upon several challenges and extensions in nonparametric inference:</li>
</ol>
<p>Curse of Dimensionality: In multivariate settings, many nonparametric methods suffer from the curse of dimensionality, where the amount of data needed to achieve a certain level of accuracy increases exponentially with the number of predictor variables.</p>
<p>Measurement Error: When the predictor variables are measured with error, it can significantly affect the performance of nonparametric estimators. Deconvolution techniques are needed to correct for this bias.</p>
<p>Inverse Problems: These problems involve inferring an unknown function or parameter from indirect measurements. They are often ill-posed, requiring regularization techniques, many of which have nonparametric flavors.</p>
<p>Nonparametric Bayes: This area combines the flexibility of nonparametric models with the framework of Bayesian inference, using priors over infinite-dimensional spaces of functions or distributions (e.g., Dirichlet process mixtures).</p>
<p>Semiparametric Inference: These methods combine parametric and nonparametric components in a model. For example, one might model the effect of some covariates parametrically while leaving the effect of others unspecified.</p>
<p>Correlated Errors: The standard assumptions of independence often do not hold in practice. Nonparametric methods for data with correlated errors are more complex and less developed than those for independent data.</p>
<p>Classification: While the main focus is on regression and density estimation, nonparametric methods are also used for classification problems, such as <img src="https://latex.codecogs.com/png.latex?k">-nearest neighbors or kernel-based methods.</p>
<p>Shape-Restricted Inference: In some applications, prior knowledge about the shape of the function (e.g., monotonicity, convexity) is available. Nonparametric methods that incorporate these shape constraints can lead to improved estimates.</p>
<p>Testing: Nonparametric hypothesis testing aims to compare distributions or test for relationships without assuming specific parametric forms.</p>
<p>Computational Issues: Implementing nonparametric methods can be computationally intensive, especially for large datasets or complex techniques like bootstrap or wavelet analysis.</p>
<p>These topics highlight the ongoing research and the breadth of problems that nonparametric inference seeks to address, often requiring specialized techniques beyond the basic methodologies of regression and density estimation.</p>
</section>
<section id="some-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="some-thoughts">Some Thoughts</h2>
<p>The book provide a comprehensive overview of nonparametric inference, starting from fundamental concepts like cdf and functional estimation and progressing to advanced topics such as wavelet analysis, minimax theory, and specialized problems. The book emphasizes both the theoretical underpinnings and the practical application of various nonparametric methods for regression, density estimation, and other inference tasks. The inclusion of exercises at the end of each chapter suggests that the book is intended as a textbook for students and researchers in statistics and related fields.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Title},
  url = {https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>‚ÄúTitle.‚Äù</span> <a href="https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/">https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/</a>.
</div></div></section></div> ]]></description>
  <category>draft</category>
  <category>review</category>
  <guid>https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/</guid>
  <pubDate>Fri, 14 Mar 2025 02:32:17 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/images/lit-review-cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Multi-column Deep Neural Networks for Image Classification</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="cire≈üan2012multicolumndeepneuralnetworks">(Cire≈üan, Meier, and Schmidhuber 2012)</span> titled ‚ÄúMulti-column Deep Neural Networks for Image Classification‚Äù, the authors, Dan Cire≈üan, Ueli Meier, Juergen Schmidhuber introduce a biologically plausible deep artificial neural network architecture that achieves near-human performance on tasks such as the recognition of handwritten digits or traffic signs. The method uses small receptive fields of convolutional winner-take-all neurons to yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. The authors demonstrate that their approach outperforms humans on a traffic sign recognition benchmark and improves the state-of-the-art on various image classification benchmarks.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks. ‚Äî <span class="citation" data-cites="cire≈üan2012multicolumndeepneuralnetworks">(Cire≈üan, Meier, and Schmidhuber 2012)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
</section>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="cire≈üan2012multicolumndeepneuralnetworks">(Cire≈üan, Meier, and Schmidhuber 2012)</span> the authors make significant strides in the field of image classification by demonstrating the effectiveness of multi-column deep neural networks (DNNs). <mark>This work is noteworthy for its pioneering approach in applying deep learning techniques to image classification tasks, which have since become the foundation of modern computer vision systems.</mark></p>
<div class="no-row-height column-margin column-container"><div id="ref-cire≈üan2012multicolumndeepneuralnetworks" class="csl-entry">
Cire≈üan, Dan, Ueli Meier, and Juergen Schmidhuber. 2012. <span>‚ÄúMulti-Column Deep Neural Networks for Image Classification.‚Äù</span> <a href="https://arxiv.org/abs/1202.2745">https://arxiv.org/abs/1202.2745</a>.
</div></div></section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<p>The authors present a system that uses several deep neural networks, each operating as a ‚Äúcolumn,‚Äù which are trained independently. The outputs of these networks are then averaged to form the final prediction. This multi-column approach exploits the diversity between different networks and boosts classification accuracy, reducing the impact of overfitting and improving generalization. Notably, the method achieved state-of-the-art results on several image classification benchmarks at the time, including the MNIST digit recognition task.</p>
<p>One of the central contributions of this paper is the demonstration of how <mark>combining multiple deep networks can outperform single networks in complex image classification tasks</mark>. The authors trained their models on NVIDIA GPUs, which allowed them to scale deep networks efficiently‚Äîa relatively new practice when this paper was published, underscoring its innovative edge.</p>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Improvement on Benchmarks</strong>: The multi-column DNN approach delivered unprecedented accuracy on datasets like MNIST, achieving an error rate of just 0.23%. This represents one of the early breakthroughs that paved the way for deep learning in computer vision.</p></li>
<li><p><strong>Effective Use of Parallelism</strong>: The paper highlights the use of modern GPUs to efficiently train deep networks, illustrating how hardware advancements can accelerate research progress.</p></li>
<li><p><strong>Generalizability</strong>: While the paper focuses on MNIST and other datasets, the multi-column DNN framework offers a flexible approach to other image classification tasks. The general architecture and training methodology could be adapted to more complex datasets, making this work highly relevant across a variety of image recognition problems.</p></li>
<li><p><strong>Robustness</strong>: By averaging outputs from multiple networks, the system reduces the sensitivity to the specific architecture or initialization of a single network. This ensemble-like approach increases robustness and reduces error rates.</p></li>
</ul>
</section>
<section id="weaknesses" class="level2">
<h2 class="anchored" data-anchor-id="weaknesses">Weaknesses</h2>
<ul>
<li><p><strong>Lack of Theoretical Insight</strong>: Although the empirical results are impressive, the paper does not delve deeply into the theoretical reasons behind the success of multi-column architectures. It remains unclear how much of the performance gain is due to ensembling versus the intrinsic strength of the individual networks.</p></li>
<li><p><strong>Computational Cost</strong>: The approach requires training multiple deep neural networks independently, which could be computationally expensive for larger datasets or higher-dimensional inputs. While GPUs mitigate this to an extent, scaling the multi-column approach to larger tasks would demand significant computational resources.</p></li>
<li><p><strong>Limited Applicability to Other Modalities</strong>: The paper focuses solely on image classification. While it hints at the potential for multi-column networks in other domains (e.g., audio or text), the paper doesn‚Äôt explore these extensions or provide empirical evidence beyond the image domain.</p></li>
</ul>
</section>
<section id="impact-and-relevance" class="level2">
<h2 class="anchored" data-anchor-id="impact-and-relevance">Impact and Relevance</h2>
<p>This paper marked a turning point for deep learning in computer vision, showing the power of combining deep networks for complex tasks like image classification. Its success on benchmarks like MNIST helped popularize deep learning as a dominant method for pattern recognition and set the stage for more advanced techniques. Although it primarily focuses on image classification, the insights regarding ensemble learning through independent deep networks have since inspired various approaches in different machine learning areas, including speech recognition and natural language processing.</p>
<p>The paper is particularly significant when viewed in the context of its time (2012), as it predated the massive adoption of deep learning across industries. Its methods were fundamental to later developments in deep convolutional neural networks, which have become a cornerstone of state-of-the-art models in computer vision tasks today.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Ciresan, Meier, and Schmidhuber‚Äôs work on multi-column deep neural networks represents a crucial step forward in the development of image classification techniques. Its impact on deep learning, especially in terms of model ensembling and parallelization using GPUs, cannot be overstated. While it comes with some computational challenges and lacks deep theoretical explanation, the paper‚Äôs practical results and novel approach have solidified its place as a landmark contribution in the history of deep learning.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Multi-Column {Deep} {Neural} {Networks} for {Image}
    {Classification}},
  url = {https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>‚ÄúMulti-Column Deep Neural Networks for Image
Classification.‚Äù</span> <a href="https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/">https://orenbochman.github.io/reviews/2012/Multi-column
deep neural networks for image classi cation/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/</guid>
  <pubDate>Fri, 14 Mar 2025 02:32:17 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2012/dropout/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="hinton2012improvingneuralnetworkspreventing">(Hinton et al. 2012)</span> titled ‚ÄúImproving Neural Networks by Preventing Co-Adaptation of Feature Detectors‚Äù, the authors, Hinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov introduces a new regularization technique called ‚ÄúDropout‚Äù that helps to prevent overfitting in neural networks. Dropout is a simple and effective way to improve the performance of neural networks by preventing co-adaptation of feature detectors. The authors show that dropout can be used to improve the performance of a wide range of neural networks, including deep networks, convolutional networks, and recurrent networks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hinton2012improvingneuralnetworkspreventing" class="csl-entry">
Hinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. <span>‚ÄúImproving Neural Networks by Preventing Co-Adaptation of Feature Detectors.‚Äù</span> <a href="https://arxiv.org/abs/1207.0580">https://arxiv.org/abs/1207.0580</a>.
</div></div></section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ‚Äúthinned‚Äù networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification, and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
</blockquote>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>, introduces the dropout technique as an innovative method to prevent overfitting in neural networks. Overfitting occurs when a model performs well on training data but poorly on unseen test data, particularly when dealing with a large number of parameters and limited training samples. The paper addresses this by proposing the use of dropout, a regularization technique that randomly omits units (neurons) during training.</p>
</section>
<section id="core-ideas" class="level2">
<h2 class="anchored" data-anchor-id="core-ideas">Core Ideas</h2>
<p>The central concept behind dropout is to prevent co-adaptation of feature detectors. In a traditional neural network, feature detectors can co-adapt to specific patterns in the training data, which leads to poor generalization. By randomly omitting neurons with a probability of 0.5 during training, each neuron is forced to contribute independently to the final output. This reduces the reliance on specific sets of neurons and ensures that each feature detector learns useful patterns.</p>
<p>Another significant advantage of dropout is that it acts as an efficient form of model averaging. Training with dropout can be seen as training an ensemble of neural networks that share parameters, making it computationally feasible to obtain better generalization without having to train multiple models.</p>
</section>
<section id="experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results</h2>
<p>The authors demonstrate the effectiveness of dropout on several benchmark datasets, including MNIST, CIFAR-10, ImageNet, TIMIT, and the Reuters corpus.</p>
<ul>
<li>MNIST: Dropout reduced the error rate from 160 errors to around 110 by applying 50% dropout to hidden units and 20% dropout to input units.</li>
<li>TIMIT: Dropout improved frame classification accuracy in speech recognition tasks, reducing the error rate by 3% in comparison to standard training methods.</li>
<li>CIFAR-10: The authors achieved a 16.6% error rate without dropout and 15.6% with dropout, outperforming previous state-of-the-art results.</li>
<li>ImageNet: Dropout applied to deep convolutional neural networks (CNNs) reduced the error rate from 48.6% to 42.4%.</li>
<li>Reuters Corpus: Dropout reduced classification error from 31.05% to 29.62%.</li>
</ul>
</section>
<section id="theoretical-contributions" class="level2">
<h2 class="anchored" data-anchor-id="theoretical-contributions">Theoretical Contributions</h2>
<p>The theoretical underpinning of dropout is grounded in model averaging and regularization. In standard practice, model averaging is performed by training multiple models and averaging their predictions, but this approach is computationally expensive. Dropout provides a far more efficient alternative by implicitly training an ensemble of models that share parameters, thus achieving the benefits of model averaging without the overhead of training separate models.</p>
<p>Additionally, dropout mitigates the problem of overfitting by introducing noise during training, making the model more robust. At test time, all units are used, but their outgoing weights are scaled to reflect the fact that fewer units were active during training.</p>
</section>
<section id="discussion-and-impact" class="level2">
<h2 class="anchored" data-anchor-id="discussion-and-impact">Discussion and Impact</h2>
<p>The introduction of dropout represents a major step forward in the development of deep learning models, as it allows for better generalization across a variety of tasks. Its simplicity, coupled with its effectiveness, has made dropout a standard tool in neural network training. The experiments conducted in the paper demonstrate its utility across a wide range of tasks, from image recognition to speech processing, providing compelling evidence of its broad applicability.</p>
<p>The idea of preventing co-adaptation of feature detectors to improve generalization is an elegant solution to a longstanding problem in neural network training. By ensuring that each neuron must work independently, dropout forces the model to learn more robust features that generalize well to unseen data.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This paper is a highly influential paper that introduced a novel technique for improving the generalization of deep learning models. The results speak for themselves, with dropout achieving state-of-the-art performance across multiple datasets and tasks. The technique has since become a standard part of neural network training, revolutionizing the field and contributing to the success of deep learning in real-world applications.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Improving {Neural} {Networks} by {Preventing} {Co-Adaptation}
    of {Feature} {Detectors}},
  url = {https://orenbochman.github.io/reviews/2012/dropout/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>‚ÄúImproving Neural Networks by Preventing
Co-Adaptation of Feature Detectors.‚Äù</span> <a href="https://orenbochman.github.io/reviews/2012/dropout/">https://orenbochman.github.io/reviews/2012/dropout/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/dropout/</guid>
  <pubDate>Fri, 14 Mar 2025 02:32:17 GMT</pubDate>
</item>
</channel>
</rss>
