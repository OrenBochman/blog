---
date: 2017-02-02
title: Mostly Harmless Econometrics --Review
description: Mostly Harmless Econometrics is a book that provides an introduction to causal inference from an econometrics perspective. It covers a wide range of topics, including regression analysis, instrumental variables, fixed effects, differences-in-differences, panel data, regression discontinuity, quantile regression, and standard errors.
categories: [econometrics, statistics, causal inference, podcast, review]
keywords: [econometrics, causal inference, statistics, regression, instrumental variables, fixed effects, differences-in-differences, panel data, regression discontinuity, quantile regression, standard errors]
bibliography: ./bibliography.bib
image: cover.gif
---

![book cover](cover.gif){.column-margin .nolightbox}

Econometrics is Data Science for quants and is one of the older names for this profession!

Econometrics is the study of statistical methods used in economics. It is a branch of economics that aims to give empirical content to economic relations. Econometrics is a combination of economic theory, mathematical economics, and statistical methods. It is used to test hypotheses and forecast future trends. Econometrics is used in many fields, including finance, marketing, and public policy. It is a powerful tool for analyzing economic data and making informed decisions.

Mostly Harmless Econometrics by Angrist and Pischke provides a practical guide to applied econometrics, emphasizing causal inference using real-world examples and focusing on the conceptual robustness of core techniques like regression and instrumental variables. The authors advocate for the importance of experimental and quasi-experimental research designs, alongside a clear understanding of potential outcomes and the challenges of selection bias. The text explores methods for analyzing treatment effects under various conditions, including heterogeneous effects, non-compliance, and limited dependent variables, while also addressing crucial aspects of statistical inference, such as standard errors and clustered data. Ultimately, the book aims to equip applied researchers with the tools to ask meaningful causal questions and employ econometric methods thoughtfully and effectively.


::: {.callout-note}
## TL;DR - Mostly Harmless Review

![Econometrics in a nutshell](/images/in_the_nut_shell_coach_retouched.jpg)

Outline of [@angrist2008mostly] -- Mostly Harmless Econometrics Review  

This has a number of data analysis techniques you might not know if you didn't major in economics. 
One reason I liked it, is that it gives a good introduction to causal inference from an econometrics perspective - many text look at it from a medical one. The book covers a wide range of topics, including regression analysis, instrumental variables, fixed effects, differences-in-differences, panel data, regression discontinuity, quantile regression, and standard errors. 

I also used Quantile Regression in my work and this book is also a good introduction to that technique.
 
:::

## Podcast on Mostly Harmless Econometrics

<audio controls="1">
  <source src="podcast.mp3" data-external="1" type="audio/mpeg">
  </source>
</audio>



## Outline

Here is an outline of the book "Mostly Harmless Econometrics" based on the table of contents:

*   **Part I: Introduction**
    *   **Chapter 1: Questions about Questions**
        *   Discusses the basis for a successful research project and organizes a research agenda around four "Frequently Asked Questions (FAQs)".
        *   These FAQs concern the relationship of interest, the ideal experiment, the identification strategy, and the mode of statistical inference.
        *   Emphasizes that a coherent research agenda is the foundation of useful statistical analysis.
        *   Uses the example of schooling and wages to illustrate the concept of an ideal experiment.
        *   Explains the term "identification strategy" as how researchers use observational data to approximate a real experiment.
        *   Mentions that Chapters 3-6 primarily focus on conceptual issues related to identification strategy.
        *   Notes that Chapter 8 covers practical problems related to statistical inference.
        *   Highlights that the following chapters are concerned with econometric questions arising after the research FAQs have been answered.
    *   **Chapter 2: The Experimental Ideal**
        *   Discusses how randomized trials provide an ideal benchmark for causal questions.
        *   Explains how random assignment solves the selection problem.
        *   Uses the Tennessee STAR experiment as a case in point for credible research design using random assignment.
        *   Notes the logistical difficulties, duration, and costs associated with randomized trials.
        *   Discusses regression analysis of experiments.

*   **Part II: The Core**
    *   **Chapter 3: Making Regression Make Sense**
        *   Introduces regression models as a tool for estimating treatment-control differences in experiments.
        *   Explains that regression estimates with randomly assigned regressors have a causal interpretation.
        *   Discusses the Conditional Expectation Function (CEF) and its properties.
        *   Explains regression fundamentals, including population parameters versus estimators.
        *   Covers saturated models, main effects, and other regression terminology.
        *   Addresses regression and causality, including the Conditional Independence Assumption (CIA) and omitted variables bias.
        *   Presents the omitted variables bias formula.
        *   Discusses the concept of "bad control".
        *   Explores heterogeneity and nonlinearity in regression.
        *   Covers the connection between regression and matching techniques.
        *   Explains how to control for covariates using the propensity score.
        *   Compares propensity-score methods and regression.
        *   Details various aspects of regression analysis.
    *   **Chapter 4: Instrumental Variables in Action**
        *   Discusses the use of instrumental variables (IV) to address causality when there are unobserved confounders.
        *   Explains the concept of an instrument and the exclusion restriction.
        *   Covers asymptotic two-stage least squares (2SLS) inference.
        *   Discusses over-identification and the 2SLS minimand.
        *   Explores two-sample IV and split-sample IVF.
        *   Addresses IV with heterogeneous potential outcomes.
        *   Explains Local Average Treatment Effects (LATE).
        *   Defines the compliant subpopulation, always-takers, and never-takers.
        *   Discusses IV in randomized trials.
        *   Explores counting and characterizing compliers.
        *   Covers generalizing LATE with multiple instruments and covariates.
        *   Discusses average causal response with variable treatment intensity.
        *   Provides details on IV, including common 2SLS mistakes, peer effects, and limited dependent variables.
        *   Examines the bias of 2SLS.
    *   **Chapter 5: Parallel Worlds: Fixed Effects, Differences-in-Differences, and Panel Data**
        *   Focuses on strategies that use data with a time or cohort dimension to control for unobserved-but-fixed omitted variables.
        *   Explains individual fixed effects models.
        *   Covers differences-in-differences (DD) research designs.
        *   Discusses regression DD.
        *   Compares fixed effects and lagged dependent variables .

*   **Part III: Extensions** 
    *   **Chapter 6: Getting a Little Jumpy: Regression Discontinuity Designs**
        *   Explores regression discontinuity (RD) research designs that exploit precise knowledge of treatment assignment rules.
        *   Discusses sharp RD designs .
        *   Explains that fuzzy RD is a form of IV.
    *   **Chapter 7: Quantile Regression**
        *   Discusses the use of quantile regression for estimating effects on distributions.
        *   Presents the quantile regression model .
        *   Covers censored quantile regression .
        *   Explains the quantile regression approximation property.
        *   Discusses quantile treatment effects .
    *   **Chapter 8: Nonstandard Standard Error Issues**
        *   Covers important inference problems that are missed by the textbook asymptotic approach.
        *   Discusses heteroskedasticity-consistent standard errors .
        *   Explores clustering and serial correlation in panel and difference-in-difference models.
        *   Addresses issues with fewer than 42 clusters .



## Frequently Asked Questions about Econometrics

1. What is the fundamental challenge in estimating causal effects using observational data, and how does the "experimental ideal" address this? The primary challenge is the "selection problem," where individuals with different characteristics systematically choose or are sorted into different treatment or exposure groups. This means observed differences in outcomes between groups might be due to pre-existing differences rather than the treatment itself. The "experimental ideal" solves this by random assignment, ensuring that, on average, the treatment and control groups are identical in all respects except for the treatment. This eliminates the correlation between the treatment and potential confounding factors, allowing for a direct estimation of the causal effect.
2. How does regression analysis relate to the Conditional Expectation Function (CEF), and what is the significance of the CEF in econometric modeling? The Conditional Expectation Function, E[y|X], represents the expected value of a dependent variable y given a set of independent variables X. It describes the underlying relationship between these variables. Linear regression can be seen as a linear approximation to this potentially non-linear CEF. The CEF is significant because it provides a fundamental way to understand how changes in X are associated with changes in the average value of y. Importantly, any random variable can be decomposed into its CEF based on X and a residual that is mean-independent of X.
3. What is the Conditional Independence Assumption (CIA) in the context of regression and causality, and what happens when this assumption is violated? The Conditional Independence Assumption states that after controlling for a set of observed covariates, the treatment assignment is independent of the potential outcomes. In other words, conditional on X, the treatment is as good as randomly assigned. If the CIA holds, regression analysis can provide consistent estimates of causal effects. However, if this assumption is violated due to omitted variables that are correlated with both the treatment and the outcome (and are not included in the regression), the estimated treatment effect will be biased. The omitted variable bias formula helps quantify the direction and magnitude of this bias.
4. What is instrumental variables (IV) estimation, and when is it a useful technique for estimating causal effects? Instrumental variables (IV) estimation is a technique used to estimate causal effects when there is likely endogeneity, meaning the treatment variable is correlated with the error term in the outcome equation. This correlation can arise from omitted variables, simultaneity, or measurement error. IV works by finding an "instrument" – a variable that is correlated with the endogenous treatment variable (the "first stage") but uncorrelated with the outcome except through its effect on the treatment (the "exclusion restriction"). IV estimation, often implemented using Two-Stage Least Squares (2SLS), can provide consistent estimates of the causal effect in such situations.
5. What is the Local Average Treatment Effect (LATE) framework in instrumental variables, and how does it differ from the Average Treatment Effect (ATE)? The Local Average Treatment Effect (LATE) framework acknowledges that instrumental variables estimation, under certain conditions like monotonicity (where the instrument always affects treatment in the same direction), identifies the causal effect of a treatment only for a specific subpopulation: the "compliers." Compliers are individuals whose treatment status would change if the value of the instrument were different. LATE differs from the Average Treatment Effect (ATE), which is the average causal effect of the treatment across the entire population. It's crucial to understand that IV estimates provide LATE, not necessarily ATE, and the characteristics of the compliers determine the generalizability of the findings.
6. How do fixed effects and differences-in-differences (DID) methods help in addressing unobserved heterogeneity in panel data? Fixed effects methods are used with panel data (data observed for the same units over multiple time periods) to control for time-invariant unobserved heterogeneity at the individual or group level. By focusing on changes within each unit over time, fixed effects eliminate the bias caused by these unobserved factors that do not change. Differences-in-differences (DID) is another panel data technique used to estimate the causal effect of a treatment or policy change by comparing the changes in outcomes over time between a treatment group and a control group that did not experience the intervention. DID relies on the assumption of parallel trends in the absence of the treatment.
7. What is regression discontinuity design (RDD), and under what conditions can it be used to estimate causal effects? Regression discontinuity design (RDD) is a quasi-experimental method that exploits a sharp cutoff point determining assignment to a treatment or intervention. Individuals just above and just below this threshold are assumed to be similar in all other respects except for their treatment status. In a "sharp" RDD, treatment is a deterministic function of a variable crossing the threshold. In a "fuzzy" RDD, the probability of treatment changes discontinuously at the threshold, leading to an instrumental variables framework. RDD can estimate the local average treatment effect around the cutoff point if the assignment variable is continuous and individuals cannot precisely manipulate their position relative to the threshold.
8. Why are standard error considerations important in econometrics, especially in the context of heteroskedasticity and clustered data? Standard errors are crucial for statistical inference, as they provide a measure of the precision of coefficient estimates. Traditional ordinary least squares (OLS) standard errors are valid under homoskedasticity (constant variance of errors) and independence of errors. However, if heteroskedasticity (non-constant variance) or clustering (correlation of errors within groups) is present, standard OLS standard errors will be incorrect, leading to misleading hypothesis tests and confidence intervals. **Robust standard errors** (like HC0, HC1, HC2, HC3) are designed to be valid under heteroskedasticity, while clustered standard errors account for within-group correlation, providing more reliable inference in these common scenarios.
NotebookLM can be inaccurate; please double check its responses.


