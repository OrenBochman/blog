[
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html",
    "title": "Pandas Productivity Challenge?",
    "section": "",
    "text": "Just a mini-rant on Pandas. Pandas is a replacement for Excel and SQL for Python data scientist. I would this replacement should make us more productive than an analyst using Excel.\nPandas has a learning curve.\nIt is pretty strong when we consider automation of tasks, and applying a function of an algorithms that is not available in excel.\nIt is weak when it comes to anything interactive exploration is faster if you can interactively filtering, sorting, freeze headers, and your index columns, apply formatting and conditional formatting. Define pivots tables using drag and drop.\nFinally bushiness analysts often use BI tools and while R has Shiny Python is kind of weak in this regard as well particularly when working in a notebook.\nSo can we bridge this divide and make ourselves as agile and efficent on pandas as an analyst is in Excel ?"
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#pandas-or-point-and-click",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#pandas-or-point-and-click",
    "title": "Pandas Productivity Challenge?",
    "section": "Pandas or Point and Click",
    "text": "Pandas or Point and Click\nPandas is Python’s programmatic spreadsheet based on R’s DataFrames. R community is very pragmatic and the data frames have evolved to improve performance and increase agility. They have a tidyverse package and tribbles. Pandas lags behind and while easier to code then R it often requires more code to get things done and the code can get pretty ugly. Pandas advocates often point out that spreadsheets fail around 1.5 million cells. But what they fail to mention is that getting pandas to be fast on a large dataset requires deep understanding of pandas, its api, numpy. I won’t even go into memory management. I’d say most of the ugly code is going to be very slow on big data and you’ll run out of memory. While SQL and excel have had serious effort at optimizing performance - pandas is pretty pathetic in this regard.\nI expected Pandas to be fast and intuitive on tasks like\n\nsubsetting by column type\nprinting subset rows by criteria of several columns\n\nThe first thing I expected is to be able to do tasks I did with excel or google sheets faster and better. What I mean is that I expected to map most tasks from one to the other and to be able to automate faster. Some tasks map better then others."
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#functional-programming",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#functional-programming",
    "title": "Pandas Productivity Challenge?",
    "section": "Functional Programming",
    "text": "Functional Programming\nI also noticed that Pandas provides access to subsets using [rows,cols] operator and the iloc and loc methods. I had also expected a modern functional interface to process data using RX style coding via functional primitives like map, flatmap, groupby, zip, filter, and so on. I notched some exist but no one seems to be using them in the idiomatic way. R’s tidyverse and Magrit had evolved very quickly in this direction why didn’t pandas?\nThe two biggest disappointments are reports and dashboards. Reports in excel are a no-brainer. Solid reporting can be essential when taking a data pipeline to production. Dashboards are a both a productivity enhancer and a power multiplier in BI tools like Tableau, Power BI, Google Data Studio, informatica etc. Interactive dashboards can be amazing for exploring datasets that change a lot like marketing. These are all challenges with python and pandas in a Jupyter notebook as a starting point.\nWhile hacking functional programming into pandas should be not to hard I believe that engineering it would be more sensible as it would allow high performance optimizations for larger datasets."
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#reports",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#reports",
    "title": "Pandas Productivity Challenge?",
    "section": "Reports",
    "text": "Reports\nI expected to be able to create a tabular display for my data with filtering, sorting, conditional styles, paging, sparklines. Pandas styler does some of these and there are a number of libraries that do paging sorting filtering based on javascript datatables library other allow sparklines and charts based on chartjs. They don’t work together - each does its own thing.. But they are so fragile, relying on javascript and breaking as one changes to different notebook. This is a major problem with python - there are many environments and no established method way to get code to work well on most of these.\nYou might notice that the a big issues is interoperability. Ideally there should be support for reports with:\n\nset number of rows displayed (all, 10 etc)\npaging\nfield filtering (interactive)\nscrolling to show all columns\nColumn\n\nvisual indication of type (numeric, categorical, ordinal, temporal, time series, geojson)\nvisual summaries\n\nbarchart or area chart - with tooltips for leading values\nbars with error indicators\n\n\nSparklines (line chart, histogram, candle chart)\nPivot tables\n\nBinning\nWith sub-aggregates and over all aggregates.\n\nConditional formatting (heatmap, barchart)\nHighlighting (missing values, minimum, maximum, median, mode, quartile, outliers, mean & number of sd, custom maps).\nSampling of most interesting rows…"
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#interactive-pivot-tables",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#interactive-pivot-tables",
    "title": "Pandas Productivity Challenge?",
    "section": "interactive pivot tables",
    "text": "interactive pivot tables\nPivottable.js, interactive pivot tables and charts\n\nInstallation\n!pip install pivottablejs\nfrom pivottablejs import pivot_ui\npivot_ui(df,outfile_path=’pivottablejs.html’)\nHTML(‘pivottablejs.html’)"
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#interactive-sorting-and-filtering",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#interactive-sorting-and-filtering",
    "title": "Pandas Productivity Challenge?",
    "section": "interactive sorting and filtering",
    "text": "interactive sorting and filtering\n\nqgrd"
  },
  {
    "objectID": "posts/2020/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime.html",
    "href": "posts/2020/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime.html",
    "title": "How to avoid cross site scripting (XSS) errors with the Jupyter local runtime for Colab",
    "section": "",
    "text": "google colab\n\n\n\nSo the trick is\n\nto use --NotebookApp.allow_origin and --no-browser\nand get the token from the command line when connecting to Google collab.\n\njupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' \\\n  --port=9090 --no-browser\n\n\n\ngoogle colab\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {How to Avoid Cross Site Scripting {(XSS)} Errors with the\n    {Jupyter} Local Runtime for {Colab}},\n  date = {2020-02-20},\n  url = {https://orenbochman.github.io/blog//posts/2020/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “How to Avoid Cross Site Scripting (XSS)\nErrors with the Jupyter Local Runtime for Colab.” February 20,\n2020. https://orenbochman.github.io/blog//posts/2020/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime.html."
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html",
    "href": "posts/2023/2023-12-20-autogluon/index.html",
    "title": "AutoGluon Cheetsheets",
    "section": "",
    "text": "AutoGluon is a powerful framework for auto-ML."
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "href": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "title": "AutoGluon Cheetsheets",
    "section": "Tabular",
    "text": "Tabular\n\n\n\nTabular"
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "href": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "title": "AutoGluon Cheetsheets",
    "section": "Time Series",
    "text": "Time Series\n\n\n\nTime Series"
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "href": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "title": "AutoGluon Cheetsheets",
    "section": "Multimodal",
    "text": "Multimodal\n\n\n\nMultimodal\n\n\n\n\n\nautogluon\nTabular\nTime Series\nMultimodal"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html",
    "href": "posts/2024/hungarian/hungarian.html",
    "title": "hungarian cheat sheet",
    "section": "",
    "text": "I’ve been learning Hungarian on Duolingo in the last year. Before that I learned in the hungarian Debrecen Summer School for a three courses and before that the full Pimsleur hungarian course. The main problem with Duolingo is that it approach to grammar is nonexistent they explicitly state you should master it by example. This suggest that an innovative idea: add a machine readable grammar that accepts all the content in the course and that rejects what the course rejects. This is then rendered as a human readable document using a script. Unfortunately build a good grammar and making it human readable are both non-trivial tasks at this point in time!?\nI’ve put together this cheat sheet to help accelerate the learning. I hope with more time I’ll also be able to use an LLM to generate more dynamical training regimens then Duolingo can.\nAt this point I have some issues with word order of some sentences. Which I hope to figure out. Talking with native hungarian friends is both amusing and disappointing. They could make no coherent explanation of why the sentences follow that order. One real world issue is that hungarian sentences can have very nuanced meaning based on the word order yet try to translate these meanings to english which does not usually make such nuanced interpretation and the meaning seems nonsensical in many cases or artificial, making you think why would you be saying this where in Hungarian that what that word order has a natural meaning.\nNote that this is an issue when looking at parse trees - it can be a challenge to understand what some different trees actually mean - particularly as an random tree can be drawn it probably wouldn’t necessarily correspond to a sentence in english. To tie this up Hungarian with its flexible word order admits far more parse trees."
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#alphabet",
    "href": "posts/2024/hungarian/hungarian.html#alphabet",
    "title": "hungarian cheat sheet",
    "section": "Alphabet",
    "text": "Alphabet"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#vowels",
    "href": "posts/2024/hungarian/hungarian.html#vowels",
    "title": "hungarian cheat sheet",
    "section": "Vowels",
    "text": "Vowels\n\n\n\nAPA hungarian chart\n\n\n\nVowel Harmony\n\n\n\nAPA vowels chart\n\n\n\n\n\nback vowels\na, á, o, ó, u, ú\n\n\nfront vowels\ne, é, i, í, ö, ő, ü, ű\n\n\nunrounded\ne, é, i, í\n\n\nrounded\nö, ő, ü, ű"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#pronouns",
    "href": "posts/2024/hungarian/hungarian.html#pronouns",
    "title": "hungarian cheat sheet",
    "section": "Pronouns",
    "text": "Pronouns\n\n\nPersonal Pronouns\n\n\n\nÉn\nI\nMi\nus\n\n\nTe\nyou\nTi\nyou (pl.)\n\n\nŐ\nxe\nŐk\nthey (pl.)\n\n\nÖn\nyou (pol. sg.)\nÖnök\nyou (pol. pl.)\n\n\n\n\n\nReflexive Pronouns\n\n\n\nMagam\nmyself\nMagunk\nourselves\n\n\nMagad\nyourself\nMagatok\nyourselves\n\n\nMaga\nxirself\nMaguk\nthemselves"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#cases",
    "href": "posts/2024/hungarian/hungarian.html#cases",
    "title": "hungarian cheat sheet",
    "section": "Cases",
    "text": "Cases\n\n\n\ncase\nsuffix\n\n\n\n\naccusative\n-t/-ot/-et/öt/-at\n\n\ndative\n-nak/-nek\n\n\nillative\n-ba/-be\n\n\ninessive\n-ban/-ben\n\n\nelative\n-ból/-ből\n\n\nallative\n-hoz/-hez/-höz\n\n\nadessive\n-nál/-nél\n\n\nablative\n-tól/-től\n\n\nsublative\n-ra/-re\n\n\nsuperessive\n-n/-on/-en/-ön\n\n\ndelative\n-ról/-ről\n\n\ninstrumental\n-val/-vel\n\n\ncausal-final\n-ért\n\n\nterminative\n-ig\n\n\ntemporal\n-kor\n\n\ntranslative\n-vá/-vé\n\n\ngenitive\n-é"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#plurals",
    "href": "posts/2024/hungarian/hungarian.html#plurals",
    "title": "hungarian cheat sheet",
    "section": "Plurals",
    "text": "Plurals\n\n\n\nback vowel\n-k/-ok/-ak\n\n\nfront vowel\n-k/-ek/-ök"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#pl.-adjectives",
    "href": "posts/2024/hungarian/hungarian.html#pl.-adjectives",
    "title": "hungarian cheat sheet",
    "section": "Pl. Adjectives",
    "text": "Pl. Adjectives\n\n\n\n\n\n\n\n\nending in a / e\nbv: -’k\nfv: -’k\n\n\nending in i / ú / ű\nbv: -ak\nfv: -ek\n\n\nending in ó / ő\nbv (participle): -ak OR -k\nfv (participle): -ek OR -k\n\n\nending in ó / ő\nbv (regular): -k\nfv (regular): -k\n\n\nending in a consonant\nbv: -ak\nfv: -ek\n\n\natlan / etlen adjectives\nbv: -ok\nfv: -ek\n\n\nethnonyms ending in i\nbv: -ak\nfv: -ek\n\n\nall other ethnonyms\nbv: -ok\nfv: -ek/-ök"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#sg.-possession",
    "href": "posts/2024/hungarian/hungarian.html#sg.-possession",
    "title": "hungarian cheat sheet",
    "section": "Sg. Possession",
    "text": "Sg. Possession\n\n\n\npronoun\nbv\nfv\n\n\n\n\nÉn\n-m/-om/-am\n-m/-em/-öm\n\n\nTe\n-d/-od/-ad\n-d/-ed/-öd\n\n\nŐ (Ön)\n-ja/-a\n-je/-e\n\n\nMi\n-nk/-unk\n-nk/-ünk\n\n\nTi\n-tok/-otok/-atok\n-tok/-etek/-ötök\n\n\nŐk (Önök)\n-juk/-uk\n-jük/-ük"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#pl.-possession",
    "href": "posts/2024/hungarian/hungarian.html#pl.-possession",
    "title": "hungarian cheat sheet",
    "section": "Pl. Possession",
    "text": "Pl. Possession\n\n\n\npronoun\nbv\nfv\n\n\n\n\nÉn\n-im -aim -jaim\n-im -eim -jeim\n\n\nTe\n-id -aid -jaid\n-id -eid -jeid\n\n\nŐ Ön\n-i -ai -jai\n-i -ei -jei\n\n\nMi\n-ink -aink -jaink\n-ink -eink -jeink\n\n\nTi\n-itok -aitok -jaitok\n-itek -eitek -jeitek\n\n\nŐk Önök\n-ik -aik -jaik\n-ik -eik -jeik"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#acc.-adjectives",
    "href": "posts/2024/hungarian/hungarian.html#acc.-adjectives",
    "title": "hungarian cheat sheet",
    "section": "Acc. Adjectives",
    "text": "Acc. Adjectives\n\n\n\n\n\n\n\n\nending in a / e\nbv: -’t\nfv: -’t\n\n\nending in other vowels\nbv: -t\nfv: -t\n\n\nending in a consonant\nbv: -at\nfv: -et\n\n\natlan / etlen adjectives\nbv: -t\nfv: -t\n\n\nethnonyms (vowel)\nbv: -’t\nfv: -’t\n\n\nethnonyms (consonant) 1\nbv: -ot\nfv: -et/-öt\n\n\n\n1 no link vowel after j, l, ly, n, ny, r, s, sz, z, zs"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#verbs---present-tense",
    "href": "posts/2024/hungarian/hungarian.html#verbs---present-tense",
    "title": "hungarian cheat sheet",
    "section": "Verbs - Present Tense",
    "text": "Verbs - Present Tense\n\nDefinite\n\n\n\n\n\n\n\n\npronoun\nbv\nfv\n\n\n\n\nÉn\n-om\n-em/-öm\n\n\nTe\n-od\n-ed/-öd\n\n\nŐ Ön 2\n-ja\nfv -i\n\n\nMi\n-juk\n-jük\n\n\nTi\n-játok\n-itek\n\n\nŐk (Önök) 3\n-ják\n-ik\n\n\n\n2 if stem ends in s, sz, z, dz – leading j in the ending turns into the last letter3 if stem ends in s, sz, z, dz – leading j in the ending turns into the last letter\n\nIndefinite (Regular Verbs)\n\n\n\npronoun\nbv\nfv\n\n\n\n\nÉn\n-ok\n-ek/-ök\n\n\nTe\n-sz\n-sz\n\n\nŐ Ön\nØ\nØ\n\n\nMi\n-unk\n-ünk\n\n\nTi\n-tok\n-tek/-tök\n\n\nŐk Önök\n-nak\n-nek\n\n\n\n\n\nIndefinite (-ik Verbs)\n\n\n\npronoun\nbv\nfv\n\n\n\n\nÉn\n-om\n-em/-öm\n\n\nTe\n-sz\n-sz\n\n\nŐ Ön\n-ik\n-ik\n\n\nMi\n-unk\n-ünk\n\n\nTi\n-tok\n-tek/-tök\n\n\nŐk Önök\n-nak\n-nek"
  },
  {
    "objectID": "posts/2024/hungarian/hungarian.html#indefinite-verb-ending-in-s-sz-z-dz",
    "href": "posts/2024/hungarian/hungarian.html#indefinite-verb-ending-in-s-sz-z-dz",
    "title": "hungarian cheat sheet",
    "section": "Indefinite (verb ending in s, sz, z, dz)",
    "text": "Indefinite (verb ending in s, sz, z, dz)\n\nTe 4\n\n→ bv: -ol\n→ fv: -el\n\n\n4 only for “te” conjugation\n \n\n\n\nAPA hungarian chart\nAPA vowels chart"
  },
  {
    "objectID": "posts/2024/quarto_bootstrap/card.html",
    "href": "posts/2024/quarto_bootstrap/card.html",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "Quarto HTML pages and dashboards are built using bootstrap.\nQuato themes are also based on bootstrap,\nQuarto also supports pandoc for div and spans.\n\nThus it is possible to pop bootstrap css into divs and spans and get formatting using bootstrap.\nIt is also possible to create bootstrap components by structuring markdown with the appropriate css and styling.\n\n\nc.f. bootstrap documentation\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nprimary secondary success danger warning info light dark\n\n\n\n\n\nPraeterea iter est quasdam res quas ex communi.\n\n\nPraeterea iter est quasdam res quas ex communi.\n\n\n\n\nthis isn’t visible using the light theme and only partially supported in the dark theme\n\n\n\n\nClick to toggle popover\n\nspan span"
  },
  {
    "objectID": "posts/2024/quarto_bootstrap/card.html#cards",
    "href": "posts/2024/quarto_bootstrap/card.html#cards",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "c.f. bootstrap documentation\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere"
  },
  {
    "objectID": "posts/2024/quarto_bootstrap/card.html#using-grid",
    "href": "posts/2024/quarto_bootstrap/card.html#using-grid",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "Some quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere"
  },
  {
    "objectID": "posts/2024/quarto_bootstrap/card.html#colored-links",
    "href": "posts/2024/quarto_bootstrap/card.html#colored-links",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "primary secondary success danger warning info light dark"
  },
  {
    "objectID": "posts/2024/quarto_bootstrap/card.html#trunctated-text",
    "href": "posts/2024/quarto_bootstrap/card.html#trunctated-text",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "Praeterea iter est quasdam res quas ex communi.\n\n\nPraeterea iter est quasdam res quas ex communi."
  },
  {
    "objectID": "posts/2024/quarto_bootstrap/card.html#badges-new-pill",
    "href": "posts/2024/quarto_bootstrap/card.html#badges-new-pill",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "this isn’t visible using the light theme and only partially supported in the dark theme"
  },
  {
    "objectID": "posts/2024/quarto_bootstrap/card.html#popover",
    "href": "posts/2024/quarto_bootstrap/card.html#popover",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "Click to toggle popover\n\nspan span"
  },
  {
    "objectID": "posts/2024/mermaid_mindmap/index.html",
    "href": "posts/2024/mermaid_mindmap/index.html",
    "title": "😁 Quarto 💖 Mermaid🧜 Mindmaps 🧠",
    "section": "",
    "text": "mermaid\n\n\nQuarto at last supports 🧠 Mindmap charts using Mermaid 🧜‍♀️ charts.\n\n\n\n\n\nmindmap\n  root)mindmap(\n    Origins\n      Long history\n      ::icon(fa fa-book)\n      Popularisation\n        British popular psychology author Tony Buzan\n    Research\n      On effectiveness&lt;br/&gt;and features\n      On Automatic creation\n      ::icon(mdi mdi-skull-outline)\n        Uses\n            Creative techniques\n            Strategic planning\n            Argument mapping\n    Tools\n      Pen and paper\n      ::icon(fa fa-book)\n      [\"`🧜 Mermaid`\"]\n      [for peace]\n      ::icon(mdi mdi-twitter)\n      \n\n\n\n\n\n\n\nFor details on creating mind maps c.f.\nAlso note: that the icon features for including fontawesome and material icons is not working!?\nA a workaround adding the following to the page frontmatter\nformat: \n  html:\n    include-in-header:\n      - text: |\n          &lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css\"&gt; \n          &lt;link href=\"https://cdnjs.cloudflare.com/ajax/libs/MaterialDesign-Webfont/7.4.47/css/materialdesignicons.min.css\" rel=\"stylesheet\"&gt;\n\n\n\n\nmermaid\n\n\nCitationBibTeX citation:@online{bochman20224,\n  author = {Bochman, Oren},\n  title = {😁 {Quarto} 💖 {Mermaid🧜} {Mindmaps} 🧠},\n  date = {20224/02/12},\n  url = {https://orenbochman.github.io/blog//posts/2024/mermaid_mindmap},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 20224–2AD. “😁 Quarto 💖 Mermaid🧜 Mindmaps\n🧠.” 20224–2AD. https://orenbochman.github.io/blog//posts/2024/mermaid_mindmap."
  },
  {
    "objectID": "posts/2024/welcome/index.html",
    "href": "posts/2024/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\n\nthumbnail\n\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nthumbnail\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2024-01-26},\n  url = {https://orenbochman.github.io/blog//posts/2024/welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. “Welcome To My Blog.” January 26,\n2024. https://orenbochman.github.io/blog//posts/2024/welcome."
  },
  {
    "objectID": "posts/2024/post-with-code/index.html",
    "href": "posts/2024/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is an obligatory post with executable code.\n\n11 + 1\n\n\n1\n\nthis is an annotation\n\n\n\n\n2\n\n\nand this is a figure with a caption\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\nIt’s also useful to have a small sample of printing a table from a pandas dataframe and a quick access to :panda: pandas a fluent wrangling block\n\n1import numpy as np\nimport pandas as pd\nfrom itables import show\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n2df = (    pd.read_csv('./data/Salary Data.csv')\n3          .dropna()\n4          .drop_duplicates()\n5          .assign(is_male=lambda x: x['Gender'].apply(lambda y: 1 if y == 'Male' else 0),\n6                  is_PhD=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'PhD' else 0),\n                  is_BA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Bachelor\\'s' else 0),\n                  is_MA=lambda x: x['Education Level'].apply(lambda y: 1 if y == 'Master\\'s' else 0),\n                 \n          )\n7          .rename(columns={'Years of Experience':'xp'})\n8          .drop(['Gender','Education Level','Job Title'],axis=1)\n\n    )\n\n#df['Education Level'] = edu_label_encoder.fit_transform(df['Education Level'])\n#job_title_encoder = LabelEncoder()\n#df['Job Title']=job_title_encoder.fit_transform(df['Job Title'])\n9show(df)\n\n\n1\n\nimport the usual suspects\n\n2\n\nload the salary dataset\n\n3\n\nremove rows with missing values\n\n4\n\nremove duplicate entries\n\n5\n\nrecode gender to is_male\n\n6\n\nrecode categorical education level to dummies\n\n7\n\nrename columns\n\n8\n\ndrop columns\n\n9\n\npeek at the data\n\n\n\n\n\n\nTable 1\n\n\n\n\n\n\n\n    \n      \n      Age\n      xp\n      Salary\n      is_male\n      is_PhD\n      is_BA\n      is_MA\n    \n  Loading... (need help?)\n\n\n\n\n\nraw Salary DataSet\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {Post {With} {Code}},\n  date = {2024-01-28},\n  url = {https://orenbochman.github.io/blog//posts/2024/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. “Post With Code.” January 28, 2024. https://orenbochman.github.io/blog//posts/2024/post-with-code."
  },
  {
    "objectID": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html",
    "href": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html",
    "title": "A/B testing cost and risks?",
    "section": "",
    "text": "While it is not a forgone conclusion that CRO driven by A/B test will be a major disaster but it is likely that without some expert supervision it can end up a costing more and taking longer. The statistical concepts are not fairly basic but most of them are misunderstood."
  },
  {
    "objectID": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html#setup-costs",
    "href": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html#setup-costs",
    "title": "A/B testing cost and risks?",
    "section": "Setup costs",
    "text": "Setup costs\nYou generally need to pay a developer to code the alternatives, and to setup the code that does the the sampling and book keeping. An IT guy to deploy it and an analyst or data scientist to analyse it and then a marketing manager to decide to take some action (ok the intervention and decide to do more testing.) ## Exploration costs and diminishing return When we want to romanticize researching a new intervention we call it Exploration and then we use the less romantic term Exploitation to refer to making using our current best intervention to get outcomes from our web marketing efforts. A/B tests shunts a percentage of you traffic (often 50%) for the duration of the experiment to an intervention we call B we expect to beat out current best effort which we call A. But since pretty smart people chose A it can take many attempts to find a B that better. Also each time you find a better alternative it get harder and you end up getting smaller improvements this is due to what is called a smaller effect size for the intervention. Consider that you will eventually get to an optimum and need to look at a different strategy to make improvements. The good news is that each time your exploration works out and you get a better outcome the test will end up driving up your kpi and goals. But the bad news is that most of the time you wont be getting a win and conducting the test will cost you in lost action. Most A/B testing platforms can track your goals and will try to minimize the negative impact of the test using power of Bayesian statistics. You should make use of these if they are an option. But if you are getting started you might not have the benefit of integrating this type of stats into your experiment. Also even the amazingly capable people who build these tools can get it wrong and have had to rewriting their systems. The main reason that testing has diminishing returns is that you it is much easier to test for big changes early but small subtle changes which have small effects take longer to achieve statistical significance because we are trying to separate two very similar signals coming from A and B. Each win generally means that the next test will take longer be less likely to be a win and therefore cost more on average.\nAs the are usually diminishing returns from running A/B test. If your landing page has a low conversion rate, say 1.5%. You can do experiments and you might be able to reduce bounce rate by 40% percent which should increase your conversion rate to 1.2%. Next you might be able to increase time on page by 300% using better videos that might boost your conversion rate to 3.5, your might be able to pick a more effective cal to action and get around to 3.7% conversion rates. And you might also add an aggressive exit popup and get to 4.2%. That is a dream scenario that CTR consultants dream about. But if your are not working in a startup and you have a solid marketing team you will probably start with many of the choices near optimum and end up at 3% conversion rate. And each successful test will give you a .1% improvement. That might be ok but such a small improvement will take much longer to reach significance and therefore end up costing more. With all the test you ever do you might never reach 4% Over time you should expect that A/B test will be measuring ever smaller effects and will require more traffic to arrive at a statistically significant result. Longer test will cost more."
  },
  {
    "objectID": "posts/2011/2011-11-29-text-mining-with-r/index.html",
    "href": "posts/2011/2011-11-29-text-mining-with-r/index.html",
    "title": "Text Mining With R",
    "section": "",
    "text": "Computational Linguistics tasks:"
  },
  {
    "objectID": "posts/2011/2011-11-29-text-mining-with-r/index.html#text-preprocessing",
    "href": "posts/2011/2011-11-29-text-mining-with-r/index.html#text-preprocessing",
    "title": "Text Mining With R",
    "section": "Text preprocessing",
    "text": "Text preprocessing\n\n4tm_corpus &lt;- tm_map(tm_corpus, tolower)\ninspect(tm_corpus)\n\n\n4\n\nthis makes all the tokens lowercase\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs, hospitals, doctors                                                                                                                                      \n [2] smog, pollution, micro-plastics, environment.                                                                                                                  \n [3] doctors, hospitals, healthcare                                                                                                                                 \n [4] pollution, environment, water.                                                                                                                                 \n [5] i love nlp with deep learning.                                                                                                                                 \n [6] i love machine learning.                                                                                                                                       \n [7] he said he was keeping the wolf from the door.                                                                                                                 \n [8] time flies like an arrow, fruit flies like a banana.                                                                                                           \n [9] pollution, greenhouse gasses, ghg, hydrofluorocarbons, ozone hole, global warming. montreal protocol.                                                          \n[10] greenhouse gasses, hydrofluorocarbons, perfluorocarbons, sulfur hexafluoride, carbon dioxide, carbon monoxide, co2, hydrofluorocarbons, methane, nitrous oxide.\n\n\n\n5tm_corpus &lt;- tm_map(tm_corpus, content_transformer(removePunctuation))\ninspect(tm_corpus)\n\n\n5\n\nthis removes punctuation tokens\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                              \n [2] smog pollution microplastics environment                                                                                                             \n [3] doctors hospitals healthcare                                                                                                                         \n [4] pollution environment water                                                                                                                          \n [5] i love nlp with deep learning                                                                                                                        \n [6] i love machine learning                                                                                                                              \n [7] he said he was keeping the wolf from the door                                                                                                        \n [8] time flies like an arrow fruit flies like a banana                                                                                                   \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                       \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co2 hydrofluorocarbons methane nitrous oxide\n\n\n\n6tm_corpus &lt;- tm_map(tm_corpus, removeWords, stopwords(\"english\"))\ninspect(tm_corpus)\n\n\n6\n\nthis removes stop words\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                              \n [2] smog pollution microplastics environment                                                                                                             \n [3] doctors hospitals healthcare                                                                                                                         \n [4] pollution environment water                                                                                                                          \n [5]  love nlp  deep learning                                                                                                                             \n [6]  love machine learning                                                                                                                               \n [7]  said   keeping  wolf   door                                                                                                                         \n [8] time flies like  arrow fruit flies like  banana                                                                                                      \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                       \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co2 hydrofluorocarbons methane nitrous oxide\n\n\n\n7tm_corpus &lt;- tm_map(tm_corpus, removeNumbers)\ninspect(tm_corpus)\n\n\n7\n\nthis removes numbers\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                             \n [2] smog pollution microplastics environment                                                                                                            \n [3] doctors hospitals healthcare                                                                                                                        \n [4] pollution environment water                                                                                                                         \n [5]  love nlp  deep learning                                                                                                                            \n [6]  love machine learning                                                                                                                              \n [7]  said   keeping  wolf   door                                                                                                                        \n [8] time flies like  arrow fruit flies like  banana                                                                                                     \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                      \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co hydrofluorocarbons methane nitrous oxide\n\n\n\n8tm_corpus &lt;- tm_map(tm_corpus, stemDocument, language=\"english\")\ninspect(tm_corpus)\n\n\n8\n\nthis stems the words\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drug hospit doctor                                                                                                                       \n [2] smog pollut microplast environ                                                                                                           \n [3] doctor hospit healthcar                                                                                                                  \n [4] pollut environ water                                                                                                                     \n [5] love nlp deep learn                                                                                                                      \n [6] love machin learn                                                                                                                        \n [7] said keep wolf door                                                                                                                      \n [8] time fli like arrow fruit fli like banana                                                                                                \n [9] pollut greenhous gass ghg hydrofluorocarbon ozon hole global warm montreal protocol                                                      \n[10] greenhous gass hydrofluorocarbon perfluorocarbon sulfur hexafluorid carbon dioxid carbon monoxid co hydrofluorocarbon methan nitrous oxid\n\n\n\n9tm_corpus &lt;- tm_map(tm_corpus, stripWhitespace)\ninspect(tm_corpus)\n\n\n9\n\nRemoving Whitespaces - a single white space or group of whitespaces may be considered to be a token within a corpus. This is how we remove these token\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drug hospit doctor                                                                                                                       \n [2] smog pollut microplast environ                                                                                                           \n [3] doctor hospit healthcar                                                                                                                  \n [4] pollut environ water                                                                                                                     \n [5] love nlp deep learn                                                                                                                      \n [6] love machin learn                                                                                                                        \n [7] said keep wolf door                                                                                                                      \n [8] time fli like arrow fruit fli like banana                                                                                                \n [9] pollut greenhous gass ghg hydrofluorocarbon ozon hole global warm montreal protocol                                                      \n[10] greenhous gass hydrofluorocarbon perfluorocarbon sulfur hexafluorid carbon dioxid carbon monoxid co hydrofluorocarbon methan nitrous oxid\n\n\n\ndtm &lt;- DocumentTermMatrix(tm_corpus)\ninspect(dtm)\n\n&lt;&lt;DocumentTermMatrix (documents: 10, terms: 43)&gt;&gt;\nNon-/sparse entries: 53/377\nSparsity           : 88%\nMaximal term length: 17\nWeighting          : term frequency (tf)\nSample             :\n    Terms\nDocs doctor environ fli gass hospit hydrofluorocarbon learn like love pollut\n  1       1       0   0    0      1                 0     0    0    0      0\n  10      0       0   0    1      0                 2     0    0    0      0\n  2       0       1   0    0      0                 0     0    0    0      1\n  3       1       0   0    0      1                 0     0    0    0      0\n  4       0       1   0    0      0                 0     0    0    0      1\n  5       0       0   0    0      0                 0     1    0    1      0\n  6       0       0   0    0      0                 0     1    0    1      0\n  7       0       0   0    0      0                 0     0    0    0      0\n  8       0       0   2    0      0                 0     0    2    0      0\n  9       0       0   0    1      0                 1     0    0    0      1\n\n\n\nfindFreqTerms(dtm, 2)\n\n [1] \"doctor\"            \"hospit\"            \"environ\"          \n [4] \"pollut\"            \"learn\"             \"love\"             \n [7] \"fli\"               \"like\"              \"gass\"             \n[10] \"greenhous\"         \"hydrofluorocarbon\" \"carbon\"           \n\n\n\nfindAssocs(dtm, \"polution\", 0.8)\n\n$polution\nnumeric(0)\n\n\n\nas.matrix(dtm)\n\n    Terms\nDocs doctor drug hospit environ microplast pollut smog healthcar water deep\n  1       1    1      1       0          0      0    0         0     0    0\n  2       0    0      0       1          1      1    1         0     0    0\n  3       1    0      1       0          0      0    0         1     0    0\n  4       0    0      0       1          0      1    0         0     1    0\n  5       0    0      0       0          0      0    0         0     0    1\n  6       0    0      0       0          0      0    0         0     0    0\n  7       0    0      0       0          0      0    0         0     0    0\n  8       0    0      0       0          0      0    0         0     0    0\n  9       0    0      0       0          0      1    0         0     0    0\n  10      0    0      0       0          0      0    0         0     0    0\n    Terms\nDocs learn love nlp machin door keep said wolf arrow banana fli fruit like time\n  1      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  2      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  3      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  4      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  5      1    1   1      0    0    0    0    0     0      0   0     0    0    0\n  6      1    1   0      1    0    0    0    0     0      0   0     0    0    0\n  7      0    0   0      0    1    1    1    1     0      0   0     0    0    0\n  8      0    0   0      0    0    0    0    0     1      1   2     1    2    1\n  9      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  10     0    0   0      0    0    0    0    0     0      0   0     0    0    0\n    Terms\nDocs gass ghg global greenhous hole hydrofluorocarbon montreal ozon protocol\n  1     0   0      0         0    0                 0        0    0        0\n  2     0   0      0         0    0                 0        0    0        0\n  3     0   0      0         0    0                 0        0    0        0\n  4     0   0      0         0    0                 0        0    0        0\n  5     0   0      0         0    0                 0        0    0        0\n  6     0   0      0         0    0                 0        0    0        0\n  7     0   0      0         0    0                 0        0    0        0\n  8     0   0      0         0    0                 0        0    0        0\n  9     1   1      1         1    1                 1        1    1        1\n  10    1   0      0         1    0                 2        0    0        0\n    Terms\nDocs warm carbon dioxid hexafluorid methan monoxid nitrous oxid perfluorocarbon\n  1     0      0      0           0      0       0       0    0               0\n  2     0      0      0           0      0       0       0    0               0\n  3     0      0      0           0      0       0       0    0               0\n  4     0      0      0           0      0       0       0    0               0\n  5     0      0      0           0      0       0       0    0               0\n  6     0      0      0           0      0       0       0    0               0\n  7     0      0      0           0      0       0       0    0               0\n  8     0      0      0           0      0       0       0    0               0\n  9     1      0      0           0      0       0       0    0               0\n  10    0      2      1           1      1       1       1    1               1\n    Terms\nDocs sulfur\n  1       0\n  2       0\n  3       0\n  4       0\n  5       0\n  6       0\n  7       0\n  8       0\n  9       0\n  10      1\n\n\nload(url(“https://cbail.github.io/Trump_Tweets.Rdata”)) head(trumptweets$text)"
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html",
    "href": "posts/2011/2011-08-11-time-management/index.html",
    "title": "Time management Tips",
    "section": "",
    "text": "Effective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance. In this article, we’ll explore various time management strategies, focusing on learning from others, project management techniques, and effective meetings."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#introduction",
    "href": "posts/2011/2011-08-11-time-management/index.html#introduction",
    "title": "Time management Tips",
    "section": "",
    "text": "Effective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance. In this article, we’ll explore various time management strategies, focusing on learning from others, project management techniques, and effective meetings."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#learning-from-others",
    "href": "posts/2011/2011-08-11-time-management/index.html#learning-from-others",
    "title": "Time management Tips",
    "section": "Learning from Others",
    "text": "Learning from Others\nTo improve your time management skills, consider learning from different sources:\n\nWisdom from experienced individuals: Look for insights and advice from people who have been successful in managing their time. They can provide valuable tips and practical approaches to help you make the most of your time.\nFresh perspectives from younger generations: Younger people often have unique and innovative ideas on how to approach time management. Their perspectives can provide fresh insights on the world and how to navigate it efficiently.\nCassandra’s prophecies: Although they may seem pessimistic or overly cautious, pay attention to people who predict potential problems or obstacles. By considering their warnings, you can proactively prepare for possible challenges, preventing self-fulfilling prophecies."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#project-management",
    "href": "posts/2011/2011-08-11-time-management/index.html#project-management",
    "title": "Time management Tips",
    "section": "Project management",
    "text": "Project management\n\nMeasure work by output, not input: Focus on the results produced rather than the amount of time spent on a task. This will encourage efficiency and productivity.\nDefine specific goals: Clearly outline the objectives, responsibilities, and tasks associated with each project or job.\nTrack goal progress: Regularly monitor the progress of your goals to ensure they are on track for completion.\nProvide periodic progress reports: Keep stakeholders informed with brief, regular updates on project status.\nSet high-quality performance objectives: Ensure that your goals are challenging, attainable, and aligned with your overall mission.\nMaintain a project list: Keep a running list of all projects, breaking them down into smaller, manageable tasks if necessary.\nFocus on the next step, not the final goal: Concentrate on completing the immediate task at hand rather than getting overwhelmed by the overall project.\nPrioritize tasks effectively: Determine which tasks are most important and tackle them first.\nSchedule your day realistically: Plan your day in a way that allows for adequate time to complete tasks without feeling rushed or overwhelmed."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#conducting-efficient-meetings",
    "href": "posts/2011/2011-08-11-time-management/index.html#conducting-efficient-meetings",
    "title": "Time management Tips",
    "section": "Conducting Efficient Meetings:",
    "text": "Conducting Efficient Meetings:\n\nEstablish clear meeting objectives: Ensure that every meeting has a specific purpose, such as group bonding, reaching a group decision, or conducting peer-to-peer negotiations.\nUse alternative communication methods when appropriate: Utilize emails and letters for unidirectional information flow or for maintaining records, as they can be more efficient than meetings for certain tasks."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#conclusion",
    "href": "posts/2011/2011-08-11-time-management/index.html#conclusion",
    "title": "Time management Tips",
    "section": "Conclusion:",
    "text": "Conclusion:\nBy implementing these time management strategies, you can increase your productivity, achieve your goals, and maintain a healthy work-life balance. Remember, the key to success is consistently refining and adapting your approach to time management as you encounter new challenges and opportunities."
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html",
    "href": "posts/2011/regex-cheatsheet/index.html",
    "title": "RegEX",
    "section": "",
    "text": "Place content here"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#getting-started",
    "href": "posts/2011/regex-cheatsheet/index.html#getting-started",
    "title": "RegEX",
    "section": "Getting Started",
    "text": "Getting Started\n\nIntroduction\nThis is a quick cheat sheet to getting started with regular expressions.\n\n\nCharacter Classes\n\n\n\nPattern\nDescription\n\n\n\n\n[abc]\nA single character of: a, b or c\n\n\n[^abc]\nA character except: a, b or c\n\n\n[a-z]\nA character in the range: a-z\n\n\n[^a-z]\nA character not in the range: a-z\n\n\n[0-9]\nA digit in the range: 0-9\n\n\n[a-zA-Z]\nA character in the range:a-z or A-Z\n\n\n[a-zA-Z0-9]\nA character in the range: a-z, A-Z or 0-9\n\n\n\n\n\nQuantifiers\n\n\n\nPattern\nDescription\n\n\n\n\na?\nZero or one of a\n\n\na*\nZero or more of a\n\n\na+\nOne or more of a\n\n\n[0-9]+\nOne or more of 0-9\n\n\na{3}\nExactly 3 of a\n\n\na{3,}\n3 or more of a\n\n\na{3,6}\nBetween 3 and 6 of a\n\n\na*\nGreedy quantifier\n\n\na*?\nLazy quantifier\n\n\na*+\nPossessive quantifier\n\n\n\n\n\nCommon Metacharacters\n\n\n^\n{\n+\n&lt;\n[\n*\n)\n&gt;\n.\n(\n|\n$\n\\\n?\n\n\nEscape these special characters with \\\n\n\nMeta Sequences\n\n\n\n\n\n\n\nPattern\nDescription\n\n\n\n\n.\nAny single character\n\n\n\\s\nAny whitespace character\n\n\n\\S\nAny non-whitespace character\n\n\n\\d\nAny digit, Same as [0-9]\n\n\n\\D\nAny non-digit, Same as [^0-9]\n\n\n\\w\nAny word character\n\n\n\\W\nAny non-word character\n\n\n\\X\nAny Unicode sequences, linebreaks included\n\n\n\\C\nMatch one data unit\n\n\n\\R\nUnicode newlines\n\n\n\\v\nVertical whitespace character\n\n\n\\V\nNegation of anything except newlines and vertical tabs\n\n\n\\h\nHorizontal whitespace character\n\n\n\\H\nNegation of \n\n\n\\K\nReset match\n\n\n\\n\nMatch nth subpattern\n\n\n\\pX\nUnicode property X\n\n\n\\p{...}\nUnicode property or script category\n\n\n\\PX\nNegation of \n\n\n\\P{...}\nNegation of \n\n\n\\Q...\\E\nQuote; treat as literals\n\n\n\\k&lt;name&gt;\nMatch subpattern name\n\n\n\\k'name'\nMatch subpattern name\n\n\n\\k{name}\nMatch subpattern name\n\n\n\\gn\nMatch nth subpattern\n\n\n\\g{n}\nMatch nth subpattern\n\n\n\\g&lt;n&gt;\nRecurse nth capture group\n\n\n\\g'n'\nRecurses nth capture group.\n\n\n\\g{-n}\nMatch nth relative previous subpattern\n\n\n\\g&lt;+n&gt;\nRecurse nth relative upcoming subpattern\n\n\n\\g'+n'\nMatch nth relative upcoming subpattern\n\n\n\\g'letter'\nRecurse named capture group letter\n\n\n\\g{letter}\nMatch previously-named capture group letter\n\n\n\\g&lt;letter&gt;\nRecurses named capture group letter\n\n\n\\xYY\nHex character YY\n\n\n\\x{YYYY}\nHex character YYYY\n\n\n\\ddd\nOctal character ddd\n\n\n\\cY\nControl character Y\n\n\n[\\b]\nBackspace character\n\n\n\\\nMakes any character literal\n\n\n\n\n\nAnchors\n\n\n\nPattern\nDescription\n\n\n\n\n\\G\nStart of match\n\n\n^\nStart of string\n\n\n$\nEnd of string\n\n\n\\A\nStart of string\n\n\n\\Z\nEnd of string\n\n\n\\z\nAbsolute end of string\n\n\n\\b\nA word boundary\n\n\n\\B\nNon-word boundary\n\n\n\n\n\nSubstitution\n\n\n\nPattern\nDescription\n\n\n\n\n\\0\nComplete match contents\n\n\n\\1\nContents in capture group 1\n\n\n$1\nContents in capture group 1\n\n\n${foo}\nContents in capture group foo\n\n\n\\x20\nHexadecimal replacement values\n\n\n\\x{06fa}\nHexadecimal replacement values\n\n\n\\t\nTab\n\n\n\\r\nCarriage return\n\n\n\\n\nNewline\n\n\n\\f\nForm-feed\n\n\n\\U\nUppercase Transformation\n\n\n\\L\nLowercase Transformation\n\n\n\\E\nTerminate any Transformation\n\n\n\n\n\nGroup Constructs\n\n\n\nPattern\nDescription\n\n\n\n\n(...)\nCapture everything enclosed\n\n\n(a|b)\nMatch either a or b\n\n\n(?:...)\nMatch everything enclosed\n\n\n(?&gt;...)\nAtomic group (non-capturing)\n\n\n(?|...)\nDuplicate subpattern group number\n\n\n(?#...)\nComment\n\n\n\n|(?'name'...) | Named Capturing Group| |(?&lt;name&gt;...) | Named Capturing Group| |(?P&lt;name&gt;...) | Named Capturing Group|\n|(?imsxXU) | Inline modifiers| |(?(DEFINE)...) | Pre-define patterns before using them|\n\n\nAssertions\n\n\n\n-\n-\n\n\n\n\n(?(1)yes|no)\nConditional statement\n\n\n(?(R)yes|no)\nConditional statement\n\n\n(?(R#)yes|no)\nRecursive Conditional statement\n\n\n(?(R&name)yes|no)\nConditional statement\n\n\n(?(?=...)yes|no)\nLookahead conditional\n\n\n(?(?&lt;=...)yes|no)\nLookbehind conditional\n\n\n\n\n\nLookarounds\n\n\n\n-\n-\n\n\n\n\n(?=...)\nPositive Lookahead\n\n\n(?!...)\nNegative Lookahead\n\n\n(?&lt;=...)\nPositive Lookbehind\n\n\n(?&lt;!...)\nNegative Lookbehind\n\n\n\nLookaround lets you match a group before (lookbehind) or after (lookahead) your main pattern without including it in the result.\n\n\nFlags/Modifiers\n\n\n\nPattern\nDescription\n\n\n\n\ng\nGlobal\n\n\nm\nMultiline\n\n\ni\nCase insensitive\n\n\nx\nIgnore whitespace\n\n\ns\nSingle line\n\n\nu\nUnicode\n\n\nX\neXtended\n\n\nU\nUngreedy\n\n\nA\nAnchor\n\n\nJ\nDuplicate group names\n\n\n\n\n\nRecurse\n\n\n\n-\n-\n\n\n\n\n(?R)\nRecurse entire pattern\n\n\n(?1)\nRecurse first subpattern\n\n\n(?+1)\nRecurse first relative subpattern\n\n\n(?&name)\nRecurse subpattern name\n\n\n(?P=name)\nMatch subpattern name\n\n\n(?P&gt;name)\nRecurse subpattern name\n\n\n\n\n\nPOSIX Character Classes\n\n\n\n\n\n\n\n\nCharacter Class\nSame as\nMeaning\n\n\n\n\n[[:alnum:]]\n[0-9A-Za-z]\nLetters and digits\n\n\n[[:alpha:]]\n[A-Za-z]\nLetters\n\n\n[[:ascii:]]\n[\\x00-\\x7F]\nASCII codes 0-127\n\n\n[[:blank:]]\n[\\t ]\nSpace or tab only\n\n\n[[:cntrl:]]\n[\\x00-\\x1F\\x7F]\nControl characters\n\n\n[[:digit:]]\n[0-9]\nDecimal digits\n\n\n[[:graph:]]\n[[:alnum:][:punct:]]\nVisible characters (not space)\n\n\n[[:lower:]]\n[a-z]\nLowercase letters\n\n\n[[:print:]]\n[ -~] == [ [:graph:]]\nVisible characters\n\n\n[[:punct:]]\n[!“#$%&’()*+,-./:;&lt;=&gt;?@[]^_`{|}~]\nVisible punctuation characters\n\n\n[[:space:]]\n[\nWhitespace\n\n\n[[:upper:]]\n[A-Z]\nUppercase letters\n\n\n[[:word:]]\n[0-9A-Za-z_]\nWord characters\n\n\n[[:xdigit:]]\n[0-9A-Fa-f]\nHexadecimal digits\n\n\n[[:&lt;:]]\n[\\b(?=\\w)]\nStart of word\n\n\n[[:&gt;:]]\n[\\b(?&lt;=\\w)]\nEnd of word\n\n\n\n{.show-header}\n\n\nControl verb\n\n\n\n-\n-\n\n\n\n\n(*ACCEPT)\nControl verb\n\n\n(*FAIL)\nControl verb\n\n\n(*MARK:NAME)\nControl verb\n\n\n(*COMMIT)\nControl verb\n\n\n(*PRUNE)\nControl verb\n\n\n(*SKIP)\nControl verb\n\n\n(*THEN)\nControl verb\n\n\n(*UTF)\nPattern modifier\n\n\n(*UTF8)\nPattern modifier\n\n\n(*UTF16)\nPattern modifier\n\n\n(*UTF32)\nPattern modifier\n\n\n(*UCP)\nPattern modifier\n\n\n(*CR)\nLine break modifier\n\n\n(*LF)\nLine break modifier\n\n\n(*CRLF)\nLine break modifier\n\n\n(*ANYCRLF)\nLine break modifier\n\n\n(*ANY)\nLine break modifier\n\n\n\\R\nLine break modifier\n\n\n(*BSR_ANYCRLF)\nLine break modifier\n\n\n(*BSR_UNICODE)\nLine break modifier\n\n\n(*LIMIT_MATCH=x)\nRegex engine modifier\n\n\n(*LIMIT_RECURSION=d)\nRegex engine modifier\n\n\n(*NO_AUTO_POSSESS)\nRegex engine modifier\n\n\n(*NO_START_OPT)\nRegex engine modifier"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-examples",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-examples",
    "title": "RegEX",
    "section": "Regex examples",
    "text": "Regex examples\n\nCharacters\n\n\n\n\n\n\n\nPattern\nMatches\n\n\n\n\nring\nMatch ring springboard etc.\n\n\n.\nMatch a, 9, + etc.\n\n\nh.o\nMatch hoo, h2o, h/o etc.\n\n\nring\\?\nMatch ring?\n\n\n\\(quiet\\)\nMatch (quiet)\n\n\nc:\\\\windows\nMatch c:\n\n\n\nUse \\ to search for these special characters:  [ \\ ^ $ . | ? * + ( ) { }\n\n\nAlternatives\n\n\n\nPattern\nMatches\n\n\n\n\ncat|dog\nMatch cat or dog\n\n\nid|identity\nMatch id or identity\n\n\nidentity|id\nMatch id or identity\n\n\n\nOrder longer to shorter when alternatives overlap\n\n\nCharacter classes\n\n\n\nPattern\nMatches\n\n\n\n\n[aeiou]\nMatch any vowel\n\n\n[^aeiou]\nMatch a NON vowel\n\n\nr[iau]ng\nMatch ring, wrangle, sprung, etc.\n\n\ngr[ae]y\nMatch gray or grey\n\n\n[a-zA-Z0-9]\nMatch any letter or digit\n\n\n[\\u3a00-\\ufa99]\nMatch any Unicode Hàn (中文)\n\n\n\nIn [ ] always escape . \\ ] and sometimes ^ - .\n\n\nShorthand classes\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n\\w\n“Word” character (letter, digit, or underscore)\n\n\n\\d\nDigit\n\n\n\\s\nWhitespace (space, tab, vtab, newline)\n\n\n\\W, \\D, or \\S\nNot word, digit, or whitespace\n\n\n[\\D\\S]\nMeans not digit or whitespace, both match\n\n\n[^\\d\\s]\nDisallow digit and whitespace\n\n\n\n\n\nOccurrences\n\n\n\n\n\n\n\nPattern\nMatches\n\n\n\n\ncolou?r\nMatch color or colour\n\n\n[BW]ill[ieamy's]*\nMatch Bill, Willy, William’s etc.\n\n\n[a-zA-Z]+\nMatch 1 or more letters\n\n\n\\d{3}-\\d{2}-\\d{4}\nMatch a SSN\n\n\n[a-z]\\w{1,7}\nMatch a UW NetID\n\n\n\n\n\nGreedy versus lazy\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n*  + {n,}greedy\nMatch as much as possible\n\n\n&lt;.+&gt;\nFinds 1 big match in &lt;b&gt;bold&lt;/b&gt;\n\n\n*?  +? {n,}?lazy\nMatch as little as possible\n\n\n&lt;.+?&gt;\nFinds 2 matches in &lt;b&gt;bold&lt;/b&gt;\n\n\n\n\n\nScope\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n\\b\n“Word” edge (next to non “word” character)\n\n\n\\bring\nWord starts with “ring”, ex ringtone\n\n\nring\\b\nWord ends with “ring”, ex spring\n\n\n\\b9\\b\nMatch single digit 9, not 19, 91, 99, etc..\n\n\n\\b[a-zA-Z]{6}\\b\nMatch 6-letter words\n\n\n\\B\nNot word edge\n\n\n\\Bring\\B\nMatch springs and wringer\n\n\n^\\d*$\nEntire string must be digits\n\n\n^[a-zA-Z]{4,20}$\nString must have 4-20 letters\n\n\n^[A-Z]\nString must begin with capital letter\n\n\n[\\.!?\"')]$\nString must end with terminal puncutation\n\n\n\n\n\nModifiers\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n(?i)[a-z]*(?-i)\nIgnore case ON / OFF\n\n\n(?s).*(?-s)\nMatch multiple lines (causes . to match newline)\n\n\n(?m)^.*;`(?-m)`  | &lt;yel&gt;^&lt;/yel&gt; & &lt;yel&gt; match lines not whole string\n\n\n\n(?x)\n#free-spacing mode, this EOL comment ignored\n\n\n(?-x)\nfree-spacing mode OFF\n\n\n/regex/ismx\nModify mode for entire string\n\n\n\n\n\nGroups\n\n\n\nPattern\nMeaning\n\n\n\n\n(in\\|out)put\nMatch input or output\n\n\n\\d{5}(-\\d{4})?\nUS zip code (“+ 4” optional)\n\n\n\nParser tries EACH alternative if match fails after group.  Can lead to catastrophic backtracking.\n\n\nBack references\n\n\n\n\n\n\n\nPattern\nMatches\n\n\n\n\n(to) (be) or not \\1 \\2\nMatch to be or not to be\n\n\n([^\\s])\\1{2}\nMatch non-space, then same twice more   aaa, …\n\n\n\\b(\\w+)\\s+\\1\\b\nMatch doubled words\n\n\n\n\n\nNon-capturing group\n\n\n\nPattern\nMeaning\n\n\n\n\non(?:click\\|load)\nFaster than: on(click\\|load)\n\n\n\nUse non-capturing or atomic groups when possible\n\n\nAtomic groups\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n(?&gt;red\\|green\\|blue)\nFaster than non-capturing\n\n\n(?&gt;id\\|identity)\\b\nMatch id, but not identity\n\n\n\n“id” matches, but \\b fails after atomic group, parser doesn’t backtrack into group to retry ‘identity’   If alternatives overlap, order longer to shorter.\n\n\nLookaround\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n(?= )\nLookahead, if you can find ahead\n\n\n(?! )\nLookahead,if you can not find ahead\n\n\n(?&lt;= )\nLookbehind, if you can find behind\n\n\n(?&lt;! )\nLookbehind, if you can NOT find behind\n\n\n\\b\\w+?(?=ing\\b)\nMatch warbling, string, fishing, …\n\n\n\\b(?!\\w+ing\\b)\\w+\\b\nWords NOT ending in “ing”\n\n\n(?&lt;=\\bpre).*?\\b\nMatch pretend, present, prefix, …\n\n\n\\b\\w{3}(?&lt;!pre)\\w*?\\b\nWords NOT starting with “pre”\n\n\n\\b\\w+(?&lt;!ing)\\b\nMatch words NOT ending in “ing”\n\n\n\n\n\nIf-then-else\nMatch “Mr.” or “Ms.” if word “her” is later in string\nM(?(?=.*?\\bher\\b)s|r)\\.\nrequires lookaround for IF condition"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-python",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-python",
    "title": "RegEX",
    "section": "RegEx in Python",
    "text": "RegEx in Python\n\nGetting started\nImport the regular expressions module\nimport re\n\n\nExamples\n\nre.search()\n&gt;&gt;&gt; sentence = 'This is a sample string'\n&gt;&gt;&gt; bool(re.search(r'this', sentence, flags=re.I))\nTrue\n&gt;&gt;&gt; bool(re.search(r'xyz', sentence))\nFalse\n\n\nre.findall()\n&gt;&gt;&gt; re.findall(r'\\bs?pare?\\b', 'par spar apparent spare part pare')\n['par', 'spar', 'spare', 'pare']\n&gt;&gt;&gt; re.findall(r'\\b0*[1-9]\\d{2,}\\b', '0501 035 154 12 26 98234')\n['0501', '154', '98234']\n\n\nre.finditer()\n&gt;&gt;&gt; m_iter = re.finditer(r'[0-9]+', '45 349 651 593 4 204')\n&gt;&gt;&gt; [m[0] for m in m_iter if int(m[0]) &lt; 350]\n['45', '349', '4', '204']\n\n\nre.split()\n&gt;&gt;&gt; re.split(r'\\d+', 'Sample123string42with777numbers')\n['Sample', 'string', 'with', 'numbers']\n\n\nre.sub()\n&gt;&gt;&gt; ip_lines = \"catapults\\nconcatenate\\ncat\"\n&gt;&gt;&gt; print(re.sub(r'^', r'* ', ip_lines, flags=re.M))\n* catapults\n* concatenate\n* cat\n\n\nre.compile()\n&gt;&gt;&gt; pet = re.compile(r'dog')\n&gt;&gt;&gt; type(pet)\n&lt;class '_sre.SRE_Pattern'&gt;\n&gt;&gt;&gt; bool(pet.search('They bought a dog'))\nTrue\n&gt;&gt;&gt; bool(pet.search('A cat crossed their path'))\nFalse\n\n\n\nFunctions\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nre.findall\nReturns a list containing all matches\n\n\nre.finditer\nReturn an iterable of match objects (one for each match)\n\n\nre.search\nReturns a Match object if there is a match anywhere in the string\n\n\nre.split\nReturns a list where the string has been split at each match\n\n\nre.sub\nReplaces one or many matches with a string\n\n\nre.compile\nCompile a regular expression pattern for later use\n\n\nre.escape\nReturn string with all non-alphanumerics backslashed\n\n\n\n\n\nFlags\n\n\n\n\n\n\n\n\n-\n-\n-\n\n\n\n\nre.I\nre.IGNORECASE\nIgnore case\n\n\nre.M\nre.MULTILINE\nMultiline\n\n\nre.L\nre.LOCALE\nMake \\w,\\b,\\s locale dependent\n\n\nre.S\nre.DOTALL\nDot matches all (including newline)\n\n\nre.U\nre.UNICODE\nMake \\w,\\b,\\d,\\s unicode dependent\n\n\nre.X\nre.VERBOSE\nReadable style"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-javascript",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-javascript",
    "title": "RegEX",
    "section": "Regex in JavaScript",
    "text": "Regex in JavaScript\n\ntest()\nlet textA = 'I like APPles very much';\nlet textB = 'I like APPles';\nlet regex = /apples$/i\n \n// Output: false\nconsole.log(regex.test(textA));\n \n// Output: true\nconsole.log(regex.test(textB));\n\n\nsearch()\nlet text = 'I like APPles very much';\nlet regexA = /apples/;\nlet regexB = /apples/i;\n \n// Output: -1\nconsole.log(text.search(regexA));\n \n// Output: 7\nconsole.log(text.search(regexB));\n\n\nexec()\nlet text = 'Do you like apples?';\nlet regex= /apples/;\n \n// Output: apples\nconsole.log(regex.exec(text)[0]);\n \n// Output: Do you like apples?\nconsole.log(regex.exec(text).input);\n\n\nmatch()\nlet text = 'Here are apples and apPleS';\nlet regex = /apples/gi;\n \n// Output: [ \"apples\", \"apPleS\" ]\nconsole.log(text.match(regex));\n\n\nsplit()\nlet text = 'This 593 string will be brok294en at places where d1gits are.';\nlet regex = /\\d+/g\n \n// Output: [ \"This \", \" string will be brok\", \"en at places where d\", \"gits are.\" ] \nconsole.log(text.split(regex))\n\n\nmatchAll()\nlet regex = /t(e)(st(\\d?))/g;\nlet text = 'test1test2';\nlet array = [...text.matchAll(regex)];\n\n// Output: [\"test1\", \"e\", \"st1\", \"1\"]\nconsole.log(array[0]);\n\n// Output: [\"test2\", \"e\", \"st2\", \"2\"]\nconsole.log(array[1]);\n\n\nreplace()\nlet text = 'Do you like aPPles?';\nlet regex = /apples/i\n \n// Output: Do you like mangoes?\nlet result = text.replace(regex, 'mangoes');\nconsole.log(result);\n\n\nreplaceAll()\nlet regex = /apples/gi;\nlet text = 'Here are apples and apPleS';\n\n// Output: Here are mangoes and mangoes\nlet result = text.replaceAll(regex, \"mangoes\");\nconsole.log(result);"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-php",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-php",
    "title": "RegEX",
    "section": "Regex in PHP",
    "text": "Regex in PHP\n\nFunctions\n\n\n\n\n\n\n\n-\n-\n\n\n\n\npreg_match()\nPerforms a regex match\n\n\npreg_match_all()\nPerform a global regular expression match\n\n\npreg_replace_callback()\nPerform a regular expression search and replace using a callback\n\n\npreg_replace()\nPerform a regular expression search and replace\n\n\npreg_split()\nSplits a string by regex pattern\n\n\npreg_grep()\nReturns array entries that match a pattern\n\n\n\n\n\npreg_replace\n$str = \"Visit Microsoft!\";\n$regex = \"/microsoft/i\";\n\n// Output: Visit CheatSheets!\necho preg_replace($regex, \"CheatSheets\", $str); \n\n\npreg_match\n$str = \"Visit CheatSheets\";\n$regex = \"#cheatsheets#i\";\n\n// Output: 1\necho preg_match($regex, $str);\n\n\npreg_matchall\n$regex = \"/[a-zA-Z]+ (\\d+)/\";\n$input_str = \"June 24, August 13, and December 30\";\nif (preg_match_all($regex, $input_str, $matches_out)) {\n\n    // Output: 2\n    echo count($matches_out);\n\n    // Output: 3\n    echo count($matches_out[0]);\n\n    // Output: Array(\"June 24\", \"August 13\", \"December 30\")\n    print_r($matches_out[0]);\n\n    // Output: Array(\"24\", \"13\", \"30\")\n    print_r($matches_out[1]);\n}\n\n\npreg_grep\n$arr = [\"Jane\", \"jane\", \"Joan\", \"JANE\"];\n$regex = \"/Jane/\";\n\n// Output: Jane\necho preg_grep($regex, $arr);\n\n\npreg_split\n$str = \"Jane\\tKate\\nLucy Marion\";\n$regex = \"@\\s@\";\n\n// Output: Array(\"Jane\", \"Kate\", \"Lucy\", \"Marion\")\nprint_r(preg_split($regex, $str));"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-java",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-java",
    "title": "RegEX",
    "section": "Regex in Java",
    "text": "Regex in Java\n\nStyles\n\nFirst way\nPattern p = Pattern.compile(\".s\", Pattern.CASE_INSENSITIVE);\nMatcher m = p.matcher(\"aS\");  \nboolean s1 = m.matches();  \nSystem.out.println(s1);   // Outputs: true\n\n\nSecond way\nboolean s2 = Pattern.compile(\"[0-9]+\").matcher(\"123\").matches();  \nSystem.out.println(s2);   // Outputs: true\n\n\nThird way\nboolean s3 = Pattern.matches(\".s\", \"XXXX\");  \nSystem.out.println(s3);   // Outputs: false\n\n\n\nPattern Fields\n\n\n\n-\n-\n\n\n\n\nCANON_EQ\nCanonical equivalence\n\n\nCASE_INSENSITIVE\nCase-insensitive matching\n\n\nCOMMENTS\nPermits whitespace and comments\n\n\nDOTALL\nDotall mode\n\n\nMULTILINE\nMultiline mode\n\n\nUNICODE_CASE\nUnicode-aware case folding\n\n\nUNIX_LINES\nUnix lines mode\n\n\n\n\n\nMethods\n\nPattern\n\nPattern compile(String regex [, int flags])\nboolean matches([String regex, ] CharSequence input)\nString[] split(String regex [, int limit])\nString quote(String s)\n\n\n\nMatcher\n\nint start([int group | String name])\nint end([int group | String name])\nboolean find([int start])\nString group([int group | String name])\nMatcher reset()\n\n\n\nString\n\nboolean matches(String regex)\nString replaceAll(String regex, String replacement)\nString[] split(String regex[, int limit])\n\nThere are more methods …\n\n\n\nExamples\nReplace sentence:\nString regex = \"[A-Z\\n]{5}$\";\nString str = \"I like APP\\nLE\";\n\nPattern p = Pattern.compile(regex, Pattern.MULTILINE);\nMatcher m = p.matcher(str);\n\n// Outputs: I like Apple!\nSystem.out.println(m.replaceAll(\"pple!\"));\nArray of all matches:\nString str = \"She sells seashells by the Seashore\";\nString regex = \"\\\\w*se\\\\w*\";\n\nPattern p = Pattern.compile(regex, Pattern.CASE_INSENSITIVE);\nMatcher m = p.matcher(str);\n\nList&lt;String&gt; matches = new ArrayList&lt;&gt;();\nwhile (m.find()) {\n    matches.add(m.group());\n}\n\n// Outputs: [sells, seashells, Seashore]\nSystem.out.println(matches);"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-mysql",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-mysql",
    "title": "RegEX",
    "section": "Regex in MySQL",
    "text": "Regex in MySQL\n\nFunctions\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nREGEXP\nWhether string matches regex\n\n\nREGEXP_INSTR()\nStarting index of substring matching regex (NOTE: Only MySQL 8.0+)\n\n\nREGEXP_LIKE()\nWhether string matches regex (NOTE: Only MySQL 8.0+)\n\n\nREGEXP_REPLACE()\nReplace substrings matching regex (NOTE: Only MySQL 8.0+)\n\n\nREGEXP_SUBSTR()\nReturn substring matching regex (NOTE: Only MySQL 8.0+)\n\n\n\n\n\nREGEXP\nexpr REGEXP pat \n\nExamples\nmysql&gt; SELECT 'abc' REGEXP '^[a-d]';\n1\nmysql&gt; SELECT name FROM cities WHERE name REGEXP '^A';\nmysql&gt; SELECT name FROM cities WHERE name NOT REGEXP '^A';\nmysql&gt; SELECT name FROM cities WHERE name REGEXP 'A|B|R';\nmysql&gt; SELECT 'a' REGEXP 'A', 'a' REGEXP BINARY 'A';\n1   0\n\n\n\nREGEXP_REPLACE\nREGEXP_REPLACE(expr, pat, repl[, pos[, occurrence[, match_type]]])\n\nExamples\nmysql&gt; SELECT REGEXP_REPLACE('a b c', 'b', 'X');\na X c\nmysql&gt; SELECT REGEXP_REPLACE('abc ghi', '[a-z]+', 'X', 1, 2);\nabc X\n\n\n\nREGEXP_SUBSTR\nREGEXP_SUBSTR(expr, pat[, pos[, occurrence[, match_type]]])\n\nExamples\nmysql&gt; SELECT REGEXP_SUBSTR('abc def ghi', '[a-z]+');\nabc\nmysql&gt; SELECT REGEXP_SUBSTR('abc def ghi', '[a-z]+', 1, 3);\nghi\n\n\n\nREGEXP_LIKE\nREGEXP_LIKE(expr, pat[, match_type])\n\nExamples\nmysql&gt; SELECT regexp_like('aba', 'b+')\n1\nmysql&gt; SELECT regexp_like('aba', 'b{2}')\n0\nmysql&gt; # i: case-insensitive\nmysql&gt; SELECT regexp_like('Abba', 'ABBA', 'i');\n1\nmysql&gt; # m: multi-line\nmysql&gt; SELECT regexp_like('a\\nb\\nc', '^b$', 'm');\n1\n\n\n\nREGEXP_INSTR\nREGEXP_INSTR(expr, pat[, pos[, occurrence[, return_option[, match_type]]]])\n\nExamples\nmysql&gt; SELECT regexp_instr('aa aaa aaaa', 'a{3}');\n2\nmysql&gt; SELECT regexp_instr('abba', 'b{2}', 2);\n2\nmysql&gt; SELECT regexp_instr('abbabba', 'b{2}', 1, 2);\n5\nmysql&gt; SELECT regexp_instr('abbabba', 'b{2}', 1, 3, 1);\n7"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html",
    "href": "posts/2011/2011-08-26-R-books/index.html",
    "title": "R Books",
    "section": "",
    "text": "In this updated post I included some R books you might want to look at if you are getting started with R for data science\nFellow data scientists, do not be overwhelmed by vast multiplicities of numbers, for they are but symbols of the natural order. Embrace their uncertainty and seek to understand the patterns within it. I offer you some of the first R books I came across. I got started with R in 2011, and I decided to update it to focus on stats and ml books I’ve come across with an attempt to list my favorites with their lecture notes and video lectures where available."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#software-for-data-analysis-programming-with-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#software-for-data-analysis-programming-with-r",
    "title": "R Books",
    "section": "Software for data analysis: programming with R",
    "text": "Software for data analysis: programming with R\n\n\n\n\n\nSoftware for data analysis\n\n\n\nChambers, John M. 2008. Software for Data Analysis Programming with r. New York; London: Springer. http://www.amazon.de/Software-Data-Analysis-Programming-Statistics/dp/0387759352.\nIn (Chambers 2008) the author presents the essential guidebook for those who wish to learn how to use the R programming language for data analysis. Chambers is a renowned statistician, and he shares his expertise in the field of data analysis through this book. The book covers a wide range of topics related to data analysis, including data structures, object-oriented programming, graphics, and statistical modeling. It also offers a practical approach to understanding R programming, with an emphasis on building applications that can handle large datasets. Overall, this is a valuable resource for those who want to learn about R programming for data analysis. It is a comprehensive guide that covers all the essential aspects of data analysis and provides hands-on experience with R programming. The book is written in a clear and concise manner, making it easy to follow even for beginners."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#principles-of-statistical-data-handling",
    "href": "posts/2011/2011-08-26-R-books/index.html#principles-of-statistical-data-handling",
    "title": "R Books",
    "section": "Principles of Statistical Data Handling",
    "text": "Principles of Statistical Data Handling\n\n\n\n\n\nPrinciples of Statistical Data Handling\n\n\n\nDavidson, Fred. 1996. Principles of Statistical Data Handling. https://doi.org/10.4135/9781483348902.\nIn (Davidson 1996) the author offers a guide to the foundations of this field, including exploratory data analysis, hypothesis testing, and model building. Through careful attention and disciplined study, one can cultivate a deep understanding of the methods and techniques that underlie statistical data handling, and by following, we shall approach data with a rational and objective mindset, illuminating with our analytical skills the meaningful insights and so make more informed decisions.\nA Stoic would say “Remember, the data is not what you see, but what you make of it. So, approach it with a clear mind, free from bias and preconceptions, and seek the truth that lies hidden within.” By applying oneself to these principles with diligence and perseverance, we may yet unlock the full potential of statistical data handling, and make a valuable contribution to the world of data science."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#probability-statistics-for-engineers-and-scientists",
    "href": "posts/2011/2011-08-26-R-books/index.html#probability-statistics-for-engineers-and-scientists",
    "title": "R Books",
    "section": "Probability & Statistics for Engineers and Scientists",
    "text": "Probability & Statistics for Engineers and Scientists\n\n\n\n\n\nProbability & Statistics for Engineers and Scientists\n\n\n\nWalpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying Ye. 2007. Probability & Statistics for Engineers and Scientists. 8th ed. Upper Saddle River: Pearson Education.\nIn (Walpole et al. 2007), the autor presents a path to comprehension of probability and statistics is laid out before you.\nIt is a journey of discovery that will require patience, diligence, and a willingness to learn. The author presents the tools and techniques needed to analyze data and draw meaningful conclusions. By using R, one can unlock the secrets hidden in the data.\nFear not mistakes, for they are but stepping stones towards deeper understanding, only take care to learn from them, and use the knowledge gained to improve your understanding daily.\nWith each chapter, you will gain a greater understanding of the complex and interconnected world of probability and statistics.\nEmbrace the journey, and may the numbers guide you towards enlightenment."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#introduction-to-probability-and-statistics-using-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#introduction-to-probability-and-statistics-using-r",
    "title": "R Books",
    "section": "Introduction to Probability and Statistics Using R",
    "text": "Introduction to Probability and Statistics Using R\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\n\n\n\nKerns, G. Jay. 2018. Introduction to Probability and Statistics Using r.\nIn (Kerns 2018), recommended by my old friend Adam Hyland, the author covers the basic concepts of probability and statistics using the R programming language.\nIt is a useful resource for data scientists who wish to gain a deeper understanding of probability and statistics and how to apply them.\nStarting with basic probability, distributions, hypothesis testing, regression analysis, it then proceeds to more advanced topics such as Bayesian statistics, machine learning, and time series analysis.\nEach chapter presents clear explanations, examples, and R code to help the reader grasp the theoretical concepts and apply them in practice. By including a wide range of real-world examples and datasets, it helps the readers conect the concepts and techniques with thier application to real data.\nA complimentry copy is available at this link"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#statistical-computing-with-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#statistical-computing-with-r",
    "title": "R Books",
    "section": "Statistical Computing with R",
    "text": "Statistical Computing with R\n\n\n\n\n\nStatistical Computing with R\n\n\n\nRizzo, Maria L. 2019. Statistical Computing with r Maria l. Rizzo. Second edition. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press, Taylor & Francis Group.\nIn (Rizzo 2019) the author presents is a comprehensive guide to the analysis and manipulation of data using R. Within, we are introduced to a wide variety of statistical concepts and tools that enable us to explore and understand complex datasets.\nStandard statistical techniques used in data analysis, such as probability, hypothesis testing are covered. We learn about the normal distribution and its importance in statistical analysis, as well as the Poisson distribution, which is used to model counts of events.\nThe book introduces us to the use of statistical transformations, such as the log transformation, which is often used to make skewed data more normal. We also learn about density estimation and the use of histograms and kernel density estimates to visualize data.\nThe concepts of sampling and random variables are explored, as well as the calculation of sample means and standard errors. We also learn about the use of random samples from Monte Carlo simulation to approximate probabilities and calculate statistics.\nThe book covers the use of algorithms and samplers, such as the Metropolis-Hastings algorithm, to explore parameter space and to generate samples from distributions of interest. We learn about the importance of convergence and the use of proposals in Monte Carlo sampling.\nThe concepts of bias and variance are explored, as well as the calculation of confidence intervals and the use of hypothesis testing to evaluate statistical significance. We also learn about the use of the bootstrap and jackknife methods to find the level of uncertainty in our estimates.\nThroughout the book, we are introduced to the use of R for statistical computing. We learn about the use of formulas to specify statistical models, as well as the use of packages for data manipulation and visualization.\nOverall, “Statistical Computing with R” is an essential resource for anyone interested in using statistical methods to analyze data. It provides a lucid and comprehensive treatment of statistical concepts and their practical implementation using R."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#bayesian-methods-of-data-analysis",
    "href": "posts/2011/2011-08-26-R-books/index.html#bayesian-methods-of-data-analysis",
    "title": "R Books",
    "section": "Bayesian methods of Data Analysis",
    "text": "Bayesian methods of Data Analysis\n\n\n\n\n\nBayesian methods of Data Analysis\n\n\n\nCarlin, B. P., and T. A. Louis. 2008. Bayesian Methods for Data Analysis. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=GTJUt8fcFx8C.\nIn (Carlin and Louis 2008) the author presents us with a framework that is grounded in the philosophy of probability theory. We learn to seek a baseline model then approach the problem at hand with a Bayesian perspective.\nThrough the use of Bayesian models, we can compute the conditional distributions of our data and evaluate the error and loss functions. We must consider convergence, the choice of priors, and how they are specified. We use Bayes’ rule to compute the posterior distribution and marginal likelihood, and we obtain point estimates and credible intervals.\nThe use of the Gibbs sampler and the Metropolis-Hastings algorithm in MCMC methods are presented as tools to obtain a sample from the posterior distribution. We use WinBUGS code and Monte Carlo simulations to produce results that are in line with the data observed.\nWe are introduced to the concept of the Bayes factor, and how it is used to compare models. We also understand how the use of the Jeffreys prior, the hyperprior, and the conjugate prior can be used to simplify our computations.\nIn Bayesian methods, we use the full conditional distributions to obtain the joint posterior distribution of our parameters. We also compute the marginal posterior distribution, which can be used to obtain a credible interval.\nWe are shown how to deal with univariate and multivariate data, and how to model the random effects and covariate effects. We also understand how to evaluate the performance of our models through histograms, percentiles, and plots.\nIn this work, we are presented with a practical and useful guide to Bayesian methods that can be applied to a variety of problems."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#information-theory-inference-and-learning-algorithms",
    "href": "posts/2011/2011-08-26-R-books/index.html#information-theory-inference-and-learning-algorithms",
    "title": "R Books",
    "section": "Information Theory, Inference and Learning Algorithms",
    "text": "Information Theory, Inference and Learning Algorithms\n\n\n\n\n\nInformation Theory, Inference and Learning Algorithms\n\n\n\nMacKay, David J. C. 2003. Information Theory, Inference, and Learning Algorithms. Copyright Cambridge University Press.\nInformation Theory, Inference and Learning Algorithms (MacKay 2003) by David J.C. MacKay FRS - Course notes: Information Theory, Pattern Recognition, and Neural Networks.\nThis is not an R book as far as I recall but it is available online, together with lectures by the author. I recommend this book and videos for anyone interested bayesian data analysis. The author was a physicist, a leading bayesian and pioneer in Bayesian Neural Networks whose work is very relavant even today (2024)"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#references",
    "href": "posts/2011/2011-08-26-R-books/index.html#references",
    "title": "R Books",
    "section": "References",
    "text": "References\n\n\n\nSoftware for data analysis\nPrinciples of Statistical Data Handling\nProbability & Statistics for Engineers and Scientists\nIntroduction to Probability and Statistics Using R\nStatistical Computing with R\nBayesian methods of Data Analysis\nInformation Theory, Inference and Learning Algorithms"
  },
  {
    "objectID": "posts/2021/2021-08-18-transfer-learning-nlp/2021-08-18-transfer-learning-nlp.html",
    "href": "posts/2021/2021-08-18-transfer-learning-nlp/2021-08-18-transfer-learning-nlp.html",
    "title": "Transfer learning in NLP",
    "section": "",
    "text": "Transfer learning in NLP\n\n\nTransfer learning is the ability to reuse knowledge learned in one domain to another.\n\nConvergence\nIf we got a LEC (large enough corpus) say we got all the english transcription of the data collected by the NSA for a year. That should give a fairly comprehensive view the language.\nSupposing we have a smaller data sets could we use statistical methods to predict how well the semantics or grammar etc in that corpus converge to the LEC.\nI call this notion the convergence to an ideal language models. And I recon that these datasets will converge fairly well as most language speakers should be able to understand them almost perfectly.\n\n\nDivergence\nOn the other hand there is also a second related notion of divergence. This is something we notice in NLP when we use a dataset from a different domain for an unrelated task and it fails abysmally.\nThis divergence is due to there being difference in how the language is being used even if it is still easy to understand. For most people spoken language AKA Sassure’s ‘Parole’ is a much smaller subset of the vocabulary and other construct they can understand collectively which might understand AKA ‘Langue’. and while the two may be convergent they have a quantifiable divergence. Quantifiable in the sense that the probabilities or perplexities of a sentence form a different corpus would be somewhat different for a word model approximating each corpus.\n\n\nA Transfer Function\nSuppose we had a good langue models for different substrates of a language than one would suppose it should be possible to generate a function to translate from one to the the other. This function would take an utterance from one, capture its meaning and find the most likely form it should take under the second model.\nIn deep learning one might create such a function by\n\npre-training on one corpus and then freezing most of the layers and training on the second corpus.\ncreating a encoder decoder pair for each and using one to model to encode and the other decode.\n\nThe idea of there being a convergent\n\nHow can we qualify and quantify the difference between different datasets/language models drawn from a newspaper/encyclopedic/technical papers/novels/tweets/forums/transcribed radio shows/ conversational language.\n\ndifferent words\nword probabilities\nsimilar words (replaceable words)\nrelated words (word affinity)\nsemantics\n\ntop word sense per lexeme\nword sense distribution per lexeme\n\nn-gram probabilities.\ndifference in the grammar.\n\nmean length of sentence.\nmean complexity of vocabulary.\nco-reference frequency.\nuse of pronouns and wh-words.\nother constructs\nquestions\nuse of affect, sarcasm\n\netc\n\nSuppose we had could characterize all these moderately well using a number of covariance matrices and distributions or even HMM.\nCould we use a KL distribution to simulate one from the other.\nCould we build a transformer to translate one into the other.\nHow would we test/evaluate these.\n\nTest case:\nHebrew/English movie subtitles\n\n\n\nTransfer learning in NLP\n\n\n\nCitationBibTeX citation:@online{bochman2021,\n  author = {Bochman, Oren},\n  title = {Transfer Learning in {NLP}},\n  date = {2021-08-13},\n  url = {https://orenbochman.github.io/blog//posts/2021/2021-08-18-transfer-learning-nlp/2021-08-18-transfer-learning-nlp.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2021. “Transfer Learning in NLP.” August 13,\n2021. https://orenbochman.github.io/blog//posts/2021/2021-08-18-transfer-learning-nlp/2021-08-18-transfer-learning-nlp.html."
  },
  {
    "objectID": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html",
    "href": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html",
    "title": "Python Graphs",
    "section": "",
    "text": "Some tricks collected from the SO and the web"
  },
  {
    "objectID": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#how-to-use-an-image-as-a-background",
    "href": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#how-to-use-an-image-as-a-background",
    "title": "Python Graphs",
    "section": "How to use an image as a background",
    "text": "How to use an image as a background\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(0)\nx = np.random.uniform(0.0,10.0,15)\ny = np.random.uniform(0.0,10.0,15)\ndatafile = 'lena.jpg'\nimg = plt.imread(datafile)\nplt.scatter(x,y,zorder=1)\nplt.imshow(img, zorder=0, extent=[0.5, 8.0, 1.0, 7.0])\nplt.show()\ncaveat This requires you know where the image corners need to be!\nsources:\n\nPlot over an image background in python\nAdding a background image to a plot with known corner coordinates"
  },
  {
    "objectID": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#creating-a-pictogram",
    "href": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#creating-a-pictogram",
    "title": "Python Graphs",
    "section": "Creating a pictogram",
    "text": "Creating a pictogram\nimport matplotlib.pyplot as plt\nfrom pywaffle import Waffle\ndata = {'Democratic': 48, 'Republican': 46, 'Libertarian': 3}\nfig = plt.figure(\n    FigureClass=Waffle, \n    rows=5, \n    values=data, \n    colors=[\"#232066\", \"#983D3D\", \"#DCB732\"],\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)},\n    icons='child', \n    font_size=12, \n    icon_legend=True\n)\nplt.show()\nnote:\nPyWaffle supports Font Awesome icons in the chart. plotly is an alternative. Sources: - How to make a pictogram / icon chart?"
  },
  {
    "objectID": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#adding-images-to-the-axes-of-a-plot",
    "href": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#adding-images-to-the-axes-of-a-plot",
    "title": "Python Graphs",
    "section": "Adding images to the axes of a plot",
    "text": "Adding images to the axes of a plot\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage,AnnotationBbox\ndef get_flag(name):\n    path = \"data/flags/Flags/flags/flags/24/{}.png\".format(name.title())\n    im = plt.imread(path)\n    return im\ndef offset_image(coord, name, ax):\n    img = get_flag(name)\n    im = OffsetImage(img, zoom=0.72)\n    im.image.axes = ax\n    ab = AnnotationBbox(im, (coord, 0),  xybox=(0., -16.), frameon=False,\n                        xycoords='data',  boxcoords=\"offset points\", pad=0)\n    ax.add_artist(ab)\ncountries = [\"Norway\", \"Spain\", \"Germany\", \"Canada\", \"China\"]\nvaluesA = [20, 15, 30, 5, 26]\nfig, ax = plt.subplots()\nax.bar(range(len(countries)), valuesA, width=0.5,align=\"center\")\nax.set_xticks(range(len(countries)))\nax.set_xticklabels(countries)\nax.tick_params(axis='x', which='major', pad=26)\nfor i, c in enumerate(countries):\n    offset_image(i, c, ax)\nplt.show()\n\nAdd image annotations to bar plots"
  },
  {
    "objectID": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#plot-images-at-the-end-of-a-bar-chart",
    "href": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#plot-images-at-the-end-of-a-bar-chart",
    "title": "Python Graphs",
    "section": "Plot images at the end of a bar chart",
    "text": "Plot images at the end of a bar chart\nimport matplotlib.pyplot as plt\nfrom iso3166 import countries\nimport matplotlib.image as mpimg\n\ndef pos_image(x, y, pays, haut):\n    pays = countries.get(pays).alpha2.lower()\n    fichier = \"/usr/share/iso-flags-png-320x240\"\n    fichier += f\"/{pays}.png\"\n    im = mpimg.imread(fichier)\n    ratio = 4 / 3\n    w = ratio * haut\n    ax.imshow(im,\n              extent=(x - w, x, y, y + haut),\n              zorder=2)\n\nplt.style.use('seaborn')\nfig, ax = plt.subplots()\nliste_pays = [('France', 10), ('USA', 9), ('Spain', 5), ('Italy', 5)]\nX = [p[1] for p in liste_pays]\nY = [p[0] for p in liste_pays]\nhaut = .8\nr = ax.barh(y=Y, width=X, height=haut, zorder=1)\ny_bar = [rectangle.get_y() for rectangle in r]\nfor pays, y in zip(liste_pays, y_bar):\n    pos_image(pays[1], y, pays[0], haut)\nplt.show()\nand a second solution which put the flag in the axes if the bar is too short.\nimport numpy as np\nimport requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\ndef offset_image(x, y, label, bar_is_too_short, ax):\n    response = requests.get(f'https://www.countryflags.io/{label}/flat/64.png')\n    img = plt.imread(BytesIO(response.content))\n    im = OffsetImage(img, zoom=0.65)\n    im.image.axes = ax\n    x_offset = -25\n    if bar_is_too_short:\n        x = 0\n    ab = AnnotationBbox(im, (x, y), xybox=(x_offset, 0), frameon=False,\n                        xycoords='data', boxcoords=\"offset points\", pad=0)\n    ax.add_artist(ab)\nlabels = ['CW', 'CV', 'GW', 'SX', 'DO']\ncolors = ['crimson', 'dodgerblue', 'teal', 'limegreen', 'gold']\nvalues = 2 ** np.random.randint(2, 10, len(labels))\nheight = 0.9\nplt.barh(y=labels, width=values, height=height, color=colors, align='center', alpha=0.8)\nmax_value = values.max()\nfor i, (label, value) in enumerate(zip(labels, values)):\n    offset_image(value, i, label, bar_is_too_short=value &lt; max_value / 10, ax=plt.gca())\nplt.subplots_adjust(left=0.15)\nplt.show()\n\nHow to show country flag at the end of the bar chart"
  },
  {
    "objectID": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#add-outline-arond-flagged-items-in-a-plotly-candlestick-chart",
    "href": "posts/2021/2021-09-16-python-graphs/2021-09-14-python-graphs.html#add-outline-arond-flagged-items-in-a-plotly-candlestick-chart",
    "title": "Python Graphs",
    "section": "Add outline arond flagged items in a plotly candlestick chart",
    "text": "Add outline arond flagged items in a plotly candlestick chart\nimport pandas as pd\nimport plotly.graph_objects as go\ndf = pd.DataFrame({\"data_minu\": ['30/10 09:00','30/10 09:05','30/10,09:10','30/10 09:15','30/10 09:20','30/10 09:25','30/10 09:30','30/10 09:35','30/10 09:40','30/10 09:45'],\n                   \"Open\":['10','17','23','20','8','22','24','25','29','22'],\n                   \"High\":['21','27','25','29','24','27','28','32','29','25'],\n                   \"Low\":['6','12','18','9','5','8','24','18','15','10'],\n                   \"Close\":['17','24','22','10','21','25','26','30','18','10'],\n                   \"Flag\": ['0','1','1','1','0','1','1','1','0','0']})     \nfig = go.Figure(data=[go.Candlestick(x=tickvals, #df['data_minu'],\n                open=df['Open'], high=df['High'],\n                low=df['Low'], close=df['Close'])\n                     ])  \ntickvals =[k*0.5 for k in range(len(df))]\nticktext=list(df[\"data_minu\"])\nfig.update_layout(xaxis_rangeslider_visible=False, xaxis_tickvals=tickvals, xaxis_ticktext=ticktext) \nfor k, flag in  enumerate(df['Flag']):\n    if int(flag):\n        fig.add_shape(dict(type='rect',\n                          xref='x', yref='y',\n                          layer='below', \n                          x0 = tickvals[k]-0.2, y0 = float(df.loc[k, 'Low'])-1,\n                          x1 = tickvals[k]+0.2, y1 = float(df.loc[k, 'High'])+1,\n                          fillcolor='orange', #'RoyalBlue',\n                          opacity=0.35))"
  },
  {
    "objectID": "posts/2021/2021-09-08-wave-net-review/2021-09-08-wave-net-review.html",
    "href": "posts/2021/2021-09-08-wave-net-review/2021-09-08-wave-net-review.html",
    "title": "WaveNet Review",
    "section": "",
    "text": "The WaveNet paper is a few years old. But it seems to come up in various contexts - mostly in the area of sound synthesis. It was the first paper to do waveform generation directly from a neural net instead of modeling vocoder parameters, used an innovative convolutional net technique and to top it all is an autoregressive model which was some years before probabilistic neural networks were being used. Here is some info on it and some follow up papers. (Perhaps I’ll split them up later.)\n\nWaveNet\nIs a fully probabilistic and autoregressive CNN model for the for TTS synthesis task. At this point CNN were mostly used for Image processing and RNN which are much harder to train were used for Sequence to Sequence modeling. The main WaveNet innovation is that it was a CNN that could handle contexts over lone term sequences (it needed to handle 16,000 samples per second over serval second).\nWaveNet It was based on the PixelCNN van den Oord, Aaron, Kalchbrenner, Nal, and Kavukcuoglu, Koray Pixel recurrent neural networks 2016 van den Oord, Aaron, Kalchbrenner, Nal, Vinyals, Oriol, Espeholt, Lasse, Graves, Alex, and Kavukcuoglu, Koray. Conditional image generation with PixelCNN decoders 2016 architecture. To handle long-range temporal dependencies needed for raw audio generation, the authors developed a new architectures based on dilated causal convolutions, which exhibit very large receptive fields.\nWaveNet is unique in its ability to synthesize sound directly where previous models required additional steps. Later TTS systems utilise WaveNet as a component, tweaking the joint language model with the linguistic context representations.\nTypical TTS pipelines have two parts: 1. text_analysis(text sequence) - \\implies sentence segmentation - \\implies word segmentation - \\implies Text normalization - \\implies POS tagging - \\implies grapheme to phoneme conversion - \\implies phoneme sequence + linguistic contexts. 2. Speech synthesis(output) - \\implies synthesized speech waveform. - prosody prediction - speech waveform generation\nThe two main approaches for the speech synthesis part are:\n\nnon-parametric example-based AKA concatenative speech synthesis due to Moulines & Charpentier, 1990; Sagisaka et al.,1992; Hunt & Black, 1996 which builds up the utterance from units of recorded speech.\nparametric, model-based AKA statistical parametric speech synthesis due to (Yoshimura, 2002; Zen et al., 2009). which uses a generative model to synthesize the speech. The statistical parametric approach first extracts a sequence of vocoder parameters (Dudley, 1939) o = {o_1, ... , o_N } from speech signals x = {x_1, ... , x_T } and linguistic features l from the text W, where N and T correspond to the numbers of vocoder parameter vectors and speech signals.\n\nTypically a vocoder parameter vector o_n is extracted at every 5 ms. It often includes:\n- `cepstra` [[Imai & Furuichi, 1988]() or       `line spectral pairs` [Itakura, 1975]() which represent vocal tract transfer function.\n- `fundamental frequency` $F_0$ and `aperiodicity` [Kawahara et al., 2001](), which represent characteristics of vocal source excitation signals. \nThen a set of generative models, such as a HMM Yoshimura, 2002, feed-forward neural network Zen et al., 2013; RNN Tuerk & Robinson, 1993; Karaali et al., 1997; Fan et al., 2014, is trained from the extracted vocoder parameters and linguistic features.\n\n\nResources\n\npaper WAVENET: A GENERATIVE MODEL FOR RAW AUDIO\nblogpost: wavenet-generative-model-raw-audio\n2 minute papers coverage: WaveNet by Google DeepMind | Two Minute Papers #93\nDigital Marketing Competitor Analysis: Guide to Evaluating Competition\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2021,\n  author = {Bochman, Oren},\n  title = {WaveNet {Review}},\n  date = {2021-08-29},\n  url = {https://orenbochman.github.io/blog//posts/2021/2021-09-08-wave-net-review/2021-09-08-wave-net-review.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2021. “WaveNet Review.” August 29, 2021. https://orenbochman.github.io/blog//posts/2021/2021-09-08-wave-net-review/2021-09-08-wave-net-review.html."
  },
  {
    "objectID": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html",
    "href": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html",
    "title": "Hackathon session link dumps & notes",
    "section": "",
    "text": "getting started"
  },
  {
    "objectID": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html#some-tutorials",
    "href": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html#some-tutorials",
    "title": "Hackathon session link dumps & notes",
    "section": "Some Tutorials",
    "text": "Some Tutorials\n\n“Where can I run this? An introduction to Wikimedia Cloud Services” by andrewbogott & bstorm\nIntro to PAWS for Python beginners - Chico Venancio\nIntro to Toolhub for tool maintainers - bd808 (video, 25min)\nUntangling Mediawiki - Daniel Kinzler (video, 20min)\nWikibase starting from scratch Luca Mauri (video, 20min)\nLua modules training - tomaomg (video, 40min)\nAn introduction to user scripts and gadgets (slides)\nOpportunities for new developers in the Wikipedia community including an overview of the technical areas and projects - Srishti Sethi (video)\nResources, tools, and recommendations in technical areas relevant to smaller wikis that are just getting started"
  },
  {
    "objectID": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html#open-refine",
    "href": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html#open-refine",
    "title": "Hackathon session link dumps & notes",
    "section": "Open Refine",
    "text": "Open Refine\nJane says:Nice use of string use to reconcile metadata but of course the weak link is when the spelling is slightly different in titles. I guess a carry-over of the same problem in Wikipedia versus Wikidata - 120 years of Olympic history: athletes and results kaggle dataset cc0 dataset (includes a scraper for https://www.sports-reference.com/) which could help with handling other data like world cup, winter games, summer games, etc."
  },
  {
    "objectID": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html#listeria",
    "href": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html#listeria",
    "title": "Hackathon session link dumps & notes",
    "section": "Listeria",
    "text": "Listeria\n\nlisteria bot\nhttps://en.wikipedia.org/wiki/Template:Wikidata_list\n[Magnus Manske made Listeria to create lists on Wikimedia projects]"
  },
  {
    "objectID": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html#paws-pywikibot",
    "href": "posts/2021/2021-08-13-hackathon-notes/2021-08-13-hackathon-notes.html#paws-pywikibot",
    "title": "Hackathon session link dumps & notes",
    "section": "Paws + Pywikibot",
    "text": "Paws + Pywikibot\n\nhttps://wikitech.wikimedia.org/wiki/PAWS/PAWS_and_Pywikibot\nhttps://public.paws.wmcloud.org/User:SRodlund_(WMF)/Using-Pywikibot-with-Paws.ipynb\nhttps://meta.wikimedia.org/wiki/Abstract_Wikipedia\ncloud vps\nORES it has its own project but it runs on the production cloud.\nhttps://public.paws.wmcloud.org/User:99of9/Covid-19.ipynb\nDeep Speech\n\n\n\n\ngetting started"
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html",
    "title": "10 Tips To Improve Your Workflow",
    "section": "",
    "text": "This was a place holder page but I do have some input here - perhaps not specifically 10 tips.\nI use to keep nots with MS OneNote. Before that I used some other software. OneNote works well on windows and after I moved to a mac I started to get more and more friction. Another of the down sides of using OneNote is how hard it is to publish anything. This got worse when it migrated to the cloud. Eventually I realized I need to move to a more flexible platform. However the move would only work if it would reduce the overall time and work required to do this work."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#setup-your-spell-checker",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#setup-your-spell-checker",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Setup your spell checker",
    "text": "Setup your spell checker\nIt takes forever to fix 100 of typos once you realize you have been typing blind for weeks."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#develop-and-use-a-markdown-cheat-sheet",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#develop-and-use-a-markdown-cheat-sheet",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Develop and use a markdown cheat sheet",
    "text": "Develop and use a markdown cheat sheet\n\ndevelop or steel one\nit can save going back and forth.\nparticularly for exotic things like matrix maths."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#put-the-preview-below",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#put-the-preview-below",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Put the preview below",
    "text": "Put the preview below\n\nPut the preview the editor and not on the side. It a little awkward at first but provides a more realistic picture of what’s happening."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#use-a-local-style-sheet.",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#use-a-local-style-sheet.",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Use a local style sheet.",
    "text": "Use a local style sheet.\n\nI work in vs code - I use a small (4 or 5 style in the post)\nThis lets me preview these in the editor and still get a decent approximation of the final output.\nThis also lets you tweak layout things so you can improve your layout iteratively."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#automate-the-boring-stuff-with-python",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#automate-the-boring-stuff-with-python",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Automate the boring stuff with Python",
    "text": "Automate the boring stuff with Python\n\nThere is a book with this name - look it up.\nI made a set of python function for processing pdf files to extract images from papers.\nI made these for various needs but soon shifted to automating some features I missed when leaving OneNote.\nDoing OCR was a big plus with OneNote. It turned out less of a challenge than I thought it would.\nI still have not fully automated extraction of non bitmap from pdf but I’m sure this too will be an option soon."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#capture-images-directly-to-a-your-image-folder.",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#capture-images-directly-to-a-your-image-folder.",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Capture images directly to a your image folder.",
    "text": "Capture images directly to a your image folder.\n\nI had to run some script to change the location on te mac for screen captures.\nI set the save as destination these in the browser.\nI set it also as the target of the scripts I use."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#do-image-insertions-wholesale-prices.",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#do-image-insertions-wholesale-prices.",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Do image insertions wholesale prices.",
    "text": "Do image insertions wholesale prices.\n\nlacking a image preview + drag and drop adding lots of images is one of the biggest pains. I do it in three steps\nscreen caps to the img folder - as mentioned above.\nselect each and add a meaningful name.\nmove to a target folder for the post.\ncopy all the files relative paths\nrun a regex to convert them to md format."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#drafts-a-todo-list",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#drafts-a-todo-list",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Drafts & a Todo List",
    "text": "Drafts & a Todo List\n\nkeep draft and a todo list around.\nit can help kep you on point and avoid missing those good moments of inspiration."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#push-changes-quickly-and-often",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#push-changes-quickly-and-often",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Push changes quickly and often",
    "text": "Push changes quickly and often\n\nthat a test first idea\nbasically you get more of a flow going when the work result cycles are shorter\nthink ‘Pommadoro technique’."
  },
  {
    "objectID": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#find-ways-to-bring-your-content-to-md-format.",
    "href": "posts/2021/2021-04-07-tips-to-improve-your-workflow/2021-04-07-tips-to-improve-your-workflow.html#find-ways-to-bring-your-content-to-md-format.",
    "title": "10 Tips To Improve Your Workflow",
    "section": "Find ways to bring your content to MD format.",
    "text": "Find ways to bring your content to MD format.\n\nMD ❤️ charts.\nMD ❤️ code\nMD ❤️ math\nJupyter notebooks are easily exported.\nEmbedding videos is a challenge."
  },
  {
    "objectID": "posts/2021/2021-07-14-type-witness-evolving-idiom/2021-07-14-type-witness-evolving-idiom.html",
    "href": "posts/2021/2021-07-14-type-witness-evolving-idiom/2021-07-14-type-witness-evolving-idiom.html",
    "title": "A type of Witness and an evolving Idiom",
    "section": "",
    "text": "Writing better code = writing more readable code.\nI code using many programming languages, and I’d like my code to be readable later down the road when it is time to maintain it. When this is not done the project will incur a technical debt.\nSo what is readable code. Writing lots of comments in the code is not the way to go. Comments within a function should only have to explain what remains unclear when looking at the code. A common example is in the handling of edge cased. Literate programming proponent Donald Knuth recommended using comment blocks to explain the intent/algorithm being employed.\nOne trade-off that I meet when doing integration rich project is a choice between cross-platform code (something that could be read by a C/C++/Java/JavaScript/Go developer) or idiomatic code which while more concise would be challenging to a newbie.\nOne such idiom which I first learned in Java is the use of anonymous function say for defining a one off event handler. UI code rich with nested anonymous object made me very uncomfortable decoding what those anonymous objects would have been in a “simpler” world. However, I noticed all this idiomatic anonymous one time objects actually hold one or two lines of handle code. That is a lot of boilerplate code which drowns out the real implementation.\nSo one take away - idiomatic code may be indecipherable to a maintainer.\nThe plot thickness when In a latter edition of Java this idiom was “expanded” when it became possible to instantiate an anonymous object from just an interface by providing implementation for all the anonymous overrides.\nAnd later yet another twist Java introduces lambdas expression reducing the boilerplate code by eliminating both the types being passed and the method name of the handler which was overridden. This shorthand makes code more concise as the compiler takes on the responsibility of sorting out the pesky details. We write less boilerplate code and perhaps create fewer bugs. But is it more or less expressive - is the learning curve steeper?\nIn Android, perhaps the final bastion of Java UI developers idiomatic coding leads to …\nFor example an Iterator loop versus a for each loop versus a for loop.\nIn Android, we often see something like:\n// view references\nprivate ListView mMessageListView;\n...\nprotected void onCreate(Bundle savedInstanceState) {\n...\nmMessageListView = findViewById(R.id.messageListView);\n...\n// Init message ListView and its adapterList&lt;FriendlyMessage&gt; friendlyMessages = new ArrayList&lt;&gt;();mMessageAdapter = new MessageAdapter(this, R.layout.item_message, friendlyMessages);mMessageListView.setAdapter(mMessageAdapter);\n\n\n\nCitationBibTeX citation:@online{bochman2021,\n  author = {Bochman, Oren},\n  title = {A Type of {Witness} and an Evolving {Idiom}},\n  date = {2021-07-14},\n  url = {https://orenbochman.github.io/blog//posts/2021/2021-07-14-type-witness-evolving-idiom/2021-07-14-type-witness-evolving-idiom.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2021. “A Type of Witness and an Evolving\nIdiom.” July 14, 2021. https://orenbochman.github.io/blog//posts/2021/2021-07-14-type-witness-evolving-idiom/2021-07-14-type-witness-evolving-idiom.html."
  },
  {
    "objectID": "posts/2021/2021-04-27-wingrad-schema/2021-04-27-wingrad-schema.html",
    "href": "posts/2021/2021-04-27-wingrad-schema/2021-04-27-wingrad-schema.html",
    "title": "Q&A and the Winograd schemas",
    "section": "",
    "text": "The classical Q&A task is fairly easy to solve. But how well does the IR system understand/model the data. The challenge to test this aspect of the system is to provide test of increasing difficulty.\nDifficulty can be due to different reasons:\n\nDue to polysemy of words representing the concepts in the query.\nDue to synonymy used for the text in the document\nDue to ambiguity. Ambiguity actually comes in several flavours, morphological (pos) and semantic (word sense) and in the real world there are frequently several ambiguities in a single sentence. Being able to resolve should allow the system to reject many irrelevant results.\nDue to inference required to reach the answer when the underlying facts are available in the text.\nAnother challenge might be due to common-sense reasoning\nAnother challenge might arise when resolving the query requires specialized domain knowledge.\n\nIR systems typically index information in a document and respond to queries by ranking the document based on some correlation between the contents of the question and the document. Most Q&A systems go one step further and learn to point at the answer within the document. In a search engine this might be the snippet or the link might be able to highlight the result in the actual. To properly evaluate the above capabilities is difficult because one need to be certain the challenge cannot be solved using a much simpler capability.\nHowever for most of these challanges it is entierly possible that the text does not have a answer to extract. In Winograd schemas the resolution of the ambiguous pronoun switches between the two variants of the sentence.\n\nthe animal didn’t cross the street because it was too tired the animal didn’t cross the street because it was too wide The city councilmen refused the demonstrators a permit because they feared violence. The city councilmen refused the demonstrators a permit because they advocated violence. The trophy doesn’t fit in the brown suitcase because it is too big/small. What is too big/small? suitcase trophy Jim comforted Kevin because he was so upset.\nWho was upset? Jim Kevin Joan made sure to thank Susan for all the help she had given. Q. Who had given the help? Joan Susan Babar wonders how he can get new clothing. Luckily, a very rich old man who has always been fond of little elephants understands right away that he is longing for a fine suit. As he likes to make people happy, he gives him his wallet. he is longing for a fine suit Answer 2.A Babar\n\n\nPaul tried to call George on the phone, but he wasn’t successful/available. Who wasn’t successful/available Paul George The man couldn’t lift his son because he was so weak/hevay. Who was weak/heavy fater son The large ball crashed right through the table because it was made of steel/styrofoam. What was made of steel/styrofoam the ball the table The egg hit the wall and it broke. What broke: the egg the wall The truck hit the wall and it broke. What broke: the truck the wall\n\n\nMary scolded/infuriated Jane because she had stolen a tennis racket. who was scolded/infuriated? mary jane\n\nHow would one go about generating winograd schema:\n\nfind/generate a sentence with one or more pronoun.\ndrop the sentence if has a co-reference count of 1. (no room for ambiguity)\ndrop sentence if pronoun can be resolved using subcategorization frames.\ndrop the sentence if the pronoun can be solved via selectional restrictions. Recall that these restrictions are preferences.\nOntological:\n\nIdentifying NE\nresolving relation between NE\n\n\nnote: if generating one may be able to ensure that:\n\nschema is a chalanage of knowledge within specific domains.\n\ncause and effect\nsocial\npsychological\nspatial\n\n\nInference\n\nThe Nile is the longest river with a length of 6,650 km starting at the Rukarara river is the most distant headwater of the Nile. The Amazon river in south america is 150 km shorter than the Nile. Both the Yangtze river and the yellow river are in china. The former is jut 100 km shy of the amazon while the latter is 5,464 km long.\n\n\nReferences\n\nhttps://cs.nyu.edu/davise/papers/WSKR2012.pdf\nhttps://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html\nhttp://commonsensereasoning.org/disambiguation.html\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2021,\n  author = {Bochman, Oren},\n  title = {Q\\&A and the {Winograd} Schemas},\n  date = {2021-04-27},\n  url = {https://orenbochman.github.io/blog//posts/2021/2021-04-27-wingrad-schema/2021-04-27-wingrad-schema.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2021. “Q&A and the Winograd Schemas.”\nApril 27, 2021. https://orenbochman.github.io/blog//posts/2021/2021-04-27-wingrad-schema/2021-04-27-wingrad-schema.html."
  },
  {
    "objectID": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html",
    "href": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html",
    "title": "Modeling Events",
    "section": "",
    "text": "One of the entities captured by named entity resolution task are events. However events have a complex structure which I would like to outline here.\nStories have structure. Perhaps the most discernible is the narrative structure. Stories consist in a number of building blocks: events, dialogue, exposition and characters the first three are the foundation of the narrative and are how the character are presented as the story unfolds.\nEvents are the atoms of a story’s narrative structure. Each bit of dialogue can be viewed as an event, with some extra structure (text, subtext). Exposition is a technique of providing information happening off screen - typically from the back story the history of the characters from before the first event. and as such should be represented as events."
  },
  {
    "objectID": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#modelling-events",
    "href": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#modelling-events",
    "title": "Modeling Events",
    "section": "Modelling events",
    "text": "Modelling events\nAn event can be modeled as a classic A.I. Frame. We could start with the 6 W:\n\n\n\n\n\nclassDiagram\n    class BasicEvent\n    BasicEvent : +int what # the action\n    BasicEvent : +int who  # subject and object\n    BasicEvent : +int when \n    BasicEvent : +int how\n    BasicEvent : +int where\n    BasicEvent : +int why\n\n\n\n\n\n\nThe parts seem self explanatory. However when it comes to who one might want to more clearly specify between subject and object. Also if the subject was passive or active. There may also be an instrument required to carry out the action."
  },
  {
    "objectID": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#sequences",
    "href": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#sequences",
    "title": "Modeling Events",
    "section": "Sequences",
    "text": "Sequences\nEvents are less interesting by themselves. Events naturally fit into cause and effect sequences. They are often packaged in sequences called scenes, which sharing a common location but represent small changes in the story. Larger scenes are sometimes called set pieces perhaps in reference to the requirement of an expensive stage set. Scenes are grouped into acts which typically end with a big change called a turning point again in allusion to a turn in the plot."
  },
  {
    "objectID": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#information-sets",
    "href": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#information-sets",
    "title": "Modeling Events",
    "section": "Information sets",
    "text": "Information sets\nThis is an term from game theory which refers to a set of players who are privy to a bit of information. In the story a big part of the emotional charge is crafted by manipulating information sets. The key members of the information set are:\n\nThe audience\nThe protagonist (main character)\nOther characters\n\nOnce an information set is established it becomes possible for the manipulation of information to become a part of the story. Characters will speculate on the missing information regarding such events, and may well act on their beliefs."
  },
  {
    "objectID": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#order-of-presentation",
    "href": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#order-of-presentation",
    "title": "Modeling Events",
    "section": "Order of presentation",
    "text": "Order of presentation\nThe order that events are portrayed is usually the order that they happened but often for dramatic effect some events may be shown out of order."
  },
  {
    "objectID": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#counterfactual-events",
    "href": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#counterfactual-events",
    "title": "Modeling Events",
    "section": "Counterfactual Events",
    "text": "Counterfactual Events\nIf we view our world as the sum of all possible worlds then counterfactuals are representatives of the possible world other than this one. Counterfactuals are what might have been. Dream sequences and thoughts can contain such sequences of events."
  },
  {
    "objectID": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#value-change",
    "href": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#value-change",
    "title": "Modeling Events",
    "section": "Value change",
    "text": "Value change\nStory value is best viewed as generalization of sentiment. For each event in the story there may be a corresponding improvement or the opposite for the lot of the subject of the action. How this correlates with the inner and outer desires can take many forms but this often drives the story."
  },
  {
    "objectID": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#references",
    "href": "posts/2021/2021-04-09-modeling-events/2021-04-09-modeling-events.html#references",
    "title": "Modeling Events",
    "section": "References",
    "text": "References\n\n“for February 13. 2020, the Daily Script is: Heavy Metal - 7/18/1980 draft by Dan Goldberg & Len Blum” retrieved on February 13. 2020.\nframe net\nthematic roles"
  },
  {
    "objectID": "posts/2021/2021-04-25-bayesian-agent/test.html",
    "href": "posts/2021/2021-04-25-bayesian-agent/test.html",
    "title": "tikz in Quarto!",
    "section": "",
    "text": "Complete pooling\n\n\n\n\n\n\n\nComplete pooling\n\n\nCitationBibTeX citation:@online{bochman,\n  author = {Bochman, Oren},\n  title = {Tikz in {Quarto!}},\n  url = {https://orenbochman.github.io/blog//posts/2021/2021-04-25-bayesian-agent/test.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. n.d. “Tikz in Quarto!” https://orenbochman.github.io/blog//posts/2021/2021-04-25-bayesian-agent/test.html."
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html",
    "title": "Automatic Summarization Task",
    "section": "",
    "text": "The material provided in the fast.ai course did not go into depth on the summarization. This is perhaps due to also covering Q&A which is rather similar. I had some ideas when working on the assignment building an abstractive summerier based on GMT2. many ideas came from my background in information retrieval. I had noticed that the issues like coverage and repetition were anathema to summarization from its inception. When I looked for more information I found the following video which together with a review paper can provide a good intro to this subject. I also found links to the papers mentioned and extracted some of their abstracts. I have to admit that looking at all the algorithms critically I found some new ideas for tackling problems, beyond what I had come up with on my own.\n\nNotes from the following lecture by Masa Nekic given at NDC Conferences.\nThe talk provides:\n\na starter ontology.\na review of algorithms.\nsome evaluation methods\nsome tools.\n\nGiven time, I may add data set and review of more modern literature since seq2seq RNN model."
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#ontological-mindmap",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#ontological-mindmap",
    "title": "Automatic Summarization Task",
    "section": "Ontological Mindmap",
    "text": "Ontological Mindmap\n\n\n\n\n\nmindmap\n  Root((Summarization&lt;br&gt;Task))\n    id1[Input Based]\n        id11(Single document) \n        id12(Multi document)\n    id2[Contextal]\n        id21[Generic]\n        id22(Domain Specific)\n        id23(Query)\n           id231{{from IR}}\n    id3[Output Based]\n        id31(Extractive)\n          id311{{Picks sentences from the text}}\n        id32(Abstractive)\n          id321{{Generates from scratch}}\n\n\n\n\n\n\nNote: the Query based approach intersects with the NLP QA task."
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#extractive-vs.-abstractive",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#extractive-vs.-abstractive",
    "title": "Automatic Summarization Task",
    "section": "Extractive vs. Abstractive",
    "text": "Extractive vs. Abstractive\nThe “Summarizing before exams” meme demonstrates the extractive approach. The “abridged classics” meme demonstrates the abstractive approach."
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#extractive-summaries-illustrated",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#extractive-summaries-illustrated",
    "title": "Automatic Summarization Task",
    "section": "Extractive Summaries Illustrated",
    "text": "Extractive Summaries Illustrated\nExtractive algorithms locate and rank the content of a document.\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martin’s series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain. The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\nSet on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs. One arc is about the Iron Throne of the Seven Kingdoms and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm’s deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night’s Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North.\n\nSummary (Extractive):\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martin's series of fantasy novels Set on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs\n\nExtractive Summaries draw text verbatim from the source.\n\nThis was the more common approach in NLP\nit is closely related to IR and Q&A task.\nTheir main challenges of this approach are: a lack balance, when some parts over represented while others under represented. a lack of cohesion, as extracted text retains dangling pronouns etc."
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#abstractive-summaries-illustrated",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#abstractive-summaries-illustrated",
    "title": "Automatic Summarization Task",
    "section": "Abstractive Summaries Illustrated",
    "text": "Abstractive Summaries Illustrated\nAbstractive algorithms add generation of the extracted content.\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and fire, George R. R. Martin’s series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain, The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\nSet on the fictional continents of Westeros and Essos, Game of Thrones has several plots and a large ensemble cast and follows several story arcs. One arc is about the Iron Throne of the Seven Kingdoms and follows a web of alliances and conflicts among the noble dynasties either vying to claim the throne or fighting for independence from it. Another focuses on the last descendant of the realm’s deposed ruling dynasty, who has been exiled and is plotting a return to the throne, while another story arc follows the Night’s Watch, a brotherhood defending the realm against the fierce peoples and legendary creatures of the North. Summary (Abstractive): Game of Thrones is a TV show based on book series A Song of Ice and Fire, written by G. R. R. Martin. All eight seasons were filmed in many beautiful countries across **three different continents**. Game of Thrones has a very complex story with several plots and story arcs — from conflicts between Westeros  nobility to claim the Iron Throne and rule over Seven Kingdoms to fight between brotherhood called Night's watch and enemies from the North.\n\n\nAbstractive Summaries are not constrained to using text drawn the source. They can draw on common-sense and domain knowledge external to the document.\nThis is the more challenging approach in NLP\nTheir main issues are:\n\ngood coverage.\navoiding repetition.\ncan provide better compression."
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#positional-method",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#positional-method",
    "title": "Automatic Summarization Task",
    "section": "Positional method",
    "text": "Positional method\n\nIntroduced in (Baxendale 1958)\n200 paragraphs\nFirst and last sentence of a paragraph are topic sentences (85% vs 7%)\n\n\nBaxendale, P. B. 1958. “Machine-Made Index for Technical Literature—an Experiment.” IBM Journal of Research and Development 2 (4): 354–61. https://doi.org/10.1147/rd.24.0354.\ne.g.\n\nGame of Thrones is an American fantasy drama television series created by David Benioff and D. B. Weiss for HBO. It is an adaptation of A Song of Ice and Fire, George R. R. Martinis series of fantasy novels, the first of which is A Game of Thrones. The show was both produced and filmed in Belfast and elsewhere in the United Kingdom. Filming locations also included Canada, Croatia, Iceland, Malta, Morocco, and Spain. The series premiered on HBO in the United States on April 17, 2011, and concluded on May 19, 2019, with 73 episodes broadcast over eight seasons.\n\n\n\n\n\ns8-luhn-method-1958"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#luhns-method",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#luhns-method",
    "title": "Automatic Summarization Task",
    "section": "Luhn’s method",
    "text": "Luhn’s method\n\nIntroduced in (Luhn 1958)\nFrequency of content terms\nData pre-processing\n\nStop words removal\nStemming (cats cat)\n\n\n\nLuhn, H. P. 1958. “The Automatic Creation of Literature Abstracts.” IBM Journal of Research and Development 2 (2): 159–65. https://doi.org/10.1147/rd.22.0159.\n\n\n\ns9-luhn-method-formula\n\n\nSelect sentences with highest concentrations of salient content terms\n Score = \\frac{\\text{Salient Words}^2}{  \\text{Terms in chunk} }\n\n\n\n\ns10-edmundson-method"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#edmundsons-method",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#edmundsons-method",
    "title": "Automatic Summarization Task",
    "section": "Edmundson’s method",
    "text": "Edmundson’s method\nIntroduced in (Edmundson 1969)\n\nEdmundson, H. P. 1969. “New Methods in Automatic Extracting.” Journal of theACM 16 (2): 264–85. https://courses.ischool.berkeley.edu/i256/f06/papers/edmonson69.pdf.\n\nPosition (P)\nWord frequency (F)\nCue words (C)\nBonus words — pointing to the important sentence\nStigma words — negative effect on the sentence importance\nNull words — neutral or irrelevant to the importance of the sentence\nDocument structure (S)\n\nLinear combination of these 4 features:\n score = \\alpha_1 P + \\alpha_2 F + \\alpha_3 C + \\alpha_4 S \n\nThis paper describes new methods of automatically extracting documents for screening purposes, i.e. the computer selection of sentences having the greatest potential for conveying to the reader the substance of the document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: pragmatic words (cue words); title and heading words; and structural indicators (sentence location). The research has resulted in an operating system and a research methodology. The extracting system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed components dominate the frequency component in the production of better extracts."
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#frump---fast-reading-understanding-and-memory-program",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#frump---fast-reading-understanding-and-memory-program",
    "title": "Automatic Summarization Task",
    "section": "FRUMP - Fast Reading Understanding and Memory Program",
    "text": "FRUMP - Fast Reading Understanding and Memory Program\n\n\n\ns12-FRUMP-demo\n\n\n\nIntroduced in (DeJong 1979)\nknowledge-based summarization system.\nTemplate filling approach based on UPI news stories.\nFirst abstractive method.\n50 sketchy scripts\n\nContain important events that are expected to occur in a specific situation\nSummarizer looks for instances of salient events, filling in as many as possible.\n\nIssues - 50 scripts were not enough.\n\n\nDeJong, Gerald. 1979. “Prediction and Substantiation: A New Approach to Natural Language Processing.” Cogn. Sci. 3: 251–73. https://api.semanticscholar.org/CorpusID:28841837.\n\nThis paper describes a new approach to natural language processing which results in a very robust and efficient system. The approach taken is to integrate the parser with the rest of the system. This enables the parser to benefit from predictions that the rest of the system makes in the course of its processing. These predictions can be invaluable as guides to the parser in such difficult problem areas as resolving referents and selecting meanings of ambiguous words. A program, called FRUMP for Fast Reading Understanding and Memory Program, employs this approach to parsing. FRUMP skims articles rather than reading them for detail. The program works on the relatively unconstrained domain of news articles. It routinely understands stories it has never before seen. The program’s success is largely due to its radically different approach to parsing.\n\n\nMy insights:\nThis approach has two interesting ideas.\n\nKR using templates or frames.\nKR using scripts is even more powerful method.\n\n\nA modern take on this might involve using a classifier to identify sentences as\n\nFacts\n\ngeneral knowledge (simple)\ndomain knowledge (complex or technical)\n\nOpinions\n\ngeneral knowledge (similar to many documents)\ndomain expert. (similar to a few)\n\nEvents (narrative structure)\nDeductive (logic, inference, statistical, syllogism)\nOthers\n\nUsing a generative approach would allow a deep model to generate its own KR features and templates. An adversarial approach might split this into two nets one to generate and another to test.\nAnalyzing existing summaries and clustering them might allow one to begin summarize using a preferred template rather than starting from scratch. Clustering, deleting and generalizing from existing summaries may be a means for improving abstractive work.\nPutting a focus on the added value of\n\nout of document facts and vocabulary\nhow humans/abstractive summaries differ from extractive ones."
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#naive-bayes-classification",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#naive-bayes-classification",
    "title": "Automatic Summarization Task",
    "section": "Naive Bayes Classification",
    "text": "Naive Bayes Classification\n\nIntroduced in (Kupiec, Pedersen, and Chen 1995)\nFirst trainable method\nTraining set: original documents and manually created extracts\nUsed Naive Bayes classifier:\n\n\nKupiec, Julian, Jan O. Pedersen, and Francine R. Chen. 1995. “A Trainable Document Summarizer.” In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. https://courses.ischool.berkeley.edu/i256/f06/papers/kupiec95.pdf.\n P (s \\in S \\vert F_1 ... F_k) = \\frac{P (F_1 ... F_k \\vert s \\in S ) P(s \\in S )} {P (F_1 ... F_k)}  \n\nBy assuming statistical independence of the features it reduces to:\n\n  P (s \\in S \\vert F_1 ... F_k)  = \\frac{ \\displaystyle \\prod_{j \\in J} P (F_j \\vert s \\in S ) P(s \\in S )} { \\displaystyle \\prod_{j \\in J} P (F_i)} \n\nPerformance:\n\nFor 25% extracts - 84% precision\nFor smaller summaries - 74% improvement over lead summaries"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#maximum-entropy-classification",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#maximum-entropy-classification",
    "title": "Automatic Summarization Task",
    "section": "Maximum Entropy Classification",
    "text": "Maximum Entropy Classification\n\nIntroduced in (Osborne 2002)\n\nMaximum entropy models are performing better than Naive Bayes approach\n\n\nOsborne, Miles. 2002. “Using Maximum Entropy for Sentence Extraction.” In Annual Meeting of the Association for Computational Linguistics. https://aclanthology.org/W02-0401.pdf."
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#mmr",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#mmr",
    "title": "Automatic Summarization Task",
    "section": "MMR",
    "text": "MMR\n\nIntroduced in (Carbonell and Goldstein-Stewart 1998)\nMaximal Marginal Relevance\nQuery based summaries.\n\n\nCarbonell, Jaime G., and Jade Goldstein-Stewart. 1998. “The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries.” In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf.\n\n\\text{MMR} = \\arg \\max[\\lambda Sim_1(s_i,Q)-(1-\\lambda) \\max Sim_2(s_i, s_j)]\n\nWhere:\n\nQ - user query\nR - ranked list of sentences\nS - already retrieved sentences\nSim - similarity metrics\n\\lambda - hyper-parameter controlling importance of query or other sentence.\n\n\nThis paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting appropriate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization… the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection. - The Use of MMR (abstract)\n\n\n\nMy insights:\n\nMMR seems to have a binomial formulation.\nBy avoiding to pin down the metric it is possible to use embedding similarity with this formulation.\nMMR offers a formal metric for measuring added value (utility) For Sentences in a summary.\nIt can work with or without a query.\nIt could be adapted as a regularization term in a summarizer loss function.\nIt could be used on a summary to weigh each sentence’s utility.\nIf one were able to generate multiple candidates for a factum MMR could be used to easily rank them.\n\n\n\n\ns16-Mead-Centroid"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#mead",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#mead",
    "title": "Automatic Summarization Task",
    "section": "Mead",
    "text": "Mead\n\nIntroduced in (Radev, Jing, and Budzikowska 2000)\nCentroid-based method\nSingle and multi document\n\n\nRadev, Dragomir R., Hongyan Jing, and Malgorzata Budzikowska. 2000. “Centroid-Based Summarization of Multiple Documents: Sentence Extraction, Utility-Based Evaluation, and User Studies.” ArXiv cs.CL/0005020. https://arxiv.org/pdf/cs/0005020.pdf.\n\nWe present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization. - Centroid-based summarization of multiple documents (abstract)\n\n\nMy insights:\nClustering has its benefits:\n\nEach centroid corresponds a candidate topic.\nCluster size establishes a natural hierarchy for ranking topics.\nCluster centrality provides the a hierarchy for ranking sentence within topics.\nThe centroids may be used in a generative context, to bootstrap attention to each topic !?\nA query similarity can used with the centroids to rank in response to a query (for Q&A)\n\n\n\n\n\nLexrank"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#lexrank",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#lexrank",
    "title": "Automatic Summarization Task",
    "section": "LexRank",
    "text": "LexRank\n\nIntroduced in (Erkan and Radev 2004) 1(https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume22/erkan04a-html/erkan04a.html)\nGraph based method.\nLexical centrality.\n\n\nErkan, Günes, and Dragomir R. Radev. 2004. “LexRank: Graph-Based Lexical Centrality as Salience in Text Summarization.” ArXiv abs/1109.2128. https://api.semanticscholar.org/CorpusID:506350.\n1 page\n\n\n\nlexrank rank\n\n\n\nWe introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.\n\n\n\n\n\nlexrank graph\n\n\nIdea:\nsimilar to page rank where pages vote for each other:\n\nCreate an adjacency matrix using cosine similarity.\nRepresenting sentences as nodes in the graph\nConnecting nodes based on inter-sentence cosine similarity matrix\nuses eigenvector centrality from this matrix.\nthe sentence with the highest rank would be linked to many other important sentences. Are they very similar or not ?\na threshold is used to determine how many connected components should are used.\n\n\nmy insights:\n\nAlgorithmically lexrank is a more sophisticated way of clustering like the MEAD algorithm. According to the paper, lexrank performed better.\nGraph algorithms are computationally expensive for large graphs. This could mean that the approach would not scale.\nTo build the matrix they used a cosine similarity - but on using words. Replacing words with their embeddings should yield even better results with lower costs.\nThere are a number of centrality measures on graphs. A high eigenvector score means that a node is connected to many nodes who themselves have high scores. The paper looked at Degree, LexRank with threshold, and continuous LexRank. This is clearly a place where one may be able to do better.\nTfiDf is another way to rank concepts.\na problem is that the underlying assumptions for creating the graphical models are difficult to justify. Building a graph from web pages using links seems natural while constructing a graph using similarity between sentences perhaps in different documents seems contrived. Sentences may capture several concepts and arguments may span several sentences. Similar sentences may have very different meaning and different sentences may have the same meaning.\n\n\n\n\n\nseq2seq"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#what-makes-a-good-summary",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#what-makes-a-good-summary",
    "title": "Automatic Summarization Task",
    "section": "What makes a good summary?",
    "text": "What makes a good summary?\n\nGoals:\n\nOptimize topic coverage\nOptimize readability\n\nEvaluation criteria:\n\nSalience\nLength\nStructure and coherence\nBalance\nGrammar\nNon-redundancy\n\nTypes of evaluation methods\n\nExtrinsic techniques\n\nTask based\nCan a person make the same decision with summary as with the entire document?\n\nIntrinsic techniques\n\nComparing summaries against gold standards\n\n\n\n\n\n\nPrecision & Recall"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#precision-and-recall",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#precision-and-recall",
    "title": "Automatic Summarization Task",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nstarting with a contingency matrix we can get to:\n Precision =\\frac{True_+}{ False_+ + True_+} \n Recall = \\frac{True_+}{True_+ + False_-} \nthese can also be combined into an f-score is a harmonic mean of precision and recall.\n\nMy insights\nPrecision and Recall make more sense for IR settings, i.e. when we have a query.\n\n\n\n\ns24-utility"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#utility",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#utility",
    "title": "Automatic Summarization Task",
    "section": "Utility",
    "text": "Utility\n\nUtility is interesting from economic or game theoretic perspective. It indicates an option of applying RL\nUtility is usually translated as a loss function in ML!\n\n\n\n\n\ns25-pyramid"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#pyramid-method",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#pyramid-method",
    "title": "Automatic Summarization Task",
    "section": "Pyramid method",
    "text": "Pyramid method\n\nBased on semantic content units\nUsed for multi-document summarization\n\n\n\n\n\ns25-rougue"
  },
  {
    "objectID": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#rouge-n",
    "href": "posts/2021/2021-04-24-summerization/2021-04-24-summerization.html#rouge-n",
    "title": "Automatic Summarization Task",
    "section": "ROUGE-N",
    "text": "ROUGE-N\n\nBased on Bleu (used for MT)\nR stands for Recall (Recall-Oriented Understudy for Gisting Evaluation)\nROUGE-N metric compares an automatic summary with a set of reference summaries using the n-gram overlap between the documents\n\n\nROUGE_N - = \\frac{\\sum_{s\\in S_H} \\sum_{g_n \\in S}C_m(g_n)}\n                      {\\sum_{s\\in S_H} \\sum_{g_n \\in S}C(g_n) }\n\n\nS_H is a set of manual summaries\nS is an individual manual summary\ng_n is a N-gram\nC(g_n) is number of occurrences of gn in reference summaries\nC_m(g_n) is number of co-occurrences of g_n in both reference and automatic summary"
  },
  {
    "objectID": "posts/2021/2021-05-29-djvu-to-pdf/2021-05-29-djvu-to-pdf.html",
    "href": "posts/2021/2021-05-29-djvu-to-pdf/2021-05-29-djvu-to-pdf.html",
    "title": "Ebook Hacks",
    "section": "",
    "text": "some formulas for ebooks"
  },
  {
    "objectID": "posts/2021/2021-05-29-djvu-to-pdf/2021-05-29-djvu-to-pdf.html#other-options-for-pdfsettings",
    "href": "posts/2021/2021-05-29-djvu-to-pdf/2021-05-29-djvu-to-pdf.html#other-options-for-pdfsettings",
    "title": "Ebook Hacks",
    "section": "Other options for PDFSETTINGS:",
    "text": "Other options for PDFSETTINGS:\n\n/screen selects low-resolution output similar to the Acrobat Distiller “Screen Optimized” setting.\n/ebook selects medium-resolution output similar to the Acrobat Distiller “eBook” setting.\n/printer selects output similar to the Acrobat Distiller “Print Optimized” setting.\n/prepress selects output similar to Acrobat Distiller “Prepress Optimized” setting.\n/default selects output intended to be useful across a wide variety of uses, possibly at the expense of a larger output file."
  },
  {
    "objectID": "posts/2021/2021-05-29-djvu-to-pdf/2021-05-29-djvu-to-pdf.html#references",
    "href": "posts/2021/2021-05-29-djvu-to-pdf/2021-05-29-djvu-to-pdf.html#references",
    "title": "Ebook Hacks",
    "section": "References",
    "text": "References\n\nhigh-level Output Devices\nPs2pdf\nhttps://ss64.com/osx/sips.html\nhttps://zaiste.net/posts/command-line-resizing-images/\nhttps://github.com/readyready15728/misc/blob/master/epub-recompression.md\nhttps://superuser.com/questions/350201/convert-many-images-to-one-pdf-on-mac\nhttps://linuxatty.wordpress.com/2018/12/12/fix-for-imagemagick-convert-errors-with-pdf-files/\nhttps://www.linuxadictos.com/en/convertir-jpg-a-pdf.html\nhttps://stackoverflow.com/questions/20531079/adding-an-image-to-a-pdf-with-pdftk\nhttps://apple.stackexchange.com/questions/12709/how-can-i-convert-jpg-into-pdf-easily\nhttps://unix.stackexchange.com/questions/39464/how-to-query-pdf-page-size-from-the-command-line\n\n\n\n\nsome formulas for ebooks"
  },
  {
    "objectID": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html",
    "href": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html",
    "title": "Inlining Citations for Wikipedia articles",
    "section": "",
    "text": "Inlining citations\nOne of the problems with AI is that what is called inference is usually prediction and not the deeper logic ability we associate with aristotle’s syllogism. Modern test for general intelligence also look at coreference resolution and some other abilities that indicate that the model has learned a good representation of the text. The problem with large language models testing on small data sets is that they learn to cheat. This is a different problem from overfitting what happens is that the model learn to defeat the weakness in the dataset rather than addressing the real problem. It does not help that researchers often help it (perhaps even inadvertently).\nHere is an interesting idea find task that are indicative of general intelligence or at least of understanding of non atomic concepts like complex relations, logical inference where multiple deductions may be required to make an inference. And so on. This essentially is going to force the AI to learn multiple level of abstractions.\nSome challenges with setting up a challenge at multiple levels of abstraction is that for the lower level like free text we have lots of data but for more abstract we have orders of magnitude less. One reason why your run of the mill language model can’t do complex reasoning is that it is learning to fill certain gaps in a text but these gaps only require short and rather simple contexts. If the completions were harder it would be have to learn more complex representations and abstractions. Such a model I’d presume have the added benefit of learning to economizing on the use of simpler abstractions. Finally if we had concrete entities we are considering at different levels than we may be able to explore visualize how they are captured by the model. (Are there neurons specializing in entities or relations at certain levels of abstractions.)\nOne approach that I find incredibly attractive is to create a synthetic dataset for this. Creating an extensive syllogism and logical deduction seems of interest particularly as there are few of these in most text. Also this type of thinking is rather limited. First order logic adds to predicate calculus the universal and existential quantifiers and as such seems to deal with absolutes. But absolutes are limiting. You will find few statements of this form in text. When people make use of them they are easily refuted. On the other hand if we are compiling an ontology we easily come across a myriad of such statements but they seem to a human rather trivial - more a fact or definition rather than a high-level of abstraction one can use in a deduction.\nOn the other hand certain types of writing does seem to have more of this - Scientific writing. Perhaps am advantage is that most of it is structured in such a way that there is a section on conclusions. However even encyclopedia articles tend to exhibit more logical deduction that regular texts.\nData :"
  },
  {
    "objectID": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#word2vec",
    "href": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#word2vec",
    "title": "Inlining Citations for Wikipedia articles",
    "section": "word2vec",
    "text": "word2vec\nAn algorithm to create an embeddings for a word given its context. where we use\ncontext_left word context_right\nwhere the context is bounded by a window using a hyper parameter."
  },
  {
    "objectID": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#cite2vec",
    "href": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#cite2vec",
    "title": "Inlining Citations for Wikipedia articles",
    "section": "cite2vec",
    "text": "cite2vec\nAn algorithm to create an embeddings for citations based on its context.\nHere we would have\ncontext_left il-citation context_right\nwhich could be viewed as\ncontext_left &lt;journal, domain, authors, institutions, doi, abstract, keywords, year, page, \n\nsection, line, ...&gt; context_right\nalso we might expand using other features like reference-graph, headings, f-index, etc … &gt;"
  },
  {
    "objectID": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#multilevel-approach-to-citations",
    "href": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#multilevel-approach-to-citations",
    "title": "Inlining Citations for Wikipedia articles",
    "section": "Multilevel approach to citations",
    "text": "Multilevel approach to citations\nIt is always great to have a baseline model to check against and one could do worse than a naive bayes classifier (assuming independence of citation features) or full and partial pooling between these features using multilevel modeling.\nWhy naive bayes and not a large regression model ? It might arise that the number of features levels might exceed the number of rows. This would be highly problematic if least squares is used to optimise the model as is typically the case with regressions."
  },
  {
    "objectID": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#citealigner",
    "href": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#citealigner",
    "title": "Inlining Citations for Wikipedia articles",
    "section": "citeAligner",
    "text": "citeAligner\nonce we have embeddings we can learn to align these with the text using an attention based model using an encoder/decoder."
  },
  {
    "objectID": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#large-scale-embeddings-project",
    "href": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#large-scale-embeddings-project",
    "title": "Inlining Citations for Wikipedia articles",
    "section": "Large scale embeddings project:",
    "text": "Large scale embeddings project:\nGenerating embeddings from wikipedia and wikidata. Here items would encompass, links, named entities, categories, references to wikidata items, lists & info-boxes, wikidata items and media such as images from commons or held locally."
  },
  {
    "objectID": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#named-entity-classifier",
    "href": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#named-entity-classifier",
    "title": "Inlining Citations for Wikipedia articles",
    "section": "Named Entity classifier",
    "text": "Named Entity classifier\nThis is a second tool to automate named entity etc classification based on the embeddings."
  },
  {
    "objectID": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#dataset-generator-aka-dataminer-tool.",
    "href": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#dataset-generator-aka-dataminer-tool.",
    "title": "Inlining Citations for Wikipedia articles",
    "section": "Dataset Generator AKA DataMiner tool.",
    "text": "Dataset Generator AKA DataMiner tool.\nThis should be be a set of simple python code to extract datasets from a wiki instance, from federated wikidata projects. The generator could be a file for downstream use or a small foot print generator to be used in an directly in training models.\nIdeally it should be easy to extend by the scientific community and be put on an instance with its own cloud infrastructure (though access to spot instances might make sense)\n\nFilters mentioned above:\n\nArticles in categories.\nLinks in articles.\nNamed entities in articles.\nImages & their Metadata .\nRegex.\nWikidata parent.\nMetadata in image.\nChat pages / conversations / etc.\nPolicy pages.\nOther namespaces.\nRevision.\n\nHuman\nBots\n\nDeleted data.\nMeta data on edits (SNA or edits summaries times etc).\nPage readership stats.\nWikipedia editors\n\nBot editors\n\nExternal links\n\nGood links\nDead links\nFrom online databases\n\nWith DOI\n\n\nCode\n\nTemplates and their metadata\nLua modules\n\n\n\n\nOther algorithms and models\nBeside generating items it might be useful to provide the output of some algorithms and some pre-trained models for based on the output of the dataset or on some specific datasets. These could then be integrated in downstream project with significant saving or time cost and energy.\n\nembeddings e.g. word2vec\nranking e.g. page rank.\n\nNote tool should be coded as to not be limited to media wiki projects as we know that wikis tend to have massive knowledge gaps.\nIf the corpus tools can reconcile using external authorities, it should be possible to estimate scope of gaps and provide linked data to missing items from outside media wiki. This is a concept that might also reduce bias by increasing diversity."
  },
  {
    "objectID": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#graph-based-approaches",
    "href": "posts/2021/2021-08-13-inlining-citations/2021-08-13-inlining-citations.html#graph-based-approaches",
    "title": "Inlining Citations for Wikipedia articles",
    "section": "Graph based approaches",
    "text": "Graph based approaches\nthe graph neural network seems to have some interesting work on citation graphs that may also be able to converge to additional graphs of documents, journals, authors and ontologies from Wikipedia and academia.\nNote: another application this may also be a potential tool for investigating paper mills.\n\n\n\nInlining citations"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oren Bochman’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n10 Tips To Improve Your Workflow\n\n\nHow to blog like a life-hacker.\n\n\n\nOren Bochman\n\n\nApr 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA type of Witness and an evolving Idiom\n\n\nwriting better code = writing more readable code.\n\n\n\nOren Bochman\n\n\nJul 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nA/B testing cost and risks?\n\n\nA/B testing cost and risks and some recommendation.\n\n\n\nOren Bochman\n\n\nJul 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttention for sensor fusion\n\n\nAttention for sensor fusion\n\n\n\nOren Bochman\n\n\nSep 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nAutogluon is a auto-ml framework, here are three cheetsheet for accellerating data science workloads\n\n\n\nOren Bochman\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Summarization Task\n\n\nConcepts, slide commentaries and Lecture notes on Automatic text Summarization by Masa Nekic\n\n\n\nOren Bochman\n\n\nApr 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBash\n\n\nThis is a quick reference cheat sheet to getting started with linux bash shell scripting.\n\n\n\nOren Bochman\n\n\nSep 10, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian agents\n\n\n\n\n\n\nOren Bochman\n\n\nApr 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nD3.js in in Quarto Observable\n\n\n\n\n\n\nOren Bochman\n\n\nJan 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Intuitions\n\n\n\n\n\n\nOren Bochman\n\n\nOct 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker for data science\n\n\nPost description\n\n\n\nOren Bochman\n\n\nNov 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEbook Hacks\n\n\n\n\n\n\nOren Bochman\n\n\nMay 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Approaches to Attention-based NMT\n\n\nReview of the paper on dot product attention for the deeplearning.ai NLP specialization.\n\n\n\nOren Bochman\n\n\nMar 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcel 2019 for Marketing Statistics in pandas\n\n\nExcel 2019 for Marketing Statistics in pandas\n\n\n\nOren Bochman\n\n\nSep 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHackathon session link dumps & notes\n\n\nWikipedia Hackathon notes\n\n\n\nOren Bochman\n\n\nAug 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to avoid cross site scripting (XSS) errors with the Jupyter local runtime for Colab\n\n\n\n\n\n\nOren Bochman\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInlining Citations for Wikipedia articles\n\n\nAn algorithm for Inlining Citations for Wikipedia articles.\n\n\n\nOren Bochman\n\n\nAug 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJekyll take 3\n\n\nMy attempts to get the jekyll version of this site to also build locally.\n\n\n\nOren Bochman\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Models Are Open Knowledge Graphs\n\n\n\n\n\n\nOren Bochman\n\n\nAug 11, 2202\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage models and explainability\n\n\nLanguage models and explainability\n\n\n\nOren Bochman\n\n\nSep 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinkage 2021-04-07\n\n\n\n\n\n\nOren Bochman\n\n\nApr 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathJax 3 fix for Jekyll hosted on Github pages\n\n\nIssues and workarounds for MatchJax 3.0.\n\n\n\nOren Bochman\n\n\nApr 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeme bank\n\n\nA meme is an idea, behavior, style, or usage that spreads from person to person within a culture. A meme bank would be a zoo for cataloging and breeding memes.\n\n\n\nOren Bochman\n\n\nDec 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Events\n\n\nModeling Events.\n\n\n\nOren Bochman\n\n\nApr 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-armed bandits problem\n\n\nThe Multi-armed bandits problem\n\n\n\nOren Bochman\n\n\nMay 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Models\n\n\nDifferent Multilevel Models Types\n\n\n\nOren Bochman\n\n\nMay 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPandas Productivity Challenge?\n\n\nJust a little rant on Pandas various contexts\n\n\n\nOren Bochman\n\n\nMar 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nOren Bochman\n\n\nJan 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Graphs\n\n\nPython Graph Cookbook\n\n\n\nOren Bochman\n\n\nAug 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A and the Winograd schemas\n\n\n\n\n\n\nOren Bochman\n\n\nApr 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto 💖 Bootstrap 😁\n\n\n\n\n\n\nOren Bochman\n\n\nJan 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Books\n\n\nIn this updated post I included some R books you might want to look at if you are getting started with R for data science.\n\n\n\nOren Bochman\n\n\nAug 26, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nRegEX\n\n\nA quick reference for regular expressions (regex), including symbols, ranges, grouping, assertions and some sample patterns to get you started.\n\n\n\nOren Bochman\n\n\nNov 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSet Up M1 MacBooks for DS & ML\n\n\nSet Up M1 MacBooks for DS & ML\n\n\n\nOren Bochman\n\n\nMay 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSine function\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Gradient Descent - The good parts\n\n\nStochastic Gradient Descent - The good parts\n\n\n\nOren Bochman\n\n\nAug 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorytelling and other essentials\n\n\nStorytelling and other essentials,\n\n\n\nOren Bochman\n\n\nSep 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow probability\n\n\n\n\n\n\nOren Bochman\n\n\nJun 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With Python\n\n\n\n\n\n\nOren Bochman\n\n\nNov 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With R\n\n\n\n\n\n\nOren Bochman\n\n\nNov 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Great Migration\n\n\nsome migration notes from Blooger to Jekyl to Quarto blog.\n\n\n\nOren Bochman\n\n\nJan 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\n\n\n\n\nOren Bochman\n\n\nNov 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime management Tips\n\n\nEffective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a…\n\n\n\nOren Bochman\n\n\nAug 11, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer learning in NLP\n\n\nTransfer learning in NLP\n\n\n\nOren Bochman\n\n\nAug 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nTransformations in Linguistic Representation\n\n\n\n\n\n\nOren Bochman\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWaveNet Review\n\n\nThe WaveNet paper is kind of old. Yet it seems to come up in various contexts. Some thoughts on this.\n\n\n\nOren Bochman\n\n\nAug 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nOren Bochman\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is in a citation?\n\n\nCreating Citation Web Components\n\n\n\nOren Bochman\n\n\nAug 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWikisym 2012\n\n\n\n\n\n\nOren Bochman\n\n\nJul 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nbrace expansion\n\n\n\n\n\n\nOren Bochman\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\ncommand line\n\n\ncommand line cheea sheet macos + zsh + git\n\n\n\nOren Bochman\n\n\nMay 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nentropy for uncertainty quantification\n\n\n\n\n\n\nOren Bochman\n\n\nSep 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nhungarian cheat sheet\n\n\n\n\n\n\nOren Bochman\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy melt down\n\n\nnumpy melt down.\n\n\n\nOren Bochman\n\n\nNov 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext annotation with BRAT\n\n\n\n\n\n\nOren Bochman\n\n\nJan 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\ntikz in Quarto!\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n😁 Quarto 💖 Mermaid🧜 Mindmaps 🧠\n\n\n\n\n\n\nOren Bochman\n\n\nFeb 12, 20224\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "model-thinking.html",
    "href": "model-thinking.html",
    "title": "Oren Bochman’s Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html",
    "href": "notes/dnn/dnn-07/l_07.html",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n:::"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#getting-targets-when-modeling-sequences",
    "href": "notes/dnn/dnn-07/l_07.html#getting-targets-when-modeling-sequences",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Getting targets when modeling sequences",
    "text": "Getting targets when modeling sequences\n\nWhen applying machine learning to sequences, we often want to turn an input sequence into an output sequence that lives in a different domain.\n\nE. g. turn a sequence of sound pressures into a sequence of word identities.\n\nWhen there is no separate target sequence, we can get a teaching signal by trying to predict the next term in the input sequence.\n\nThe target output sequence is the input sequence with an advance of 1 step.\nThis seems much more natural than trying to predict one pixel in an image from the other pixels, or one patch of an image from the rest of the image.\nFor temporal sequences there is a natural order for the predictions.\n\nPredicting the next term in a sequence blurs the distinction between supervised and unsupervised learning.\n\nIt uses methods designed for supervised learning, but it doesn’t require a separate teaching signal."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#memoryless-models-for-sequences",
    "href": "notes/dnn/dnn-07/l_07.html#memoryless-models-for-sequences",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Memoryless models for sequences",
    "text": "Memoryless models for sequences\n\n\n\n\nMemoryless models\n\n\n\n\nAutoregressive models Predict the next term in a sequence from a fixed number of previous terms using “delay taps”.\nFeed-forward neural nets These generalize autoregressive models by using one or more layers of non-linear hidden units. e.g. Bengio’s first language model."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#beyond-memoryless-models",
    "href": "notes/dnn/dnn-07/l_07.html#beyond-memoryless-models",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Beyond memoryless models",
    "text": "Beyond memoryless models\n\nIf we give our generative model some hidden state, and if we give this hidden state its own internal dynamics, we get a much more interesting kind of model.\n\nIt can store information in its hidden state for a long time.\nIf the dynamics is noisy and the way it generates outputs from its hidden state is noisy, we can never know its exact hidden state.\nThe best we can do is to infer a probability distribution over the space of hidden state vectors.\n\nThis inference is only tractable for two types of hidden state model.\n\nThe next three slides are mainly intended for people who already know about these two types of hidden state model. They show how RNNs differ.\nDo not worry if you cannot follow the details."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#linear-dynamical-systems-engineers-love-them",
    "href": "notes/dnn/dnn-07/l_07.html#linear-dynamical-systems-engineers-love-them",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Linear Dynamical Systems (engineers love them!)",
    "text": "Linear Dynamical Systems (engineers love them!)\n\n\n\nlinear dynamic systems\n\n\n\nThese are generative models. They have a realvalued hidden state that cannot be observed directly.\n\nThe hidden state has linear dynamics with Gaussian noise and produces the observations using a linear model with Gaussian noise.\nThere may also be driving inputs.\n\nTo predict the next output (so that we can shoot down the missile) we need to infer the hidden state.\n\nA linearly transformed Gaussian is a Gaussian. So the distribution over the hidden."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#hidden-markov-models-computer-scientists-love-them",
    "href": "notes/dnn/dnn-07/l_07.html#hidden-markov-models-computer-scientists-love-them",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Hidden Markov Models (computer scientists love them!)",
    "text": "Hidden Markov Models (computer scientists love them!)\n\n\n\nHidden Markov Models\n\n\n\nHidden Markov Models have a discrete oneof-N hidden state. Transitions between states are stochastic and controlled by a transition matrix. The outputs produced by a state are stochastic.\n\nWe cannot be sure which state produced a given output. So the state is “hidden”.\nIt is easy to represent a probability distribution across N states with N numbers.\n\nTo predict the next output we need to infer the probability distribution over hidden states.\n\nHMMs have efficient algorithms for"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#a-fundamental-limitation-of-hmms",
    "href": "notes/dnn/dnn-07/l_07.html#a-fundamental-limitation-of-hmms",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "A fundamental limitation of HMMs",
    "text": "A fundamental limitation of HMMs\n\nConsider what happens when a hidden Markov model generates data.\n\nAt each time step it must select one of its hidden states. So with N hidden states it can only remember log(N) bits about what it generated so far.\n\nConsider the information that the first half of an utterance contains about the second half:\n\nThe syntax needs to fit (e.g. number and tense agreement).\nThe semantics needs to fit. The intonation needs to fit.\nThe accent, rate, volume, and vocal tract characteristics must all fit.\n\nAll these aspects combined could be 100 bits of information that the first half of an utterance needs to convey to the second half. 2^100"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#recurrent-neural-networks",
    "href": "notes/dnn/dnn-07/l_07.html#recurrent-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nrnns.png\n\n\n\nRNNs are very powerful, because they combine two properties:\n\nDistributed hidden state that allows them to store a lot of information about the past efficiently.\nNon-linear dynamics that allows them to update their hidden state in complicated ways.\n\nWith enough neurons and time, RNNs can compute anything that can be computed by your computer."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#do-generative-models-need-to-be-stochastic",
    "href": "notes/dnn/dnn-07/l_07.html#do-generative-models-need-to-be-stochastic",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Do generative models need to be stochastic?",
    "text": "Do generative models need to be stochastic?\n\nLinear dynamical systems and hidden Markov models are stochastic models.\n\nBut the posterior probability distribution over their hidden states given the observed data so far is a deterministic function of the data.\n\nRecurrent neural networks are deterministic.\n\nSo think of the hidden state of an RNN as the equivalent of the deterministic probability distribution over hidden states in a linear dynamical system or hidden Markov model. ## Recurrent neural networks\n\nWhat kinds of behaviour can RNNs exhibit?\n\nThey can oscillate. Good for motor control?\nThey can settle to point attractors. Good for retrieving memories?\nThey can behave chaotically. Bad for information processing?\nRNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects.\n\nBut the computational power of RNNs makes them very hard to train.\n\nFor many years we could not exploit the computational power of RNNs despite some heroic efforts (e.g. Tony Robinson’s speech recognizer)."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-equivalence-between-feedforward-nets-and-recurrent-nets",
    "href": "notes/dnn/dnn-07/l_07.html#the-equivalence-between-feedforward-nets-and-recurrent-nets",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The equivalence between feedforward nets and recurrent nets",
    "text": "The equivalence between feedforward nets and recurrent nets\n\n\nAssume that there is a time delay of 1 in using each connection.\nThe recurrent net is just a layered net that keeps reusing the same weights."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#reminder-backpropagation-with-weight-constraints",
    "href": "notes/dnn/dnn-07/l_07.html#reminder-backpropagation-with-weight-constraints",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Reminder: Backpropagation with weight constraints",
    "text": "Reminder: Backpropagation with weight constraints\n\n\nIt is easy to modify the backprop algorithm to incorporate linear constraints between the weights.\nWe compute the gradients as usual, and then modify the gradients so that they satisfy the constraints.\n\nSo if the weights started off satisfying the constraints, they will continue to satisfy them."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#backpropagation-through-time",
    "href": "notes/dnn/dnn-07/l_07.html#backpropagation-through-time",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Backpropagation through time",
    "text": "Backpropagation through time\n\nWe can think of the recurrent net as a layered, feed-forward net with shared weights and then train the feed-forward net with weight constraints.\nWe can also think of this training algorithm in the time domain:\n\nThe forward pass builds up a stack of the activities of all the units at each time step.\nThe backward pass peels activities off the stack to compute the error derivatives at each time step.\nAfter the backward pass we add together the derivatives at all the different times for each weight."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#an-irritating-extra-issue",
    "href": "notes/dnn/dnn-07/l_07.html#an-irritating-extra-issue",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "An irritating extra issue",
    "text": "An irritating extra issue\n\nWe need to specify the initial activity state of all the hidden and output units.\nWe could just fix these initial states to have some default value like 0.5.\nBut it is better to treat the initial states as learned parameters.\nWe learn them in the same way as we learn the weights.\n\nStart off with an initial random guess for the initial states.\nAt the end of each training sequence, backpropagate through time all the way to the initial states to get the gradient of the error function with respect to each initial state.\nAdjust the initial states by following the negative gradient."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#providing-input-to-recurrent-networks",
    "href": "notes/dnn/dnn-07/l_07.html#providing-input-to-recurrent-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Providing input to recurrent networks",
    "text": "Providing input to recurrent networks\n\n\nWe can specify inputs in several ways:\n\nSpecify the initial states of all the units.\nSpecify the initial states of a subset of the units.\nSpecify the states of the same subset of the units at every time step.\n\nThis is the natural way to model most sequential data."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#teaching-signals-for-recurrent-networks",
    "href": "notes/dnn/dnn-07/l_07.html#teaching-signals-for-recurrent-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Teaching signals for recurrent networks",
    "text": "Teaching signals for recurrent networks\n\n\nWe can specify targets in several ways:\n\nSpecify desired final activities of all the units\nSpecify desired activities of all units for the last few steps\n\nGood for learning attractors\nIt is easy to add in extra error derivatives as we backpropagate.\n\nSpecify the desired activity of a subset of the units.\n\nThe other units are input or hidden units."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#a-good-toy-problem-for-a-recurrent-network",
    "href": "notes/dnn/dnn-07/l_07.html#a-good-toy-problem-for-a-recurrent-network",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "A good toy problem for a recurrent network",
    "text": "A good toy problem for a recurrent network"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-algorithm-for-binary-addition",
    "href": "notes/dnn/dnn-07/l_07.html#the-algorithm-for-binary-addition",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The algorithm for binary addition",
    "text": "The algorithm for binary addition\n\n\n\nFinite State Automaton\n\n\nThis is a finite state automaton. It decides what transition to make by looking at the next column. It prints after making the transition. It moves from right to left over the two input numbers."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#a-recurrent-net-for-binary-addition",
    "href": "notes/dnn/dnn-07/l_07.html#a-recurrent-net-for-binary-addition",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "A recurrent net for binary addition",
    "text": "A recurrent net for binary addition\n\n\nThe network has two input units and one output unit.\nIt is given two input digits at each time step.\nThe desired output at each time step is the output for the column that was provided as input two time steps ago.\n\nIt takes one time step to update the hidden units based on the two input digits.\nIt takes another time step for the hidden units to cause the output"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-connectivity-of-the-network",
    "href": "notes/dnn/dnn-07/l_07.html#the-connectivity-of-the-network",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The connectivity of the network",
    "text": "The connectivity of the network\n\nThe 3 hidden units are fully interconnected in both directions. - This allows a hidden activity pattern at one time step to vote for the hidden activity pattern at the next time step. - The input units have feedforward connections that allow then to vote for the next hidden activity pattern."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#what-the-network-learns",
    "href": "notes/dnn/dnn-07/l_07.html#what-the-network-learns",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "What the network learns",
    "text": "What the network learns\n\nIt learns four distinct patterns of activity for the 3 hidden units. These patterns correspond to the nodes in the finite state automaton.\n\nDo not confuse units in a neural network with nodes in a finite state automaton. Nodes are like activity vectors.\nThe automaton is restricted to be in exactly one state at each time. The hidden units are restricted to have exactly one vector of activity at each time.\n\nA recurrent network can emulate a finite state automaton, but it is exponentially more powerful. With N hidden neurons it has 2^N possible binary activity vectors (but only N^2 weights)\n\nThis is important when the input stream has two separate things going on at once.\nA finite state automaton needs to square its number of states.\nAn RNN needs to double its number of units."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-backward-pass-is-linear",
    "href": "notes/dnn/dnn-07/l_07.html#the-backward-pass-is-linear",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The backward pass is linear",
    "text": "The backward pass is linear\n - There is a big difference between the forward and backward passes. - In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding. - The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double. - The forward pass determines the slope of the linear function used for backpropagating through each neuron."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-problem-of-exploding-or-vanishing-gradients",
    "href": "notes/dnn/dnn-07/l_07.html#the-problem-of-exploding-or-vanishing-gradients",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The problem of exploding or vanishing gradients",
    "text": "The problem of exploding or vanishing gradients\n\nWhat happens to the magnitude of the gradients as we backpropagate through many layers?\n\nIf the weights are small, the gradients shrink exponentially.\nIf the weights are big the gradients grow exponentially.\n\nTypical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.\nIn an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.\n\nWe can avoid this by initializing the weights very carefully.\n\nEven with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.\n\nSo RNNs have difficulty dealing with long-range dependencies."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#why-the-back-propagated-gradient-blows-up",
    "href": "notes/dnn/dnn-07/l_07.html#why-the-back-propagated-gradient-blows-up",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Why the back-propagated gradient blows up",
    "text": "Why the back-propagated gradient blows up\n\n\nIf we start a trajectory within an attractor, small changes in where we start make no difference to where we end up.\nBut if we start almost exactly on the boundary, tiny changes can make a huge difference."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#four-effective-ways-to-learn-an-rnn",
    "href": "notes/dnn/dnn-07/l_07.html#four-effective-ways-to-learn-an-rnn",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Four effective ways to learn an RNN",
    "text": "Four effective ways to learn an RNN\n\nLong Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.\nHessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.\n\nThe HF optimizer ( Martens & Sutskever, 2011) is good at this.\n\nEcho State Networks: Initialize the inputàhidden and hiddenàhidden and outputàhidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input.\n\nESNs only need to learn the hiddenàoutput connections.\n\nGood initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#long-short-term-memory-lstm",
    "href": "notes/dnn/dnn-07/l_07.html#long-short-term-memory-lstm",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Long Short Term Memory (LSTM)",
    "text": "Long Short Term Memory (LSTM)\n\nHochreiter & Schmidhuber (1997) solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).\nThey designed a memory cell using logistic and linear units with multiplicative interactions.\nInformation gets into the cell whenever its “write” gate is on.\nThe information stays in the cell so long as its “keep” gate is on.\nInformation can be read from the cell by turning on its “read” gate."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#implementing-a-memory-cell-in-a-neural-network",
    "href": "notes/dnn/dnn-07/l_07.html#implementing-a-memory-cell-in-a-neural-network",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Implementing a memory cell in a neural network",
    "text": "Implementing a memory cell in a neural network\n\nTo preserve information for a long time in the activities of an RNN, we use a circuit that implements an analog memory cell. - A linear unit that has a self-link with a weight of 1 will maintain its state.\n- Information is stored in the cell by activating its write gate. - Information is retrieved by activating the read gate. - We can backpropagate through this circuit because logistics are have nice derivatives."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#backpropagation-through-a-memory-cell",
    "href": "notes/dnn/dnn-07/l_07.html#backpropagation-through-a-memory-cell",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Backpropagation through a memory cell",
    "text": "Backpropagation through a memory cell"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#reading-cursive-handwriting",
    "href": "notes/dnn/dnn-07/l_07.html#reading-cursive-handwriting",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Reading cursive handwriting",
    "text": "Reading cursive handwriting\n\nThis is a natural task for an RNN.\nThe input is a sequence of (x,y,p) coordinates of the tip of the pen, where p indicates whether the pen is up or down.\nThe output is a sequence of characters.\nGraves & Schmidhuber (2009) showed that RNNs with LSTM are currently the best systems for reading cursive writing.\n\nThey used a sequence of small images as input rather than pen coordinates. A demonstration of online handwriting recognition by an RNN with Long Short Term Memory (from Alex Graves)\n\nThe movie that follows shows several different things:\nRow 1: This shows when the characters are recognized.\n\nIt never revises its output so difficult decisions are more delayed.\n\nRow 2: This shows the states of a subset of the memory cells.\n\nNotice how they get reset when it recognizes a character.\n\nRow 3: This shows the writing. The net sees the x and y coordinates.\n\nOptical input actually works a bit better than pen coordinates.\n\nRow 4: This shows the gradient backpropagated all the way to the x and y inputs from the currently most active character.\n\nThis lets you see which bits of the data are influencing the decision.\n\n\n\n\n\nMemoryless models\nlinear dynamic systems\nHidden Markov Models\nrnns.png\nFinite State Automaton"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html",
    "href": "notes/dnn/dnn-07/l07b.html",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n::: column-margin"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#the-equivalence-between-feedforward-nets-and-recurrent-nets",
    "href": "notes/dnn/dnn-07/l07b.html#the-equivalence-between-feedforward-nets-and-recurrent-nets",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "The equivalence between feedforward nets and recurrent nets",
    "text": "The equivalence between feedforward nets and recurrent nets\n\n\nAssume that there is a time delay of 1 in using each connection.\nThe recurrent net is just a layered net that keeps reusing the same weights."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#reminder-backpropagation-with-weight-constraints",
    "href": "notes/dnn/dnn-07/l07b.html#reminder-backpropagation-with-weight-constraints",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "Reminder: Backpropagation with weight constraints",
    "text": "Reminder: Backpropagation with weight constraints\n\n\nIt is easy to modify the backprop algorithm to incorporate linear constraints between the weights.\nWe compute the gradients as usual, and then modify the gradients so that they satisfy the constraints.\n\nSo if the weights started off satisfying the constraints, they will continue to satisfy them."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#backpropagation-through-time",
    "href": "notes/dnn/dnn-07/l07b.html#backpropagation-through-time",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "Backpropagation through time",
    "text": "Backpropagation through time\n\nWe can think of the recurrent net as a layered, feed-forward net with shared weights and then train the feed-forward net with weight constraints.\nWe can also think of this training algorithm in the time domain:\n\nThe forward pass builds up a stack of the activities of all the units at each time step.\nThe backward pass peels activities off the stack to compute the error derivatives at each time step.\nAfter the backward pass we add together the derivatives at all the different times for each weight."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#an-irritating-extra-issue",
    "href": "notes/dnn/dnn-07/l07b.html#an-irritating-extra-issue",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "An irritating extra issue",
    "text": "An irritating extra issue\n\nWe need to specify the initial activity state of all the hidden and output units.\nWe could just fix these initial states to have some default value like 0.5.\nBut it is better to treat the initial states as learned parameters.\nWe learn them in the same way as we learn the weights.\n\nStart off with an initial random guess for the initial states.\nAt the end of each training sequence, backpropagate through time all the way to the initial states to get the gradient of the error function with respect to each initial state.\nAdjust the initial states by following the negative gradient."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#providing-input-to-recurrent-networks",
    "href": "notes/dnn/dnn-07/l07b.html#providing-input-to-recurrent-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "Providing input to recurrent networks",
    "text": "Providing input to recurrent networks\n\n\nWe can specify inputs in several ways:\n\nSpecify the initial states of all the units.\nSpecify the initial states of a subset of the units.\nSpecify the states of the same subset of the units at every time step.\n\nThis is the natural way to model most sequential data."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#teaching-signals-for-recurrent-networks",
    "href": "notes/dnn/dnn-07/l07b.html#teaching-signals-for-recurrent-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "Teaching signals for recurrent networks",
    "text": "Teaching signals for recurrent networks\n\n\nWe can specify targets in several ways:\n\nSpecify desired final activities of all the units\nSpecify desired activities of all units for the last few steps\n\nGood for learning attractors\nIt is easy to add in extra error derivatives as we backpropagate.\n\nSpecify the desired activity of a subset of the units.\n\nThe other units are input or hidden units."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html",
    "href": "notes/dnn/dnn-07/l07d.html",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n::: column-margin"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html#the-backward-pass-is-linear",
    "href": "notes/dnn/dnn-07/l07d.html#the-backward-pass-is-linear",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "The backward pass is linear",
    "text": "The backward pass is linear\n - There is a big difference between the forward and backward passes. - In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding. - The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double. - The forward pass determines the slope of the linear function used for backpropagating through each neuron."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html#the-problem-of-exploding-or-vanishing-gradients",
    "href": "notes/dnn/dnn-07/l07d.html#the-problem-of-exploding-or-vanishing-gradients",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "The problem of exploding or vanishing gradients",
    "text": "The problem of exploding or vanishing gradients\n\nWhat happens to the magnitude of the gradients as we backpropagate through many layers?\n\nIf the weights are small, the gradients shrink exponentially.\nIf the weights are big the gradients grow exponentially.\n\nTypical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.\nIn an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.\n\nWe can avoid this by initializing the weights very carefully.\n\nEven with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.\n\nSo RNNs have difficulty dealing with long-range dependencies."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html#why-the-back-propagated-gradient-blows-up",
    "href": "notes/dnn/dnn-07/l07d.html#why-the-back-propagated-gradient-blows-up",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "Why the back-propagated gradient blows up",
    "text": "Why the back-propagated gradient blows up\n\n\nIf we start a trajectory within an attractor, small changes in where we start make no difference to where we end up.\nBut if we start almost exactly on the boundary, tiny changes can make a huge difference."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html#four-effective-ways-to-learn-an-rnn",
    "href": "notes/dnn/dnn-07/l07d.html#four-effective-ways-to-learn-an-rnn",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "Four effective ways to learn an RNN",
    "text": "Four effective ways to learn an RNN\n\nLong Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.\nHessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.\n\nThe HF optimizer ( Martens & Sutskever, 2011) is good at this.\n\nEcho State Networks: Initialize the inputàhidden and hiddenàhidden and outputàhidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input.\n\nESNs only need to learn the hiddenàoutput connections.\n\nGood initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html",
    "href": "notes/dnn/dnn-14/l_14.html",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#training-a-deep-network-by-stacking-rbms",
    "href": "notes/dnn/dnn-14/l_14.html#training-a-deep-network-by-stacking-rbms",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Training a deep network by stacking RBMs",
    "text": "Training a deep network by stacking RBMs\n\nFirst train a layer of features that receive input directly from the pixels.\nThen treat the activations of the trained features as if they were pixels and learn features of features in a second hidden layer.\nThen do it again.\nIt can be proved that each time we add another layer of features we improve a variational lower bound on the log probability of generating the training data.\n\nThe proof is complicated and only applies to unreal cases.\nIt is based on a neat equivalence between an RBM and an infinitely deep belief net (see lecture 14b)."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#combining-two-rbms-to-make-a-dbn",
    "href": "notes/dnn/dnn-14/l_14.html#combining-two-rbms-to-make-a-dbn",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Combining two RBMs to make a DBN",
    "text": "Combining two RBMs to make a DBN\n\n\n\n\n\nRDBM Combo\n\n\n\nThe generative model after learning 3 layers\n\n\n\n\nRDBM Combo\n\n\nTo generate data:\n\nGet an equilibrium sample from the toplevel RBM by performing alternating Gibbs sampling for a long time.\nPerform a top-down pass to get states for all the other layers.\n\nThe lower level bottom-up connections are not part of the generative model. They are just used for inference."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#the-generative-model-after-learning-3-layers",
    "href": "notes/dnn/dnn-14/l_14.html#the-generative-model-after-learning-3-layers",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "The generative model after learning 3 layers",
    "text": "The generative model after learning 3 layers"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#an-aside-averaging-factorial-distributions",
    "href": "notes/dnn/dnn-14/l_14.html#an-aside-averaging-factorial-distributions",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "An aside: Averaging factorial distributions",
    "text": "An aside: Averaging factorial distributions\n\nIf you average some factorial distributions, you do NOT get a factorial distribution.\n\nIn an RBM, the posterior over 4 hidden units is factorial for each visible vector.\n\nPosterior for v1: 0.9, 0.9, 0.1, 0.1\nPosterior for v2: 0.1, 0.1, 0.9, 0.9\nAggregated = 0.5, 0.5, 0.5, 0.5\nConsider the binary vector 1,1,0,0.\n\nin the posterior for v1, p(1,1,0,0) = 0.9^4 = 0.43\nin the posterior for v2, p(1,1,0,0) = 0.1^4 = .0001 – in the aggregated posterior, p(1,1,0,0) = 0.215.\n\nIf the aggregated posterior was factorial it would have p = 0.5^4"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-does-greedy-learning-work",
    "href": "notes/dnn/dnn-14/l_14.html#why-does-greedy-learning-work",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why does greedy learning work?",
    "text": "Why does greedy learning work?\nThe weights, W, in the bottom level RBM define many different distributions: p(v|h); p(h|v); p(v,h); p(h); p(v).\nWe can express the RBM model as\n\np(v)= \\sum_h p(h) p(v \\mid h)\n\nIf we leave p(v|h) alone and improve p(h), we will improve p(v).\nTo improve p(h), we need it to be a better model than p(h;W) of the aggregated posterior distribution over hidden vectors produced by applying W transpose to the data."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#fine-tuning-with-a-contrastive-version-of-the-wake-sleep-algorithm",
    "href": "notes/dnn/dnn-14/l_14.html#fine-tuning-with-a-contrastive-version-of-the-wake-sleep-algorithm",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Fine-tuning with a contrastive version of the wake-sleep algorithm",
    "text": "Fine-tuning with a contrastive version of the wake-sleep algorithm\nAfter learning many layers of features, we can fine-tune the features to improve generation.\n\nDo a stochastic bottom-up pass\n\nThen adjust the top-down weights of lower layers to be good at reconstructing the feature activities in the layer below.\n\nDo a few iterations of sampling in the top level RBM\n\nThen adjust the weights in the top-level RBM using CD.\n\nDo a stochastic top-down pass\n\nThen Adjust the bottom-up weights to be good at reconstructing the feature activities in the layer above."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#the-dbn-used-for-modeling-the-joint-distribution-of-mnist-digits-and-their-labels",
    "href": "notes/dnn/dnn-14/l_14.html#the-dbn-used-for-modeling-the-joint-distribution-of-mnist-digits-and-their-labels",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "The DBN used for modeling the joint distribution of MNIST digits and their labels",
    "text": "The DBN used for modeling the joint distribution of MNIST digits and their labels\n\n\n\n\nThe first two hidden layers are learned without using labels.\nThe top layer is learned as an RBM for modeling the labels concatenated with the features in the second hidden layer.\nThe weights are then fine-tuned to be a better generative model using contrastive wake-sleep."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#fine-tuning-for-discrimination",
    "href": "notes/dnn/dnn-14/l_14.html#fine-tuning-for-discrimination",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Fine-tuning for discrimination",
    "text": "Fine-tuning for discrimination\n\nFirst learn one layer at a time by stacking RBMs.\nTreat this as “pre-training” that finds a good initial set of weights which can then be fine-tuned by a local search procedure.\n\nContrastive wake-sleep is a way of fine-tuning the model to be better at generation.\n\nBackpropagation can be used to fine-tune the model to be better at discrimination.\n\nThis overcomes many of the limitations of standard backpropagation.\nIt makes it easier to learn deep nets.\nIt makes the nets generalize better."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-backpropagation-works-better-with-greedy-pre-training-the-optimization-view",
    "href": "notes/dnn/dnn-14/l_14.html#why-backpropagation-works-better-with-greedy-pre-training-the-optimization-view",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why backpropagation works better with greedy pre-training: The optimization view",
    "text": "Why backpropagation works better with greedy pre-training: The optimization view\n\nGreedily learning one layer at a time scales well to really big networks, especially if we have locality in each layer.\nWe do not start backpropagation until we already have sensible feature detectors that should already be very helpful for the discrimination task.\n\nSo the initial gradients are sensible and backpropagation only needs to perform a local search from a sensible starting point"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-backpropagation-works-better-with-greedy-pre-training-the-overfitting-view",
    "href": "notes/dnn/dnn-14/l_14.html#why-backpropagation-works-better-with-greedy-pre-training-the-overfitting-view",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why backpropagation works better with greedy pre-training: The overfitting view",
    "text": "Why backpropagation works better with greedy pre-training: The overfitting view\n\nMost of the information in the final weights comes from modeling the distribution of input vectors.\n\nThe input vectors generally contain a lot more information than the labels.\nThe precious information in the labels is only used for the fine tuning.\n\nThe fine-tuning only modifies the features slightly to get the category boundaries right. It does not need to discover new features.\nThis type of back-propagation works well even if most of the training data is unlabeled.\n\nThe unlabeled data is still very useful for discovering good features.\n\nAn objection: Surely, many of the features will be useless for any particular discriminative task (consider shape & pose).\n\nBut the ones that are useful will be much more useful than the raw inputs."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#results-on-the-permutation-invariant-mnist-task",
    "href": "notes/dnn/dnn-14/l_14.html#results-on-the-permutation-invariant-mnist-task",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Results on the permutation-invariant MNIST task",
    "text": "Results on the permutation-invariant MNIST task\n\n\n\n\n\n\n\npaper\nError rate\n\n\n\n\nBackprop net with one or two hidden layers (Platt; Hinton)\n1.6%\n\n\nBackprop with L2 constraints on incoming weights\n1/4%\n\n\nSupport Vector Machines (Decoste & Schoelkopf, 2002)\n1.4%\n\n\nGenerative model of joint density of images and labels (+ generative fine-tuning)\n1.25%\n\n\nGenerative model of unlabelled digits followed by gentle backpropagation (Hinton & Salakhutdinov, 2006)\n1.15%-&gt;1.0%"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#unsupervised-pre-training-also-helps-for-models-that-have-more-data-and-better-priors",
    "href": "notes/dnn/dnn-14/l_14.html#unsupervised-pre-training-also-helps-for-models-that-have-more-data-and-better-priors",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Unsupervised “pre-training” also helps for models that have more data and better priors",
    "text": "Unsupervised “pre-training” also helps for models that have more data and better priors\n\nIn Ranzato et al. (2006) the authors used an additional 600,000 distorted digits.\nThey also used convolutional multilayer neural networks.\n\nBack-propagation alone: 0.49% Unsupervised layer-by-layer pre-training followed by back-prop: 0.39% (record at the time)"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#phone-recognition-on-the-timit-benchmark-mohamed-dahl-hinton-2009-2012",
    "href": "notes/dnn/dnn-14/l_14.html#phone-recognition-on-the-timit-benchmark-mohamed-dahl-hinton-2009-2012",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Phone recognition on the TIMIT benchmark (Mohamed, Dahl, & Hinton, 2009 & 2012)",
    "text": "Phone recognition on the TIMIT benchmark (Mohamed, Dahl, & Hinton, 2009 & 2012)\n\n\n\n\nAfter standard post-processing using a bi-phone model, a deep net with 8 layers gets 20.7% error rate.\nThe best previous speaker independent result on TIMIT was 24.4% and this required averaging several models.\nLi Deng (at MSR) realized that this result could change the way speech recognition was done. It has!"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#learning-dynamics-of-deep-nets-the-next-4-slides-describe-work-by-yoshua-bengios-group",
    "href": "notes/dnn/dnn-14/l_14.html#learning-dynamics-of-deep-nets-the-next-4-slides-describe-work-by-yoshua-bengios-group",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Learning Dynamics of Deep Nets the next 4 slides describe work by Yoshua Bengio’s group",
    "text": "Learning Dynamics of Deep Nets the next 4 slides describe work by Yoshua Bengio’s group"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#effect-of-unsupervised-pre-training",
    "href": "notes/dnn/dnn-14/l_14.html#effect-of-unsupervised-pre-training",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Effect of Unsupervised Pre-training",
    "text": "Effect of Unsupervised Pre-training"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#effect-of-depth",
    "href": "notes/dnn/dnn-14/l_14.html#effect-of-depth",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Effect of Depth",
    "text": "Effect of Depth"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#trajectories-of-the-learning-in-function-space",
    "href": "notes/dnn/dnn-14/l_14.html#trajectories-of-the-learning-in-function-space",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Trajectories of the learning in function space",
    "text": "Trajectories of the learning in function space\n\n\n\nEach point is a model in function space\n\nColor = epoch\nTop: trajectories without pre-training. Each trajectory converges to a different local min.\nBottom: Trajectories with pre-training.\nNo overlap!"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-unsupervised-pre-training-makes-sense",
    "href": "notes/dnn/dnn-14/l_14.html#why-unsupervised-pre-training-makes-sense",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why unsupervised pre-training makes sense",
    "text": "Why unsupervised pre-training makes sense\n\n\n\n\n\nchain graph\n\n\n\nIf image-label pairs were generated this way, it would make sense to try to go straight from images to labels. For example, do the pixels have even parity?\n\n\n\n\n\n\nv graph\n\n\nIf image-label pairs are generated this way, it makes sense to first learn to recover the stuff that caused the image by inverting the high bandwidth pathway."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#modeling-real-valued-data",
    "href": "notes/dnn/dnn-14/l_14.html#modeling-real-valued-data",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Modeling real-valued data",
    "text": "Modeling real-valued data\n\nFor images of digits, intermediate intensities can be represented as if they were probabilities by using “mean-field” logistic units.\n\nWe treat intermediate values as the probability that the pixel is inked.\n\nThis will not work for real images.\n\nIn a real image, the intensity of a pixel is almost always, almost exactly the average of the neighboring pixels.\nMean-field logistic units cannot represent precise intermediate values."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#a-standard-type-of-real-valued-visible-unit",
    "href": "notes/dnn/dnn-14/l_14.html#a-standard-type-of-real-valued-visible-unit",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "A standard type of real-valued visible unit",
    "text": "A standard type of real-valued visible unit\n\n\n\n\nModel pixels as Gaussian variables. Alternating Gibbs sampling is still easy, though learning needs to be much slower.\n\n\nE(v,h) = \\sum_{i\\in vis} \\frac{(v_i-b_i)^2}{2\\sigma^2_i} - \\sum_j b_jh_j- \\sum_{i,j} \\frac{v_i}{\\sigma_i} h_i w_{ij}\n where:\n\nthe first term is parabolic containment function\nthe last term is energy-gradient produced by the total input to a visible unit"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#gaussian-binary-rbms",
    "href": "notes/dnn/dnn-14/l_14.html#gaussian-binary-rbms",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Gaussian-Binary RBM’s",
    "text": "Gaussian-Binary RBM’s\n\n\n\n\nLots of people have failed to get these to work properly. Its extremely hard to learn tight variances for the visible units.\n\nIt took a long time for us to figure out why it is so hard to learn the visible variances.\n\nWhen sigma is small, we need many more hidden units than visible units.\nThis allows small weights to produce big top-down effects.\nWhen sigma is much less than 1, the bottom-up effects are too big and the top-down effects are too small."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#stepped-sigmoid-units-a-neat-way-to-implement-integer-values",
    "href": "notes/dnn/dnn-14/l_14.html#stepped-sigmoid-units-a-neat-way-to-implement-integer-values",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Stepped sigmoid units: A neat way to implement integer values",
    "text": "Stepped sigmoid units: A neat way to implement integer values\n\n\n\n\n\nstepped sigmoid\n\n\n\nMake many copies of a stochastic binary unit.\nAll copies have the same weights and the same adaptive bias, b, but they have different fixed offsets to the bias:\n\nb −0.5,\\ b −1.5,\\ b −2.5, b −3.5,\\ \\ldots\n ## Fast approximations\n\n\n\n\n\n\nlogarithmic approximation\n\n\n\n\n\nlinear approximation\n\n\n\n\\langle y \\rangle = \\sum_{n=1}^{\\infty} \\sigma (x + 0.5− n)  ≈ log(1+ e^x) ≈ max(0, x + noise)\n\n\nContrastive divergence learning works well for the sum of stochastic logistic units with offset biases.\nThe noise variance is \\sigma(y)\nIt also works for RELUs, which are faster to compute than the sum of many logistic units with different biases."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#a-nice-property-of-rectified-linear-units",
    "href": "notes/dnn/dnn-14/l_14.html#a-nice-property-of-rectified-linear-units",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "A nice property of rectified linear units",
    "text": "A nice property of rectified linear units\n\nIf a RELU has a bias of zero, it exhibits scale equivariance:\nThis is a very nice property to have for images.\nIt is like the equivariance to translation exhibited by convolutional nets.\n\n\nR(shift(x)) = shift(R(x))\n # Lecture 14e : RBMs are Infinite Sigmoid Belief Nets\nThis is advanced material"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#another-view-of-why-layer-by-layer-learning-works-hinton2006fast",
    "href": "notes/dnn/dnn-14/l_14.html#another-view-of-why-layer-by-layer-learning-works-hinton2006fast",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Another view of why layer-by-layer learning works — Hinton, Osindero, and Teh (2006)",
    "text": "Another view of why layer-by-layer learning works — Hinton, Osindero, and Teh (2006)\n\nThere is an unexpected equivalence between RBM’s and directed networks with many layers that all share the same weight matrix.\n\nThis equivalence also gives insight into why contrastive divergence learning works.\n\nAn RBM is actually just an infinitely deep sigmoid belief net with a lot of weight sharing.\n\nThe Markov chain we run when we want to sample from the equilibrium distribution of an RBM can be viewed as a sigmoid belief net."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#an-infinite-sigmoid-belief-net-that-is-equivalent-to-an-rbm",
    "href": "notes/dnn/dnn-14/l_14.html#an-infinite-sigmoid-belief-net-that-is-equivalent-to-an-rbm",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "An infinite sigmoid belief net that is equivalent to an RBM",
    "text": "An infinite sigmoid belief net that is equivalent to an RBM\n\n\n\n\nThe distribution generated by this infinite directed net with replicated weights is the equilibrium distribution for a compatible pair of conditional distributions: p(v \\mid h) and p(h \\mid v) that are both defined by W\n\nA top-down pass of the directed net is exactly equivalent to letting a Restricted Boltzmann Machine settle to equilibrium.\nSo this infinite directed net defines the same distribution as an RBM"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#inference-in-an-infinite-sigmoid-belief-net",
    "href": "notes/dnn/dnn-14/l_14.html#inference-in-an-infinite-sigmoid-belief-net",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Inference in an infinite sigmoid belief net",
    "text": "Inference in an infinite sigmoid belief net\n\n\n\n\nThe variables in h_0 are conditionally independent given v_0.\n\nInference is trivial. Just multiply v_0 by W^T\nThe model above h0 implements a complementary prior.\nMultiplying v0 by gives the product of the likelihood term and the prior term.\nThe complementary prior cancels the explaining away.\n\nInference in the directed net is exactly equivalent to letting an RBM settle to equilibrium starting at the data.\n\n\n\n\n\nThe learning rule for a sigmoid belief net is:\n\n\n  \\Delta w_{ij} ∝ s_j(s_i − p_i)\n\nwhere s_j^1 is an unbiased sample from p^i_0\n\nWith replicated weights this rule becomes:\n\n\n  s_j^0 s_i^0 - s_j^\\infty s_j^\\infty"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#learning-a-deep-directed-network",
    "href": "notes/dnn/dnn-14/l_14.html#learning-a-deep-directed-network",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Learning a deep directed network",
    "text": "Learning a deep directed network\n\nFirst learn with all the weights tied. This is exactly equivalent to learning an RBM.\n\nThink of the symmetric connections as a shorthand notation for an infinite directed net with tied weights.\n\nWe ought to use maximum likelihood learning, but we use CD1 as a shortcut.\nThen freeze the first layer of weights in both directions and learn the remaining weights (still tied together).\n\nThis is equivalent to learning another RBM, using the aggregated posterior distribution of h0 as the data."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#what-happens-when-the-weights-in-higher-layers-become-different-from-the-weights-in-the-first-layer",
    "href": "notes/dnn/dnn-14/l_14.html#what-happens-when-the-weights-in-higher-layers-become-different-from-the-weights-in-the-first-layer",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "What happens when the weights in higher layers become different from the weights in the first layer?",
    "text": "What happens when the weights in higher layers become different from the weights in the first layer?\n\nThe higher layers no longer implement a complementary prior.\n\nSo performing inference using the frozen weights in the first layer is no longer correct.\nBut its still pretty good.\nUsing this incorrect inference procedure gives a variational lower bound on the log probability of the data.\n\nThe higher layers learn a prior that is closer to the aggregated posterior distribution of the first hidden layer.\n\nThis improves the network’s model of the data.\nIn (Hinton, Osindero, and Teh 2006) the authors prove that this improvement is always bigger than the loss in the variational bound caused by using less accurate inference."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#what-is-really-happening-in-contrastive-divergence-learning",
    "href": "notes/dnn/dnn-14/l_14.html#what-is-really-happening-in-contrastive-divergence-learning",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "What is really happening in contrastive divergence learning?",
    "text": "What is really happening in contrastive divergence learning?\n\nContrastive divergence learning in this RBM is equivalent to ignoring the small derivatives contributed by the tied weights in higher layers."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-is-it-ok-to-ignore-the-derivatives-in-higher-layers",
    "href": "notes/dnn/dnn-14/l_14.html#why-is-it-ok-to-ignore-the-derivatives-in-higher-layers",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why is it OK to ignore the derivatives in higher layers?",
    "text": "Why is it OK to ignore the derivatives in higher layers?\n\nWhen the weights are small, the Markov chain mixes fast.\n\nSo the higher layers will be close to the equilibrium distribution (i.e they will have “forgotten” the data vector).\nAt equilibrium the derivatives must average to zero, because the current weights are a perfect model of the equilibrium distribution!\n\nAs the weights grow we may need to run more iterations of CD.\n\nThis allows CD to continue to be a good approximation to maximum likelihood.\nBut for learning layers of features, it does not need to be a good approximation to maximum likelihood\n\n\n\n\n\nRDBM Combo\nRDBM Combo\nchain graph\nv graph\nstepped sigmoid\nlogarithmic approximation\nlinear approximation"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html",
    "href": "notes/dnn/dnn-10/l_10.html",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#combining-networks-the-bias-variance-trade-off",
    "href": "notes/dnn/dnn-10/l_10.html#combining-networks-the-bias-variance-trade-off",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Combining networks: The bias-variance trade-off",
    "text": "Combining networks: The bias-variance trade-off\n\nWhen the amount of training data is limited, we get overfitting.\nAveraging the predictions of many different models is a good way to reduce overfitting.\nIt helps most when the models make very different predictions.\nFor regression, the squared error can be decomposed into a “bias” term and a “variance” term.\nThe bias term is big if the model has too little capacity to fit the data.\nThe variance term is big if the model has so much capacity that it is good at fitting the sampling error in each particular training set.\nBy averaging away the variance we can use individual models with high capacity. These models have high variance but low bias."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#how-the-combined-predictor-compares-with-the-individual-predictors",
    "href": "notes/dnn/dnn-10/l_10.html#how-the-combined-predictor-compares-with-the-individual-predictors",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "How the combined predictor compares with the individual predictors",
    "text": "How the combined predictor compares with the individual predictors\n\nOn any one test case, some individual predictors may be better than the combined predictor.\nBut different individual predictors will be better on different cases.\nIf the individual predictors disagree a lot, the combined predictor is typically better than all of the individual predictors when we average over test cases.\nSo we should try to make the individual predictors disagree (without making them much worse individually)."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#combining-networks-reduces-variance",
    "href": "notes/dnn/dnn-10/l_10.html#combining-networks-reduces-variance",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Combining networks reduces variance",
    "text": "Combining networks reduces variance\nWe want to compare two expected squared errors: Pick a predictor at random versus use the average of all the predictors:"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#a-picture",
    "href": "notes/dnn/dnn-10/l_10.html#a-picture",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "A picture",
    "text": "A picture\n\n\nThe predictors that are further than average from t make bigger than average squared errors.\nThe predictors that are nearer than average to t make smaller then average squared errors.\nThe first effect dominates because squares work like that.\nDon’t try averaging if you want to synchronize a bunch of clocks!\n\nThe noise is not Gaussian\n\n\n\n\\frac{ (\\bar{y}+\\epsilon)^2 + (\\bar{y} + \\epsilon )^2}{2} = \\bar{y}^2 + \\epsilon^2"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#what-about-discrete-distributions-over-class-labels",
    "href": "notes/dnn/dnn-10/l_10.html#what-about-discrete-distributions-over-class-labels",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "What about discrete distributions over class labels?",
    "text": "What about discrete distributions over class labels?\n\n\n\n\n\nSuppose that one model gives the correct label probability and the other model gives it\nIs it better to pick one model at random, or is it better to average the two probabilities?\n\n\n\n\\log \\Biggr( \\frac{ p_i + p_j }{2} \\Biggr) \\ge \\frac{\\log p_i + \\log p_j}{2}"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#overview-of-ways-to-make-predictors-differ",
    "href": "notes/dnn/dnn-10/l_10.html#overview-of-ways-to-make-predictors-differ",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Overview of ways to make predictors differ",
    "text": "Overview of ways to make predictors differ\n\nRely on the learning algorithm getting stuck in different local optima.\nA dubious hack (but worth a try).\nUse lots of different kinds of models, including ones that are not neural networks.\nDecision trees\nGaussian Process models\nSupport Vector Machines\nand many others.\nFor neural network models, make them different by using:\n\nDifferent numbers of hidden layers.\nDifferent numbers of units per layer.\nDifferent types of unit.\nDifferent types or strengths of weight penalty.\nDifferent learning algorithms."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#making-models-differ-by-changing-their-training-data",
    "href": "notes/dnn/dnn-10/l_10.html#making-models-differ-by-changing-their-training-data",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Making models differ by changing their training data",
    "text": "Making models differ by changing their training data\n\n\n \n\nBagging: Train different models on different subsets of the data.\n\nBagging gets different training sets by using sampling with replacement: \\{a,b,c,d,e\\} \\to &lt;a, c, c, d, d, \\ldots &gt;\nRandom forests use lots of different decision trees trained using bagging. They work well.\n\nWe could use bagging with neural nets but its very expensive.\nBoosting: Train a sequence of low capacity models. Weight the training cases differently for each model in the sequence.\n\nBoosting up-weights cases that previous models got wrong.\nAn early use of boosting was with neural nets for MNIST.\nIt focused the computational resources on modeling the tricky cases."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#mixtures-of-experts",
    "href": "notes/dnn/dnn-10/l_10.html#mixtures-of-experts",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Mixtures of Experts",
    "text": "Mixtures of Experts\n\n\nCan we do better that just averaging models in a way that does not depend on the particular training case? – Maybe we can look at the input data for a particular case to help us decide which model to rely on. – This may allow particular models to specialize in a subset of the training cases. – They do not learn on cases for which they are not picked. So they can ignore stuff they are not good at modeling. Hurray for nerds!\nThe key idea is to make each expert focus on predicting the right answer for the cases where it is already doing better than the other experts. – This causes specialization. # A spectrum of models\n\nVery local models – e.g. Nearest neighbors - Very fast to fit - Just store training cases - Local smoothing would obviously improve things.\nFully global models - e. g. A polynomial - May be slow to fit and also unstable. - Each parameter depends on all the data. Small changes to data can cause big changes to the fit."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#multiple-local-models",
    "href": "notes/dnn/dnn-10/l_10.html#multiple-local-models",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Multiple local models",
    "text": "Multiple local models\n\nInstead of using a single global model or lots of very local models, use several models of intermediate complexity.\n\nGood if the dataset contains several different regimes which have different relationships between input and output.\n\ne.g. financial data which depends on the state of the economy.\n\n\nBut how do we partition the dataset into regimes?"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#partitioning-based-on-input-alone-versus-partitioning-based-on-the-input-output-relationship",
    "href": "notes/dnn/dnn-10/l_10.html#partitioning-based-on-input-alone-versus-partitioning-based-on-the-input-output-relationship",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Partitioning based on input alone versus partitioning based on the input-output relationship",
    "text": "Partitioning based on input alone versus partitioning based on the input-output relationship\n\n\nWe need to cluster the training cases into subsets, one for each local model.\n\nThe aim of the clustering is NOT to find clusters of similar input vectors.\nWe want each cluster to have a relationship between input and output that can be well-modeled by one local model."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#a-picture-of-why-averaging-models-during-training-causes-cooperation-not-specialization",
    "href": "notes/dnn/dnn-10/l_10.html#a-picture-of-why-averaging-models-during-training-causes-cooperation-not-specialization",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "A picture of why averaging models during training causes cooperation not specialization",
    "text": "A picture of why averaging models during training causes cooperation not specialization\n\nDo we really want to move the output of model i away from the target value?"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#an-error-function-that-encourages-cooperation",
    "href": "notes/dnn/dnn-10/l_10.html#an-error-function-that-encourages-cooperation",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "An error function that encourages cooperation",
    "text": "An error function that encourages cooperation\nIf we want to encourage cooperation, we compare the average of all the predictors with the target and train to reduce the discrepancy. – This can overfit badly. It makes the model much more powerful than training each predictor separately. \nE=(t - \\lt y_i \\gt_i )^2\n\nwhere:\n\ny_i is the average over all predictors."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#an-error-function-that-encourages-specialization",
    "href": "notes/dnn/dnn-10/l_10.html#an-error-function-that-encourages-specialization",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "An error function that encourages specialization",
    "text": "An error function that encourages specialization\n\nIf we want to encourage specialization we compare each predictor separately with the target.\nWe also use a “manager” to determine the probability of picking each expert.\n\nMost experts end up ignoring most targets\n\n\n\n  E =  \\lt p_i(t- y_i)^2 \\gt_i\n\nwhere:\n\ny_i is the avarage over all predictors.\np_i probability of the manager picking expert i for this case."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#the-mixture-of-experts-architecture-almost",
    "href": "notes/dnn/dnn-10/l_10.html#the-mixture-of-experts-architecture-almost",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "The mixture of experts architecture (almost)",
    "text": "The mixture of experts architecture (almost)\nA simple cost function 1\n\n  E = \\sum_i p_i (t- y_i)^2\n\n\n\n\nsimplified mixture of experts architecture"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#the-derivatives-of-the-simple-cost-function",
    "href": "notes/dnn/dnn-10/l_10.html#the-derivatives-of-the-simple-cost-function",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "The derivatives of the simple cost function",
    "text": "The derivatives of the simple cost function\n\nIf we differentiate w.r.t. the outputs of the experts we get a signal for training each expert.\nIf we differentiate w.r.t. the outputs of the gating network we get a signal for training the gating net.\n\nWe want to raise p for all experts that give less than the average squared error of all the experts (weighted by p)\n\n\n\np_j = \\frac{e^{x_j}}{\\sum_i e^x_i}\n\n\nE = \\sum_i p_i (t- y_i)^2\n\\tag{1}\n\n\\frac{\\partial E}{\\partial y_i} = p_i(t-y_i)\n\\tag{2}\n\n\\frac{\\partial E}{\\partial x_i} = p_i\\Bigg((t-y_i)^2-E\\Bigg)\n\\tag{3}"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#a-better-cost-function-for-mixtures-of-experts-jacobs1991adaptive",
    "href": "notes/dnn/dnn-10/l_10.html#a-better-cost-function-for-mixtures-of-experts-jacobs1991adaptive",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "A better cost function for mixtures of experts (Jacobs et al. 1991)",
    "text": "A better cost function for mixtures of experts (Jacobs et al. 1991)\n\n\n\n\n\nThink of each expert as making a prediction that is a Gaussian distribution around its output (with variance 1).\nThink of the manager as deciding on a scale for each of these Gaussians. The scale is called a “mixing proportion”. e.g {0.4 0.6}\nMaximize the log probability of the target value under this mixture of Gaussians model i.e. the sum of the two scaled Gaussians."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#the-probability-of-the-target-under-a-mixture-of-gaussians",
    "href": "notes/dnn/dnn-10/l_10.html#the-probability-of-the-target-under-a-mixture-of-gaussians",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "The probability of the target under a mixture of Gaussians",
    "text": "The probability of the target under a mixture of Gaussians\n\np(t^c | MoE) = \\sum_i p_i^c {\\color{red}\\frac{1}{\\sqrt{2\\pi}}} e^−\\frac{1}{2} (t^c−y_i^c )^2\n\\tag{4}\nwhere:\n\nlhs - prob. of target value on case c given the mixture.\nMoE — Mixture of Experts\ny_i = output of expert i\nthe constant in red - normalization term for a Gaussian with \\sigma^2=1"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#full-bayesian-learning",
    "href": "notes/dnn/dnn-10/l_10.html#full-bayesian-learning",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Full Bayesian Learning",
    "text": "Full Bayesian Learning\n\nInstead of trying to find the best single setting of the parameters (as in Maximum Likelihood or MAP) compute the full posterior distribution over all possible parameter settings.\n\nThis is extremely computationally intensive for all but the simplest models (its feasible for a biased coin).\n\nTo make predictions, let each different setting of the parameters make its own prediction and then combine all these predictions by weighting each of them by the posterior probability of that setting of the parameters.\n\nThis is also very computationally intensive.\n\nThe full Bayesian approach allows us to use complicated models even when we do not have much data."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#overfitting-a-frequentist-illusion",
    "href": "notes/dnn/dnn-10/l_10.html#overfitting-a-frequentist-illusion",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Overfitting: A frequentist illusion?",
    "text": "Overfitting: A frequentist illusion?\n\nIf you do not have much data, you should use a simple model, because a complex one will overfit.\n\nThis is true.\nBut only if you assume that fitting a model means choosing a single best setting of the parameters.\n\nIf you use the full posterior distribution over parameter settings, overfitting disappears.\n\nWhen there is very little data, you get very vague predictions because many different parameters settings have significant posterior probability"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#a-classic-example-of-overfitting",
    "href": "notes/dnn/dnn-10/l_10.html#a-classic-example-of-overfitting",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "A classic example of overfitting",
    "text": "A classic example of overfitting\n\n\n\n\n\noverfitting\n\n\n\n\n\nnot-overfitting\n\n\n\nWhich model do you believe?\n\nThe complicated model fits the data better.\nBut it is not economical and it makes silly predictions.\n\nBut what if we start with a reasonable prior over all fifth-order polynomials and use the full posterior distribution.\n\nNow we get vague and sensible predictions.\n\nThere is no reason why the amount of data should influence our prior beliefs about the complexity of the model."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#approximating-full-bayesian-learning-in-a-neural-net",
    "href": "notes/dnn/dnn-10/l_10.html#approximating-full-bayesian-learning-in-a-neural-net",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Approximating full Bayesian learning in a neural net",
    "text": "Approximating full Bayesian learning in a neural net\n\nIf the neural net only has a few parameters we could put a grid over the parameter space and evaluate p( W | D ) at each grid-point.\n\nThis is expensive, but it does not involve any gradient descent and there are no local optimum issues.\n\nAfter evaluating each grid point we use all of them to make predictions on test data\n\nThis is also expensive, but it works much better than ML learning when the posterior is vague or multimodal (this happens when data is scarce).\n\n\n\np(t_{test} \\mid \\text{input}_{test}) = \\sum_{g \\in grid} p(W_g \\mid D) p(t_{test} \\mid \\text{input}_{test}, W_g )\n\\tag{5}"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#an-example-of-full-bayesian-learning",
    "href": "notes/dnn/dnn-10/l_10.html#an-example-of-full-bayesian-learning",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "An example of full Bayesian learning",
    "text": "An example of full Bayesian learning\n\n\n\n\nAllow each of the 6 weights or biases to have the 9 possible values -2,\\ -1.5,\\ -1,\\ -0.5,\\ 0,\\ 0.5,\\ 1,\\ 1.5,\\ 2\n\nThere are 9^6 grid-points in parameter space\n\nFor each grid-point compute the probability of the observed outputs of all the training cases.\nMultiply the prior for each grid-point by the likelihood term and renormalize to get the posterior probability for each grid-point.\nMake predictions by using the posterior probabilities to average the predictions made by the different grid-points."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#what-can-we-do-if-there-are-too-many-parameters-for-a-grid",
    "href": "notes/dnn/dnn-10/l_10.html#what-can-we-do-if-there-are-too-many-parameters-for-a-grid",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "What can we do if there are too many parameters for a grid?",
    "text": "What can we do if there are too many parameters for a grid?\n\nThe number of grid points is exponential in the number of parameters.\n\nSo we cannot deal with more than a few parameters using a grid.\n\nIf there is enough data to make most parameter vectors very unlikely, only a tiny fraction of the grid points make a significant contribution to the predictions.\n\nMaybe we can just evaluate this tiny fraction\n\nIdea: 💡 It might be good enough to just sample weight vectors according to their posterior probabilities.\n\n\np(y_{test} \\mid \\text{input}_{test},D) = \\sum_{i} {\\color{green}{ p(W_i \\mid D)}} p(y_{test} \\mid \\text{input}_{test}, W_i )\n\nwhere:\n\nthe green term - Sample weight vectors with this probability"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#sampling-weight-vectors",
    "href": "notes/dnn/dnn-10/l_10.html#sampling-weight-vectors",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Sampling weight vectors",
    "text": "Sampling weight vectors\n\nIn standard backpropagation we keep moving the weights in the direction that decreases the cost.\n\ni.e. the direction that increases the log likelihood plus the log prior, summed over all training cases.\nEventually, the weights settle into a local minimum or get stuck on a plateau or just move so slowly that we run out of patience."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#one-method-for-sampling-weight-vectors",
    "href": "notes/dnn/dnn-10/l_10.html#one-method-for-sampling-weight-vectors",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "One method for sampling weight vectors",
    "text": "One method for sampling weight vectors\n\nSuppose we add some Gaussian noise to the weight vector after each update.\n\nSo the weight vector never settles down.\nIt keeps wandering around, but it tends to prefer low cost regions of the weight space.\nCan we say anything about how often it will visit each possible setting of the weights?"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#the-wonderful-property-of-markov-chain-monte-carlo",
    "href": "notes/dnn/dnn-10/l_10.html#the-wonderful-property-of-markov-chain-monte-carlo",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "The wonderful property of Markov Chain Monte Carlo",
    "text": "The wonderful property of Markov Chain Monte Carlo\n\nAmazing fact: If we use just the right amount of noise, and if we let the weight vector wander around for long enough before we take a sample, we will get an unbiased sample from the true posterior over weight vectors.\n\nThis is called a “Markov Chain Monte Carlo” method.\nMCMC makes it feasible to use full Bayesian learning with thousands of parameters.\n\nThere are related MCMC methods that are more complicated but more efficient:\n\nWe don’t need to let the weights wander around for so long before we get samples from the posterior."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#full-bayesian-learning-with-mini-batches",
    "href": "notes/dnn/dnn-10/l_10.html#full-bayesian-learning-with-mini-batches",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Full Bayesian learning with mini-batches",
    "text": "Full Bayesian learning with mini-batches\n\nIf we compute the gradient of the cost function on a random mini-batch we will get an unbiased estimate with sampling noise.\n\nMaybe we can use the sampling noise to provide the noise that an MCMC method needs!\n\nIn (Ahn, Korattikara, and Welling 2012) 2 the authors showed how to do this fairly efficiently.\n\nSo full Bayesian learning is now possible with lots of parameters."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#two-ways-to-average-models",
    "href": "notes/dnn/dnn-10/l_10.html#two-ways-to-average-models",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Two ways to average models",
    "text": "Two ways to average models\n\nMIXTURE: We can combine models by averaging their output probabilities:\n\nModel A: .3 .2 .5 Model B: .1 .8 .1 Combined .2 .5 .3\n\nPRODUCT: We can combine models by taking the geometric means of their output probabilities:\n\nModel A: .3 .2 .5 Model B: .1 .8 .1 Combined .03 .16 .05 /sum"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#dropout-an-efficient-way-to-average-many-large-neural-nets",
    "href": "notes/dnn/dnn-10/l_10.html#dropout-an-efficient-way-to-average-many-large-neural-nets",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Dropout: An efficient way to average many large neural nets",
    "text": "Dropout: An efficient way to average many large neural nets\npreprint (Hinton et al. 2012)\nTODO add picture\n\nConsider a neural net with one hidden layer.\nEach time we present a training example, we randomly omit each hidden unit with probability 0.5.\nSo we are randomly sampling from 2^H different architectures. – All architectures share weights"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#dropout-as-a-form-of-model-averaging",
    "href": "notes/dnn/dnn-10/l_10.html#dropout-as-a-form-of-model-averaging",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Dropout as a form of model averaging",
    "text": "Dropout as a form of model averaging\n\nWe sample from 2^H models. So only a few of the models ever get trained, and they only get one training example.\n\nThis is as extreme as bagging can get.\n\nThe sharing of the weights means that every model is very strongly regularized.\n\nIt’s a much better regularizer than L2 or L1 penalties that pull the weights towards zero."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#but-what-do-we-do-at-test-time",
    "href": "notes/dnn/dnn-10/l_10.html#but-what-do-we-do-at-test-time",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "But what do we do at test time?",
    "text": "But what do we do at test time?\n\nWe could sample many different architectures and take the geometric mean of their output distributions.\nIt better to use all of the hidden units, but to halve their outgoing weights.\n\nThis exactly computes the geometric mean of the predictions of all 2^H models."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#what-if-we-have-more-hidden-layers",
    "href": "notes/dnn/dnn-10/l_10.html#what-if-we-have-more-hidden-layers",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "What if we have more hidden layers?",
    "text": "What if we have more hidden layers?\n\nUse dropout of 0.5 in every layer.\nAt test time, use the “mean net” that has all the outgoing weights halved.\n\nThis is not exactly the same as averaging all the separate dropped out models, but it’s a pretty good approximation, and its fast.\nAlternatively, run the stochastic model several times on the same input.\n\nThis gives us an idea of the uncertainty in the answer."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#what-about-the-input-layer",
    "href": "notes/dnn/dnn-10/l_10.html#what-about-the-input-layer",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "What about the input layer?",
    "text": "What about the input layer?\n\nIt helps to use dropout there too, but with a higher probability of keeping an input unit.\n\nThis trick is already used by the denoising autoencoders developed by Pascal Vincent, Hugo Larochelle and Yoshua Bengio."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#how-well-does-dropout-work",
    "href": "notes/dnn/dnn-10/l_10.html#how-well-does-dropout-work",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "How well does dropout work?",
    "text": "How well does dropout work?\n\nThe record breaking object recognition net developed by Alex Krizhevsky (see lecture 5) uses dropout and it helps a lot.\nIf your deep neural net is significantly overfitting, dropout will usually reduce the number of errors by a lot.\n\nAny net that uses “early stopping” can do better by using dropout (at the cost of taking quite a lot longer to train).\n\nIf your deep neural net is not overfitting you should be using a bigger one!"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#another-way-to-think-about-dropout",
    "href": "notes/dnn/dnn-10/l_10.html#another-way-to-think-about-dropout",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Another way to think about dropout",
    "text": "Another way to think about dropout\n\nIf a hidden unit knows which other hidden units are present, it can co-adapt to them on the training data.\n\nBut complex co-adaptations are likely to go wrong on new test data.\nBig, complex conspiracies are not robust.\n\nIf a hidden unit has to work well with combinatorially many sets of co-workers, it is more likely to do something that is individually useful.\n\nBut it will also tend to do something that is marginally useful given what its co-workers achieve.\n\n\n\n\n\nsimplified mixture of experts architecture\noverfitting\nnot-overfitting"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#footnotes",
    "href": "notes/dnn/dnn-10/l_10.html#footnotes",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is a better cost function based on a mixture model.↩︎\npdf↩︎"
  },
  {
    "objectID": "notes/dnn/dnn-10/r1.html",
    "href": "notes/dnn/dnn-10/r1.html",
    "title": "Deep Neural Networks — Readings I for Lesson 10",
    "section": "",
    "text": "Serial ensambles like bagging operate using a cooperative loss function for the ensemble. Parallel ensembles should use a competitive loss function for the ensamle. Neural networks are slow to train and are best combined in parallel. The paper considers the type of losses function that promotes.\n\n\n\n\nIn (Nowlan and Hinton 1990), the authors offer a fascinating insights into the mechanics of ensembling, which is the approach of aggregating several lower capacity models into a single high capacity model.\nThe ensembling is a form of the divide and conquer heuristic.\n\nBagging uses a parallel approach where we average the outcomes. By Fisher’s fundamental theorem of natural selection the more diverse the models in an ensemble the faster it will the ensemble will learn.\nBoosting uses a sequential approach - each subsequent model works on the residual of the previous\n\nIn the course, Hinton make the case that when ensembling neural networks one should use competition, rather than the cooperative approach that is the basis of bagging and boosting submodels.\nThe big idea here is to get the sub model to specialize thus getting the ML system to converge faster.\n\nConverging faster means shorter training time or\nGetting the same results on a smaller dataset — and not having enough data is a common problem.\n\nMixture of experts involves lots of overhead and so it is an approach we see less often. It is a way of squeezing more accuracy out of an ML model and so it tends to re-surface when a problem has matured and people are looking for small gains from better ML ops.\n\nUnable to display PDF file. Download instead.\n\n\n\n“We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.”\n\n\n\nThis paper is the first time I read about ensembles - and was an introduction. I would later read much more in Intorduction to statistical learning using R1. As time goes by ensembles keep getting more of my attention. We put them to work in setting that provides higher capacity models for small data setting. Also, the gating network is like a meta model which may be adapted to quantify uncertainty for each expert at the training case level.\nThe architecture shown bellow uses expert networks trained on a vowel discrimination (classification) task alongside a gating network whose responsibility is to pick the best classifier for the input.\n\n\n\n\n\nensemble architecture\n\n\nI had been familiar with the idea that the gating network is responsible to convert the output of the experts to the actual experts. It turns out that the gating network also needs to learn which expert is better on a given type of input, and that it also controls the data expert get. This allocation can be hard (each training case goes to one expert) or soft (several experts are allocated). I also noted that some of the prior work was authored by Bastro, the leading authority on Reinforcement Learning. In prior work the gating network the learn to allocate training cases to one or a few expert - which allows them specialize (the weights are decoupled) also learns to The earlier idea is to utilize or learn to partition the training data so that one can train specialized models that are local experts on the problem space and then use some linear combination of the expert’s predictions to make predictions. But using such a linear combination requires that the expert cancel each other’s output.\n\n\n\n\nE^c= ||\\vec{d^c} -\\sum_i p_i^c \\vec o_i^c||^2\n\\tag{1}\nwhere :\n\n\\vec o_i^c is the output vector of expert i on case c.\n\\vec d_c is the desired output for case c.\n\nThe authors say that the cooperative loss function in (1) foster an unwanted coupling between the experts, in the sense that a change in one expert’s weights will create a residual loss seen by the other experts in (1). This leads to cooperation but each expert has learn to neutralize the residual it sees from the others experts. So in both cases all models contribute to the inference, instead of just one or a few, which is counter to the idea of being an expert on a subset of the data.\n\n\n\nIn (Jacobs et al. 1991) the authors used a hard selection mechanism by modifying the objective function to encourage competition and foster greater specialization by using only activate one expert at a time. This paper suggest that it is enough to modify the loss so that the experts compete. The idea being that “the selector acts as a multiple input, single output stochastic switch; the probability that the switch will select the output from expert j is p_j governed by:\n\nE^c = &lt;||\\vec d^c - \\vec o^c ||&gt; =\\sum_i\\ p_i^c||\\vec d^c- \\vec o_i||^2\n\\tag{2}\nwhere :\n\np_j = \\frac{e^{x_j}}{\\sum_i e^x_i}\n\nsoon we are shown a much better loss function:\n\n\n\ndoes not encourage cooperation rather than specialization, which required using many experts in each prediction. Later work added penalty terms in the objective function to gate a single active exert in the prediction. Jacobs, Jordan, and Barton, 1990. The paper offers an alternative error function that encourages specialization.\nThe difference difference between the error functions.\n\n\n\n\nE^c= -log\\sum_i\\ p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}\n\\tag{3}\n\n\n\nThe error defined in Equation 2 is simply the negative log probability of generating the desired output vector under the mixture of Gaussian’s model described at the end of the next section.\nTo see why this error function works better, it is helpful to compare the derivatives of the two error functions with respect to the output of an expert. From from Equation 2 we get:\n\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -2p_i^c(\\vec d^c-\\vec o_c^c)\n\\tag{4}\nwhile the derivative from Equation 3 gives us:\n\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -\\bigg[\\frac{p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}}{\\sum_j p_j^c e^{\\frac{1}{2}||d^c- \\vec o_j||^2}}\\bigg](\\vec d^c-\\vec o_c^c)\n\\tag{5}\nIn Equation 4 the term \\vec p^c_i is used to weigh the derivative for expert i, while in equation 5 the weighting term takes into account how well expert i does relative to other experts, which is a more useful measure of the relevance of expert i to training case c, especially early in the training. Suppose, that the gating network initially gives equal weights to all experts and ||d^c-\\vec o_j||&gt;1 for all the experts. Equation 4 will adapt the best-fitting expert the slowest, whereas Equation 5 will adapt it the fastest.\n\n\n\nIf two loss function are not enough, the authors now suggest a third loss function. This loss looks at the distance from the average vector. \nlogP^c= -log\\sum_i\\ p_i^c K e^{-\\frac{1}{2}||\\vec\\mu_i- \\vec o^c||^2}\n\\tag{6}"
  },
  {
    "objectID": "notes/dnn/dnn-10/r1.html#reading-adaptive-mixtures-of-local-experts",
    "href": "notes/dnn/dnn-10/r1.html#reading-adaptive-mixtures-of-local-experts",
    "title": "Deep Neural Networks — Readings I for Lesson 10",
    "section": "",
    "text": "Serial ensambles like bagging operate using a cooperative loss function for the ensemble. Parallel ensembles should use a competitive loss function for the ensamle. Neural networks are slow to train and are best combined in parallel. The paper considers the type of losses function that promotes.\n\n\n\n\nIn (Nowlan and Hinton 1990), the authors offer a fascinating insights into the mechanics of ensembling, which is the approach of aggregating several lower capacity models into a single high capacity model.\nThe ensembling is a form of the divide and conquer heuristic.\n\nBagging uses a parallel approach where we average the outcomes. By Fisher’s fundamental theorem of natural selection the more diverse the models in an ensemble the faster it will the ensemble will learn.\nBoosting uses a sequential approach - each subsequent model works on the residual of the previous\n\nIn the course, Hinton make the case that when ensembling neural networks one should use competition, rather than the cooperative approach that is the basis of bagging and boosting submodels.\nThe big idea here is to get the sub model to specialize thus getting the ML system to converge faster.\n\nConverging faster means shorter training time or\nGetting the same results on a smaller dataset — and not having enough data is a common problem.\n\nMixture of experts involves lots of overhead and so it is an approach we see less often. It is a way of squeezing more accuracy out of an ML model and so it tends to re-surface when a problem has matured and people are looking for small gains from better ML ops.\n\nUnable to display PDF file. Download instead.\n\n\n\n“We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.”\n\n\n\nThis paper is the first time I read about ensembles - and was an introduction. I would later read much more in Intorduction to statistical learning using R1. As time goes by ensembles keep getting more of my attention. We put them to work in setting that provides higher capacity models for small data setting. Also, the gating network is like a meta model which may be adapted to quantify uncertainty for each expert at the training case level.\nThe architecture shown bellow uses expert networks trained on a vowel discrimination (classification) task alongside a gating network whose responsibility is to pick the best classifier for the input.\n\n\n\n\n\nensemble architecture\n\n\nI had been familiar with the idea that the gating network is responsible to convert the output of the experts to the actual experts. It turns out that the gating network also needs to learn which expert is better on a given type of input, and that it also controls the data expert get. This allocation can be hard (each training case goes to one expert) or soft (several experts are allocated). I also noted that some of the prior work was authored by Bastro, the leading authority on Reinforcement Learning. In prior work the gating network the learn to allocate training cases to one or a few expert - which allows them specialize (the weights are decoupled) also learns to The earlier idea is to utilize or learn to partition the training data so that one can train specialized models that are local experts on the problem space and then use some linear combination of the expert’s predictions to make predictions. But using such a linear combination requires that the expert cancel each other’s output.\n\n\n\n\nE^c= ||\\vec{d^c} -\\sum_i p_i^c \\vec o_i^c||^2\n\\tag{1}\nwhere :\n\n\\vec o_i^c is the output vector of expert i on case c.\n\\vec d_c is the desired output for case c.\n\nThe authors say that the cooperative loss function in (1) foster an unwanted coupling between the experts, in the sense that a change in one expert’s weights will create a residual loss seen by the other experts in (1). This leads to cooperation but each expert has learn to neutralize the residual it sees from the others experts. So in both cases all models contribute to the inference, instead of just one or a few, which is counter to the idea of being an expert on a subset of the data.\n\n\n\nIn (Jacobs et al. 1991) the authors used a hard selection mechanism by modifying the objective function to encourage competition and foster greater specialization by using only activate one expert at a time. This paper suggest that it is enough to modify the loss so that the experts compete. The idea being that “the selector acts as a multiple input, single output stochastic switch; the probability that the switch will select the output from expert j is p_j governed by:\n\nE^c = &lt;||\\vec d^c - \\vec o^c ||&gt; =\\sum_i\\ p_i^c||\\vec d^c- \\vec o_i||^2\n\\tag{2}\nwhere :\n\np_j = \\frac{e^{x_j}}{\\sum_i e^x_i}\n\nsoon we are shown a much better loss function:\n\n\n\ndoes not encourage cooperation rather than specialization, which required using many experts in each prediction. Later work added penalty terms in the objective function to gate a single active exert in the prediction. Jacobs, Jordan, and Barton, 1990. The paper offers an alternative error function that encourages specialization.\nThe difference difference between the error functions.\n\n\n\n\nE^c= -log\\sum_i\\ p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}\n\\tag{3}\n\n\n\nThe error defined in Equation 2 is simply the negative log probability of generating the desired output vector under the mixture of Gaussian’s model described at the end of the next section.\nTo see why this error function works better, it is helpful to compare the derivatives of the two error functions with respect to the output of an expert. From from Equation 2 we get:\n\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -2p_i^c(\\vec d^c-\\vec o_c^c)\n\\tag{4}\nwhile the derivative from Equation 3 gives us:\n\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -\\bigg[\\frac{p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}}{\\sum_j p_j^c e^{\\frac{1}{2}||d^c- \\vec o_j||^2}}\\bigg](\\vec d^c-\\vec o_c^c)\n\\tag{5}\nIn Equation 4 the term \\vec p^c_i is used to weigh the derivative for expert i, while in equation 5 the weighting term takes into account how well expert i does relative to other experts, which is a more useful measure of the relevance of expert i to training case c, especially early in the training. Suppose, that the gating network initially gives equal weights to all experts and ||d^c-\\vec o_j||&gt;1 for all the experts. Equation 4 will adapt the best-fitting expert the slowest, whereas Equation 5 will adapt it the fastest.\n\n\n\nIf two loss function are not enough, the authors now suggest a third loss function. This loss looks at the distance from the average vector. \nlogP^c= -log\\sum_i\\ p_i^c K e^{-\\frac{1}{2}||\\vec\\mu_i- \\vec o^c||^2}\n\\tag{6}"
  },
  {
    "objectID": "notes/dnn/dnn-10/r1.html#my-wrap-up",
    "href": "notes/dnn/dnn-10/r1.html#my-wrap-up",
    "title": "Deep Neural Networks — Readings I for Lesson 10",
    "section": "My wrap up 🎬",
    "text": "My wrap up 🎬\nI may not fully grasped all the ideas behind this loss and it requires reading additional papers as it was not covered in the lectures. The results parts compares number of epochs needed for different models ensembles and neural networks to reach some level of accuracy on the validation set. The application is also rather complex, but the vowel clustering task itself seems rather simple.\nI was glad I went over this aper as the notions of faster training, competitive loss, associative learning seem to resurface in rather varied contexts2 and knowing this results and paper is a decent starting point in the area.\n\nReferences\n\n\nJacobs, Robert A, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. “Adaptive Mixtures of Local Experts.” Neural Computation 3 (1): 79–87. https://doi.org/10.1162/neco.1991.3.1.79.\n\n\nNowlan, Steven, and Geoffrey E Hinton. 1990. “Evaluation of Adaptive Mixtures of Competing Experts.” In Advances in Neural Information Processing Systems, edited by R. P. Lippmann, J. Moody, and D. Touretzky. Vol. 3. Morgan-Kaufmann. https://proceedings.neurips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf.\n\n\n\n\n\nensemble architecture"
  },
  {
    "objectID": "notes/dnn/dnn-10/r1.html#footnotes",
    "href": "notes/dnn/dnn-10/r1.html#footnotes",
    "title": "Deep Neural Networks — Readings I for Lesson 10",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ncitation needed↩︎\nevolution of language, reinforcement learning, game theory, social dilemmas↩︎"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06d.html",
    "href": "notes/dnn/dnn-06/l06d.html",
    "title": "Deep Neural Networks - Notes for lecture 6d",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06d.html#the-intuition-behind-separate-adaptive-learning-rates",
    "href": "notes/dnn/dnn-06/l06d.html#the-intuition-behind-separate-adaptive-learning-rates",
    "title": "Deep Neural Networks - Notes for lecture 6d",
    "section": "The intuition behind separate adaptive learning rates",
    "text": "The intuition behind separate adaptive learning rates\n\n\nIn a multilayer net, the appropriate learning rates can vary widely between weights:\n\nThe magnitudes of the gradients are often very different for different layers, especially if the initial weights are small.\nThe fan-in of a unit determines the size of the “overshoot” effects caused by simultaneously changing many of the incoming weights of a unit to correct the same error.\n\nSo use a global learning rate (set by hand) multiplied by an appropriate local gain that is determined empirically for each weight.\n\nGradients can get very small in the early layers of very deep nets.\nThe fan-in often varies widely between layers."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06d.html#one-way-to-determine-the-individual-learning-rates",
    "href": "notes/dnn/dnn-06/l06d.html#one-way-to-determine-the-individual-learning-rates",
    "title": "Deep Neural Networks - Notes for lecture 6d",
    "section": "One way to determine the individual learning rates",
    "text": "One way to determine the individual learning rates\n\nStart with a local gain of 1 for every weight.\n\nIncrease the local gain if the gradient for that weight does not change sign.\nUse small additive increases and multiplicative decreases (for mini-batch)\n\nThis ensures that big gains decay rapidly when oscillations start.\nIf the gradient is totally random the gain will hover around 1 when we increase by plus \\delta half the time and decrease by times 1-\\delta half the time.\n\n\n\n\\Delta w_{ij} = -\\epsilon  g_{ij} \\frac{∂E}{∂_{w_{ij}}}\n \n\\text{if } (\\frac{∂E}{∂_{w_{ij}}}(t)\\frac{∂E}{∂_{w_{ij}}}(t-1))&gt;0\n \n\\text{then } g_{ij}(t) = g_{ij}(t − 1) + .05\n\n\n\\text{else } g_{ij}(t) = g_{ij} δ (t − 1 ) \\times .95"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06d.html#tricks-for-making-adaptive-learning-rates-work-better",
    "href": "notes/dnn/dnn-06/l06d.html#tricks-for-making-adaptive-learning-rates-work-better",
    "title": "Deep Neural Networks - Notes for lecture 6d",
    "section": "Tricks for making adaptive learning rates work better",
    "text": "Tricks for making adaptive learning rates work better\n\nLimit the gains to lie in some reasonable range\n\ne.g. [0.1, 10] or [.01, 100]\n\nUse full batch learning or big minibatches\n\nThis ensures that changes in the sign of the gradient are not mainly due to the sampling error of a minibatch.\n\nAdaptive learning rates can be combined with momentum.\n\nUse the agreement in sign between the current gradient for a weight and the velocity for that weight (Jacobs, 1989).\n\n\nAdaptive learning rates only deal with axis-aligned effects.\n\nMomentum 🚀 does not care about the alignment of the axes."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html",
    "href": "notes/dnn/dnn-06/l06a.html",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#reminder-the-error-surface-for-a-linear-neuron",
    "href": "notes/dnn/dnn-06/l06a.html#reminder-the-error-surface-for-a-linear-neuron",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "Reminder: The error surface for a linear neuron",
    "text": "Reminder: The error surface for a linear neuron\n\n\n\n\n\nerror surface\n\n\n\nThe error surface lies in a space with a horizontal axis for each weight and one vertical axis for the error.\n\nFor a linear neuron with a squared error, it is a quadratic bowl.\nVertical cross-sections are parabolas.\n\nHorizontal cross-sections are ellipses.\n\n\nFor multi-layer, non-linear nets the error surface is much more complicated.\n\nBut locally, a piece of a quadratic bowl is usually a very good approximation."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#convergence-speed-of-full-batch-learning-when-the-error-surface-is-a-quadratic-bowl",
    "href": "notes/dnn/dnn-06/l06a.html#convergence-speed-of-full-batch-learning-when-the-error-surface-is-a-quadratic-bowl",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "Convergence speed of full batch learning when the error surface is a quadratic bowl",
    "text": "Convergence speed of full batch learning when the error surface is a quadratic bowl\n\n\n\n\n\nerror surface\n\n\n\nGoing downhill reduces the error, but the direction of steepest descent does not point at the minimum unless the ellipse is a circle.\n\nThe gradient is big in the direction in which we only want to travel a small distance.\n\nThe gradient is small in the direction in which we want to travel a large distance.\n\nEven for non-linear multi-layer nets, the error surface is locally quadratic, so the same speed issues apply."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#how-the-learning-goes-wrong",
    "href": "notes/dnn/dnn-06/l06a.html#how-the-learning-goes-wrong",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "How the learning goes wrong",
    "text": "How the learning goes wrong\n\n\n\n\n\nerror surface\n\n\n\nIf the learning rate is big, the weights slosh to and fro across the ravine.\n\nIf the learning rate is too big, this oscillation diverges.\n\nWhat we would like to achieve:\n\nMove quickly in directions with small but consistent gradients.\nMove slowly in directions with big but inconsistent gradients."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#stochastic-gradient-descent-sgd",
    "href": "notes/dnn/dnn-06/l06a.html#stochastic-gradient-descent-sgd",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "Stochastic gradient descent SGD",
    "text": "Stochastic gradient descent SGD\n\nIf the dataset is highly redundant, the gradient on the first half is almost identical to the gradient on the second half.\n\nSo instead of computing the full gradient, update the weights using the gradient on the first half and then get a gradient for the new weights on the second half.\nThe extreme version of this approach updates weights after each case. Its called online.\n\nMini-batches are usually better than online.\n\nLess computation is used updating the weights.\nComputing the gradient for many cases simultaneously uses matrix-matrix multiplies which are very efficient, especially on GPUs\n\nMini-batches need to be balanced for classes"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#two-types-of-learning-algorithm",
    "href": "notes/dnn/dnn-06/l06a.html#two-types-of-learning-algorithm",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "Two types of learning algorithm",
    "text": "Two types of learning algorithm\n\nIf we use the full gradient computed from all the training cases, there are many clever ways to speed up learning (e.g. non-linear conjugate gradient).\n\nThe optimization community has studied the general problem of optimizing smooth non-linear functions for many years.\n\nMultilayer neural nets are not typical of the problems they study so their methods may need a lot of adaptation.\n\n\nFor large neural networks with very large and highly redundant training sets, it is nearly always best to use mini-batch learning.\n\nThe mini-batches may need to be quite big when adapting fancy methods.\nBig mini-batches are more computationally efficient."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#a-basic-mini-batch-gradient-descent-algorithm",
    "href": "notes/dnn/dnn-06/l06a.html#a-basic-mini-batch-gradient-descent-algorithm",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "A basic mini-batch gradient descent algorithm",
    "text": "A basic mini-batch gradient descent algorithm\n\nGuess an initial learning rate.\n\nIf the error keeps geang worse or oscillates wildly, reduce the learning rate.\nIf the error is falling fairly consistently but slowly, increase the learning rate.\n\n\nWrite a simple program to automate this way of adjusting the learning rate.\nTowards the end of mini-batch learning it nearly always helps to turn down the learning rate.\n\nThis removes fluctuations in the final weights caused by the variations between minibatches.\n\n\nTurn down the learning rate when the error stops decreasing.\n\nUse the error on a separate validation set\n\n\n\n\n\nerror surface\nerror surface\nerror surface"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html",
    "href": "notes/dnn/dnn-06/l06b.html",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#be-careful-about-turning-down-the-learning-rate",
    "href": "notes/dnn/dnn-06/l06b.html#be-careful-about-turning-down-the-learning-rate",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Be careful about turning down the learning rate",
    "text": "Be careful about turning down the learning rate\n\n\n\n\nTurning down the learning rate reduces the random fluctuations in the error due to the different gradients on different mini-batches. -So we get a quick win.\n\nBut then we get slower learning.\n\nDon’t turn down the learning rate too soon!"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#initializing-the-weights",
    "href": "notes/dnn/dnn-06/l06b.html#initializing-the-weights",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Initializing the weights",
    "text": "Initializing the weights\n\nIf two hidden units have exactly the same bias and exactly the same incoming and outgoing weights, they will always get exactly the same gradient.\n\nSo they can never learn to be different features.\nWe break symmetry by initializing the weights to have small random values.\n\nIf a hidden unit has a big fan-in, small changes on many of its incoming weights can cause the learning to overshoot.\n\nWe generally want smaller incoming weights when the fan-in is big, so initialize the weights to be proportional to sqrt(fan-in).\n\nWe can also scale the learning rate the same way."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#shifting-the-inputs",
    "href": "notes/dnn/dnn-06/l06b.html#shifting-the-inputs",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Shifting the inputs",
    "text": "Shifting the inputs\n\n\n\n\n\nShifting the inputs\n\n\n\nWhen using steepest descent, shifting the input values makes a big difference. -It usually helps to transform each component of the input vector so that it has zero mean over the whole training set.\n\nThe hypberbolic tangent (which is 2*logistic -1) produces hidden activations that are roughly zero mean.\n-In this respect its beYer than the logistic."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#scaling-the-inputs",
    "href": "notes/dnn/dnn-06/l06b.html#scaling-the-inputs",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Scaling the inputs",
    "text": "Scaling the inputs\n\n\n\n\n\nScaling the inputs\n\n\n\nWhen using steepest descent, scaling the input values makes a big difference.\n\nIt usually helps to transform each component of the input vector so that it has unit variance over the whole training set."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#a-more-thorough-method-decorrelate-the-input-components",
    "href": "notes/dnn/dnn-06/l06b.html#a-more-thorough-method-decorrelate-the-input-components",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "A more thorough method: Decorrelate the input components",
    "text": "A more thorough method: Decorrelate the input components\n\nFor a linear neuron, we get a big win by decorrelating each component of the input from the other input components.\nThere are several different ways to decorrelate inputs. A reasonable method is to use Principal Components Analysis.\n\nDrop the principal components with the smallest eigenvalues.\n\nThis achieves some dimensionality reduction.\n\nDivide the remaining principal components by the square roots of their eigenvalues. For a linear neuron, this converts an axis aligned elliptical error surface into a circular one.\n\nFor a circular error surface, the gradient points straight towards the minimum."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#common-problems-that-occur-in-multilayer-networks",
    "href": "notes/dnn/dnn-06/l06b.html#common-problems-that-occur-in-multilayer-networks",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Common problems that occur in multilayer networks",
    "text": "Common problems that occur in multilayer networks\n\nIf we start with a very big learning rate, the weights of each hidden unit will all become very big and positive or very big and negative.\n\nThe error derivatives for the hidden units will all become tiny and the error will not decrease.\nThis is usually a plateau, but people often mistake it for a local minimum.\n\nIn classification networks that use a squared error or a cross-entropy error, the best guessing strategy is to make each output unit always produce an output equal to the proportion of time it should be a 1.\n\nThe network finds this strategy quickly and may take a long time to improve on it by making use of the input.\n\nThis is another plateau that looks like a local minimum."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#four-ways-to-speed-up-mini-batch-learning",
    "href": "notes/dnn/dnn-06/l06b.html#four-ways-to-speed-up-mini-batch-learning",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Four ways to speed up mini-batch learning",
    "text": "Four ways to speed up mini-batch learning\n\nUse “momentum”\n\nInstead of using the gradient to change the position of the weight “particle”, use it to change the velocity.\n\n\nUse separate adaptive learning rates for each parameter\n\nSlowly adjust the rate using the consistency of the gradient for that parameter.\n\nrmsprop: Divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n\nThis is the mini-batch version of just using the sign of the gradient.\n\n\nTake a fancy method from the optimization literature that makes use of curvature information (not this lecture)\n\nAdapt it to work for neural nets\nAdapt it to work for mini-batches.\n\n\n\n\n\nShifting the inputs\nScaling the inputs"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html",
    "href": "notes/dnn/dnn-08/l_08.html",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#how-much-can-we-reduce-the-error-by-moving-in-a-given-direction",
    "href": "notes/dnn/dnn-08/l_08.html#how-much-can-we-reduce-the-error-by-moving-in-a-given-direction",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "How much can we reduce the error by moving in a given direction?",
    "text": "How much can we reduce the error by moving in a given direction?\n\nIf we choose a direction to move in and we keep going in that direction, how much does the error decrease before it starts rising again? We assume the curvature is constant (i.e. it’s a quadratic error surface).\n\nAssume the magnitude of the gradient decreases as we move down the gradient (i.e. the error surface is convex upward).\n\nThe maximum error reduction depends on the ratio of the gradient to the curvature. So a good direction to move in is one with a high ratio of gradient to curvature, even if the gradient itself is small.\n\nHow can we find directions like these?"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#newtons-method",
    "href": "notes/dnn/dnn-08/l_08.html#newtons-method",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Newton’s method",
    "text": "Newton’s method\n\nThe basic problem with steepest descent on a quadratic error surface is that the gradient is not the direction we want to go in.\n\nIf the error surface has circular cross-sections, the gradient is fine.\nSo lets apply a linear transformation that turns ellipses into circles.\n\nNewton’s method multiplies the gradient vector by the inverse of the curvature matrix, H \n\\Delta w = − \\epsilon H(w)^{-1}\\frac{dE}{dw}dw\n\n\nOn a real quadratic surface it jumps to the minimum in one step.\nUnfortunately, with only a million weights, the curvature matrix has a trillion terms and it is totally infeasible to invert it."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#curvature-matrices",
    "href": "notes/dnn/dnn-08/l_08.html#curvature-matrices",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Curvature Matrices",
    "text": "Curvature Matrices\n\n\n\n\n\nCurvature Matrices\n\n\n\nEach element in the curvature matrix specifies how the gradient in one direction changes as we move in some other direction.\n\nThe off-diagonal terms correspond to twists in the error surface.\n\nThe reason steepest descent goes wrong is that the gradient for one weight gets messed up by the simultaneous changes to all the other weights.\nThe curvature matrix determines the sizes of these interactions."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#how-to-avoid-inverting-a-huge-matrix",
    "href": "notes/dnn/dnn-08/l_08.html#how-to-avoid-inverting-a-huge-matrix",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "How to avoid inverting a huge matrix",
    "text": "How to avoid inverting a huge matrix\n\nThe curvature matrix has too many terms to be of use in a big network.\n\nMaybe we can get some benefit from just using the terms along the leading diagonal (Le Cun). But the diagonal terms are only a tiny fraction of the interactions (they are the self-interactions).\n\nThe curvature matrix can be approximated in many different ways\n\nHessian-free methods, LBFGS, ….\n\nIn the HF method, we make an approximation to the curvature matrix and then, assuming that approximation is correct, we minimize the error using an efficient technique called conjugate gradient. Then we make another approximation to the curvature matrix and minimize again.\n\nFor RNNs its important to add a penalty for changing any of the hidden activities too much."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#conjugate-gradient",
    "href": "notes/dnn/dnn-08/l_08.html#conjugate-gradient",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Conjugate gradient",
    "text": "Conjugate gradient\n\nThere is an alternative to going to the minimum in one step by multiplying by the inverse of the curvature matrix.\n\nUse a sequence of steps each of which finds the minimum along one direction.\n\nEnsure that each new direction is “conjugate” to the previous directions so you do not mess up the minimization you already did.\n\n“conjugate” means that as you go in the new direction, you do not change the gradients in the previous directions."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#a-picture-of-conjugate-gradient",
    "href": "notes/dnn/dnn-08/l_08.html#a-picture-of-conjugate-gradient",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "A picture of conjugate gradient",
    "text": "A picture of conjugate gradient\n\n\n\n\n\nconjugate gradient\n\n\n\nThe gradient in the direction of the first step is zero at all points on the green line.\nSo if we move along the green line we don’t mess up the minimization we already did in the first direction."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#what-does-conjugate-gradient-achieve",
    "href": "notes/dnn/dnn-08/l_08.html#what-does-conjugate-gradient-achieve",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "What does conjugate gradient achieve?",
    "text": "What does conjugate gradient achieve?\n\nAfter N steps, conjugate gradient is guaranteed to find the minimum of an N-dimensional quadratic surface. Why?\nAfter many less than N steps it has typically got the error very close to the minimum value.\nConjugate gradient can be applied directly to a non-quadratic error surface and it usually works quite well (non-linear conjugate grad.)\nThe HF optimizer uses conjugate gradient for minimization on a genuinely quadratic surface where it excels.\nThe genuinely quadratic surface is the quadratic approximation to the true surface."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#modeling-text-advantages-of-working-with-characters",
    "href": "notes/dnn/dnn-08/l_08.html#modeling-text-advantages-of-working-with-characters",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Modeling text: Advantages of working with characters",
    "text": "Modeling text: Advantages of working with characters\n\nThe web is composed of character strings.\nAny learning method powerful enough to understand the world by reading the web ought to find it trivial to learn which strings make words (this turns out to be true, as we shall see).\n\nPre-processing text to get words is a big hassle\n\nWhat about morphemes (prefixes, suffixes etc)\nWhat about subtle effects like “sn” words?\nWhat about New York?\nWhat about Finnish: ymmartamattomyydellansakaan"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#an-obvious-recurrent-neural-net",
    "href": "notes/dnn/dnn-08/l_08.html#an-obvious-recurrent-neural-net",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "An obvious recurrent neural net",
    "text": "An obvious recurrent neural net\n\n\n\n\n\nsimple rnn"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#a-sub-tree-in-the-tree-of-all-character-strings",
    "href": "notes/dnn/dnn-08/l_08.html#a-sub-tree-in-the-tree-of-all-character-strings",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "A sub-tree in the tree of all character strings",
    "text": "A sub-tree in the tree of all character strings\n\n\n\n\n\nsub tree\n\n\n\nIf the nodes are implemented as hidden states in an RNN, different nodes can share structure because they use distributed representations.\nThe next hidden representation needs to depend on the conjunction of the current character and the current hidden representation"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#multiplicative-connections",
    "href": "notes/dnn/dnn-08/l_08.html#multiplicative-connections",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Multiplicative connections",
    "text": "Multiplicative connections\n\nInstead of using the inputs to the recurrent net to provide additive extra input to the hidden units, we could use the current input character to choose the whole hidden-to-hidden weight matrix.\n\nBut this requires 86x1500x1500 parameters\nThis could make the net overfit.\n\nCan we achieve the same kind of multiplicative interaction using fewer parameters?\n\nWe want a different transition matrix for each of the 86 characters, but we want these 86 character-specific weight matrices to share parameters (the characters 9 and 8 should have similar matrices)."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#using-factors-to-implement-multiplicative-interactions",
    "href": "notes/dnn/dnn-08/l_08.html#using-factors-to-implement-multiplicative-interactions",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Using factors to implement multiplicative interactions",
    "text": "Using factors to implement multiplicative interactions\n\n\n\n\n\nusing factors\n\n\n\nWe can get groups a and b to interact multiplicatively by using “factors”.\n\nEach factor first computes a weighted sum for each of its input groups.\nThen it sends the product of the weighted sums to its output group.\n\n\n\n\\begin{aligned}\n  \\color{blue}{c_f} &= \\color{red}{(b^T w_f)}\\color{green}{(a^T a_f)} v_f  \n\\end{aligned}\n - bkue - vector of inputs to group c - red - scalar input to f from group b - green - scalar input to f from group a"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#using-factors-to-implement-a-set-of-basis-matrices",
    "href": "notes/dnn/dnn-08/l_08.html#using-factors-to-implement-a-set-of-basis-matrices",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Using factors to implement a set of basis matrices",
    "text": "Using factors to implement a set of basis matrices\n\nWe can think about factors another way:\n\nEach factor defines a rank 1 transition matrix from a to c.\n\n\n\n\\begin{aligned}\n  c_f &= (b^T w_f)(a^T a_f)v_f  \\\\\n  c_f &= \\color{purple}{(b^T w_f)}\\color{pink}{(u_fv_f^T)}a  \\\\\n  c &= \\big(\\sum_f(b^T w_f)(u_f v_f^T)\\big)a\n\\end{aligned}\n\n\npurple - scaler coefficient\npink - outer product transition matrix with rank 1"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#using-3-way-factors-to-allow-a-character-to-create-a-whole-transition-matrix",
    "href": "notes/dnn/dnn-08/l_08.html#using-3-way-factors-to-allow-a-character-to-create-a-whole-transition-matrix",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Using 3-way factors to allow a character to create a whole transition matrix",
    "text": "Using 3-way factors to allow a character to create a whole transition matrix\n\n\n\nUsing 3-way factors\n\n\nEach character, k, determines a gain w_{kf} for each of these matrices."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#training-the-character-model",
    "href": "notes/dnn/dnn-08/l_08.html#training-the-character-model",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Training the character model",
    "text": "Training the character model\n\nIlya Sutskever used 5 million strings of 100 characters taken from wikipedia. For each string he starts predicting at the 11th character.\nUsing the HF optimizer, it took a month on a GPU board to get a really good model.\nIlya’s current best RNN is probably the best single model for character prediction (combinations of many models do better).\nIt works in a very different way from the best other models.\n\nIt can balance quotes and brackets over long distances. Models that rely on matching previous contexts cannot do this."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#how-to-generate-character-strings-from-the-model",
    "href": "notes/dnn/dnn-08/l_08.html#how-to-generate-character-strings-from-the-model",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "How to generate character strings from the model",
    "text": "How to generate character strings from the model\n\nStart the model with its default hidden state.\nGive it a “burn-in” sequence of characters and let it update its hidden state after each character.\nThen look at the probability distribution it predicts for the next character.\nPick a character randomly from that distribution and tell the net that this was the character that actually occurred.\n\ni.e. tell it that its guess was correct, whatever it guessed.\n\nContinue to let it pick characters until bored.\nLook at the character strings it produces to see what it “knows”.\n\n\nHe was elected President during the Revolutionary War and forgave Opus Paul at Rome. The regime of his crew of England, is now Arab women’s icons in and the demons that use something between the characters‘ sisters in lower coil trains were always operated on the line of the ephemerable street, respectively, the graphic or other facility for deformation of a given proportion of large segments at RTUS). The B every chord was a “strongly cold internal palette pour even the white blade.”"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#some-completions-produced-by-the-model",
    "href": "notes/dnn/dnn-08/l_08.html#some-completions-produced-by-the-model",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Some completions produced by the model",
    "text": "Some completions produced by the model\n\nSheila thrunges (most frequent)\nPeople thrunge (most frequent next character is space)\nShiela, Thrungelini del Rey (first try)\nThe meaning of life is literary recognition. (6th try)\nThe meaning of life is the tradition of the ancient human reproduction: it is less favorable to the good boy for when to remove her bigger. (one of the first 10 tries for a model trained for longer)."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#what-does-it-know",
    "href": "notes/dnn/dnn-08/l_08.html#what-does-it-know",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "What does it know?",
    "text": "What does it know?\n\nIt knows a huge number of words and a lot about proper names, dates, and numbers.\nIt is good at balancing quotes and brackets.\n\nIt can count brackets: none, one, many\n\nIt knows a lot about syntax but its very hard to pin down exactly what form this knowledge has.\nIts syntactic knowledge is not modular.\nIt knows a lot of weak semantic associations\nE.g. it knows Plato is associated with Wittgenstein and cabbage is associated with vegetable."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#rnns-for-predicting-the-next-word",
    "href": "notes/dnn/dnn-08/l_08.html#rnns-for-predicting-the-next-word",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "RNNs for predicting the next word",
    "text": "RNNs for predicting the next word\n\nTomas Mikolov and his collaborators have recently trained quite large RNNs on quite large training sets using BPTT.\n\nThey do better than feed-forward neural nets.\nThey do better than the best other models.\nThey do even better when averaged with other models.\n\nRNNs require much less training data to reach the same level of performance as other models.\nRNNs improve faster than other methods as the dataset gets bigger.\n\nThis is going to make them very hard to beat."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#the-key-idea-of-echo-state-networks-perceptrons-again",
    "href": "notes/dnn/dnn-08/l_08.html#the-key-idea-of-echo-state-networks-perceptrons-again",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "The key idea of echo state networks (perceptrons again?)",
    "text": "The key idea of echo state networks (perceptrons again?)\n\nA very simple way to learn a feedforward network is to make the early layers random and fixed.\nThen we just learn the last layer which is a linear model that uses the transformed inputs to predict the target outputs.\n\nA big random expansion of the input vector can help.\n\nThe equivalent idea for RNNs is to fix the input \\to hidden connections and the hidden \\to hidden connections at random values and only learn the hidden \\to output connections.\n\nThe learning is then very simple (assuming linear output units).\nIts important to set the random connections very carefully so the RNN does not explode or die."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#setting-the-random-connections-in-an-echo-state-network",
    "href": "notes/dnn/dnn-08/l_08.html#setting-the-random-connections-in-an-echo-state-network",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Setting the random connections in an Echo State Network",
    "text": "Setting the random connections in an Echo State Network\n\nSet the hidden \\to hidden weights so that the length of the activity vector stays about the same after each iteration.\n\nThis allows the input to echo around the network for a long time.\n\nUse sparse connectivity (i.e. set most of the weights to zero).\n\nThis creates lots of loosely coupled oscillators.\n\nChoose the scale of the input \\to hidden connections very carefully.\n\nThey need to drive the loosely coupled oscillators without wiping out the information from the past that they already contain.\n\nThe learning is so fast that we can try many different scales for the weights and sparsenesses.\n\nThis is often necessary."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#a-simple-example-of-an-echo-state-network",
    "href": "notes/dnn/dnn-08/l_08.html#a-simple-example-of-an-echo-state-network",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "A simple example of an echo state network",
    "text": "A simple example of an echo state network\nINPUT SEQUENCE\nA real-valued time-varying value that specifies the frequency of a sine wave.\nTARGET OUTPUT SEQUENCE\nA sine wave with the currently specified frequency.\nLEARNING METHOD\nFit a linear model that takes the states of the hidden units as input and produces a single scalar output."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#beyond-echo-state-networks",
    "href": "notes/dnn/dnn-08/l_08.html#beyond-echo-state-networks",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Beyond echo state networks",
    "text": "Beyond echo state networks\n\nGood aspects of ESNs Echo state networks can be trained very fast because they just fit a linear model.\nThey demonstrate that its very important to initialize weights sensibly.\nThey can do impressive modeling of one-dimensional time-series.\n\nbut they cannot compete seriously for high-dimensional data like pre-processed speech.\n\nBad aspects of ESNs They need many more hidden units for a given task than an RNN that learns the hidden \\to hidden weights.\nIlya Sutskever (2012) has shown that if the weights are initialized using the ESN methods, RNNs can be trained very effectively.\n\nHe uses rmsprop with momentum\n\n\n\n\n\nCurvature Matrices\nconjugate gradient\nsimple rnn\nsub tree\nusing factors\nUsing 3-way factors"
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html",
    "href": "notes/dnn/dnn-16/l_16.html",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#modeling-the-joint-density-of-images-and-captions",
    "href": "notes/dnn/dnn-16/l_16.html#modeling-the-joint-density-of-images-and-captions",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Modeling the joint density of images and captions",
    "text": "Modeling the joint density of images and captions\n(Srivastava and Salakhutdinov, NIPS 2012)\n\nGoal: To build a joint density model of captions and standard computer vision feature vectors extracted from real photographs.\n\nThis needs a lot more computation than building a joint density model of labels and digit images!\n\n\n\nTrain a multilayer model of images.\nTrain a separate multilayer model of word-count vectors.\nThen add a new top layer that is connected to the top layers of both individual models.\n\nUse further joint training of the whole system to allow each modality to improve the earlier layers of the other modality.\n\n\n\nInstead of using a deep belief net, use a deep Boltzmann machine that has symmetric connections between all pairs of layers.\n\nFurther joint training of the whole DBM allows each modality to improve the earlier layers of the other modality.\nThat’s why they used a DBM.\nThey could also have used a DBN and done generative fine-tuning with contrastive wake-sleep.\n\nBut how did they pre-train the hidden layers of a deep Boltzmann Machine?\n\nStandard pre-training leads to composite model that is a DBN not a DBM."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#combining-three-rbms-to-make-a-dbm",
    "href": "notes/dnn/dnn-16/l_16.html#combining-three-rbms-to-make-a-dbm",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Combining three RBMs to make a DBM",
    "text": "Combining three RBMs to make a DBM\n\n\n\n\n\nCombining RBMs into a DBM\n\n\n\nThe top and bottom RBMs must be pretrained with the weights in one direction twice as big as in the other direction.\n\nThis can be justified!\n\nThe middle layers do geometric model averaging."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#why-convolutional-neural-networks-are-doomed",
    "href": "notes/dnn/dnn-16/l_16.html#why-convolutional-neural-networks-are-doomed",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Why convolutional neural networks are doomed",
    "text": "Why convolutional neural networks are doomed\n\nPooling loses the precise spatial relationships between higher-level parts such as a nose and a mouth.\n\nThe precise spatial relationships are needed for identity recognition.\nOverlapping the pools helps a bit.\n\nConvolutional nets that just use translations cannot extrapolate their understanding of geometric relationships to radically new viewpoints.\n\nPeople are very good at extrapolating. After seeing a new shape once they can recognize it from a different viewpoint."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#the-hierarchical-coordinate-frame-approach",
    "href": "notes/dnn/dnn-16/l_16.html#the-hierarchical-coordinate-frame-approach",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "The hierarchical coordinate frame approach",
    "text": "The hierarchical coordinate frame approach\n\n\n\n\n\nhierarchical frames\n\n\n\nUse a group of neurons to represent the conjunction of the shape of a feature and its pose relative to the retina.\n\nThe pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic coordinate frame of the feature.\n\nRecognize larger features by using the consistency of the poses of their parts."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#two-layers-in-a-hierarchy-of-parts",
    "href": "notes/dnn/dnn-16/l_16.html#two-layers-in-a-hierarchy-of-parts",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Two layers in a hierarchy of parts",
    "text": "Two layers in a hierarchy of parts\n\n\n\n\n\ntwo_layer_hierarchy\n\n\n\nA higher level visual entity is present if several lower level visual entities can agree on their predictions for its pose (inverse computer graphics!)"
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#a-crucial-property-of-the-pose-vectors",
    "href": "notes/dnn/dnn-16/l_16.html#a-crucial-property-of-the-pose-vectors",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "A crucial property of the pose vectors",
    "text": "A crucial property of the pose vectors\n\nThey allow spatial transformations to be modeled by linear operations.\n\nThis makes it easy to learn a hierarchy of visual entities.\nIt makes it easy to generalize across viewpoints.\n\nThe invariant geometric properties of a shape are in the weights, not in the activities.\n\nThe activities are equivariant: As the pose of the object varies, the activities all vary.\nThe percept of an object changes as the viewpoint changes."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#evidence-that-our-visual-systems-impose-coordinate-frames-in-order-to-represent-shapes-after-irvin-rock",
    "href": "notes/dnn/dnn-16/l_16.html#evidence-that-our-visual-systems-impose-coordinate-frames-in-order-to-represent-shapes-after-irvin-rock",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Evidence that our visual systems impose coordinate frames in order to represent shapes (after Irvin Rock)",
    "text": "Evidence that our visual systems impose coordinate frames in order to represent shapes (after Irvin Rock)\n\n\n\nWhat country is this? Hint: Sarah Palin\nThe square and the diamond are very different percepts that make different properties obvious."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#let-machine-learning-figure-out-the-hyper-parameters",
    "href": "notes/dnn/dnn-16/l_16.html#let-machine-learning-figure-out-the-hyper-parameters",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Let machine learning figure out the hyper-parameters!",
    "text": "Let machine learning figure out the hyper-parameters!\n(Snoek, Larochelle & Adams, NIPS 2012)\n\nOne of the commonest reasons for not using neural networks is that it requires a lot of skill to set hyper-parameters.\n\nNumber of layers\nNumber of units per layer\nType of unit\nWeight penalty\nLearning rate\nMomentum etc. etc.\n\nNaive grid search: Make a list of alternative values for each hyperparameter and then try all possible combinations.\n\nCan we do better than this?\n\nSampling random combinations: This is much better if some hyperparameters have no effect.\n\nIts a big waste to exactly repeat the settings of the other hyperparameters."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#machine-learning-to-the-rescue",
    "href": "notes/dnn/dnn-16/l_16.html#machine-learning-to-the-rescue",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Machine learning to the rescue",
    "text": "Machine learning to the rescue\n\nInstead of using random combinations of values for the hyper-parameters, why not look at the results so far?\n\nPredict regions of the hyperparameter space that might give better results.\nWe need to predict how well a new combination will do and also model the uncertainty of that prediction.\n\nWe assume that the amount of computation involved in evaluating one setting of the hyper-parameters is huge.\n\nMuch more than the work involved in building a model that predicts the result from knowing previous results with different settings of the hyper-parameters."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#gaussian-process-models",
    "href": "notes/dnn/dnn-16/l_16.html#gaussian-process-models",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Gaussian Process models",
    "text": "Gaussian Process models\n\nThese models assume that similar inputs give similar outputs.\n\nThis is a very weak but very sensible prior for the effects of hyper-parameters.\n\nFor each input dimension, they learn the appropriate scale for measuring similarity.\n\nIs 200 similar to 300?\nLook to see if they give similar results in the data so far.\n\nGP models do more than just predicting a single value.\n\nThey predict a Gaussian distribution of values.\n\nFor test cases that are close to several, consistent training cases the predictions are fairly sharp.\nFor test cases far from any training cases, the predictions have high variance."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#a-sensible-way-to-decide-what-to-try",
    "href": "notes/dnn/dnn-16/l_16.html#a-sensible-way-to-decide-what-to-try",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "A sensible way to decide what to try",
    "text": "A sensible way to decide what to try\n\n\n\nhedging.png\n\n\n\nKeep track of the best setting so far.\nAfter each experiment this might stay the same or it might improve if the latest result is the best.\nPick a setting of the hyperparameters such that the expected improvement in our best setting is big.\n\ndon’t worry about the downside (hedge funds!)"
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#how-well-does-bayesian-optimization-work",
    "href": "notes/dnn/dnn-16/l_16.html#how-well-does-bayesian-optimization-work",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "How well does Bayesian optimization work?",
    "text": "How well does Bayesian optimization work?\n\nIf you have the resources to run a lot of experiments, Bayesian optimization is much better than a person at finding good combinations of hyper-parameters.\n\nThis is not the kind of task we are good at.\nWe cannot keep in mind the results of 50 different experiments and see what they predict.\n\nIt’s much less prone to doing a good job for the method we like and a bad job for the method we are comparing with.\n\nPeople cannot help doing this. They try much harder for their own method because they know it ought to work better!"
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#why-we-cannot-predict-the-long-term-future",
    "href": "notes/dnn/dnn-16/l_16.html#why-we-cannot-predict-the-long-term-future",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Why we cannot predict the long-term future",
    "text": "Why we cannot predict the long-term future\n\nConsider driving at night. The number of photons you receive from the tail-lights of the car in front falls off as \\frac{1}{d^2}\nNow suppose there is fog.\n\nFor small distances its still \\frac{1}{d^2}\nBut for big distances its \\exp(-d) because fog absorbs a certain fraction of the photons per unit distance.\n\nSo the car in front becomes completely invisible at a distance at which our short-range \\frac{1}{d^2} model predicts it will be very visible.\n\nThis kills people."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#the-effect-of-exponential-progress",
    "href": "notes/dnn/dnn-16/l_16.html#the-effect-of-exponential-progress",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "The effect of exponential progress",
    "text": "The effect of exponential progress\n\nOver the short term, things change slowly and its easy to predict progress.\n\nWe can all make quite good guesses about what will be in the iPhone 6.\n\nBut in the longer run our perception of the future hits a wall, just like fog.\nSo the long term future of machine learning and neural nets is a total mystery.\n\nBut over the next five years, its highly probable that big, deep neural networks will do amazing things.\n\n\n\n\n\nCombining RBMs into a DBM\nhierarchical frames\ntwo_layer_hierarchy\nhedging.png"
  },
  {
    "objectID": "notes/dnn/dnn-glossery/glossary.html",
    "href": "notes/dnn/dnn-glossery/glossary.html",
    "title": "Glossary of terms for Deep Neural Networks",
    "section": "",
    "text": "Glossary of terms in Deep leaning and ML\n\nAccuracy\n\nThe fraction of predictions that a classification model got right.\n\nactivation\n\nemphasizes that neuron like a real neuron may be on or off. In reality a negative bias will create a threshold to activation, otherwise, the neuron will always produce output. Also called [value] or [output].\n\nactivation function\n\nThe activation function is an attempt to mimic the biological neuron’s output in response to it input. This is generally a non-linear function. Some examples are RELU, Sigmoid, Tanh, Leaky RELU, Maxout and there are many others. All other things being equal RELU has emerged as the preferred activation function to start with.\n\nAdaGrad\n\nA gradient descent learning algorithm that re-scales the gradients of each parameter, effectively giving each parameter an independent learning rate. c.f. (Duchi, Hazan, and Singer 2011).\n\nAnomaly detection\n\nThe process of identifying outliers that are considered candidates for removal from a dataset, Typically for being nonrepresentative high leverage points.\n\nAttention\n\nA mechanism that aggregate information from a set of inputs in a data-dependent manner. An attention mechanism might consist of a weighted sum over a set of inputs, where the weight for each input is computed by another part of the neural network.\n\nAttribute\n\nSynonym for feature.\n\nAutomation bias\n\nWhen a human decision-maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors.\n\nBackpropagation\n\nThe main algorithm for performing gradient descent on neural networks. First, the output values of each node are calculated (and cached) in a forward pass. Then, the partial derivative of the error with respect to each parameter is calculated in a backward pass through the graph.\n\nBagging\n\nA method to train an ensemble where each constituent model trains on a random subset of training examples sampled with replacement. E.g. a random forest is a collection of decision trees trained with bagging. The term bagging is short for bootstrap aggregating.\n\nBatch normalization\n\nNormalizing the input or output of the activation functions in a hidden layer. Batch normalization increases a network’s stability by protecting against outlier weights, enable higher learning rates and reduce **overfitting`.\n\nBatch size\n\nThe number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000. Batch size is usually fixed during training and inference by GPU memory constraints. Some frameworks like TensorFlow allow using dynamic batch sizes.\n\nBias term\n\na term that allows for the identification of the neuron threshold as the weight on a special, constant input.\n\nBayesian neural network\n\nA probabilistic neural network that accounts for uncertainty in weights and outputs. A Bayesian neural network relies on Bayes’ Theorem to calculate uncertainties in weights and predictions. A Bayesian neural network can be useful when it is important to quantify uncertainty, such as in models related to pharmaceuticals. Bayesian neural networks can also help prevent overfitting.\n\nBayesian optimization\n\nA probabilistic regression model technique for optimizing computationally expensive objective functions by instead optimizing a surrogate that quantifies the uncertainty via a Bayesian learning technique. Since Bayesian optimization is itself very expensive, it is usually used to optimize expensive-to-evaluate tasks that have a small number of parameters, such as selecting hyperparameters.\n\nBinning\n\nsynonym for bucketing\n\nBoltzmann machine\n\nan algorithm for learning the probability distribution on a set of inputs by means of weight changes using noisy responses.\n\nBoosting\n\nA machine learning technique that iteratively combines a set of simple and not very accurate classifiers (referred to as “weak” classifiers) into a classifier with high accuracy (a “strong” classifier) by upweighting the examples that the model is currently misclassifying.\n\nbucketing\n\nConverting a (usually continuous) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins.\n\ncategorical\n\nFeatures or columns in the data with a discrete set of possible values.\n\nConnection weight\n\nThe parameter which is used to set the importance to an input coming to a given neuron from another one.\n\nDelta rule\n\nthe simplest learning rule, in which weights are changed proportionally to the discrepancy between actual output and desired output.\n\nError surface\n\nthe surface in the weight space indicating how the error in the output of a neural network depends on these weights.\n\nFeature\n\na column in a training case Feed-in\n\n\nthe number of inputs for a unit\n\nFan out\n\nthe amount of spread in output from a neuron.\n\nHebb learning law\n\nmodification of a connection weight proportional to the activities of the input and output neurons.\n\nHopfield network\n\na network with symmetric connection weights and thresholding of neural response.\n\nInput\n\nis ambiguous, because more often, input is short for **input neuron`.\n\nInput unit\n\nspecial neuron receiving only input activity which is fed on to the rest of the network.\n\nLayer\n\na collection of neurons all of which receive input from a preceding set of neurons (or inputs), and send their outputs to other neurons or outside the net.\n\nLearning law\n\nrule for changing the connection weights in a neural network.\n\nLearning rate\n\namount by which the connection weights change at each learning step.\n\nMomentum\n\na term added to the weight change in back-propagation to achieve better learning by jumping out of local minima.\n\nNeuron\n\na synonym for unit emphasizing the analogy with real brains.\n\nOutput\n\nlike value but emphasizing that it’s different from the input.\n\nParameter\n\nthe weights and biases learned by the network. Additional parameters - which are not necessarily learned or not directly part of the network are called hyperparameters\n\nRecurrent neural network\n\none in which output activity is fed back into the input or hidden layers. Also called RNN Reinforcement training\n\n\nmodification of connection weights.\n\nTest set\n\nthe set of input and output patterns used to test if a neural network has been trained effectively.\n\nTraining set\n\nthe set of input-output patterns provided to train the network.\n\nTraining case\n\na row in the dataset is the most commonly used and is quite generic. Also called input and training example Training example\n\n\nemphasizes the analogy with human learning: we learn from examples.\n\nTraining point\n\nemphasizes that it’s a location in a high-dimensional space.\n\nUnit\n\na node in a neural network`. Nodes consists of an activation function, a weight, an input and output called the activation. The term unit emphasizes that it’s one component of a large network. Also referred to as a neuron** .\n\nValue\n\na synonym for activation, referencing the output value of the activation function (RELU, sigmoid, tanh, etc.) when acting on its input.\n\nWeight space\n\nA high dimensional space with each dimension corresponding to the weight of a single neuron`. Weight space corresponds to the space of all possible weights. Each point in the space is a collection of weights and each training case can be represented as a hyper-plane** passing through the origin. See also error surface\n\nloss function\n\nemphasizes that we’re minimizing it, without saying much about what the meaning of the number is.\n\nerror function\n\nemphasizes that it’s the extent to which the network gets things wrong.\n\nobjective function\n\nis very generic. This is the only one where it’s not clear whether we’re minimizing or maximizing it.\n\n\n\n\n\n\n\nReferences\n\nDuchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” Journal of Machine Learning Research 12 (7).\n\nReuseCC SA BY-NC-NDCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Glossary of Terms for {Deep} {Neural} {Networks}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-glossery/glossary.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Glossary of Terms for Deep Neural\nNetworks.” August 6, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-glossery/glossary.html."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html",
    "href": "notes/dnn/dnn-02/l02_.html",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nThis lecture covers the Perceptron convergence algorithm.\nA Perceptron is a primitive, neural network, but Hinton points out that they are still useful under the right condition.\n\nFor task that have very big feature vectors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeed-forward neural networks\n\n\n\nFeed forward networks are the subject of the first half of the course.\nThese are the most common type of neural network.\nThe first layer is the input and\nThe last layer is the output.\n\nIf there is more than one hidden layer, we call them “deep” neural networks.\n\nThey compute a series of transformations that change the similarities between cases.\n\nThe activities of the neurons in each layer are a non-linear function of the activities in the layer below.\n\n\n\n\n\n\n\n\nRecurrent neural networks\n\n\n\nThese have directed cycles in their connection graph.\n\nThat means you can sometimes get back to where you started by following the arrows.\n\nThey can have complicated dynamics and this can make them very difficult to train. – There is a lot of interest at present in finding efficient ways of training recurrent nets.\nThey are more biologically realistic.\n\n\n\n\n\n\n\nsequence to Sequence mapping\n\n\n\nRecurrent neural networks are a very natural way to model sequential data:\n\nThey are equivalent to very deep nets with one hidden layer per time slice.\nExcept that they use the same weights at every time slice and they get input at every time slice.\n\nThey have the ability to remember information in their hidden state for a long time.\n\nBut its very hard to train them to use this potential\n\n\n\n\n\n\nIn (Sutskever, Martens, and Hinton 2011) the authors trained a special type of RNN to predict the next character in a sequence.\nAfter training for a long time on a string of half a billion characters from English Wikipedia, he got it to generate new text.\n\nIt generates by predicting the probability distribution for the next character and then sampling a character from this distribution.\nThe next slide shows an example of the kind of text it generates. Notice how much it knows!\n\n\n\n\n\n\nIn 1974 Northern Denver had been overshadowed by CNL, and several Irish intelligence agencies in the Mediterranean region. However, on the Victoria, Kings Hebrew stated that Charles decided to escape during an alliance. The mansion house was completed in 1882, the second in its bridge are omitted, while closing is the proton reticulum composed below it aims, such that it is the blurring of appearing on any well-paid type of box printer.\n\n\n\n\n\nThese are like recurrent networks, but the connections between units are symmetrical (they have the same weight in both directions).\n\nJohn Hopfield (and others) realized that symmetric networks are much easier to analyze than recurrent networks. – They are also more restricted in what they can do. because they obey an energy function.\n\nFor example, they cannot model cycles.\n\n\nIn (Hopfield 1982), the author introduced symmetrically connected nets without hidden units that are now called Hopfield networks.\n\n\n\n\n\nThese are called “Boltzmann machines”.\n\nThey are much more powerful models than Hopfield nets.\nThey are less powerful than recurrent neural networks.\nThey have a beautifully simple learning algorithm.\n\nWe will cover Boltzmann machines towards the end of the course.\n\n\n\n\n\n\n\n\n\n\n\nSchematic\nDescription\n\n\n\n\n\nFeed forward nets - regression and classication for images and tabular data.\n\n\n\nRecurrent nets - sequence to sequence\n\n\n\nHopfield nets - associative memory using symmetric nets with no hidden units\n\n\n\nBoltzmann machines - symmetric nets with hidden units\n\n\n\ncredit: images from The Neural Network Zoo\n\n\n\n\n\n\n\n\nperceptrons\n\n\n\nwhy the bias can be implemented as a special input unit?\nbiases can be treated using weights using an input that is always one.\na threshold is equivalent to having a negative bias.\nwe can avoid having to figure out a separate learning rule for the bias by using a trick:\nA bias is exactly equivalent to a weight on an extra input line that always has an activation of 1.\n\n\n\ncode and image from: Implementing the Perceptron Algorithm in Python  In english:\n\nAdd an extra component with value 1 to each input vector. The “bias” weight on this component is minus the threshold. Now we can forget the threshold.\nPick training cases using any policy that ensures that every training case will keep getting picked.\n\nIf the output unit is correct, leave its weights alone.\nIf the output unit incorrectly outputs a zero, add the input vector to the weight vector.\nIf the output unit incorrectly outputs a 1, subtract the input vector from the weight vector. This is guaranteed to find a set of weights that gets the right answer for all the training cases if any such set exists. a full implementation of a perceptrons:\n\n\ndef perceptron(X, y, lr, epochs):\n    '''\n    X: inputs\n    y: labels\n    lr: learning rate\n    epochs: Number of iterations\n    m: number of training examples\n    n: number of features \n    '''\n    m, n = X.shape    \n    # Initializing parapeters(theta) to zeros.\n    # +1 in n+1 for the bias term.\n    theta = np.zeros((n+1,1))\n    \n    # list with misclassification count per iteration.\n    n_miss_list = []\n    \n    # Training.\n    for epoch in range(epochs):\n        # variable to store misclassified.\n        n_miss = 0\n        # looping for every example.\n        for idx, x_i in enumerate(X):\n            # Inserting 1 for bias, X0 = 1.\n            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          \n            # Calculating prediction/hypothesis.\n            y_hat = step_func(np.dot(x_i.T, theta))\n            # Updating if the example is misclassified.\n            if (np.squeeze(y_hat) - y[idx]) != 0:\n                theta += lr*((y[idx] - y_hat)*x_i)\n                # Incrementing by 1.\n                n_miss += 1\n        # Appending number of misclassified examples\n        # at every iteration.\n        n_miss_list.append(n_miss)\n    return theta, n_miss_list\n\n\n\n\n\n\n\n\nA point (a.k.a. location) and an arrow from the origin to that point, are often used interchangeably.\nA hyperplane is the high-dimensional equivalent of a plane in 3-D.\nThe scalar product or inner product between two vectors\n\nsum of element-wise products.\nThe scalar product between two vectors that have an angle of less than 90 degrees between them is positive.\n\nFor more than 90 degrees it’s negative.\n\n\n\n\n\n\n\n\nWeight-space\n\n\n\nHas one dimension per weight.\nA point in the space represents a particular setting of all the weights.\nAssuming that we have eliminated the threshold, each training case can be represented as a hyperplane through the origin.\n\nThe weights must lie on one side of this hyperplane to get the answer correct.\n\nEach training case defines a plane (shown as a black line)\n\nThe plane goes through the origin and is perpendicular to the input vector.\nOn one side of the plane the output is wrong because the scalar product of the weight vector with the input vector has the wrong sign.\n\n\n\n\n\n\n\nWe look at the geometrical interpretation which is the proof for the convergence of the Perceptron learning algorithm works. We are trying to find a decision surface by solving a convex optimization problem. The surface is a hyperplane represented by a line where on side is the correct set and the other is incorrect. The weight vectors form a cone: - This means that wights are closed under addition and positive scaler product. - At zero it is zero.\n\n\n\n\n\nThe cone of feasible solutions\n\n\nTo get all training cases right we need to find a point on the right side of all the planes. But there may not be any such point! If there are any weight vectors that get the right answer for all cases, they lie in a hyper-cone with its apex at the origin.\n\nThe average of two good weight vectors is a good weight vector.\nThe problem is convex.\n\nTwo training case form two hyper planes (shown in black). the good weights lie between them average of good weights is good.\nGeometry of learning using weight space:\n\nit has one dimension per weight (vertex in the graph).\n\na point in the space represents a particular setting of all the weights.\neach training case (once we eliminate the threshold/bias) is a hyper plane through the origin\nonce the threshold is eliminated each training case can be represented as a hyper place through the origin\nthe weight must lie on one side of this place to get the answer correct .\n\nCaveats:\n\nconvergence depends on the picking the right features\ndeep nets don’t use this procedure - as it only converges for single layer Perceptrons - but for more than one layer sum of a solution is not necessarily also a solution.\n\n\n\n\n\n\n\nThis story motivates the need for more powerful networks.\nThese ideas will be important in future lectures, when we’re working on moving beyond these limitations.\n\n\n\n\nFeed-forward neural networks\nRecurrent neural networks\nsequence to Sequence mapping\nperceptrons\nWeight-space\nThe cone of feasible solutions"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2a-types-of-neural-network-architectures",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2a-types-of-neural-network-architectures",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "Feed-forward neural networks\n\n\n\nFeed forward networks are the subject of the first half of the course.\nThese are the most common type of neural network.\nThe first layer is the input and\nThe last layer is the output.\n\nIf there is more than one hidden layer, we call them “deep” neural networks.\n\nThey compute a series of transformations that change the similarities between cases.\n\nThe activities of the neurons in each layer are a non-linear function of the activities in the layer below.\n\n\n\n\n\n\n\n\nRecurrent neural networks\n\n\n\nThese have directed cycles in their connection graph.\n\nThat means you can sometimes get back to where you started by following the arrows.\n\nThey can have complicated dynamics and this can make them very difficult to train. – There is a lot of interest at present in finding efficient ways of training recurrent nets.\nThey are more biologically realistic.\n\n\n\n\n\n\n\nsequence to Sequence mapping\n\n\n\nRecurrent neural networks are a very natural way to model sequential data:\n\nThey are equivalent to very deep nets with one hidden layer per time slice.\nExcept that they use the same weights at every time slice and they get input at every time slice.\n\nThey have the ability to remember information in their hidden state for a long time.\n\nBut its very hard to train them to use this potential\n\n\n\n\n\n\nIn (Sutskever, Martens, and Hinton 2011) the authors trained a special type of RNN to predict the next character in a sequence.\nAfter training for a long time on a string of half a billion characters from English Wikipedia, he got it to generate new text.\n\nIt generates by predicting the probability distribution for the next character and then sampling a character from this distribution.\nThe next slide shows an example of the kind of text it generates. Notice how much it knows!\n\n\n\n\n\n\nIn 1974 Northern Denver had been overshadowed by CNL, and several Irish intelligence agencies in the Mediterranean region. However, on the Victoria, Kings Hebrew stated that Charles decided to escape during an alliance. The mansion house was completed in 1882, the second in its bridge are omitted, while closing is the proton reticulum composed below it aims, such that it is the blurring of appearing on any well-paid type of box printer.\n\n\n\n\n\nThese are like recurrent networks, but the connections between units are symmetrical (they have the same weight in both directions).\n\nJohn Hopfield (and others) realized that symmetric networks are much easier to analyze than recurrent networks. – They are also more restricted in what they can do. because they obey an energy function.\n\nFor example, they cannot model cycles.\n\n\nIn (Hopfield 1982), the author introduced symmetrically connected nets without hidden units that are now called Hopfield networks.\n\n\n\n\n\nThese are called “Boltzmann machines”.\n\nThey are much more powerful models than Hopfield nets.\nThey are less powerful than recurrent neural networks.\nThey have a beautifully simple learning algorithm.\n\nWe will cover Boltzmann machines towards the end of the course.\n\n\n\n\n\n\n\n\n\n\n\nSchematic\nDescription\n\n\n\n\n\nFeed forward nets - regression and classication for images and tabular data.\n\n\n\nRecurrent nets - sequence to sequence\n\n\n\nHopfield nets - associative memory using symmetric nets with no hidden units\n\n\n\nBoltzmann machines - symmetric nets with hidden units\n\n\n\ncredit: images from The Neural Network Zoo"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2b-perceptrons-the-first-generation-of-neural-networks",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2b-perceptrons-the-first-generation-of-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "perceptrons\n\n\n\nwhy the bias can be implemented as a special input unit?\nbiases can be treated using weights using an input that is always one.\na threshold is equivalent to having a negative bias.\nwe can avoid having to figure out a separate learning rule for the bias by using a trick:\nA bias is exactly equivalent to a weight on an extra input line that always has an activation of 1.\n\n\n\ncode and image from: Implementing the Perceptron Algorithm in Python  In english:\n\nAdd an extra component with value 1 to each input vector. The “bias” weight on this component is minus the threshold. Now we can forget the threshold.\nPick training cases using any policy that ensures that every training case will keep getting picked.\n\nIf the output unit is correct, leave its weights alone.\nIf the output unit incorrectly outputs a zero, add the input vector to the weight vector.\nIf the output unit incorrectly outputs a 1, subtract the input vector from the weight vector. This is guaranteed to find a set of weights that gets the right answer for all the training cases if any such set exists. a full implementation of a perceptrons:\n\n\ndef perceptron(X, y, lr, epochs):\n    '''\n    X: inputs\n    y: labels\n    lr: learning rate\n    epochs: Number of iterations\n    m: number of training examples\n    n: number of features \n    '''\n    m, n = X.shape    \n    # Initializing parapeters(theta) to zeros.\n    # +1 in n+1 for the bias term.\n    theta = np.zeros((n+1,1))\n    \n    # list with misclassification count per iteration.\n    n_miss_list = []\n    \n    # Training.\n    for epoch in range(epochs):\n        # variable to store misclassified.\n        n_miss = 0\n        # looping for every example.\n        for idx, x_i in enumerate(X):\n            # Inserting 1 for bias, X0 = 1.\n            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          \n            # Calculating prediction/hypothesis.\n            y_hat = step_func(np.dot(x_i.T, theta))\n            # Updating if the example is misclassified.\n            if (np.squeeze(y_hat) - y[idx]) != 0:\n                theta += lr*((y[idx] - y_hat)*x_i)\n                # Incrementing by 1.\n                n_miss += 1\n        # Appending number of misclassified examples\n        # at every iteration.\n        n_miss_list.append(n_miss)\n    return theta, n_miss_list"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2c-a-geometrical-view-of-perceptrons",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2c-a-geometrical-view-of-perceptrons",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "A point (a.k.a. location) and an arrow from the origin to that point, are often used interchangeably.\nA hyperplane is the high-dimensional equivalent of a plane in 3-D.\nThe scalar product or inner product between two vectors\n\nsum of element-wise products.\nThe scalar product between two vectors that have an angle of less than 90 degrees between them is positive.\n\nFor more than 90 degrees it’s negative.\n\n\n\n\n\n\n\n\nWeight-space\n\n\n\nHas one dimension per weight.\nA point in the space represents a particular setting of all the weights.\nAssuming that we have eliminated the threshold, each training case can be represented as a hyperplane through the origin.\n\nThe weights must lie on one side of this hyperplane to get the answer correct.\n\nEach training case defines a plane (shown as a black line)\n\nThe plane goes through the origin and is perpendicular to the input vector.\nOn one side of the plane the output is wrong because the scalar product of the weight vector with the input vector has the wrong sign."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2d-why-the-learning-works",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2d-why-the-learning-works",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "We look at the geometrical interpretation which is the proof for the convergence of the Perceptron learning algorithm works. We are trying to find a decision surface by solving a convex optimization problem. The surface is a hyperplane represented by a line where on side is the correct set and the other is incorrect. The weight vectors form a cone: - This means that wights are closed under addition and positive scaler product. - At zero it is zero.\n\n\n\n\n\nThe cone of feasible solutions\n\n\nTo get all training cases right we need to find a point on the right side of all the planes. But there may not be any such point! If there are any weight vectors that get the right answer for all cases, they lie in a hyper-cone with its apex at the origin.\n\nThe average of two good weight vectors is a good weight vector.\nThe problem is convex.\n\nTwo training case form two hyper planes (shown in black). the good weights lie between them average of good weights is good.\nGeometry of learning using weight space:\n\nit has one dimension per weight (vertex in the graph).\n\na point in the space represents a particular setting of all the weights.\neach training case (once we eliminate the threshold/bias) is a hyper plane through the origin\nonce the threshold is eliminated each training case can be represented as a hyper place through the origin\nthe weight must lie on one side of this place to get the answer correct .\n\nCaveats:\n\nconvergence depends on the picking the right features\ndeep nets don’t use this procedure - as it only converges for single layer Perceptrons - but for more than one layer sum of a solution is not necessarily also a solution."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2e-what-perceptrons-cant-do",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2e-what-perceptrons-cant-do",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "This story motivates the need for more powerful networks.\nThese ideas will be important in future lectures, when we’re working on moving beyond these limitations.\n\n\n\n\nFeed-forward neural networks\nRecurrent neural networks\nsequence to Sequence mapping\nperceptrons\nWeight-space\nThe cone of feasible solutions"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html",
    "href": "notes/dnn/dnn-02/l02e.html",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nNow we consider why we don’t use Perceptrons\nnamely their short comings"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#the-limitations-of-perceptrons",
    "href": "notes/dnn/dnn-02/l02e.html#the-limitations-of-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "The limitations of Perceptrons",
    "text": "The limitations of Perceptrons\n\nIf you are allowed to choose the features by hand and if you use enough features, you can do almost anything.\n\nFor binary input vectors, we can have a separate feature unit for each of the exponentially many binary vectors and so we can make any possible discrimination on binary input vectors.\nThis type of table look-up won’t generalize.\n\nBut once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#what-binary-threshold-neurons-cannot-do",
    "href": "notes/dnn/dnn-02/l02e.html#what-binary-threshold-neurons-cannot-do",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "What binary threshold neurons cannot do",
    "text": "What binary threshold neurons cannot do\n\n\n\n\n\nImpossible to satisfy\n\n\n\nA binary threshold output unit cannot even tell if two single bit features are the same!\n\n\n\n\ncase\nmap\nmap\n\n\n\n\nPositive cases (same)\n(1,1) \\to 1\n(0,0) \\to 1\n\n\nNegative cases (different)\n(1,0) \\to 0\n(0,1) \\to 0\n\n\n\n\nThe four input-output pairs give four inequalities that are impossible to satisfy:\nw_1+w_2 \\ge \\theta \\qquad \\theta \\ge 0\nw_1 &lt; \\theta \\qquad w_2 &lt; \\theta"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#a-geometric-view-of-what-binary-threshold-neurons-cannot-do",
    "href": "notes/dnn/dnn-02/l02e.html#a-geometric-view-of-what-binary-threshold-neurons-cannot-do",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "A geometric view of what binary threshold neurons cannot do",
    "text": "A geometric view of what binary threshold neurons cannot do\n\n\n\n\n\ngeometric view\n\n\nImagine “data-space” in which the axes correspond to components of an input vector.\n\nEach input vector is a point in this space.\nA weight vector defines a plane in data-space.\nThe weight plane is perpendicular to the weight vector and misses the origin by a distance equal to the threshold."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#discriminating-simple-patterns-under-translation-with-wrap-around",
    "href": "notes/dnn/dnn-02/l02e.html#discriminating-simple-patterns-under-translation-with-wrap-around",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "Discriminating simple patterns under translation with wrap-around",
    "text": "Discriminating simple patterns under translation with wrap-around\n\n\n\n\n\nwrap around\n\n\n\nSuppose we just use pixels as the features.\nCan a binary threshold unit discriminate between different patterns that have the same number of on pixels?\nNot if the patterns can translate with wrap-around!"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#sketch-of-a-proof-that-a-binary-decision-unit-cannot-discriminate-patterns-with-the-same-number-of-on-pixels-assuming-translation-with-wraparound",
    "href": "notes/dnn/dnn-02/l02e.html#sketch-of-a-proof-that-a-binary-decision-unit-cannot-discriminate-patterns-with-the-same-number-of-on-pixels-assuming-translation-with-wraparound",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "Sketch of a proof that a binary decision unit cannot discriminate patterns with the same number of on pixels (assuming translation with wraparound)",
    "text": "Sketch of a proof that a binary decision unit cannot discriminate patterns with the same number of on pixels (assuming translation with wraparound)\n\nFor pattern A, use training cases in all possible translations.\n\nEach pixel will be activated by 4 different translations of pattern A.\nSo the total input received by the decision unit over all these patterns will be four times the sum of all the weights.\n\nFor pattern B, use training cases in all possible translations.\n\nEach pixel will be activated by 4 different translations of pattern B.\nSo the total input received by the decision unit over all these patterns will be four times the sum of all the weights.\n\nBut to discriminate correctly, every single case of pattern A must provide more input to the decision unit than every single case of pattern B.\n\nThis is impossible if the sums over cases are the same."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#why-this-result-was-devastating-for-perceptrons",
    "href": "notes/dnn/dnn-02/l02e.html#why-this-result-was-devastating-for-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "Why this result was devastating for Perceptrons",
    "text": "Why this result was devastating for Perceptrons\n\nThe whole point of pattern recognition is to recognize patterns despite transformations like translation.\nIn thier book Minsky and Papert (1969) the authors Marvin Minsky and Seymour Papert proove the Group Invariance Theorem which says that the part of a Perceptron that learns cannot learn to do this if the transformations form a group.\n\nTranslations with wrap-around form a group.\n\nTo deal with such transformations, a Perceptron needs to use multiple feature units to recognize transformations of informative sub-patterns.\n\nSo the tricky part of pattern recognition must be solved by the hand-coded feature detectors, not the learning procedure."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#learning-with-hidden-units",
    "href": "notes/dnn/dnn-02/l02e.html#learning-with-hidden-units",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "Learning with hidden units",
    "text": "Learning with hidden units\n\nNetworks without hidden units are very limited in the input-output mappings they can learn to model.\n\nMore layers of linear units do not help. Its still linear.\nFixed output non-linearities are not enough.\n\nWe need multiple layers of adaptive, non-linear hidden units. But how can we train such nets?\n\nWe need an efficient way of adapting all the weights, not just the last layer. This is hard.\nLearning the weights going into hidden units is equivalent to learning features.\nThis is difficult because nobody is telling us directly what the hidden units should do.\n\n\n\n\n\nImpossible to satisfy\ngeometric view\nwrap around"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html",
    "href": "notes/dnn/dnn-02/l02c.html",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html#warning",
    "href": "notes/dnn/dnn-02/l02c.html#warning",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "Warning!",
    "text": "Warning!\n\nFor non-mathematicians, this is going to be tougher than the previous material.\nYou may have to spend a long time studying the next two parts.\nIf you are not used to thinking about hyper-planes in high-dimensional spaces, now is the time to learn.\nTo deal with hyper-planes in a 14-dimensional space, visualize a 3-D space and say “fourteen” to yourself very loudly. Everyone does it. :-)\n\nBut remember that going from 13-D to 14-D creates as much extra complexity as going from 2-D to 3-D."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html#geometry-review",
    "href": "notes/dnn/dnn-02/l02c.html#geometry-review",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "Geometry review",
    "text": "Geometry review\n\nA point (a.k.a. location) and an arrow from the origin to that point, are often used interchangeably.\nA hyperplane is the high-dimensional equivalent of a plane in 3-D.\nThe scalar product or inner product between two vectors\n\nsum of element-wise products.\nThe scalar product between two vectors that have an angle of less than 90 degrees between them is positive.\nFor more than 90 degrees it’s negative."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html#weight-space",
    "href": "notes/dnn/dnn-02/l02c.html#weight-space",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "Weight-space",
    "text": "Weight-space\n\n\n\n\n\nWeight-space\n\n\n\nThis space has one dimension per weight.\nA point in the space represents a particular setting of all the weights.\nAssuming that we have eliminated the threshold, each training case can be represented as a hyperplane through the origin.\n\nThe weights must lie on one side of this hyper-plane to get the answer correct.\n\nEach training case defines a plane (shown as a black line)\n\nThe plane goes through the origin and is perpendicular to the input vector.\nOn one side of the plane the output is wrong because the scalar product of the weight vector with the input vector has the wrong sign."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html#the-cone-of-feasible-solutions",
    "href": "notes/dnn/dnn-02/l02c.html#the-cone-of-feasible-solutions",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "The cone of feasible solutions",
    "text": "The cone of feasible solutions\n\n\n\n\n\nCone of feasable soulutions\n\n\n\nTo get all training cases right we need to find a point on the right side of all the planes.\n\nThere may not be any such point!\n\nIf there are any weight vectors that get the right answer for all cases, they lie in a hyper-cone with its apex at the origin.\n\nSo the average of two good weight vectors is a good weight vector.\n\nThe problem is convex.\n\n\n\nThis is not a very good explanation - unless we also take a convex optimization course in which we define a hyperplane and a cone.\n\n\n\nWeight-space\nCone of feasable soulutions"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01e.html",
    "href": "notes/dnn/dnn-01/l01e.html",
    "title": "Deep Neural Networks - Notes for lecture 1e",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01e.html#lecture-1e-three-types-of-learning",
    "href": "notes/dnn/dnn-01/l01e.html#lecture-1e-three-types-of-learning",
    "title": "Deep Neural Networks - Notes for lecture 1e",
    "section": "Lecture 1e: Three types of learning",
    "text": "Lecture 1e: Three types of learning\n\nThe three main types of learning machine learning:\n\nSupervised learning\n\nLearn to predict an output given an input vector\n\nReinforcement learning\n\nLearn to select an action to maximize payoff.\n\nUnsupervised learning\n\nDiscover a good internal representation of the input.\n\nSemi supervised learning\n\nSemi-supervised uses a small amount of supervised data and large amount of unsupervised elarning\n\nFew/one shot learning\n\nSupervised learning with inference from one or a few examples\n\nZero shot learning\n\nSupervised learning with inference for inputs not seen in training - usually based on learned structrure\n\nTransfer learning\n\nLearning something from one data set and use it on another"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01e.html#two-types-of-supervised-learning",
    "href": "notes/dnn/dnn-01/l01e.html#two-types-of-supervised-learning",
    "title": "Deep Neural Networks - Notes for lecture 1e",
    "section": "Two types of supervised learning",
    "text": "Two types of supervised learning\n\nEach training case consists of an input vector x and a target output t.\nRegression: The target output is a real number or a whole vector of real numbers.\n\nThe price of a stock in 6 months time.\nThe temperature at noon tomorrow.\n\nClassification: The target output is a class label.\n\nThe simplest case is a choice between 1 and 0.\nWe can also have multiple alternative labels."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01e.html#how-supervised-learning-typically-works",
    "href": "notes/dnn/dnn-01/l01e.html#how-supervised-learning-typically-works",
    "title": "Deep Neural Networks - Notes for lecture 1e",
    "section": "How supervised learning typically works",
    "text": "How supervised learning typically works\n\nWe start by choosing a model-class:\n\nA model-class, f, is a way of using some numerical y=f(x;W) parameters, W, to map each input vector, x, into a predicted output y.\n\nLearning usually means adjusting the parameters to reduce the discrepancy between the target output, t, on each training case and the actual output, y, produced by the model.\n\nFor regression, \\frac{1}{2}(y-t)^2is often a sensible measure of the discrepancy.\nFor classification there are other measures that are generally more sensible (they also work better).\n\n\n\nReinforcement learning\n\nIn reinforcement learning, the output is an action or sequence of actions and the only supervisory signal is an occasional scalar reward.\n\nThe goal in selecting each action is to maximize the expected sum of the future rewards.\nWe usually use a discount factor for delayed rewards so that we don’t have to look too far into the future.\n\nReinforcement learning is difficult:\n\nThe rewards are typically delayed so its hard to know where we went wrong (or right).\nA scalar reward does not supply much information.\n\nThis course cannot cover everything and reinforcement learning is one of the important topics we will not cover.\n\n\n\nUnsupervised learning\n\nFor about 40 years, unsupervised learning was largely ignored by the machine learning community\n\nSome widely used definitions of machine learning actually excluded it.\nMany researchers thought that clustering was the only form of unsupervised learning.\n\nIt is hard to say what the aim of unsupervised learning is.\n\nOne major aim is to create an internal representation of the input that is useful for subsequent supervised or reinforcement learning.\nYou can compute the distance to a surface by using the disparity between two images. But you don’t want to learn to compute disparities by stubbing your toe thousands of times.\n\n\n\n\nOther goals for unsupervised learning\n\nIt provides a compact, low-dimensional representation of the input.\n\nHigh-dimensional inputs typically live on or near a lowdimensional manifold (or several such manifolds).\nPrincipal Component Analysis is a widely used linear method for finding a low-dimensional representation.\n\nIt provides an economical high-dimensional representation of the input in terms of learned features.\n\nBinary features are economical. – So are real-valued features that are nearly all zero.\n\nIt finds sensible clusters in the input.\n\nThis is an example of a very sparse code in which only one of the features is non-zero."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01d.html",
    "href": "notes/dnn/dnn-01/l01d.html",
    "title": "Deep Neural Networks - Notes for lecture 1d",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01d.html#visualization-of-neural-networks-is-one-of-the-few-methods-to-get-some-insights-into-what-is-going-on-inside-the-black-box.",
    "href": "notes/dnn/dnn-01/l01d.html#visualization-of-neural-networks-is-one-of-the-few-methods-to-get-some-insights-into-what-is-going-on-inside-the-black-box.",
    "title": "Deep Neural Networks - Notes for lecture 1d",
    "section": "Visualization of neural networks is one of the few methods to get some insights into what is going on inside the black box.",
    "text": "Visualization of neural networks is one of the few methods to get some insights into what is going on inside the black box.\n\nConsider a neural network with two layers of neurons.\n\nneurons in the top layer represent known shapes.\nneurons in the bottom layer represent pixel intensities.\n\nA pixel gets to vote if it has ink on it.\n\nEach inked pixel can vote for several different shapes.\n\nThe shape that gets the most votes wins.\n\n\nHow to display the weights\nGive each output unit its own “map” of the input image and display the weight coming from each pixel in the location of that pixel in the map.\nUse a black or white blob with the area representing the magnitude of the weight and the color representing the sign.\n\n\nHow to learn the weights\nShow the network an image and increment the weights from active pixels to the correct class.\nThen decrement the weights from active pixels to whatever class the network guesses\n\n\nThe learned weights\nThe details of the learning algorithm will be explained in future lectures.\n\n\nWhy the simple learning algorithm is insufficient\n\nA two layer network with a single winner in the top layer is equivalent to having a rigid template for each shape.\nThe winner is the template that has the biggest overlap with the ink.\nThe ways in which hand-written digits vary are much too complicated to be captured by simple template matches of whole shapes.\nTo capture all the allowable variations of a digit we need to learn the features that it is composed of."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01b.html",
    "href": "notes/dnn/dnn-01/l01b.html",
    "title": "Deep Neural Networks - Notes for lecture 1b",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01b.html#lecture-1b-what-are-neural-networks",
    "href": "notes/dnn/dnn-01/l01b.html#lecture-1b-what-are-neural-networks",
    "title": "Deep Neural Networks - Notes for lecture 1b",
    "section": "Lecture 1b: What are neural networks?",
    "text": "Lecture 1b: What are neural networks?\n\nSome tasks that are easy or humans, like vision, are hard for software, and vice versa (chess).\n\nReasons to study neural computation\n\nTo understand how the brain actually works.\n\nIts very big and very complicated and made of stuff that dies when you poke it around. So we need to use computer simulations.\n\nTo understand a style of parallel computation inspired by neurons and their adaptive connections.\n\nVery different style from sequential computation.\nshould be good for things that brains are good at (e.g. vision)\nShould be bad for things that brains are bad at (e.g. 23 x 71)\n\nTo solve practical problems by using novel learning algorithms inspired by the brain (this course)\n\nLearning algorithms can be very useful even if they are not how the brain actually works.\n\n\n\n\nA typical cortical neuron\n\nGross physical structure:\n\nThere is one axon that branches\nThere is a dendritic tree that collects input from other neurons.\n\nAxons typically contact dendritic trees at synapses\n\nA spike of activity in the axon causes charge to be injected into the post-synaptic neuron.\n\nSpike generation:\n\nThere is an axon hillock that generates outgoing spikes whenever enough charge has flowed in at synapses to depolarize the cell membrane.\n\n\n\n\nSynapses\n\nWhen a spike of activity travels along an axon and arrives at a synapse it causes vesicles of transmitter chemical to be released.\n\nThere are several kinds of transmitter.\n\nThe transmitter molecules diffuse across the synaptic cleft and bind to receptor molecules in the membrane of the post-synaptic neuron thus changing their shape.\n\nThis opens up holes that allow specific ions in or out.\n\n\n\n\nHow synapses adapt\n\nThe effectiveness of the synapse can be changed:\n\nvary the number of vesicles of transmitter.\nvary the number of receptor molecules.\n\nSynapses are slow, but they have advantages over RAM\n\nThey are very small and very low-power.\nThey adapt using locally available signals\n\nBut what rules do they use to decide how to change?\n\n\n\n\n\nHow the brain works on one slide!\n\nEach neuron receives inputs from other neurons\n\nA few neurons also connect to receptors.\nCortical neurons use spikes to communicate.\n\nThe effect of each input line on the neuron is controlled by a synaptic weight\n\nThe weights can be positive or negative.\n\nThe synaptic weights adapt so that the whole network learns to perform useful computations\n\nRecognizing objects, understanding language, making plans, controlling the body.\n\nYou have about neurons each with about weights.\n\nA huge number of weights can affect the computation in a very short time. Much better bandwidth than a workstation.\n\n\n\n\nModularity and the brain\n\nDifferent bits of the cortex do different things.\n\nLocal damage to the brain has specific effects.\nSpecific tasks increase the blood flow to specific regions.\n\nBut cortex looks pretty much the same all over.\n\nEarly brain damage makes functions relocate.\n\nCortex is made of general purpose stuff that has the ability to turn into special purpose hardware in response to experience.\n\nThis gives rapid parallel computation plus flexibility.\nConventional computers get flexibility by having stored sequential programs, but this requires very fast central processors to perform long sequential computations."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html",
    "href": "notes/dnn/dnn-03/l03d.html",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "Here, we start using hidden layers. To train them, we need the back propagation algorithm. Hidden layers, and this algorithm, are very important. They are the layers between the input layer and the output.\nThe story of training by perturbations also makes an appearance in the course by David MCcay, serving primarily as motivation for the back propagation algorithm.\nThis computation, just like the forward propagation, can be vectorized across multiple units in every layer, and multiple training cases.\n\n\n\nNetworks without hidden units are very limited in the input-output mappings they can model.\nAdding a layer of hand-coded features (as in a Perceptrons) makes them much more powerful but the hard bit is designing the features.\n\nWe would like to find good features without requiring insights into the task or repeated trial and error where we guess some features and see how well they work.\n\nWe need to automate the loop of designing features for a particular task and seeing how well they work.\n\n\n\n\n\nRandomly perturb one weight and see if it improves performance. If so, save the change.\n\nThis is a form of reinforcement learning.\nVery inefficient. We need to do multiple forward passes on a representative set of training cases just to change one weight. Back propagation is much better.\nTowards the end of learning, large weight perturbations will nearly always make things worse, because the weights need to have the right relative values. (so we should adapt a decreasing learning rate).\n\nWe could randomly perturb all the weights in parallel and correlate the performance gain with the weight changes.\n\nNot any better because we need lots of trials on each training case to “see” the effect of changing one weight through the noise created by all the changes to other weights.\n\nA better idea: Randomly perturb the activities of the hidden units.\n\nOnce we know how we want a hidden activity to change on a given training case, we can compute how to change the weights.\nThere are fewer activities than weights, but backpropagation still wins by a factor of the number of neurons.\n\n\n\n\n\n\nWe don’t know what the hidden units ought to do, but we can compute how fast the error changes as we change a hidden activity.\n\nInstead of using desired activities to train the hidden units, use error derivatives w.r.t. hidden activities.\nEach hidden activity can affect many output units and can therefore have many separate effects on the error. These effects must be combined.\n\nWe can compute error derivatives for all the hidden units efficiently at the same time.\n\nOnce we have the error derivatives for the hidden activities, its easy to get the error derivatives for the weights going into a hidden unit.\n\n\n\n\n\n\nFirst convert the discrepancy between each output and its target value into an error derivative.\nThen compute error derivatives in each hidden layer from error derivatives in the layer above.\nThen use error derivatives w.r.t. activities to get error derivatives w.r.t. the incoming weights. \nE =\\frac{1}{2}(t_i-y_i)^2\n\n\n\n\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)\n\n\n\n\n\n\nback proogations of errors\n\n\n\n\n\n\nbackpropagating the error derivative\n\n\n\n\n\n\nback proogations of errors\nbackpropagating the error derivative"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html#learning-with-hidden-units-again",
    "href": "notes/dnn/dnn-03/l03d.html#learning-with-hidden-units-again",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "Networks without hidden units are very limited in the input-output mappings they can model.\nAdding a layer of hand-coded features (as in a Perceptrons) makes them much more powerful but the hard bit is designing the features.\n\nWe would like to find good features without requiring insights into the task or repeated trial and error where we guess some features and see how well they work.\n\nWe need to automate the loop of designing features for a particular task and seeing how well they work."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html#learning-by-perturbing-weights",
    "href": "notes/dnn/dnn-03/l03d.html#learning-by-perturbing-weights",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "Randomly perturb one weight and see if it improves performance. If so, save the change.\n\nThis is a form of reinforcement learning.\nVery inefficient. We need to do multiple forward passes on a representative set of training cases just to change one weight. Back propagation is much better.\nTowards the end of learning, large weight perturbations will nearly always make things worse, because the weights need to have the right relative values. (so we should adapt a decreasing learning rate).\n\nWe could randomly perturb all the weights in parallel and correlate the performance gain with the weight changes.\n\nNot any better because we need lots of trials on each training case to “see” the effect of changing one weight through the noise created by all the changes to other weights.\n\nA better idea: Randomly perturb the activities of the hidden units.\n\nOnce we know how we want a hidden activity to change on a given training case, we can compute how to change the weights.\nThere are fewer activities than weights, but backpropagation still wins by a factor of the number of neurons."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html#the-idea-behind-backpropagation",
    "href": "notes/dnn/dnn-03/l03d.html#the-idea-behind-backpropagation",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "We don’t know what the hidden units ought to do, but we can compute how fast the error changes as we change a hidden activity.\n\nInstead of using desired activities to train the hidden units, use error derivatives w.r.t. hidden activities.\nEach hidden activity can affect many output units and can therefore have many separate effects on the error. These effects must be combined.\n\nWe can compute error derivatives for all the hidden units efficiently at the same time.\n\nOnce we have the error derivatives for the hidden activities, its easy to get the error derivatives for the weights going into a hidden unit."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html#sketch-of-back-propagation-on-a-single-case",
    "href": "notes/dnn/dnn-03/l03d.html#sketch-of-back-propagation-on-a-single-case",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "First convert the discrepancy between each output and its target value into an error derivative.\nThen compute error derivatives in each hidden layer from error derivatives in the layer above.\nThen use error derivatives w.r.t. activities to get error derivatives w.r.t. the incoming weights. \nE =\\frac{1}{2}(t_i-y_i)^2\n\n\n\n\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)\n\n\n\n\n\n\nback proogations of errors\n\n\n\n\n\n\nbackpropagating the error derivative\n\n\n\n\n\n\nback proogations of errors\nbackpropagating the error derivative"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html",
    "href": "notes/dnn/dnn-03/l_03.html",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nslides for the lesson\nWhy is a new algorithm needed?"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#why-the-perceptron-learning-procedure-cannot-be-generalised-to-hidden-layers",
    "href": "notes/dnn/dnn-03/l_03.html#why-the-perceptron-learning-procedure-cannot-be-generalised-to-hidden-layers",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Why the perceptron learning procedure cannot be generalised to hidden layers?",
    "text": "Why the perceptron learning procedure cannot be generalised to hidden layers?\n\nRecall: by convexity, the Perceptron convergence algorithm guarantees that each time the weights change, they get closer to every “generously feasible” set of weights. 😄\n\nThis guarantee cannot be extended to more complex networks which wights are non-convex, i.e. the average of two good solutions may be a bad solution. 1 😢\n\nSo “multi-layer” neural networks do not use the perceptron learning procedure.\n\nThey should never have been called multi-layer perceptrons."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#a-different-way-to-show-that-a-learning-procedure-makes-progress",
    "href": "notes/dnn/dnn-03/l_03.html#a-different-way-to-show-that-a-learning-procedure-makes-progress",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "A different way to show that a learning procedure makes progress",
    "text": "A different way to show that a learning procedure makes progress\n\nInstead of showing the weights get closer to a good set of weights, show that the actual output values get closer the target values.\n\nThis can be true even for non-convex problems in which there are many quite different sets of weights that work well and averaging two good sets of weights may give a bad set of weights.\nIt is not true for perceptron learning.\n\nThe simplest example is a linear neuron with a squared error measure."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#linear-neurons",
    "href": "notes/dnn/dnn-03/l_03.html#linear-neurons",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Linear neurons",
    "text": "Linear neurons\n\nCalled linear filters in electrical engineering and linear transforms in linear algebra and can be represented by martracies\nWe don’t use linear neurons in practice:\n\nWithout a non-linearity in the unit, a stack of N layers can be replaced by a single layer 2\nThis lecture just demonstrates the analysis we will use with non-linear units.\n\nThe neuron’s output is the real valued weighted sum of its inputs\nThe goal of learning is to minimize the total error over all training cases.\n\nHere error is the squared difference between the desired output and the actual output.\n\n\n\n\\textcolor{green}{\\overbrace{y}^{\\text{output}}} = \\sum_{n \\in train} \\textcolor{red}{\\overbrace{w_i}^{\\text{weights}}} \\textcolor{blue}{\\underbrace{x_i}_{\\text{inputs}}}= \\vec{w}^T\\cdot\\vec{x}\n where:\n\ny is the neuron’s estimate of the desired output\nx is the input vector\nw is the weight vector"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#why-dont-we-solve-it-analytically",
    "href": "notes/dnn/dnn-03/l_03.html#why-dont-we-solve-it-analytically",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Why don’t we solve it analytically?",
    "text": "Why don’t we solve it analytically?\n\nIt is straight-forward to write down a set of equations, one per training case, and to solve for the best set of weights.\nThis is the standard engineering approach so why don’t we use it?\nScientific answer: We want a method that real neurons could use.\nEngineering answer: We want a method that can be generalized to multi-layer, non-linear neural networks.\nThe analytic solution relies on it being linear and having a squared error measure.\nIterative methods are usually less efficient but they are much easier to generalize.\n\n\nA toy example\n\nEach day you get lunch at the cafeteria.\n\nYour diet consists of fish, chips, and ketchup.\nYou get several portions of each.\n\nThe cashier only tells you the total price of the meal\n\nAfter several days, you should be able to figure out the price of each portion.\n\nThe iterative approach: Start with random guesses for the prices and then adjust them to get a better fit to the observed prices of whole meals.\n\n\n\nSolving the equations iteratively\n\nEach meal price gives a linear constraint on the prices of the portions: \n\\text{price} = X_\\text{fish} W_\\text{fish} + X_\\text{chips} W_\\text{chips} + X_\\text{ketchup}W_\\text{ketchup}      \n\n\nThe prices of the portions are like the weights in of a linear neuron. \nW = (w_\\text{fish} , W_\\text{ chips} , W_\\text{ketchup} )\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\nThe true weights used by the cashier\n\n\n\n\n\nthe true weights\n\n\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\nA model of the cashier with arbitrary initial weights\n\n\n\n\n\na toy problem\n\n\n\nResidual error = 350\nThe “delta-rule” for learning is: \\Delta w_i = \\epsilon x_i (t - y)\nWith a learning rate \\epsilon of 1/35, the weight changes are:+20, +50, +30\nThis gives new weights of: 70, 100, 80.\nThe weight for chips got worse, but over all the weights are better\n\ny reducing errors, individual weight estimate may be getting worse\nCalculating the change in the weights:\ncalculate our output using forward propagation\n\n\nDeriving the delta rule\n\ny = \\sum_{n \\in train} w_i x_i= \\vec{w}^T\\vec{x}\n Define the error as the squared residuals summed over all training cases:\n\nE = \\frac{1}{2}\\sum_{n \\in train} (t_n−y_n)^2\n\nuse the chain rule to get error derivatives for weights\n\n\\frac{d E}{\\partial w_i}=\\frac{1}{2}\\sum_{n \\in train}\\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n}=\\frac{1}{2}\\sum_{n \\in train}x_i^n(t^n−y^n)\n\nthe batch delta rule changes the weight in proportion to their error derivative summed on all training cases times the learning rate\n\n\\Delta w_i = −\\epsilon \\frac{d E}{\\partial w_i} = \\sum_{n \\in train} \\epsilon x_i^n (t^n−y^n)"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#behaviour-of-the-iterative-learning-procedure",
    "href": "notes/dnn/dnn-03/l_03.html#behaviour-of-the-iterative-learning-procedure",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Behaviour of the iterative learning procedure",
    "text": "Behaviour of the iterative learning procedure\n\nDoes the learning procedure eventually get the right answer?\n\nThere may be no perfect answer.\nBy making the learning rate small enough we can get as close as we desire to the best answer.\n\nHow quickly do the weights converge to their correct values?\n\nIt can be very slow if two input dimensions are highly correlated. If you almost always have the same number of portions of ketchup and chips, it is hard to decide how to divide the price between ketchup and chips"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#the-relationship-between-the-online-delta-rule-and-the-learning-rule-for-perceptrons",
    "href": "notes/dnn/dnn-03/l_03.html#the-relationship-between-the-online-delta-rule-and-the-learning-rule-for-perceptrons",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "The relationship between the online delta-rule and the learning rule for perceptrons",
    "text": "The relationship between the online delta-rule and the learning rule for perceptrons\n\nIn perceptron learning, we increment or decrement the weight vector by the input vector.\n\nBut we only change the weights when we make an error.\n\nIn the online version of the delta-rule we increment or decrement the weight vector by the input vector scaled by the residual error and the learning rate.\n\nSo we have to choose a learning rate. This is annoying\n\n\n\nresidual error\n\nit’s the amount by which we got the answer wrong.\n\n\nA very central concept is introduced without being made very explicit: we use derivatives for learning, i.e. for making the weights better. Try to understand why those concepts are indeed very related.\n\non-line learning\n\nmeans that we change the weights after every training example that we see, and we typically cycle through the collection of available training examples."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#lecture-3b-the-error-surface-for-a-linear-neuron",
    "href": "notes/dnn/dnn-03/l_03.html#lecture-3b-the-error-surface-for-a-linear-neuron",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Lecture 3b: The error surface for a linear neuron",
    "text": "Lecture 3b: The error surface for a linear neuron\n\n\n\n\n\n\n\nerror surface of a linear neuron\n\n\n\n\nThe error surface lies in a space with a horizontal axis for each weight and one vertical axis for the error.\n\nFor a linear neuron with a squared error, it is a quadratic bowl.\nVertical cross-sections are parabolas.\nHorizontal cross-sections are ellipses.\n\nFor multi-layer, non-linear nets the error surface is much more complicated.\n\n\nOnline versus batch learning\n\n\n\n\n\nWhy learning can be slow\n\n\nIf the ellipse is very elongated, the direction of steepest descent is almost perpendicular to the direction towards the minimum!\nThe red gradient vector has a large component along the short axis of the ellipse and a small component along the long axis of the ellipse.\nThis is just the opposite of what we want."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#lecture-3c-learning-the-weights-of-a-logistic-output-neuron",
    "href": "notes/dnn/dnn-03/l_03.html#lecture-3c-learning-the-weights-of-a-logistic-output-neuron",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Lecture 3c: Learning the weights of a logistic output neuron",
    "text": "Lecture 3c: Learning the weights of a logistic output neuron\n\n\n\nLogistic neurons AKA linear filters - useful to understand the algorithm but in reality we need to use non linear activation function.\n\nLogistic neurons\nThese give a real-valued output that is a smooth and bounded function of their total input. They have nice derivatives which make learning easy.\n\nz = b + \\sum _i x_i w_i\n\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\n\n\nlogistic activation function\n\n\n\n\n\nThe derivatives of a logistic neuron\nThe derivatives of the logit, z, with respect to the inputs and the weights are very simple:\n\nz = b + \\sum _i x_i w_i \\tag{the logit}\n\n\n\\frac{\\partial z}{\\partial w_i} = x_i \\;\\;\\;\\;\\; \\frac{\\partial z}{\\partial x_i} = w_i\n\nThe derivative of the output with respect to the logit is simple if you express it in terms of the output:\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\\frac{d y}{d z} = y( 1-y)\n\nsince\n\ny = \\frac{1}{1+e^{-z}}=(1+e^{-z})^{-1}\n differentiating  \\frac{d y}{d z} = \\frac{-1(-e^{-z})}{(1+e^{-z})^2} =\\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}  = y( 1-y)  Using the chain rule to get the derivatives needed for learning the weights of a logistic unit To learn the weights we need the derivative of the output with respect to each weight:\n\n\\frac{d y}{\\partial w_i}  =\\frac{\\partial z}{\\partial w_i} \\frac{dy}{dz}  = x_iy( 1-y)\n\n\n\\frac{d E}{\\partial w_i}  = \\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n} = - \\sum \\green{x_i^n}\\red{ y^n( 1-y^n)}\\green{(t^n-y^n)}\n\nwhere the green part corresponds to the delta rule and the extra term in red is simply the slope of the logistic.\nThe error function is still:\n\nE =\\frac{1}{2}(y−t)^2\n\nNotice how after Hinton explained what the derivative is for a logistic unit, he considers the job to be done. That’s because the learning rule is always simply some learning rate multiplied by the derivative."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#lecture-3d-the-back-propagation-algorithm",
    "href": "notes/dnn/dnn-03/l_03.html#lecture-3d-the-back-propagation-algorithm",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Lecture 3d: The back-propagation algorithm",
    "text": "Lecture 3d: The back-propagation algorithm\n\n\n\nHere, we start using hidden layers. To train them, we need the back propagation algorithm. Hidden layers, and this algorithm, are very important. They are the layers between the input layer and the output.\nThe story of training by perturbations also makes an appearance in the course by David MCcay, serving primarily as motivation for the back propagation algorithm.\nThis computation, just like the forward propagation, can be vectorized across multiple units in every layer, and multiple training cases.\n\nLearning with hidden units (again)\n\nNetworks without hidden units are very limited in the input-output mappings they can model.\n\nAdding a layer of hand-coded features (as in a Perceptrons) makes them much more powerful but the hard bit is designing the features.\n\nWe would like to find good features without requiring insights into the task or repeated trial and error where we guess some features and see how well they work.\n\nWe need to automate the loop of designing features for a particular task and seeing how well they work.\n\n\n\nLearning by perturbing weights\n\nRandomly perturb one weight and see if it improves performance. If so, save the change.\n\nThis is a form of reinforcement learning.\nVery inefficient. We need to do multiple forward passes on a representative set of training cases just to change one weight. Back propagation is much better.\nTowards the end of learning, large weight perturbations will nearly always make things worse, because the weights need to have the right relative values. (so we should adapt a decreasing learning rate).\n\nWe could randomly perturb all the weights in parallel and correlate the performance gain with the weight changes.\n\nNot any better because we need lots of trials on each training case to “see” the effect of changing one weight through the noise created by all the changes to other weights.\n\nA better idea: Randomly perturb the activities of the hidden units.\n\nOnce we know how we want a hidden activity to change on a given training case, we can compute how to change the weights.\nThere are fewer activities than weights, but backpropagation still wins by a factor of the number of neurons.\n\n\n\n\nThe idea behind backpropagation\n\nWe don’t know what the hidden units ought to do, but we can compute how fast the error changes as we change a hidden activity.\n\nInstead of using desired activities to train the hidden units, use error derivatives w.r.t. hidden activities.\n\nEach hidden activity can affect many output units and can therefore have many separate effects on the error. These effects must be combined.\n\n\nWe can compute error derivatives for all the hidden units efficiently at the same time.\n\nOnce we have the error derivatives for the hidden activities, its easy to get the error derivatives for the weights going into a hidden unit."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#sketch-of-back-propagation-on-a-single-case",
    "href": "notes/dnn/dnn-03/l_03.html#sketch-of-back-propagation-on-a-single-case",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Sketch of back propagation on a single case",
    "text": "Sketch of back propagation on a single case\n\nFirst convert the discrepancy between each output and its target value into an error derivative.\nThen compute error derivatives in each hidden layer from error derivatives in the layer above.\nThen use error derivatives w.r.t. activities to get error derivatives w.r.t. the incoming weights. \nE =\\frac{1}{2}(t_i-y_i)^2\n\n\n\n\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)\n\n\n\n\n\n\nback proogations of erros\n\n\n\n\n\n\nbackproogating the error derivative"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#lecture-3e-using-the-derivatives-computed-by-backpropagation",
    "href": "notes/dnn/dnn-03/l_03.html#lecture-3e-using-the-derivatives-computed-by-backpropagation",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Lecture 3e: Using the derivatives computed by backpropagation",
    "text": "Lecture 3e: Using the derivatives computed by backpropagation\n\n\n\n\nThe backpropagation algorithm is an efficient way of computing the error derivative \\frac{dE}{dw} for every weight on a single training case. There are many decisions needed on how to derive new weights using there derivatives.\n\nOptimization issues: How do we use the error derivatives on individual cases to discover a good set of weights? (lecture 6)\nGeneralization issues: How do we ensure that the learned weights work well for cases we did not see during training? (lecture 7)\n\nWe now have a very brief overview of these two sets of issues.\nHow often to update weights ?\n\nOnline - after every case.\nMini Batch - after a small sample of training cases.\nFull Batch - after a full sweep of training data.\n\nHow much to update? (c.f. lecture 6)\n\nfixed learning rate\nadaptable global learning rate\nadaptable learning rate per weight\ndon’t use steepest descent (velocity/momentum/second order methods)\n\n\n\nOverfitting: The downside of using powerful models\n\nRegularization - How to ensure that learned weights work well for cases we did not see during training?\n\nThe training data contains information about the regularities in the mapping from input to output. But it also contains two types of noise.\n\nThe target values may be unreliable (usually only a minor worry).\nThere is sampling error. There will be accidental regularities just because of the particular training cases that were chosen.\n\nWhen we fit the model, it cannot tell which regularities are real and which are caused by sampling error.\n\nSo it fits both kinds of regularity.\nIf the model is very flexible it can model the sampling error really well. This is a disaster.\n\n\n\n\nA simple example of overfitting\n\n\nWhich output value should you predict for this test input?\nWhich model do you trust?\n\nThe complicated model fits the data better.\nBut it is not economical.\n\nA model is convincing when it fits a lot of data surprisingly well.\n\nIt is not surprising that a complicated model can fit a small amount of data well.\nModels fit both signal and noise.\n\n\n\n\nHow to reduce overfitting\n\nA large number of different methods have been developed to reduce overfitting.\n\nWeight-decay\nWeight-sharing - reduce model flexibility by adding constraints on weights\nEarly stopping - stop training when by monitoring the Test error.\nModel averaging - use an ensemble of models\nBayesian fitting of neural nets - like averaging but weighed\nDropout - (hide data from half the net)\nGenerative pre-training - (more data)\n\nMany of these methods will be described in lecture 7.\n\n\n\n\nthe true weights\na toy problem\nerror surface of a linear neuron\nlogistic activation function\nback proogations of erros\nbackproogating the error derivative"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#footnotes",
    "href": "notes/dnn/dnn-03/l_03.html#footnotes",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\na convex set includes all weighted sums↩︎\nthink multiplying N-matracies just gives a single matrix ↩︎"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03e.html",
    "href": "notes/dnn/dnn-03/l03e.html",
    "title": "Deep Neural Networks - Notes for lecture 3e",
    "section": "",
    "text": "The backpropagation algorithm is an efficient way of computing the error derivative \\frac{dE}{dw} for every weight on a single training case. There are many decisions needed on how to derive new weights using there derivatives.\n\nOptimization issues: How do we use the error derivatives on individual cases to discover a good set of weights? (lecture 6)\nGeneralization issues: How do we ensure that the learned weights work well for cases we did not see during training? (lecture 7)\n\nWe now have a very brief overview of these two sets of issues.\nHow often to update weights ?\n\nOnline - after every case.\nMini Batch - after a small sample of training cases.\nFull Batch - after a full sweep of training data.\n\nHow much to update? (c.f. lecture 6)\n\nfixed learning rate\nadaptable global learning rate\nadaptable learning rate per weight\ndon’t use steepest descent (velocity/momentum/second order methods)\n\n\n\n\n\nRegularization - How to ensure that learned weights work well for cases we did not see during training?\n\nThe training data contains information about the regularities in the mapping from input to output. But it also contains two types of noise.\n\nThe target values may be unreliable (usually only a minor worry).\nThere is sampling error. There will be accidental regularities just because of the particular training cases that were chosen.\n\nWhen we fit the model, it cannot tell which regularities are real and which are caused by sampling error.\n\nSo it fits both kinds of regularity.\nIf the model is very flexible it can model the sampling error really well. This is a disaster.\n\n\n\n\n\n\n\n\n\noverfitting\n\n\n\n\nWhich output value should you predict for this test input?\nWhich model do you trust?\n\nThe complicated model fits the data better.\nBut it is not economical.\n\nA model is convincing when it fits a lot of data surprisingly well.\n\nIt is not surprising that a complicated model can fit a small amount of data well.\nModels fit both signal and noise.\n\n\n\n\n\n\nA large number of different methods have been developed to reduce overfitting.\n\nWeight-decay\nWeight-sharing - reduce model flexibility by adding constraints on weights\nEarly stopping - stop training when by monitoring the Test error.\nModel averaging - use an ensemble of models\nBayesian fitting of neural nets - like averaging but weighed\nDropout - (hide data from half the net)\nGenerative pre-training - (more data)\n\nMany of these methods will be described in lecture 7.\n\n\n\n\noverfitting"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03e.html#overfitting-the-downside-of-using-powerful-models",
    "href": "notes/dnn/dnn-03/l03e.html#overfitting-the-downside-of-using-powerful-models",
    "title": "Deep Neural Networks - Notes for lecture 3e",
    "section": "",
    "text": "Regularization - How to ensure that learned weights work well for cases we did not see during training?\n\nThe training data contains information about the regularities in the mapping from input to output. But it also contains two types of noise.\n\nThe target values may be unreliable (usually only a minor worry).\nThere is sampling error. There will be accidental regularities just because of the particular training cases that were chosen.\n\nWhen we fit the model, it cannot tell which regularities are real and which are caused by sampling error.\n\nSo it fits both kinds of regularity.\nIf the model is very flexible it can model the sampling error really well. This is a disaster."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03e.html#a-simple-example-of-overfitting",
    "href": "notes/dnn/dnn-03/l03e.html#a-simple-example-of-overfitting",
    "title": "Deep Neural Networks - Notes for lecture 3e",
    "section": "",
    "text": "overfitting\n\n\n\n\nWhich output value should you predict for this test input?\nWhich model do you trust?\n\nThe complicated model fits the data better.\nBut it is not economical.\n\nA model is convincing when it fits a lot of data surprisingly well.\n\nIt is not surprising that a complicated model can fit a small amount of data well.\nModels fit both signal and noise."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03e.html#how-to-reduce-overfitting",
    "href": "notes/dnn/dnn-03/l03e.html#how-to-reduce-overfitting",
    "title": "Deep Neural Networks - Notes for lecture 3e",
    "section": "",
    "text": "A large number of different methods have been developed to reduce overfitting.\n\nWeight-decay\nWeight-sharing - reduce model flexibility by adding constraints on weights\nEarly stopping - stop training when by monitoring the Test error.\nModel averaging - use an ensemble of models\nBayesian fitting of neural nets - like averaging but weighed\nDropout - (hide data from half the net)\nGenerative pre-training - (more data)\n\nMany of these methods will be described in lecture 7.\n\n\n\n\noverfitting"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05a.html",
    "href": "notes/dnn/dnn-05/l05a.html",
    "title": "Deep Neural Networks - Notes for lecture 5a",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05a.html#things-that-make-it-hard-to-recognize-objects",
    "href": "notes/dnn/dnn-05/l05a.html#things-that-make-it-hard-to-recognize-objects",
    "title": "Deep Neural Networks - Notes for lecture 5a",
    "section": "Things that make it hard to recognize objects",
    "text": "Things that make it hard to recognize objects\n\n\n\n\nSegmentation: Real scenes are cluHered with other objects:\n\nIts hard to tell which pieces go together as parts of the same object.\nParts of an object can be hidden behind other objects.\n\nLighting: The intensties of the pixels are determined as much by the lighting as by the objects.\nDeformation: Objects can deform in a variety of non-affine ways:\n\ne.g a hand-written 2 can have a large loop or just a cusp.\n\n\nAffordances: Object classes are often defined by how they are used:\n\nChairs are things designed for sitting on so they have a wide variety of physical shapes."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05a.html#more-things-that-make-it-hard-to-recognize-objects",
    "href": "notes/dnn/dnn-05/l05a.html#more-things-that-make-it-hard-to-recognize-objects",
    "title": "Deep Neural Networks - Notes for lecture 5a",
    "section": "More things that make it hard to recognize objects",
    "text": "More things that make it hard to recognize objects\n\n\n\n\nViewpoint: Changes in viewpoint cause changes in images that standard learning methods cannot cope with.\n\nInformation hops between input dimensions (i.e. pixels)\n\n\nImagine a medical database in which the age of a patient sometimes hops to the input dimension that normally codes for weight!\n\nTo apply machine learning we would first want to eliminate this dimension-hopping"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html",
    "href": "notes/dnn/dnn-05/l_05.html",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#things-that-make-it-hard-to-recognize-objects",
    "href": "notes/dnn/dnn-05/l_05.html#things-that-make-it-hard-to-recognize-objects",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Things that make it hard to recognize objects",
    "text": "Things that make it hard to recognize objects\n\n\n\n\nSegmentation: Real scenes are cluHered with other objects:\n\nIts hard to tell which pieces go together as parts of the same object.\nParts of an object can be hidden behind other objects.\n\nLighting: The intensties of the pixels are determined as much by the lighting as by the objects.\nDeformation: Objects can deform in a variety of non-affine ways:\n\ne.g a hand-written 2 can have a large loop or just a cusp.\n\n\nAffordances: Object classes are often defined by how they are used:\n\nChairs are things designed for sitting on so they have a wide variety of physical shapes."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#more-things-that-make-it-hard-to-recognize-objects",
    "href": "notes/dnn/dnn-05/l_05.html#more-things-that-make-it-hard-to-recognize-objects",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "More things that make it hard to recognize objects",
    "text": "More things that make it hard to recognize objects\n\n\n\n\nViewpoint: Changes in viewpoint cause changes in images that standard learning methods cannot cope with.\n\nInformation hops between input dimensions (i.e. pixels)\n\n\nImagine a medical database in which the age of a patient sometimes hops to the input dimension that normally codes for weight!\n\nTo apply machine learning we would first want to eliminate this dimension-hopping"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#some-ways-to-achieve-viewpoint-invariance",
    "href": "notes/dnn/dnn-05/l_05.html#some-ways-to-achieve-viewpoint-invariance",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Some ways to achieve viewpoint invariance",
    "text": "Some ways to achieve viewpoint invariance\n\nWe are so good at viewpoint invariance that it is hard to appreciate how difficult it is.\n\nIts one of the main difficulties in making computers perceive.\nWe still don’t have generally accepted solutions.\n\nThere are several different approaches:\n\nUse redundant invariant features.\nPut a box around the object and use normalized pixels.\n\nLecture 5c: Use replicated features with pooling. This is called “convolutional neural nets”\nUse a hierarchy of parts that have explicit poses relative to the camera (this will be described in detail later in the course)."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-invariant-feature-approach",
    "href": "notes/dnn/dnn-05/l_05.html#the-invariant-feature-approach",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The invariant feature approach",
    "text": "The invariant feature approach\n\nExtract a large, redundant set of features that are invariant under transformations\n\ne.g. pair of roughly parallel lines with a red dot between them.\nThis is what baby herring gulls use to know where to peck for food.\n\nWith enough invariant features, there is only one way to assemble them into an object.\n\nWe don’t need to represent the relationships between features directly because they are captured by other features.\n\nFor recognition, we must avoid forming features from parts of different objects."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-judicious-normalization-approach",
    "href": "notes/dnn/dnn-05/l_05.html#the-judicious-normalization-approach",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The judicious normalization approach",
    "text": "The judicious normalization approach\n\nPut a box around the object and use it as a coordinate frame for a set of normalized pixels.\n\nThis solves the dimension-hopping problem. If we choose the box correctly, the same part of an object always occurs on the same normalized pixels.\n\nThe box can provide invariance to many degrees of freedom: translation, rotation, scale, shear, stretch …\nBut choosing the box is difficult because of:\nSegmentation errors, occlusion, unusual orientations. • We need to recognize the shape to get the box right!"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-brute-force-normalization-approach",
    "href": "notes/dnn/dnn-05/l_05.html#the-brute-force-normalization-approach",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The brute force normalization approach",
    "text": "The brute force normalization approach\n\nWhen training the recognizer, use well-segmented, upright images to fit the correct box.\nAt test time try all possible boxes in a range of positions and scales.\n\nThis approach is widely used for detecting upright things like faces and house numbers in unsegmented images.\n\nIt is much more efficient if the recognizer can cope with some variation in position and scale so that we can use a coarse grid when trying all possible boxes."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-replicated-feature-approach-currently-the-dominant-approach-for-neural-networks",
    "href": "notes/dnn/dnn-05/l_05.html#the-replicated-feature-approach-currently-the-dominant-approach-for-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The replicated feature approach (currently the dominant approach for neural networks)",
    "text": "The replicated feature approach (currently the dominant approach for neural networks)\n\n\n\n\nUse many different copies of the same feature detector with different positions.\n\nCould also replicate across scale and orientation (tricky and expensive)\nReplication greatly reduces the number of free parameters to be learned.\n\nUse several different feature types, each with its own map of replicated detectors.\n\nAllows each patch of image to be represented in several ways."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#backpropagation-with-weight-constraints",
    "href": "notes/dnn/dnn-05/l_05.html#backpropagation-with-weight-constraints",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Backpropagation with weight constraints",
    "text": "Backpropagation with weight constraints\n\nIt’s easy to modify the backpropagation algorithm to incorporate linear constraints between the weights.\n\nWe compute the gradients as usual, and then modify the gradients so that they satisfy the constraints.\n\nSo if the weights started off satisfying the constraints, they will continue to satisfy them."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#what-does-replicating-the-feature-detectors-achieve",
    "href": "notes/dnn/dnn-05/l_05.html#what-does-replicating-the-feature-detectors-achieve",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "What does replicating the feature detectors achieve?",
    "text": "What does replicating the feature detectors achieve?\n\n\n \n\nEquivariant activities: Replicated features do not make the neural activities invariant to translation. The activities are equivariant.\nInvariant knowledge: If a feature is useful in some locations during training, detectors for that feature will be available in all locations during testing."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#pooling-the-outputs-of-replicated-feature-detectors",
    "href": "notes/dnn/dnn-05/l_05.html#pooling-the-outputs-of-replicated-feature-detectors",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Pooling the outputs of replicated feature detectors",
    "text": "Pooling the outputs of replicated feature detectors\n• Get a small amount of translational invariance at each level by averaging four neighboring replicated detectors to give a single output to the next level. – This reduces the number of inputs to the next layer of feature extraction, thus allowing us to have many more different feature maps. – Taking the maximum of the four works slightly better. • Problem: After several levels of pooling, we have lost information about the precise positions of things. – This makes it impossible to use the precise spatial relationships between high-level parts for recognition"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-architecture-of-lenet5",
    "href": "notes/dnn/dnn-05/l_05.html#the-architecture-of-lenet5",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The architecture of LeNet5",
    "text": "The architecture of LeNet5\n\n\n\nThe architecture of LeNet5"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-82-errors-made-by-lenet5",
    "href": "notes/dnn/dnn-05/l_05.html#the-82-errors-made-by-lenet5",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The 82 errors made by LeNet5",
    "text": "The 82 errors made by LeNet5\n\n\n\nerrors made by LeNet5\n\n\nNotice that most of the errors are cases that people find quite easy.\nThe human error rate is probably 20 to 30 errors but nobody has had the patience to measure it."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#priors-and-prejudice",
    "href": "notes/dnn/dnn-05/l_05.html#priors-and-prejudice",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Priors and Prejudice",
    "text": "Priors and Prejudice\n\nWe can put our prior knowledge about the task into the network by designing appropriate:\n\nConnectivity.\nWeight constraints.\nNeuron activation functions\n\nThis is less intrusive than handdesigning the features.\n\nBut it still prejudices the network towards the particular way of solving the problem that we had in mind.\n\nAlternatively, we can use our prior knowledge to create a whole lot more training data.\n\nThis may require a lot of work (Hofman&Tresp, 1993)\nIt may make learning take much longer.\n\nIt allows optimization to discover clever ways of using the multi-layer network that we did not think of.\n\nAnd we may never fully understand how it does it."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-brute-force-approach",
    "href": "notes/dnn/dnn-05/l_05.html#the-brute-force-approach",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The brute force approach",
    "text": "The brute force approach\n\nLeNet uses knowledge about the invariances to design:\n\nthe local connectivity\nthe weight-sharing\nthe pooling.\n\nThis achieves about 80 errors.\n\nThis can be reduced to about 40 errors by using many different transformations of the input and other tricks (Ranzato 2008)\n\nCiresan et. al. (2010) inject knowledge of invariances by creating a huge amount of carefully designed extra training data:\n\nFor each training image, they produce many new training examples by applying many different transformations.\nThey can then train a large, deep, dumb net on a GPU without much overfitting.\n\nThey achieve about 35 errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-errors-made-by-the-ciresan-et.-al.-net",
    "href": "notes/dnn/dnn-05/l_05.html#the-errors-made-by-the-ciresan-et.-al.-net",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The errors made by the Ciresan et. al. net",
    "text": "The errors made by the Ciresan et. al. net\n\n\n\nerrors made by the Ciresan\n\n\nThe top printed digit is the right answer. The bottom two printed digits are the network’s best two guesses.\nThe right answer is almost always in the top 2 guesses.\nWith model averaging they can now get about 25 errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#how-to-detect-a-significant-drop-in-the-error-rate",
    "href": "notes/dnn/dnn-05/l_05.html#how-to-detect-a-significant-drop-in-the-error-rate",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "How to detect a significant drop in the error rate",
    "text": "How to detect a significant drop in the error rate\n\n\n\n\n\nMcNemar test 1\n\n\n\n\n\nMcNemar test 1\n\n\n\nIs 30 errors in 10,000 test cases significantly beHer than 40 errors?\n\nIt all depends on the particular errors!\nThe McNemar test uses the particular errors and can be much more powerful than a test that just uses the number of errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-ilsvrc-2012-competition-on-imagenet",
    "href": "notes/dnn/dnn-05/l_05.html#the-ilsvrc-2012-competition-on-imagenet",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The ILSVRC-2012 competition on ImageNet",
    "text": "The ILSVRC-2012 competition on ImageNet\n\nThe dataset has 1.2 million highresolution training images.\nThe classification task:\n\nGet the “correct” class in your top 5 bets. There are 1000 classes.\n\nThe localization task:\n\nFor each bet, put a box around the object. Your box must have at least 50% overlap with the correct box.\n\nSome of the best existing computer vision methods were tried on this dataset by leading computer vision groups from Oxford, INRIA, XRCE, …\n\nComputer vision systems use complicated multi-stage systems.\nThe early stages are typically hand-tuned by optimizing a few parameters\n\n\nExamples from the test set (with the network’s guesses)\n\n\n\n\n\nExamples from the test set"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#error-rates-on-the-ilsvrc-2012-competition",
    "href": "notes/dnn/dnn-05/l_05.html#error-rates-on-the-ilsvrc-2012-competition",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Error rates on the ILSVRC-2012 competition",
    "text": "Error rates on the ILSVRC-2012 competition"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#a-neural-network-for-imagenet",
    "href": "notes/dnn/dnn-05/l_05.html#a-neural-network-for-imagenet",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "A neural network for ImageNet",
    "text": "A neural network for ImageNet\n\nAlex Krizhevsky (NIPS 2012) developed a very deep convolutional neural net of the type pioneered by Yann Le Cun. Its architecture was:\n\n7 hidden layers not counting some max pooling layers.\nThe early layers were convolutional.\nThe last two layers were globally connected.\n\nThe activation functions were:\n\nRectified linear units in every hidden layer. These train much faster and are more expressive than logistic units.\nCompetitive normalization to suppress hidden activities when nearby units have stronger activities. This helps with variations in intensity."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#tricks-that-significantly-improve-generalization",
    "href": "notes/dnn/dnn-05/l_05.html#tricks-that-significantly-improve-generalization",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Tricks that significantly improve generalization",
    "text": "Tricks that significantly improve generalization\n\nTrain on random 224x224 patches from the 256x256 images to get more data. Also use left-right reflections of the images.\nAt test time, combine the opinions from ten different patches: The four 224x224 corner patches plus the central 224x224 patch plus the reflections of those five patches.\nUse dropout to regularize the weights in the globally connected layers (which contain most of the parameters).\n\nDropout means that half of the hidden units in a layer are randomly removed for each training example.\nThis stops hidden units from relying too much on other hidden units."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-hardware-required-for-alexs-net",
    "href": "notes/dnn/dnn-05/l_05.html#the-hardware-required-for-alexs-net",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The hardware required for Alex’s net",
    "text": "The hardware required for Alex’s net\n\nHe uses a very efficient implementation of convolutional nets on two Nvidia GTX 580 Graphics Processor Units (over 1000 fast liHle cores)\n\nGPUs are very good for matrix-matrix multiplies.\nGPUs have very high bandwidth to memory.\nThis allows him to train the network in a week.\nIt also makes it quick to combine results from 10 patches at test time.\n\nWe can spread a network over many cores if we can communicate the states fast enough.\nAs cores get cheaper and datasets get bigger, big neural nets will improve faster than old-fashioned (i.e. pre Oct 2012) computer vision systems."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#finding-roads-in-high-resolution-images",
    "href": "notes/dnn/dnn-05/l_05.html#finding-roads-in-high-resolution-images",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Finding roads in high-resolution images",
    "text": "Finding roads in high-resolution images\n\n\n\n\n\nFinding roads\n\n\n\nIn (Mnih and Hinton 2012) the author, Vlad Mnih, used a non-convolutional net with local fields and multiple layers of rectified linear units to find roads in cluHered aerial images.\nIt takes a large image patch and predicts a binary road label for the central 16x16 pixels.\nThere is lots of labeled training data available for this task.\nThe task is hard for many reasons:\n\nOcclusion by buildings trees and cars.\nShadows, Lighting changes\nMinor viewpoint changes\n\nThe worst problems are incorrect labels:\n\nBadly registered maps\nArbitrary decisions about what counts as a road.\n\nBig neural nets trained on big image patches with millions of examples are the only hope.\n\n\n\n\nThe architecture of LeNet5\nerrors made by LeNet5\nerrors made by the Ciresan\nMcNemar test 1\nMcNemar test 1\nExamples from the test set\nFinding roads"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html",
    "href": "notes/dnn/dnn-05/l05d.html",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#the-ilsvrc-2012-competition-on-imagenet",
    "href": "notes/dnn/dnn-05/l05d.html#the-ilsvrc-2012-competition-on-imagenet",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "The ILSVRC-2012 competition on ImageNet",
    "text": "The ILSVRC-2012 competition on ImageNet\n\nThe dataset has 1.2 million highresolution training images.\nThe classification task:\n\nGet the “correct” class in your top 5 bets. There are 1000 classes.\n\nThe localization task:\n\nFor each bet, put a box around the object. Your box must have at least 50% overlap with the correct box.\n\nSome of the best existing computer vision methods were tried on this dataset by leading computer vision groups from Oxford, INRIA, XRCE, …\n\nComputer vision systems use complicated multi-stage systems.\nThe early stages are typically hand-tuned by optimizing a few parameters\n\n\nExamples from the test set (with the network’s guesses)\n\n\n\n\n\nExamples from the test set"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#error-rates-on-the-ilsvrc-2012-competition",
    "href": "notes/dnn/dnn-05/l05d.html#error-rates-on-the-ilsvrc-2012-competition",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "Error rates on the ILSVRC-2012 competition",
    "text": "Error rates on the ILSVRC-2012 competition"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#a-neural-network-for-imagenet",
    "href": "notes/dnn/dnn-05/l05d.html#a-neural-network-for-imagenet",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "A neural network for ImageNet",
    "text": "A neural network for ImageNet\n\nAlex Krizhevsky (NIPS 2012) developed a very deep convolutional neural net of the type pioneered by Yann Le Cun. Its architecture was:\n\n7 hidden layers not counting some max pooling layers.\nThe early layers were convolutional.\nThe last two layers were globally connected.\n\nThe activation functions were:\n\nRectified linear units in every hidden layer. These train much faster and are more expressive than logistic units.\nCompetitive normalization to suppress hidden activities when nearby units have stronger activities. This helps with variations in intensity."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#tricks-that-significantly-improve-generalization",
    "href": "notes/dnn/dnn-05/l05d.html#tricks-that-significantly-improve-generalization",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "Tricks that significantly improve generalization",
    "text": "Tricks that significantly improve generalization\n\nTrain on random 224x224 patches from the 256x256 images to get more data. Also use left-right reflections of the images.\nAt test time, combine the opinions from ten different patches: The four 224x224 corner patches plus the central 224x224 patch plus the reflections of those five patches.\nUse dropout to regularize the weights in the globally connected layers (which contain most of the parameters).\n\nDropout means that half of the hidden units in a layer are randomly removed for each training example.\nThis stops hidden units from relying too much on other hidden units."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#the-hardware-required-for-alexs-net",
    "href": "notes/dnn/dnn-05/l05d.html#the-hardware-required-for-alexs-net",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "The hardware required for Alex’s net",
    "text": "The hardware required for Alex’s net\n\nHe uses a very efficient implementation of convolutional nets on two Nvidia GTX 580 Graphics Processor Units (over 1000 fast liHle cores)\n\nGPUs are very good for matrix-matrix multiplies.\nGPUs have very high bandwidth to memory.\nThis allows him to train the network in a week.\nIt also makes it quick to combine results from 10 patches at test time.\n\nWe can spread a network over many cores if we can communicate the states fast enough.\nAs cores get cheaper and datasets get bigger, big neural nets will improve faster than old-fashioned (i.e. pre Oct 2012) computer vision systems."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#finding-roads-in-high-resolution-images",
    "href": "notes/dnn/dnn-05/l05d.html#finding-roads-in-high-resolution-images",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "Finding roads in high-resolution images",
    "text": "Finding roads in high-resolution images\n\n\n\n\n\nFinding roads\n\n\n\nIn (Mnih and Hinton 2012) the author, Vlad Mnih, used a non-convolutional net with local fields and multiple layers of rectified linear units to find roads in cluHered aerial images.\nIt takes a large image patch and predicts a binary road label for the central 16x16 pixels.\nThere is lots of labeled training data available for this task.\nThe task is hard for many reasons:\n\nOcclusion by buildings trees and cars.\nShadows, Lighting changes\nMinor viewpoint changes\n\nThe worst problems are incorrect labels:\n\nBadly registered maps\nArbitrary decisions about what counts as a road.\n\nBig neural nets trained on big image patches with millions of examples are the only hope.\n\n\n\n\nExamples from the test set\nFinding roads"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html",
    "href": "notes/dnn/dnn-04/l_04.html",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nWe have the basic method for creating hidden layers (backprop), we’re going to see what can be achieved with them. We start to ask how the network learns to use its hidden units, with a toy application to family trees and a real application to language modeling."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4a-learning-to-predict-the-next-word",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4a-learning-to-predict-the-next-word",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4a: Learning to predict the next word",
    "text": "Lecture 4a: Learning to predict the next word\n\n\n\n\nA simple example of relational information\n\n\n\n\n\nrelational information\n\n\n\n\nAnother way to express the same information\n\nMake a set of propositions using the 12 relationships:\n\nson, daughter, nephew, niece, father, mother, uncle, aunt\nbrother, sister, husband, wife\n\n(Colin has-father James)\n(Colin has-mother Victoria)\n(James has-wife Victoria) this follows from the two above\n(Charlotte has-brother Colin)\n(Victoria has-brother Arthur)\n(Charlotte has-uncle Arthur) this follows from the above\n\n\n\nA relational learning task\n\nGiven a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules (x has-mother y) & (y has-husband z) =&gt; (x has-father z)\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\nCan a neural network capture the same knowledge by searching through a continuous space of weights?\n\n\n\nThe structure of the neural net\n\n\n\n\n\nstructure of the neural net\n\n\n\n\n\n\nvisulization of 6 neuron weights\n\n\n\n\n\nthe relational data\n\n\n\n\n\nWhat the network learns ?\n\nThe six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\n\nNationality, generation, branch of the family tree.\n\nThese features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example:\n\nInput person is of generation 3 and\nrelationship requires answer to be one generation up\nimplies\nOutput person is of generation 2\n\n\n\n\nAnother way to see that it works\n\nTrain the network on all but 4 of the triples that can be made using the 12 relationships\n\nIt needs to sweep through the training set many times adjusting the weights slightly each time.\n\nThen test it on the 4 held-out cases.\n\nIt gets about 3/4 correct.\nThis is good for a 24-way choice.\nOn much bigger datasets we can train on a much smaller fraction of the data.\n\n\n\n\nA large-scale example\n\nSuppose we have a database of millions of relational facts of the form (A R B).\n\nWe could train a net to discover feature vector representations of the terms that allow the third term to be predicted from the first two.\nThen we could use the trained net to find very unlikely triples. These are good candidates for errors in the database.\n\nInstead of predicting the third term, we could use all three terms as input and predict the probability that the fact is correct.\n\nTo train such a net we need a good source of false facts.\n\n\n\n\nA relational learning task\n\nGiven a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules:\n\nHasMother(x,y)\\ and\\ HasHusband(y,z) \\implies HasFather(x, z)\n\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\n\nCan a neural network capture the same knowledge by searching through a continuous space of weights?\n\n\n\nThe structure of the neural net\n\nThe six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\nNationality, generation, branch of the family tree. These features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example: Input person is of generation 3 and relationship requires answer to be one generation up implies Output person is of generation 2 This video introduces distributed representations. It’s not actually about predicting words, but it’s building up to that. It does a great job of looking inside the brain of a neural network. That’s important, but not always easy to do."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4b-a-brief-diversion-into-cognitive-science",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4b-a-brief-diversion-into-cognitive-science",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4b: A brief diversion into cognitive science",
    "text": "Lecture 4b: A brief diversion into cognitive science\nThis video is part of the course, i.e. it’s not optional, despite what Geoff says in the beginning of the video. This video gives a high-level interpretation of what’s going on in the family tree network. This video contrasts two types of inference:\n\nConscious inference, based on relational knowledge.\nUnconscious inference, based on distributed representations."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4c-another-diversion-the-softmax-output-function",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4c-another-diversion-the-softmax-output-function",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4c: Another diversion: The Softmax output function",
    "text": "Lecture 4c: Another diversion: The Softmax output function\nA Softmax cost function is a general-purpose ML component/technique for combining binary discriminators into a probability distribution to construct a classifier We’ve seen binary threshold output neurons and logistic output neurons. This video presents a third type.\nThis one only makes sense if we have multiple output neurons.\n\nProblems with squared error\n\nThe squared error measure has some drawbacks:\n\nIf the desired output is 1 and the actual output is 0.00000001 there is almost no gradient for a logistic unit to fix up the error.\nIf we are trying to assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but we are depriving the network of this knowledge.\n\nIs there a different cost function that works better?\n\nYes: Force the outputs to represent a probability distribution across discrete alternatives\n\n\n\n\nSoftmax\nThe output units in a softmax group use a non-local non-linearity:\n\ny_i = \\frac{e^{z_i}}{\\sum_{j\\in group} e^{z_i}}\n\n\n\\frac{\\partial y_i}{\\partial z_i} = y_i(1-y_i)\n\n\n\n\n\n\nCross-entropy: the right cost function to use with SoftMax\n\nC=-\\sum_j t_j \\log y_i\n \n\\frac {\\partial C}{\\partial z_i} = - \\sum_j t_j \\frac {\\partial C}{\\partial y_i} \\frac {\\partial y_u}{\\partial z_i} = y_i -t_i\n\n\nThe right cost function is the negative log probability of the right answer.\nC has a very big gradient when the target value is 1 and the output is almost zero.\n\nA value of 0.000001 is much better than 0.000000001\nThe steepness of dC/dy exactly balances the flatness of dy/dz\n\n\nthe cross entropy cost function - is the correct cost function to use with SoftMax\nArchitectural Note:\nSoftMax unit +Cross-Entropy loss function =&gt; for classification"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4d-neuro-probabilistic-language-models",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4d-neuro-probabilistic-language-models",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4d: Neuro-probabilistic language models",
    "text": "Lecture 4d: Neuro-probabilistic language models\nThis is the first of several applications of neural networks that we’ll studying in some detail, in this course.\nSynonyms: word embedding; word feature vector; word encoding.\nAll of these describe the learned collection of numbers that is used to represent a word. “embedding” emphasizes that it’s a location in a high-dimensional space: it’s where the words are embedded in that space. When we check to see which words are close to each other, we’re thinking about that embedding.\n“feature vector” emphasizes that it’s a vector instead of a scalar, and that it’s componential, i.e. composed of multiple feature values.\n“encoding” is very generic and doesn’t emphasize anything specific. looks at the trigram model\n\nA basic problem in speech recognition\n\nWe cannot identify phonemes perfectly in noisy speech\n\nThe acoustic input is often ambiguous: there are several different words that fit the acoustic signal equally well.\n\nPeople use their understanding of the meaning of the utterance to hear the right words.\n\nWe do this unconsciously when we wreck a nice beach.\nWe are very good at it.\n\nThis means speech recognizers have to know which words are likely to come next and which are not.\n\nFortunately, words can be predicted quite well without full understanding.\n\n\n\n\nThe standard “trigram” method\n\nTake a huge amount of text and count the frequencies of all triples of words.\nUse these frequencies to make bets on the relative probabilities of words given the previous two words:\n\n\n\\frac{p(w_3=c|w_2=b,w_1=a)}{p(w_3=d|w_2=b,w_1=a)}=\\frac{count(abc)}{count(abd)}\n\n\nUntil very recently this was the state-of-the-art.\nWe cannot use a much bigger context because there are too many possibilities to store and the counts would mostly be zero.\nWe have to “back-off” to digrams when the count for a trigram is too small.\n\nThe probability is not zero just because the count is zero!\n\n\n\n\nInformation that the trigram model fails to use\n\nSuppose we have seen the sentence “the cat got squashed in the garden on friday”\nThis should help us predict words in the sentence “the dog got flattened in the yard on monday”\nA trigram model does not understand the similarities between\n\ncat/dog squashed/flattened garden/yard friday/monday\n\nTo overcome this limitation, we need to use the semantic and syntactic features of previous words to predict the features of the next word.\n\nUsing a feature representation also allows a context that contains many more previous words (e.g. 10)."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4e-ways-to-deal-with-the-large-number-of-possible-outputs",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4e-ways-to-deal-with-the-large-number-of-possible-outputs",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4e: Ways to deal with the large number of possible outputs",
    "text": "Lecture 4e: Ways to deal with the large number of possible outputs\nWhen softmax is very big it becomes hard to train and store.\n\nA serial architecture, based on trying candidate next words, using feature vectors (like in the family example). This means fewer parameters, but still a lot of work.\nUsing a binary tree.\nCollobert & Weston’s search for good feature vectors for words, without trying to predict the next word in a sentence."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#bengios-neural-net-for-predicting-the-next-word",
    "href": "notes/dnn/dnn-04/l_04.html#bengios-neural-net-for-predicting-the-next-word",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Bengio’s neural net for predicting the next word",
    "text": "Bengio’s neural net for predicting the next word\n\n\n\nA serial architecture\n\n\n\nA serial architecture\n\n\n\nA serial architecture\n\n\n\n\nLearning in the serial architecture\n\nAfter computing the logit score for each candidate word, use all of the logits in a softmax to get word probabilities.\nThe difference between the word probabilities and their target probabilities gives cross-entropy error derivatives.\n\nThe derivatives try to raise the score of the correct candidate and lower the scores of its high-scoring rivals.\n\nWe can save a lot of time if we only use a small set of candidates suggested by some other kind of predictor.\n\nFor example, we could use the neural net to revise the probabilities of the words that the trigram model thinks are likely.\n\n\n\n\nLearning to predict the next word by predicting a path through a tree\nIn Mnih, Yuecheng, and Hinton (2009) the authors show how to improve a state-of-the-art neural network language model that converts the previous “context” words into feature vectors and combines these feature vectors linearly to predict the feature vector of the next word.\nSignificant improvements in predictive accuracy are achieved by using a non-linear subnetwork to modulate the effects of the context words or to produce a non-linear correction term when predicting the feature vector.\n\n\n\n\n\nSoftmax as a tree\n\n\n\nArrange all the words in a binary tree with words as the leaves.\nUse the previous context to generate a prediction vector, v.\n\nCompare v with a learned vector, u, at each node of the tree.\nApply the logistic function to the scalar product of u and v to predict the probabilities of taking the two branches of the tree.\n\n\n\n\nA picture of the learning\n\n\n \n\n\nA convenient decomposition\n\nMaximizing the log probability of picking the target word is equivalent to maximizing the sum of the log probabilities of taking all the branches on the path that leads to the target word.\n\nSo during learning, we only need to consider the nodes on the correct path. This is an exponential win: log(N) instead of N.\nFor each of these nodes, we know the correct branch and we know the current probability of taking it so we can get derivatives for learning both the prediction vector v and that node vector u.\n\nUnfortunately, it is still slow at test time."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#a-simpler-way-to-learn-feature-vectors-for-words",
    "href": "notes/dnn/dnn-04/l_04.html#a-simpler-way-to-learn-feature-vectors-for-words",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "A simpler way to learn feature vectors for words",
    "text": "A simpler way to learn feature vectors for words\nThis method comes from the paper (Collobert and Weston 2008)\n\n\n\n\nDisplaying the learned feature vectors in a 2-D map\n\nWe can get an idea of the quality of the learned feature vectors by displaying them in a 2-D map.\n\nDisplay very similar vectors very close to each other.\nUse a multi-scale method called “t-sne” that also displays similar clusters near each other.\n\nThe learned feature vectors capture lots of subtle semantic distinctions, just by looking at strings of words.\n\nNo extra supervision required.\nThe information is all in the contexts that the word is used in.\nConsider “She scrommed him with the frying pan.”\n\n\n\n\nPart of a 2-D map of the 2500 most common words\n\n \n\n\n\ntsne_output\n\n\n\n\n\n\nrelational information\nstructure of the neural net\nvisulization of 6 neuron weights\nthe relational data\nA serial architecture\nA serial architecture\nSoftmax as a tree\ntsne_output"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html",
    "href": "notes/dnn/dnn-04/l_04e.html",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "When softmax is very big it becomes hard to train and store.\n\nA serial architecture, based on trying candidate next words, using feature vectors (like in the family example). This means fewer parameters, but still a lot of work.\nUsing a binary tree.\nCollobert & Weston’s search for good feature vectors for words, without trying to predict the next word in a sentence.\n\n\n\n\n\n\n\n\nA serial architecture\n\n\n\nSurprise this uses the same architecture\n\n\n\n\n\n\n\n\n\nA serial architecture\n\n\n\nNo Surprise this to uses basicaly the same architecture - only we are looking back.\n\n\n\n\n\nAfter computing the logit score for each candidate word, use all of the logits in a softmax to get word probabilities.\nThe difference between the word probabilities and their target probabilities gives cross-entropy error derivatives.\n\nThe derivatives try to raise the score of the correct candidate and lower the scores of its high-scoring rivals.\n\nWe can save a lot of time if we only use a small set of candidates suggested by some other kind of predictor.\n\nFor example, we could use the neural net to revise the probabilities of the words that the trigram model thinks are likely.\n\n\n\n\n\nIn Mnih, Yuecheng, and Hinton (2009) the authors show how to improve a state-of-the-art neural network language model that converts the previous “context” words into feature vectors and combines these feature vectors linearly to predict the feature vector of the next word.\nSignificant improvements in predictive accuracy are achieved by using a non-linear subnetwork to modulate the effects of the context words or to produce a non-linear correction term when predicting the feature vector.\n\n\n\n\n\nSoftmax as a tree\n\n\n\nArrange all the words in a binary tree with words as the leaves.\nUse the previous context to generate a prediction vector, v.\n\nCompare v with a learned vector, u, at each node of the tree.\nApply the logistic function to the scalar product of u and v to predict the probabilities of taking the two branches of the tree.\n\n\n\n\n\n\n\n \n\n\n\n\nMaximizing the log probability of picking the target word is equivalent to maximizing the sum of the log probabilities of taking all the branches on the path that leads to the target word.\n\nSo during learning, we only need to consider the nodes on the correct path. This is an exponential win: log(N) instead of N.\nFor each of these nodes, we know the correct branch and we know the current probability of taking it so we can get derivatives for learning both the prediction vector v and that node vector u.\n\nUnfortunately, it is still slow at test time.\n\n\n\n\nThis method comes from the paper (Collobert and Weston 2008)\n\n\n\n\n\n\n\nWe can get an idea of the quality of the learned feature vectors by displaying them in a 2-D map.\n\nDisplay very similar vectors very close to each other.\nUse a multi-scale method called “t-sne” that also displays similar clusters near each other.\n\nThe learned feature vectors capture lots of subtle semantic distinctions, just by looking at strings of words.\n\nNo extra supervision required.\nThe information is all in the contexts that the word is used in.\nConsider “She scrommed him with the frying pan.”\n\n\n\n\n\n\n\n\n\n\n\n\ntsne_output\n\n\n\n\n\ntsne_output\n\n\n\n\n\ntsne_output\n\n\n\nA serial architecture\nA serial architecture\nSoftmax as a tree\ntsne_output\ntsne_output\ntsne_output"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#bengios-neural-net-for-predicting-the-next-word",
    "href": "notes/dnn/dnn-04/l_04e.html#bengios-neural-net-for-predicting-the-next-word",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "A serial architecture\n\n\n\nSurprise this uses the same architecture"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#a-serial-architecture",
    "href": "notes/dnn/dnn-04/l_04e.html#a-serial-architecture",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "A serial architecture\n\n\n\nNo Surprise this to uses basicaly the same architecture - only we are looking back."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#learning-in-the-serial-architecture",
    "href": "notes/dnn/dnn-04/l_04e.html#learning-in-the-serial-architecture",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "After computing the logit score for each candidate word, use all of the logits in a softmax to get word probabilities.\nThe difference between the word probabilities and their target probabilities gives cross-entropy error derivatives.\n\nThe derivatives try to raise the score of the correct candidate and lower the scores of its high-scoring rivals.\n\nWe can save a lot of time if we only use a small set of candidates suggested by some other kind of predictor.\n\nFor example, we could use the neural net to revise the probabilities of the words that the trigram model thinks are likely."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#learning-to-predict-the-next-word-by-predicting-a-path-through-a-tree",
    "href": "notes/dnn/dnn-04/l_04e.html#learning-to-predict-the-next-word-by-predicting-a-path-through-a-tree",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "In Mnih, Yuecheng, and Hinton (2009) the authors show how to improve a state-of-the-art neural network language model that converts the previous “context” words into feature vectors and combines these feature vectors linearly to predict the feature vector of the next word.\nSignificant improvements in predictive accuracy are achieved by using a non-linear subnetwork to modulate the effects of the context words or to produce a non-linear correction term when predicting the feature vector.\n\n\n\n\n\nSoftmax as a tree\n\n\n\nArrange all the words in a binary tree with words as the leaves.\nUse the previous context to generate a prediction vector, v.\n\nCompare v with a learned vector, u, at each node of the tree.\nApply the logistic function to the scalar product of u and v to predict the probabilities of taking the two branches of the tree."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#a-convenient-decomposition",
    "href": "notes/dnn/dnn-04/l_04e.html#a-convenient-decomposition",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "Maximizing the log probability of picking the target word is equivalent to maximizing the sum of the log probabilities of taking all the branches on the path that leads to the target word.\n\nSo during learning, we only need to consider the nodes on the correct path. This is an exponential win: log(N) instead of N.\nFor each of these nodes, we know the correct branch and we know the current probability of taking it so we can get derivatives for learning both the prediction vector v and that node vector u.\n\nUnfortunately, it is still slow at test time."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#a-simpler-way-to-learn-feature-vectors-for-words",
    "href": "notes/dnn/dnn-04/l_04e.html#a-simpler-way-to-learn-feature-vectors-for-words",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "This method comes from the paper (Collobert and Weston 2008)"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#displaying-the-learned-feature-vectors-in-a-2-d-map",
    "href": "notes/dnn/dnn-04/l_04e.html#displaying-the-learned-feature-vectors-in-a-2-d-map",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "We can get an idea of the quality of the learned feature vectors by displaying them in a 2-D map.\n\nDisplay very similar vectors very close to each other.\nUse a multi-scale method called “t-sne” that also displays similar clusters near each other.\n\nThe learned feature vectors capture lots of subtle semantic distinctions, just by looking at strings of words.\n\nNo extra supervision required.\nThe information is all in the contexts that the word is used in.\nConsider “She scrommed him with the frying pan.”"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#part-of-a-2-d-map-of-the-2500-most-common-words",
    "href": "notes/dnn/dnn-04/l_04e.html#part-of-a-2-d-map-of-the-2500-most-common-words",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "tsne_output\n\n\n\n\n\ntsne_output\n\n\n\n\n\ntsne_output\n\n\n\nA serial architecture\nA serial architecture\nSoftmax as a tree\ntsne_output\ntsne_output\ntsne_output"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04c.html",
    "href": "notes/dnn/dnn-04/l_04c.html",
    "title": "Deep Neural Networks - Notes for lecture 4c",
    "section": "",
    "text": "A Softmax cost function is a general-purpose ML component/technique for combining binary discriminators into a probability distribution to construct a classifier We’ve seen binary threshold output neurons and logistic output neurons. This video presents a third type.\nThis one only makes sense if we have multiple output neurons.\n\n\n\nThe squared error measure has some drawbacks:\n\nIf the desired output is 1 and the actual output is 0.00000001 there is almost no gradient for a logistic unit to fix up the error.\nIf we are trying to assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but we are depriving the network of this knowledge.\n\nIs there a different cost function that works better?\n\nYes: Force the outputs to represent a probability distribution across discrete alternatives\n\n\n\n\n\nThe output units in a softmax group use a non-local non-linearity:\n\ny_i = \\frac{e^{z_i}}{\\sum_{j\\in group} e^{z_i}}\n\n\n\\frac{\\partial y_i}{\\partial z_i} = y_i(1-y_i)\n\n\n\n\n\n\n\n\nC=-\\sum_j t_j \\log y_i\n \n\\frac {\\partial C}{\\partial z_i} = - \\sum_j t_j \\frac {\\partial C}{\\partial y_i} \\frac {\\partial y_u}{\\partial z_i} = y_i -t_i\n\n\nThe right cost function is the negative log probability of the right answer.\nC has a very big gradient when the target value is 1 and the output is almost zero.\n\nA value of 0.000001 is much better than 0.000000001\nThe steepness of dC/dy exactly balances the flatness of dy/dz\n\n\nthe cross entropy cost function - is the correct cost function to use with SoftMax\nArchitectural Note:\nSoftMax unit +Cross-Entropy loss function =&gt; for classification"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04c.html#problems-with-squared-error",
    "href": "notes/dnn/dnn-04/l_04c.html#problems-with-squared-error",
    "title": "Deep Neural Networks - Notes for lecture 4c",
    "section": "",
    "text": "The squared error measure has some drawbacks:\n\nIf the desired output is 1 and the actual output is 0.00000001 there is almost no gradient for a logistic unit to fix up the error.\nIf we are trying to assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but we are depriving the network of this knowledge.\n\nIs there a different cost function that works better?\n\nYes: Force the outputs to represent a probability distribution across discrete alternatives"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04c.html#softmax",
    "href": "notes/dnn/dnn-04/l_04c.html#softmax",
    "title": "Deep Neural Networks - Notes for lecture 4c",
    "section": "",
    "text": "The output units in a softmax group use a non-local non-linearity:\n\ny_i = \\frac{e^{z_i}}{\\sum_{j\\in group} e^{z_i}}\n\n\n\\frac{\\partial y_i}{\\partial z_i} = y_i(1-y_i)"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04c.html#cross-entropy-the-right-cost-function-to-use-with-softmax",
    "href": "notes/dnn/dnn-04/l_04c.html#cross-entropy-the-right-cost-function-to-use-with-softmax",
    "title": "Deep Neural Networks - Notes for lecture 4c",
    "section": "",
    "text": "C=-\\sum_j t_j \\log y_i\n \n\\frac {\\partial C}{\\partial z_i} = - \\sum_j t_j \\frac {\\partial C}{\\partial y_i} \\frac {\\partial y_u}{\\partial z_i} = y_i -t_i\n\n\nThe right cost function is the negative log probability of the right answer.\nC has a very big gradient when the target value is 1 and the output is almost zero.\n\nA value of 0.000001 is much better than 0.000000001\nThe steepness of dC/dy exactly balances the flatness of dy/dz\n\n\nthe cross entropy cost function - is the correct cost function to use with SoftMax\nArchitectural Note:\nSoftMax unit +Cross-Entropy loss function =&gt; for classification"
  },
  {
    "objectID": "notes/dnn/dnn-dropout/2017-08-06-dropout.html",
    "href": "notes/dnn/dnn-dropout/2017-08-06-dropout.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "My thoughts are that we should be able to do better than this version of dropout. - Shortcoming: - Dropout on units can render the net very poor. - Drop out slows training down - since we don’t update half the units and probably a large number of the weights. - For different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures. - We should track dropout related stats to better understand the confidence of the model. - A second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout? 1. Start with some combinations \\binom k n of the weights. where k = | {training\\; set}|*{minibatch\\_size}. We use the same dropout for each mini-batch, then switch. 2. Each epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ? 3. When we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net. 4. Ideally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step. 5. We could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism. 6. We should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "notes/dnn/dnn-dropout/2017-08-06-dropout.html#dropout",
    "href": "notes/dnn/dnn-dropout/2017-08-06-dropout.html#dropout",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "My thoughts are that we should be able to do better than this version of dropout. - Shortcoming: - Dropout on units can render the net very poor. - Drop out slows training down - since we don’t update half the units and probably a large number of the weights. - For different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures. - We should track dropout related stats to better understand the confidence of the model. - A second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout? 1. Start with some combinations \\binom k n of the weights. where k = | {training\\; set}|*{minibatch\\_size}. We use the same dropout for each mini-batch, then switch. 2. Each epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ? 3. When we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net. 4. Ideally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step. 5. We could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism. 6. We should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "notes/rhetoric/biblography.html",
    "href": "notes/rhetoric/biblography.html",
    "title": "Rhetoric Bibliography",
    "section": "",
    "text": "(Corbett and Connors 1999), (Engell 1999), (Torricelli and Carroll 2000), (Lucas 2011), (Farrell 1993)"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#textbooks",
    "href": "notes/rhetoric/biblography.html#textbooks",
    "title": "Rhetoric Bibliography",
    "section": "",
    "text": "(Corbett and Connors 1999), (Engell 1999), (Torricelli and Carroll 2000), (Lucas 2011), (Farrell 1993)"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#part-i-the-nature-of-rhetoric-a-long-struggle",
    "href": "notes/rhetoric/biblography.html#part-i-the-nature-of-rhetoric-a-long-struggle",
    "title": "Rhetoric Bibliography",
    "section": "Part I: The Nature of Rhetoric / A Long Struggle",
    "text": "Part I: The Nature of Rhetoric / A Long Struggle\n\nNature and Purposes of Rhetoric; Original Five Canons; Three Kinds of Persuasive Discourse; Importance of Rhetoric Today (Corbett and Connors 1999, p15–26)\nIsaac Backus, Speech to Massachusetts Constitutional Convention [from “Mr. President, I have said very little” to “any people upon earth.”]\nFrederick Douglass, Fifth of July Oration\nCaleb Bingham, from The Columbian Orator\nDavid Blight, Editor’s Introduction to The Columbian Orator, “The Peculiar Dialogue Between Caleb Bingham and Frederick Douglass”\nJames Engell, from The Committed Word: Literature and Public Values, ch. 1, “The Committed Word”\nHenry Clay on Slavery and Abolition\nAbraham Lincoln, “A House Divided”\nDouglas Wilson, from Honor’s Voice, The Transformation of Abraham Lincoln\nW.E.B. DuBois on “The Battle for Humanity” (Torricelli and Carroll 2000, p17–20)\nThurgood Marshall, from Brown v. Board of Education (Torricelli and Carroll 2000, p198–99)\nJohn Quincy Adams, from Lectures on Rhetoric and Oratory, Inaugural Oration; Lecture I, “General View of Rhetoric and Oratory”\nMartin Luther King, Jr., “Letter from Birmingham Jail” (Corbett and Connors 1999, p301–14) with analysis of arrangements (Corbett and Connors 1999, p315–19), and analysis of style by Richard P. Fulkerson (Corbett and Connors 1999, p478–83)\nKing, Call to End Segregation–“I have a dream…” (Torricelli and Carroll 2000, p234–37)\nThomas B. Farrell, An Ethically and Aesthetically Significant Art from Norms of Rhetorical Culture, hereafter “Norms.” Optional but encouraged.\nLyndon Johnson, Address to Congress, Voting Rights Act of 1965 (Torricelli and Carroll 2000, p259–65)"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#part-ii-thesis-invention-persuasion-topics-shaping-the-republic",
    "href": "notes/rhetoric/biblography.html#part-ii-thesis-invention-persuasion-topics-shaping-the-republic",
    "title": "Rhetoric Bibliography",
    "section": "Part II: Thesis, Invention, Persuasion, Topics / Shaping the Republic",
    "text": "Part II: Thesis, Invention, Persuasion, Topics / Shaping the Republic\n\nThe Thesis; Three Modes of Persuasion: logos, pathos, ethos (Corbett and Connors 1999, p27–84; Lucas 2011, p325–346, p142–159)\n“Looking for an Argument” (Corbett and Connors 1999, p130–37; Lucas 2011, p82–93)\nGeorge Campbell, from The Philosophy of Rhetoric, 1, i, I, “Eloquence”\nDelivery of Speeches (Lucas 2011, p67–71, p239–252)\nPatrick Henry, Speech in Virginia Convention of Delegates\nThomas Paine, from Common Sense\nBernard Bailyn, from The Ideological Origins of the American Revolution, ch. 1, “The Literature of Revolution”\nAlan Heimert, from Religion and the American Mind\nDeclaration of Independence\nPauline Maier, from American Scripture: Making the Declaration of Independence\nAristotle, from Rhetoric\nRhetoric as ethical practice (Lucas 2011, p30–40; Farrell 1993, p61–83) Optional\nAlexander Hamilton, Federalist No. 1\nMadison, Federalist No. 10 (Corbett and Connors 1999, p214–20) and analysis by Mark Ashin [220-30]\nJames Madison, Federalist No. 37\nHenry, against the Constitution, Virginia Ratifying Convention\nMadison, for the Constitution, Virginia Ratifying Convention"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#part-iii-topics-foreign-affairs",
    "href": "notes/rhetoric/biblography.html#part-iii-topics-foreign-affairs",
    "title": "Rhetoric Bibliography",
    "section": "Part III Topics / Foreign Affairs",
    "text": "Part III Topics / Foreign Affairs\n\nTopics [Corbett and Connors (1999) p84-120; also p209-14, Part V below]\nSpecial Topics (Corbett and Connors 1999, p120–30)\nWoodrow Wilson Requests Declaration of War (Torricelli and Carroll 2000, p37–39)\nF.D. Roosevelt on the “four freedoms” (Torricelli and Carroll 2000, p120–23)\n\nJ.F. Kennedy, “Ich bin ein Berliner” (Torricelli and Carroll 2000, p232–33)\n\nJ.Q. Adams, from Lecture II, “Eloquence Considered”\nShen Tong on Tiananmen Square (Torricelli and Carroll 2000, p385–88)\nElizabeth Linder and Connie Mack, Congressional Hearing (Farrell 1993, p304–08)\nRhetoric as a particular practice of civic virtue (Farrell 1993, p93–100) Optional"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#part-iv-designing-arguments-justice-social-choice",
    "href": "notes/rhetoric/biblography.html#part-iv-designing-arguments-justice-social-choice",
    "title": "Rhetoric Bibliography",
    "section": "Part IV Designing Arguments / Justice, Social Choice",
    "text": "Part IV Designing Arguments / Justice, Social Choice\n\nArrangement [Organization] and Parts Lucas (2011)\nRose Schneiderman on Triangle Shirtwaist Fire (Torricelli and Carroll 2000, p32–33)\nAl Smith Assails William Randolph Hearst (Torricelli and Carroll 2000, p57–58)\nJohn L. Lewis on “Brutality and Oppression” of Big Business (Torricelli and Carroll 2000, p112–14)\nSusan B. Anthony on Women’s Rights before a Congressional Committee\nPhyllis Schlafly Opposes Women’s Rights Movement (Torricelli and Carroll 2000, p329–35)\nHugh Blair, from Lectures on Rhetoric and Belles Lettres, Parts of a Discourse\nMargaret Sanger Promotes Birth Control (Torricelli and Carroll 2000, p68–70)\nSarah Weddington, from Roe v. Wade (Torricelli and Carroll 2000, p302–04)\nMario Cuomo on abortion, an analysis (Farrell 1993, p213–24)\nJ.Q. Adams, from Lecture XI, “Deliberative Oratory”\nGeorge Campbell, from 1, i, V, “Of Intuitive and Deductive Evidence”\nClarence Darrow on sentencing of Leopold and Loeb (Torricelli and Carroll 2000, p76–78)\nSarah Brady on Gun Control (Torricelli and Carroll 2000, p424–26)\nCharlton Heston on Second Amendment Rights (Torricelli and Carroll 2000, p426–31)\nJ. Robert Oppenheimer on Morality and Science (Torricelli and Carroll 2000, p151–53)\nRachel Carson on “Exceeding Beauty of the Earth” (Torricelli and Carroll 2000, p202–05)\nCarson, “The Obligation to Endure” from Silent Spring (Corbett and Connors 1999, p185–90) and topical analysis (Corbett and Connors 1999, p190–95)"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#part-v-style-emphasis-praise-and-remembrance",
    "href": "notes/rhetoric/biblography.html#part-v-style-emphasis-praise-and-remembrance",
    "title": "Rhetoric Bibliography",
    "section": "Part V Style & Emphasis / Praise and Remembrance",
    "text": "Part V Style & Emphasis / Praise and Remembrance\n\nStyle (Corbett and Connors 1999, 337–69, 478–83; Lucas 2011, 221–27); review Delivery (Lucas 2011, p67–71, p239–252)\nJane Addams, Address to Union League Club on George Washington (Torricelli and Carroll 2000, p8–11)\nCarl Sandburg, from Address to Congress on Abraham Lincoln (Torricelli and Carroll 2000, p212–15)\nMartin Luther King, Jr., on Four Martyred Girls (Torricelli and Carroll 2000, p237–39)\nJ.Q. Adams, from Lecture XXVI, “Perspicuity”\nGeorge Campbell, 1, ii, VI, “Of Perspicuity”\nThe New Yorker Obituary for Katherine Sergeant White (Corbett and Connors 1999, p209–11) and analysis of the topics in the obituary (Corbett and Connors 1999, p211–14)\nThomas Sheridan, from Elocution\n“Tragedy as Eulogy,” Edward Kennedy on John Kennedy (Farrell 1993, p118–34) Optional\nJay Heinrichs, “How Harvard Destroyed Rhetoric” in Harvard Magazine"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#part-vi-who-speaks-political-debate-controversy",
    "href": "notes/rhetoric/biblography.html#part-vi-who-speaks-political-debate-controversy",
    "title": "Rhetoric Bibliography",
    "section": "Part VI Who speaks? / Political Debate, Controversy",
    "text": "Part VI Who speaks? / Political Debate, Controversy\n\nBenjamin Franklin, Speech at the Constitutional Convention\nGeorge Washington, from Farewell Address\nJ.Q. Adams, from Lecture XV, “Qualities of an Orator”\nJoseph McCarthy, from a speech to honor Lincoln (Torricelli and Carroll 2000, p173–76)\nMargaret Chase Smith, against hate and fear as political tactics (Torricelli and Carroll 2000, p176–79)\n“Rhetoric in the Army-McCarthy Hearings” (Farrell 1993, p39–47)\nAmy Gutmann, “The Lure & Dangers of Extremist Rhetoric”\nDwight Eisenhower, from Farewell Address (Torricelli and Carroll 2000, p219–21)\nShirley Chisholm, on Government Spending (Torricelli and Carroll 2000, p279–82)\nBarry Goldwater, accepting 1964 nomination (Torricelli and Carroll 2000, p249–52)\nBarbara Jordan, 1973 Impeachment of Richard Nixon (Torricelli and Carroll 2000, p312–16)\nRonald Reagan, “Make America Great Again,” from speech accepting 1980 nomination (Torricelli and Carroll 2000, p341–45)\nMario Cuomo, from 1984 convention speech (Torricelli and Carroll 2000, 354–59)\nElie Wiesel to Reagan on planned visit to Bitburg Cemetery (Torricelli and Carroll 2000, p363–66)\nThe example of Bitburg, analysis (Farrell 1993, p291–93)"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#part-vii-figures-metaphor-the-poetic-and-our-polity",
    "href": "notes/rhetoric/biblography.html#part-vii-figures-metaphor-the-poetic-and-our-polity",
    "title": "Rhetoric Bibliography",
    "section": "Part VII Figures & Metaphor / The Poetic and Our Polity",
    "text": "Part VII Figures & Metaphor / The Poetic and Our Polity\n\nFigures of Speech; the Value of Imitation, (Corbett and Connors 1999, p377–448; Lucas 2011, p227–232)\nDaniel Webster, from Eulogy on Adams and Jefferson, their Literary Character and Eloquence\nWilliam Faulkner, Nobel Prize Acceptance Speech (Torricelli and Carroll 2000, p179–80)\nJohn Kennedy, tribute to Robert Frost (Torricelli and Carroll 2000, 242–44)\nIsaac Bashevis Singer, Nobel Prize Acceptance Speech (Torricelli and Carroll 2000, 327–29)"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#part-viii-a-brief-history-inaugurating-hope",
    "href": "notes/rhetoric/biblography.html#part-viii-a-brief-history-inaugurating-hope",
    "title": "Rhetoric Bibliography",
    "section": "Part VIII A Brief History / Inaugurating Hope",
    "text": "Part VIII A Brief History / Inaugurating Hope\n\nFDR, First Inaugural Address (Torricelli and Carroll 2000, 99–102) and analysis (Farrell 1993, 83–93)\nJFK, Inaugural Address (Corbett and Connors 1999, 459–61) and analysis (Corbett and Connors 1999, 461–72)\nLincoln, Second Inaugural Address\nDavid Zarefsky, “Approaching Lincoln’s Second Inaugural Address” -“A Survey of Rhetoric” (Corbett and Connors 1999, 489–543)"
  },
  {
    "objectID": "notes/rhetoric/biblography.html#part-ix-study-commitment-remarks-at-a-dedication",
    "href": "notes/rhetoric/biblography.html#part-ix-study-commitment-remarks-at-a-dedication",
    "title": "Rhetoric Bibliography",
    "section": "Part IX Study & Commitment / Remarks at a Dedication",
    "text": "Part IX Study & Commitment / Remarks at a Dedication\n\nEngell, from The Committed Word, ch. 9, “Lincoln’s Language, and Ours,” and ch. 10, “Recommitment”\nLincoln, Gettysburg Address\nWilliam Barton, from Lincoln at Gettysburg\n\n\nReferences\n\n\nCorbett, Edward P. J., and Robert J. Connors. 1999. Classical Rhetoric for the Modern Student. 4th ed. Oxford University Press.\n\n\nEngell, James. 1999. The Committed Word: Literature and Public Values. University Park: Penn State Press.\n\n\nFarrell, T. B. 1993. Norms of Rhetorical Culture. Yale University Press. https://books.google.co.il/books?id=AkJEiCdIimgC.\n\n\nLucas, Stephen E. 2011. The Art of Public Speaking. 11th ed. McGraw-Hill.\n\n\nTorricelli, Robert G., and Andrew Carroll. 2000. In Our Own Words: Extraordinary Speeches of the American Century. Simon; Schuster."
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html",
    "href": "notes/rhetoric/martin_luther_king_jr.html",
    "title": "I Have a Dream speach",
    "section": "",
    "text": "I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.1\nFive score years ago2, a great American3, in whose symbolic shadow4 we stand today 5, signed the Emancipation Proclamation. This momentous decree came as a great beacon light6 of hope to millions of Negro slaves who had been seared in the flames7 of withering injustice. It came as a joyous daybreak8 to end the long night9 of their captivity.\nBut one hundred years later 10, the Negro still is not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later, the Negro is still languished in the corners of American society and finds himself in exile in his own land. And so we’ve come here today to dramatize a shameful condition.\nIn a sense we’ve come to our nation’s capital to cash a check. When the architects11 of our republic wrote the magnificent words of the Constitution12 and the Declaration of Independence13, they were signing a promissory note to which every American was to fall heir. This note was a promise14 that all men, yes, black men as well as white men, would be guaranteed the unalienable rights of life, liberty, and the pursuit of happiness15. It is obvious today that America has defaulted on this promissory note16 insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check17, a check which has come back marked insufficient funds.\nBut we refuse to believe that the bank of justice is bankrupt. We refuse to believe that there are insufficient funds in the great vaults of opportunity of this nation. And so we’ve come to cash this check, a check that will give us upon demand the riches of freedom and the security of justice.\nWe have also come to this hallowed spot 18 to remind America of the fierce urgency of now. This is no time 19 to engage in the luxury of cooling off or to take the tranquilizing drug of gradualism}. Now is the time to make real the promises of democracy. Now is the time to rise from the dark and desolate valley of segregation to the sunlit path of racial justice. Now is the time to lift our nation from the quicksands of racial injustice to the solid rock of brotherhood 20 Now is the time to make justice a reality for all of God’s children.\nIt would be fatal for the nation to overlook the urgency of the moment. This sweltering summer of the Negro’s legitimate discontent will not pass until there is an invigorating autumn of freedom and equality 21. 1963 is not an end, but a beginning. And those who hope that the Negro needed to blow off steam and will now be content will have a rude awakening if the nation returns to business as usual. There will be neither rest nor tranquility in America until the Negro is granted his citizenship rights. The whirlwinds of revolt will continue to shake the foundations of our nation until the bright day of justice emerges.\nBut there is something that I must say to my people, who stand on the warm threshold which leads into the palace of justice: in the process of gaining our rightful place, we must not be guilty of wrongful deeds. Let us not seek to satisfy our thirst for freedom by drinking from the cup of bitterness and hatred. We must forever conduct our struggle on the high plane of dignity and discipline. We must not allow our creative protest to degenerate into physical violence. Again and again, we must rise to the majestic heights of meeting physical force with soul force. The marvelous new militancy which has engulfed the Negro community must not lead us to a distrust of all white people, for many of our white brothers, as evidenced by their presence here today, have come to realize that their destiny is tied up with our destiny, and they have come to realize that their freedom is inextricably bound to our freedom. We cannot 22 walk alone.\nAnd as we walk, we must make the pledge that we shall always march ahead. We cannot turn back. There are those who are asking the devotees of civil rights, “When will you be satisfied?” We can never be satisfied as long as the Negro is the victim of the unspeakable horrors of police brutality. We can never be satisfied as long as our bodies, heavy with the fatigue of travel, cannot gain lodging in the motels of the highways and the hotels of the cities. 23 We cannot be satisfied as long as the Negro’s basic mobility is from a smaller ghetto to a larger one. We can never be satisfied as long as our children are stripped of their selfhood and robbed of their dignity by signs stating for whites only. We cannot be satisfied as long as a Negro in Mississippi cannot vote and a Negro in New York believes he has nothing for which to vote. No, no, we are not satisfied and we will not be satisfied until justice rolls down like waters and righteousness like a mighty stream 24.\nI am not unmindful that some of you have come here out of great trials 25 and tribulations. Some of you have come fresh from narrow jail cells. Some of you have come from areas where your quest for freedom left you battered by the storms of persecution and staggered by the winds of police brutality. You have been the veterans of creative suffering 26. Continue to work with the faith that unearned suffering is redemptive. Go back to Mississippi, go back to Alabama, go back to South Carolina, go back to Georgia, go back to Louisiana, go back to the slums and ghettos of our northern cities, knowing that somehow this situation can and will be changed. Let us not wallow in the valley of despair.\nI say to you today, my friends, so even though we face the difficulties of today and tomorrow, I still have a dream. It is a dream deeply rooted in the American dream. 27\nI have a dream that one day this nation will rise up and live out the true meaning of its creed: “We hold these truths to be self-evident, that all men are created equal.” 28\nI have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood. I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice. I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character. I have a dream today.\nI have a dream that one day down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of “interposition” and “nullification”, one day right there in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers. I have a dream today.\nI have a dream that one day every valley shall be exalted, every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight, and the glory of the Lord shall be revealed, and all flesh shall see it together.\nThis is our hope. This is the faith that I go back to the South with. With this faith we will be able to hew out of the mountain of despair a stone of hope. With this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\nThis will be the day, this will be the day when all of God’s children will be able to sing with new meaning: “My country, ’tis of thee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrim’s pride, from every mountainside, let freedom ring!” 29\nAnd if America is to be a great nation, this must become true. So let freedom ring from the prodigious hilltops of New Hampshire. Let freedom ring from the mighty mountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let freedom ring from the snow-capped Rockies of Colorado. Let freedom ring from the curvaceous slopes of California. But not only that: Let freedom ring from Stone Mountain of Georgia. Let freedom ring from Lookout Mountain of Tennessee. Let freedom ring from every hill and molehill of Mississippi. From every mountainside, let freedom ring.\nAnd when this happens, and when we allow freedom to ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God’s children, black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: “Free at last! Free at last! Thank God Almighty, we are free at last!” 30\nTaken from: Robert Torricelli, ed., In Our Own Words: Extraordinary Speeches of the American Century (New York: Simon & Schuster, 2000), pg. 234"
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#i-have-a-dream-martin-luther-king-jr.",
    "href": "notes/rhetoric/martin_luther_king_jr.html#i-have-a-dream-martin-luther-king-jr.",
    "title": "I Have a Dream speach",
    "section": "",
    "text": "I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.1\nFive score years ago2, a great American3, in whose symbolic shadow4 we stand today 5, signed the Emancipation Proclamation. This momentous decree came as a great beacon light6 of hope to millions of Negro slaves who had been seared in the flames7 of withering injustice. It came as a joyous daybreak8 to end the long night9 of their captivity.\nBut one hundred years later 10, the Negro still is not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later, the Negro is still languished in the corners of American society and finds himself in exile in his own land. And so we’ve come here today to dramatize a shameful condition.\nIn a sense we’ve come to our nation’s capital to cash a check. When the architects11 of our republic wrote the magnificent words of the Constitution12 and the Declaration of Independence13, they were signing a promissory note to which every American was to fall heir. This note was a promise14 that all men, yes, black men as well as white men, would be guaranteed the unalienable rights of life, liberty, and the pursuit of happiness15. It is obvious today that America has defaulted on this promissory note16 insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check17, a check which has come back marked insufficient funds.\nBut we refuse to believe that the bank of justice is bankrupt. We refuse to believe that there are insufficient funds in the great vaults of opportunity of this nation. And so we’ve come to cash this check, a check that will give us upon demand the riches of freedom and the security of justice.\nWe have also come to this hallowed spot 18 to remind America of the fierce urgency of now. This is no time 19 to engage in the luxury of cooling off or to take the tranquilizing drug of gradualism}. Now is the time to make real the promises of democracy. Now is the time to rise from the dark and desolate valley of segregation to the sunlit path of racial justice. Now is the time to lift our nation from the quicksands of racial injustice to the solid rock of brotherhood 20 Now is the time to make justice a reality for all of God’s children.\nIt would be fatal for the nation to overlook the urgency of the moment. This sweltering summer of the Negro’s legitimate discontent will not pass until there is an invigorating autumn of freedom and equality 21. 1963 is not an end, but a beginning. And those who hope that the Negro needed to blow off steam and will now be content will have a rude awakening if the nation returns to business as usual. There will be neither rest nor tranquility in America until the Negro is granted his citizenship rights. The whirlwinds of revolt will continue to shake the foundations of our nation until the bright day of justice emerges.\nBut there is something that I must say to my people, who stand on the warm threshold which leads into the palace of justice: in the process of gaining our rightful place, we must not be guilty of wrongful deeds. Let us not seek to satisfy our thirst for freedom by drinking from the cup of bitterness and hatred. We must forever conduct our struggle on the high plane of dignity and discipline. We must not allow our creative protest to degenerate into physical violence. Again and again, we must rise to the majestic heights of meeting physical force with soul force. The marvelous new militancy which has engulfed the Negro community must not lead us to a distrust of all white people, for many of our white brothers, as evidenced by their presence here today, have come to realize that their destiny is tied up with our destiny, and they have come to realize that their freedom is inextricably bound to our freedom. We cannot 22 walk alone.\nAnd as we walk, we must make the pledge that we shall always march ahead. We cannot turn back. There are those who are asking the devotees of civil rights, “When will you be satisfied?” We can never be satisfied as long as the Negro is the victim of the unspeakable horrors of police brutality. We can never be satisfied as long as our bodies, heavy with the fatigue of travel, cannot gain lodging in the motels of the highways and the hotels of the cities. 23 We cannot be satisfied as long as the Negro’s basic mobility is from a smaller ghetto to a larger one. We can never be satisfied as long as our children are stripped of their selfhood and robbed of their dignity by signs stating for whites only. We cannot be satisfied as long as a Negro in Mississippi cannot vote and a Negro in New York believes he has nothing for which to vote. No, no, we are not satisfied and we will not be satisfied until justice rolls down like waters and righteousness like a mighty stream 24.\nI am not unmindful that some of you have come here out of great trials 25 and tribulations. Some of you have come fresh from narrow jail cells. Some of you have come from areas where your quest for freedom left you battered by the storms of persecution and staggered by the winds of police brutality. You have been the veterans of creative suffering 26. Continue to work with the faith that unearned suffering is redemptive. Go back to Mississippi, go back to Alabama, go back to South Carolina, go back to Georgia, go back to Louisiana, go back to the slums and ghettos of our northern cities, knowing that somehow this situation can and will be changed. Let us not wallow in the valley of despair.\nI say to you today, my friends, so even though we face the difficulties of today and tomorrow, I still have a dream. It is a dream deeply rooted in the American dream. 27\nI have a dream that one day this nation will rise up and live out the true meaning of its creed: “We hold these truths to be self-evident, that all men are created equal.” 28\nI have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood. I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice. I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character. I have a dream today.\nI have a dream that one day down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of “interposition” and “nullification”, one day right there in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers. I have a dream today.\nI have a dream that one day every valley shall be exalted, every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight, and the glory of the Lord shall be revealed, and all flesh shall see it together.\nThis is our hope. This is the faith that I go back to the South with. With this faith we will be able to hew out of the mountain of despair a stone of hope. With this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\nThis will be the day, this will be the day when all of God’s children will be able to sing with new meaning: “My country, ’tis of thee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrim’s pride, from every mountainside, let freedom ring!” 29\nAnd if America is to be a great nation, this must become true. So let freedom ring from the prodigious hilltops of New Hampshire. Let freedom ring from the mighty mountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let freedom ring from the snow-capped Rockies of Colorado. Let freedom ring from the curvaceous slopes of California. But not only that: Let freedom ring from Stone Mountain of Georgia. Let freedom ring from Lookout Mountain of Tennessee. Let freedom ring from every hill and molehill of Mississippi. From every mountainside, let freedom ring.\nAnd when this happens, and when we allow freedom to ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God’s children, black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: “Free at last! Free at last! Thank God Almighty, we are free at last!” 30\nTaken from: Robert Torricelli, ed., In Our Own Words: Extraordinary Speeches of the American Century (New York: Simon & Schuster, 2000), pg. 234"
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#footnotes",
    "href": "notes/rhetoric/martin_luther_king_jr.html#footnotes",
    "title": "I Have a Dream speach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis short Exordium, AKA introduction, is all King needs to arrive at his point; his audience knows who he is and why they are gathered↩︎\nallusion to Gutenberg address↩︎\nAbraham Lincoln↩︎\nmetaphor use of light and darkness throughout, with Biblical resonances at play.↩︎\nThis appeal to the example of Lincoln is also an appeal to ethos, suggesting that King and his fellow organizers are carrying on Lincoln’s legacy and his greatness.↩︎\nmetaphor use of light and darkness throughout, with Biblical resonances at play.↩︎\nmetaphor use of light and darkness throughout, with Biblical resonances at play.↩︎\nPsalm 30:5↩︎\nmetaphor use of light and darkness throughout, with Biblical resonances at play.↩︎\nanaphora AKA repetition of “One hundred years later …”, create an increasing sense of urgency.↩︎\nHere, King begins to move out of his opening statement of fact by making an interpretive claim: the rights of citizenship laid out by the nation’s founding documents should apply equally to its people of color. There are several ways to imagine the divisions of this speech’s argument, but we might say that its confirmatio, or central argument, commences here.↩︎\nThe idea of constitutional rights as an “unfulfilled promise” was suggested by Clarence Jones This metaphor is embedded within political context of the deceleration of independence which is named and quoted↩︎\nThe idea of constitutional rights as an “unfulfilled promise” was suggested by Clarence Jones This metaphor is embedded within political context of the deceleration of independence which is named and quoted↩︎\nAnastrophe or reversal of order↩︎\nThe Declaration of Independence states, “We hold these Truths to be self-evident, that all Men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty, and the Pursuit of Happiness…”↩︎\nThe set of logical propositions here makes an appeal to logos, or reason: if the founding documents offer all citizens life, liberty, and the pursuit of happiness, but people of color have not been allowed to share in these benefits, then they are owed a debt.↩︎\nHere, King uses the rhetorical device of apposition, in which a phrase is added to the sentence to re-name a term (here, “bad check”) in different, more precise, or more forceful words.↩︎\nallusion to the Lincoln Memorial - this is more voice merging↩︎\nThe extended anaphora of this passage builds into a crescendo of intensity.↩︎\nThis phrase is an excellent example of King’s striking sonic effects: the short, front vowels (“i,” “a”) and whispering sibilants (“s,” “z” and “j” sounds) of the first half contrast with the booming solidity of the voiced plosive consonants (“b,” “d”) and long, round back vowels (“o” and “oo”) of the second half, which give it a sense of confidence and certainty.↩︎\nallusion to the opening lines of Shakespeare’s Richard III “Now is the winter of our discontent / Made glorious summer…”)↩︎\nNote King’s use of varied sentence lengths for a modulated effect: he moves from the longest sentence of the paragraph (at 62 words) to its shortest (at 4), which sums it up succinctly. This closes the paragraph with powerful emphasis on its core idea.↩︎\nLike many other parts of the speech, the image King paints here makes a powerful appeal to pathos, moving the emotions of any listener capable of identifying with the exhausted traveler.↩︎\nA reference to Amos 5:24, in the language of the American Standard Bible, 1901: “But let justice roll down as waters, and righteousness as a mighty stream.” The prophet Amos in this passage protests against injustice, economic inequality, and religious hypocrisy. We might see this reference as an appeal to ethos: it shows King’s knowledge of (and positions him as the successor to) a Biblical prophetic tradition.↩︎\nHere, King appears to respond to a potential objection to his call for continued nonviolent protest: that he and his fellow activists have faced great injustice and even physical violence. How can they keep going, and keep refusing to reflect back their attackers’ violence and hatred?↩︎\nHere King suggests that, like the suffering of Christ, the suffering of civil rights activists will help bring change.↩︎\nHere King suggests that, like the suffering of Christ, the suffering of civil rights activists will help bring change.↩︎\nThis second return to the logical moves with which King opened the speech also helps contribute to the sense of completion and unity.↩︎\nfrom secular Hymn “America” previous anthem of US, and closely resembling Archibald Carey Jr.’s address to the 1952 Republican National Convention↩︎\nfrom free at last↩︎"
  },
  {
    "objectID": "notes/XAI/l04/Counterfactual-Explanations.html#course-leaders",
    "href": "notes/XAI/l04/Counterfactual-Explanations.html#course-leaders",
    "title": "4 Counterfactual Explanations - Explaining and Debugging",
    "section": "Course Leaders:",
    "text": "Course Leaders:\n\nBitya Neuhof #DataNights\nYasmin Bokobza #Microsoft"
  },
  {
    "objectID": "notes/XAI/l04/Counterfactual-Explanations.html#speaker",
    "href": "notes/XAI/l04/Counterfactual-Explanations.html#speaker",
    "title": "4 Counterfactual Explanations - Explaining and Debugging",
    "section": "Speaker:",
    "text": "Speaker:\n\nAmit Sharma #Microsoft\n\nAmit Sharma is a Principal Researcher at Microsoft Research India. His work bridges causal inference techniques with machine learning, to make ML models generalize better, be explainable and avoid hidden biases. To this end, Amit has co-led the development of the open-source DoWhy library for causal inference and DiCE library for counterfactual explanations. The broader theme in his work is how machine learning can be used for better decision-making, especially in sensitive domains. In this direction, Amit collaborates with NIMHANS on mental health technology, including a recent app, MindNotes, that encourages people to break the stigma and reach out to professionals.\nHis work has received many awards including:\n\na Best Paper Award at ACM CHI 2021 conference,\nBest Paper Honorable Mention at ACM CSCW 2016 conference,\nthe 2012 Yahoo! Key Scientific Challenges Award and\nthe 2009 Honda Young Engineer and Scientist Award.\n\nAmit received his:\n\nPh.D. in computer science from Cornell University and\nB.Tech. in Computer Science and Engineering from the Indian Institute of Technology (IIT) Kharagpur.\nProfile"
  },
  {
    "objectID": "notes/XAI/l03/index.html",
    "href": "notes/XAI/l03/index.html",
    "title": "3 Explaining Deep Neural Network Models",
    "section": "",
    "text": "series poster"
  },
  {
    "objectID": "notes/XAI/l03/index.html#series-poseter",
    "href": "notes/XAI/l03/index.html#series-poseter",
    "title": "3 Explaining Deep Neural Network Models",
    "section": "",
    "text": "series poster"
  },
  {
    "objectID": "notes/XAI/l03/index.html#session-video",
    "href": "notes/XAI/l03/index.html#session-video",
    "title": "3 Explaining Deep Neural Network Models",
    "section": "Session Video",
    "text": "Session Video\nnot available!\n\n\n\nseries poster"
  },
  {
    "objectID": "notes/TFP/2021-06-01-tfp-week1-overview/2021-06-01-tfp-week1-overview.html",
    "href": "notes/TFP/2021-06-01-tfp-week1-overview/2021-06-01-tfp-week1-overview.html",
    "title": "TensorFlow probability Independent",
    "section": "",
    "text": "These are my course notes on Probabilistic Deep Learning with TensorFlow by Dr Kevin Webster of Imperial College London 2\nThe course url\nOne point to note is that many ML courses delve into the “dry” engineering aspect and gloss over the “motivating” examples which are just a demonstration and not the main item. There are two issues with this approach. One is that a model needs some kind of environment to test and evaluate it. And the student benefits more by learning how to think and reason about the techniques than by just learning to replicate the code as a black box.\nSo to get the most out of a course you may to review complementary materials. Your milage may vary depending on the learning curve:\nOn the flip side you should be aware that just as much as teachers like to skip over details they can’t resist showing you certain techniques. So try to master these. Try to think where else they may be used and try to incorporate them into your work before they fade away. Look over the title of recent papers by the profferers and don’t be shy to asks them about methods or problems they have worked on - this can have many many benefits.\nI recommend you should approach the material with plenty of enthusiasm and invest energy in making up for any such shortcoming.\nLearn to own the code as soon as possible. Can you remember the methods. Understand the problem being presented. The problem definition begins with a task and is ML delineated by the variables and features available in the data. Two big questions to consider are what prior knowledge should be incorporated into the model and what types of uncertainty is one dealing with. With deeper understanding of the relations of the features one can modify the model to make these explicit. These can be a hierarchy, correlations, variance and covariance, conditioning, independence, causation and other structures things like mediation.\nNext one should consider a baseline model? This should be the simplest model possible - in the realm of probability this is usually a model with a single most important parameter - the mean. You should try to implement this. Jeremy Howard frequently points out that more sophisticated models may not fate much better. This type of model is getting you to think more like a statistician and you will immediately be able to think about improvement. Adding a second term with a higher moment. What about first order interactions between variables and higher order interactions. Can pooling help, can partial pooling help more. But rather than implementing all these you should consider what other algorithms could be used. Once you have a baseline model you should be able to try out these model with two or three lines of code which leads to the next major point\nHow should the model be evaluated. What’s the best metric (KPI) for the task. Loss, accuracy. What regularization can keep complexity and overfitting at bay. How do we handle a model which outputs both a prediction and a certainty level? Can we train faster of better. Should we use regret. Some models can just be queried others require extra care and work to coax a good result. These could be using embeddings with a vector space metrics, doing a beam search or other methods. Like I mentioned above always be on the look out for new methods to mine you models.\nFinlay its time to think about main item. Can we break down the model conceptually break into it moving parts. What are the model assumptions? Working with probabilistic and statistical models provides a data scientist with many diagnostic capabilities which should become part of the analysis. When assumptions are violates outcomes may require attention but often the model will still function.\nThink what is the neural net part trying to do and what is the probabilistic component doing? E.g. The NN might look for faces and the probabilistic part may just be a softmax which squishes the outputs into a probability distribution.\nThough I mentioned evaluation before, more in the context of the tasks and baseline model. You will need to have a second go with you specific model in mind. Look at over/under fitting and at tunning hyper-parameter and how do we tune them."
  },
  {
    "objectID": "notes/TFP/2021-06-01-tfp-week1-overview/2021-06-01-tfp-week1-overview.html#skills",
    "href": "notes/TFP/2021-06-01-tfp-week1-overview/2021-06-01-tfp-week1-overview.html#skills",
    "title": "TensorFlow probability Independent",
    "section": "Skills:",
    "text": "Skills:\nthe official learning objectives are: - Sample and compute probabilities from a range of Distribution objects - Create multivariate independent distributions from separate distributions - Understand and manipulate shape semantics for Distribution objects - Understand and apply broadcasting rules in the creation of distributions - Create and train parameterised distributions to optimise data likelihood but there is a lot more to be learned here: 1. recall basic probability theory: definitions of expectation, variance, standard deviation. 1. create distributions tensors for univariate, bi-variate and multivariate. 1. sample from a distribution 1. understand the sample, broadcast, event components of a tensor. 1. reshape distributions dimensions from batch to event. 1. understand how broadcasting works. 1. learnable parameters for a distribution. 1. develop a Naive Bayes classifier for newsgroup articles. 1. develop a custom training loop in tensor flow"
  },
  {
    "objectID": "notes/TFP/2021-06-01-tfp-week1-overview/2021-06-01-tfp-week1-overview.html#distributions---the-sample-the-batch-and-the-event",
    "href": "notes/TFP/2021-06-01-tfp-week1-overview/2021-06-01-tfp-week1-overview.html#distributions---the-sample-the-batch-and-the-event",
    "title": "TensorFlow probability Independent",
    "section": "Distributions - the sample the batch and the event",
    "text": "Distributions - the sample the batch and the event\nThe way we are taught probability and statistics and later how we approach multivariate data in data science reinforces a static and low dimensional view of data. But when we want to work with Graphical models, Bayesian networks, Multilevel models and later with probabilistic neral networks we need to expand our thinking about these. One key new skill one must develop is creating distributions with specific shapes and transforming to new shapes. This manipulation of tensors should already be familiar from work with convolutional or sequence models. But is those cases you typically only need to handle shapes at the input or output of the model and the framework mostly deals with these nitty gritty details. With TensorFlow probability shapes these details need to be part of one’s thinking. Unless they are second nature you will struggle to implementing the with the more complex parts and TFP is just a part of a much bigger picture."
  },
  {
    "objectID": "notes/TFP/2021-06-01-tfp-week1-overview/2021-06-01-tfp-week1-overview.html#resources",
    "href": "notes/TFP/2021-06-01-tfp-week1-overview/2021-06-01-tfp-week1-overview.html#resources",
    "title": "TensorFlow probability Independent",
    "section": "Resources:",
    "text": "Resources:\nif like me, you initially feel that numpy array broadcasting feels strange and awkward you may want to look at:\n\nNumpy Broadcasting Guide\nArray Broadcasting in Numpy"
  },
  {
    "objectID": "notes/TFP/2021-06-02-tfp-week1-independet/2021-06-02-tfp-week1-independet.html",
    "href": "notes/TFP/2021-06-02-tfp-week1-independet/2021-06-02-tfp-week1-independet.html",
    "title": "TensorFlow probability Independent",
    "section": "",
    "text": "These are my course notes on Probabilistic Deep Learning with TensorFlow by Dr Kevin Webster of Imperial College London 2 The course url"
  },
  {
    "objectID": "notes/TFP/2021-06-02-tfp-week1-independet/2021-06-02-tfp-week1-independet.html#distributions---the-sample-the-batch-and-the-event",
    "href": "notes/TFP/2021-06-02-tfp-week1-independet/2021-06-02-tfp-week1-independet.html#distributions---the-sample-the-batch-and-the-event",
    "title": "TensorFlow probability Independent",
    "section": "Distributions - the sample the batch and the event",
    "text": "Distributions - the sample the batch and the event\nThe way we are taught probability and statistics and later how we approach multivariate data in data science reinforces a static and low dimensional view of data. But when we want to work with Graphical models, Bayesian networks, Multilevel models and later with probabilistic neral networks we need to expand our thinking about these. One key new skill one must develop is creating distributions with specific shapes and transforming to new shapes. This manipulation of tensors should already be familiar from work with convolutional or sequence models. But is those cases you typically only need to handle shapes at the input or output of the model and the framework mostly deals with these nitty gritty details. With TensorFlow probability shapes these details need to be part of one’s thinking. Unless they are second nature you will struggle to implementing the with the more complex parts and TFP is just a part of a much bigger picture."
  },
  {
    "objectID": "notes/TFP/2021-06-02-tfp-week1-independet/2021-06-02-tfp-week1-independet.html#independent---moving-dimensions-from-batch-to-event",
    "href": "notes/TFP/2021-06-02-tfp-week1-independet/2021-06-02-tfp-week1-independet.html#independent---moving-dimensions-from-batch-to-event",
    "title": "TensorFlow probability Independent",
    "section": "Independent - moving dimensions from batch to event",
    "text": "Independent - moving dimensions from batch to event\nIndeed! This is taking multiple low dimensional distribution and histing them to be higher dimensional. This is a super powerful idea. I first came the notion when researching complex values distribution in one of my early attempts at representing uncertainty and fuzziness. What little data that I could find said these can be viewed as two distributions…. Some alternatives: A multivariate normal with a diagonal covarience matrix\nA multivariate normal with a full covarience matrix"
  },
  {
    "objectID": "notes/TFP/2021-06-02-tfp-week1-independet/2021-06-02-tfp-week1-independet.html#resources",
    "href": "notes/TFP/2021-06-02-tfp-week1-independet/2021-06-02-tfp-week1-independet.html#resources",
    "title": "TensorFlow probability Independent",
    "section": "Resources:",
    "text": "Resources:\nif like me, you initially feel that numpy array broadcasting feels strange and awkward you may want to look at: - Numpy Broadcasting Guide - Array Broadcasting in Numpy"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html",
    "href": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html",
    "title": "TensorFlow probability",
    "section": "",
    "text": "These are my course notes on Probabilistic Deep Learning with TensorFlow by Dr Kevin Webster of Imperial College London 2\nThe course url\nOne point to note is that many ML courses delve into the “dry” engineering aspect and gloss over the “motivating” examples which are just a demonstration and not the main item. There are two issues with this approach. One is that a model needs some kind of environment to test and evaluate it. And the student benefits more by learning how to think and reason about the techniques than by just learning to replicate the code as a black box.\nSo to get the most out of a course you may to review complementary materials. Your milage may vary depending on the learning curve:\nOn the flip side you should be aware that just as much as teachers like to skip over details they can’t resist showing you certain techniques. So try to master these. Try to think where else they may be used and try to incorporate them into your work before they fade away. Look over the title of recent papers by the profferers and don’t be shy to asks them about methods or problems they have worked on - this can have many many benefits.\nI recommend you should approach the material with plenty of enthusiasm and invest energy in making up for any such shortcoming.\nLearn to own the code as soon as possible. Can you remember the methods. Understand the problem being presented. The problem definition begins with a task and is ML delineated by the variables and features available in the data. Two big questions to consider are what prior knowledge should be incorporated into the model and what types of uncertainty is one dealing with. With deeper understanding of the relations of the features one can modify the model to make these explicit. These can be a hierarchy, correlations, variance and covariance, conditioning, independence, causation and other structures things like mediation.\nNext one should consider a baseline model? This should be the simplest model possible - in the realm of probability this is usually a model with a single most important parameter - the mean. You should try to implement this. Jeremy Howard frequently points out that more sophisticated models may not fate much better. This type of model is getting you to think more like a statistician and you will immediately be able to think about improvement. Adding a second term with a higher moment. What about first order interactions between variables and higher order interactions. Can pooling help, can partial pooling help more. But rather than implementing all these you should consider what other algorithms could be used. Once you have a baseline model you should be able to try out these model with two or three lines of code which leads to the next major point How should the model be evaluated. What’s the best metric (KPI) for the task. Loss, accuracy. What regularization can keep complexity and overfitting at bay. How do we handle a model which outputs both a prediction and a certainty level? Can we train faster of better. Should we use regret. Some models can just be queried others require extra care and work to coax a good result. These could be using embeddings with a vector space metrics, doing a beam search or other methods. Like I mentioned above always be on the look out for new methods to mine you models. Finlay its time to think about main item. Can we break down the model conceptually break into it moving parts. What are the model assumptions? Working with probabilistic and statistical models provides a data scientist with many diagnostic capabilities which should become part of the analysis. When assumptions are violates outcomes may require attention but often the model will still function. Think what is the neural net part trying to do and what is the probabilistic component doing? E.g. The NN might look for faces and the probabilistic part may just be a softmax which squishes the outputs into a probability distribution.\nThough I mentioned evaluation before, more in the context of the tasks and baseline model. You will need to have a second go with you specific model in mind. Look at over/under fitting and at tunning hyper-parameter and how do we tune them."
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#naive-bayes-to-classify-new-groups",
    "href": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#naive-bayes-to-classify-new-groups",
    "title": "TensorFlow probability",
    "section": "Naive Bayes to classify new groups",
    "text": "Naive Bayes to classify new groups\n\nnaive bayes assumes feature independence\nlaplace smoothing is a blunt instrument.\nclipping is even more blunt"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#skills",
    "href": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#skills",
    "title": "TensorFlow probability",
    "section": "Skills:",
    "text": "Skills:\n\nrecall basic probability theory: definitions of expectation, variance, standard deviation.\ncreate distributions tensors for univariate, bi-variate and multivariate.\nsample from a distribution\nunderstand the sample, broadcast, event components of a tensor.\nreshape distributions dimensions from batch to event.\nunderstand how broadcasting works.\nlearnable parameters for a distribution.\ndevelop a Naive Bayes classifier for newsgroup articles.\ndevelop a custom training loop in tensor flow"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#distributions---the-sample-the-batch-and-the-event",
    "href": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#distributions---the-sample-the-batch-and-the-event",
    "title": "TensorFlow probability",
    "section": "Distributions - the sample the batch and the event",
    "text": "Distributions - the sample the batch and the event\nThe way we are taught probability and statistics and later how we approach multivariate data in data science reinforces a static and low dimensional view of data. But when we want to work with Graphical models, Bayesian networks, Multilevel models and later with probabilistic neral networks we need to expand our thinking about these. One key new skill one must develop is creating distributions with specific shapes and transforming to new shapes.\nThis manipulation of tensors should already be familiar from work with convolutional or sequence models. But is those cases you typically only need to handle shapes at the input or output of the model and the framework mostly deals with these nitty gritty details. With TensorFlow probability shapes these details need to be part of one’s thinking. Unless they are second nature you will struggle to implementing the with the more complex parts and TFP is just a part of a much bigger picture."
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#broadcasting-rules-np.newaxis",
    "href": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#broadcasting-rules-np.newaxis",
    "title": "TensorFlow probability",
    "section": "Broadcasting Rules & np.newaxis",
    "text": "Broadcasting Rules & np.newaxis\nHere are two essential techniques for tensor manipulation you must master which come from the Numpy library. Two more concepts come from numpy. The first is tensor promotion using a numpy.newaxis as follows\na = np.array([2, 0, 2, 1])     # shape (4, ) i.e 1d vector\n\n# we can promote a 1d vector in two  ways:\n\n# insert new dimension before the existing one:\ncol_vector = a[: np.newaxis]  # shape (4,1) i.e. 2.d vector\n\n# insert new dimension after the existing one:\nrow_vector = a[np.newaxis,:]  # shape (1,4) i.e. 2d vector \nor visually:  Image credit.\nNumpy’s broadcasting rule: 1. Prepend 1s to the smaller shape, 2. check that the axes of both arrays have sizes that are equal or 1, 3. stretch the arrays in their size-1 axes. https://stackoverflow.com/questions/29241056/how-does-numpy-newaxis-work-and-when-to-use-it"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#bijectors",
    "href": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#bijectors",
    "title": "TensorFlow probability",
    "section": "Bijectors",
    "text": "Bijectors"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#autoencoder-flows",
    "href": "notes/TFP/2021-06-05-tfp-week1-naive-bayes/2021-06-05-tfp-week1-naive-bayes.html#autoencoder-flows",
    "title": "TensorFlow probability",
    "section": "Autoencoder Flows",
    "text": "Autoencoder Flows\nOne of the standard components of many sequence models is an element that allows access to input before the current position but masks the input beyond the current position. This is done by subtracting a large number from it. This is used in transformers with self attention which are tasked with generating a sequence from a sequence.\n\nMADE\nHere is an extract from the abstract: &gt; “Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability.” &gt; &gt; - MADE: Masked Autoencoder for Distribution Estimation (Germain et all 2015)\n\n\nReal NVP\nQ. whats is this NVP alphabet soup?\n\nNVP\n\nnon volume preserving.\n\n\n\nUnsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.\n\nDensity estimation using Real NVP (Laurent et all 2017) ## Resources: if like me, you initially feel that numpy array broadcasting feels strange and awkward you may want to look at:\nNumpy Broadcasting Guide\nArray Broadcasting in Numpy"
  },
  {
    "objectID": "tfp.html",
    "href": "tfp.html",
    "title": "Tensor Flow Probability",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nTensorFlow probability Independent\n\n\n\n\n\n\nOren Bochman\n\n\nJun 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow probability Independent\n\n\nCourse notes for Probabilistic Deep Learning with TensorFlow by Dr Kevin Webster\n\n\n\nOren Bochman\n\n\nJun 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic Deep Learning with TensorFlow\n\n\nCourse notes for ‘Probabilistic Deep Learning with TensorFlow’ by Dr Kevin Webster\n\n\n\nOren Bochman\n\n\nJun 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow probability\n\n\n\n\n\n\nOren Bochman\n\n\nJun 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow probability\n\n\n\n\n\n\nOren Bochman\n\n\nJun 1, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "xai.html",
    "href": "xai.html",
    "title": "Explainable AI",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLecture 5 — Explainable AI in practice\n\n\nHow to properly incorporate explanations in machine learning projects and what aspects should you keep in mind? Over the past few years the need to explain the output of…\n\n\n\nOren Bochman\n\n\nMar 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 Counterfactual Explanations - Explaining and Debugging\n\n\nHow to explain a machine learning model such that the explanation is truthful to the model and yet interpretable to people? This question is key to ML explanations research…\n\n\n\nOren Bochman\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 Explaining Deep Neural Network Models\n\n\nRigorously understanding how Deep Learning models function may allow us to steer and control their behavior and avoid unwanted consequences. In addition, today’s highly…\n\n\n\nOren Bochman\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 Local Explanations - Concept and Methods\n\n\nMachine learning models can be analyzed at a high level using global explanations, such as linear model coefficients. However, there are several limitations to these global…\n\n\n\nOren Bochman\n\n\nMar 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to XAI\n\n\nIn this introduction lecture on explainability in AI, we will delve into the key topics that surround this emerging field. We will first provide an overview of the…\n\n\n\nOren Bochman\n\n\nMar 5, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "tikz in Quarto!\n\n\n\n\n\n\n\n\n\n\n\n\n\nSine function\n\n\n\n\n\n\n\n\n\n\n\n\n\n😁 Quarto 💖 Mermaid🧜 Mindmaps 🧠\n\n\n\n\n\n\n\n\nFeb 12, 20224\n\n\n\n\n\n\n\nLanguage Models Are Open Knowledge Graphs\n\n\npaper review\n\n\n\n\n\nAug 11, 2202\n\n\n\n\n\n\n\nhungarian cheat sheet\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\nThe Great Migration\n\n\nFrom Blogger to Jekyl and finaly to Quarto.\n\n\n\n\n\nJan 30, 2024\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\n\n\n\n\n\nQuarto 💖 Bootstrap 😁\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\n\n\n\n\n\nD3.js in in Quarto Observable\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nBeacuase auto-ml is a Superpower\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\nTransformations in Linguistic Representation\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n\nentropy for uncertainty quantification\n\n\n\n\n\n\n\n\nSep 22, 2022\n\n\n\n\n\n\n\nWikisym 2012\n\n\nConference Report\n\n\n\n\n\nJul 26, 2022\n\n\n\n\n\n\n\nSet Up M1 MacBooks for DS & ML\n\n\n\n\n\n\n\n\nMay 5, 2022\n\n\n\n\n\n\n\ncommand line\n\n\n\n\n\n\n\n\nMay 5, 2022\n\n\n\n\n\n\n\nMulti-armed bandits problem\n\n\n\n\n\n\n\n\nMay 2, 2022\n\n\n\n\n\n\n\nMeme bank\n\n\nmemes a problem-solving approach\n\n\n\n\n\nDec 30, 2021\n\n\n\n\n\n\n\nExcel 2019 for Marketing Statistics in pandas\n\n\n\n\n\n\n\n\nSep 24, 2021\n\n\n\n\n\n\n\nLanguage models and explainability\n\n\n\n\n\n\n\n\nSep 24, 2021\n\n\n\n\n\n\n\nAttention for sensor fusion\n\n\n\n\n\n\n\n\nSep 24, 2021\n\n\n\n\n\n\n\nStorytelling and other essentials\n\n\n\n\n\n\n\n\nSep 2, 2021\n\n\n\n\n\n\n\nStochastic Gradient Descent - The good parts\n\n\n\n\n\n\n\n\nAug 29, 2021\n\n\n\n\n\n\n\nWaveNet Review\n\n\n\n\n\n\n\n\nAug 29, 2021\n\n\n\n\n\n\n\nPython Graphs\n\n\n\n\n\n\n\n\nAug 29, 2021\n\n\n\n\n\n\n\nWhat is in a citation?\n\n\n\n\n\n\n\n\nAug 29, 2021\n\n\n\n\n\n\n\nHackathon session link dumps & notes\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\n\n\n\n\n\nInlining Citations for Wikipedia articles\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\n\n\n\n\n\nTransfer learning in NLP\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\n\n\n\n\n\nA type of Witness and an evolving Idiom\n\n\n\n\n\n\n\n\nJul 14, 2021\n\n\n\n\n\n\n\nTensorFlow probability\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\n\n\n\n\n\nEbook Hacks\n\n\n\n\n\n\n\n\nMay 29, 2021\n\n\n\n\n\n\n\nMultilevel Models\n\n\n\n\n\n\n\n\nMay 16, 2021\n\n\n\n\n\n\n\nQ&A and the Winograd schemas\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\n\n\n\n\n\nAutomatic Summarization Task\n\n\n\n\n\n\n\n\nApr 24, 2021\n\n\n\n\n\n\n\nBayesian agents\n\n\n\n\n\n\n\n\nApr 14, 2021\n\n\n\n\n\n\n\nModeling Events\n\n\n\n\n\n\n\n\nApr 9, 2021\n\n\n\n\n\n\n\nLinkage 2021-04-07\n\n\n\n\n\n\n\n\nApr 7, 2021\n\n\n\n\n\n\n\n10 Tips To Improve Your Workflow\n\n\n\n\n\n\n\n\nApr 7, 2021\n\n\n\n\n\n\n\nJekyll take 3\n\n\nGetting\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\nMathJax 3 fix for Jekyll hosted on Github pages\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\n\n\n\n\n\nEffective Approaches to Attention-based NMT\n\n\nPaper review for the deeplearning.ai NLP specialization\n\n\n\n\n\nMar 21, 2021\n\n\n\n\n\n\n\nnumpy melt down\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\n\n\n\n\n\nRegEX\n\n\ncheat sheet\n\n\n\n\n\nNov 25, 2020\n\n\n\n\n\n\n\nDeep Learning Intuitions\n\n\n\n\n\n\n\n\nOct 25, 2020\n\n\n\n\n\n\n\nbrace expansion\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n\nPandas Productivity Challenge?\n\n\n\n\n\n\n\n\nMar 3, 2020\n\n\n\n\n\n\n\nHow to avoid cross site scripting (XSS) errors with the Jupyter local runtime for Colab\n\n\n\n\n\n\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\nDocker for data science\n\n\n\n\n\n\n\n\nNov 24, 2019\n\n\n\n\n\n\n\ntext annotation with BRAT\n\n\n\n\n\n\n\n\nJan 16, 2018\n\n\n\n\n\n\n\nA/B testing cost and risks?\n\n\n\n\n\n\n\n\nJul 30, 2017\n\n\n\n\n\n\n\nText Mining With Python\n\n\na number of NLP tasks in Python\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nText Mining With R\n\n\na number of NLP tasks in R\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\nan update on NLP with R\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nBash\n\n\ncheet sheet\n\n\n\n\n\nSep 10, 2011\n\n\n\n\n\n\n\nR Books\n\n\nbooks reviews & recommendations\n\n\n\n\n\nAug 26, 2011\n\n\n\n\n\n\n\nTime management Tips\n\n\nWe could all use a productivity boost\n\n\n\n\n\nAug 11, 2011\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Notes to myself, answers to questions no botherd to ask\nThis is a space to share my insights about my interests.\nI’ve decided to migrate to Quarto and see what the platform can do."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "Notes to myself, answers to questions no botherd to ask\nThis is a space to share my insights about my interests.\nI’ve decided to migrate to Quarto and see what the platform can do."
  },
  {
    "objectID": "about.html#newsletter",
    "href": "about.html#newsletter",
    "title": "About",
    "section": "Newsletter 💖",
    "text": "Newsletter 💖\nIf you enjoyed this post, then don’t miss out on any future posts by subscribing to my email newsletter"
  },
  {
    "objectID": "about.html#buy-me-coffe",
    "href": "about.html#buy-me-coffe",
    "title": "About",
    "section": "Buy Me Coffe 😁",
    "text": "Buy Me Coffe 😁"
  },
  {
    "objectID": "qa_demo2.html",
    "href": "qa_demo2.html",
    "title": "Lecture 1 — Introduction to XAI",
    "section": "",
    "text": "bug\n\n\n\n\n\nbug\n\n\n\n\n\nbug\n\n\n\n\n\nbug\n\n\n\n\n\n\n\n\nbug\nbug\nbug\nbug"
  },
  {
    "objectID": "qa_demo2.html#series-poster",
    "href": "qa_demo2.html#series-poster",
    "title": "Lecture 1 — Introduction to XAI",
    "section": "",
    "text": "bug\n\n\n\n\n\nbug\n\n\n\n\n\nbug\n\n\n\n\n\nbug\n\n\n\n\n\n\n\n\nbug\nbug\nbug\nbug"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html",
    "href": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html",
    "title": "TensorFlow probability",
    "section": "",
    "text": "These are my course notes on Probabilistic Deep Learning with TensorFlow by Dr Kevin Webster of Imperial College London 2 The course url\nOne point to note is that many ML courses delve into the “dry” engineering aspect and gloss over the “motivating” examples which are just a demonstration and not the main item. There are two issues with this approach. One is that a model needs some kind of environment to test and evaluate it. And the student benefits more by learning how to think and reason about the techniques than by just learning to replicate the code as a black box.\nSo to get the most out of a course you may to review complementary materials. Your milage may vary depending on the learning curve:\nOn the flip side you should be aware that just as much as teachers like to skip over details they can’t resist showing you certain techniques. So try to master these. Try to think where else they may be used and try to incorporate them into your work before they fade away. Look over the title of recent papers by the profferers and don’t be shy to asks them about methods or problems they have worked on - this can have many many benefits.\nI recommend you should approach the material with plenty of enthusiasm and invest energy in making up for any such shortcoming.\nLearn to own the code as soon as possible. Can you remember the methods. Understand the problem being presented. The problem definition begins with a task and is ML delineated by the variables and features available in the data. Two big questions to consider are what prior knowledge should be incorporated into the model and what types of uncertainty is one dealing with. With deeper understanding of the relations of the features one can modify the model to make these explicit. These can be a hierarchy, correlations, variance and covariance, conditioning, independence, causation and other structures things like mediation.\nNext one should consider a baseline model? This should be the simplest model possible - in the realm of probability this is usually a model with a single most important parameter - the mean. You should try to implement this. Jeremy Howard frequently points out that more sophisticated models may not fate much better.\nThis type of model is getting you to think more like a statistician and you will immediately be able to think about improvement. Adding a second term with a higher moment. What about first order interactions between variables and higher order interactions. Can pooling help, can partial pooling help more. But rather than implementing all these you should consider what other algorithms could be used. Once you have a baseline model you should be able to try out these model with two or three lines of code which leads to the next major point\nHow should the model be evaluated. What’s the best metric (KPI) for the task. Loss, accuracy. What regularization can keep complexity and overfitting at bay. How do we handle a model which outputs both a prediction and a certainty level? Can we train faster of better. Should we use regret. Some models can just be queried others require extra care and work to coax a good result. These could be using embeddings with a vector space metrics, doing a beam search or other methods. Like I mentioned above always be on the look out for new methods to mine you models.\nFinlay its time to think about main item. Can we break down the model conceptually break into it moving parts. What are the model assumptions? Working with probabilistic and statistical models provides a data scientist with many diagnostic capabilities which should become part of the analysis. When assumptions are violates outcomes may require attention but often the model will still function. Think what is the neural net part trying to do and what is the probabilistic component doing? E.g. The NN might look for faces and the probabilistic part may just be a softmax which squishes the outputs into a probability distribution.\nThough I mentioned evaluation before, more in the context of the tasks and baseline model. You will need to have a second go with you specific model in mind. Look at over/under fitting and at tunning hyper-parameter and how do we tune them."
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#naive-bayes-to-classify-new-groups",
    "href": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#naive-bayes-to-classify-new-groups",
    "title": "TensorFlow probability",
    "section": "Naive Bayes to classify new groups",
    "text": "Naive Bayes to classify new groups\n\nnaive bayes assumes feature independence\nlaplace smoothing is a blunt instrument.\nclipping is even more blunt"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#skills",
    "href": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#skills",
    "title": "TensorFlow probability",
    "section": "Skills:",
    "text": "Skills:\n\nrecall basic probability theory: definitions of expectation, variance, standard deviation.\ncreate distributions tensors for univariate, bi-variate and multivariate.\nsample from a distribution\nunderstand the sample, broadcast, event components of a tensor.\nreshape distributions dimensions from batch to event.\nunderstand how broadcasting works.\nlearnable parameters for a distribution.\ndevelop a Naive Bayes classifier for newsgroup articles.\ndevelop a custom training loop in tensor flow"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#distributions---the-sample-the-batch-and-the-event",
    "href": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#distributions---the-sample-the-batch-and-the-event",
    "title": "TensorFlow probability",
    "section": "Distributions - the sample the batch and the event",
    "text": "Distributions - the sample the batch and the event\nThe way we are taught probability and statistics and later how we approach multivariate data in data science reinforces a static and low dimensional view of data. But when we want to work with Graphical models, Bayesian networks, Multilevel models and later with probabilistic neral networks we need to expand our thinking about these. One key new skill one must develop is creating distributions with specific shapes and transforming to new shapes.\nThis manipulation of tensors should already be familiar from work with convolutional or sequence models. But is those cases you typically only need to handle shapes at the input or output of the model and the framework mostly deals with these nitty gritty details. With TensorFlow probability shapes these details need to be part of one’s thinking. Unless they are second nature you will struggle to implementing the with the more complex parts and TFP is just a part of a much bigger picture."
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#broadcasting-rules-np.newaxis",
    "href": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#broadcasting-rules-np.newaxis",
    "title": "TensorFlow probability",
    "section": "Broadcasting Rules & np.newaxis",
    "text": "Broadcasting Rules & np.newaxis\nHere are two essential techniques for tensor manipulation you must master which come from the Numpy library. Two more concepts come from numpy.\nThe first is tensor promotion using a numpy.newaxis as follows\n\nimport numpy as np\n\na = np.array([2, 0, 2, 1])     # shape (4, ) i.e 1d vector\nprint(a)\n# we can promote a 1d vector in two  ways:\n\n# insert new dimension before the existing one:\ncol_vector = a[: np.newaxis]  # shape (4,1) i.e. 2.d vector\nprint(col_vector)\n# insert new dimension after the existing one:\nrow_vector = a[np.newaxis,:]  # shape (1,4) i.e. 2d vector \nprint(row_vector)\n\n[2 0 2 1]\n[2 0 2 1]\n[[2 0 2 1]]\n\n\nor visually:\n\nImage credit.\nNumpy’s broadcasting rule:\n\nPrepend 1s to the smaller shape,\ncheck that the axes of both arrays have sizes that are equal or 1,\nstretch the arrays in their size-1 axes.\n\nhow does numpy newaxis work and when to use it"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#bijectors",
    "href": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#bijectors",
    "title": "TensorFlow probability",
    "section": "Bijectors",
    "text": "Bijectors"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#autoencoder-flows",
    "href": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#autoencoder-flows",
    "title": "TensorFlow probability",
    "section": "Autoencoder Flows",
    "text": "Autoencoder Flows\nOne of the standard components of many sequence models is an element that allows access to input before the current position but masks the input beyond the current position. This is done by subtracting a large number from it. This is used in transformers with self attention which are tasked with generating a sequence from a sequence.\n\nMADE\nHere is an extract from the abstract: &gt; “Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability.” &gt; &gt; - MADE: Masked Autoencoder for Distribution Estimation (Germain et all 2015)\n\n\nReal NVP\nQ. whats is this NVP alphabet soup?\n\nNVP\n\nnon volume preserving.\n\n\n\nUnsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.\n\nDensity estimation using Real NVP (Laurent et all 2017)"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#resources",
    "href": "notes/TFP/2021-06-05-tfp-week3-normalizaing-flows/2021-06-05-tfp-week3-normalizaing-flows.html#resources",
    "title": "TensorFlow probability",
    "section": "Resources:",
    "text": "Resources:\nif like me, you initially feel that numpy array broadcasting feels strange and awkward you may want to look at:\n\nNumpy Broadcasting Guide\nArray Broadcasting in Numpy"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html",
    "href": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html",
    "title": "Probabilistic Deep Learning with TensorFlow",
    "section": "",
    "text": "These are my course notes on Probabilistic Deep Learning with TensorFlow by Dr Kevin Webster of Imperial College London 2 The course url\nOne point to note is that many ML courses delve into the “dry” engineering aspect and gloss over the “motivating” examples which are just a demonstration and not the main item. There are two issues with this approach. One is that a model needs some kind of environment to test and evaluate it. And the student benefits more by learning how to think and reason about the techniques than by just learning to replicate the code as a black box. So to get the most out of a course you may to review complementary materials. Your milage may vary depending on the learning curve: - videos, particularly ones that give a survey of the problems you are working on - articles with similar techniques. - books on theory - research papers if nothing else is available On the flip side you should be aware that just as much as teachers like to skip over details they can’t resist showing you certain techniques. So try to master these. Try to think where else they may be used and try to incorporate them into your work before they fade away. Look over the title of recent papers by the profferers and don’t be shy to asks them about methods or problems they have worked on - this can have many many benefits. I recommend you should approach the material with plenty of enthusiasm and invest energy in making up for any such shortcoming. Learn to own the code as soon as possible. Can you remember the methods. Understand the problem being presented. The problem definition begins with a task and is ML delineated by the variables and features available in the data. Two big questions to consider are what prior knowledge should be incorporated into the model and what types of uncertainty is one dealing with. With deeper understanding of the relations of the features one can modify the model to make these explicit. These can be a hierarchy, correlations, variance and covariance, conditioning, independence, causation and other structures things like mediation. Next one should consider a baseline model? This should be the simplest model possible - in the realm of probability this is usually a model with a single most important parameter - the mean. You should try to implement this. Jeremy Howard frequently points out that more sophisticated models may not fate much better. This type of model is getting you to think more like a statistician and you will immediately be able to think about improvement. Adding a second term with a higher moment. What about first order interactions between variables and higher order interactions. Can pooling help, can partial pooling help more. But rather than implementing all these you should consider what other algorithms could be used. Once you have a baseline model you should be able to try out these model with two or three lines of code which leads to the next major point How should the model be evaluated. What’s the best metric (KPI) for the task. Loss, accuracy. What regularization can keep complexity and overfitting at bay. How do we handle a model which outputs both a prediction and a certainty level? Can we train faster of better. Should we use regret. Some models can just be queried others require extra care and work to coax a good result. These could be using embeddings with a vector space metrics, doing a beam search or other methods. Like I mentioned above always be on the look out for new methods to mine you models. Finlay its time to think about main item. Can we break down the model conceptually break into it moving parts. What are the model assumptions? Working with probabilistic and statistical models provides a data scientist with many diagnostic capabilities which should become part of the analysis. When assumptions are violates outcomes may require attention but often the model will still function. Think what is the neural net part trying to do and what is the probabilistic component doing? E.g. The NN might look for faces and the probabilistic part may just be a softmax which squishes the outputs into a probability distribution. Though I mentioned evaluation before, more in the context of the tasks and baseline model. You will need to have a second go with you specific model in mind. Look at over/under fitting and at tunning hyper-parameter and how do we tune them."
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html#naive-bayes-to-classify-new-groups",
    "href": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html#naive-bayes-to-classify-new-groups",
    "title": "Probabilistic Deep Learning with TensorFlow",
    "section": "Naive Bayes to classify new groups",
    "text": "Naive Bayes to classify new groups\n\nnaive bayes assumes feature independence\nlaplace smoothing is a blunt instrument.\nclipping is even more blunt"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html#skills",
    "href": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html#skills",
    "title": "Probabilistic Deep Learning with TensorFlow",
    "section": "Skills:",
    "text": "Skills:\n\nrecall basic probability theory: definitions of expectation, variance, standard deviation.\ncreate distributions tensors for univariate, bi-variate and multivariate.\nsample from a distribution\nunderstand the sample, broadcast, event components of a tensor.\nreshape distributions dimensions from batch to event.\nunderstand how broadcasting works.\nlearnable parameters for a distribution.\ndevelop a Naive Bayes classifier for newsgroup articles.\ndevelop a custom training loop in tensor flow ## Distributions - the sample the batch and the event The way we are taught probability and statistics and later how we approach multivariate data in data science reinforces a static and low dimensional view of data. But when we want to work with Graphical models, Bayesian networks, Multilevel models and later with probabilistic neral networks we need to expand our thinking about these. One key new skill one must develop is creating distributions with specific shapes and transforming to new shapes. This manipulation of tensors should already be familiar from work with convolutional or sequence models. But is those cases you typically only need to handle shapes at the input or output of the model and the framework mostly deals with these nitty gritty details. With TensorFlow probability shapes these details need to be part of one’s thinking. Unless they are second nature you will struggle to implementing the with the more complex parts and TFP is just a part of a much bigger picture. ## Broadcasting Rules & np.newaxis  Here are two essential techniques for tensor manipulation you must master which come from the Numpy library. Two more concepts come from numpy. The first is tensor promotion using a numpy.newaxis as follows\n\n\nimport numpy as np\na = np.array([2, 0, 2, 1])     # shape (4, ) i.e 1d vector\n\n# we can promote a 1d vector in two  ways:\n\n# insert new dimension before the existing one:\ncol_vector = a[: np.newaxis]  # shape (4,1) i.e. 2.d vector\n\n# insert new dimension after the existing one:\nrow_vector = a[np.newaxis,:]  # shape (1,4) i.e. 2d vector \n\nor visually:  Image credit. Numpy’s broadcasting rule: 1. Prepend 1s to the smaller shape, 2. check that the axes of both arrays have sizes that are equal or 1, 3. stretch the arrays in their size-1 axes. https://stackoverflow.com/questions/29241056/how-does-numpy-newaxis-work-and-when-to-use-it"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html#bijectors",
    "href": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html#bijectors",
    "title": "Probabilistic Deep Learning with TensorFlow",
    "section": "Bijectors",
    "text": "Bijectors"
  },
  {
    "objectID": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html#autoencoder-flows",
    "href": "notes/TFP/2021-06-05-tfp-week1-broadcasting-rules/2021-06-05-tfp-week1-broadcasting-rules.html#autoencoder-flows",
    "title": "Probabilistic Deep Learning with TensorFlow",
    "section": "Autoencoder Flows",
    "text": "Autoencoder Flows\nOne of the standard components of many sequence models is an element that allows access to input before the current position but masks the input beyond the current position. This is done by subtracting a large number from it. This is used in transformers with self attention which are tasked with generating a sequence from a sequence. ### MADE Here is an extract from the abstract: &gt; “Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability.” &gt; &gt; - MADE: Masked Autoencoder for Distribution Estimation (Germain et all 2015)\n\nReal NVP\nQ. whats is this NVP alphabet soup? NVP : non volume preserving. &gt; Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations. &gt; &gt; - Density estimation using Real NVP (Laurent et all 2017) ## Resources: if like me, you initially feel that numpy array broadcasting feels strange and awkward you may want to look at: - Numpy Broadcasting Guide - Array Broadcasting in Numpy"
  },
  {
    "objectID": "notes/XAI/l05/index.html",
    "href": "notes/XAI/l05/index.html",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "",
    "text": "The XAI course provides a comprehensive overview of explainable AI, covering both theory and practice, and exploring various use cases for explainability.\nParticipants will learn not only how to generate explanations, but also how to evaluate and effectively communicate these explanations to diverse stakeholders.\noverview link"
  },
  {
    "objectID": "notes/XAI/l05/index.html#series-poster",
    "href": "notes/XAI/l05/index.html#series-poster",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Series Poster",
    "text": "Series Poster\n\n\n\nseries poster"
  },
  {
    "objectID": "notes/XAI/l05/index.html#session-descriprion",
    "href": "notes/XAI/l05/index.html#session-descriprion",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Session Descriprion",
    "text": "Session Descriprion\nHow to properly incorporate explanations in machine learning projects and what aspects should you keep in mind? Over the past few years the need to explain the output of machine learning models has received growing attention. Explanations not only reveal the reasons behind models predictions and increase users’ trust in the model, but they can be used for different purposes. To fully utilize explanations and incorporate them into machine learning projects the following aspects of explanations should taken into consideration — explanation goals, the explanation method, and explanations’ quality. In this talk, we will discuss how to select the appropriate explanation method based on the intended purpose of the explanation. Then, we will present two approaches for evaluating explanations, including practical examples of evaluation metrics, while highlighting the importance of assessing explanation quality. Next, we will examine the various purposes explanation can serve, along with the stage of the machine learning pipeline the explanation should be incorporated in. Finally we will present a real use case of script classification as malware-related in Microsoft and how we can benefit from high-dimensional explanations in this context."
  },
  {
    "objectID": "notes/XAI/l05/index.html#session-video",
    "href": "notes/XAI/l05/index.html#session-video",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Session Video",
    "text": "Session Video\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\nThe task here was to identify bad insurgence claims. e.g.\n\nwhen the product was out of warranty\nthe item was not insured\nthe damage was not covered.\n\nthe model found many claim that the insurance people had not and they were skeptical.\nthe data scientist was on the spot and needed local explanations.\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\nso we have some use cases, how do we do it!?"
  },
  {
    "objectID": "notes/XAI/l02/index.html",
    "href": "notes/XAI/l02/index.html",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "",
    "text": "XAI is all about illuminating the opaque inner working of black box model. These are the type of models data scientist prefer to deploy to production as they tend to give better results. The rub is that many end users and other stakeholders like executives may not trust the predictions made by such models. After all we all learned that:\nXAI empowers the data scientist with post hoc methods that manipulate the black box model and make the outcomes more aproachable to users.\nThere are added benefits - when we use local explanations to understand why the model is giving bad predictions for specific entries. This understanding is the best way to move forward and improve the model. We can also use these to understand the biases that tend to creep into our model so we can take steps to mitigate it.\nThis is a fascinating session on XAI, building on the previous session. I’ve embedded the video below.\nThe speakers did not provide code samples. I have tried to add some code samples but any shortcoming are mine."
  },
  {
    "objectID": "notes/XAI/l02/index.html#series-poster",
    "href": "notes/XAI/l02/index.html#series-poster",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Series Poster",
    "text": "Series Poster\n\n\n\nseries poster"
  },
  {
    "objectID": "notes/XAI/l02/index.html#session-video",
    "href": "notes/XAI/l02/index.html#session-video",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Session Video",
    "text": "Session Video\nThis is the video for this session:"
  },
  {
    "objectID": "notes/XAI/l02/index.html#instructor-biographies",
    "href": "notes/XAI/l02/index.html#instructor-biographies",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Instructor Biographies",
    "text": "Instructor Biographies\n\nBitya Neuhof\n\nPh.D student, Statistics & Data Science\nHUJI\nBitya is a Ph.D. student in Statistics and Data Science at the Hebrew University, exploring and developing explainable AI methods. Before her PhD she worked as a Data Scientist specializing in analyzing high-dimensional tabular data. Bitya is also a Core-Team member at Baot, the largest Israeli community of experienced women in R&D.\nlinkedin profile\n\nYasmin Bokobza\n\nML Scientist Leader\nMicrosoft\nYasmin is a ML Scientist Leader and Mentor in the Startups Accelerator program at Microsoft. Her work focuses on developing (ML) models for Microsoft Cloud Computing Platforms and Services. Part of her work has been filed as patents, published in Microsoft Journal of Applied Research (MSJAR), and presented at various conferences, meetups and webinars. Previously her work focused on the security field developing ML models to detect cyber-attacks and methods to harvest leaked information in social networks using socialbots and crawler and detecting the source of the leak. She is listed as a cyber threat detection method patent author and part of her research was published at the IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. Yasmin graduated fast track for an MSc degree, that focused on ML & Security, in the department of Information Systems Engineering at Ben-Gurion University in Israel.\nlinkedin profile"
  },
  {
    "objectID": "notes/XAI/l02/index.html#agenda",
    "href": "notes/XAI/l02/index.html#agenda",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Agenda",
    "text": "Agenda\n\nApproaches:\n\nPost-hoc - create a new model to explain the main model.\nTransparent/Intrinsic models - e.g. a probabilistic model\n\nLocal v.s. Global\nPost-hoc Explainability\n\nTechnique Categorization\nLime\nSHAP\n\nConclusions"
  },
  {
    "objectID": "notes/XAI/l02/index.html#explainability-approaches",
    "href": "notes/XAI/l02/index.html#explainability-approaches",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Explainability approaches",
    "text": "Explainability approaches\n\n\n\nExplainability approaches\n\n\n\nPost hoc techniques - make use an explainer model to provide explanations.\nTransparent models - can be queried directly to provide explanations\n\nprobabilistic models\ndecision trees\nregression models"
  },
  {
    "objectID": "notes/XAI/l02/index.html#global-explanations",
    "href": "notes/XAI/l02/index.html#global-explanations",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Global Explanations",
    "text": "Global Explanations\n\nGlobal explanations describe the average behavior of a ML model.\n\nWhat for?\n\nProvide insights into the overall behavior of ML model\nCan help identify patterns and relations in the data learned by the model\n\nTechniques:\n\nDecision Tree\n\nWhy?\n\nAnalyze the general behavior of the model\nIdentify important features for the model’s predictions\nFeature selection\nModel optimization\n\nWhy Not?\n\nWhat is a sensible way to aggregate a model ?\nMay Oversimplify a complex model.\nWhich leads to inaccurate interpretations."
  },
  {
    "objectID": "notes/XAI/l02/index.html#local-explanations",
    "href": "notes/XAI/l02/index.html#local-explanations",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Local Explanations",
    "text": "Local Explanations\n\nLocal explanations are interpretation of the ML prediction for individual instances. 1\n\nWhat for?\n\nProvide a detailed understanding of how a model arrived at its prediction for a specific input.\nCan help identify and correct model errors\nFoster trust in stakeholders whom are skeptical of black box models.\n\nTechniques:\n\nLIME (Local Interpretable Model Agnostic Explanations), introduced in (Ribeiro, Singh, and Guestrin 2016)\nSHAP (Shapely Additive Explanations), introduced in (Lundberg and Lee 2017)\n\nWhy?\n\nProvides insights into predictions for specific rows.\nA complex model can be simple locally. 2\nCan explain changes of prediction for rows without changes in the model.\n\nWhy Not?\n\nLimited in scope.\nDoes not provide a holistic understanding of the model.\nConstitutionally expensive for large datasets\n\n\n\n\n\n\n\nTable 1: Local & Global method Comparison"
  },
  {
    "objectID": "notes/XAI/l02/index.html#lime-for-tabular-data",
    "href": "notes/XAI/l02/index.html#lime-for-tabular-data",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "LIME for Tabular Data",
    "text": "LIME for Tabular Data\n\n\n\nLIME for Tabular\n\n\n\nfrom  lime import lime_tabular\n\ny_pred = dt_clf_model.predict(X_test)\n\nfeature_names=X_train.columns\nlime_explainer = lime_tabular.LimeTabularExplainer(\n      training_data=X_train.to_numpy(),\n      feature_names=feature_names,\n      class_names=['Salary'],\n      categorical_features=['is_male','is_BA','is_MA','is_PhD'],\n      verbose=True,\n      mode='regression')\n\ni = np.random.randint(0, X_test.shape[0])\n\nexp = lime_explainer.explain_instance(X_test.values[i,:], \n                                      dt_clf_model.predict, \n                                      num_features=5,\n                                      num_samples=100)\nexp.as_list()\n\nIntercept 89860.82769506171\nPrediction_local [127249.76176855]\nRight: 117647.05882352941\n\n\n/home/oren/work/blog/env/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning:\n\nX does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n\n\n\n[('9.00 &lt; xp &lt;= 16.00', 14266.36525567986),\n ('is_PhD &lt;= 0.00', 9656.644998137324),\n ('is_MA &lt;= 0.00', 5032.763928787686),\n ('37.00 &lt; Age &lt;= 44.00', 4979.7066261167365),\n ('0.00 &lt; is_male &lt;= 1.00', 3453.4532647694036)]\ntext output for a lime explainer\n\n\n\n\n\nLIME for Tabular Viz\n\n\n\nexp.show_in_notebook(show_table=True)\n\n\n\nTable 6\n\n\n\n\n        \n        \n        \n        \n        \n        \nA graphical LIME explaination for an entry in the Salary DataSet”\n\n\n\n\n\n\nimport shap\nexplainer = shap.TreeExplainer(dt_clf_model,X_test)\nshap_values = explainer.shap_values(X)\nshap_values[i]\n\narray([-15111.68966346,  18420.82584135,      0.        ,      0.        ,\n          823.07704327,      0.        ])"
  },
  {
    "objectID": "notes/XAI/l02/index.html#lime-an-intuitive-explantion",
    "href": "notes/XAI/l02/index.html#lime-an-intuitive-explantion",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "LIME an intuitive explantion",
    "text": "LIME an intuitive explantion\n 1. Our data is a complex manifold with non-convex boundry pink region 2. repeat: 1. We pick a single row r_i in the data set which we call an instance. 2. We then perturb it by modifying the instance randomly p_i=x_i + \\delta 3. We generate a prediction for the perturbation using our black box model \\hat y_{p_i} 4. We reweigh each perturbation using the relative distance of the prediction: w \\propto | \\hat{y} - \\hat y_{p_i} |\nMore precisely, the explanation for a data point x is the model g that minimizes the locality-aware loss L(f,g,Π_x) measuring how unfaithful g approximates the model to be explained f in its vicinity Π_x while keeping the model complexity denoted low.\n\n\\arg\\min _g L(f,g,\\pi_x)+\\Omega(g)\n\nTherefore, LIME experiences a trade off between model fidelity and complexity\nfor more information on lime consult (Molnar 2022 ) section on Lime ."
  },
  {
    "objectID": "notes/XAI/l02/index.html#lime-for-images",
    "href": "notes/XAI/l02/index.html#lime-for-images",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "LIME for Images",
    "text": "LIME for Images\n\n\n\nLIME for Images\n\n\n\n\n\n\nLIME for Images"
  },
  {
    "objectID": "notes/XAI/l02/index.html#lime-pros-cons",
    "href": "notes/XAI/l02/index.html#lime-pros-cons",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "LIME pros & Cons",
    "text": "LIME pros & Cons\n\n\n\nLIME Pros & Cons"
  },
  {
    "objectID": "notes/XAI/l02/index.html#shap",
    "href": "notes/XAI/l02/index.html#shap",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "SHAP",
    "text": "SHAP\n\n\n\nSHAP\n\n\n\n\n\n\nTerminology\n\n\n\n\n\n\nShapley Values\n\n\n\n\nLink to Wikipedia article\nLloyd Shapley was the Noble Memorial Prize Laureate for this gem back in in 2012\nFar a cooperative game it considers all coalitions and lets us see how much each is contributing to overall surplus.\nThis idea can then be used to decide how divide the surplus (profit) most fairly.\nThink how the extremest can set the tone for a coalition by threatening to break it up.\n\n\n 1. Efficiency - The sum of the Shapley values of all agents equals the value of the grand coalition, so that all the gain is distributed among the agents: 2. Symmetry - equal treatment of equals 3. Linearity - If two coalition games described by gain functions {\\displaystyle v} and {\\displaystyle w} are combined, then the distributed gains should correspond to the gains derived from {\\displaystyle v} and the gains derived from {\\displaystyle w} 4. Monotonically 5. Null Player - The Shapley value \\varphi _{i}(v) of a null player i in a game v is zero.\n\n\n\n\nShapley Formula\n\n\n\n\n\n\nIn ML\n\n\n\n\n\n\nShapley Problems\n\n\n\n\n\n\nShapley for ML\n\n\n\n\n\n\nSHAP"
  },
  {
    "objectID": "notes/XAI/l02/index.html#shap---shapley-addative-explanations",
    "href": "notes/XAI/l02/index.html#shap---shapley-addative-explanations",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "SHAP - Shapley Addative Explanations",
    "text": "SHAP - Shapley Addative Explanations\n\n\n\nKernel SHAP\n\n\n\n\n\n\nTree SHAP\n\n\n\n\n\n\nDecision Tree\n\n\n\n\n\n\nTreeExplainer\n\n\n\n\n\n\nKernel Explainer\n\n\n\n\n\n\nSHAP Visualization\n\n\n\n\n\n\nLocal Waterfall Plot\n\n\n\n\n\n\nLocal Bar Plot\n\n\n\n\n\n\nGlobal Bar Plot\n\n\n\n\n\n\nGlobal Bar Plot\n\n\n\n\n\n\nGlobal Beeswarm\n\n\n\n\n\n\nGlobal Scatter Plot\n\n\n\n\n\n\nGloble Scatter Plot\n\n\n\n\n\n\nGloble Scatter Plot\n\n\n\n\n\n\nModel Hierarchy"
  },
  {
    "objectID": "notes/XAI/l02/index.html#hierarchy",
    "href": "notes/XAI/l02/index.html#hierarchy",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Hierarchy",
    "text": "Hierarchy\n\n\n\nLocal Uncertainty\n\n\n\n\n\n\nReferences"
  },
  {
    "objectID": "notes/XAI/l02/index.html#conclusion",
    "href": "notes/XAI/l02/index.html#conclusion",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Conclusion",
    "text": "Conclusion\nThis course presented so much information it is easy to loose sight of some key point, so here are a few conclusions.\n\nother approaches which include EDA.\nusing more transparent models e.g. regressions or statistical models.\nby far the most prevalent approach in XAI is post hoc methods.\ndefined global and local explanations and noted their limitations.\n\nWhat do we mean by explanations in XAI:\n\ncould be any number of visualization.\ncould be a simplified model. 💡 locally a complex manifold may look flat.\ncould be a ranking of the features by their contribution. 💡 SHAP and MIE\ncould be by picking related examples 💡 KNN\n\n\nReferences\n\n\nBreiman, L., J. Friedman, C. J. Stone, and R. A. Olshen. 1984. Classification and Regression Trees. Taylor & Francis. https://books.google.co.il/books?id=JwQx-WOmSyQC.\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emily Pitkin. 2013. “Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.” Journal of Computational and Graphical Statistics 24: 44–65. https://api.semanticscholar.org/CorpusID:88519447.\n\n\nLundberg, Scott, and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” https://arxiv.org/abs/1705.07874.\n\n\nMolnar, Christoph. 2022. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. 2nd ed. https://christophm.github.io/interpretable-ml-book.\n\n\nPoyiadzi, Rafael, Kacper Sokol, Raúl Santos-Rodriguez, Tijl De Bie, and Peter A. Flach. 2019. “FACE: Feasible and Actionable Counterfactual Explanations.” CoRR abs/1909.09369. http://arxiv.org/abs/1909.09369.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “Why Should I Trust You?: Explaining the Predictions of Any Classifier.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135–44. KDD ’16. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2939672.2939778.\n\n\nSelvaraju, Ramprasaath R., Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra. 2016. “Grad-CAM: Why Did You Say That? Visual Explanations from Deep Networks via Gradient-Based Localization.” CoRR abs/1610.02391. http://arxiv.org/abs/1610.02391.\n\n\nZilke, Jan Ruben, Eneldo Loza Mencía, and Frederik Janssen. 2016. “DeepRED - Rule Extraction from Deep Neural Networks.” In IFIP Working Conference on Database Semantics. https://api.semanticscholar.org/CorpusID:10289003.\n\n\n\n\n\nseries poster\nExplainability approaches\nTable 1: Local & Global method Comparison\nTable 2: Techniques Categorization Table\nTable 3: Post-hoc Explainability Table\nLIME Post-hoc\nsalary prediction dataset overview\nSalary Prediction DS\nPreprocessing\nFigure 1: A simple decision tree for the Salary DataSet\nLIME for Tabular\nLIME for Tabular Viz\nLIME for Images\nLIME for Images\nLIME Pros & Cons\nSHAP\nTerminology\nShapley Values\nShapley Formula\nIn ML\nShapley Problems\nShapley for ML\nSHAP\nKernel SHAP\nTree SHAP\nDecision Tree\nTreeExplainer\nKernel Explainer\nSHAP Visualization\nLocal Waterfall Plot\nLocal Bar Plot\nGlobal Bar Plot\nGlobal Bar Plot\nGlobal Beeswarm\nGlobal Scatter Plot\nGloble Scatter Plot\nGloble Scatter Plot\nModel Hierarchy\nLocal Uncertainty\nReferences"
  },
  {
    "objectID": "notes/XAI/l02/index.html#footnotes",
    "href": "notes/XAI/l02/index.html#footnotes",
    "title": "2 Local Explanations - Concept and Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e. for a breakdown for the given prediction↩︎\nthink anomalies and sub-populations↩︎"
  },
  {
    "objectID": "notes/XAI/l01/index.html",
    "href": "notes/XAI/l01/index.html",
    "title": "Lecture 1 — Introduction to XAI",
    "section": "",
    "text": "The XAI course provides a comprehensive overview of explainable AI, covering both theory and practice, and exploring various use cases for explainability.\nParticipants will learn not only how to generate explanations, but also how to evaluate and effectively communicate these explanations to diverse stakeholders.\noverview link"
  },
  {
    "objectID": "notes/XAI/l01/index.html#series-poster",
    "href": "notes/XAI/l01/index.html#series-poster",
    "title": "Introduction to XAI",
    "section": "Series Poster",
    "text": "Series Poster\n\n\n\nseries poster"
  },
  {
    "objectID": "notes/XAI/l01/index.html#session-description",
    "href": "notes/XAI/l01/index.html#session-description",
    "title": "Introduction to XAI",
    "section": "Session Description",
    "text": "Session Description\n\nIn this introduction lecture on explainability in AI, we will delve into the key topics that surround this emerging field.\nOverall, this lecture will provide a comprehensive introduction to explainability in AI, covering the key topics and terminology that are essential for understanding this field."
  },
  {
    "objectID": "notes/XAI/l01/index.html#session-video",
    "href": "notes/XAI/l01/index.html#session-video",
    "title": "Introduction to XAI",
    "section": "Session Video",
    "text": "Session Video"
  },
  {
    "objectID": "notes/XAI/l01/index.html#what-is-explainability",
    "href": "notes/XAI/l01/index.html#what-is-explainability",
    "title": "Introduction to XAI",
    "section": "What is Explainability?",
    "text": "What is Explainability?\n\nExplainability Definition"
  },
  {
    "objectID": "notes/XAI/l01/index.html#anaplan",
    "href": "notes/XAI/l01/index.html#anaplan",
    "title": "Lecture 1 — Introduction to XAI",
    "section": "Anaplan",
    "text": "Anaplan"
  },
  {
    "objectID": "notes/XAI/l01/index.html#footnotes",
    "href": "notes/XAI/l01/index.html#footnotes",
    "title": "Introduction to XAI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnfortunately, this is a circular definition.\nTrees are highly sensitive to small changes in the data\nThe Bonferroni point, or adjusted p value is the point at which you need to adjust the p-value threshold due to multiple comparisons when performing feature selection . In simpler terms, it’s about accounting for the increased chance of falsely identifying significant features when you test many features simultaneously\nThe last lecture provides some insights and charts to assist this step!"
  },
  {
    "objectID": "notes/rhetoric/igezo-szotar.html",
    "href": "notes/rhetoric/igezo-szotar.html",
    "title": "igezo-szotar",
    "section": "",
    "text": "ad\náll\nállít\nbeszél\n\n\nbújik\ncsap\ncsíp\ncsúszik\n\n\ndob\ndől\nenged\nesik\n\n\neszik\nég\nél\népít\n\n\nér\nfekszik\nfér\nfizet\n\n\nfog\nfolyik\nfordít\nfordul\n\n\nforgat\nfúj\nfut.\ngondol\n\n\nhagy\nhajt\nhív\nhoz\n\n\nhúz\nismer\nír\njár\n\n\njátszik\njön\njut.\nkap\n\n\nkapcsol\nkel\nken\nkér\n\n\nkerül\nköt\nlát\nlép\n\n\nlő\nmarad\nmegy\nmér\n\n\nmond\nmos\nmutat.\nnéz\n\n\nnyom\nnyújt\nolvas\nszt\n\n\npakol\nragad\nrak\nráz\n\n\nszakad\nszáll\nszalad\nszárnít\n\n\nszámol\nszed\nszorít\nszorul\n\n\nszól\nszúr\ntalál\ntanul.\n\n\ntart\ntesz\ntelik\ntér\n\n\ntölt\ntör\ntörik\ntűz\n\n\nugrik\nül\nüt.\nvág\n\n\nválaszt.\nválik\nvált\nvesz\n\n\nver\nvet\nvezet\nvisz\n\n\nvon\nvonul\nzár"
  },
  {
    "objectID": "notes/rhetoric/igezo-szotar.html#sec-ad",
    "href": "notes/rhetoric/igezo-szotar.html#sec-ad",
    "title": "igezo-szotar",
    "section": "AD",
    "text": "AD\n\n\n\n\n\n\n\n\n\nátad\n\n\n\n\n\n\nátad vkinek vmit\nhand over sth to sy\n\n\n\n\nátadja a szot vkinek\ncall upon sy to speak\n\n\n\n\nátadja az üdvözletet vkinek\nconvey sy’s be st wishes to sy, say hello to sy for sy\n\n\n\n\nátadja a helyét vkinek\ngive up one’s seat to sy\n\n\n\nbead\n\n\n\n\n\n\nbeadja a gyógyszert vkinek\nadminister medicine to sy\n\n\n\n\nbeadja a kérvényt vhová\nsubmit / present a request; file a petition\n\n\n\n\nbeadja a felmondását\nhand in one’ s notice\n\n\n\nelad\n\n\n\n\n\n\nelad vmit vmennyiért\nsell sth for sth\n\n\n\nelőad\n\n\n\n\n\n\nelőad vmit\nperform, enact, recite etc. sth\n\n\n\n\nelőad\nlecture\n\n\n\nfelad\n\n\n\n\n\n\nfelad (pl. levelet, csomagot)\npost (e.g. a letter, a parcel)\n\n\n\n\nfeladja a kabátot vkire\nhelp sy on with their coat\n\n\n\n\nfeladja a leckét\nassign homework\n\n\n\nhozzáad\n\n\n\n\n\n\nhozzáad vmihez vmit\nadd sth to sth\n\n\n\n\nhozzáad vkihez vkit\nmarry sy off\n\n\n\nidead\n\n\n\n\n\n\nidead vmit\ngive sth to me\n\n\n\nkiad\n\n\n\n\n\n\nkiad egy könyvet\npublish a book\n\n\n\n\nkiad egy szobát / egy lakást\nrent a room / a house\n\n\n\n\nkiadja a munkát vkinek\ndistribute work\n\n\n\n\nkiadja a mérgét / dühét\nvent one’s rage\n\n\n\nlead\n\n\n\n\n\n\nlead vmit vhol\nleave sth swhere, hand sth in\n\n\n\n\nlead vmennyit\nlose weight\n\n\n\nmegad\n\n\n\n\n\n\nmegadja az adósságát\npay off one’s debts"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04b.html",
    "href": "notes/dnn/dnn-04/l_04b.html",
    "title": "Deep Neural Networks - Notes for lecture 4b",
    "section": "",
    "text": "This video is part of the course, i.e. it’s not optional, despite what Geoff says in the beginning of the video. This video gives a high-level interpretation of what’s going on in the family tree network. This video contrasts two types of inference:\n\nConscious inference, based on relational knowledge.\nUnconscious inference, based on distributed representations.\n\n\n\n• There has been a long debate in cognitive science between two rival theories of what it means to have a concept: The feature theory: A concept is a set of semantic features. – This is good for explaining similarities between concepts. – Its convenient: a concept is a vector of feature activities. The structuralist theory: The meaning of a concept lies in its relationships to other concepts. – So conceptual knowledge is best expressed as a relational graph. – Minsky used the limitations of perceptrons as evidence against feature vectors and in favor of relational graph representations.\n\n\n\n\nThese two theories need not be rivals. A neural net can use vectors of semantic features to implement a relational graph.\n\nIn the neural network that learns family trees, no explicit inference is required to arrive at the intuitively obvious consequences of the facts that have been explicitly learned.\nThe net can “intuit” the answer in a forward pass.\n\nWe may use explicit rules for conscious, deliberate reasoning, but we do a lot of commonsense, analogical reasoning by just “seeing” the answer with no conscious intervening steps.\n\nEven when we are using explicit rules, we need to just see which rules to apply.\n\n\n\n\n\n\nThe obvious way to implement a relational graph in a neural net is to treat a neuron as a node in the graph and a connection as a binary relationship. But this “localist” method will not work:\n\nWe need many different types of relationship and the connections in a neural net do not have discrete labels.\nWe need ternary relationships as well as binary ones. e.g. A is between B and C.\n\nThe right way to implement relational knowledge in a neural net is still an open issue.\n\nBut many neurons are probably used for each concept and each neuron is probably involved in many concepts. This is called a “distributed representation”."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04b.html#what-the-family-trees-example-tells-us-about-concepts",
    "href": "notes/dnn/dnn-04/l_04b.html#what-the-family-trees-example-tells-us-about-concepts",
    "title": "Deep Neural Networks - Notes for lecture 4b",
    "section": "",
    "text": "• There has been a long debate in cognitive science between two rival theories of what it means to have a concept: The feature theory: A concept is a set of semantic features. – This is good for explaining similarities between concepts. – Its convenient: a concept is a vector of feature activities. The structuralist theory: The meaning of a concept lies in its relationships to other concepts. – So conceptual knowledge is best expressed as a relational graph. – Minsky used the limitations of perceptrons as evidence against feature vectors and in favor of relational graph representations."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04b.html#both-sides-are-wrong",
    "href": "notes/dnn/dnn-04/l_04b.html#both-sides-are-wrong",
    "title": "Deep Neural Networks - Notes for lecture 4b",
    "section": "",
    "text": "These two theories need not be rivals. A neural net can use vectors of semantic features to implement a relational graph.\n\nIn the neural network that learns family trees, no explicit inference is required to arrive at the intuitively obvious consequences of the facts that have been explicitly learned.\nThe net can “intuit” the answer in a forward pass.\n\nWe may use explicit rules for conscious, deliberate reasoning, but we do a lot of commonsense, analogical reasoning by just “seeing” the answer with no conscious intervening steps.\n\nEven when we are using explicit rules, we need to just see which rules to apply."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04b.html#localist-and-distributed-representations-of-concepts",
    "href": "notes/dnn/dnn-04/l_04b.html#localist-and-distributed-representations-of-concepts",
    "title": "Deep Neural Networks - Notes for lecture 4b",
    "section": "",
    "text": "The obvious way to implement a relational graph in a neural net is to treat a neuron as a node in the graph and a connection as a binary relationship. But this “localist” method will not work:\n\nWe need many different types of relationship and the connections in a neural net do not have discrete labels.\nWe need ternary relationships as well as binary ones. e.g. A is between B and C.\n\nThe right way to implement relational knowledge in a neural net is still an open issue.\n\nBut many neurons are probably used for each concept and each neuron is probably involved in many concepts. This is called a “distributed representation”."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04d.html",
    "href": "notes/dnn/dnn-04/l_04d.html",
    "title": "Deep Neural Networks - Notes for lecture 4d",
    "section": "",
    "text": "Lecture 4d: Neuro-probabilistic language models\nThis is the first of several applications of neural networks that we’ll studying in some detail, in this course.\nSynonyms: word embedding; word feature vector; word encoding.\nAll of these describe the learned collection of numbers that is used to represent a word. “embedding” emphasizes that it’s a location in a high-dimensional space: it’s where the words are embedded in that space. When we check to see which words are close to each other, we’re thinking about that embedding.\n“feature vector” emphasizes that it’s a vector instead of a scalar, and that it’s componential, i.e. composed of multiple feature values.\n“encoding” is very generic and doesn’t emphasize anything specific. looks at the trigram model\n\nA basic problem in speech recognition\n\nWe cannot identify phonemes perfectly in noisy speech\n\nThe acoustic input is often ambiguous: there are several different words that fit the acoustic signal equally well.\n\nPeople use their understanding of the meaning of the utterance to hear the right words.\n\nWe do this unconsciously when we wreck a nice beach.\nWe are very good at it.\n\nThis means speech recognizers have to know which words are likely to come next and which are not.\n\nFortunately, words can be predicted quite well without full understanding.\n\n\n\n\nThe standard “trigram” method\n\nTake a huge amount of text and count the frequencies of all triples of words.\nUse these frequencies to make bets on the relative probabilities of words given the previous two words:\n\n\n\\frac{p(w_3=c|w_2=b,w_1=a)}{p(w_3=d|w_2=b,w_1=a)}=\\frac{count(abc)}{count(abd)}\n\n\nUntil very recently this was the state-of-the-art.\nWe cannot use a much bigger context because there are too many possibilities to store and the counts would mostly be zero.\nWe have to “back-off” to digrams when the count for a trigram is too small.\n\nThe probability is not zero just because the count is zero!\n\n\n\n\nInformation that the trigram model fails to use\n\nSuppose we have seen the sentence “the cat got squashed in the garden on friday”\nThis should help us predict words in the sentence “the dog got flattened in the yard on monday”\nA trigram model does not understand the similarities between\n\ncat/dog squashed/flattened garden/yard friday/monday\n\nTo overcome this limitation, we need to use the semantic and syntactic features of previous words to predict the features of the next word.\n\nUsing a feature representation also allows a context that contains many more previous words (e.g. 10).\n\n\n\n\n\n\n\nReuseCC SA BY-NC-NDCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} for Lecture 4d},\n  date = {2017-08-14},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-04/l_04d.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes for Lecture\n4d.” August 14, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-04/l_04d.html."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html",
    "href": "notes/dnn/dnn-04/l_04a.html",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "relational information\n\n\n\n\n\n\nMake a set of propositions using the 12 relationships:\n\nson, daughter, nephew, niece, father, mother, uncle, aunt\nbrother, sister, husband, wife\n\n(Colin has-father James)\n(Colin has-mother Victoria)\n(James has-wife Victoria) this follows from the two above\n(Charlotte has-brother Colin)\n(Victoria has-brother Arthur)\n(Charlotte has-uncle Arthur) this follows from the above\n\n\n\n\n\nGiven a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules (x has-mother y) & (y has-husband z) =&gt; (x has-father z)\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\nCan a neural network capture the same knowledge by searching through a continuous space of weights?\n\n\n\n\n\n\n\n\n\nstructure of the neural net\n\n\n\n\n\n\nvisulization of 6 neuron weights\n\n\n\n\n\nthe relational data\n\n\n\n\n\n\n\nThe six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\n\nNationality, generation, branch of the family tree.\n\nThese features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example:\n\nInput person is of generation 3 and\nrelationship requires answer to be one generation up\nimplies\nOutput person is of generation 2\n\n\n\n\n\n\nTrain the network on all but 4 of the triples that can be made using the 12 relationships\n\nIt needs to sweep through the training set many times adjusting the weights slightly each time.\n\nThen test it on the 4 held-out cases.\n\nIt gets about 3/4 correct.\nThis is good for a 24-way choice.\nOn much bigger datasets we can train on a much smaller fraction of the data.\n\n\n\n\n\n\nSuppose we have a database of millions of relational facts of the form (A R B).\n\nWe could train a net to discover feature vector representations of the terms that allow the third term to be predicted from the first two.\nThen we could use the trained net to find very unlikely triples. These are good candidates for errors in the database.\n\nInstead of predicting the third term, we could use all three terms as input and predict the probability that the fact is correct.\n\nTo train such a net we need a good source of false facts.\n\n\n\n\n\n\nGiven a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules:\n\nHasMother(x,y)\\ and\\ HasHusband(y,z) \\implies HasFather(x, z)\n\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\n\nCan a neural network capture the same knowledge by searching through a continuous space of weights?\n\n\n\n\n\nThe six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\nNationality, generation, branch of the family tree. These features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example: Input person is of generation 3 and relationship requires answer to be one generation up implies Output person is of generation 2 This video introduces distributed representations. It’s not actually about predicting words, but it’s building up to that. It does a great job of looking inside the brain of a neural network. That’s important, but not always easy to do.\n\n\n\n\nrelational information\nstructure of the neural net\nvisulization of 6 neuron weights\nthe relational data"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#a-simple-example-of-relational-information",
    "href": "notes/dnn/dnn-04/l_04a.html#a-simple-example-of-relational-information",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "relational information"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#another-way-to-express-the-same-information",
    "href": "notes/dnn/dnn-04/l_04a.html#another-way-to-express-the-same-information",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Make a set of propositions using the 12 relationships:\n\nson, daughter, nephew, niece, father, mother, uncle, aunt\nbrother, sister, husband, wife\n\n(Colin has-father James)\n(Colin has-mother Victoria)\n(James has-wife Victoria) this follows from the two above\n(Charlotte has-brother Colin)\n(Victoria has-brother Arthur)\n(Charlotte has-uncle Arthur) this follows from the above"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#a-relational-learning-task",
    "href": "notes/dnn/dnn-04/l_04a.html#a-relational-learning-task",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Given a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules (x has-mother y) & (y has-husband z) =&gt; (x has-father z)\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\nCan a neural network capture the same knowledge by searching through a continuous space of weights?"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#the-structure-of-the-neural-net",
    "href": "notes/dnn/dnn-04/l_04a.html#the-structure-of-the-neural-net",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "structure of the neural net\n\n\n\n\n\n\nvisulization of 6 neuron weights\n\n\n\n\n\nthe relational data"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#what-the-network-learns",
    "href": "notes/dnn/dnn-04/l_04a.html#what-the-network-learns",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "The six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\n\nNationality, generation, branch of the family tree.\n\nThese features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example:\n\nInput person is of generation 3 and\nrelationship requires answer to be one generation up\nimplies\nOutput person is of generation 2"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#another-way-to-see-that-it-works",
    "href": "notes/dnn/dnn-04/l_04a.html#another-way-to-see-that-it-works",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Train the network on all but 4 of the triples that can be made using the 12 relationships\n\nIt needs to sweep through the training set many times adjusting the weights slightly each time.\n\nThen test it on the 4 held-out cases.\n\nIt gets about 3/4 correct.\nThis is good for a 24-way choice.\nOn much bigger datasets we can train on a much smaller fraction of the data."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#a-large-scale-example",
    "href": "notes/dnn/dnn-04/l_04a.html#a-large-scale-example",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Suppose we have a database of millions of relational facts of the form (A R B).\n\nWe could train a net to discover feature vector representations of the terms that allow the third term to be predicted from the first two.\nThen we could use the trained net to find very unlikely triples. These are good candidates for errors in the database.\n\nInstead of predicting the third term, we could use all three terms as input and predict the probability that the fact is correct.\n\nTo train such a net we need a good source of false facts."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#a-relational-learning-task-1",
    "href": "notes/dnn/dnn-04/l_04a.html#a-relational-learning-task-1",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Given a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules:\n\nHasMother(x,y)\\ and\\ HasHusband(y,z) \\implies HasFather(x, z)\n\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\n\nCan a neural network capture the same knowledge by searching through a continuous space of weights?"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#the-structure-of-the-neural-net-1",
    "href": "notes/dnn/dnn-04/l_04a.html#the-structure-of-the-neural-net-1",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "The six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\nNationality, generation, branch of the family tree. These features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example: Input person is of generation 3 and relationship requires answer to be one generation up implies Output person is of generation 2 This video introduces distributed representations. It’s not actually about predicting words, but it’s building up to that. It does a great job of looking inside the brain of a neural network. That’s important, but not always easy to do.\n\n\n\n\nrelational information\nstructure of the neural net\nvisulization of 6 neuron weights\nthe relational data"
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html",
    "href": "notes/dnn/dnn-11/l_11.html",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11a-hopfield-nets",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11a-hopfield-nets",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11a: Hopfield Nets",
    "text": "Lecture 11a: Hopfield Nets\n(Hopfield 1982)\nNow, we leave behind the feedforward deterministic networks that are trained with backpropagation gradients. We’re going to see quite a variety of different neural networks now. These networks do not have output units. These networks have units that can only be in states 0 and 1. These networks do not have units of which the state is simply a function of the state of other units. These networks are, instead, governed by an “energy function”. Best way to really understand Hopfield networks: Go through the example of the Hopfield network finding a low energy state, by yourself. Better yet, think of different weights, and do the exercise with those. Typically, we’ll use Hopfield networks where the units have state 0 or 1; not -1 or 1."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11b-dealing-with-spurious-minima",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11b-dealing-with-spurious-minima",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11b: Dealing with spurious minima",
    "text": "Lecture 11b: Dealing with spurious minima\nThe last in-video question is not easy. Try to understand how the perceptron learning procedure is used in a Hopfield net; it’s not very thoroughly explained."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11c-hopfield-nets-with-hidden-units",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11c-hopfield-nets-with-hidden-units",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11c: Hopfield nets with hidden units",
    "text": "Lecture 11c: Hopfield nets with hidden units\nThis video introduces some sophisticated concepts, and is not entirely easy. An “excitatory connection” is a connection of which the weight is positive. “inhibitory”, likewise, means a negative weight. We look for an energy minimum, “given the state of the visible units”. That means that we look for a low energy configuration, and we’ll consider only configurations in which the visible units are in the state that’s specified by the data. So we’re only going to consider flipping the states of the hidden units. Be sure to really understand the last two sentences that Geoffrey speaks in this video."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11d-using-stochastic-units-to-improve-search",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11d-using-stochastic-units-to-improve-search",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11d: Using stochastic units to improve search",
    "text": "Lecture 11d: Using stochastic units to improve search\nWe’re still working with a mountain landscape analogy. This time, however, it’s not an analogy for parameter space, but for state space. A particle is, therefore, not a weight vector, but a configuration. What’s the same is that we’re, in a way, looking for low points in the landscape. We’re also using the physics analogy of systems that can be in different states, each with their own energy, and subject to a temperature. This analogy is introduced in slide 2. This is the analogy that originally inspired Hopfield networks. The idea is that at a high temperature, the system is more inclined to transition into configurations with high energy, even though it still prefers low energy. 3:25: “the amount of noise” means the extent to which the decisions are random. 4:20: If T really were 0, we’d have division by zero, which is not good. What we really mean here is “as T gets really, really small (but still positive)”. For mathematicians: it’s the limit as T goes to zero from above. Thermal equilibrium, and this whole random process of exploring states, is much like the exploration of weight vectors that we can use in Bayesian methods. It’s called a Markov Chain, in both cases."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11e-how-a-boltzmann-machine-models-data",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11e-how-a-boltzmann-machine-models-data",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11e: How a Boltzmann machine models data",
    "text": "Lecture 11e: How a Boltzmann machine models data\nNow, we’re making a generative model of binary vectors. In contrast, mixtures of Gaussians are a generative model of real-valued vectors. 4:38: Try to understand how a mixture of Gaussians is also a causal generative model. 4:58: A Boltzmann Machine is an energy-based generative model. 5:50: Notice how this is the same as the earlier definition of energy. What’s new is that it’s mentioning visible and hidden units separately, instead of treating all units the same way."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html",
    "href": "notes/dnn/dnn-05/l05b.html",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html#some-ways-to-achieve-viewpoint-invariance",
    "href": "notes/dnn/dnn-05/l05b.html#some-ways-to-achieve-viewpoint-invariance",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "Some ways to achieve viewpoint invariance",
    "text": "Some ways to achieve viewpoint invariance\n\nWe are so good at viewpoint invariance that it is hard to appreciate how difficult it is.\n\nIts one of the main difficulties in making computers perceive.\nWe still don’t have generally accepted solutions.\n\nThere are several different approaches:\n\nUse redundant invariant features.\nPut a box around the object and use normalized pixels.\n\nLecture 5c: Use replicated features with pooling. This is called “convolutional neural nets”\nUse a hierarchy of parts that have explicit poses relative to the camera (this will be described in detail later in the course)."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html#the-invariant-feature-approach",
    "href": "notes/dnn/dnn-05/l05b.html#the-invariant-feature-approach",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "The invariant feature approach",
    "text": "The invariant feature approach\n\nExtract a large, redundant set of features that are invariant under transformations\n\ne.g. pair of roughly parallel lines with a red dot between them.\nThis is what baby herring gulls use to know where to peck for food.\n\nWith enough invariant features, there is only one way to assemble them into an object.\n\nWe don’t need to represent the relationships between features directly because they are captured by other features.\n\nFor recognition, we must avoid forming features from parts of different objects."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html#the-judicious-normalization-approach",
    "href": "notes/dnn/dnn-05/l05b.html#the-judicious-normalization-approach",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "The judicious normalization approach",
    "text": "The judicious normalization approach\n\nPut a box around the object and use it as a coordinate frame for a set of normalized pixels.\n\nThis solves the dimension-hopping problem. If we choose the box correctly, the same part of an object always occurs on the same normalized pixels.\n\nThe box can provide invariance to many degrees of freedom: translation, rotation, scale, shear, stretch …\nBut choosing the box is difficult because of:\nSegmentation errors, occlusion, unusual orientations.\n\nWe need to recognize the shape to get the box right!"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html#the-brute-force-normalization-approach",
    "href": "notes/dnn/dnn-05/l05b.html#the-brute-force-normalization-approach",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "The brute force normalization approach",
    "text": "The brute force normalization approach\n\nWhen training the recognizer, use well-segmented, upright images to fit the correct box.\nAt test time try all possible boxes in a range of positions and scales.\n\nThis approach is widely used for detecting upright things like faces and house numbers in unsegmented images.\n\nIt is much more efficient if the recognizer can cope with some variation in position and scale so that we can use a coarse grid when trying all possible boxes."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html",
    "href": "notes/dnn/dnn-05/l05c.html",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-replicated-feature-approach-currently-the-dominant-approach-for-neural-networks",
    "href": "notes/dnn/dnn-05/l05c.html#the-replicated-feature-approach-currently-the-dominant-approach-for-neural-networks",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The replicated feature approach (currently the dominant approach for neural networks)",
    "text": "The replicated feature approach (currently the dominant approach for neural networks)\n\n\n\n\nUse many different copies of the same feature detector with different positions.\n\nCould also replicate across scale and orientation (tricky and expensive)\nReplication greatly reduces the number of free parameters to be learned.\n\nUse several different feature types, each with its own map of replicated detectors.\n\nAllows each patch of image to be represented in several ways."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#backpropagation-with-weight-constraints",
    "href": "notes/dnn/dnn-05/l05c.html#backpropagation-with-weight-constraints",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "Backpropagation with weight constraints",
    "text": "Backpropagation with weight constraints\n\nIt’s easy to modify the backpropagation algorithm to incorporate linear constraints between the weights.\n\nWe compute the gradients as usual, and then modify the gradients so that they satisfy the constraints.\n\nSo if the weights started off satisfying the constraints, they will continue to satisfy them."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#what-does-replicating-the-feature-detectors-achieve",
    "href": "notes/dnn/dnn-05/l05c.html#what-does-replicating-the-feature-detectors-achieve",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "What does replicating the feature detectors achieve?",
    "text": "What does replicating the feature detectors achieve?\n\n\n \n\nEquivariant activities: Replicated features do not make the neural activities invariant to translation. The activities are equivariant.\nInvariant knowledge: If a feature is useful in some locations during training, detectors for that feature will be available in all locations during testing."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#pooling-the-outputs-of-replicated-feature-detectors",
    "href": "notes/dnn/dnn-05/l05c.html#pooling-the-outputs-of-replicated-feature-detectors",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "Pooling the outputs of replicated feature detectors",
    "text": "Pooling the outputs of replicated feature detectors\n• Get a small amount of translational invariance at each level by averaging four neighboring replicated detectors to give a single output to the next level. – This reduces the number of inputs to the next layer of feature extraction, thus allowing us to have many more different feature maps. – Taking the maximum of the four works slightly better. • Problem: After several levels of pooling, we have lost information about the precise positions of things. – This makes it impossible to use the precise spatial relationships between high-level parts for recognition"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-architecture-of-lenet5",
    "href": "notes/dnn/dnn-05/l05c.html#the-architecture-of-lenet5",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The architecture of LeNet5",
    "text": "The architecture of LeNet5\n\n\n\nThe architecture of LeNet5"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-82-errors-made-by-lenet5",
    "href": "notes/dnn/dnn-05/l05c.html#the-82-errors-made-by-lenet5",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The 82 errors made by LeNet5",
    "text": "The 82 errors made by LeNet5\n\n\n\nerrors made by LeNet5\n\n\nNotice that most of the errors are cases that people find quite easy.\nThe human error rate is probably 20 to 30 errors but nobody has had the patience to measure it."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#priors-and-prejudice",
    "href": "notes/dnn/dnn-05/l05c.html#priors-and-prejudice",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "Priors and Prejudice",
    "text": "Priors and Prejudice\n\nWe can put our prior knowledge about the task into the network by designing appropriate:\n\nConnectivity.\nWeight constraints.\nNeuron activation functions\n\nThis is less intrusive than handdesigning the features.\n\nBut it still prejudices the network towards the particular way of solving the problem that we had in mind.\n\nAlternatively, we can use our prior knowledge to create a whole lot more training data.\n\nThis may require a lot of work (Hofman&Tresp, 1993)\nIt may make learning take much longer.\n\nIt allows optimization to discover clever ways of using the multi-layer network that we did not think of.\n\nAnd we may never fully understand how it does it."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-brute-force-approach",
    "href": "notes/dnn/dnn-05/l05c.html#the-brute-force-approach",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The brute force approach",
    "text": "The brute force approach\n\nLeNet uses knowledge about the invariances to design:\n\nthe local connectivity\nthe weight-sharing\nthe pooling.\n\nThis achieves about 80 errors.\n\nThis can be reduced to about 40 errors by using many different transformations of the input and other tricks (Ranzato 2008)\n\nCiresan et. al. (2010) inject knowledge of invariances by creating a huge amount of carefully designed extra training data:\n\nFor each training image, they produce many new training examples by applying many different transformations.\nThey can then train a large, deep, dumb net on a GPU without much overfitting.\n\nThey achieve about 35 errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-errors-made-by-the-ciresan-et.-al.-net",
    "href": "notes/dnn/dnn-05/l05c.html#the-errors-made-by-the-ciresan-et.-al.-net",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The errors made by the Ciresan et. al. net",
    "text": "The errors made by the Ciresan et. al. net\n\n\n\nerrors made by the Ciresan\n\n\nThe top printed digit is the right answer. The bottom two printed digits are the network’s best two guesses.\nThe right answer is almost always in the top 2 guesses.\nWith model averaging they can now get about 25 errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#how-to-detect-a-significant-drop-in-the-error-rate",
    "href": "notes/dnn/dnn-05/l05c.html#how-to-detect-a-significant-drop-in-the-error-rate",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "How to detect a significant drop in the error rate",
    "text": "How to detect a significant drop in the error rate\n\n\n\n\n\nMcNemar test 1\n\n\n\n\n\nMcNemar test 1\n\n\n\nIs 30 errors in 10,000 test cases significantly beHer than 40 errors?\n\nIt all depends on the particular errors!\nThe McNemar test uses the particular errors and can be much more powerful than a test that just uses the number of errors.\n\n\n\n\n\nThe architecture of LeNet5\nerrors made by LeNet5\nerrors made by the Ciresan\nMcNemar test 1\nMcNemar test 1"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html",
    "href": "notes/dnn/dnn-03/l03a.html",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Lecure video\n\n\n\nRecall: by convexity, the Perceptron convergence algorithm guarantees that each time the weights change, they get closer to every generously feasible set of weights. 😄\n\nThis guarantee cannot be extended to more complex networks which wights are non-convex, i.e. the average of two good solutions may be a bad solution. 1 😢\n\nmulti-layer neural networks cannot use the perceptron learning procedure.\n\nThey should never have been called multi-layer perceptrons. 2\n\n\n\n\n\n\nInstead of showing the weights get closer to a good set of weights, show that the actual output values get closer the target values.\n\nThis can be true even for non-convex problems in which there are many quite different sets of weights that work well and averaging two good sets of weights may give a bad set of weights.\nIt is not true for perceptron learning.\n\nThe simplest example is a linear neuron with a squared error measure.\n\n\n\n\n\nCalled linear filters in electrical engineering and linear transforms in linear algebra and can be represented by martracies\nWe don’t use linear neurons in practice:\n\nWithout a non-linearity in the unit, a stack of N layers can be replaced by a single layer 3\nThis lecture just demonstrates the analysis we will use with non-linear units.\n\nThe neuron’s output is the real valued weighted sum of its inputs\nThe goal of learning is to minimize the total error over all training cases.\n\nHere error is the squared difference between the desired output and the actual output.\n\n\n\n{\\color{green}{\\overbrace{y}^{\\text{output}}}} = \\sum_{n \\in train} {\\color{red}{\\overbrace{w_i}^{\\text{weights}}}} {\\color{blue}{\\underbrace{x_i}_{\\text{inputs}}}}= \\vec{w}^T\\cdot\\vec{x}\n where:\n\ny is the neuron’s estimate of the desired output\nx is the input vector\nw is the weight vector\n\n\n\n\n\nIt is straight-forward to write down a set of equations, one per training case, and to solve for the best set of weights.\nThis is the standard engineering approach so why don’t we use it?\nScientific answer: We want a method that real neurons could use.\nEngineering answer: We want a method that can be generalized to multi-layer, non-linear neural networks.\nThe analytic solution relies on it being linear and having a squared error measure.\nIterative methods are usually less efficient but they are much easier to generalize.\n\n\n\n\nEach day you get lunch at the cafeteria.\n\nYour diet consists of fish, chips, and ketchup.\nYou get several portions of each.\n\nThe cashier only tells you the total price of the meal\n\nAfter several days, you should be able to figure out the price of each portion.\n\nThe iterative approach: Start with random guesses for the prices and then adjust them to get a better fit to the observed prices of whole meals.\n\n\n\n\n\nEach meal price gives a linear constraint on the prices of the portions: \n\\text{price} = X_\\text{fish} W_\\text{fish} + X_\\text{chips} W_\\text{chips} + X_\\text{ketchup}W_\\text{ketchup}      \n\n\nThe prices of the portions are like the weights in of a linear neuron. \nW = (w_\\text{fish} , W_\\text{ chips} , W_\\text{ketchup} )\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\n\n\n\n\n\n\nthe true weights\n\n\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\n\n\n\n\n\na toy problem\n\n\n\nResidual error = 350\nThe “delta-rule” for learning is: \\Delta w_i = \\epsilon x_i (t - y)\nWith a learning rate \\epsilon of 1/35, the weight changes are:+20, +50, +30\nThis gives new weights of: 70, 100, 80.\nThe weight for chips got worse, but over all the weights are better\n\ny reducing errors, individual weight estimate may be getting worse\nCalculating the change in the weights:\ncalculate our output using forward propagation\n\n\n\n\ny = \\sum_{n \\in train} w_i x_i= \\vec{w}^T\\vec{x}\n Define the error as the squared residuals summed over all training cases:\n\nE = \\frac{1}{2}\\sum_{n \\in train} (t_n−y_n)^2\n\nuse the chain rule to get error derivatives for weights\n\n\\frac{d E}{\\partial w_i}=\\frac{1}{2}\\sum_{n \\in train}\\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n}=\\frac{1}{2}\\sum_{n \\in train}x_i^n(t^n−y^n)\n\nthe batch delta rule changes the weight in proportion to their error derivative summed on all training cases times the learning rate\n\n\\Delta w_i = −\\epsilon \\frac{d E}{\\partial w_i} = \\sum_{n \\in train} \\epsilon x_i^n (t^n−y^n)\n\n\n\n\n\n\nDoes the learning procedure eventually get the right answer?\n\nThere may be no perfect answer.\nBy making the learning rate small enough we can get as close as we desire to the best answer.\n\nHow quickly do the weights converge to their correct values?\n\nIt can be very slow if two input dimensions are highly correlated. If you almost always have the same number of portions of ketchup and chips, it is hard to decide how to divide the price between ketchup and chips\n\n\n\n\n\n\nIn perceptron learning, we increment or decrement the weight vector by the input vector.\n\nBut we only change the weights when we make an error.\n\nIn the online version of the delta-rule we increment or decrement the weight vector by the input vector scaled by the residual error and the learning rate.\n\nSo we have to choose a learning rate. This is annoying\n\n\n\nresidual error\n\nit’s the amount by which we got the answer wrong.\n\n\nA very central concept is introduced without being made very explicit: we use derivatives for learning, i.e. for making the weights better. Try to understand why those concepts are indeed very related.\n\non-line learning\n\nmeans that we change the weights after every training example that we see, and we typically cycle through the collection of available training examples.\n\n\n\n\n\nthe true weights\na toy problem"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#why-the-perceptron-learning-procedure-cannot-be-generalised-to-hidden-layers",
    "href": "notes/dnn/dnn-03/l03a.html#why-the-perceptron-learning-procedure-cannot-be-generalised-to-hidden-layers",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Recall: by convexity, the Perceptron convergence algorithm guarantees that each time the weights change, they get closer to every generously feasible set of weights. 😄\n\nThis guarantee cannot be extended to more complex networks which wights are non-convex, i.e. the average of two good solutions may be a bad solution. 1 😢\n\nmulti-layer neural networks cannot use the perceptron learning procedure.\n\nThey should never have been called multi-layer perceptrons. 2"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#a-different-way-to-show-that-a-learning-procedure-makes-progress",
    "href": "notes/dnn/dnn-03/l03a.html#a-different-way-to-show-that-a-learning-procedure-makes-progress",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Instead of showing the weights get closer to a good set of weights, show that the actual output values get closer the target values.\n\nThis can be true even for non-convex problems in which there are many quite different sets of weights that work well and averaging two good sets of weights may give a bad set of weights.\nIt is not true for perceptron learning.\n\nThe simplest example is a linear neuron with a squared error measure."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#linear-neurons",
    "href": "notes/dnn/dnn-03/l03a.html#linear-neurons",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Called linear filters in electrical engineering and linear transforms in linear algebra and can be represented by martracies\nWe don’t use linear neurons in practice:\n\nWithout a non-linearity in the unit, a stack of N layers can be replaced by a single layer 3\nThis lecture just demonstrates the analysis we will use with non-linear units.\n\nThe neuron’s output is the real valued weighted sum of its inputs\nThe goal of learning is to minimize the total error over all training cases.\n\nHere error is the squared difference between the desired output and the actual output.\n\n\n\n{\\color{green}{\\overbrace{y}^{\\text{output}}}} = \\sum_{n \\in train} {\\color{red}{\\overbrace{w_i}^{\\text{weights}}}} {\\color{blue}{\\underbrace{x_i}_{\\text{inputs}}}}= \\vec{w}^T\\cdot\\vec{x}\n where:\n\ny is the neuron’s estimate of the desired output\nx is the input vector\nw is the weight vector"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#why-dont-we-solve-it-analytically",
    "href": "notes/dnn/dnn-03/l03a.html#why-dont-we-solve-it-analytically",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "It is straight-forward to write down a set of equations, one per training case, and to solve for the best set of weights.\nThis is the standard engineering approach so why don’t we use it?\nScientific answer: We want a method that real neurons could use.\nEngineering answer: We want a method that can be generalized to multi-layer, non-linear neural networks.\nThe analytic solution relies on it being linear and having a squared error measure.\nIterative methods are usually less efficient but they are much easier to generalize.\n\n\n\n\nEach day you get lunch at the cafeteria.\n\nYour diet consists of fish, chips, and ketchup.\nYou get several portions of each.\n\nThe cashier only tells you the total price of the meal\n\nAfter several days, you should be able to figure out the price of each portion.\n\nThe iterative approach: Start with random guesses for the prices and then adjust them to get a better fit to the observed prices of whole meals.\n\n\n\n\n\nEach meal price gives a linear constraint on the prices of the portions: \n\\text{price} = X_\\text{fish} W_\\text{fish} + X_\\text{chips} W_\\text{chips} + X_\\text{ketchup}W_\\text{ketchup}      \n\n\nThe prices of the portions are like the weights in of a linear neuron. \nW = (w_\\text{fish} , W_\\text{ chips} , W_\\text{ketchup} )\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\n\n\n\n\n\n\nthe true weights\n\n\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\n\n\n\n\n\na toy problem\n\n\n\nResidual error = 350\nThe “delta-rule” for learning is: \\Delta w_i = \\epsilon x_i (t - y)\nWith a learning rate \\epsilon of 1/35, the weight changes are:+20, +50, +30\nThis gives new weights of: 70, 100, 80.\nThe weight for chips got worse, but over all the weights are better\n\ny reducing errors, individual weight estimate may be getting worse\nCalculating the change in the weights:\ncalculate our output using forward propagation\n\n\n\n\ny = \\sum_{n \\in train} w_i x_i= \\vec{w}^T\\vec{x}\n Define the error as the squared residuals summed over all training cases:\n\nE = \\frac{1}{2}\\sum_{n \\in train} (t_n−y_n)^2\n\nuse the chain rule to get error derivatives for weights\n\n\\frac{d E}{\\partial w_i}=\\frac{1}{2}\\sum_{n \\in train}\\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n}=\\frac{1}{2}\\sum_{n \\in train}x_i^n(t^n−y^n)\n\nthe batch delta rule changes the weight in proportion to their error derivative summed on all training cases times the learning rate\n\n\\Delta w_i = −\\epsilon \\frac{d E}{\\partial w_i} = \\sum_{n \\in train} \\epsilon x_i^n (t^n−y^n)"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#behaviour-of-the-iterative-learning-procedure",
    "href": "notes/dnn/dnn-03/l03a.html#behaviour-of-the-iterative-learning-procedure",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Does the learning procedure eventually get the right answer?\n\nThere may be no perfect answer.\nBy making the learning rate small enough we can get as close as we desire to the best answer.\n\nHow quickly do the weights converge to their correct values?\n\nIt can be very slow if two input dimensions are highly correlated. If you almost always have the same number of portions of ketchup and chips, it is hard to decide how to divide the price between ketchup and chips"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#the-relationship-between-the-online-delta-rule-and-the-learning-rule-for-perceptrons",
    "href": "notes/dnn/dnn-03/l03a.html#the-relationship-between-the-online-delta-rule-and-the-learning-rule-for-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "In perceptron learning, we increment or decrement the weight vector by the input vector.\n\nBut we only change the weights when we make an error.\n\nIn the online version of the delta-rule we increment or decrement the weight vector by the input vector scaled by the residual error and the learning rate.\n\nSo we have to choose a learning rate. This is annoying\n\n\n\nresidual error\n\nit’s the amount by which we got the answer wrong.\n\n\nA very central concept is introduced without being made very explicit: we use derivatives for learning, i.e. for making the weights better. Try to understand why those concepts are indeed very related.\n\non-line learning\n\nmeans that we change the weights after every training example that we see, and we typically cycle through the collection of available training examples.\n\n\n\n\n\nthe true weights\na toy problem"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#footnotes",
    "href": "notes/dnn/dnn-03/l03a.html#footnotes",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "Footnotes",
    "text": "Footnotes\n\n\na convex set includes all weighted sums↩︎\nno one calls them that anymore↩︎\nthink multiplying N-matracies just gives a single matrix ↩︎"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03b.html",
    "href": "notes/dnn/dnn-03/l03b.html",
    "title": "Deep Neural Networks - Notes for lecture 3b",
    "section": "",
    "text": "error surface of a linear neuron\n\n\n\n\nThe error surface lies in a space with a horizontal axis for each weight and one vertical axis for the error.\n\nFor a linear neuron with a squared error, it is a quadratic bowl.\nVertical cross-sections are parabolas.\nHorizontal cross-sections are ellipses.\n\nFor multi-layer, non-linear nets the error surface is much more complicated.\n\n\n\n\n\n\n\n\nOnline v.s. batch learning\n\n\n\n\n\n\n\n\n\n\nWhy learning can be slow\n\n\n\nWhen the ellipse is elongated, the direction of steepest descent is almost perpendicular to the direction towards the minimum!\nThe red gradient vector has a large component along the short axis of the ellipse and a small component along the long axis of the ellipse.\nThis is just the opposite of what we want.\n\n\n\n\nerror surface of a linear neuron\nOnline v.s. batch learning\nWhy learning can be slow"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03b.html#online-versus-batch-learning",
    "href": "notes/dnn/dnn-03/l03b.html#online-versus-batch-learning",
    "title": "Deep Neural Networks - Notes for lecture 3b",
    "section": "",
    "text": "Online v.s. batch learning"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03b.html#why-learning-can-be-slow",
    "href": "notes/dnn/dnn-03/l03b.html#why-learning-can-be-slow",
    "title": "Deep Neural Networks - Notes for lecture 3b",
    "section": "",
    "text": "Why learning can be slow\n\n\n\nWhen the ellipse is elongated, the direction of steepest descent is almost perpendicular to the direction towards the minimum!\nThe red gradient vector has a large component along the short axis of the ellipse and a small component along the long axis of the ellipse.\nThis is just the opposite of what we want.\n\n\n\n\nerror surface of a linear neuron\nOnline v.s. batch learning\nWhy learning can be slow"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03c.html",
    "href": "notes/dnn/dnn-03/l03c.html",
    "title": "Deep Neural Networks - Notes for lecture 3c",
    "section": "",
    "text": "Logistic neurons AKA linear filters - useful to understand the algorithm but in reality we need to use non linear activation function.\n\n\nThese give a real-valued output that is a smooth and bounded function of their total input. They have nice derivatives which make learning easy.\n\nz = b + \\sum _i x_i w_i\n\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\n\n\nlogistic activation function\n\n\n\n\n\n\nThe derivatives of the logit, z, with respect to the inputs and the weights are very simple:\n\nz = b + \\sum _i x_i w_i \\tag{the logit}\n\n\n\\frac{\\partial z}{\\partial w_i} = x_i \\;\\;\\;\\;\\; \\frac{\\partial z}{\\partial x_i} = w_i\n\nThe derivative of the output with respect to the logit is simple if you express it in terms of the output:\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\\frac{d y}{d z} = y( 1-y)\n\nsince\n\ny = \\frac{1}{1+e^{-z}}=(1+e^{-z})^{-1}\n differentiating  \\frac{d y}{d z} = \\frac{-1(-e^{-z})}{(1+e^{-z})^2} =\\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}  = y( 1-y)  Using the chain rule to get the derivatives needed for learning the weights of a logistic unit To learn the weights we need the derivative of the output with respect to each weight:\n\n\\frac{d y}{\\partial w_i}  =\\frac{\\partial z}{\\partial w_i} \\frac{dy}{dz}  = x_iy( 1-y)\n\n\n\\frac{d E}{\\partial w_i}  = \\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n} = - \\sum {\\color{green}{x_i^n}}{\\color{red}{ y^n( 1-y^n)}}{\\color{green}{(t^n-y^n)}}\n\nwhere the green part corresponds to the delta rule and the extra term in red is simply the slope of the logistic.\nThe error function is still:\n\nE =\\frac{1}{2}(y−t)^2\n\nNotice how after Hinton explained what the derivative is for a logistic unit, he considers the job to be done. That’s because the learning rule is always simply some learning rate multiplied by the derivative.\n\n\n\nlogistic activation function"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03c.html#logistic-neurons",
    "href": "notes/dnn/dnn-03/l03c.html#logistic-neurons",
    "title": "Deep Neural Networks - Notes for lecture 3c",
    "section": "",
    "text": "These give a real-valued output that is a smooth and bounded function of their total input. They have nice derivatives which make learning easy.\n\nz = b + \\sum _i x_i w_i\n\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\n\n\nlogistic activation function"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03c.html#the-derivatives-of-a-logistic-neuron",
    "href": "notes/dnn/dnn-03/l03c.html#the-derivatives-of-a-logistic-neuron",
    "title": "Deep Neural Networks - Notes for lecture 3c",
    "section": "",
    "text": "The derivatives of the logit, z, with respect to the inputs and the weights are very simple:\n\nz = b + \\sum _i x_i w_i \\tag{the logit}\n\n\n\\frac{\\partial z}{\\partial w_i} = x_i \\;\\;\\;\\;\\; \\frac{\\partial z}{\\partial x_i} = w_i\n\nThe derivative of the output with respect to the logit is simple if you express it in terms of the output:\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\\frac{d y}{d z} = y( 1-y)\n\nsince\n\ny = \\frac{1}{1+e^{-z}}=(1+e^{-z})^{-1}\n differentiating  \\frac{d y}{d z} = \\frac{-1(-e^{-z})}{(1+e^{-z})^2} =\\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}  = y( 1-y)  Using the chain rule to get the derivatives needed for learning the weights of a logistic unit To learn the weights we need the derivative of the output with respect to each weight:\n\n\\frac{d y}{\\partial w_i}  =\\frac{\\partial z}{\\partial w_i} \\frac{dy}{dz}  = x_iy( 1-y)\n\n\n\\frac{d E}{\\partial w_i}  = \\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n} = - \\sum {\\color{green}{x_i^n}}{\\color{red}{ y^n( 1-y^n)}}{\\color{green}{(t^n-y^n)}}\n\nwhere the green part corresponds to the delta rule and the extra term in red is simply the slope of the logistic.\nThe error function is still:\n\nE =\\frac{1}{2}(y−t)^2\n\nNotice how after Hinton explained what the derivative is for a logistic unit, he considers the job to be done. That’s because the learning rule is always simply some learning rate multiplied by the derivative.\n\n\n\nlogistic activation function"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html",
    "href": "notes/dnn/dnn-01/l01a.html",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nThese is the first installment of notes to the course “Deep Neural Networks” by Geoffrey Hinton I took on Coursera\nThis was one of the first course online on the subject.\nHinton was one of the leading researchers on deep learning, his students are some of the most important reaserchers today. He introduced some algorithms and methods that were not published.\nThis course is dated, the SOTA results have improved, it does not cover transformers and probably all the results have been beaten as this is a fast moving field.\nStill this is an interesting, if mathematically sophisticated introduction to deep learning."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#what-is-ml",
    "href": "notes/dnn/dnn-01/l01a.html#what-is-ml",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "What is ML?",
    "text": "What is ML?\n\nIt is very hard 😱 to write programs that solve problems like recognizing a 3d object from a novel viewpoint in new lighting conditions in a cluttered scene.\n\nWe don’t know what program to write because we don’t know 😕 how its done in our brain. 🧠\nEven if we had an clue 💡 how to do it — the program would be 😱 horrendously complicated.\n\nIt is hard to write a program to compute the probability that a credit card transaction is fraudulent.\n\nThere may not be any rules that are both simple and reliable. We need to combine a very large number of weak rules.\nFraud is a moving target. The program needs to keep changing."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#the-ml-approach",
    "href": "notes/dnn/dnn-01/l01a.html#the-ml-approach",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "The ML approach",
    "text": "The ML approach\n\nInstead of writing a program by hand for each specific task, we collect lots of examples that specify the correct output for a given input.\nA ML algorithm 🤖 then takes these examples and produces a program that does the job.\n\n]The program produced by the ML algorithm 🤖] may look very different from a typical hand-written program]{.mark}. It may contain millions of numbers 🙀 …\nIf we do it right, the program works for new cases as well as the ones we trained it on.\nIf the data changes the program can change too by training on the new data.\n\nMassive amounts of computation are now cheaper than paying someone to write a task-specific program."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#some-examples-of-tasks-best-solved-by-ml",
    "href": "notes/dnn/dnn-01/l01a.html#some-examples-of-tasks-best-solved-by-ml",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "Some examples of tasks best solved by ML",
    "text": "Some examples of tasks best solved by ML\n\nRecognizing patterns:\n\nObjects in real scenes\nFacial identities or facial expressions\nSpoken words\n\nRecognizing anomalies:\n\nUnusual sequences of credit card transactions\nUnusual patterns of sensor readings in a nuclear power plant1\n\nPrediction:\n\nFuture stock prices or currency exchange rates2.\nWhich movies will a person like3?"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#the-standard-example-of-ml",
    "href": "notes/dnn/dnn-01/l01a.html#the-standard-example-of-ml",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "The standard example of ML",
    "text": "The standard example of ML\n\n\n\n\n\nDrosophila melanogaster Proboscis\n\n\n\nA lot of genetics is done on fruit flies.\n\nThey are convenient because they breed fast.\nWe already know a lot about them.\n\nThe MNIST database of hand-written digits is the the ML equivalent of fruit flies. 🦋\n\nThey are publicly available and we can learn them quite fast in a moderate-sized neural net.\nWe know a huge amount about how well various ML methods do on MNIST.\n\nWe will use MNIST as our standard task."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#beyond-mnist-the-imagenet-task",
    "href": "notes/dnn/dnn-01/l01a.html#beyond-mnist-the-imagenet-task",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "Beyond MNIST — The ImageNet task",
    "text": "Beyond MNIST — The ImageNet task\n\n1000 different object classes in 1.3 million high-resolution training images from the web.\n\nBest system in 2010 competition got 47% error for its first choice and 25% error for its top 5 choices.\n\nJitendra Malik, an eminent neural net sceptic 💀, said that this competition is a good test of whether DNNs 🧠 work well for object recognition.\nA very deep neural net (Krizhevsky, Sutskever, and Hinton 2012) gets less that 40% error for its first choice and less than 20% for its top 5 choices.\n\n\nThe Speech recognition task\n\nA speech recognition system has several stages:\n\nPre-processing: Convert the sound wave into a vector of acoustic coefficients. Extract a new vector about every 10 mille seconds.\nThe acoustic model: Use a few adjacent vectors of acoustic coefficients to place bets on which part of which phoneme is being spoken.\nDecoding: Find the sequence of bets that does the best job of fitting the acoustic data and also fitting a model of the kinds of things people say.\n\nDeep neural networks pioneered by George Dahl and Abdel-rahman Mohamed are now replacing the previous ML method for the acoustic model."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#phone-recognition-on-the-timit-benchmark",
    "href": "notes/dnn/dnn-01/l01a.html#phone-recognition-on-the-timit-benchmark",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "Phone recognition on the TIMIT benchmark",
    "text": "Phone recognition on the TIMIT benchmark\n He discusses work from from Mohamed, Dahl, and Hinton (2012)\n\nAfter standard post-processing using a bi-phone model, a deep net with 8 layers gets 20.7% error rate.\nThe best previous speaker independent result on TIMIT 4 was 24.4% and this required averaging several models.5\nNLP researcher Li Deng at Microsoft Research realized that this result could change the way speech recognition was done.\n\n\n\n\nDrosophila melanogaster Proboscis"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#footnotes",
    "href": "notes/dnn/dnn-01/l01a.html#footnotes",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nuse a program no one can interprest to control a nuclear reactor 😒↩︎\n😕 if there be predicted at all↩︎\na recommendation system is probably best for this 😎↩︎\nis a corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. Each transcribed element has been delineated in time. TIMIT was designed to further acoustic-phonetic knowledge and automatic speech recognition systems↩︎\nthis is a massive jump in SOTA performance↩︎"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01_.html",
    "href": "notes/dnn/dnn-01/l01_.html",
    "title": "Deep Neural Networks - Notes for Lesson 1",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nThese is the first installment of notes to the course “Deep Neural Networks” by Geoffrey Hinton I took on Coursera\nThis was one of the first course online on the subject.\nHinton was one of the leading researchers on deep learning, his students are some of the most important reaserchers today. He introduced some algorithms and methods that were not published.\nThis course is now outdated - it does not cover transformers and probably all the results have been beaten as this is a fast moving field.\nStill this is an interesting, if mathematicaly sophisticated introduction to deep learning.\n\n\n\n\nReuseCC SA BY-NC-NDCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} for {Lesson} 1},\n  date = {2017-07-01},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-01/l01_.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes for Lesson\n1.” July 1, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-01/l01_.html."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html",
    "href": "notes/dnn/dnn-01/l01c.html",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#lecture-1c-some-simple-models-of-neurons",
    "href": "notes/dnn/dnn-01/l01c.html#lecture-1c-some-simple-models-of-neurons",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Lecture 1c: Some simple models of neurons",
    "text": "Lecture 1c: Some simple models of neurons\n\n\nIdealized neurons\n\nTo model things we have to idealize them (e.g. atoms)\n\nIdealization removes complicated details that are not essential for understanding the main principles.\nIt allows us to apply mathematics and to make analogies to other, familiar systems.\nOnce we understand the basic principles, its easy to add complexity to make the model more faithful.\n\nIt is often worth understanding models that are known to be wrong (but we must not forget that they are wrong!)\n\nE.g. neurons that communicate real values rather than discrete spikes of activity.\n\n\n\n\nLinear neurons\n\n\n\n\nlinear activation function\n\n\n\n\nThese are simple but computationally limited\n\nIf we can make them learn we may get insight into more complicated neurons.\n\n\n\ny=b+\\sum_i{ x_i \\times w_i}\n\nwhere:\n\ny is the output\n\nb is the bias\ni is the index over input connectinos\n\nx_i is the ith input\nw_i is the weight on ith input\n\nBias is often conveniently chosen to be 0 which is odd considering that it is the constraint on the activation. This is handled formally by a technique called batch normalization\nThese are simple but computationally limited.\n\nIf we can make them learn we may get insight into more complicated neurons.\n\n\ny=b+\\sum_i{ x_i \\times w_i}"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#binary-threshold-units",
    "href": "notes/dnn/dnn-01/l01c.html#binary-threshold-units",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Binary threshold units",
    "text": "Binary threshold units\n Binary threshold units are due to Warren McCulloch and Walter Pitts from their McCulloch and Pitts (1943). They were in turn influenced by earlier work by John Von Neumann the father of modern computer and game theory.\n\n\n\n\n\n\n\n\nWarren Sturgis Mcculloch\nWalter Pitts\nJohnvon Neumann\n\n\n\n\n\n\n\n\n\n\n\nFirst compute a weighted sum of the inputs.\nThen send out a fixed size spike of activity if the weighted sum exceeds a threshold.\nMcCulloch and Pitts thought that each spike is like the truth value of a proposition and each neuron combines truth values to compute the truth value of another proposition!\n\nThere are two ways to write these mathematicaly:\n\nz = \\sum_i{ x_i w_i}\\\\\n\\theta = -b \\\\\ny = \\left\\{\n   \\begin{array}{ll}\n       1 & \\text{if} \\space z \\ge \\theta \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n    \\right.\n\nusing bias\n\nz = b+ \\sum_i{ x_i w_i}\\\\\ny = \\left\\{\n   \\begin{array}{ll}\n       1 & \\text{if} \\space z \\ge 0 \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n    \\right."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#relu---rectified-linear-neurons-aka-linear-threshold-neurons",
    "href": "notes/dnn/dnn-01/l01c.html#relu---rectified-linear-neurons-aka-linear-threshold-neurons",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "RELU - REctified Linear Neurons AKA Linear Threshold neurons",
    "text": "RELU - REctified Linear Neurons AKA Linear Threshold neurons\n\n\n\nRELU activation function\n\n\n\nThey compute a linear weighted sum of their inputs.\nThe output is a non-linear function of the total input.\n\n\nz = b + \\sum _i x_iw_i \\\\\n\n\ny = \\left\\{\n   \\begin{array}{ll}\n       z & \\text{if} \\space z \\gt 0 \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n   \\right."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#sigmoid-neurons",
    "href": "notes/dnn/dnn-01/l01c.html#sigmoid-neurons",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Sigmoid neurons",
    "text": "Sigmoid neurons\n\n\n\nSigmoid activation function\n\n\n\nThese give a real-valued output that is a smooth and bounded function of their total input.\nTypically they use the logistic function\nHave nice derivatives which make learning easy.\n\n\nz = b + \\sum _i x_iw_i \\\\\n\\space\\\\\ny = \\frac{1}{1+e^{-z}}"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#stochastic-binary-neurons",
    "href": "notes/dnn/dnn-01/l01c.html#stochastic-binary-neurons",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Stochastic binary neurons",
    "text": "Stochastic binary neurons\n\n\n\nbinary activation function\n\n\nThese use the same equations as logistic units. - But they treat the output of the logistic as the probability of producing a spike in a short time window.\nWe can do a similar trick for rectified linear units:\n\nThe output is treated as the Poisson rate for spikes.\n\n\nz = b + \\sum _i x_iw_i \\\\\n\\space\\\\\np(s=1) = \\frac{1}{1+e^{-z}}"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#choosing-an-activation-function",
    "href": "notes/dnn/dnn-01/l01c.html#choosing-an-activation-function",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Choosing an activation function",
    "text": "Choosing an activation function\nFirst let us note that many other activation function exist, this table list the following:\n\n\n\nactivation functions\n\n\nAt this point in the course we do not go into how one should pick a preferred activation function for the given problem. Some ideas for this are mentioned during the course. If we look at this from an engineering perspective some units tend to work well with other units and there are some other constraints like the range of inputs.\n\nLinear units\nTheir main benefit is that they help us write down the mathematically familiar linear model which is great for getting a basic insight into the problem. We can analyze this model in term of linear and or abstract algebra using concepts like spaces, subspace, solutions, eigenvectors, eigenvalues and so on. Unfortunately linear units they are not expressive enough to perform as a basis of an efficient universal approximator. A linear model is equivalent to a large logistic regression as each variable will effect all other variables. So once we developed some intuition about our linear model we would want to switch to a non-liner units and make use of the full power of neural networks.\n\n\nBinary threshold units\nTheir main benefit seem to be for modeling logical gates or logical circuits. Cons: have only zero and infinite gradients so are unsuitable for use in networks that are trained using gradient descent. They are used however in Hopfield networks. We will also consider later using a fully baysian approch to neural networks where we don’t need stochastic gradient descent - instead using MCMC search. It would seem that is such a settings using binary threshold units would dramatically decrease the search space.\n\n\nRELU\nThis is the simplest non linear units - using it is essentially introducing constraints in the form of inequalities. It should only be used in a hidden layer. A classification will need to add a Softmax and a regression a linear function. RELUs can die - so a Leaky RELU can be a better choice. \n\n\nSigmoid\nThis is continuous and has a gradient between 0 and 1 - pros: Sigmoid with weight initialized to zero behave like a linear system. As the weights increase towards they networks. - cons: saturate and kill gradients also when output is not centered about 0 then gradients tend to go to far to 0 or 1. They converge slowly.\n\n\nTANH\n\npros: very high values are similar (~1) and very low values are also similar (~1)\ncons: sub optimal for a deep network, as gradient diminish in the deeper parts of the model.\n\nRMSProp will compensate for that, but still changing to RELU will improve convergence speed c.f. (user8272359 2017). It is better then Sigmoid as it avoids the exploding gradient problem\n\n\n\nlinear activation function\nRELU activation function\nSigmoid activation function\nbinary activation function\nactivation functions"
  },
  {
    "objectID": "notes/dnn/dnn-15/l_15.html",
    "href": "notes/dnn/dnn-15/l_15.html",
    "title": "Deep Neural Networks - Notes for Lesson 15",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-15/l_15.html#lecture-15e-learning-binary-codes-for-image-retrieval",
    "href": "notes/dnn/dnn-15/l_15.html#lecture-15e-learning-binary-codes-for-image-retrieval",
    "title": "Deep Neural Networks - Notes for Lesson 15",
    "section": "Lecture 15e: Learning binary codes for image retrieval",
    "text": "Lecture 15e: Learning binary codes for image retrieval\nIt is essential that you understand video 15d before you try 15e. 7:13. Don’t worry if you don’t understand that last comment."
  },
  {
    "objectID": "notes/dnn/dnn-15/l_15.html#lecture-15f-shallow-autoencoders-for-pre-training",
    "href": "notes/dnn/dnn-15/l_15.html#lecture-15f-shallow-autoencoders-for-pre-training",
    "title": "Deep Neural Networks - Notes for Lesson 15",
    "section": "Lecture 15f: Shallow autoencoders for pre-training",
    "text": "Lecture 15f: Shallow autoencoders for pre-training\nThis video is quite separate from the others of chapter 15.\nCNN Architecture & hyper parameters\nConvolutional Neural Network example INPUT [F,F,3]\nCONV [F,F,K] - basis sensor RELU [F,F,K ] - elementwise activation POOL [F/2,F/2,S] - down sampling\nFC - convers volume to class probability Hyper parameters: K – depth is the number of filters/kernels to use say 12 F - the RECEPTIVE FIELD or spatial extent of the filters – pixels width and height a neuron sees say 32x32 S – the STRIDE = step size for the offset used for sliding the filters so that there is an overlap neurons – say 1 P the amount of PADDING= padding round input with zeros, used because output and input might otherwise have different sizes\nAs of 2015 per STRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET the recommendation is to Removing\nPooling Removing normalization also recommended\nINPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]M -&gt; [FC -&gt; RELU]K -&gt; FC\nSeems FC and CONV are functionally equivalent and can be interchanged. Some other techniques/layers types: 1x1 convolution Dilated convolutions (acting on spaced out pixels) Replacing Max Pooling with ROI region of interrest pooling Loss layer – represent the overall error Dropout layer - Regularization by droping a unit with probabpility p DropConnect - Regularization by dropping connections instead of units\nStochastic pooling\nWeight decay = 0.001 Image whitening and contrast normalization in preprocessing"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html",
    "href": "notes/dnn/dnn-02/l02b.html",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nThe lecture starts with the history of Perceptrons (Wikipedia contributors 2024)\nThen covers with The perceptron convergence procedure. Next is a deeper dive into the computational geometry of Perceptrons\nI also added a python implementation from scratch."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#the-standard-paradigm-for-statistical-pattern-recognition",
    "href": "notes/dnn/dnn-02/l02b.html#the-standard-paradigm-for-statistical-pattern-recognition",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "The standard paradigm for statistical pattern recognition",
    "text": "The standard paradigm for statistical pattern recognition\n\n\n\nThe standard Perceptron architecture\n\n\n\nConvert the raw input vector into a vector of feature activations. Use hand-written programs based on common-sense to define the features.\nLearn how to weight each of the feature activations to get a single scalar quantity.\nIf this quantity is above some threshold, decide that the input vector is a positive example of the target class."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#the-history-of-perceptrons",
    "href": "notes/dnn/dnn-02/l02b.html#the-history-of-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "The history of Perceptrons",
    "text": "The history of Perceptrons\n\nPerceptrons were introduced in (Rosenblatt 1962) by Frank Rosenblatt who popularized them\n\nThey appeared to have a very powerful learning algorithm.\nLots of grand claims were made for what they could learn to do.\n\nIn (Minsky and Papert 1969) the authors, analysed what Perceptrons could do and showed their limitations. 1\n\nMany people thought these limitations applied to all neural network models.\n\nThe perceptron learning procedure is still widely used today for tasks with enormous feature vectors that contain many millions of features.\n\n\n\n\n\n\nA perceptron\n\n\n\nwhy the bias can be implemented as a special input unit?\nbiases can be treated using weights using an input that is always one.\na threshold is equivalent to having a negative bias.\nwe can avoid having to figure out a separate learning rule for the bias by using a trick:\nA bias is exactly equivalent to a weight on an extra input line that always has an activation of 1.\n\n\nBinary threshold neurons (decision units)\n\n\n\nBinary Theshold Unit\n\n\n\nIntroduced in (Mcculloch and Pitts 1943)\n\nFirst compute a weighted sum of the inputs from other neurons (plus a bias).\nThen output a 1 if the weighted sum exceeds zero.\n\n\n\nz = b+ \\sum_i{ x_i w_i}\n \ny = \\left\\{\n   \\begin{array}{ll}\n       1 & \\text{if} \\space z \\ge 0 \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n    \\right."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#how-to-learn-biases-using-the-same-rule-as-we-use-for-learning-weights",
    "href": "notes/dnn/dnn-02/l02b.html#how-to-learn-biases-using-the-same-rule-as-we-use-for-learning-weights",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "How to learn biases using the same rule as we use for learning weights",
    "text": "How to learn biases using the same rule as we use for learning weights\n\nA threshold is equivalent to having a negative bias.\nWe can avoid having to figure out a separate learning rule for the bias by using a trick:\n\nA bias is exactly equivalent to a weight on an extra input line that always has an activity of 1.\nWe can now learn a bias as if it were a weight."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#the-perceptron-convergence-procedure-training-binary-output-neurons-as-classifiers",
    "href": "notes/dnn/dnn-02/l02b.html#the-perceptron-convergence-procedure-training-binary-output-neurons-as-classifiers",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "The Perceptron convergence procedure: Training binary output neurons as classifiers",
    "text": "The Perceptron convergence procedure: Training binary output neurons as classifiers\n\nAdd an extra component with value 1 to each input vector. The bias weight on this component is minus the threshold. Now we can forget the threshold.\nPick training cases using any policy that ensures that every training case will keep getting picked.\n\nIf the output unit is correct, leave its weights alone.\nIf the output unit incorrectly outputs a zero, add the input vector to the weight vector.\nIf the output unit incorrectly outputs a 1, subtract the input vector from the weight vector.\n\nThis is guaranteed to find a set of weights that gets the right answer for all the training cases if any such set exists."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#a-full-implementation-of-a-perceptrons",
    "href": "notes/dnn/dnn-02/l02b.html#a-full-implementation-of-a-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "A full implementation of a perceptrons:",
    "text": "A full implementation of a perceptrons:\ncode and image from: Implementing the Perceptron Algorithm in Python\n\n\n\nPerceptron\n\n\n\nfrom sklearn import datasets\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nX, y = datasets.make_blobs(n_samples=150,n_features=2,\n                           centers=2,cluster_std=1.05,\n                           random_state=2)\n#Plotting\nfig = plt.figure(figsize=(10,8))\nplt.plot(X[:, 0][y == 0], X[:, 1][y == 0], 'r^')\nplt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'bs')\nplt.xlabel(\"feature 1\")\nplt.ylabel(\"feature 2\")\nplt.title('Random Classification Data with 2 classes')\n\ndef step_func(z):\n        return 1.0 if (z &gt; 0) else 0.0\n      \ndef perceptron(X, y, lr, epochs):\n    '''\n    X: inputs\n    y: labels\n    lr: learning rate\n    epochs: Number of iterations\n    m: number of training examples\n    n: number of features \n    '''\n    m, n = X.shape    \n    # Initializing parapeters(theta) to zeros.\n    # +1 in n+1 for the bias term.\n    theta = np.zeros((n+1,1))\n    \n    # list with misclassification count per iteration.\n    n_miss_list = []\n    \n    # Training.\n    for epoch in range(epochs):\n        # variable to store misclassified.\n        n_miss = 0\n        # looping for every example.\n        for idx, x_i in enumerate(X):\n            # Inserting 1 for bias, X0 = 1.\n            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          \n            # Calculating prediction/hypothesis.\n            y_hat = step_func(np.dot(x_i.T, theta))\n            # Updating if the example is misclassified.\n            if (np.squeeze(y_hat) - y[idx]) != 0:\n                theta += lr*((y[idx] - y_hat)*x_i)\n                # Incrementing by 1.\n                n_miss += 1\n        # Appending number of misclassified examples\n        # at every iteration.\n        n_miss_list.append(n_miss)\n    return theta, n_miss_list\n\n\n\n\n\n\n\n\n\ndef plot_decision_boundary(X, theta):\n    \n    # X --&gt; Inputs\n    # theta --&gt; parameters\n    \n    # The Line is y=mx+c\n    # So, Equate mx+c = theta0.X0 + theta1.X1 + theta2.X2\n    # Solving we find m and c\n    x1 = [min(X[:,0]), max(X[:,0])]\n    m = -theta[1]/theta[2]\n    c = -theta[0]/theta[2]\n    x2 = m*x1 + c\n    \n    # Plotting\n    fig = plt.figure(figsize=(10,8))\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"r^\")\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n    plt.xlabel(\"feature 1\")\n    plt.ylabel(\"feature 2\")\n    plt.title('Perceptron Algorithm')\n    plt.plot(x1, x2, 'y-')\n\n\ntheta, miss_l = perceptron(X, y, 0.5, 100)\nplot_decision_boundary(X, theta)\n\n\n\n\n\n\n\n\n\n\n\nThe standard Perceptron architecture\nA perceptron\nBinary Theshold Unit\nPerceptron"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#footnotes",
    "href": "notes/dnn/dnn-02/l02b.html#footnotes",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe main results are available here↩︎"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02d.html",
    "href": "notes/dnn/dnn-02/l02d.html",
    "title": "Deep Neural Networks - Notes for lecture 2d",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nIn this lecture we try to explain intuitively why the perceptron algorithm works\nAlso we consider why it may fail."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02d.html#why-the-learning-procedure-works-first-attempt",
    "href": "notes/dnn/dnn-02/l02d.html#why-the-learning-procedure-works-first-attempt",
    "title": "Deep Neural Networks - Notes for lecture 2d",
    "section": "Why the learning procedure works (first attempt)",
    "text": "Why the learning procedure works (first attempt)\n\n\n\n\n\n\n\nidea of the proof\n\n\nwe want to get closer all the feasible solution\n\nConsider the squared distance d_a^2+d_b^2 between any feasible weight vector and the current weight vector. – Hopeful claim: Every time the perceptron makes a mistake, the learning algorithm moves the current weight vector closer to all feasible weight vectors\nWe look at the geometrical interpretation which is the proof for the convergence of the Perceptron learning algorithm works. We are trying to find a decision surface by solving a convex optimization problem.\nThe surface is a hyper-plane represented by a line where on side is the correct set and the other is incorrect. The weight vectors form a cone:\n\nThis means that wights are closed under addition and positive scaler product.\nAt zero it is zero.\n\n\n\n\n\n\nfixing it up\n\n\nwe now use a generously feasible sub-cone of the feasible cone shown in a dotted line\n\nSo consider “generously feasible” weight vectors that lie within the feasible region by a margin at least as great as the length of the input vector that defines each constraint plane.\n\nEvery time the perceptron makes a mistake, the squared distance to all of these generously feasible weight vectors is always decreased by at least the squared length of the update vector."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02d.html#informal-sketch-of-proof-of-convergence",
    "href": "notes/dnn/dnn-02/l02d.html#informal-sketch-of-proof-of-convergence",
    "title": "Deep Neural Networks - Notes for lecture 2d",
    "section": "Informal sketch of proof of convergence",
    "text": "Informal sketch of proof of convergence\n\nEach time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the “generously feasible” region.\nThe squared distance decreases by at least the squared length of the input vector.\nSo after a finite number of mistakes, the weight vector must lie in the feasible region if this region exists.\n\n\n\n\nidea of the proof\nfixing it up"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html",
    "href": "notes/dnn/dnn-02/l02a.html",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nIn this lecture we covered the main types of networks studied in the course"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html#feed-forward-neural-networks",
    "href": "notes/dnn/dnn-02/l02a.html#feed-forward-neural-networks",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "Feed-forward neural networks",
    "text": "Feed-forward neural networks\n\n\n\nFeed-forward neural networks\n\nFeed forward networks are the subject of the first half of the course.\nThese are the most common type of neural network.\nThe first layer is the input and\nThe last layer is the output.\n\nIf there is more than one hidden layer, we call them “deep” neural networks.\n\nThey compute a series of transformations that change the similarities between cases.\n\nThe activities of the neurons in each layer are a non-linear function of the activities in the layer below."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html#recurrent-networks",
    "href": "notes/dnn/dnn-02/l02a.html#recurrent-networks",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "Recurrent networks",
    "text": "Recurrent networks\n\n\n\n\n\nRecurrent neural networks\n\n\n\nThese have directed cycles in their connection graph.\n\nThat means you can sometimes get back to where you started by following the arrows.\n\nThey can have complicated dynamics and this can make them very difficult to train. – There is a lot of interest at present in finding efficient ways of training recurrent nets.\nThey are more biologically realistic.\n\n\nRecurrent neural networks for modeling sequences\n\n\n\n\n\nsequence to Sequence mapping\n\n\n\nRecurrent neural networks are a very natural way to model sequential data:\n\nThey are equivalent to very deep nets with one hidden layer per time slice.\nExcept that they use the same weights at every time slice and they get input at every time slice.\n\nThey have the ability to remember information in their hidden state for a long time.\n\nBut its very hard to train them to use this potential\n\n\n\n\nAn example of what RNNs can now do\n\nIn (Sutskever, Martens, and Hinton 2011) the authors trained a special type of RNN to predict the next character in a sequence.\nAfter training for a long time on a string of half a billion characters from English Wikipedia, he got it to generate new text.\n\nIt generates by predicting the probability distribution for the next character and then sampling a character from this distribution.\nThe next slide shows an example of the kind of text it generates. Notice how much it knows!\n\n\n\n\nSample text generated one character at a time by Ilya Sutskever’s RNN\n\nIn 1974 Northern Denver had been overshadowed by CNL, and several Irish intelligence agencies in the Mediterranean region. However, on the Victoria, Kings Hebrew stated that Charles decided to escape during an alliance. The mansion house was completed in 1882, the second in its bridge are omitted, while closing is the proton reticulum composed below it aims, such that it is the blurring of appearing on any well-paid type of box printer."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html#symmetrically-connected-networks",
    "href": "notes/dnn/dnn-02/l02a.html#symmetrically-connected-networks",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "Symmetrically connected networks",
    "text": "Symmetrically connected networks\n\nThese are like recurrent networks, but the connections between units are symmetrical (they have the same weight in both directions).\n\nJohn Hopfield (and others) realized that symmetric networks are much easier to analyze than recurrent networks. – They are also more restricted in what they can do. because they obey an energy function.\n\nFor example, they cannot model cycles.\n\n\nIn (Hopfield 1982), the author introduced symmetrically connected nets without hidden units that are now called Hopfield networks."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html#symmetrically-connected-networks-with-hidden-units",
    "href": "notes/dnn/dnn-02/l02a.html#symmetrically-connected-networks-with-hidden-units",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "Symmetrically connected networks with hidden units",
    "text": "Symmetrically connected networks with hidden units\n\nCalled Boltzmann machines.\n\nThey are much more powerful models than Hopfield nets.\nThey are less powerful than recurrent neural networks.\nThey have a beautifully simple learning algorithm.\n\nWe will cover Boltzmann machines towards the end of the course.\n\n\nSummary of Networks Architectures\n\n\n\n\n\n\n\nSchematic\nDescription\n\n\n\n\n\nFeed forward nets - regression and classification for images and tabular data.\n\n\n\nRecurrent nets - sequence to sequence\n\n\n\nHopfield nets - associative memory using symmetric nets with no hidden units\n\n\n\nBoltzmann machines - symmetric nets with hidden units\n\n\n\ncredit: images from The Neural Network Zoo\n\n\n\nRecurrent neural networks\nsequence to Sequence mapping"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html",
    "href": "notes/dnn/dnn-13/l_13.html",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#a-brief-history-of-backpropagation",
    "href": "notes/dnn/dnn-13/l_13.html#a-brief-history-of-backpropagation",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "A brief history of backpropagation",
    "text": "A brief history of backpropagation\n\nThe backpropagation algorithm for learning multiple layers of features was invented several times in the 70’s and 80’s:\n\nBryson & Ho (1969) linear\nWerbos (1974)\nRumelhart et. al. in 1981\nParker (1985)\nLeCun (1985)\nRumelhart et. al. (1985)\n\nBackpropagation clearly had great promise for learning multiple layers of non-linear feature detectors.\nBut by the late 1990’s most serious researchers in machine learning had given up on it.\n\nIt was still widely used in psychological models and in practical applications such as credit card fraud detection."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#why-backpropagation-failed",
    "href": "notes/dnn/dnn-13/l_13.html#why-backpropagation-failed",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Why backpropagation failed",
    "text": "Why backpropagation failed\n\nThe popular explanation of why backpropagation failed in the 90’s:\n\nIt could not make good use of multiple hidden layers. (except in convolutional nets)\nIt did not work well in recurrent networks or deep auto-encoders.\nSupport Vector Machines worked better, required less expertise, produced repeatable results, and had much fancier theory.\n\nThe real reasons it failed:\n\nComputers were thousands of times too slow.\nLabeled datasets were hundreds of times too small.\nDNN were too small and inadequately initialized.\n\nThese issues prevented it from being successful for tasks where it would eventually be a big win."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#a-spectrum-of-ml-tasks",
    "href": "notes/dnn/dnn-13/l_13.html#a-spectrum-of-ml-tasks",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "A 🌈 spectrum 🌈 of ML 🤖 tasks",
    "text": "A 🌈 spectrum 🌈 of ML 🤖 tasks\n\nLow-dimensional data (e.g. less than 100 dimensions)\nLots of noise in the data.\nNot much structure in the data. The structure can be captured by a fairly simple model.\nThe main problem is separating true structure from noise.\n\nNot ideal for non-Bayesian neural nets. Try SVM or GP.\n\nHigh-dimensional data (e.g. more than 100 dimensions)\nThe noise is not the main problem.\nThere is a huge amount of structure in the data, but its too complicated to be represented by a simple model.\nThe main problem is figuring out a way to represent the complicated structure so that it can be learned.\n\nLet backpropagation figure it out."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#why-svms-were-never-a-good-bet-for-ai-tasks-that-need-good-representations",
    "href": "notes/dnn/dnn-13/l_13.html#why-svms-were-never-a-good-bet-for-ai-tasks-that-need-good-representations",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Why SVMs were never a good bet for AI tasks that need good representations",
    "text": "Why SVMs were never a good bet for AI tasks that need good representations\n\nView 1: SVM’s are just a clever reincarnation of Perceptrons.\n\nThey expand the input to a (very large) layer of nonlinear non-adaptive features.\nThey only have one layer of adaptive weights.\nThey have a very efficient way of fitting the weights that controls overfitting.\n\nView 2: SVM’s are just a clever reincarnation of Perceptrons.\n\nThey use each input vector in the training set to define a non-adaptive “pheature”.\n\nThe global match between a test input and that training input.\n\nThey have a clever way of simultaneously doing feature selection and finding weights on the remaining features."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#what-is-wrong-with-back-propagation",
    "href": "notes/dnn/dnn-13/l_13.html#what-is-wrong-with-back-propagation",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "What is wrong with back-propagation?",
    "text": "What is wrong with back-propagation?\n\nIt requires labeled training data.\n\nAlmost all data is unlabeled.\n\nThe learning time does not scale well\n\nIt is very slow in networks with multiple hidden layers.\nWhy?\n\nIt can get stuck in poor local optima.\n\nThese are often quite good, but for deep nets they are far from optimal.\nShould we retreat to models that allow convex optimization?"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#overcoming-the-limitations-of-back-propagation-by-using-unsupervised-learning",
    "href": "notes/dnn/dnn-13/l_13.html#overcoming-the-limitations-of-back-propagation-by-using-unsupervised-learning",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Overcoming the limitations of back-propagation by using unsupervised learning",
    "text": "Overcoming the limitations of back-propagation by using unsupervised learning\n\nKeep the efficiency and simplicity of using a gradient method for adjusting the weights, but use it for modeling the structure of the sensory input.\n\nAdjust the weights to maximize the probability that a generative model would have generated the sensory input.\nIf you want to do computer vision, first learn computer graphics.\n\nThe learning objective for a generative model:\n\nMaximise p(x) not p(y \\mid x)\n\nWhat kind of generative model should we learn?\n\nAn energy-based model like a Boltzmann machine?\nA causal model made of idealized neurons?\nA hybrid of the two?"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#artificial-intelligence-and-probability",
    "href": "notes/dnn/dnn-13/l_13.html#artificial-intelligence-and-probability",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Artificial Intelligence and Probability",
    "text": "Artificial Intelligence and Probability\n\n“Many ancient Greeks supported Socrates opinion that deep, inexplicable thoughts came from the gods. Today’s equivalent to those gods is the erratic, even probabilistic neuron. It is more likely that increased randomness of neural behavior is the problem of the epileptic and the drunk, not the advantage of the brilliant.” — P.H. Winston, “Artificial Intelligence”, 1977. (The first AI textbook)\n\n\n“All of this will lead to theories of computation which are much less rigidly of an all-or-none nature than past and present formal logic … There are numerous indications to make us believe that this new system of formal logic will move closer to another discipline which has been little linked in the past with logic. This is thermodynamics primarily in the form it was received from Boltzmann.”\n— John von Neumann, “The Computer and the Brain”, 1958 (unfinished manuscript)"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#the-marriage-of-graph-theory-and-probability-theory",
    "href": "notes/dnn/dnn-13/l_13.html#the-marriage-of-graph-theory-and-probability-theory",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "The marriage of graph theory and probability theory",
    "text": "The marriage of graph theory and probability theory\n\nIn the 1980’s there was a lot of work in AI that used bags of rules for tasks such as medical diagnosis and exploration for minerals.\n\nFor practical problems, they had to deal with uncertainty.\nThey made up ways of doing this that did not involve probabilities!\n\nGraphical models: Pearl, Heckerman, Lauritzen, and many others showed that probabilities worked better.\n\nGraphs were good for representing what depended on what.\nProbabilities then had to be computed for nodes of the graph, given the states of other nodes.\n\nBelief Nets: For sparsely connected, directed acyclic graphs, clever inference algorithms were discovered."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#belief-nets",
    "href": "notes/dnn/dnn-13/l_13.html#belief-nets",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Belief Nets",
    "text": "Belief Nets\n\n\n\n\n\nSigmoid belief net\n\n\n\nA belief net is a directed acyclic graph composed of stochastic variables.\nWe get to observe some of the variables and we would like to solve two problems:\nThe inference problem: Infer the states of the unobserved variables.\nThe learning problem: Adjust the interactions between variables to make the network more likely to generate the training data."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#graphical-models-versus-neural-networks",
    "href": "notes/dnn/dnn-13/l_13.html#graphical-models-versus-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Graphical Models versus Neural Networks",
    "text": "Graphical Models versus Neural Networks\n\nEarly graphical models used experts to define the graph structure and the conditional probabilities.\n\nThe graphs were sparsely connected.\nResearchers initially focused on doing correct inference, not on learning.\n\nFor neural nets, learning was central. Hand-wiring the knowledge was not cool (OK, maybe a little bit).\n\nKnowledge came from learning the training data.\n\nNeural networks did not aim for interpretability or sparse connectivity to make inference easy.\n\nNevertheless, there are neural network versions of belief nets."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#two-types-of-generative-neural-network-composed-of-stochastic-binary-neurons",
    "href": "notes/dnn/dnn-13/l_13.html#two-types-of-generative-neural-network-composed-of-stochastic-binary-neurons",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Two types of generative neural network composed of stochastic binary neurons",
    "text": "Two types of generative neural network composed of stochastic binary neurons\n\n\n\n\n\nSigmoid belief net\n\n\n\nEnergy-based: We connect binary stochastic neurons using symmetric connections to get a Boltzmann Machine.\n\nIf we restrict the connectivity in a special way, it is easy to learn a Boltzmann machine.\nBut then we only have one hidden layer.\n\nCausal: We connect binary stochastic neurons in a directed acyclic graph to get a Sigmoid Belief Net (Neal 1992)."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#connectionist-learning-of-belief-networks---paper",
    "href": "notes/dnn/dnn-13/l_13.html#connectionist-learning-of-belief-networks---paper",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Connectionist learning of belief networks - Paper",
    "text": "Connectionist learning of belief networks - Paper\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#learning-sigmoid-belief-nets",
    "href": "notes/dnn/dnn-13/l_13.html#learning-sigmoid-belief-nets",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Learning Sigmoid Belief Nets",
    "text": "Learning Sigmoid Belief Nets\n\n\n\n\n\nSigmoid Belief Nets\n\n\n\nIt is easy to generate an unbiased example at the leaf nodes, so we can see what kinds of data the network believes in.\nIt is hard to infer the posterior distribution over all possible configurations of hidden causes.\nIt is hard to even get a sample from the posterior.\nSo how can we learn sigmoid belief nets that have millions of parameters?"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#the-learning-rule-for-sigmoid-belief-nets",
    "href": "notes/dnn/dnn-13/l_13.html#the-learning-rule-for-sigmoid-belief-nets",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "The learning rule for sigmoid belief nets",
    "text": "The learning rule for sigmoid belief nets\n\n\n\n\n\nsigmoid belief nets\n\n\n\nLearning is easy if we can get an unbiased sample from the posterior distribution over hidden states given the observed data.\nFor each unit, maximize the log prob. that its binary state in the sample from the posterior would be generated by the sampled binary states of its parents.\n\n\np_i = p(s_= 1) = \\frac {1}{1+ \\exp \\bigg(-b_i -\\sum_j s_j w_{ji} \\bigg )}\n\n\n\\Delta w_{ij} = \\epsilon s_j (s_i-p_i)"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#explaining-away-judea-pearl",
    "href": "notes/dnn/dnn-13/l_13.html#explaining-away-judea-pearl",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Explaining away (Judea Pearl)",
    "text": "Explaining away (Judea Pearl)\n\n\n\n\nEven if two hidden causes are independent in the prior, they can become dependent when we observe an effect that they can both influence.\n\nIf we learn that there was an earthquake it reduces the probability that the house jumped because of a truck."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#why-its-hard-to-learn-sigmoid-belief-nets-one-layer-at-a-time",
    "href": "notes/dnn/dnn-13/l_13.html#why-its-hard-to-learn-sigmoid-belief-nets-one-layer-at-a-time",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Why it’s hard to learn sigmoid belief nets one layer at a time",
    "text": "Why it’s hard to learn sigmoid belief nets one layer at a time\n\n\n\n\nTo learn W, we need to sample from the posterior distribution in the first hidden layer.\nProblem 1: The posterior is not factorial because of “explaining away”.\nProblem 2: The posterior depends on the prior as well as the likelihood.\n\nSo to learn W, we need to know the weights in higher layers, even if we are only approximating the posterior. All the weights interact.\n\nProblem 3: We need to integrate over all possible configurations in the higher layers to get the prior for first hidden layer. Its hopeless!"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#some-methods-for-learning-deep-belief-nets",
    "href": "notes/dnn/dnn-13/l_13.html#some-methods-for-learning-deep-belief-nets",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Some methods for learning deep belief nets",
    "text": "Some methods for learning deep belief nets\n\nMonte Carlo methods can be used to sample from the posterior (Neal 1992).\n\nBut its painfully slow for large, deep belief nets.\n\nIn the 1990’s people developed variational methods for learning deep belief nets.\n\nThese only get approximate samples from the posterior.\n\nLearning with samples from the wrong distribution:\n\nMaximum likelihood learning requires unbiased samples from the posterior.\n\nWhat happens if we sample from the wrong distribution but still use the maximum likelihood learning rule?\n\nDoes the learning still work or does it do crazy things?"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#an-apparently-crazy-idea",
    "href": "notes/dnn/dnn-13/l_13.html#an-apparently-crazy-idea",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "An apparently crazy idea",
    "text": "An apparently crazy idea\n\nIt’s hard to learn complicated models like Sigmoid Belief Nets.\nThe problem is that it’s hard to infer the posterior distribution over hidden configurations when given a data vector.\n\nIts hard even to get a sample from the posterior.\n\nCrazy idea: do the inference wrong.\n\nMaybe learning will still work.\nThis turns out to be true for SBNs.\n\nAt each hidden layer, we assume (wrongly) that the posterior over hidden configurations factorizes into a product of distributions for each separate hidden unit."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#factorial-distributions",
    "href": "notes/dnn/dnn-13/l_13.html#factorial-distributions",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Factorial distributions",
    "text": "Factorial distributions\n\nIn a factorial distribution, the probability of a whole vector is just the product of the probabilities of its individual terms:\nindividual probabilities of three hidden units in a layer 0.3 0.6 0.8\nprobability that the hidden units have state 1,0,1 if the distribution is factorial. p(1, 0, 1) = 0.3× (1− 0.6) \\times 0.8\nA general distribution over binary vectors of length N has 2^N degrees of freedom (actually 2^N-1 because the probabilities must add to 1). A factorial distribution only has N degrees of freedom."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#the-wake-sleep-algorithm-hinton-et.-al.-1995",
    "href": "notes/dnn/dnn-13/l_13.html#the-wake-sleep-algorithm-hinton-et.-al.-1995",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "The wake-sleep algorithm (Hinton et. al. 1995)",
    "text": "The wake-sleep algorithm (Hinton et. al. 1995)\nUnable to display PDF file. Download instead.\n\n\n\n\n\nwake sleep alg\n\n\n\nWake phase: Use recognition weights to perform a bottom-up pass.\n\nTrain the generative weights to reconstruct activities in each layer from the layer above.\n\nSleep phase: Use generative weights to generate samples from the model.\n\nTrain the recognition weights to reconstruct activities in each layer from the layer below."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#the-flaws-in-the-wake-sleep-algorithm",
    "href": "notes/dnn/dnn-13/l_13.html#the-flaws-in-the-wake-sleep-algorithm",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "The flaws in the wake-sleep algorithm",
    "text": "The flaws in the wake-sleep algorithm\n\nThe recognition weights are trained to invert the generative model in parts of the space where there is no data.\n\nThis is wasteful.\n\nThe recognition weights do not follow the gradient of the log probability of the data. They only approximately follow the gradient of the variational bound on this probability.\n\nThis leads to incorrect mode-averaging\n\nThe posterior over the top hidden layer is very far from independent because of explaining away effects.\nNevertheless, Karl Friston thinks this is how the brain works."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#mode-averaging",
    "href": "notes/dnn/dnn-13/l_13.html#mode-averaging",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Mode averaging",
    "text": "Mode averaging\n\n\n\n\n\n\n\nIf we generate from the model, half the instances of a 1 at the data layer will be caused by a (1,0) at the hidden layer and half will be caused by a (0,1).\n\nSo the recognition weights will learn to produce (0.5, 0.5)\nThis represents a distribution that puts half its mass on 1,1 or 0,0: very improbable hidden configurations.\n\nIts much better to just pick one mode.\n\nThis is the best recognition model you can get if you assume that the posterior over hidden states factorizes.\n\n\n\n\n\nSigmoid belief net\nSigmoid belief net\nSigmoid Belief Nets\nsigmoid belief nets\nwake sleep alg"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#footnotes",
    "href": "notes/dnn/dnn-13/l_13.html#footnotes",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe course Graphical Model on Coursera is rather divergent from Neural Networks.↩︎"
  },
  {
    "objectID": "notes/dnn/dnn-references/index.html",
    "href": "notes/dnn/dnn-references/index.html",
    "title": "Deep Neural Networks - Some Questions",
    "section": "",
    "text": "Some questions I have possed on DNN\nQ1. Is there a way to assess the impact of a trainng case or a batch on the model’s, specific layers and specific units? A1. Over the years since I posed this question I have noticed that it is something researchers seem to have looked at. - At first glance it seems like it is im[pssible to assess the impact. SGD works on mini batches or the full data. - But when we analy`se MNIST errors we take the worst misclassifications and we can look at the activation they generate at different level. We can see the activation that leads to a misclassification. So it turns out that it is possible. - Hinton also desribed full using MCMC for full baysian learning . Mackay also put DNN on more or less solid baysian footing. I have not implementated it so I cannot speak to the details but intuitively with a posterior it should be possible to condition on a point.\nLets imagine we could be advised by a “demon” regarding the can assess the over all contribution to signal or noise of different aspects of our model according to the following typology: First kind – overall model Second kind – each hyper parameter\nThird kind – at each layer Fourth kind – at each unit (neuron) Fifth kind – at the weights level Sixth Kind - part of an training item that activates neurons (pixels/sounds/words) I’m considering an analytics platform that would be based on collecting data from Jason Yosinski’s data visualization toolbox\nOne way to do this is to have a procedure that can quickly unlearn/forget training sets then do a diff. (might not be very useful if there are millions of weights) We may need some measure of uncertainty from non parametric methods that describes how if we are adding more learning points in places that are fitting our manifold at new point which are like new (learning new atoms or their relations) or we are just moving the surface back and forth at a single location or its neighborhood.\ne.g. learn the feature that differentiates birds from bees (generalizing) rather than modelling different points of view for each instance of bird and bee (modeling noise).\nFor each row in the data set what do we learn from it ?\nmore atomic concepts Relations on atomic concepts better smoothing – fitting missing data Short term relationships a&gt;b long distance relation a&gt;b&gt;…&gt;c&gt;d\nNN loves more data - more features, more layers more observation but the model can be grow very big and if we use lots of data we will need to train for a very long time\nI would like to explore the following ideas\nrunning some parametric algorithm on the data to bootstrap the neural net’s prior distributions closer the final values\nsimilar to the above I’d like to training nn dynamically and possibly non parametrically (you can have more CPU, memory, storage, data etc. but you get penalized for it) The TF graph should be expanded/contracted layers membership increased or decreased layers increased, hyper params adjusted during training.\nBayesian methods allow choices to be made about where in input space new data should be collected in order that it be the most informative (MacKay, 1992c). Such use of the model itself to guide the collection of data during training is known as active learning.\nMacKay, D. J. C. (1992c). Information-based objective functions for active data selection. Neural Computation 4 (4), 590-604.\nThe relative importance of different inputs can be determined using the Bayesian technique of automatic relevance determination (MacKay, 1994a, 1995b; Neal, 1994), based on the use of a separate regularization coefficient for each input. If a particular coefficient acquires a large value, this indicates that the corresponding input is irrelevant and can be eliminated.\nNeal, R. M. (1994). Bayesian Learning for Neural Networks. Ph.D. thesis, University of Toronto, Canada.\nMacKay, D. J. C. (1994a). Bayesian methods for backpropagation networks. In E. Domany, J. L. van Hemmen, and K. Schulten (Eds.), Models of Neural Networks III, Chapter 6. New York: Springer-Verlag.\nMacKay, D. J. C. (1995b). Bayesian non-linear modelling for the 1993 energy prediction competition. In G. Heidbreder (Ed.), Maximum Entropy and Bayesian Methods\nQuestions: In your own words describe a neural network\nA Neural Network consists of a graph with the inputs in one side and outputs on the other and between them are hidden units. All these nodes are connected with the connection strength between of the vertex connecting the units called its weight. Generally the graph is bipartite and can thus be organized using layers.\nThe graph can be trained so that the\nWeights are the vertices\nActions – the nodes ? what are these Model selection - Chaos –\nWhat is the importance of relative weights – within the same layer, between layers Given answers for the above should we use that for bootstrapping the wights instead of using random weights.\nGeometry of second order methods. Won’t using Mini Batched steps help where there is a complex structure.\nWhat is there are many local minima in our surface – how can we learn it all if we are always growing downhill. What happens if we have a chaotic surface – I think we can get this with a logistic function - What about an oscillation.\nDifference between first and second order learning methods\nIn reinforcement models the game being played is a markov decision process\nDo GAN take this concept one step further ?\nFor DNN what filters/kernels are initially selected. Are some different basis functions going to work better than others.\nAlso how about making some basis functions size independent by adding a 3by three five by five seven by seven etc. version.\nFor video filters that are time dependent. Also what about using non orthogonal basis.\nAlso what about forcing the system to drop basis which is redundant\nFor DNN we see that usually we have square on square configurations to reduce and mix the data. What about triangular or hexagonal architecture. Howa bout looking at RGB&Grey\nPostscript:\nBatch normalization: Accelerating … Input: Values of overa mini-batch: B = Parameters to be leamed: -y, ’3 Output: {Yi Xi — 11B 2 // mini-b;\nPix2Pix\nAttention - all you need is attention\nGroup Equivariant Convolutional Networks\nSteerable CNNs\nlogarithmic spiral\nfractal affine embeddings\nsimulate stereo vision modes\nVisualization\ndistil journal\nActivation-atlas\n\nhttps://aiyproject.withgoogle.com/open_speech_recording\n\nhttps://github.com/petewarden/open-speech-recording\nhttps://distill.pub/2016/augmented-rnns/\n\nAttention and Augmented Recurrent Neural Networks\n\nhttp://colah.github.io/\nhttps://github.com/sunnyyeti/Solutions-to-Neural-Network-for-machine-learning-by-Geoffrey-Hinton-on-Coursera\nhttps://github.com/BradNeuberg/hinton-coursera/blob/master/assignment3/a3.m\nhttps://github.com/Chouffe/hinton-coursera/tree/master/hw3\nhttps://github.com/tensorflow/compression/blob/master/examples/bls2017.py\nhttps://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n\nnlp\n\nhttps://arxiv.org/abs/1803.06643\nhttps://arxiv.org/abs/1811.00937\n\n\n\n\n\nReuseCC SA BY-NC-NDCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Some} {Questions}},\n  date = {2017-12-21},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-references},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Some\nQuestions.” December 21, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-references."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html",
    "href": "notes/dnn/dnn-06/l_06.html",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6a-overview-of-mini-batch-gradient-descent",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6a-overview-of-mini-batch-gradient-descent",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6a: Overview of mini-batch gradient descent",
    "text": "Lecture 6a: Overview of mini-batch gradient descent\nNow we’re going to discuss numerical optimization: how best to adjust the weights and biases, using the gradient information from the backprop algorithm. This video elaborates on the most standard neural net optimization algorithm (mini-batch gradient descent), which we’ve seen before. We’re elaborating on some issues introduced in video 3e."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6b-a-bag-of-tricks-for-mini-batch-gradient-descent",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6b-a-bag-of-tricks-for-mini-batch-gradient-descent",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6b: A bag of tricks for mini-batch gradient descent",
    "text": "Lecture 6b: A bag of tricks for mini-batch gradient descent\ninitializing weights: we must not initialize units with equal weights as they can never become different. we cannot use zero as it will remain zero we want to avoid explosion and vanishing weights fan in - the number of inputs\nPart 1 is about transforming the data to make learning easier. At 1:10, there’s a comment about random weights and scaling. The “it” in that comment is the average size of the input to the unit. At 1:15, the “good principle”: what he means is INVERSELY proportional. At 4:38, Geoff says that the hyperbolic tangent is twice the logistic minus one. This is not true, but it’s almost true. As an exercise, find out’s missing in that equation. At 5:08, Geoffrey suggests that with a hyperbolic tangent unit, it’s more difficult to sweep things under the rug than with a logistic unit. I don’t understand his comment, so if you don’t either, don’t worry. This comment is not essential in this course: we’re never using hyperbolic tangents in this course. Part 2 is about changing the stochastic gradient descent algorithm in sophisticated ways. We’ll look into these four methods in more detail, later on in the course. Jargon: “stochastic gradient descent” is mini-batch or online gradient descent. The term emphasizes that it’s not full-batch gradient descent. “stochastic” means that it involves randomness. However, this algorithm typically does not involve randomness. However, it would be truly stochastic if we would randomly pick 100 training cases from the entire training set, every time we need the next mini-batch. We call traditional “stochastic gradient descent” stochastic because it is, in effect, very similar to that truly stochastic version. Jargon: a “running average” is a weighted average over the recent past, where the most recent past is weighted most heavily."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6c-the-momentum-method",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6c-the-momentum-method",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6c: The momentum method",
    "text": "Lecture 6c: The momentum method\nDrill down into momentum mentioned before.\nThe biggest challenge in this video is to think of the error surface as a mountain landscape. If you can do that, and you understand the analogy well, this video will be easy. You may have to go back to video 3b, which introduces the error surface. Important concepts in this analogy: “ravine”, “a low point on the surface”, “oscillations”, “reaching a low altitude”, “rolling ball”, “velocity”. All of those have meaning on the “mountain landscape” side of the analogy, as well as on the “neural network learning” side of the analogy. The meaning of “velocity” in the “neural network learning” side of the analogy is the main idea of the momentum method. Vocabulary: the word “momentum” can be used with three different meanings, so it’s easy to get confused. It can mean the momentum method for neural network learning, i.e. the idea that’s introduced in this video. This is the most appropriate meaning of the word. It can mean the viscosity constant (typically 0.9), sometimes called alpha, which is used to reduce the velocity. It can mean the velocity. This is not a common meaning of the word. Note that one may equivalently choose to include the learning rate in the calculation of the update from the velocity, instead of in the calculation of the velocity."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6d-adaptive-learning-rates-for-each-connection",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6d-adaptive-learning-rates-for-each-connection",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6d: Adaptive learning rates for each connection",
    "text": "Lecture 6d: Adaptive learning rates for each connection\nThis is really “for each parameter”, i.e. biases as well as connection strengths. Vocabulary: a “gain” is a multiplier. This video introduces a basic idea (see the video title), with a simple implementation. In the next video, we’ll see a more sophisticated implementation. You might get the impression from this video that the details of how best to use such methods are not universally agreed on. That’s true. It’s research in progress."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6e-rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6e-rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6e: Rmsprop: Divide the gradient by a running average of its recent magnitude",
    "text": "Lecture 6e: Rmsprop: Divide the gradient by a running average of its recent magnitude\nThis is another method that treats every weight separately. rprop uses the method of video 6d, plus that it only looks at the sign of the gradient. Make sure to understand how momentum is like using a (weighted) average of past gradients. Synonyms: “moving average”, “running average”, “decaying average”. All of these describe the same method of getting a weighted average of past observations, where recent observations are weighted more heavily than older ones. That method is shown in video 6e at 5:04. (there, it’s a running average of the square of the gradient) “moving average” and “running average” are fairly generic. “running average” is the most commonly used phrase. “decaying average” emphasizes the method that’s used to compute it: there’s a decay factor in there, like the alpha in the momentum method."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html",
    "href": "notes/dnn/dnn-06/l06c.html",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#the-intuition-behind-the-momentum-method",
    "href": "notes/dnn/dnn-06/l06c.html#the-intuition-behind-the-momentum-method",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "The intuition behind the momentum method",
    "text": "The intuition behind the momentum method\n\n\n\n\n\nMomentum intution\n\n\n\nImagine a ball on the error surface. The location of the ball in the horizontal plane represents the weight vector.\n\nThe ball starts off by following the gradient, but once it has velocity, it no longer does steepest descent.\n\nIts momentum makes it keep going in the previous direction.\n\nIt damps oscillations in directions of high curvature by combining gradients with opposite signs.\nIt builds up speed in directions with a gentle but consistent gradient."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#the-equations-of-the-momentum-method",
    "href": "notes/dnn/dnn-06/l06c.html#the-equations-of-the-momentum-method",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "The equations of the momentum method",
    "text": "The equations of the momentum method\nThe effect of the gradient is to increment the previous velocity. The velocity also decays by \\alpha which is slightly less then 1. \nv(t) =α v(t −1)−ε \\frac{∂E}{∂w}(t)\n\nThe weight change is equal to the current velocity.\n\n\\begin{align}\nΔw(t) &= v(t) \\\\\n      &= α v(t −1)−ε \\frac{∂E}{∂w}(t)  \\\\\n      &= α Δw(t −1)−ε \\frac{∂E}{∂w}(t)\n\\end{align}\n\nThe weight change can be expressed in terms of the previous weight change and the current gradient."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#the-behavior-of-the-momentum-method",
    "href": "notes/dnn/dnn-06/l06c.html#the-behavior-of-the-momentum-method",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "The behavior of the momentum method",
    "text": "The behavior of the momentum method\n\nIf the error surface is a tilted plane, the ball reaches a terminal velocity.\n\nIf the momentum is close to 1, this is much faster than simple gradient descent.\n\nAt the beginning of learning there may be very large gradients.\n\nSo it pays to use a small momentum (e.g. 0.5).\nOnce the large gradients have disappeared and the weights are stuck in a ravine the momentum can be smoothly raised to its final value (e.g. 0.9 or even 0.99)\n\nThis allows us to learn at a rate that would cause divergent oscillations without the momentum.\n\n\nv(∞) = \\frac{1}{1−α} \\biggr( −ε \\frac{∂E}{∂w} \\biggr)"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#a-better-type-of-momentum-nesterov-1983",
    "href": "notes/dnn/dnn-06/l06c.html#a-better-type-of-momentum-nesterov-1983",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "A better type of momentum (Nesterov 1983)",
    "text": "A better type of momentum (Nesterov 1983)\n\nThe standard momentum method first computes the gradient at the current location and then takes a big jump in the direction of the updated accumulated gradient.\nIlya Sutskever (2012 unpublished) suggested a new form of momentum that often works better.\n\nInspired by the Nesterov method for optimizing convex functions.\n\n\nFirst make a big jump in the direction of the previous accumulated gradient.\nThen measure the gradient where you end up and make a correction.\n\nIts better to correct a mistake after you have made it!"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#a-picture-of-the-nesterov-method",
    "href": "notes/dnn/dnn-06/l06c.html#a-picture-of-the-nesterov-method",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "A picture of the Nesterov method",
    "text": "A picture of the Nesterov method\n\n\n\nbrown vector = jump,\nred vector = correction,\ngreen vector = accumulated gradient blue vectors = standard momentum\n\nFirst make a big jump in the direction of the previous accumulated gradient.\nThen measure the gradient where you end up and make a correction.\n\n\n\n\nMomentum intution"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html",
    "href": "notes/dnn/dnn-06/l06e.html",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#rprop-using-only-the-sign-of-the-gradient",
    "href": "notes/dnn/dnn-06/l06e.html#rprop-using-only-the-sign-of-the-gradient",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "rprop: Using only the sign of the gradient",
    "text": "rprop: Using only the sign of the gradient\n\nThe magnitude of the gradient can be very different for different weights and can change during learning.\n\nThis makes it hard to choose a single global learning rate.\n\nFor full batch learning, we can deal with this variation by only using the sign of the gradient.\n\nThe weight updates are all of the same magnitude.\nThis escapes from plateaus with tiny gradients quickly.\n\n\nrprop: This combines the idea of only using the sign of the gradient with the idea of adapting the step size separately for each weight.\n\nIncrease the step size for a weight multiplicatively (e.g. times 1.2) if the signs of its last two gradients agree.\nOtherwise decrease the step size multiplicatively (e.g. times 0.5).\nLimit the step sizes to be less than 50 and more than a millionth (Mike Shuster’s advice)."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#why-rprop-does-not-work-with-mini-batches",
    "href": "notes/dnn/dnn-06/l06e.html#why-rprop-does-not-work-with-mini-batches",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "Why rprop does not work with mini-batches",
    "text": "Why rprop does not work with mini-batches\n\nThe idea behind stochastic gradient descent is that when the learning rate is small, it averages the gradients over successive minibatches.\n\nConsider a weight that gets a gradient of +0.1 on nine minibatches and a gradient of -0.9 on the tenth mini-batch.\n\nWe want this weight to stay roughly where it is.\n\nrprop would increment the weight nine times and decrement it once by about the same amount (assuming any adaptation of the step sizes is small on this time-scale).\n\nSo the weight would grow a lot.\n\nIs there a way to combine:\n\nThe robustness of rprop.\nThe efficiency of mini-batches.\nThe effective averaging of gradients over mini-batches."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#rmsprop-a-mini-batch-version-of-rprop",
    "href": "notes/dnn/dnn-06/l06e.html#rmsprop-a-mini-batch-version-of-rprop",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "rmsprop: A mini-batch version of rprop",
    "text": "rmsprop: A mini-batch version of rprop\n\nrprop is equivalent to using the gradient but also dividing by the size of the gradient.\n\nThe problem with mini-batch rprop is that we divide by a different number for each mini-batch. So why not force the number we divide by to be very similar for adjacent mini-batches?\n\n\nrmsprop Keep a moving average of the squared gradient for each weight\n\n\nMeanSquare(w,t) = 0.9 \\times MeanSquare(w, t−1) \\times 0.1 \\bigg(\\frac{∂E}{∂w}(t)\\bigg)^2\n - Dividing the gradient by \\sqrt{MeanSquare(w, t)} makes the learning work much better (Tijmen Tieleman, unpublished)."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#further-developments-of-rmsprop",
    "href": "notes/dnn/dnn-06/l06e.html#further-developments-of-rmsprop",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "Further developments of rmsprop",
    "text": "Further developments of rmsprop\n\nCombining rmsprop with standard momentum🚀\n\nMomentum does not help as much as it normally does. Needs more investigation.\n\nCombining rmsprop with Nesterov momentum🚀 (Sutskever 2012) [elshamy2023improving]\n\nIt works best if the RMS of the recent gradients is used to divide the correction rather than the jump in the direction of accumulated corrections.\n\nCombining rmsprop with adaptive learning rates for each connection\n\nNeeds more investigation.\n\nOther methods related to rmsprop\n\nYann LeCun’s group has a fancy version in (Schaul, Zhang, and LeCun 2013)"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#summary-of-learning-methods-for-neural-networks",
    "href": "notes/dnn/dnn-06/l06e.html#summary-of-learning-methods-for-neural-networks",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "Summary of learning methods for neural networks",
    "text": "Summary of learning methods for neural networks\n\nFor small datasets (e.g. 10,000 cases) or bigger datasets without much redundancy, use a full-batch method.\n\nConjugate gradient, LBFGS …\nadaptive learning rates, rprop …\n\nFor big, redundant datasets use minibatches.\n\nTry gradient descent with momentum. 🚀\nTry rmsprop (with momentum🚀 ?)\nTry LeCun’s latest recipe.\n\nWhy there is no simple recipe:\n\nNeural nets differ a lot:\n\nVery deep nets (especially ones with narrow bottlenecks).\nRecurrent nets.\n\nWide shallow nets. -Tasks differ a lot:\nSome require very accurate weights, some don’t.\nSome have many very rare cases (e.g. words).\n\n\n\n\n\n\nsome learning rate algorithms"
  },
  {
    "objectID": "notes/dnn/dnn-12/l_12.html",
    "href": "notes/dnn/dnn-12/l_12.html",
    "title": "Deep Neural Networks - Notes for Lesson 12",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n\n\n\n\nLecture 12a: Boltzmann machine learning\nClarification: The energy is linear in the weights, but quadratic in the states. What matters for this argument is just that it’s linear in the weights.\n\n\nLecture 12b: More efficient ways to get the statistics\n\n\nLecture 12c: Restricted Boltmann Machines\nHere, a “particle” is a configuration. These particles are moving around the configuration space, which, when considered with the energy function, is our mountain landscape.\nIt’s called a reconstruction because it’s based on the visible vector at t=0 (via the hidden vector at t=0). It will, typically, be quite similar to the visible vector at t=0.\nA “fantasy” configuration is one drawn from the model distribution by running a Markov Chain for a long time.\nThe word “fantasy” is chosen as part of the analogy of a Boltzmann Machine vs. a brain that learned several memories.\n\n\nLecture 12d: An example of RBM learning\nThis is not an easy video. Prerequisite is a rather extensive understanding of what an RBM does. Be sure to understand video 12c quite well before proceeding with 12d.\nPrerequisite for this video is that you understand the “reconstruction” concept of the previous video.\nThe first slide is about an RBM, but uses much of the same phrases that we previously used to talk about deterministic feedforward networks.\nThe hidden units are described as feature detectors, or “features” for short.\nThe weights are shown as arrows, even though a Boltzmann Machine has undirected connections.\nThat’s because calculating the probability of the hidden units turning on, given the state of the visible units, is exactly like calculating the real-valued state of a logistic hidden unit, in a deterministic feedforward network.\nHowever, in a Boltzmann Machine, that number is then treated as a probability of turning on, and an actual state of 1 or 0 is chosen, randomly, based on that probability. We’ll make further use of that similarity next week.\n2:30. That procedure for changing energies, that was just explained, is a repeat (in different words) of the Contrastive Divergence story of the previous video. If you didn’t fully realize that, then review.\n\n\nLecture 12e: RBMs for collaborative filtering\n\n\n\n\nReuseCC SA BY-NC-NDCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} for {Lesson} 12},\n  date = {2017-11-01},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-12/l_12.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes for Lesson\n12.” November 1, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-12/l_12.html."
  },
  {
    "objectID": "notes/dnn/dnn-10/r2.html",
    "href": "notes/dnn/dnn-10/r2.html",
    "title": "Deep Neural Networks — Readings II for Lesson 10",
    "section": "",
    "text": "In the paper (Hinton et al. 2012) the authors discuss using dropout as a regularization mechanism to reduce overfitting in deep neural networks.\nUnable to display PDF file. Download instead.\n\n\nWhen a large feed forward neural network is trained on a small training set, it typically performs poorly on held-out test data. This “overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorically large variety of internal contexts in which it must operate. Random “dropout” gives big improvements on many benchmark tasks and sets new records for speech and object recognition.\n\n\n\nThis is a paper about using dropout to as a regularization tool, to prevent nodes co-adaptation within parts of the neural network. As I see it if the network has sufficient capacity it will memorize all the training data and then will perform rather poorly on the holdout data and in real world inference. What happen during overfitting is that the network learn both the signal and the noise. In general the law of number works in our favour and the network and since the signal is stronger than the noise we do not initially overfit. However, as the remaining unlearned signal becomes more rare it becomes harder for the model to separate if from the noise. Rare signals will tend to appear less often than certain common noise patterns. Most regularization techniques try to boost the signal. In this case by effectively reducing the capacity and creating, and making the network overall less cohesive. Dropout effectively reduces the network’s capacity during training. It forces the network to create redundent components which relay less on other units. Another regularization is also used: instead of using L2 on the weights vector, L2 norm penalty is used on each weight. If the weight updates violates the constraints, they are normalized. This is motivated by a wish to start with a high learning rate which would otherwise lead to very large weights. This should intuitively allow the net to initially benefit from the stronger signal while reserving more opportunity for later epochs to leave their mark.\nAt trainng time the full network is used nut the Tha authors claim that dropout is equivilent to avareging many random networks. A point they fail to mention is that\n“Dropout is considerably simpler to implement than Bayesian model averaging which weights each model by its posterior probability given the training data. For complicated model classes, like feed forward neural networks, Bayesian methods typically use a Markov chain Monte Carlo method to sample models from the posterior distribution (14). By contrast, dropout with a probability of 0.5 assumes that all the models will eventually be given equal importance in the combination but the learning of the shared weights takes this into account.”\nMy thoughts are that we should be able to do better than this version of dropout.\n\nShortcoming:\nDropout on units can render the net very poor.\nDrop out slows training down - since we don’t update half the units and probably a large number of the weights.\nFor different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures.\nWe should track dropout related stats to better understand the confidence of the model.\nA second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout?\n\n\nStart with some combinations \\binom k n of the weights. where k = | {training\\; set}|*{minibatch\\_size}. We use the same dropout for each mini-batch, then switch.\nEach epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ?\nWhen we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net.\nIdeally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step.\nWe could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism.\nWe should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "notes/dnn/dnn-10/r2.html#reading-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors",
    "href": "notes/dnn/dnn-10/r2.html#reading-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors",
    "title": "Deep Neural Networks — Readings II for Lesson 10",
    "section": "",
    "text": "In the paper (Hinton et al. 2012) the authors discuss using dropout as a regularization mechanism to reduce overfitting in deep neural networks.\nUnable to display PDF file. Download instead.\n\n\nWhen a large feed forward neural network is trained on a small training set, it typically performs poorly on held-out test data. This “overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorically large variety of internal contexts in which it must operate. Random “dropout” gives big improvements on many benchmark tasks and sets new records for speech and object recognition.\n\n\n\nThis is a paper about using dropout to as a regularization tool, to prevent nodes co-adaptation within parts of the neural network. As I see it if the network has sufficient capacity it will memorize all the training data and then will perform rather poorly on the holdout data and in real world inference. What happen during overfitting is that the network learn both the signal and the noise. In general the law of number works in our favour and the network and since the signal is stronger than the noise we do not initially overfit. However, as the remaining unlearned signal becomes more rare it becomes harder for the model to separate if from the noise. Rare signals will tend to appear less often than certain common noise patterns. Most regularization techniques try to boost the signal. In this case by effectively reducing the capacity and creating, and making the network overall less cohesive. Dropout effectively reduces the network’s capacity during training. It forces the network to create redundent components which relay less on other units. Another regularization is also used: instead of using L2 on the weights vector, L2 norm penalty is used on each weight. If the weight updates violates the constraints, they are normalized. This is motivated by a wish to start with a high learning rate which would otherwise lead to very large weights. This should intuitively allow the net to initially benefit from the stronger signal while reserving more opportunity for later epochs to leave their mark.\nAt trainng time the full network is used nut the Tha authors claim that dropout is equivilent to avareging many random networks. A point they fail to mention is that\n“Dropout is considerably simpler to implement than Bayesian model averaging which weights each model by its posterior probability given the training data. For complicated model classes, like feed forward neural networks, Bayesian methods typically use a Markov chain Monte Carlo method to sample models from the posterior distribution (14). By contrast, dropout with a probability of 0.5 assumes that all the models will eventually be given equal importance in the combination but the learning of the shared weights takes this into account.”\nMy thoughts are that we should be able to do better than this version of dropout.\n\nShortcoming:\nDropout on units can render the net very poor.\nDrop out slows training down - since we don’t update half the units and probably a large number of the weights.\nFor different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures.\nWe should track dropout related stats to better understand the confidence of the model.\nA second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout?\n\n\nStart with some combinations \\binom k n of the weights. where k = | {training\\; set}|*{minibatch\\_size}. We use the same dropout for each mini-batch, then switch.\nEach epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ?\nWhen we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net.\nIdeally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step.\nWe could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism.\nWe should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "notes/dnn/dnn-10/r2.html#further-readind",
    "href": "notes/dnn/dnn-10/r2.html#further-readind",
    "title": "Deep Neural Networks — Readings II for Lesson 10",
    "section": "Further Readind",
    "text": "Further Readind\nc.f. Gal and Ghahramani (2016)"
  },
  {
    "objectID": "notes/dnn/dnn-10/r2.html#my-wrap-up",
    "href": "notes/dnn/dnn-10/r2.html#my-wrap-up",
    "title": "Deep Neural Networks — Readings II for Lesson 10",
    "section": "My wrap up 🎬",
    "text": "My wrap up 🎬\n\nGame theoretic framework have to formalize cooperative and competitive aspects of learning and how these might influence network architectures.\n\nc.f. David Balduzzi (2015) Semantics, Representations and Grammars for Deep Learning.\n\n\nThere has been lots of progress in training single models for multiple tasks. - c.f. Lukasz Kaiser et all. (2017) One Model To Learn Them All. - covered in this video: One Neural network learns EVERYTHING?! which uses mixture of expert layer which come from later work: Noam Shazeer, Azalia Mirhoseini,Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean (2017) - Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer in which mixture of experts is used within large neural networks\n\nReferences\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In Proceedings of the 33rd International Conference on Machine Learning, edited by Maria Florina Balcan and Kilian Q. Weinberger, 48:1050–59. Proceedings of Machine Learning Research. New York, New York, USA: PMLR. https://proceedings.mlr.press/v48/gal16.html.\n\n\nHinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.” https://doi.org/10.48550/arXiv.1207.0580."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html",
    "href": "notes/dnn/dnn-09/l_09.html",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#reminder-overfitting",
    "href": "notes/dnn/dnn-09/l_09.html#reminder-overfitting",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Reminder: Overfitting",
    "text": "Reminder: Overfitting\n\nThe training data contains information about the regularities in the mapping from input to output. But it also contains sampling error.\n\nThere will be accidental regularities just because of the particular training cases that were chosen.\n\nWhen we fit the model, it cannot tell which regularities are real and which are caused by sampling error.\n\nSo it fits both kinds of regularity. If the model is very flexible it can model the sampling error really well."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#preventing-overfitting",
    "href": "notes/dnn/dnn-09/l_09.html#preventing-overfitting",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Preventing overfitting",
    "text": "Preventing overfitting\nFour approaches to reduce overfitting due to too many parameters to training rows:\n\nGet more data - the best option.\nUse a model that has the right capacity:\n\nenough to fit the true regularities.\nnot enough to fit spurious data.1\n\nAverage models.\n\nEnsambling use models with different forms.\nBagging train the model on different subsets the training data. 4.: Bayesian:\nuse a single NN architecture but average the predictions made by many weight vectors\n\nGetting more data via augmentation (increases signal to noise)\n\nconsider normalization\nSample-wise\n\nFeature wise pixel standardization\nPCA whitening - reduces dimension + whiting\nZCA the idea is to reducing effect of correlation in adjacent pixels by normalizing feature variance and reducing correlation at features. (does not reduce dimensions of the data)\n\n\n\n\n\n\n\n\n\ntransform\nimage\n\n\n\n\nOriginal\n\n\n\nFeature Standardization\n\n\n\nZCA whitening\n\n\n\nRandom Rotations\n\n\n\nRandom shifts\n\n\n\nRandom Flips\n\n\n\nRandom affine transforms\n\n\n\nContrast Stretching\n\n\n\nHistogram Equalization\n\n\n\nAdaptive Histogram Equalization\n\n\n\nCLAHE contrast stretching + adaptive histogram equalization = Contrast limited adaptive histogram equalization\n\n\n\n\nconsider augmentation. random crop/rotation/shear/mirroring/flip scaling blocking out rectangles elastic deformation mesh (used in Unet) contrast stretching + adaptive histogram equalization = Contrast limited adaptive histogram equalization (CLAHE) ZCA whitening transform\nStandardized Feature MNIST Images ZCA Whitening MNIST Images random affine transforms\nContrast Stretching Histogram Equalization Adaptive Histogram Equalization \ncode: image-augmentation-deep-learning-keras (on nminst) data augmentation with elastic deformations"
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#how-to-choose-meta-parameters-that-control-capacity",
    "href": "notes/dnn/dnn-09/l_09.html#how-to-choose-meta-parameters-that-control-capacity",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "How to choose meta parameters that control capacity2",
    "text": "How to choose meta parameters that control capacity2\n\nThe wrong method is to try lots of alternatives and see which gives the best performance on the test set. – This is easy to do, but it gives a false impression of how well the method works. – The settings that work best on the test set are unlikely to work as well on a new test set drawn from the same distribution.\nAn extreme example: Suppose the test set has random answers that do not depend on the input.\nThe best architecture will do better than chance on the test set.\nBut it cannot be expected to do better than chance on a new test set."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#cross-validation-a-better-way-to-choose-meta-parameters",
    "href": "notes/dnn/dnn-09/l_09.html#cross-validation-a-better-way-to-choose-meta-parameters",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Cross-validation: A better way to choose meta parameters",
    "text": "Cross-validation: A better way to choose meta parameters\n\nDivide the total dataset into three subsets:\n\nTraining data is used for learning the parameters of the model.\nValidation data is not used for learning but is used for deciding what settings of the meta parameters work best.\nTest data is used to get a final, unbiased estimate of how well the network works. We expect this estimate to be worse than on the validation data.\n\nWe could divide the total dataset into one final test set and N other subsets and train on all but one of those subsets to get N different estimates of the validation error rate.\n\nThis is called N-fold cross-validation.\nThe N estimates are not independent."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#preventing-overfitting-by-early-stopping",
    "href": "notes/dnn/dnn-09/l_09.html#preventing-overfitting-by-early-stopping",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Preventing overfitting by early stopping",
    "text": "Preventing overfitting by early stopping\n\nIf we have lots of data and a big model, its very expensive to keep re-training it with different sized penalties on the weights.\nIt is much cheaper to start with very small weights and let them grow until the performance on the validation set starts getting worse.\n\nBut it can be hard to decide when performance is getting worse.\n\nThe capacity of the model is limited because the weights have not had time to grow big."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#why-early-stopping-works",
    "href": "notes/dnn/dnn-09/l_09.html#why-early-stopping-works",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Why early stopping works",
    "text": "Why early stopping works\n\nWhen the weights are very small, every hidden unit is in its linear range.\n\nSo a net with a large layer of hidden units is linear.\nIt has no more capacity than a linear net in which the inputs are directly connected to the outputs!\n\nAs the weights grow, the hidden units start using their non-linear ranges so the capacity grows."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#limiting-the-size-of-the-weights",
    "href": "notes/dnn/dnn-09/l_09.html#limiting-the-size-of-the-weights",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Limiting the size of the weights",
    "text": "Limiting the size of the weights\n\n\n\n\n\nlimiting weights\n\n\nThe standard L2 weight penalty involves adding an extra term to the cost function that penalizes the squared weights. - This keeps the weights small unless they have big error derivatives.\n\nC = E + \\frac{\\lambda}{2}\\sum_i w_i^2\n\n\n\\frac{\\partial C}{\\partial w_i} = \\frac{\\partial E}{\\partial w_i} + \\lambda w_i\n\nwhen\n\n\\frac{\\partial C}{\\partial w_i} = 0 \\implies w_i=-\\frac{1}{\\lambda} \\frac{\\partial E}{\\partial w_i}\n There is some math in this video. It’s not complicated math. You should make sure to understand it."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#lecture-9c-using-noise-as-a-regularizer",
    "href": "notes/dnn/dnn-09/l_09.html#lecture-9c-using-noise-as-a-regularizer",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Lecture 9c: Using noise as a regularizer",
    "text": "Lecture 9c: Using noise as a regularizer\nadding noise to the input can have a regularizing effect. Think lets add noise to a picture - it drowns out most of the small features say blurring them. But for large items - we are trying to learn - they look mostly the same. The advantage is that adding noise is easy. Anyhow - this is more of a buildup of an abstract idea that will later be interpreted using full Bayesian learning. L2 weight-decay via noisy inp Suppose we add Gaussian noise to the inputs. — The variance of the noise is amplified by the squared weight before going into the next layer. In a simple net with a linear output unit directly connected to the inputs, the amplified noise gets added to the output. This makes an additive contribution to the This slide serves to show that noise is not a crazy idea. The penalty strength can be thought of as being\nσ2i , or twice that (to compensate for the 1/2 in the weight decay cost function), but that detail is not important here. Second slide (the math slide)\nThe reason why the middle term is zero is that all of the epsilons have mean zero. You may notice that the result is not exactly like the L2 penalty of the previous video: the factor 1/2 is missing. Or equivalently, the strength of the penalty is not sigma i squared, but twice that. The main point, however, is that this noise is equivalent to an L2 penalty. Jargon: overfitting, underfitting, generalization, and regularization Overfitting can be thought of as the model being too confident about what the data is like: more confident than would be justified, given the limited amount of training data that it was trained on. If an alien from outer space would take one look at a street full of cars (each car being a training case), and it so happens that there were only two Volkswagens there, one dark red and one dark blue, then the alien might conclude “all Volkswagens on Earth are of dark colours.” That would be overfitting. If, on the other hand, the alien would be so reluctant to draw conclusions that he even fails to conclude that cars typically have four wheels, then that would be underfitting. We seek the middle way, where we don’t draw more than a few unjustified conclusions, but we do draw most of the conclusions that really are justified. Regularization means forcing the model to draw fewer conclusions, thus limiting overfitting. If we overdo it, we end up underfitting. Jargon: “generalization” typically means the successful avoidance of both overfitting and underfitting. Since overfitting is harder to avoid, “generalization” often simply means the absence of (severe) overfitting. The “accidental regularities” that training data contains are often complicated patterns. However, NNs can learn complicated patterns quite well. Jargon: “capacity” is learning capacity. It’s the amount of potential (artificial) brain power in a model, and it mostly depends on the number of learned parameters (weights & biases)."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#lecture-9d-introduction-to-the-full-bayesian-approach",
    "href": "notes/dnn/dnn-09/l_09.html#lecture-9d-introduction-to-the-full-bayesian-approach",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Lecture 9d: Introduction to the full Bayesian approach",
    "text": "Lecture 9d: Introduction to the full Bayesian approach\nThe full Bayesian approach could provide an alternative to using SGD. However with the exception of very simple models it is usually computationally intractable as it requires finding the prior distribution for all the parameters.\nWe can start with an prior P(params) - and adjust it given each training item.\nGiven some data we would have to calculate its likelihood i.e. p(data)\nBut to do this we would need to see how it effects all parameter settings - this is the real issue as for 10 settings for 100 nodes we would need to test 10^100 weight combinations…\nthis is an outline of the Bayesian approach.\nthere is a prior distribution over parameters, there is data, say the training data and we can calculate its likelihood and combine it with the prior to get a posterior.\nWith sufficient Bayesian updating will in the limit beat an uninformative prior.\nbut he does not go into how much data. The Bayesian framework The Bayesian framework assumes that we distribution for everything. — The prior may be very vague. — When we see some data, we combine our with a likelihood term to get a posterior distr — The likelihood term takes into account how observed data is given the parameters of th\na 100 coin tosses motivates the frequentist approach which uses the (ML) maximal likelihood estimate of the probability.\nNext calculates the ml is 0.53 by differentiating and setting the derivative equal to zero. Next asks what if we have only one coin toss. which is a kin to asking “what if the experiment is too small and there are unobserved outcomes?” in which case we cannot account for their likelihood in a ML estimate. A coin tossing example Suppose we know nothing about coins excep tossing event produces a head with some unl probability p and a tail with probability I-p.  — Our model of a coin has one parameter, p Suppose we observe 100 tosses and there al What is p? here D is the data and W is a set of weights.\nBayes Theorem joint probability prior probability of weight vector W probabilit data give p(W) p(DlW) ID)\nHowever, it may be possible to approximate a prior. The terms “prior”, “likelihood term”, and “posterior” are explained in a more mathematical way at the end of the video, so if you’re confused, just keep in mind that a mathematical explanation follows. For the coin example, try not to get confused about the difference between “p” (the probability of seeing heads) and “P” (the abbreviation for “probability”). Jargon: “maximum likelihood” means maximizing the likelihood term, without regard to any prior that there may be. At 8:22 there’s a slightly incorrect statement in the explanation, though not in the slide. The mean is not at .53 (although it is very close to that). What’s really at .53 is the mode, a.k.a. the peak, a.k.a. the most likely value. The Bayesian approach is to average the network’s predictions, at test time, where “average” means that we use network parameters according to the posterior distribution over parameter settings given the training data. Essentially, we’re averaging the predictions from many predictors: each possible parameter setting is a predictor, and the weight for that weighted average is the posterior probability of that parameter setting. “It’s helpful to know that whenever you see a squared error being minimized, you can make a probabilistic interpretation of what’s going on, and in that probabilistic interpretation, you’ll be maximizing the  log probability under a Gausian.” So the proper Bayesian approach, is to find the full posterior distribution over all possible weight vectors. If there’s more than a handful of weights, that’s hopelessly difficult when you have  a non-linear net. Bayesians have a lot of ways of  approximating this distribution, often using Monte Carlo methods.  But for the time being, let’s try and do something simpler.  Let’s just try to find the most probable weight vector.  So the single setting of the weights that’s most probable given the prior knowledge we have and given the data. So what we’re going to try and do is find  an optimal value of W by starting with some random weight vector, and then  adjusting it in the direction that improves the probability of that weight  factor given the data. It will only be a local optimum. The Bayesian interpretation of weight -log I D) = —logp(D I W) 1 (yc -tc)2 1)\nassuming that the model makes a Gaussian prediction — log p(W) 20w t assuming a for the weig"
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#supervised-maximum-likelihood-learning",
    "href": "notes/dnn/dnn-09/l_09.html#supervised-maximum-likelihood-learning",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Supervised Maximum Likelihood Learning",
    "text": "Supervised Maximum Likelihood Learning\nIn this video, we use Bayesian thinking (which is widely accepted as very reasonable) to justify weight decay (which may sound like an arbitrary hack). Maximum A Posteriori (MAP) learning means looking for that setting of the network parameters that has greatest posterior probability given the data. As such it’s somewhat different from the simpler “Maximum Likelihood” learning, where we look for the setting of the parameters that has the greatest likelihood term: there, we don’t have a prior over parameter settings, so it’s not very Bayesian at all. Slide 1 introduces Maximum Likelihood learning. Try to understand well what that has to do with the Bayesian “likelihood term”, before going on to the next slide. The reason why we use Gaussians for our likelihood and prior is that that makes the math simple, and fortunately it’s not an insane choice to make. However, it is somewhat arbitrary. 10:15: Don’t worry about the absence of the factor 1/2 in the weight decay strength. It doesn’t change the story in any essential way.\n\n\n\nlimiting weights"
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#footnotes",
    "href": "notes/dnn/dnn-09/l_09.html#footnotes",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Footnotes",
    "text": "Footnotes\n\n\na bit cheeky consider the massive capacity of most NN↩︎\nlike the number of hidden units or the size of the weight penalty↩︎"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html",
    "href": "notes/dnn/dnn-07/l07c.html",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n:::"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#a-good-toy-problem-for-a-recurrent-network",
    "href": "notes/dnn/dnn-07/l07c.html#a-good-toy-problem-for-a-recurrent-network",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "A good toy problem for a recurrent network",
    "text": "A good toy problem for a recurrent network"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#the-algorithm-for-binary-addition",
    "href": "notes/dnn/dnn-07/l07c.html#the-algorithm-for-binary-addition",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "The algorithm for binary addition",
    "text": "The algorithm for binary addition\n\n\n\nFinite State Automaton\n\n\nThis is a finite state automaton. It decides what transition to make by looking at the next column. It prints after making the transition. It moves from right to left over the two input numbers."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#a-recurrent-net-for-binary-addition",
    "href": "notes/dnn/dnn-07/l07c.html#a-recurrent-net-for-binary-addition",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "A recurrent net for binary addition",
    "text": "A recurrent net for binary addition\n\n\nThe network has two input units and one output unit.\nIt is given two input digits at each time step.\nThe desired output at each time step is the output for the column that was provided as input two time steps ago.\n\nIt takes one time step to update the hidden units based on the two input digits.\nIt takes another time step for the hidden units to cause the output"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#the-connectivity-of-the-network",
    "href": "notes/dnn/dnn-07/l07c.html#the-connectivity-of-the-network",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "The connectivity of the network",
    "text": "The connectivity of the network\n\nThe 3 hidden units are fully interconnected in both directions. - This allows a hidden activity pattern at one time step to vote for the hidden activity pattern at the next time step. - The input units have feedforward connections that allow then to vote for the next hidden activity pattern."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#what-the-network-learns",
    "href": "notes/dnn/dnn-07/l07c.html#what-the-network-learns",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "What the network learns",
    "text": "What the network learns\n\nIt learns four distinct patterns of activity for the 3 hidden units. These patterns correspond to the nodes in the finite state automaton.\n\nDo not confuse units in a neural network with nodes in a finite state automaton. Nodes are like activity vectors.\nThe automaton is restricted to be in exactly one state at each time. The hidden units are restricted to have exactly one vector of activity at each time.\n\nA recurrent network can emulate a finite state automaton, but it is exponentially more powerful. With N hidden neurons it has 2^N possible binary activity vectors (but only N^2 weights)\n\nThis is important when the input stream has two separate things going on at once.\nA finite state automaton needs to square its number of states.\nAn RNN needs to double its number of units.\n\n\n\n\n\nFinite State Automaton"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html",
    "href": "notes/dnn/dnn-07/l07a.html",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n::: column-margin"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#getting-targets-when-modeling-sequences",
    "href": "notes/dnn/dnn-07/l07a.html#getting-targets-when-modeling-sequences",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Getting targets when modeling sequences",
    "text": "Getting targets when modeling sequences\n\nWhen applying machine learning to sequences, we often want to turn an input sequence into an output sequence that lives in a different domain.\n\nE. g. turn a sequence of sound pressures into a sequence of word identities.\n\nWhen there is no separate target sequence, we can get a teaching signal by trying to predict the next term in the input sequence.\n\nThe target output sequence is the input sequence with an advance of 1 step.\nThis seems much more natural than trying to predict one pixel in an image from the other pixels, or one patch of an image from the rest of the image.\nFor temporal sequences there is a natural order for the predictions.\n\nPredicting the next term in a sequence blurs the distinction between supervised and unsupervised learning.\n\nIt uses methods designed for supervised learning, but it doesn’t require a separate teaching signal."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#memoryless-models-for-sequences",
    "href": "notes/dnn/dnn-07/l07a.html#memoryless-models-for-sequences",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Memoryless models for sequences",
    "text": "Memoryless models for sequences\n\n\n\n\n\nMemoryless models\n\n\n\nAutoregressive models Predict the next term in a sequence from a fixed number of previous terms using delay taps.\nFeed-forward neural nets These generalize autoregressive models by using one or more layers of non-linear hidden units. e.g. Bengio’s first language model."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#beyond-memoryless-models",
    "href": "notes/dnn/dnn-07/l07a.html#beyond-memoryless-models",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Beyond memoryless models",
    "text": "Beyond memoryless models\n\nIf we give our generative model some hidden state, and if we give this hidden state its own internal dynamics, we get a much more interesting kind of model.\n\nIt can store information in its hidden state for a long time.\nIf the dynamics is noisy and the way it generates outputs from its hidden state is noisy, we can never know its exact hidden state.\nThe best we can do is to infer a probability distribution over the space of hidden state vectors.\n\nThis inference is only tractable for two types of hidden state model.\n\nThe next three slides are mainly intended for people who already know about these two types of hidden state model. They show how RNNs differ.\nDo not worry if you cannot follow the details."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#linear-dynamical-systems-engineers-love-them",
    "href": "notes/dnn/dnn-07/l07a.html#linear-dynamical-systems-engineers-love-them",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Linear Dynamical Systems (engineers love them!)",
    "text": "Linear Dynamical Systems (engineers love them!)\n\n\n\nlinear dynamic systems\n\n\n\nThese are generative models. They have a real valued hidden state that cannot be observed directly.\n\nThe hidden state has linear dynamics with Gaussian noise and produces the observations using a linear model with Gaussian noise.\nThere may also be driving inputs.\n\nTo predict the next output (so that we can shoot down the missile) we need to infer the hidden state.\n\nA linearly transformed Gaussian is a Gaussian. So the distribution over the hidden."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#hidden-markov-models-computer-scientists-love-them",
    "href": "notes/dnn/dnn-07/l07a.html#hidden-markov-models-computer-scientists-love-them",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Hidden Markov Models (computer scientists love them!)",
    "text": "Hidden Markov Models (computer scientists love them!)\n\n\n\nHidden Markov Models\n\n\n\nHidden Markov Models have a discrete one of-N hidden state. Transitions between states are stochastic and controlled by a transition matrix. The outputs produced by a state are stochastic.\n\nWe cannot be sure which state produced a given output. So the state is “hidden”.\nIt is easy to represent a probability distribution across N states with N numbers.\n\nTo predict the next output we need to infer the probability distribution over hidden states.\n\nHMMs have efficient algorithms for"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#a-fundamental-limitation-of-hmms",
    "href": "notes/dnn/dnn-07/l07a.html#a-fundamental-limitation-of-hmms",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "A fundamental limitation of HMMs",
    "text": "A fundamental limitation of HMMs\n\nConsider what happens when a hidden Markov model generates data.\n\nAt each time step it must select one of its hidden states. So with N hidden states it can only remember log(N) bits about what it generated so far.\n\nConsider the information that the first half of an utterance contains about the second half:\n\nThe syntax needs to fit (e.g. number and tense agreement).\nThe semantics needs to fit. The intonation needs to fit.\nThe accent, rate, volume, and vocal tract characteristics must all fit.\n\nAll these aspects combined could be 100 bits of information that the first half of an utterance needs to convey to the second half. 2^100"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#recurrent-neural-networks",
    "href": "notes/dnn/dnn-07/l07a.html#recurrent-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nrnns.png\n\n\n\nRNNs are very powerful, because they combine two properties:\n\nDistributed hidden state that allows them to store a lot of information about the past efficiently.\nNon-linear dynamics that allows them to update their hidden state in complicated ways.\n\nWith enough neurons and time, RNNs can compute anything that can be computed by your computer."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#do-generative-models-need-to-be-stochastic",
    "href": "notes/dnn/dnn-07/l07a.html#do-generative-models-need-to-be-stochastic",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Do generative models need to be stochastic?",
    "text": "Do generative models need to be stochastic?\n\nLinear dynamical systems and hidden Markov models are stochastic models.\n\nBut the posterior probability distribution over their hidden states given the observed data so far is a deterministic function of the data.\n\nRecurrent neural networks are deterministic.\n\nSo think of the hidden state of an RNN as the equivalent of the deterministic probability distribution over hidden states in a linear dynamical system or hidden Markov model. ## Recurrent neural networks\n\nWhat kinds of behavior can RNNs exhibit?\n\nThey can oscillate. Good for motor control?\nThey can settle to point attractors. Good for retrieving memories?\nThey can behave chaotically. Bad for information processing?\nRNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects.\n\nBut the computational power of RNNs makes them very hard to train.\n\nFor many years we could not exploit the computational power of RNNs despite some heroic efforts (e.g. Tony Robinson’s speech recognizer).\n\n\n\n\n\nMemoryless models\nlinear dynamic systems\nHidden Markov Models\nrnns.png"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html",
    "href": "notes/dnn/dnn-07/l07e.html",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n::: column-margin"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html#long-short-term-memory-lstm",
    "href": "notes/dnn/dnn-07/l07e.html#long-short-term-memory-lstm",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "Long Short Term Memory (LSTM)",
    "text": "Long Short Term Memory (LSTM)\n\nHochreiter & Schmidhuber (1997) solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).\nThey designed a memory cell using logistic and linear units with multiplicative interactions.\nInformation gets into the cell whenever its write gate is on.\nThe information stays in the cell so long as its keep gate is on.\nInformation can be read from the cell by turning on its read gate."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html#implementing-a-memory-cell-in-a-neural-network",
    "href": "notes/dnn/dnn-07/l07e.html#implementing-a-memory-cell-in-a-neural-network",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "Implementing a memory cell in a neural network",
    "text": "Implementing a memory cell in a neural network\n\nTo preserve information for a long time in the activities of an RNN, we use a circuit that implements an analog memory cell. - A linear unit that has a self-link with a weight of 1 will maintain its state.\n- Information is stored in the cell by activating its write gate. - Information is retrieved by activating the read gate. - We can backpropagate through this circuit because logistics have nice derivatives."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html#backpropagation-through-a-memory-cell",
    "href": "notes/dnn/dnn-07/l07e.html#backpropagation-through-a-memory-cell",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "Backpropagation through a memory cell",
    "text": "Backpropagation through a memory cell"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html#reading-cursive-handwriting",
    "href": "notes/dnn/dnn-07/l07e.html#reading-cursive-handwriting",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "Reading cursive handwriting",
    "text": "Reading cursive handwriting\n\nThis is a natural task for an RNN.\nThe input is a sequence of (x,y,p) coordinates of the tip of the pen, where p indicates whether the pen is up or down.\nThe output is a sequence of characters.\nGraves & Schmidhuber (2009) showed that RNNs with LSTM are currently the best systems for reading cursive writing.\n\nThey used a sequence of small images as input rather than pen coordinates. A demonstration of online handwriting recognition by an RNN with Long Short Term Memory (from Alex Graves)\n\nThe movie that follows shows several things:\nRow 1: This shows when the characters are recognized.\n\nIt never revises its output so difficult decisions are more delayed.\n\nRow 2: This shows the states of a subset of the memory cells.\n\nNotice how they get reset when it recognizes a character.\n\nRow 3: This shows the writing. The net sees the x and y coordinates.\n\nOptical input actually works a bit better than pen coordinates.\n\nRow 4: This shows the gradient backpropagated all the way to the x and y inputs from the currently most active character.\n\nThis lets you see which bits of the data are influencing the decision."
  },
  {
    "objectID": "qa_demo1.html",
    "href": "qa_demo1.html",
    "title": "Lecture 1 — Introduction to XAI",
    "section": "",
    "text": "good\n\n\n\n\n\nbetter\n\n\n \n\n\n\nhttps://www.youtube.com/watch?v=6qisPX7o-bg\n\n\n\n\n\ngood\nbetter"
  },
  {
    "objectID": "qa_demo1.html#series-poster",
    "href": "qa_demo1.html#series-poster",
    "title": "Lecture 1 — Introduction to XAI",
    "section": "",
    "text": "good\n\n\n\n\n\nbetter\n\n\n \n\n\n\nhttps://www.youtube.com/watch?v=6qisPX7o-bg\n\n\n\n\n\ngood\nbetter"
  },
  {
    "objectID": "dnn.html",
    "href": "dnn.html",
    "title": "Oren Bochman’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\nOren Bochman\n\n\nAug 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 1\n\n\nNotes for Deep learning focusing on the basics\n\n\n\nOren Bochman\n\n\nJul 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 10\n\n\nThis module we look at why it helps to combine multiple NN to improve generalization\n\n\n\nOren Bochman\n\n\nOct 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 11\n\n\nThis module deals with Boltzmann machine learning\n\n\n\nOren Bochman\n\n\nOct 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 12\n\n\nThis module deals with Boltzmann machine learning\n\n\n\nOren Bochman\n\n\nNov 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 13\n\n\nThe ups and downs of backpropagation\n\n\n\nOren Bochman\n\n\nNov 11, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 14\n\n\nNotes for Deep learning focusing on the basics\n\n\n\nOren Bochman\n\n\nNov 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 15\n\n\nNotes for Deep learning focusing on the basics\n\n\n\nOren Bochman\n\n\nDec 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 16\n\n\nNotes for Deep learning focusing on the basics\n\n\n\nOren Bochman\n\n\nDec 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 2\n\n\nNotes for Deep learning focusing on Perceptrons\n\n\n\nOren Bochman\n\n\nJul 16, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 3\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\nOren Bochman\n\n\nAug 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 4\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hinton on Coursera\n\n\n\nOren Bochman\n\n\nAug 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 5\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\nOren Bochman\n\n\nAug 17, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 6\n\n\nWe delve into mini-batch gradient descent as well as discuss adaptive learning rates.\n\n\n\nOren Bochman\n\n\nAug 23, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7\n\n\nThis module explores training recurrent neural networks\n\n\n\nOren Bochman\n\n\nSep 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7a\n\n\nModeling sequences — A brief overview\n\n\n\nOren Bochman\n\n\nSep 2, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7b\n\n\nTraining RNNs with back propagation\n\n\n\nOren Bochman\n\n\nSep 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7c\n\n\nA toy example of training an RNN\n\n\n\nOren Bochman\n\n\nSep 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7d\n\n\nWhy it is difficult to train an RNN\n\n\n\nOren Bochman\n\n\nSep 5, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7e\n\n\nTraining RNNs with back propagation\n\n\n\nOren Bochman\n\n\nSep 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 8\n\n\nWe continue our look at recurrent neural networks\n\n\n\nOren Bochman\n\n\nSep 11, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 9\n\n\nWe discuss strategies to make neural networks generalize better\n\n\n\nOren Bochman\n\n\nSep 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1a\n\n\nWhy do we need machine learning?\n\n\n\nOren Bochman\n\n\nJul 2, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1b\n\n\nNotes for Deep learning focusing on why do we need machine learning?\n\n\n\nOren Bochman\n\n\nJul 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1c\n\n\nNotes for Deep learning focusing on simple models of neurons\n\n\n\nOren Bochman\n\n\nJul 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1d\n\n\nNotes for Deep learning focusing on a simple example of learning learning?\n\n\n\nOren Bochman\n\n\nJul 5, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1e\n\n\nNotes for Deep learning focusing on the three types of learning\n\n\n\nOren Bochman\n\n\nJul 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2a\n\n\nNotes for Deep learning focusing on types of neural network architectures\n\n\n\nOren Bochman\n\n\nJul 17, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2b\n\n\nNotes for Deep learning focusing on Perceptrons, the first generation of neural networks.\n\n\n\nOren Bochman\n\n\nJul 18, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2c\n\n\nNotes for Deep learning focusing on a geometrical view of perceptrons\n\n\n\nOren Bochman\n\n\nJul 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2d\n\n\nNotes for Deep learning focusing on why the learning works?\n\n\n\nOren Bochman\n\n\nJul 20, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2e\n\n\nNotes for Deep learning focusing What Perceptrons can not do\n\n\n\nOren Bochman\n\n\nJul 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3a\n\n\nFocusing on Learning the weights of a linear neuron\n\n\n\nOren Bochman\n\n\nAug 2, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3b\n\n\nThe error surface for a linear neuron\n\n\n\nOren Bochman\n\n\nAug 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3c\n\n\nLearning the weights of a logistic output neuron\n\n\n\nOren Bochman\n\n\nAug 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3d\n\n\nThe back-propagation algorithm\n\n\n\nOren Bochman\n\n\nAug 5, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3e\n\n\nUsing the derivatives computed by backpropagation\n\n\n\nOren Bochman\n\n\nAug 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4a\n\n\nLearning to predict the next word\n\n\n\nOren Bochman\n\n\nAug 11, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4b\n\n\nA brief diversion into cognitive science\n\n\n\nOren Bochman\n\n\nAug 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4c\n\n\nAnother diversion — The Softmax output function\n\n\n\nOren Bochman\n\n\nAug 13, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4d\n\n\nNeuro-probabilistic language models\n\n\n\nOren Bochman\n\n\nAug 14, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4e\n\n\nWays to deal with the large number of possible outputs\n\n\n\nOren Bochman\n\n\nAug 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 5a\n\n\nWhy object recognition is difficult\n\n\n\nOren Bochman\n\n\nAug 18, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 5b\n\n\nWays to achieve viewpoint invariance\n\n\n\nOren Bochman\n\n\nAug 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 5c\n\n\nConvolutional neural networks for hand-written digit recognition\n\n\n\nOren Bochman\n\n\nAug 20, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 5d\n\n\nWhy object recognition is difficult\n\n\n\nOren Bochman\n\n\nAug 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6a\n\n\nOverview of mini-batch gradient descent\n\n\n\nOren Bochman\n\n\nAug 24, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6b\n\n\nA bag of tricks for mini-batch gradient descent\n\n\n\nOren Bochman\n\n\nAug 25, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6c\n\n\nThe momentum method\n\n\n\nOren Bochman\n\n\nAug 26, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6d\n\n\nAdaptive learning rates for each connection\n\n\n\nOren Bochman\n\n\nAug 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6e\n\n\nrmsprop - divide the gradient by a running average of its recent magnitude\n\n\n\nOren Bochman\n\n\nAug 28, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Some Questions\n\n\nUnresolved questions on Deep learning.\n\n\n\nOren Bochman\n\n\nDec 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks — Readings I for Lesson 10\n\n\nReview & summary of — Evaluation of Adaptive Mixtures of Competing Experts\n\n\n\nOren Bochman\n\n\nOct 2, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks — Readings II for Lesson 10\n\n\nReview & summary of — Improving neural networks by preventing co-adaptation of feature detectors\n\n\n\nOren Bochman\n\n\nOct 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nGlossary of terms for Deep Neural Networks\n\n\nGlossary of terms in Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\nOren Bochman\n\n\nAug 6, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html",
    "href": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html",
    "title": "Effective Approaches to Attention-based NMT",
    "section": "",
    "text": "deeplearning.ai"
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html#neural-machine-translation",
    "href": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html#neural-machine-translation",
    "title": "Effective Approaches to Attention-based NMT",
    "section": "§2 Neural Machine Translation:",
    "text": "§2 Neural Machine Translation:\nThis section provides a summary of the the NMT task using 4 equations:\nIn particular they note that in the decoder the conditional probability of the target given the source is of the form:\n\nlog \\space p(y \\vert x) = \\sum_{j=1}^m log \\space p (y_j \\vert y_{&lt;j} , s)\n\nWhere x_i are the source sentence and y_i are the target sentence.\n\np (y_j \\vert y{&lt;j} , s) = softmax (g(h_j))\n Here, h_j is the RNN hidden unit, abstractly computed as:\n\nh_j = f(h_{j-1},s)\n\nOur training objective is formulated as follows\n\nJ_t=\\sum_{(x,y)\\in D} -log \\space p(x \\vert y)\n\nWith D being our parallel training corpus."
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html#overview-of-attention",
    "href": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html#overview-of-attention",
    "title": "Effective Approaches to Attention-based NMT",
    "section": "§3 Overview of attention",
    "text": "§3 Overview of attention\n\n\n\npage 3\n\n\nNext they provide a recap of the attention mechanism to set their starting point:\n\nSpecifically, given the target hidden state h_t and the source-side context vector c_t, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:\n\n\n\\bar{h}_t = tanh(W_c[c_t;h_t])\n &gt; The attentional vector \\bar{h}_t is then fed through the softmax layer to produce the predictive distribution formulated as: p(y_t|y{&lt;t}, x) = softmax(W_s\\bar{h}_t)"
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html#global-attention",
    "href": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html#global-attention",
    "title": "Effective Approaches to Attention-based NMT",
    "section": "§3.1 Global attention",
    "text": "§3.1 Global attention\nThis is defined in §3.1 of the paper as:\n\n\\begin{align}\n   a_t(s) & = align(h_t,\\bar{h}_s)  \\newline\n   & = \\frac{ e^{score(h_t,\\bar{h}_s)} }{ \\sum_{s'} e^{score(h_t,\\bar{h}_s)} } \\newline\n   & = softmax(score(h_t,\\bar{h}_s))\n   \\end{align}\n where h_t and h_s are the target and source sequences and score() which is referred to as a content-based function as one of three alternative forms provided:\n\nDot product attention:\n\nscore(h_t,\\bar{h}_s)=h_t^T\\bar{h}_s\n\nThis form combines the source and target using a dot product. Geometrically this essentially a projection operation.\n\n\nGeneral attention:\n\nscore(h_t,\\bar{h}_s)=h_t^TW_a\\bar{h}_s\n\nthis form combines the source and target using a dot product after applying a learned attention weights to the source. Geometrically this is a projection of the target on a linear transformation of the source or scaled dot product attention as it is now known\n\n\nConcatenative attention:\n\nscore(h_t,\\bar{h}_s)=v_a^T tanh(W_a [h_t;\\bar{h}_s])\n\nThis is a little puzzling v_a^T is not accounted for and seems to be a learned attention vector which is projected onto the linearly weighted combination of the hidden states of the encoder and decoder.\nthey also mention having considered using a location based function\nlocation :\n\na_t = softmax(W_a h_t)\n\nwhich is just a linear transform of the hidden target state h_t"
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html#local-attention",
    "href": "posts/2021/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation/2021-03-21-review-of-effective-approaches-to-attention-based-neural-machine-translation.html#local-attention",
    "title": "Effective Approaches to Attention-based NMT",
    "section": "§3.2 Local Attention",
    "text": "§3.2 Local Attention\n in §3.2 they consider a local attention mechanism. This is a resource saving modification of global attention using the simple concept of applying the mechanism within a fixed sized window.\n\nWe propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word. This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al. (2015) to tackle the image caption generation task.\n\n\nOur local attention mechanism selectively focuses on a small window of context and is differentiable. … In concrete details, the model first generates an aligned position p_t for each target word at time t. The context vector c_t\n\nis then derived as a weighted average over the set of source hidden states within the window [p_t−D, p_t+D]; Where D is empirically selected.\nThe big idea here is to use a fixed window size for this step to conserve resources when translating paragraphs or documents - a laudable notion for times where LSTM gobbled up resources in proportion to the sequence length…\nThey also talk about monotonic alignment where p_t=t and predictive alignment\n\np_t=S\\cdot sigmoid(v_p^Ttanh(W_ph_t))\n\n\na_t(s)=align(h_t,\\bar{h}_s)e^{(-\\frac{(s-p_t)^2}{s\\sigma^2})}\n\nwith align() as defined above and\n\n\\sigma=\\frac{D}{2}\n\n\n\n\n\npage 5\n\n\nThe rest of the paper has details about the experiment and is of less interest\n\n\n\n\npage 6\n\n\n\n\n\n\npage 7\n\n\n\n In §5.4 In alignment quality\n\n\n\n\npage 9\n\n\nsome sample translations\n\n the references\n\n This is appendix A which shows the visualization of alignment weights.\n\n\n\n\ndeeplearning.ai\nrnn\nalignment-visulization\npage 1\npage 2\npage 3\npage 5\npage 6\npage 7\npage 9"
  },
  {
    "objectID": "posts/2021/2021-10-15-storytelling-and-other-essentials/2021-10-15-storytelling-and-other-essentials.html",
    "href": "posts/2021/2021-10-15-storytelling-and-other-essentials/2021-10-15-storytelling-and-other-essentials.html",
    "title": "Storytelling and other essentials",
    "section": "",
    "text": "Years of supporting Digital marketers have demonstrated the Importance of story telling. More so today when we can use the innovative tools of causal inference, which are rife with paradoxes and contradictory results. There are stories that need to be told, occasionally there are stories that you should keep to yourself but most of all when dealing with data you need to be honest. Few moments were less trying when you have have to tell the client that not only is the trend they presented to their boss without checking has reversed but that it had never happened. Data analysis can be very complicated and if you learn people cannot resist the temptations to inflate figures then you need"
  },
  {
    "objectID": "posts/2021/2021-10-15-storytelling-and-other-essentials/2021-10-15-storytelling-and-other-essentials.html#the-big-reversal",
    "href": "posts/2021/2021-10-15-storytelling-and-other-essentials/2021-10-15-storytelling-and-other-essentials.html#the-big-reversal",
    "title": "Storytelling and other essentials",
    "section": "The big reversal",
    "text": "The big reversal\nSome time back an analyst of an enterprise client asked me to look at their account and figure out if a recently launched service was a success or a failure. I had expected this call a while back. I had written the measurement plan after consulting with the system analyst then since the dev team had allocated three hours for implementing the document I was asked to help and implement it using a tag Managment solution and used the allocated three hours to verify all the edge cases. At this point the marketing department got an invoice and decided their budget was running low, as they had ended up paying for the dev teams oversight, and told me to postpone the reporting indefinitely. The ops did a blue green launch and then a full launch and then lots of data started coming in.\nAfter a few months of data collection the analyst looked at the data and saw lots of abandoned sessions. The measurement plan was of little help, instead of a 50 page presentation I had provided a long page of strangely named virtual pages referencing a two UML charts they had sent me. The plan had been one of the simplest I had turned out and the trick had been choosing just the right names. The fact that the service had many exit points and only three entry points was not helpful and that success could reset or terminate the sessions early. The terms marketing sieve or anti funnel come to mind. It was real difficult to see if end users were reaching a goal or giving up in transit. Reporting would have managed the aggregation of the data using a number of clever regular expressions and some more to allow one to drill down into the different modules. The analyst could not see almost any successful sessions. The problem was a second more complex service was now being finalized and Marketing needed to present the previous project to the CEO as part of a quarterly meeting.\nSo I used GA To find the results manually, I saved massive url and their screenshots to permit reproducing the analysis at a later day as any analysis we did not automate would have to be repeated and the clients would then compare to old data and it would invariably lead to contradictions. Alse the CRO team liked to provide highly optimistic reports of their success. This would lead to moments of great consternation when the trend suddenly reversed. The queries were run any differently the outcome would And convince managment when the analyst and later their department heads calls to finalize the presentation. The data ended up showing that both overall and for the three main use cases the service was a win for end users and had reduced a significant load at the call center at peak hours. This was one of the best results in the quarterly presentation and though we told the story we never did a reporting solution. The takeaway is that few analytics products reach production.\nData collected and cleaned was analyzed and reported and then comes the time to take action. But when we considered segments and sub segments the data recommended a opposing action. For almost every segmentation the recommended action reverses.The situation was that it was impossible and to understand if a recently launched product was a success or a failure and furthermore if this was due to the promotion or the product design, and what should the client do next, drop it, keep it and if so which channels to use and which market segments to proceed dfocus acquisition efforts. The elephant cannot come from the data alone.\nLike in many situations the client\nAnother situation is the A/B testing. Where clients interested in becoming more data driven are often tempted to cut a test short of a conclusive outcome and risk making a decision that will negatively impact their business instead of driving it forward.\nStorytelling also operates on a cognitive level helping transform the abstractions of data analysis into more engaging material."
  },
  {
    "objectID": "posts/2021/2021-11-12-language-models-and-explainability/2021-11-12-language-models-and-explainability.html",
    "href": "posts/2021/2021-11-12-language-models-and-explainability/2021-11-12-language-models-and-explainability.html",
    "title": "Language models and explainability",
    "section": "",
    "text": "word cloud"
  },
  {
    "objectID": "posts/2021/2021-11-12-language-models-and-explainability/2021-11-12-language-models-and-explainability.html#why-are-deep-language-models-sota",
    "href": "posts/2021/2021-11-12-language-models-and-explainability/2021-11-12-language-models-and-explainability.html#why-are-deep-language-models-sota",
    "title": "Language models and explainability",
    "section": "Why are deep language models SOTA?",
    "text": "Why are deep language models SOTA?\nIn the era of deep learning language model are trained and represented by neural networks. Neural language models have higher capacity for capturing evidence than other language models such as Markov chains. Another facet of higher capacity is that these models learn both the structure and what features are present in the data. I mentioned Markov chains as they are mathematically close to conditional probability distributions with simplifying assumption (similar to back off) which are models. Higher capacity allows neural language model to encode more details from the corpus for a given network size than other models. However, since storage is so cheap what this advantage translates to how well the model can recall and generalize from a limited amount of training data. We should note that neural nets are slow to train slow to evaluate, require lots of data and their high capacity often leads to overfitting as they will learn both signal and noise. Research has shown that for many NLP tasks using the many tricks, models are very simple and research has demonstrated that one can reach equivalent results by fine-tuning non-neural models with the same data. How have language models evolved in the last few years?\nRNNs (recurrent neural networks)\nThe initial breakthrough in NLP were in machine translation using RNNs (recurrent neural networks). But RNN turned out to be one of the most challenging type of models to train. Initially the main problem was to get RNN models to converge - they often entered a chaotic phase and then never leave it later as we learned to get these to converge better than the vanishing and exploding gradient problems came to light which in the case of word distribution was more challenging as the models are multiplicative and when run on GPUs that have low precision representation of floats leads to loss of information due to round off errors. Finally, RNN take longer to train as they grow longer as the models typically need to run N times in series (for N units) which makes poor use of the high parallelism offered by the GPU hardware. In retrospect, we can see that RNN are slow and difficult to train and tend to suffer from information due to an inability to allow information to be accessed over long distances. Some of these issues we addressed by LSTMs and later GRUs. These are more complicated architectures that aim to improve information routing. Using units like RELU and Batch normalization helped to reduce the vanishing and exploding gradients. Which let RNN become effective at a length of about 100 units. But they still had to run in series, and they still tend to lose information. This changed when attention mechanisms and transformers architecture were used in seq2seq models. These models could be evaluated for n-units in a single step. They were mathematically much simpler than LSTM and GRUs and could handle sequences of hundreds of units. This allowed them to pick up long distance structure of language that had eluded RNN based models. More recent improvements like the multi-headed attention and the reformer architecture have led to models capable of handling book long sequences in memory. Transformers are also much better at routing information over these very long sequences.\nAs a rule of thumb the best way to improve a model is to give it more data though this will also require increasing the size of the net and training longer. Transformers would be trained in an unsupervised fashion on web scale corpuses. Models like BER and GP3 soon became some of the biggest models developed for use in Deep Learning. These language models are more flexible that the joint probability models described above in a sense that their internal state seems to capture diverse aspects of both language and of the real world and allows them to perform well on many tasks they were not trained on.\nWhen is bigger not necessarily better?\nOne of the more fatuous points made in the literature is how almost anything that improves the model works as a regularizer - a mechanism that reduces overfitting. Neural nets are notorious for overfitting data due to their high capacity and many methods like early stopping and restoring model weights, adding penalties on kernels, weights, biases and outputs to the loss. Using drop out and batch normalization are some possibilities for use to combat overfitting.\n“Providing more data works well as a regularizer”. This is best understood by considering that language acquisition is many layers ability much like a linguistic onion. The first level might be the use of 1000 core words, a limited vocabulary but all the grammatical words. This is enough for a foreigner to communicate. Next one might master 3000 words get a better grasp of intermediate grammar, and morphology. We could communicate more freely and no longer need a learner’s dictionary. To pick up idioms and phrases that are used in a newspaper one might need to reach 5000 words. To master a technical area like business or medicine one needs about 15000 more words mostly unique to that field so at 20000. But 15,000-20,000 is also the amount of words used by native language speakers. A dictionary for English might have 150,000 words which would give one access to reading literature and even many obsolete words used in older religious works. After that we can consider encyclopedias full of technical information. Wikipedia has over 6,000,000 articles covering many technical and non-technical areas. News and Web sized corpus are even larger.\nWhen we are training a language model it would acquire the most common grammatical words and their grammar. It will, with enough data proceed to learn to acquire the semantics of core concepts in the language even with a limited corpus. At any corpus suited to a given level of words in our onion there will be some words from the next which appear too rarely to fully master. But by the time the model has a distributed representation of say 2000-8000 words and perhaps much sooner the model should be able to comprehend unknown words based on its context and other words most familiar for that context. Models like BERT are trained specifically to predict missing words from sentences.\nThis follows from the law of large numbers arises from a random walk. We know that real signals like features tend to aggregate over time while spurious signals tend to cancel out. The challenge is that each time we double our data we will add new features in the form of rare words phrases entities and even senses for words that have not been seen before. At the same time evidence will build up due to chance for phrases or grammatical structures. The net will try to learn all of these.\nLike technical term in a news corpus may we will make sense because the net is trained to penalize bad input and as more evidence is presented it will learn to tell apart spurious data patterns that arise due to chance from real patterns. The law of large numbers means that distributions tend - but it needs to see the real ones patterns in such quantity that their signals overpowers noise. (Adding more data also adds more chance that some spurious patterns will appear frequently)much more frequently.\nThe unreasonable effectiveness of transformers soon lead us to see that test used in evaluating the language skill, comprehension ability of these models are easily beaten by ML. We are not easily able to make tests that are easy for humans and almost impossible for models. Also, it can be challenging to tell text generated by these models (and cherry-picked by the developers) from text written by human writers. This has led many people to think that these models have become truly intelligent in the sense that they comprehend texts about the real world.\nDeeper scrutiny shows that neural language models are incapable of doing basic arithmetic which is not a language skill. They are also seemed to recall and recreate approximation of text that they have seen - much like 4-GRAM models trained on the works of Shakespeare - they will spew long sequences they remember. It is not impossible to make very smart models, but it is much easier to make a model that seems smarter than it is.\nAn early result in image processing comes to mind - a model that was able to tell apart tanks from trucks was able to do this by summing all the pixels because one group was taken on a sunny day and the other an overcast day. Image classifiers will learn to focus on a single pixel per class if such pixels are unique to each class. It becomes increasingly challenging as the data grows to ensure what is going on within the model.\nModels like GPT3 used massive compute and lots of data but were developed using poor methodology, so that success on downstream tasks could not be credited to anything other than having seen similar data. This leads more NLP practitioners to consider that the larger model sizes is not necessarily a better model in the sense that a smarter/more efficient representation should be able to provide better results. While there are general research groups like Open AI who build bigger model like GPT2 and GPT3 there are other researchers who work on\nSome known issues with neural networks are that they tend to overfit the data which means that every larger datasets are required to regularize this. Recent language models like GPT2, GP3 etc. use a very large number of parameters. Scrutiny of their output leads to output that appears to be written by humans. However, this is familiar issue from 4-grams or 5-grams N-grams models where sparsity of data (say the complete works of Shakespeare) leads to more and longer sequences being memorized by the (Markov chain model) and the output being a somewhat lossy compression of the corpus. As DNN will prefer to memorize and then retrieve the data rather than to model and infer. In the case of DNN with internet sized corpuses the researchers can easily fool themselves that the model is not stitching up its output from more examples rather than learning to model what is needed to represent the language. (Note Wikipedians also frequently stitch up articles on subject they are not very knowledgeable about and only domain experts can readily find the glaring errors that betray the ineptitude.)\nSpecifically we are now seeing and therefore expecting deep language models to learn the grammar, the lexicon, semantics, pragmatic commonsense knowledge of the world, and a rudimentary logical reasoning ability.\nClassic Linguistic theory postulates that an efficient representation of language should consist of a grammar and a lexicon. This approach was shown to make sense in computational area of morphology where finite state morphologies were most efficiently represented by specified and combining a grammar with a lexicon."
  },
  {
    "objectID": "posts/2021/2021-11-12-language-models-and-explainability/2021-11-12-language-models-and-explainability.html#a-fearful-symmetry",
    "href": "posts/2021/2021-11-12-language-models-and-explainability/2021-11-12-language-models-and-explainability.html#a-fearful-symmetry",
    "title": "Language models and explainability",
    "section": "A fearful symmetry",
    "text": "A fearful symmetry\nWe may even imagine a situation where we have separated the language into a number of constructs each with its hidden states and an emission matrix which converts the signal at the interface. Most signals get processed layer by layer but on occasion they might skip or cross."
  },
  {
    "objectID": "posts/2021/2021-11-12-language-models-and-explainability/2021-11-12-language-models-and-explainability.html#how-can-we-explicitly-process-higher-level-construct",
    "href": "posts/2021/2021-11-12-language-models-and-explainability/2021-11-12-language-models-and-explainability.html#how-can-we-explicitly-process-higher-level-construct",
    "title": "Language models and explainability",
    "section": "How Can we explicitly process higher level construct",
    "text": "How Can we explicitly process higher level construct\nAs pointed earlier models that model sequences using a probabilistic grammar rather than looking just at sequences is going to capture a much richer view of language.\nThere are a couple of reasons for this. Perhaps the more significant factor is the ability to capture much longer sequences which will expose the model to much richer wealth of signals. Grammar is typically represented using production rules which typically replace a symbol with two or more new symbols. These require moderately shorter sequences, and they also have a much lower cardinality for the symbols needed to represent the grammatical entities, at least when compared to the cardinality of the words in a language or its leading categories. For rich morphologies even more significantly. Which means that grammar should be learned much faster than a detailed lexicon. Since rules are so much shorter than sentences the grammar should be learned with significantly higher confidence at any given point.\n\n\n\nword cloud"
  },
  {
    "objectID": "posts/2021/2021-09-22-what-is-in-a-citation/2021-09-22-what-is-in-a-citation.html",
    "href": "posts/2021/2021-09-22-what-is-in-a-citation/2021-09-22-what-is-in-a-citation.html",
    "title": "What is in a citation?",
    "section": "",
    "text": "I want Wikipedia style inline citations in my blog but without the massive headaches of templates. Being able to push a citation into wikidata and then cite with Qid or a DOI would be useful too."
  },
  {
    "objectID": "posts/2021/2021-09-22-what-is-in-a-citation/2021-09-22-what-is-in-a-citation.html#tldr",
    "href": "posts/2021/2021-09-22-what-is-in-a-citation/2021-09-22-what-is-in-a-citation.html#tldr",
    "title": "What is in a citation?",
    "section": "",
    "text": "I want Wikipedia style inline citations in my blog but without the massive headaches of templates. Being able to push a citation into wikidata and then cite with Qid or a DOI would be useful too."
  },
  {
    "objectID": "posts/2021/2021-09-22-what-is-in-a-citation/2021-09-22-what-is-in-a-citation.html#requirements",
    "href": "posts/2021/2021-09-22-what-is-in-a-citation/2021-09-22-what-is-in-a-citation.html#requirements",
    "title": "What is in a citation?",
    "section": "Requirements",
    "text": "Requirements\nTerminology:\n\ncounter: a global variable which is incremented to generate the number of a citation\ninline citation which looks like: [[[1]]] or (auth_title_year)\n\na link to a reference\nthe doietc of a citation.\na pop up on hover on the link with the citation\n\nreference list containing:\n\nconfiguration for styling.\noptional access to an external bib.tex file.\nthe full detailed text of the reference with:\n\nan icon for the citation type\nan icon for pdf\nan icon for open access etc.\na link ^ or a sequence ^ a b c of links back up each citation for the reference using a letters.\n\n\nlabel:\n\ngenerated token in the using the template: ${auth}_${title}_${year} where:\n\nauth is last name, of the first author\ntitle is first non stop word\nthe label is the same for repeated use\n\n\nid:\n\ngenerated token in the using the template: ${auth}_${title}_${year}_${counter}\nthe id is unique per location on the page. Ideally, the reference list would be generated using a few simple template from a data structure. The citation could hold the full citation or if the citations is already used before - just using the label This is an anonymous citation reference  [1]  This is an named citation reference &lt;sup id=\"cite_ref-kag_5-0\" class=\"reference\"&gt;  &lt;a href=\"#cite_note-kag-5\"&gt;[5]&lt;/a&gt; &lt;/sup&gt; These should be replaced with a markdown notation but that is out of the current scope of the project. Primarily as I am using Jekyll which forces to use of kramdown and I am unlikely to replace it. However specifying a bibtex file in the front matter might be of some interest too.\n\n\n$] which renders to: &lt;ref-element cid=\"cite_note-kag-5\"&gt;\nThe reflist"
  },
  {
    "objectID": "posts/2021/2021-06-10-layout-models/2021-06-10-layout-models.html",
    "href": "posts/2021/2021-06-10-layout-models/2021-06-10-layout-models.html",
    "title": "TensorFlow probability",
    "section": "",
    "text": "In a recent google ai blog post an transformer based autoencoder is shown that can be used to learn and generate document layouts.\n\nUsing Variational Transformer Networks to Automate Document Layout Design\n\nNeural Design Network: Graphic Layout Generation with Constraints\nVariational Transformer Networks for Layout Generation Current approaches to layout generation use greedy search algorithms, such as\nbeam search\nnucleus sampling\ntop-k sampling\n\nLayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis\nAwesome OCR\nGuide to LayoutParser: A Document Image Analysis Python Library\n\n\n\n\nCitationBibTeX citation:@online{bochman2021,\n  author = {Bochman, Oren},\n  title = {TensorFlow Probability},\n  date = {2021-06-01},\n  url = {https://orenbochman.github.io/blog//posts/2021/2021-06-10-layout-models/2021-06-10-layout-models.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2021. “TensorFlow Probability.” June 1,\n2021. https://orenbochman.github.io/blog//posts/2021/2021-06-10-layout-models/2021-06-10-layout-models.html."
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html",
    "href": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html",
    "title": "Language Models Are Open Knowledge Graphs",
    "section": "",
    "text": "deeplearning.ai\nIn brief this is a fascinating subject and my interest are practical. I made a quick list of tasks that are my context for looking at what is going on in this paper."
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#cross-language-completion-task",
    "href": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#cross-language-completion-task",
    "title": "Language Models Are Open Knowledge Graphs",
    "section": "Cross language completion Task:",
    "text": "Cross language completion Task:\nMost wikidata entries are missing translation of entries in non latin scripts. Given a good cross language embedding it should be possible to map know entities across language barriers and find high confidence candidates for the missing non latin scripts.\nGenerating an open knowledge graph for common-sense knowledge (similar to cyc was about) - This requires an generating both an ontology and facts.\nA large language model would acquire both a representation of frequent entities and common relations. It should also learn to identify triplets using and more so if hints are provided. (Particularly a model like BERT whose training include prediction of multiple erased locations of a plaintext."
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#wikidata-task",
    "href": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#wikidata-task",
    "title": "Language Models Are Open Knowledge Graphs",
    "section": "Wikidata Task:",
    "text": "Wikidata Task:\nGenerating open knowledge graphs for like wikidata. This is an ontology and facts that could be used to cross language barriers, that when collected would appear in info-boxes to provide a structured view of similar entities and that maps entities to canonical links."
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#lexicon-generation-task",
    "href": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#lexicon-generation-task",
    "title": "Language Models Are Open Knowledge Graphs",
    "section": "Lexicon generation Task:",
    "text": "Lexicon generation Task:\nGenerating human/machine readable lexicons based on lexical relations. Using an linguistic ontology and examples extracted from dbpedia from wiktionary."
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#analogy-task",
    "href": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#analogy-task",
    "title": "Language Models Are Open Knowledge Graphs",
    "section": "Analogy task:",
    "text": "Analogy task:\nPropose triplets for uncommon articles based on analogous entries on articles from the same category. e.g. Birthdate, and parents names from Biographies. More formally: Given a ENTITY_1  \\quad Rel  \\quad NP_i from article on ENTITY_1 find ENTITY_2 Rel NP_j in article on ENTITY_2.\nThe algorithm works by extracting NP and analyzing attention matrices under constraints (masking look back and look ahead). The paper considers different layers but got good results for the final layer. That is probably because they are using the output embedding layer rather than looking at specific features which should be highly nonlinear and should fail for most cases. In fact recent research on attention heads indicates that most attention heads can be ignored\nOther approaches to mine the model might be to:\npredict using “NP_1 BLANK \\times N NP_2” as input and collect predictions.\nrank using\n\\alpha \\times probability(Relation) + \\beta \\times alignment score \\gamma \\times map score.\ngive the “BLANK relation BLANK”as input and collect predictions. keep high then try to map the entities.\nfine tune with Wikidata triplets + masking.\nThe analogy task uses embeddings which have an algebraic structure so that NP1 - NP2 \\approx RELATION so if we average via\n$ R = _{i=0}^{N} (NP_1 - NP_2)$\nfor a known relation we should be able to use it to go either way by adding to or by subtracting from a noun phrase. To get the result one needs to find the nearest neighbor in the embeddings. We can also query BERT with the whole sentence and check if it likes it or not.\nrank using\n\\alpha \\times probability(Relation) + \\beta \\times alignment score \\gamma \\times map score.\nThis should have an advantage that this is a fact retrieved by BERT from its memory rather than extracted from a text.\nAs well as more general features that would allow it both identify them via a context and to capture and contain features corresponding to both the tra still.\nNow the paper also discusses exploring new entities and relations. This reminded me on a quite different task focused on inducting morphology from a corpus based on nested extraction of templates. The point being is that a metric was used based on how well the model could compress the texts. In this case one might then score a\nNew Entities and Relations Task:\nFind entities in new classes, find new relations for new and old classes. We would score on these on the text by cross entropy for generating the text from the triplets less the total amount of relations and entities. I.e. this is a compression metric. Also salience is of interest - we should like to separate salient entities and relations from common-sense ones and from linguistics ones. (Perhaps with another classifier to tag their tensors).\nHere is an experiment that would be interesting to do:"
  },
  {
    "objectID": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#model-compression-task",
    "href": "posts/2021/2021-03-21-review-of-language-models-are-open-knowledge-graphs/2021-03-21-review-of-language-models-are-open-knowledge-graphs.html#model-compression-task",
    "title": "Language Models Are Open Knowledge Graphs",
    "section": "Model Compression task:",
    "text": "Model Compression task:\nSome people believe that a language model should specialize on the lexicon, morphology, grammar and pragmatics and that commonsense knowledge and a knowledge graph should be kept in separate units of intelligence. This is both because for a given capacity focusing on linguistics should give more performance than also storing a big database of facts and also we know that neural nets are not effective as databases.\nThere are techniques to compress models, essentially destroy/saturate neurons and retrain for an epoch. It should be fascinating to to try to find if instances of an entity exists (The Beatles, The Rolling Stones) in different neurons and then to remove all or almost all of them and retrain the network. More likely though one might use a pre-trained model and specialize on the corpus with a procedure that more frequently replaces NP with blanks, class tags, entity tags, and relations with ontological tags. Wikidata uses P[0-9]+ and Q[0-9]+ so training with these might bootstrap an ontological representation and align it with the network and let it organize using an representation which aligns with the given ontology.\nI’d also be interested in erasing as much of the first two from a language model while retaining a merged entity for the class to canonize the main relations that occur frequently supports an article. generation, bots to insert new data into the knowledge graph, translating knowledge graphs (mapping entities from non top 5 Wikipedia’s based on English), dropping knowledge graph data\n\n\n\ndeeplearning.ai"
  },
  {
    "objectID": "posts/2021/2021-04-25-bayesian-agent/2021-04-25-bayesian-agent.html",
    "href": "posts/2021/2021-04-25-bayesian-agent/2021-04-25-bayesian-agent.html",
    "title": "Bayesian agents",
    "section": "",
    "text": "Let’s try to define a KR framework for a Bayesian agent.\nA starting point is that instead of using large monolithic context frames which are difficult to handle, and inefficient to process we use a notion of context splitting to put together smaller, more flexible units. Embeddings that are good representation would be easier to easier to acquire on short context. And hopefully if with a little luck we could put them together into a package that feeds different attention heads allowing few-shot learning to proceed.\nerDiagram    \n    Thematic-Role-Frame {\n    verb move-object\n    agent Alice\n    object wedge\n    destination block\n    }    \n    Thematic-Role-Frame ||--|| State-Change-Frame : induces     \n    State-Change-Frame {\n    object alice\n    destination happy\n    }\nSo this is splits cause action from its outcome.\nBayesian Games and Sub\nIn a Bayesian game there are a number of constructs, lets put them into a pro-ontology\nerDiagram    \n    Action-Frame {\n    verb move-object\n    agent Alice\n    object bullet\n    destination Bob\n    tool gun\n    visibility universe\n    }\n    Action-Frame ||--|| State-Change-Frame : induces\n    Action-Frame ||--|| Agent-Frame_1 : participates\n    Action-Frame ||--|| Agent-Frame_2 : participates\n    Action-Frame ||--|| Information-Set-Frame : visibility\n    State-Change-Frame {\n    object alice\n    destination angry\n    info-set universe\n    payoffs Alice_1__Bob_0\n    }\n    Information-Set-Frame{\n    name universe\n    members Alice_Bob\n    }\n    Agent-Frame_1 {\n    name Alice\n    type robber\n    info json_data1\n    }\n    Agent-Frame_2 {\n    name Alice\n    type sherif\n    info json_data1\n    }\n    Info-Frame_0 {\n    Alice-Type any\n    Bob-Type any\n    }\n    Info-Frame_1 {\n    Alice-Type robber\n    }\n    Info-Frame_2 {\n    Bob-Type sheriff\n    }\n\n    State-Change-Frame ||--|| Information-Set-Frame : visibility\nHow many information sets for k players?\nThere are 2^k and they form the power set of \\{a \\space ... \\space k\\}.\nA Bayesian game usually start with a move by nature which assigns types to players m_n: p \\to \\tau."
  },
  {
    "objectID": "posts/2021/2021-04-25-bayesian-agent/2021-04-25-bayesian-agent.html#extensive-form-game",
    "href": "posts/2021/2021-04-25-bayesian-agent/2021-04-25-bayesian-agent.html#extensive-form-game",
    "title": "Bayesian agents",
    "section": "Extensive form game",
    "text": "Extensive form game\n\nThe most important TikZ command used to draw game trees is:\n\n...node(coordinate label)[drawing/style options] at(coordinate) {node texts}...;\n\nThe command child{} is used to specify a successor of a (parent) node. Note that if the style of a particular branch needs to be modified, such as adding texts to the branch or changing its color, edge from parent must be put after node{} and all of its children.\n\n\n\n\n\n\nComplete pooling"
  },
  {
    "objectID": "posts/2021/2021-04-25-bayesian-agent/2021-04-25-bayesian-agent.html#info-sets",
    "href": "posts/2021/2021-04-25-bayesian-agent/2021-04-25-bayesian-agent.html#info-sets",
    "title": "Bayesian agents",
    "section": "info sets",
    "text": "info sets\n\n\n\n\n\ninfo-sets\n\n\n\n\n\n\n\n\n\ninfo-sets-extended\n\n\n\n\n\n\n\nComplete pooling\ninfo-sets\ninfo-sets-extended"
  },
  {
    "objectID": "posts/2021/2021-04-03-ruby-installation-snafus/2021-04-03-ruby-installation-snafus.html",
    "href": "posts/2021/2021-04-03-ruby-installation-snafus/2021-04-03-ruby-installation-snafus.html",
    "title": "Jekyll take 3",
    "section": "",
    "text": "Jekyll is once again proving too be a trouble some fellow. I could not get the theme to work decently and I wanted to have a working local copy to see how things progress before uploading to git. I tried a third reboot of the site.\nI forked a new and promising theme called flexible jekyll.\nThen removed ruby and Jekyll and reinstalled per the instructions at https://jekyllrb.com/docs/installation/macos/ primarily to avoid using sudo to install gems.\nThis time Ruby got stuck on OpenSSL.\neventually I asked Google: rbenv 3.0.0 stuck install openssl mac"
  },
  {
    "objectID": "posts/2021/2021-04-03-ruby-installation-snafus/2021-04-03-ruby-installation-snafus.html#ruby-installation-pains-on-mac-os",
    "href": "posts/2021/2021-04-03-ruby-installation-snafus/2021-04-03-ruby-installation-snafus.html#ruby-installation-pains-on-mac-os",
    "title": "Jekyll take 3",
    "section": "",
    "text": "Jekyll is once again proving too be a trouble some fellow. I could not get the theme to work decently and I wanted to have a working local copy to see how things progress before uploading to git. I tried a third reboot of the site.\nI forked a new and promising theme called flexible jekyll.\nThen removed ruby and Jekyll and reinstalled per the instructions at https://jekyllrb.com/docs/installation/macos/ primarily to avoid using sudo to install gems.\nThis time Ruby got stuck on OpenSSL.\neventually I asked Google: rbenv 3.0.0 stuck install openssl mac"
  },
  {
    "objectID": "posts/2021/2021-04-03-ruby-installation-snafus/2021-04-03-ruby-installation-snafus.html#ruby-installation-fails-in-macos-catalina-1409",
    "href": "posts/2021/2021-04-03-ruby-installation-snafus/2021-04-03-ruby-installation-snafus.html#ruby-installation-fails-in-macos-catalina-1409",
    "title": "Jekyll take 3",
    "section": "Ruby installation fails in macOS Catalina #1409",
    "text": "Ruby installation fails in macOS Catalina #1409\nI use macOs Big Sur but this issue : https://github.com/rbenv/ruby-build/issues/1409 seemed relevant. I also noticed this:\n under one f the the answers. The fire works in particular.\nIt also demonstrated an interesting technique so I decided to make a note.\nYou see\n\nNote: the Homebrew version gets updated, the ruby-build version doesn’t That is a common theme with irksome installation. But the rub is in getting the new software work with the dependency you have or install rather then the broken one it is trying to fetch.\n\n\n# Setup Compiler paths for readline and openssl\n\nlocal READLINE_PATH=$(brew --prefix readline)\nlocal OPENSSL_PATH=$(brew --prefix openssl)\nexport LDFLAGS=\"-L$READLINE_PATH/lib -L$OPENSSL_PATH/lib\"\nexport CPPFLAGS=\"-I$READLINE_PATH/include -I$OPENSSL_PATH/include\"\nexport PKG_CONFIG_PATH=\"$READLINE_PATH/lib/pkgconfig:$OPENSSL_PATH/lib/pkgconfig\"\n\n# Use the OpenSSL from Homebrew instead of ruby-build\n\n# Note: the Homebrew version gets updated, the ruby-build version doesn't\n\nexport RUBY_CONFIGURE_OPTS=\"--with-openssl-dir=$OPENSSL_PATH\"\n\n# Place openssl$1.1 at the beginning of your PATH (preempt system libs)\n\nexport PATH=$OPENSSL_PATH/bin:$PATH\n\n# Load rbenv\n\neval \"$(rbenv init -)\"\n\n# Extract the latest version of Ruby so you can do this:\n\n# rbenv install $LATEST_RUBY_VERSION\n\nexport LATEST_RUBY_VERSION=$(rbenv install -l | grep -v - | tail -1)\n\n```zsh\nsource .zshrc\nrbenv install $LATEST_RUBY_VERSION"
  },
  {
    "objectID": "posts/2021/2021-04-06-jekyll-mathjax-3.0-fix/2021-04-06-jekyll-mathjax-3.0-fix.html",
    "href": "posts/2021/2021-04-06-jekyll-mathjax-3.0-fix/2021-04-06-jekyll-mathjax-3.0-fix.html",
    "title": "MathJax 3 fix for Jekyll hosted on Github pages",
    "section": "",
    "text": "less Jekyll and more Hyde\n\n\n\nHere is an issue I opened:\nI’ve had a too many issues to list trying to cram mathjax v.3 down kramdown’s throat without it choking latex code on a new jekyll blog hosted by github pages.\nHere are a few that if fixed should make it much simpler to use mathjax.\n\n| used for absolute values and norm trigger a markdown table.\n\ndesired behavior o not replace | in non markdown blocks i.e. within the MathJax delimiters\nmy workaround is replace | with instead of ‘|’ in equations\n\n\\\\ as line breaks are replaced with hard line breaks and then ignored within the MathJax delimiters\n\ndesired behavior o not replace \\\\ in non markdown blocks i.e. within the mathjax delimiters\nworkaround use instead of ’\\ in math\n\nmathjax now looks for its delimiters rather then the tags being used.\n\nadd a math_engine_opts entry: say dollars_untaxed to suppress replacement of the mathjax $ delimiters\nmy workaround replace $ and $$ to say $ and $$ respectively. here is my current integration:\n\n\n&lt;script&gt;\n  window.MathJax = {\n    loader: {load: ['[tex]/ams']},\n    tex: {\n      inlineMath: [['$', '$']],\n      displayMath: [['$$', '$$'],],\n      tags: 'ams',\n      multlineWidth: '85%',      // width of multline environment\n      packages: {'[+]': ['ams']}},\n      formatError:               // function called when TeX syntax errors occur\n        (jax, err) =&gt; jax.formatError(err),\n  };\n&lt;/script&gt;\n&lt;script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"&gt;&lt;/script&gt;\n&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax$3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;`\nnote: migration to Quarto has rended this issue obsolete!?\n\n\n\nless Jekyll and more Hyde\n\n\nCitationBibTeX citation:@online{bochman2021,\n  author = {Bochman, Oren},\n  title = {MathJax 3 Fix for {Jekyll} Hosted on {Github} Pages},\n  date = {2021-04-04},\n  url = {https://orenbochman.github.io/blog//posts/2021/2021-04-06-jekyll-mathjax-3.0-fix/2021-04-06-jekyll-mathjax-3.0-fix.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2021. “MathJax 3 Fix for Jekyll Hosted on Github\nPages.” April 4, 2021. https://orenbochman.github.io/blog//posts/2021/2021-04-06-jekyll-mathjax-3.0-fix/2021-04-06-jekyll-mathjax-3.0-fix.html."
  },
  {
    "objectID": "posts/2021/2021-09-08-stochastic-gradient-descent/2021-09-08-stochastic-gradient-descent.html",
    "href": "posts/2021/2021-09-08-stochastic-gradient-descent/2021-09-08-stochastic-gradient-descent.html",
    "title": "Stochastic Gradient Descent - The good parts",
    "section": "",
    "text": "What’s the best way to scale a mountain ? Stochastic gradient descent (SGD) seems is quite a mouthful and it is a class of a optimization algorithms used to train deep neural networks. In these we consider the loss function a mountain that needs climbing and being lazy we make it negative so we only have to go down. Confused? If you are here you probably knew that already this article is both an overview and a repository of answers to questions I came up with about SGD over the years as I learned it and thought it."
  },
  {
    "objectID": "posts/2021/2021-09-08-stochastic-gradient-descent/2021-09-08-stochastic-gradient-descent.html#out-of-core-learning",
    "href": "posts/2021/2021-09-08-stochastic-gradient-descent/2021-09-08-stochastic-gradient-descent.html#out-of-core-learning",
    "title": "Stochastic Gradient Descent - The good parts",
    "section": "Out of core learning",
    "text": "Out of core learning\nMany of the training loops I wrote for NLP transformers use an out-of-core approach. This simply means there is a low level generator for the data set we cannot store in memory and there is a higher level loop or generator that uses an index or a hash for the data and fits in memory where we manage metadata such class, error rates, and other metadata. We then manage shuffling and augmentation and batching at the second level . One Example is when working with reformer style transformers in NLP we want to bucket together rows of certain length. Another aspect of using this approach is due to augmentation where we want to generate and cache augmented cycles. Of course all these make the notion of an epoch more complicated. But dea has to do wThis makes the notion"
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html",
    "title": "Multilevel Models",
    "section": "",
    "text": "Multilevel (hierarchical) modeling is a generalization of linear and generalized linear modeling in which regression coefficients are themselves given a model, whose parameters are also estimated from data. We illustrate the strengths and limitations of multilevel modeling through an example of the prediction of home radon levels in U.S. counties. The multilevel model is highly effective for predictions at both levels of the model, but could easily be misinterpreted for causal inference. Gelman, A. (2006)."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#fundamental-concepts",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#fundamental-concepts",
    "title": "Multilevel Models",
    "section": "Fundamental Concepts:",
    "text": "Fundamental Concepts:\nWhat is the difference between fixed effect, random effect and mixed effect models?\n\nUnder the Fixed Effect model we assume that the true effect size for all individuals is identical, and the only reason the effect size varies between studies is sampling error (error in estimating the effect size). Fixed effects are estimated using least squares (or, more generally, maximum likelihood)\nUnder the Random Effect or Population Average Effects* model we assume that the true effect size varies for each individuals and has a population statistic. random effects are estimated with shrinkage (“linear unbiased prediction” in the terminology of Robinson, 1991)\nMultilevel (hierarchical) models get their name from their functional form which relates higher level features depend on the lower level ones.\nPooling\nShrinkage AKA Partial-Pooling county specific means are pulled toward the population mean. More generally shrinkage is predictions shifting towards the population average. See Shrinkage in Mixed Effects Models\nSmoothing of uncertainty: Uncertainty about the county-specific means is lower (sometimes much lower) than if these parameters were estimated independently. (Better than the non-pooling model)\nProportional borrowing of information: Shrinkage and uncertainty reduction do not occur uniformly - information-poor counties must borrow a great deal of information from the other counties, while information-rich counties do not.\nnon-IID or Multicolinearity - Multilevel models can handle data at different levels that would be\nHomoscedasticity of the data can be handled by the model (when gradients are allowed to vary)."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#the-data",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#the-data",
    "title": "Multilevel Models",
    "section": "The Data",
    "text": "The Data\nThis is based on the dataset from ‘Radon contamination’ (Gelman and Hill 2006). Radon is a radiactive gas that is a by product of radioactive decay of uranium within the soil. The EPA did a study of radon levels in 80,000 houses. The two important predictors are:\n\nMeasurement in the basement or the first floor (radon higher in basements)\nCounty uranium level (positive correlation with radon levels).\n\nThe ‘natural’ hierarchy in the data is:\n\nthe state\ncounty level,\nhouse.\n\nIn these different models we are considering how to share information on within and between groups. This sharing of data is called pooling and is a type of learning mechanism. (It is the same pooling from game theory’s pooling equilibrium.)\nOne way to organize the different models is by increasing number of parameters.\n\nPlate notation remainder:\nRecall that plate notation is used in bayesian graphical model with repeating entities:\n\narrows indicate a dependency relation.\nfilled nodes are observed variables.\nempty nodes are latent variables\nsmall nodes are parameters\nnodes in plates/boxes with index = N repeat N times."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#complete-pooling-model",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#complete-pooling-model",
    "title": "Multilevel Models",
    "section": "Complete Pooling Model:",
    "text": "Complete Pooling Model:\n\n\n\npooled-plate-model\n\n\nThis is the simplest of all where one treat all counties the same, and estimate a single radon level.\nThe is a parametric model with 2 parameters, with alpha giving state wide radon level at the basements and \\beta the adjustment for going up a floor (there are only two floor and beta is the gradient associated with drop of levels when going up a floor.)\ny_i=\\alpha + \\beta x_i + \\epsilon_i\nwhere:\n\ni is the house number (919)\nx is the floor of the house\ny log radon level\n\\alpha is the intercept parameter corresponding to a base radon level. In this model it’s a fixed effect across all counties.\n\\beta gradient parameter - corresponding to drop of radon level between floors. Again it’s a fixed effect across all counties.\n\\epsilon is an error term may represent measurement error, temporal within-house variation, or variation among houses.\n\nthe model model:\n\n\\alpha ~ N(0,10^5)\n\\beta ~ N(0,10^5)\n\\sigma ~ HalfCauchy(0,5)\ny_i ~ $N(+ x_i,^2) $\n\nThe main issue with this model is that is ignores variation between counties. When we pool our data, we lose the information that different data points came from different counties. This means that each radon-level observation is sampled from the same probability distribution. Such a model fails to learn any variation in the sampling unit that is inherent within a group (e.g. a county). It only accounts for sampling variance."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#non-pooling-model",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#non-pooling-model",
    "title": "Multilevel Models",
    "section": "Non-Pooling Model:",
    "text": "Non-Pooling Model:\n\n\n\nunpooled-plate-model\n\n\nModel radon in each county independently.\n\ny_i=\\alpha_{j[i]} + \\beta x_{j[i]} + \\epsilon_i\n\nwhere where:\n\nj is the county number (1,…,85)\ni is the house number (1,…,919) - though just the ones in the county\nx is the floor of the house\ny log radon measurement\n\\alpha intercept or bias parameter\n\\beta gradient parameter\n\\epsilon error term\n\nwe model:\n\n[ \\alpha_j ~ N(0,10^5) for j in counties]\n[ y_i ~ $N $ for i in houses ] with parameters:\n\n\\beta ~ N(0,10^5)\n\\sigma ~ HalfCauchy(0,5)\n\n\nproviding us with a parametric model with 920 parameters.\nNext one would use MCMC to learn which parameters minimize the total error.\nThe main issue with this model is that there are lots of parameters and in some counties very few point to estimate them. (for some just two). The unpolled estimates are difficult to trust.\nWhen we analyze data unpooled, we imply that they are sampled independently from separate models. At the opposite extreme from the pooled case, this approach claims that differences between sampling units are too large to combine them:"
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#partial-pooling",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#partial-pooling",
    "title": "Multilevel Models",
    "section": "Partial Pooling",
    "text": "Partial Pooling\n\n\n\npartially-pooled-plate-model\n\n\nIn a hierarchical model, parameters are viewed as a sample from a population distribution of parameters considering them as neither entirely different or exactly the same. This is known as partial pooling.\nrecall: y_i=\\alpha_{j[i]} + \\beta x_{j[i]} + \\epsilon_i\nwe now want to estimate an county level using a weighed average between the county level and the over all level average. \\hat{\\alpha}_j \\approx \\frac{(n_j/\\sigma_y^2)\\bar{y}_j +\n(1/\\sigma_{\\alpha}^2)\\bar{y}}{(n_j/\\sigma_y^2) + (1/\\sigma_{\\alpha}^2)}\nwhere where:\n\nj is the county number (1,…,85)\ni is the house number (1,…,919) - though just the ones in the county\ny log radon measurement\n\\alpha intercept or bias parameter\n\\beta gradient parameter\n\\epsilon error term\n\nwe model:\n\n[ \\alpha_j ~ N(\\mu_a,\\sigma_a) for j in counties] with parameters:\n\n\\mu_a ~ N(0,10^5)]\n\\sigma_a ~ HalfCauchy(0,5)\n\n[ y_i ~ $N $ for i in houses ] with parameters:\n\n\\mu_y ~ N(0,10^5)] (ignored)\n\\sigma_y ~ HalfCauchy(0,5)\n\n\nWith partial pooling one expects:\n\nEstimates for counties with smaller sample sizes will shrink towards the state-wide average.\nEstimates for counties with larger sample sizes will be closer to the unpooled county estimates."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#varying-intercepts",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#varying-intercepts",
    "title": "Multilevel Models",
    "section": "Varying Intercepts",
    "text": "Varying Intercepts\n\nWe now a consider a more complex model that allows intercepts to vary across county, according to a random effect.\n\ny_i = \\alpha_{j[i]} + \\beta x_{i} + \\epsilon_i\n\nwhere\n\n\\epsilon_i \\sim N(0, \\sigma_y^2)\n\nand the intercept random effect:\n\n\\alpha_{j[i]} \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)\n\nThe slope \\beta, which lets the observation vary according to the location of measurement (basement or first floor), is still a fixed effect shared between different counties. As with the the unpooling model, we set a separate intercept for each county, but rather than fitting separate least squares regression models for each county, multilevel modeling shares strength among counties, allowing for more reasonable inference in counties with little data."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#varying-slopes",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#varying-slopes",
    "title": "Multilevel Models",
    "section": "Varying Slopes",
    "text": "Varying Slopes\n\nIn this model, one posits that counties vary according to how the location of measurement (basement or first floor) influences the radon reading. In this case the intercept \\alpha is shared between counties.\n\ny_i = \\alpha + \\beta_{j[i]} x_{i} + \\epsilon_i\n\n\nj is the county number. (1,…,85)\ni is the house number. (1,…,919) - though just the ones in the county\nx (0 for basement 1 for 1st) floor of the house.\ny log radon measurement\n\\alpha intercept parameter corresponding to a base radon level fixed across all counties.\n\\beta gradient parameter - corresponding to drop of radon level between floors\n\\epsilon_i error term considered at the house level."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#varying-intercepts-and-slopes",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#varying-intercepts-and-slopes",
    "title": "Multilevel Models",
    "section": "Varying Intercepts and Slopes",
    "text": "Varying Intercepts and Slopes\n\n\n\nvarying-intercepts-and-slopes-plate-model\n\n\nnote: the plate model would be clearer if a and b were in the same plate making the level more explicit.\nThe most general model allows both the intercept and slope to vary by county:\n\ny_i = \\alpha_{j[i]} + \\beta_{j[i]} x_{i} + \\epsilon_i\n\nwhere:\n\nj is the county number. (1,…,85)\ni is the house number. (1,…,919) - though just the ones in the county\nx (0 for basement 1 for 1st) floor of the house.\ny log radon measurement\n\\alpha_{ji} intercept parameter (corresponding to a base radon level) partially pooled across all counties.\n\\beta_{ji} gradient parameter (corresponding to drop of radon level between floors) partially pooled across all counties.\n\\epsilon_i error term considered at the house level."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#hierarchical-intercepts-and-slopes",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#hierarchical-intercepts-and-slopes",
    "title": "Multilevel Models",
    "section": "Hierarchical Intercepts and Slopes",
    "text": "Hierarchical Intercepts and Slopes\n\n\n\nhierarchical-intercepts-model\n\n\nA primary strength of multilevel models is the ability to handle predictors on multiple levels simultaneously. If we consider the varying-intercepts model above:\n\ny_i = \\alpha_{j[i]} + \\beta x_{i} + \\epsilon_i\n\nwe may, instead of a simple random effect to describe variation in the expected radon value, specify another regression model with a county-level covariate. Here, we use the county uranium reading u_j, which is thought to be related to radon levels:\n\n\\alpha_j = \\gamma_0 + \\gamma_1 u_j + \\zeta_j\n\n\n\\zeta_j \\sim N(0,\\sigma_{\\alpha}^2)\n\nThus, we are now incorporating a house-level predictor (floor or basement) as well as a county-level predictor (uranium).\nNote that the model has both indicator variables for each county, plus a county-level covariate.\nIn classical regression, this would result in collinearity. In a multilevel model, the partial pooling of the intercepts towards the expected value of the group-level linear model avoids this.\nGroup-level predictors also serve to reduce group-level variation \\sigma_{\\alpha}. An important implication of this is that the group-level estimate induces stronger pooling."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#correlation-amongst-levels",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#correlation-amongst-levels",
    "title": "Multilevel Models",
    "section": "Correlation amongst levels",
    "text": "Correlation amongst levels\n\n\n\ncorrelations-among-levels-plate-model\n\n\nIn some instances, having predictors at multiple levels can reveal correlation between individual-level variables and group residuals. We can account for this by including the average of the individual predictors as a covariate in the model for the group intercept.\n\n\\alpha_j = \\gamma_0 + \\gamma_1 u_j + \\gamma_2 \\bar{x} + \\zeta_j\n\nThese are broadly referred to as contextual effects."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#conclusions",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#conclusions",
    "title": "Multilevel Models",
    "section": "7 Conclusions",
    "text": "7 Conclusions\nBenefits of Multilevel Models:\n\nAccounting for natural hierarchical structure of observational data.\nEstimation of coefficients for (under-represented) groups.\nIncorporating individual- and group-level information when estimating group-level coefficients.\nAllowing for variation among individual-level coefficients across groups."
  },
  {
    "objectID": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#references",
    "href": "posts/2021/2021-05-16-mulitlevel-models/2021-05-16-mulitlevel-models.html#references",
    "title": "Multilevel Models",
    "section": "References",
    "text": "References\n\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (1st ed.). Cambridge University Press.\nGelman, A. (2006). Multilevel (Hierarchical) modeling: what it can and cannot do. Technometrics, 48(3), 432–435.\n\n\n\n\npooled-plate-model\nunpooled-plate-model\npartially-pooled-plate-model\nvarying-intercepts-and-slopes-plate-model\nhierarchical-intercepts-model\ncorrelations-among-levels-plate-model"
  },
  {
    "objectID": "posts/2021/2021-09-24-statistics-for-marketing-in-python/2021-09-24-statistics-for-marketing-in-python.html",
    "href": "posts/2021/2021-09-24-statistics-for-marketing-in-python/2021-09-24-statistics-for-marketing-in-python.html",
    "title": "Excel 2019 for Marketing Statistics in pandas",
    "section": "",
    "text": "Is there a port of Excel 2019 for Marketing Statistics ?"
  },
  {
    "objectID": "posts/2021/2021-09-24-statistics-for-marketing-in-python/2021-09-24-statistics-for-marketing-in-python.html#sample-size",
    "href": "posts/2021/2021-09-24-statistics-for-marketing-in-python/2021-09-24-statistics-for-marketing-in-python.html#sample-size",
    "title": "Excel 2019 for Marketing Statistics in pandas",
    "section": "Sample Size",
    "text": "Sample Size\nWe want a replacement of Excel’s =COUNT() function to get the sample size n - we need to get the row count in the data frame. Pandas also has a count() function but we should use the first item in the shape property.\ndf.shape[0]\n\npitfall:\ndf.count() will only return the count of non-NA/NaN rows for each column. ## Mean We want a replacement of Excel’s =AVERAGE() function to calculate the mean \\bar X according to: \\bar X = \\frac{\\sum X}{n}\n\n# Print the data frame\ndf.mean(axis = 0)"
  },
  {
    "objectID": "posts/2021/2021-09-24-statistics-for-marketing-in-python/2021-09-24-statistics-for-marketing-in-python.html#standard-deviation",
    "href": "posts/2021/2021-09-24-statistics-for-marketing-in-python/2021-09-24-statistics-for-marketing-in-python.html#standard-deviation",
    "title": "Excel 2019 for Marketing Statistics in pandas",
    "section": "Standard Deviation",
    "text": "Standard Deviation\nWe want a replacement of Excel’s =STDEV() function to calculate the mean \\bar X according to:\n SD = \\sqrt { \\frac{\\sum {(X-\\bar X)^2}}{n-1}}\ndf.std(axis = 0)"
  },
  {
    "objectID": "posts/2021/2021-09-24-statistics-for-marketing-in-python/2021-09-24-statistics-for-marketing-in-python.html#standard-error-of-the-mean",
    "href": "posts/2021/2021-09-24-statistics-for-marketing-in-python/2021-09-24-statistics-for-marketing-in-python.html#standard-error-of-the-mean",
    "title": "Excel 2019 for Marketing Statistics in pandas",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean\nIn this case there is no built in formula in excel for this. It would be calculated with an ugly formula that looks like: =STDEV()\\SQRT(COUNT()) to calculate the SE according to:  SE =  \\frac{SD}{\\sqrt{n}}\n\n# find standard error of the mean of all the columns\ndf.sem(axis = 0)"
  },
  {
    "objectID": "posts/2021/2021-04-07-linkage/2021-04-07-linkage.html",
    "href": "posts/2021/2021-04-07-linkage/2021-04-07-linkage.html",
    "title": "Linkage 2021-04-07",
    "section": "",
    "text": "Cleaning up my Tabs:\n\nData science stuff\n\nUSDA watercolor fruit\ngg2plot theme based on this\n[data science at the command line]\nhttps://github.com/Perishleaf?tab=repositories\ncreate an info graphic using matplotlib\nhttps://www.scmp.com/infographics/article/1243242/rain-patterns-hong-kong something people wanted to reproduce\naudience vs. critics - would be more intersting to automate the data collection of this stuff with a little bit of scraping.\ninformation is beautiful data\ncocktails and cocktails and the source.\ninfographic in python on SO\nautomating inkscape\nClipping and Masking in CSS\nLSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks\nA deductive database based on aristotelian logic\nUnderstanding the Semantic Content of Sparse Word Embeddings Using a Commonsense Knowledge Base\n\nMathJax Stuff\n\nok latex could even be faster with these macros setup to fire\nanother mathematically include blogger and Wikipedian\n\nJekyll Stuff:\n\nrocket\n\nCartoons / Comics story telling\n\nannotate images with speech bubbles\n\nlearning resources Five-step Strategy for Student Success with Online Learning\n\n\n\n\nCitationBibTeX citation:@online{bochman2021,\n  author = {Bochman, Oren},\n  title = {Linkage 2021-04-07},\n  date = {2021-04-07},\n  url = {https://orenbochman.github.io/blog//posts/2021/2021-04-07-linkage/2021-04-07-linkage.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2021. “Linkage 2021-04-07.” April 7, 2021.\nhttps://orenbochman.github.io/blog//posts/2021/2021-04-07-linkage/2021-04-07-linkage.html."
  },
  {
    "objectID": "posts/2021/2021-12-07-attention-for-sensor-fusion/2021-12-07-attention-for-sensor-fusion.html",
    "href": "posts/2021/2021-12-07-attention-for-sensor-fusion/2021-12-07-attention-for-sensor-fusion.html",
    "title": "Attention for sensor fusion",
    "section": "",
    "text": "A typical task in sequence to sequence modeling is machine translation. The standard autoencoder architecture has one encoder and one decoder. The encoder compresses the input signal into a hidden state representation and the decoder reproduces it. The decoder might have to reproduce the same sequence, or it might have to generate it in a different language or even in a new modality say as an image. All things being equal models should learn the simplest representation. For seq2seq (sequence to sequence) modeling this is a one to one mapping. Simply learning to memorize all the sentences and their translations in a single layer look up table. To alleviate this tendency to force learning over memorization techniques like bottlenecks, and drop out are used.\nBottlenecks require the encoded or hidden state to be shorter than the source sentences. But since natural language is highly redundant it is fairly easy for the model to face bottlenecks by simply learning to represent the language more efficiently and still memorize as much as it can. We call these new representations embeddings and acknowledge that they are a step in the right direction. However, one still want to reduce the tendency to memorize input output pairs while promoting the acquisition of more general translations mappings. A second way this can be done is to use drop out to destroy parts of the input or parts of the network forcing the representation to be more redundant. These are just two of a family of methods called regularization which are used to reduce overfitting, which is a term for the tendency to memorize both signal and noise.\nFor example, by encoding German text and then decoding English text one could perform machine translation. In this scenario we want our model to learn an abstract rotation matrix that lets us look up words and phrases in context (i.e. specific word senses) and get their translation. We call it a rotation since it is supposed to more or less preserve the order and length of the semantic sequence. Of course beyond learning the phrase book there is a second part to translation which is to transform the input sequence to account for differences in grammar of the output language. In this case one might envision a more abstract mapping that preserves different phrases in the text accounts for their change across languages. This mapping must be be less trivial when the semantic relation one wishes to preserving cross different interfaces such as phonetic, phonology, morphology, syntax, and pragmatics. Abstractly each may be a might be a simple mapping. In reality however they can also reorder the sentence and, they may need to be applied in the correct order, or simultaneously. Particularly challenging are cases where an explicitly marked state in the target language e.g. gender agreement is unmarked in the source sequence. In many cases errors arises when multiple generated sequences are available to the decoder. The typical beam search algorithm, will pick the most probable sequence where a better translator would proceed by eliminating options by referencing certain contexts in the surrounding text.\nOne might envision that generating grammatical sentences from a hidden state, may benefit from unsupervised training the decoder on language modeling tasks as these should boost it ability to approximate grammar by penalizing ungrammatical sequences. This has the advantage of being able to train on a corpus that are significantly larger than the typical parallel corpus. And indeed having more data has a regularizing effect, as it helps the model to better discriminate recurring patterns and effectively forget more of the noise it has learned. However, as mentioned above to learn to generate and or eliminate sequences based on what can be long term association in the text is a challenge which requires attention mechanism."
  },
  {
    "objectID": "posts/2021/2021-12-07-attention-for-sensor-fusion/2021-12-07-attention-for-sensor-fusion.html#an-overview-of-the-autoecoder.",
    "href": "posts/2021/2021-12-07-attention-for-sensor-fusion/2021-12-07-attention-for-sensor-fusion.html#an-overview-of-the-autoecoder.",
    "title": "Attention for sensor fusion",
    "section": "",
    "text": "A typical task in sequence to sequence modeling is machine translation. The standard autoencoder architecture has one encoder and one decoder. The encoder compresses the input signal into a hidden state representation and the decoder reproduces it. The decoder might have to reproduce the same sequence, or it might have to generate it in a different language or even in a new modality say as an image. All things being equal models should learn the simplest representation. For seq2seq (sequence to sequence) modeling this is a one to one mapping. Simply learning to memorize all the sentences and their translations in a single layer look up table. To alleviate this tendency to force learning over memorization techniques like bottlenecks, and drop out are used.\nBottlenecks require the encoded or hidden state to be shorter than the source sentences. But since natural language is highly redundant it is fairly easy for the model to face bottlenecks by simply learning to represent the language more efficiently and still memorize as much as it can. We call these new representations embeddings and acknowledge that they are a step in the right direction. However, one still want to reduce the tendency to memorize input output pairs while promoting the acquisition of more general translations mappings. A second way this can be done is to use drop out to destroy parts of the input or parts of the network forcing the representation to be more redundant. These are just two of a family of methods called regularization which are used to reduce overfitting, which is a term for the tendency to memorize both signal and noise.\nFor example, by encoding German text and then decoding English text one could perform machine translation. In this scenario we want our model to learn an abstract rotation matrix that lets us look up words and phrases in context (i.e. specific word senses) and get their translation. We call it a rotation since it is supposed to more or less preserve the order and length of the semantic sequence. Of course beyond learning the phrase book there is a second part to translation which is to transform the input sequence to account for differences in grammar of the output language. In this case one might envision a more abstract mapping that preserves different phrases in the text accounts for their change across languages. This mapping must be be less trivial when the semantic relation one wishes to preserving cross different interfaces such as phonetic, phonology, morphology, syntax, and pragmatics. Abstractly each may be a might be a simple mapping. In reality however they can also reorder the sentence and, they may need to be applied in the correct order, or simultaneously. Particularly challenging are cases where an explicitly marked state in the target language e.g. gender agreement is unmarked in the source sequence. In many cases errors arises when multiple generated sequences are available to the decoder. The typical beam search algorithm, will pick the most probable sequence where a better translator would proceed by eliminating options by referencing certain contexts in the surrounding text.\nOne might envision that generating grammatical sentences from a hidden state, may benefit from unsupervised training the decoder on language modeling tasks as these should boost it ability to approximate grammar by penalizing ungrammatical sequences. This has the advantage of being able to train on a corpus that are significantly larger than the typical parallel corpus. And indeed having more data has a regularizing effect, as it helps the model to better discriminate recurring patterns and effectively forget more of the noise it has learned. However, as mentioned above to learn to generate and or eliminate sequences based on what can be long term association in the text is a challenge which requires attention mechanism."
  },
  {
    "objectID": "posts/2021/2021-12-07-attention-for-sensor-fusion/2021-12-07-attention-for-sensor-fusion.html#questions",
    "href": "posts/2021/2021-12-07-attention-for-sensor-fusion/2021-12-07-attention-for-sensor-fusion.html#questions",
    "title": "Attention for sensor fusion",
    "section": "Questions:",
    "text": "Questions:\nWhy exactly do we need beam search. (Is it just to avoid generating this and that and the the sequences? or does it )\nMost transformers generate text one word at a time. To accommodate many limitations of autoregressive language models, their output is mined by a beam search alg which essentially looks at a tree of likely sequences. It can then eliminate any ungrammatical sequences and then make a more informed choice on the word it picks. The beam search alg has a horizon which limits the distance it looks at.\n\nAre there types of patterns that will be missed due to horizon’s distance?\nIs there a beam search that augments the probablitic conditioning of a sequence with an attention mechansim. The model uses attention for the current word.\nWe might be generating good sequences by the data for deciding case, gender or other agreement choices might be masked in the future. Do we gain accuracy if we are allowed to revise our choices. the word that. Is there a variant of beam search that uses multiple attention heads to pick generate a sequence."
  },
  {
    "objectID": "posts/2021/2021-12-07-attention-for-sensor-fusion/2021-12-07-attention-for-sensor-fusion.html#agreeing-to-disagree--",
    "href": "posts/2021/2021-12-07-attention-for-sensor-fusion/2021-12-07-attention-for-sensor-fusion.html#agreeing-to-disagree--",
    "title": "Attention for sensor fusion",
    "section": "Agreeing to disagree -",
    "text": "Agreeing to disagree -\nIn game theory one cannot agree to disagree since in a Bayesian game, agents who observe each others’ actions will inevitably converge to the same ‘view of the world’ even if they began with divergent information. Can we set up attention that approximates this Bayesian game. 1. Imagine we are translating a complex sequence of cause and effects working with a causal"
  },
  {
    "objectID": "posts/2018/2018-01-16-BRAT/2018-01-16-BRAT.html",
    "href": "posts/2018/2018-01-16-BRAT/2018-01-16-BRAT.html",
    "title": "text annotation with BRAT",
    "section": "",
    "text": "BRAT is a rapid annotation tool. Its old no long maintained - the original developers seem to have moved on. Requests for RTL support have been largely ignored. Some advantages:\nlooking into modernising the UI to work with RTL.\nGet brat in a docker container."
  },
  {
    "objectID": "posts/2018/2018-01-16-BRAT/2018-01-16-BRAT.html#plan-a-recreate-the-svg-element-using-d3.js",
    "href": "posts/2018/2018-01-16-BRAT/2018-01-16-BRAT.html#plan-a-recreate-the-svg-element-using-d3.js",
    "title": "text annotation with BRAT",
    "section": "plan A: Recreate the svg element using d3.js",
    "text": "plan A: Recreate the svg element using d3.js"
  },
  {
    "objectID": "posts/2018/2018-01-16-BRAT/2018-01-16-BRAT.html#plan-b",
    "href": "posts/2018/2018-01-16-BRAT/2018-01-16-BRAT.html#plan-b",
    "title": "text annotation with BRAT",
    "section": "plan B",
    "text": "plan B\nBreaking up the element into smaller web-components in polymer. Organize things better using using modern javascript"
  },
  {
    "objectID": "posts/2018/2018-01-16-BRAT/2018-01-16-BRAT.html#brat-and-rtl-support",
    "href": "posts/2018/2018-01-16-BRAT/2018-01-16-BRAT.html#brat-and-rtl-support",
    "title": "text annotation with BRAT",
    "section": "Brat and RTL support",
    "text": "Brat and RTL support"
  },
  {
    "objectID": "posts/2011/2011-11-29-npl-python/index.html",
    "href": "posts/2011/2011-11-29-npl-python/index.html",
    "title": "Text Mining With Python",
    "section": "",
    "text": "import numpy as np                           # library for scientific computing and matrix \nimport matplotlib.pyplot as plt              # visualization library\nimport string\nimport re\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer    \n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /home/oren/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\n\n1nltk.download('stopwords')\n\n\ndef process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english') \n    \n2    tweet = re.sub(r'\\$\\w*', '', tweet)\n3    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n4    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n5    tweet = re.sub(r'#', '', tweet)\n6    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n    tweets_clean = []\n    for word in tweet_tokens:\n7        if (word not in stopwords_english and\n8                word not in string.punctuation):\n            # tweets_clean.append(word)\n9            stem_word = stemmer.stem(word)\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n\n\n1\n\ndownload the stopwords\n\n2\n\nremove stock market tickers like $GE\n\n3\n\nremove old style retweet text “RT”\n\n4\n\nremove hyperlinks\n\n5\n\nremove hashtags\n\n6\n\ntokenize tweets\n\n7\n\nremove stopwords\n\n8\n\nremove punctuation\n\n9\n\nstemming word\n\n\n\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = defaultdict(int)\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs\n\ndef build_vocab(freqs):\n    vocab = [k for k, v in freq.items() if (v &gt; 1 and k != '\\n')]\n    vocab.sort()\n    return vocab\n\nprocessing unknown tokens\n\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n\n\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nNumber of tweets:  10000\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2011,\n  author = {Bochman, Oren},\n  title = {Text {Mining} {With} {Python}},\n  date = {2011-11-29},\n  url = {https://orenbochman.github.io/blog//posts/2011/2011-11-29-npl-python},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2011. “Text Mining With Python.” November\n29, 2011. https://orenbochman.github.io/blog//posts/2011/2011-11-29-npl-python."
  },
  {
    "objectID": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html",
    "href": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html",
    "title": "Tidy Text Mining With R",
    "section": "",
    "text": "Computational Linguistics tasks:"
  },
  {
    "objectID": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html#text-preprocessing",
    "href": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html#text-preprocessing",
    "title": "Tidy Text Mining With R",
    "section": "Text preprocessing",
    "text": "Text preprocessing\n\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %&gt;%\n  ungroup()\n\noriginal_books\n\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   &lt;chr&gt;                   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt;\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# ℹ 73,412 more rows\n\nlibrary(tidytext)\ntidy_books &lt;- original_books %&gt;%\n  unnest_tokens(word, text)\n\ntidy_books\n\n# A tibble: 725,055 × 4\n   book                linenumber chapter word       \n   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 and        \n 3 Sense & Sensibility          1       0 sensibility\n 4 Sense & Sensibility          3       0 by         \n 5 Sense & Sensibility          3       0 jane       \n 6 Sense & Sensibility          3       0 austen     \n 7 Sense & Sensibility          5       0 1811       \n 8 Sense & Sensibility         10       1 chapter    \n 9 Sense & Sensibility         10       1 1          \n10 Sense & Sensibility         13       1 the        \n# ℹ 725,045 more rows\n\n\n\ndata(stop_words)\n\ntidy_books &lt;- tidy_books %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_books\n\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# ℹ 217,599 more rows\n\n\n\nthis removes stop words\n\n\ntidy_books %&gt;%\n  count(word, sort = TRUE) \n\n# A tibble: 13,914 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 miss    1855\n 2 time    1337\n 3 fanny    862\n 4 dear     822\n 5 lady     817\n 6 sir      806\n 7 day      797\n 8 emma     787\n 9 sister   727\n10 house    699\n# ℹ 13,904 more rows\n\n\n\nlibrary(ggplot2)\n\ntidy_books %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(n &gt; 600) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\n#devtools::install_github(\"ropensci/gutenbergr\")\nlibrary(gutenbergr)\n\n#hgwells &lt;- gutenberg_download(c(35, 36,  159, 456, 1047, 3691, 5230, 11870, 12163, 23218, 28218, 35461,39585))\nhgwells &lt;- gutenberg_download(c(35, 36,  159))\n\nDetermining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest\n\n\n`curl` package not installed, falling back to using `url()`\nUsing mirror http://aleph.gutenberg.org\n\n\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/5/159/159.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n\n\n\ntidy_hgwells &lt;- hgwells %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\n\ntidy_hgwells %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 8,146 × 2\n   word         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 time       328\n 2 people     205\n 3 martians   165\n 4 black      152\n 5 night      140\n 6 machine    133\n 7 found      110\n 8 white      108\n 9 road       105\n10 day        102\n# ℹ 8,136 more rows\n\n\n\nbronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767))\n\n\ntidy_bronte &lt;- bronte %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_bronte %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 23,213 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 time    1065\n 2 miss     854\n 3 day      825\n 4 don’t    780\n 5 hand     767\n 6 eyes     714\n 7 night    648\n 8 heart    638\n 9 looked   601\n10 door     591\n# ℹ 23,203 more rows\n\n\n\nlibrary(tidyr)\n\nfrequency &lt;- bind_rows(mutate(tidy_bronte, author = \"Brontë Sisters\"),\n                       mutate(tidy_hgwells, author = \"H.G. Wells\"), \n                       mutate(tidy_books, author = \"Jane Austen\")) %&gt;% \n  mutate(word = str_extract(word, \"[a-z']+\")) %&gt;%\n  count(author, word) %&gt;%\n  group_by(author) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;% \n  select(-n) %&gt;% \n  pivot_wider(names_from = author, values_from = proportion) %&gt;%\n  pivot_longer(`Brontë Sisters`:`H.G. Wells`,\n               names_to = \"author\", values_to = \"proportion\")\n\nfrequency\n\n# A tibble: 54,120 × 4\n   word      `Jane Austen` author          proportion\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1 a            0.00000919 Brontë Sisters  0.0000665 \n 2 a            0.00000919 H.G. Wells      0.0000293 \n 3 aback       NA          Brontë Sisters  0.00000391\n 4 aback       NA          H.G. Wells     NA         \n 5 abaht       NA          Brontë Sisters  0.00000391\n 6 abaht       NA          H.G. Wells     NA         \n 7 abandon     NA          Brontë Sisters  0.0000313 \n 8 abandon     NA          H.G. Wells      0.0000293 \n 9 abandoned    0.00000460 Brontë Sisters  0.0000899 \n10 abandoned    0.00000460 H.G. Wells      0.000234  \n# ℹ 54,110 more rows\n\n\n\nlibrary(scales)\n\n# expect a warning about rows with missing values being removed\nggplot(frequency, aes(x = proportion, y = `Jane Austen`, \n                      color = abs(`Jane Austen` - proportion))) +\n  geom_abline(color = \"gray40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  scale_color_gradient(limits = c(0, 0.001), \n                       low = \"darkslategray4\", high = \"gray75\") +\n  facet_wrap(~author, ncol = 2) +\n  theme(legend.position=\"none\") +\n  labs(y = \"Jane Austen\", x = NULL)\n\nWarning: Removed 39274 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 39276 rows containing missing values (`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n  cor.test(data = frequency[frequency$author == \"Brontë Sisters\",], ~ proportion + `Jane Austen`)\n\n\n    Pearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 110.73, df = 10275, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7286645 0.7462983\nsample estimates:\n      cor \n0.7376071 \n\n\n\ncor.test(data = frequency[frequency$author == \"H.G. Wells\",], \n         ~ proportion + `Jane Austen`)\n\n\n    Pearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 29.497, df = 4567, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3753856 0.4241064\nsample estimates:\n      cor \n0.4000286 \n\n\nkwik and kwok\n\nlibrary(quanteda)\nlibrary(gutenbergr)\n\nausten_works = gutenberg_works(author == \"Austen, Jane\")\nausten = gutenberg_download(austen_works$gutenberg_id)\n\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/3/4/1342/1342.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n\nhead(hgwells)\n\n# A tibble: 6 × 2\n  gutenberg_id text              \n         &lt;int&gt; &lt;chr&gt;             \n1           35 \"The Time Machine\"\n2           35 \"\"                \n3           35 \"An Invention\"    \n4           35 \"\"                \n5           35 \"by H. G. Wells\"  \n6           35 \"\"                \n\n# tidy_hgwells &lt;- hgwells %&gt;%\n#   unnest_tokens(word, text) %&gt;%\n#   anti_join(stop_words)\n\n#head(tidy_hgwells)\n\nthe_corpus &lt;- corpus(austen)\nthe_tokens &lt;- tokens(the_corpus,case_insensitive = TRUE)\n\nWarning: case_insensitive argument is not used.\n\nkwic_table &lt;- kwic(the_tokens,pattern = \"lady\",index = 1:100)\n#kwic_table &lt;- kwic(tokens(tidy_hgwells$word),pattern = \"time\")\n\n#kwic_table &lt;- kwic(tokens(tidy_hgwells$word),pattern = \"machine\",index = 1:400, case_insensitive = TRUE)\nnrow(kwic_table)\n\n[1] 2008\n\nhead(kwic_table,10)\n\nKeyword-in-context with 10 matches.                                                          \n  [text61, 5]                Gloucester, by which | lady |\n  [text98, 7]                deserved by his own. | Lady |\n [text100, 8] youthful infatuation which made her | Lady |\n [text112, 6]            her kindness and advice, | Lady |\n [text118, 4]                   passed away since | Lady |\n [text122, 2]                                That | Lady |\n [text142, 2]                                  To | Lady |\n [text143, 8]              favourite, and friend. | Lady |\n [text169, 1]                                     | Lady |\n [text177, 3]                   immediately after | Lady |\n                                \n ( who died 1800 )              \n Elliot had been an excellent   \n Elliot, had never              \n Elliot mainly relied for the   \n Elliot’s death, and they       \n Russell, of steady age         \n Russell, indeed, she           \n Russell loved them all;        \n Russell’s temples had long been\n Russell out of all the"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html",
    "href": "posts/2011/bash-cheatsheet/index.html",
    "title": "Bash",
    "section": "",
    "text": "#!/bin/bash\n\nVAR=\"world\"\necho \"Hello $VAR!\" # =&gt; Hello world!\nExecute the script shell script $ bash hello.sh\n\n\n\nNAME=\"John\"\n\necho ${NAME}    # =&gt; John (Variables)\necho $NAME      # =&gt; John (Variables)\necho \"$NAME\"    # =&gt; John (Variables)\necho '$NAME'    # =&gt; $NAME (Exact string)\necho \"${NAME}!\" # =&gt; John! (Variables)\n\nNAME = \"John\"   # =&gt; Error (about space)\n\n\n\n# This is an inline Bash comment.\n: '\nThis is a\nvery neat comment\nin bash\n'\nMulti-line comments use :' to open and ' to close\n\n\n\n\n\n\nExpression\nDescription\n\n\n\n\n$1 … $9\nParameter 1 … 9\n\n\n$0\nName of the script itself\n\n\n$1\nFirst argument\n\n\n${10}\nPositional parameter 10\n\n\n$#\nNumber of arguments\n\n\n$$\nProcess id of the shell\n\n\n$*\nAll arguments\n\n\n$@\nAll arguments, starting from first\n\n\n$-\nCurrent options\n\n\n$_\nLast argument of the previous command\n\n\n\nSee: Special parameters\n\n\n\nget_name() {\n    echo \"John\"\n}\n\necho \"You are $(get_name)\"\nSee: Functions\n\n\n\nif [[ -z \"$string\" ]]; then\n    echo \"String is empty\"\nelif [[ -n \"$string\" ]]; then\n    echo \"String is not empty\"\nfi\nSee: Conditionals\n\n\n\necho {A,B}.js\n\n\n\n\nExpression\nDescription\n\n\n\n\n{A,B}\nSame as A B\n\n\n{A,B}.js\nSame as A.js B.js\n\n\n{1..5}\nSame as 1 2 3 4 5\n\n\n\nSee: Brace expansion\n\n\n\n# =&gt; I'm in /path/of/current\necho \"I'm in $(PWD)\"\n\n# Same as:\necho \"I'm in `pwd`\"\nSee: Command substitution"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#getting-started",
    "href": "posts/2011/bash-cheatsheet/index.html#getting-started",
    "title": "Bash",
    "section": "",
    "text": "#!/bin/bash\n\nVAR=\"world\"\necho \"Hello $VAR!\" # =&gt; Hello world!\nExecute the script shell script $ bash hello.sh\n\n\n\nNAME=\"John\"\n\necho ${NAME}    # =&gt; John (Variables)\necho $NAME      # =&gt; John (Variables)\necho \"$NAME\"    # =&gt; John (Variables)\necho '$NAME'    # =&gt; $NAME (Exact string)\necho \"${NAME}!\" # =&gt; John! (Variables)\n\nNAME = \"John\"   # =&gt; Error (about space)\n\n\n\n# This is an inline Bash comment.\n: '\nThis is a\nvery neat comment\nin bash\n'\nMulti-line comments use :' to open and ' to close\n\n\n\n\n\n\nExpression\nDescription\n\n\n\n\n$1 … $9\nParameter 1 … 9\n\n\n$0\nName of the script itself\n\n\n$1\nFirst argument\n\n\n${10}\nPositional parameter 10\n\n\n$#\nNumber of arguments\n\n\n$$\nProcess id of the shell\n\n\n$*\nAll arguments\n\n\n$@\nAll arguments, starting from first\n\n\n$-\nCurrent options\n\n\n$_\nLast argument of the previous command\n\n\n\nSee: Special parameters\n\n\n\nget_name() {\n    echo \"John\"\n}\n\necho \"You are $(get_name)\"\nSee: Functions\n\n\n\nif [[ -z \"$string\" ]]; then\n    echo \"String is empty\"\nelif [[ -n \"$string\" ]]; then\n    echo \"String is not empty\"\nfi\nSee: Conditionals\n\n\n\necho {A,B}.js\n\n\n\n\nExpression\nDescription\n\n\n\n\n{A,B}\nSame as A B\n\n\n{A,B}.js\nSame as A.js B.js\n\n\n{1..5}\nSame as 1 2 3 4 5\n\n\n\nSee: Brace expansion\n\n\n\n# =&gt; I'm in /path/of/current\necho \"I'm in $(PWD)\"\n\n# Same as:\necho \"I'm in `pwd`\"\nSee: Command substitution"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-parameter-expansions",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-parameter-expansions",
    "title": "Bash",
    "section": "Bash Parameter expansions",
    "text": "Bash Parameter expansions\n\nSyntax\n\n\n\nCode\nDescription\n\n\n\n\n${FOO%suffix}\nRemove suffix\n\n\n${FOO#prefix}\nRemove prefix\n\n\n${FOO%%suffix}\nRemove long suffix\n\n\n${FOO##prefix}\nRemove long prefix\n\n\n${FOO/from/to}\nReplace first match\n\n\n${FOO//from/to}\nReplace all\n\n\n${FOO/%from/to}\nReplace suffix\n\n\n${FOO/#from/to}\nReplace prefix\n\n\n\n\nSubstrings\n\n\n\nExpression\nDescription\n\n\n\n\n${FOO:0:3}\nSubstring (position, length)\n\n\n${FOO:(-3):3}\nSubstring from the right\n\n\n\n\n\nLength\n\n\n\nExpression\nDescription\n\n\n\n\n${#FOO}\nLength of $FOO\n\n\n\n\n\nDefault values\n\n\n\nExpression\nDescription\n\n\n\n\n${FOO:-val}\n$FOO, or val if unset\n\n\n${FOO:=val}\nSet $FOO to val if unset\n\n\n${FOO:+val}\nval if $FOO is set\n\n\n${FOO:?message}\nShow message and exit if $FOO is unset\n\n\n\n\n\n\nSubstitution\necho ${food:-Cake}  #=&gt; $food or \"Cake\"\nSTR=\"/path/to/foo.cpp\"\necho ${STR%.cpp}    # /path/to/foo\necho ${STR%.cpp}.o  # /path/to/foo.o\necho ${STR%/*}      # /path/to\n\necho ${STR##*.}     # cpp (extension)\necho ${STR##*/}     # foo.cpp (basepath)\n\necho ${STR#*/}      # path/to/foo.cpp\necho ${STR##*/}     # foo.cpp\n\necho ${STR/foo/bar} # /path/to/bar.cpp\n\n\nSlicing\nname=\"John\"\necho ${name}           # =&gt; John\necho ${name:0:2}       # =&gt; Jo\necho ${name::2}        # =&gt; Jo\necho ${name::-1}       # =&gt; Joh\necho ${name:(-1)}      # =&gt; n\necho ${name:(-2)}      # =&gt; hn\necho ${name:(-2):2}    # =&gt; hn\n\nlength=2\necho ${name:0:length}  # =&gt; Jo\nSee: Parameter expansion\n\n\nbasepath & dirpath\nSRC=\"/path/to/foo.cpp\"\nBASEPATH=${SRC##*/}   \necho $BASEPATH  # =&gt; \"foo.cpp\"\n\n\nDIRPATH=${SRC%$BASEPATH}\necho $DIRPATH   # =&gt; \"/path/to/\"\n\n\nTransform\nSTR=\"HELLO WORLD!\"\necho ${STR,}   # =&gt; hELLO WORLD!\necho ${STR,,}  # =&gt; hello world!\n\nSTR=\"hello world!\"\necho ${STR^}   # =&gt; Hello world!\necho ${STR^^}  # =&gt; HELLO WORLD!\n\nARR=(hello World)\necho \"${ARR[@],}\" # =&gt; hello world\necho \"${ARR[@]^}\" # =&gt; Hello World"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-arrays",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-arrays",
    "title": "Bash",
    "section": "Bash Arrays",
    "text": "Bash Arrays\n\nDefining arrays\nFruits=('Apple' 'Banana' 'Orange')\n\nFruits[0]=\"Apple\"\nFruits[1]=\"Banana\"\nFruits[2]=\"Orange\"\n\nARRAY1=(foo{1..2}) # =&gt; foo1 foo2\nARRAY2=({A..D})    # =&gt; A B C D\n\n# Merge =&gt; foo1 foo2 A B C D\nARRAY3=(${ARRAY1[@]} ${ARRAY2[@]})\n\n# declare construct\ndeclare -a Numbers=(1 2 3)\nNumbers+=(4 5) # Append =&gt; 1 2 3 4 5\n\n\nIndexing\n\n\n\n-\n-\n\n\n\n\n${Fruits[0]}\nFirst element\n\n\n${Fruits[-1]}\nLast element\n\n\n${Fruits[*]}\nAll elements\n\n\n${Fruits[@]}\nAll elements\n\n\n${#Fruits[@]}\nNumber of all\n\n\n${#Fruits}\nLength of 1st\n\n\n${#Fruits[3]}\nLength of nth\n\n\n${Fruits[@]:3:2}\nRange\n\n\n${!Fruits[@]}\nKeys of all\n\n\n\n\n\nIteration\nFruits=('Apple' 'Banana' 'Orange')\n\nfor e in \"${Fruits[@]}\"; do\n    echo $e\ndone\n\nWith index\nfor i in \"${!Fruits[@]}\"; do\n  printf \"%s\\t%s\\n\" \"$i\" \"${Fruits[$i]}\"\ndone\n\n\n\nOperations\nFruits=(\"${Fruits[@]}\" \"Watermelon\")     # Push\nFruits+=('Watermelon')                   # Also Push\nFruits=( ${Fruits[@]/Ap*/} )             # Remove by regex match\nunset Fruits[2]                          # Remove one item\nFruits=(\"${Fruits[@]}\")                  # Duplicate\nFruits=(\"${Fruits[@]}\" \"${Veggies[@]}\")  # Concatenate\nlines=(`cat \"logfile\"`)                  # Read from file\n\n\nArrays as arguments\nfunction extract()\n{\n    local -n myarray=$1\n    local idx=$2\n    echo \"${myarray[$idx]}\"\n}\nFruits=('Apple' 'Banana' 'Orange')\nextract Fruits 2     # =&gt; Orangle"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-dictionaries",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-dictionaries",
    "title": "Bash",
    "section": "Bash Dictionaries",
    "text": "Bash Dictionaries\n\nDefining\ndeclare -A sounds\nsounds[dog]=\"bark\"\nsounds[cow]=\"moo\"\nsounds[bird]=\"tweet\"\nsounds[wolf]=\"howl\"\n\n\nWorking with dictionaries\necho ${sounds[dog]} # Dog's sound\necho ${sounds[@]}   # All values\necho ${!sounds[@]}  # All keys\necho ${#sounds[@]}  # Number of elements\nunset sounds[dog]   # Delete dog\n\n\nIteration\nfor val in \"${sounds[@]}\"; do\n    echo $val\ndone\n\nfor key in \"${!sounds[@]}\"; do\n    echo $key\ndone"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-conditionals",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-conditionals",
    "title": "Bash",
    "section": "Bash Conditionals",
    "text": "Bash Conditionals\n\nInteger conditions\n\n\n\nCondition\nDescription\n\n\n\n\n[[ NUM -eq NUM ]]\nEqual\n\n\n[[ NUM -ne NUM ]]\nNot equal\n\n\n[[ NUM -lt NUM ]]\nLess than\n\n\n[[ NUM -le NUM ]]\nLess than or equal\n\n\n[[ NUM -gt NUM ]]\nGreater than\n\n\n[[ NUM -ge NUM ]]\nGreater than or equal\n\n\n(( NUM &lt; NUM ))\nLess than\n\n\n(( NUM &lt;= NUM ))\nLess than or equal\n\n\n(( NUM &gt; NUM ))\nGreater than\n\n\n(( NUM &gt;= NUM ))\nGreater than or equal\n\n\n\n\n\nString conditions\n\n\n\nCondition\nDescription\n\n\n\n\n[[ -z STR ]]\nEmpty string\n\n\n[[ -n STR ]]\nNot empty string\n\n\n[[ STR == STR ]]\nEqual\n\n\n[[ STR = STR ]]\nEqual (Same above)\n\n\n[[ STR &lt; STR ]]\nLess than (ASCII)\n\n\n[[ STR &gt; STR ]]\nGreater than (ASCII)\n\n\n[[ STR != STR ]]\nNot Equal\n\n\n[[ STR =~ STR ]]\nRegexp\n\n\n\n\n\nExample\n\nString\nif [[ -z \"$string\" ]]; then\n    echo \"String is empty\"\nelif [[ -n \"$string\" ]]; then\n    echo \"String is not empty\"\nelse\n    echo \"This never happens\"\nfi\n\n\nCombinations\nif [[ X && Y ]]; then\n    ...\nfi\n\n\nEqual\nif [[ \"$A\" == \"$B\" ]]; then\n    ...\nfi\n\n\nRegex\nif [[ '1. abc' =~ ([a-z]+) ]]; then\n    echo ${BASH_REMATCH[1]}\nfi\n\n\nSmaller\nif (( $a &lt; $b )); then\n   echo \"$a is smaller than $b\"\nfi\n\n\nExists\nif [[ -e \"file.txt\" ]]; then\n    echo \"file exists\"\nfi\n\n\n\nFile conditions\n\n\n\nCondition\nDescription\n\n\n\n\n[[ -e FILE ]]\nExists\n\n\n[[ -d FILE ]]\nDirectory\n\n\n[[ -f FILE ]]\nFile\n\n\n[[ -h FILE ]]\nSymlink\n\n\n[[ -s FILE ]]\nSize is &gt; 0 bytes\n\n\n[[ -r FILE ]]\nReadable\n\n\n[[ -w FILE ]]\nWritable\n\n\n[[ -x FILE ]]\nExecutable\n\n\n[[ f1 -nt f2 ]]\nf1 newer than f2\n\n\n[[ f1 -ot f2 ]]\nf2 older than f1\n\n\n[[ f1 -ef f2 ]]\nSame files\n\n\n\n\n\nMore conditions\n\n\n\nCondition\nDescription\n\n\n\n\n[[ -o noclobber ]]\nIf OPTION is enabled\n\n\n[[ ! EXPR ]]\nNot\n\n\n[[ X && Y ]]\nAnd\n\n\n[[ X || Y ]]\nOr\n\n\n\n\n\nlogical and, or\nif [ \"$1\" = 'y' -a $2 -gt 0 ]; then\n    echo \"yes\"\nfi\n\nif [ \"$1\" = 'n' -o $2 -lt 0 ]; then\n    echo \"no\"\nfi"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-loops",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-loops",
    "title": "Bash",
    "section": "Bash Loops",
    "text": "Bash Loops\n\nBasic for loop\nfor i in /etc/rc.*; do\n    echo $i\ndone\n\n\nC-like for loop\nfor ((i = 0 ; i &lt; 100 ; i++)); do\n    echo $i\ndone\n\n\nRanges\nfor i in {1..5}; do\n    echo \"Welcome $i\"\ndone\n\nWith step size\nfor i in {5..50..5}; do\n    echo \"Welcome $i\"\ndone\n\n\n\nAuto increment\ni=1\nwhile [[ $i -lt 4 ]]; do\n    echo \"Number: $i\"\n    ((i++))\ndone\n\n\nAuto decrement\ni=3\nwhile [[ $i -gt 0 ]]; do\n    echo \"Number: $i\"\n    ((i--))\ndone\n\n\nContinue\nfor number in $(seq 1 3); do\n    if [[ $number == 2 ]]; then\n        continue;\n    fi\n    echo \"$number\"\ndone\n\n\nBreak\nfor number in $(seq 1 3); do\n    if [[ $number == 2 ]]; then\n        # Skip entire rest of loop.\n        break;\n    fi\n    # This will only print 1\n    echo \"$number\"\ndone\n\n\nUntil\ncount=0\nuntil [ $count -gt 10 ]; do\n    echo \"$count\"\n    ((count++))\ndone\n\n\nForever\nwhile true; do\n    # here is some code.\ndone\n\n\nForever (shorthand)\nwhile :; do\n    # here is some code.\ndone\n\n\nReading lines\ncat file.txt | while read line; do\n    echo $line\ndone"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-functions",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-functions",
    "title": "Bash",
    "section": "Bash Functions",
    "text": "Bash Functions\n\nDefining functions\nmyfunc() {\n    echo \"hello $1\"\n}\n# Same as above (alternate syntax)\nfunction myfunc() {\n    echo \"hello $1\"\n}\nmyfunc \"John\"\n\n\nReturning values\nmyfunc() {\n    local myresult='some value'\n    echo $myresult\n}\nresult=\"$(myfunc)\"\n\n\nRaising errors\nmyfunc() {\n    return 1\n}\nif myfunc; then\n    echo \"success\"\nelse\n    echo \"failure\"\nfi"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-options",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-options",
    "title": "Bash",
    "section": "Bash Options",
    "text": "Bash Options\n\nOptions\n# Avoid overlay files\n# (echo \"hi\" &gt; foo)\nset -o noclobber\n\n# Used to exit upon error\n# avoiding cascading errors\nset -o errexit   \n\n# Unveils hidden failures\nset -o pipefail  \n\n# Exposes unset variables\nset -o nounset\n\n\nGlob options\n# Non-matching globs are removed  \n# ('*.foo' =&gt; '')\nshopt -s nullglob   \n\n# Non-matching globs throw errors\nshopt -s failglob  \n\n# Case insensitive globs\nshopt -s nocaseglob \n\n# Wildcards match dotfiles \n# (\"*.sh\" =&gt; \".foo.sh\")\nshopt -s dotglob    \n\n# Allow ** for recursive matches \n# ('lib/**/*.rb' =&gt; 'lib/a/b/c.rb')\nshopt -s globstar"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-history",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-history",
    "title": "Bash",
    "section": "Bash History",
    "text": "Bash History\n\nCommands\n\n\n\nCommand\nDescription\n\n\n\n\nhistory\nShow history\n\n\nsudo !!\nRun the previous command with sudo\n\n\nshopt -s histverify\nDon’t execute expanded result immediately\n\n\n\n\n\nExpansions\n\n\n\nExpression\nDescription\n\n\n\n\n!$\nExpand last parameter of most recent command\n\n\n!*\nExpand all parameters of most recent command\n\n\n!-n\nExpand nth most recent command\n\n\n!n\nExpand nth command in history\n\n\n!&lt;command&gt;\nExpand most recent invocation of command &lt;command&gt;\n\n\n\n\n\nOperations\n\n\n\n\n\n\n\nCode\nDescription\n\n\n\n\n!!\nExecute last command again\n\n\n!!:s/&lt;FROM&gt;/&lt;TO&gt;/\nReplace first occurrence of &lt;FROM&gt; to &lt;TO&gt; in most recent command\n\n\n!!:gs/&lt;FROM&gt;/&lt;TO&gt;/\nReplace all occurrences of &lt;FROM&gt; to &lt;TO&gt; in most recent command\n\n\n!$:t\nExpand only basename from last parameter of most recent command\n\n\n!$:h\nExpand only directory from last parameter of most recent command\n\n\n\n!! and !$ can be replaced with any valid expansion.\n\n\nSlices\n\n\n\n\n\n\n\nCode\nDescription\n\n\n\n\n!!:n\nExpand only nth token from most recent command (command is 0; first argument is 1)\n\n\n!^\nExpand first argument from most recent command\n\n\n!$\nExpand last token from most recent command\n\n\n!!:n-m\nExpand range of tokens from most recent command\n\n\n!!:n-$\nExpand nth token to last from most recent command\n\n\n\n!! can be replaced with any valid expansion i.e. !cat, !-2, !42, etc."
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#miscellaneous",
    "href": "posts/2011/bash-cheatsheet/index.html#miscellaneous",
    "title": "Bash",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nNumeric calculations\n$((a + 200))      # Add 200 to $a\n$(($RANDOM%200))  # Random number 0..199\n\n\nSubshells\n(cd somedir; echo \"I'm now in $PWD\")\npwd # still in first directory\n\n\nInspecting commands\ncommand -V cd\n#=&gt; \"cd is a function/alias/whatever\"\n\n\nRedirection\npython hello.py &gt; output.txt   # stdout to (file)\npython hello.py &gt;&gt; output.txt  # stdout to (file), append\npython hello.py 2&gt; error.log   # stderr to (file)\npython hello.py 2&gt;&1           # stderr to stdout\npython hello.py 2&gt;/dev/null    # stderr to (null)\npython hello.py &&gt;/dev/null    # stdout and stderr to (null)\npython hello.py &lt; foo.txt      # feed foo.txt to stdin for python\n\n\nSource relative\nsource \"${0%/*}/../share/foo.sh\"\n\n\nDirectory of script\nDIR=\"${0%/*}\"\n\n\nCase/switch\ncase \"$1\" in\n    start | up)\n    vagrant up\n    ;;\n\n    *)\n    echo \"Usage: $0 {start|stop|ssh}\"\n    ;;\nesac\n\n\nTrap errors\ntrap 'echo Error at about $LINENO' ERR\nor\ntraperr() {\n    echo \"ERROR: ${BASH_SOURCE[1]} at about ${BASH_LINENO[0]}\"\n}\n\nset -o errtrace\ntrap traperr ERR\n\n\nprintf\nprintf \"Hello %s, I'm %s\" Sven Olga\n#=&gt; \"Hello Sven, I'm Olga\n\nprintf \"1 + 1 = %d\" 2\n#=&gt; \"1 + 1 = 2\"\n\nprintf \"Print a float: %f\" 2\n#=&gt; \"Print a float: 2.000000\"\n\n\nGetting options\nwhile [[ \"$1\" =~ ^- && ! \"$1\" == \"--\" ]]; do case $1 in\n    -V | --version )\n    echo $version\n    exit\n    ;;\n    -s | --string )\n    shift; string=$1\n    ;;\n    -f | --flag )\n    flag=1\n    ;;\nesac; shift; done\nif [[ \"$1\" == '--' ]]; then shift; fi\n\n\nCheck for command’s result\nif ping -c 1 google.com; then\n    echo \"It appears you have a working internet connection\"\nfi\n\n\nSpecial variables\n\n\n\nExpression\nDescription\n\n\n\n\n$?\nExit status of last task\n\n\n$!\nPID of last background task\n\n\n$$\nPID of shell\n\n\n$0\nFilename of the shell script\n\n\n\nSee Special parameters.\n\n\nGrep check\nif grep -q 'foo' ~/.bash_history; then\n    echo \"You appear to have typed 'foo' in the past\"\nfi\n\n\nBackslash escapes\n\n \n!\n\"\n#\n&\n'\n(\n)\n,\n;\n&lt;\n&gt;\n[\n|\n\\\n]\n^\n{\n}\n`\n$\n*\n? {.cols-4 .marker-none}\n\nEscape these special characters with \\\n\n\nHeredoc\ncat &lt;&lt;END\nhello world\nEND\n\n\nGo to previous directory\npwd # /home/user/foo\ncd bar/\npwd # /home/user/foo/bar\ncd -\npwd # /home/user/foo\n\n\nReading input\necho -n \"Proceed? [y/n]: \"\nread ans\necho $ans\nread -n 1 ans    # Just one character\n\n\nConditional execution\ngit commit && git push\ngit commit || echo \"Commit failed\"\n\n\nStrict mode\nset -euo pipefail\nIFS=$'\\n\\t'\nSee: Unofficial bash strict mode\n\n\nOptional arguments\nargs=(\"$@\")\nargs+=(foo)\nargs+=(bar)\necho \"${args[@]}\"\nPut the arguments into an array and then append"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#also-see",
    "href": "posts/2011/bash-cheatsheet/index.html#also-see",
    "title": "Bash",
    "section": "Also see",
    "text": "Also see\n\nDevhints (devhints.io)\nBash-hackers wiki (bash-hackers.org)\nShell vars (bash-hackers.org)\nLearn bash in y minutes (learnxinyminutes.com)\nBash Guide (mywiki.wooledge.org)\nShellCheck (shellcheck.net)\nshell - Standard Shell (devmanual.gentoo.org)"
  },
  {
    "objectID": "posts/2019/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq.html",
    "href": "posts/2019/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq.html",
    "title": "Docker for data science",
    "section": "",
    "text": "Docker lets you install stuff in a way that avoids dependency conflicts\n\n\ndocker run -p 10000:8888 jupyter/scipy-notebook:b418b67c225b\ndocker run -it --rm -p 10000:8888 -v \"${PWD}\":/home/jovyan/work jupyter/datascience-notebook:b418b67c225b\n\n\n\n\n# Check docker memory if &gt;=4 GB\ndocker run --rm \"debian:buster-slim\" bash -c \n'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))'\n\n# Getting airflow compose file\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml'\n\n# build\ndocker-compose up airflow-init\n\n# start\ndocker-compose up"
  },
  {
    "objectID": "posts/2019/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq.html#jupyter-notebook",
    "href": "posts/2019/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq.html#jupyter-notebook",
    "title": "Docker for data science",
    "section": "",
    "text": "docker run -p 10000:8888 jupyter/scipy-notebook:b418b67c225b\ndocker run -it --rm -p 10000:8888 -v \"${PWD}\":/home/jovyan/work jupyter/datascience-notebook:b418b67c225b"
  },
  {
    "objectID": "posts/2019/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq.html#airflow",
    "href": "posts/2019/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq/2019-11-24-keys-to-the-kingdom-extracting-api-keys-from-a-json-file-with-jq.html#airflow",
    "title": "Docker for data science",
    "section": "",
    "text": "# Check docker memory if &gt;=4 GB\ndocker run --rm \"debian:buster-slim\" bash -c \n'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))'\n\n# Getting airflow compose file\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.2.3/docker-compose.yaml'\n\n# build\ndocker-compose up airflow-init\n\n# start\ndocker-compose up"
  },
  {
    "objectID": "posts/2012/2012-07-26-wikisym-2012_files/index.html",
    "href": "posts/2012/2012-07-26-wikisym-2012_files/index.html",
    "title": "Wikisym 2012",
    "section": "",
    "text": "Due to a kind grant by the WikiMedia Foundation I was able to attend Wikisym 2012 in Linz Austria what follows is my report on the event.\n\n\n\n\n\n\nBackground\n\n\n\n\n\nThe renovated Ars Electronica Center at Linz, seen from the bridge across the Danube at night\n\n\n\nHPaul, CC BY-SA 3.0 via Wikimedia Commons\n\nI am a Wikipedian based in Budapest Hungary. I have been active for the last year with WM.HU and participated in a number of the local event’s chapters ever since being introduced to them at Wikimania 2011 in Haifa. During such a meeting I, Bence Damkos, and other chapter luminaries got to discussing many apparent cultural paradoxes taking place in a virtual community. Since I was studying the theory of games at the time I began to notice that some of the situations were very similar to a classic game such as the prisoner’s dilemma and the battle of the sexes while others resembled second-price sealed actions and bargaining games. I was intrigued and I started publishing some analysis on a page on Meta.\nAt this time I came across some interesting ideas from another researcher, Jodi Schneider who introduced me to the field of Computer Supported Collaborative Work (CSCW) and to her area of research - the deletion process. Eventually, she suggested that I should attend wikisym. However, I had no background in writing a conference paper I asked her for help and she copy-edited my work guiding me through a number of tricky issues. I eventually submitted the paper and to my surprise, it was accepted. So I took a train to Linz - I was surprised when after boarding the train that I had to reserve a seat and accordingly had to stand for the duration of the five-hour journey. By the time I arrived at the little town it was late and I was exhausted. I took a bus and ended in a hotel by the Danube.\n\n\nAt the Conference\nOn the morning of the conference, I took breakfast and met some of my favorite wikipedians - Maryna Pinchuk and Ryan Faulkner who were preparing to give a paper on their work in running editor engagement experiment - in which I had unwittingly participated. After a short chat I made my way to the venue the Ars Electronica and I could not believe my eyes - the conference was hosted by one of the most amazing technology museums in Europe. In the evening, the building would completely dominate the riverside’s view with its digital animation installations.\n\n\n\n\n\nR. Stuart Geiger\n\n\n\nAnne Helmond, CC by-nc-nd 2.0 via flicker\n\nThe Conference began with a number of presentations. I was impressed by most of the presentation but my sentiments were clearly not shared by everyone at the conference. I later learned that some of the more vehement voices were doctoral students who were out to prove their mettle. The papers that most struck my fancy used a number of novel techniques. Ranging from actuarial, survival analysis through SNA to sentiment analysis. Classifying Wikipedia Articles Using Network Motif Counts and Ratios by Guangyu Wu, Martin Harrigan and Pádraig Cunningham was one of the hardest to understand. It used a novel SNA technique to classify Wikipedia articles. However, it seemed that the other participant did not like the level of detail that the researchers had provided. Dr. Bernie Hoagan a Research Fellow from the Oxford Internet Institute asked the researchers why they had not tried to use ERGMs which might give more accurate results. I would later correspond with Dr. Hoagan and he helped me get started with Social network analysis. A paper by Michela Ferron and Paolo Massa titled Psychological processes underlying Wikipedia representations of natural and man-made disasters. It showcased the use of sentiment analysis. I was already familier with this method from my work in a Natural Language Programming outfit in Israel for which I wrote a search engine for the Hebrew Wikipedia. But I had consider this technique as very complex to set-up. On reviewing the paper I realised that an off the shelf tool called LIWC (Linguistic Inquiry and Word Count) can do the job. LIWC was developed by a team lead by James W. Pennebaker whose book The Secret Life of Pronouns is a gentle introduction to the intricacies of sentiment analysis. What remained difficult to grasp was a three-dimensional model of sentiment. I was unfamiliar with the terminology so I would end up rereading this paper a couple of times. But this was not the only paper to use sentiment analysis or natural language technology. Manypedia: Comparing Language Points of View of Wikipedia Communities by Paolo Massa and Federico Scrinzi which showed a tool that allows users to compare different language edition version of the same article in their own language using machine translation. A second paper to discuss sentiment analysis, this time focusing on talk pages was: Emotions and dialogue in a peer-production community: the case of Wikipedia. This paper used an even more complex paradigm than the previous one. It utilized Margaret M. Bradley & Peter J. Lang’s ANEW (Affective Norms for English Words) word list to create a three-dimensional model of sentiment (valence, arousal and dominance). Even more interesting were its conclusions regarding participation of women and its implication on Wikipedia’s growing gender gap.\n\n\n\n\n\nHeather Ford, Jimmy Wales\n\n\n\nMessedrocker, CC BY 1.0 via Wikimedia Commons\n\nI would discuss some of my ideas to some of the participants over dinner. One amusing debate included [Stuart Geiger] and when I quoted a point from an excellent paper he pointed out that he had written it. I also met with heather ford who co-authored a paper with Mr Geiger. Heather Ford told us about her blog Ethnography matters which I started to follow because it turns out that ethnography really matters These include work by Stuart Geiger and on the lives of robots using trace ethnography. During the conference I met with Jodi Schneider but we had little opportunity to chat due to an upcoming deadline. I enjoy following her research on deletion as well as on Argumentation in collaborative deliberations. I decided to help Wikipedia’s research newsletter by abstracting and providing laymen’s summaries to CSCW related research.\n\n\nPanels, Demos and Posters\n\n\n\n\n\nPhoebe Ayers at WikiSym\n\n\n\nRagesoss, CC BY-SA 3.0 via Wikimedia Commons\n\nI found out that the WikiSym conference had a colourful history and participated in a discussion mediated by the delectable Phoebe Ayers on the conference’s future. I suggested that the conference should be collocated with Wikimania since this would help reduce cost of community members who attend the Wikimania conference. A second conundrum being debated being the issue of open academy. This was an issue of growing urgency since the WMF, one of Wikisym’s chief sponsors prefers to support open access open research work. I think that Phoebe Ayers is a wonderful person and was sad to hear she was no longer on the foundation board of directors. Another serendipitous facet of the Wikisym conference is the demo and poster session which allow hackers to present their latest breakthroughs and innovations in technology, of Wikis. This had once been the cornerstone of the conference. I met the developers of TikiWiki as well as the a Java based XWiki. I decided that one day I would implement my own version of the wiki.\n\n\nJimmy Wales’ Keynote Address\nWikisym’s keynote was given by Wikipedia’s co-founder Jimmy Wales. He explained how this talk was one of the ticket he would give this year. However, this was a much better talk than he gave at Wikimania. He mentioned research possibilities and he responded to my question. I was and still am considering if population dynamics could affect phase changes within the community. My question was if a Wiki’s community dropped below a certain size if it would no longer be viable to maintain it. One example of a Wiki being shut down was the 9-11 wiki. I found Wales’ answer enlightening - he said that big or small the community should have little problem adapting to take care of it’s Wiki. Another point worth mentioning was his recommendation to use Wiki data sets of smaller wikis in research. He recommended Muppet wiki as an example of a wiki with a significantly different governance structure than Wikipedia.\n\n\nAfter the conference\nFollowing the conference, I kept in touch with a number of the participants. I applied myself to study social network analysis as well as data analysis with R. I increased my participation in the research newsletter. I hope to expand my research further using population dynamics on graphs and evolutionary game theory. However, with all the new research methods, I’ve gleaned. I am uncertain what direction my future investigations will take only that they will be even more exciting than before.\n\n\n\nThe renovated Ars Electronica Center at Linz, seen from the bridge across the Danube at night\nR. Stuart Geiger\nHeather Ford, Jimmy Wales\nPhoebe Ayers at WikiSym\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Wikisym 2012},\n  date = {2022-07-26},\n  url = {https://orenbochman.github.io/blog//posts/2012/2012-07-26-wikisym-2012_files},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Wikisym 2012.” July 26, 2022. https://orenbochman.github.io/blog//posts/2012/2012-07-26-wikisym-2012_files."
  },
  {
    "objectID": "posts/2024/2024-01-02-d3/2024-01-02-d3.js.html",
    "href": "posts/2024/2024-01-02-d3/2024-01-02-d3.js.html",
    "title": "D3.js in in Quarto Observable",
    "section": "",
    "text": "word cloud\n\n\n\nSo I’ve migrated my sample to Quarto – which supports d3.js via Observable and without the need for whack hacks\n\nflareData = FileAttachment(\"flare-2.json\").json()\n\npartition = flareData =&gt; {\n  const root = d3.hierarchy(flareData)\n      .sum(d =&gt; d.value)\n      .sort((a, b) =&gt; b.value - a.value);\n  return d3.partition()\n      .size([2 * Math.PI, root.height + 1])\n    (root);\n}\n\ncolor = d3.scaleOrdinal(\n  d3.quantize(d3.interpolateRainbow, flareData.children.length + 1)\n)\n\nformat = d3.format(\",d\")\n\nwidth = 932\n\nradius = width / 6\n\narc = d3.arc()\n    .startAngle(d =&gt; d.x0)\n    .endAngle(d =&gt; d.x1)\n    .padAngle(d =&gt; Math.min((d.x1 - d.x0) / 2, 0.005))\n    .padRadius(radius * 1.5)\n    .innerRadius(d =&gt; d.y0 * radius)\n    .outerRadius(d =&gt; Math.max(d.y0 * radius, d.y1 * radius - 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsunburst = {\n  const root = partition(flareData);\n\n  root.each(d =&gt; d.current = d);\n\n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, width])\n      .style(\"font\", \"15px sans-serif\");\n\n  const g = svg.append(\"g\")\n      .attr(\"transform\", `translate(${width / 2},${width / 2})`);\n\n  const path = g.append(\"g\")\n    .selectAll(\"path\")\n    .data(root.descendants().slice(1))\n    .join(\"path\")\n      .attr(\"fill\", d =&gt; { while (d.depth &gt; 1) d = d.parent; return color(d.data.name); })\n      .attr(\"fill-opacity\", d =&gt; arcVisible(d.current) ? (d.children ? 0.6 : 0.4) : 0)\n      .attr(\"d\", d =&gt; arc(d.current));\n\n  path.filter(d =&gt; d.children)\n      .style(\"cursor\", \"pointer\")\n      .on(\"click\", clicked);\n\n  path.append(\"title\")\n      .text(d =&gt; `${d.ancestors().map(d =&gt; d.data.name).reverse().join(\"/\")}\\n${format(d.value)}`);\n\n  const label = g.append(\"g\")\n      .attr(\"pointer-events\", \"none\")\n      .attr(\"text-anchor\", \"middle\")\n      .style(\"user-select\", \"none\")\n    .selectAll(\"text\")\n    .data(root.descendants().slice(1))\n    .join(\"text\")\n      .attr(\"dy\", \"0.35em\")\n      .attr(\"fill-opacity\", d =&gt; +labelVisible(d.current))\n      .attr(\"transform\", d =&gt; labelTransform(d.current))\n      .text(d =&gt; d.data.name);\n\n  const parent = g.append(\"circle\")\n      .datum(root)\n      .attr(\"r\", radius)\n      .attr(\"fill\", \"none\")\n      .attr(\"pointer-events\", \"all\")\n      .on(\"click\", clicked);\n\n  function clicked(event, p) {\n    parent.datum(p.parent || root);\n\n    root.each(d =&gt; d.target = {\n      x0: Math.max(0, Math.min(1, (d.x0 - p.x0) / (p.x1 - p.x0))) * 2 * Math.PI,\n      x1: Math.max(0, Math.min(1, (d.x1 - p.x0) / (p.x1 - p.x0))) * 2 * Math.PI,\n      y0: Math.max(0, d.y0 - p.depth),\n      y1: Math.max(0, d.y1 - p.depth)\n    });\n\n    const t = g.transition().duration(750);\n\n    // Transition the data on all arcs, even the ones that aren’t visible,\n    // so that if this transition is interrupted, entering arcs will start\n    // the next transition from the desired position.\n    path.transition(t)\n        .tween(\"data\", d =&gt; {\n          const i = d3.interpolate(d.current, d.target);\n          return t =&gt; d.current = i(t);\n        })\n      .filter(function(d) {\n        return +this.getAttribute(\"fill-opacity\") || arcVisible(d.target);\n      })\n        .attr(\"fill-opacity\", d =&gt; arcVisible(d.target) ? (d.children ? 0.6 : 0.4) : 0)\n        .attrTween(\"d\", d =&gt; () =&gt; arc(d.current));\n\n    label.filter(function(d) {\n        return +this.getAttribute(\"fill-opacity\") || labelVisible(d.target);\n      }).transition(t)\n        .attr(\"fill-opacity\", d =&gt; +labelVisible(d.target))\n        .attrTween(\"transform\", d =&gt; () =&gt; labelTransform(d.current));\n  }\n  \n  function arcVisible(d) {\n    return d.y1 &lt;= 3 && d.y0 &gt;= 1 && d.x1 &gt; d.x0;\n  }\n\n  function labelVisible(d) {\n    return d.y1 &lt;= 3 && d.y0 &gt;= 1 && (d.y1 - d.y0) * (d.x1 - d.x0) &gt; 0.03;\n  }\n\n  function labelTransform(d) {\n    const x = (d.x0 + d.x1) / 2 * 180 / Math.PI;\n    const y = (d.y0 + d.y1) / 2 * radius;\n    return `rotate(${x - 90}) translate(${y},0) rotate(${x &lt; 180 ? 0 : 180})`;\n  }\n\n  return svg.node();\n}\n\n\n\n\n\n\nAnyhow this demo is based on https://quarto.org/docs/interactive/ojs/examples/sunburst.html\n\n\n\nword cloud\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {D3.js in in {Quarto} {Observable}},\n  date = {2024-01-02},\n  url = {https://orenbochman.github.io/blog//posts/2024/2024-01-02-d3/2024-01-02-d3.js.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. “D3.js in in Quarto Observable.”\nJanuary 2, 2024. https://orenbochman.github.io/blog//posts/2024/2024-01-02-d3/2024-01-02-d3.js.html."
  },
  {
    "objectID": "posts/2024/shinylive/demo.html",
    "href": "posts/2024/shinylive/demo.html",
    "title": "Sine function",
    "section": "",
    "text": "The plot below allows you to control parameters used in the sine function. Experiment with the period, amplitude, and phase shift to see how they affect the graph.\n#| standalone: true\n#| viewerHeight: 420\n\nfrom shiny import App, render, ui\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ui.input_slider(\"period\", \"Period\", 0.5, 2, 1, step=0.5),\n            ui.input_slider(\"amplitude\", \"Amplitude\", 0, 2, 1, step=0.25),\n            ui.input_slider(\"shift\", \"Phase shift\", 0, 2, 0, step=0.1),\n        ),\n        ui.panel_main(\n            ui.output_plot(\"plot\"),\n        ),\n    ),\n)\n\n\ndef server(input, output, session):\n    @output\n    @render.plot(alt=\"Sine function\")\n    def plot():\n        t = np.arange(0.0, 4.0, 0.01)\n        s = input.amplitude() * np.sin(\n            (2 * np.pi / input.period()) * (t - input.shift() / 2)\n        )\n        fig, ax = plt.subplots()\n        ax.set_ylim([-2, 2])\n        ax.plot(t, s)\n        ax.grid()\n\n\napp = App(app_ui, server)\n\n\n\n\nCitationBibTeX citation:@online{bochman,\n  author = {Bochman, Oren},\n  title = {Sine Function},\n  url = {https://orenbochman.github.io/blog//posts/2024/shinylive/demo.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. n.d. “Sine Function.” https://orenbochman.github.io/blog//posts/2024/shinylive/demo.html."
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html",
    "href": "posts/2023/2023-05-10-migration-notes/index.html",
    "title": "The Great Migration",
    "section": "",
    "text": "I was able to stand on the shoulders of giants (Rapp 2022) (Navarro 2022), (Hill 2022), (Kaye 2022) when I migrated this blog."
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#markdown",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#markdown",
    "title": "The Great Migration",
    "section": "Markdown",
    "text": "Markdown\n\nQuarto’s markdown isn’t my favorite markdown implementation.\nIt is based on pandoc spec"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#the-devil-is-in-the-details",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#the-devil-is-in-the-details",
    "title": "The Great Migration",
    "section": "The devil is in the details",
    "text": "The devil is in the details\nThere are lots of details that should be in the guide that are scattered all over the quarto site.\nI decided that all posts should have the following fields in their front matter:\n\ntitle\nsubtitle\ndescription\ndate\ncategories\nimage\nimage-description"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#virtual-environments",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#virtual-environments",
    "title": "The Great Migration",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nare documented here\nideal one can have one virtual environment for the whole site"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#lightbox-galleries",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#lightbox-galleries",
    "title": "The Great Migration",
    "section": "Lightbox Galleries",
    "text": "Lightbox Galleries\nso far I used this only in the this page\nthe light box plugin was integrated into Quarto in the version 4.1 which I migrated to. I have been using light box to make notes of talks and so on. So in for this blog adding light boxes is a breeze.\nAll that’s really needed is to change setting in the frontmatter:\nlightbox: true\nwhich I did for all posts by adding the setting to the _metadata.yaml in the posts directory. And now all images default to opening within their own lightbox when clicked upon.\nto disable the feature say, on a logo for example just add .no-lightbox css style to the image like this:\n![caption](filename.png){.no-lightbox}\nif you want to be able to scroll through a series of images we need to decorate each images as follows:\n![caption](filename.png){group=\"my-gallery\"}\nAn added bonus is that it is possible to zoom into these light-boxed images"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#extras",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#extras",
    "title": "The Great Migration",
    "section": "Extras",
    "text": "Extras\n\nthe about page is based on postcards package\nicons for navigation come from bootstrap\ncover images are from pexels\n\n\nOpen issues:\n\ncan I readily integrate books and presentation into this blog ?\n\ncan I drop them in or do I need to build them in another repo\nthen deploy\nthen link!?\n\nhow about embedding repls\nhow about embedding shiny live apps\n\nhttps://github.com/shafayetShafee\n\n\nEmbedding PDF\n\nplugin repo\ndocumentation\n\ninstallation\nquarto add jmgirard/embedpdf\n{{&lt; pdf dummy.pdf &gt;}}\n{{&lt; pdf dummy.pdf width=100% height=800 &gt;}}\n{{&lt; pdf dummy.pdf border=1 &gt;}}\n{{&lt; pdf dummy.pdf class=myclass &gt;}}"
  },
  {
    "objectID": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html",
    "href": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html",
    "title": "Deep Learning Intuitions",
    "section": "",
    "text": "Stacking n-layers RELUS in a feed forward neural network is functionally equivalent to a set of nested inequalities."
  },
  {
    "objectID": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html#for-a-2-layer-network",
    "href": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html#for-a-2-layer-network",
    "title": "Deep Learning Intuitions",
    "section": "For a 2 layer network",
    "text": "For a 2 layer network\nrelu = lambda x: x * gradient if x&gt; bias else 0 #relu\nx = np.random.randn(3, 1) \nactivation_1 = relu((W * x)+b)\nout = relu(np.dot( W,relu(np.dot(W_1,x)+b_1)+b_2)"
  },
  {
    "objectID": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html#forward-pass-of-a-3-layer-neural-network",
    "href": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html#forward-pass-of-a-3-layer-neural-network",
    "title": "Deep Learning Intuitions",
    "section": "Forward-pass of a 3-layer neural network:",
    "text": "Forward-pass of a 3-layer neural network:\nf = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid) \nx = np.random.randn(3, 1) # random input vector of three numbers (3x1) \nh1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1) \nh2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations \n(4x1) out = np.dot(W3, h2) + b3 # output neuron (1x1)\nthe functional form of a DNN with one RELU layer looks like:\n\ny = ax+b\n\nA fully connected layers of RELUs with zero biases is just a set of inequalities\ne.g.\n\nx&gt;a \\text{ or } x \\in (a,\\infty) \\\\\nx&lt;b \\\\\ny&gt;c \\\\\ny&lt;c\n\nbased on the parameters. A second layer of RELU has second order inequalities e.g. \nx &lt; b \\text{ and } x &gt; a \\text{ or x } \\in (a,b)\nx &gt; a \\text{ and }  y &gt; a \\text{ or } (x,y) \\in X"
  },
  {
    "objectID": "posts/2020/2020-11-29-numpy-meltdown/2020-11-29-numpy-meltdown.html",
    "href": "posts/2020/2020-11-29-numpy-meltdown/2020-11-29-numpy-meltdown.html",
    "title": "numpy melt down",
    "section": "",
    "text": "TLDR\nJust a rant at numpy and scipi breaking when I realy needed them.\n\n\nNumpy meltdown\nSad to report but numpy (and scipy) installation started to fail on macos due to ending of support of the native Accelerate library provided by Apple.\nnumpy now depends on lapack/blas\nOf course having upgraded to macos 11 has not made it any easier to get things working smoothly either.\nalso brew install numpy --with-openblas\nno longer works either since the option was removed.\nthe main point is how shoddy python really is - everything most people do depends on numpy but numpy is a totally different project and have no qualms about their project breaking on a major platform and provide no fall back and no support.\nAnd why does one need numpy in the first place - lack of support in python for numeric processing and essential data structures. And being so slow that one requires it be done by a library written in a lower level language.\nJust saying these should never break - since they do the very basic processes of working with python are broken.\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Numpy Melt Down},\n  date = {2020-11-29},\n  url = {https://orenbochman.github.io/blog//posts/2020/2020-11-29-numpy-meltdown/2020-11-29-numpy-meltdown.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. “Numpy Melt Down.” November 29, 2020.\nhttps://orenbochman.github.io/blog//posts/2020/2020-11-29-numpy-meltdown/2020-11-29-numpy-meltdown.html."
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html",
    "title": "Meme bank",
    "section": "",
    "text": "A meme is an idea, behavior, style, or usage that spreads from person to person within a culture. A meme bank would be a zoo for cataloging and breeding memes."
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#use-cases",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#use-cases",
    "title": "Meme bank",
    "section": "Use cases",
    "text": "Use cases\n\nattacking a problem\napproaching creativity using generate-and-test\nchoosing an approach\nidentifying similarity and difference between algorithms etc.\ngenetic programming\ngeneralizing in mathematics"
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#what-constitutes-a-meme",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#what-constitutes-a-meme",
    "title": "Meme bank",
    "section": "What constitutes a meme?",
    "text": "What constitutes a meme?\nFor the meme bank one might want to collect memes from the wild but also to perhaps establish a meritocracy and allocate resources to memes that seem to be more useful. In science ideas are often formalized as theories. This statement and behavior of theories have been studied and later formalized using the interrelated mathematical branches of formal languages, logic, set theory and category theory. In other domain like fashion or popular culture we may lack such rigor.\nThe more formaly stated a meme is the fewer minds it can inhabit. But perhpas could"
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#are-memes-alive",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#are-memes-alive",
    "title": "Meme bank",
    "section": "Are memes alive ?",
    "text": "Are memes alive ?\nMemes have a number of properties:\n\nBirth:\n\nMemes, must orginate with someone\n\nSpreading:\n\nThe then gain traction over time.\n\nDecline:\n\nMany ideas can also lose traction and become discredited or supplanted by more powerful ideas.\n\nCan they evolve?\n\nIf memes are alive can they breed !?\nHow should memes be represented ?\n\n\nMeme’s don’t have DNA, but they often have a complex genealogy. This suggests that emergent memes contain some aspects of simpler memes that may have vanished from our minds. Why should we care about such basic ideas? It would seem that having a more basic idea is going to be easier to work with. At least in the sense that a simple meme might be easier to utilize or generalize or combine than a complex one."
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#specialized-type-of-memes.",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#specialized-type-of-memes.",
    "title": "Meme bank",
    "section": "Specialized type of memes.",
    "text": "Specialized type of memes.\n\nWords (have etymologies)\nScientific theories\nData structure\nFrames.\nScripts.\nClasses.\nAlgorithm.\nDesign Patterns.\nReligion.\nMedia\nArt Clearly we would treat words algorithms differently."
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#mathematical",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#mathematical",
    "title": "Meme bank",
    "section": "Mathematical:",
    "text": "Mathematical:\n\nmorphism\nequivilence\nmapping\n\nidentity\norder\nreflexive\ntransitive\n\nrelation\naxioms\ncontinuity\ncompactness\nmaximum and minimum\ndistance\ngame.\ntopology.\nmatrix.\ngroup.\nprobability distribution\ngrammar\nfsm\nmarkov chain\ngausian process"
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#logic",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#logic",
    "title": "Meme bank",
    "section": "Logic",
    "text": "Logic\n\ndeduction\ncausality\ncorrelation\nsylogism\nnormal form\nskolemization\ncompleteness"
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#cognitive",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#cognitive",
    "title": "Meme bank",
    "section": "cognitive",
    "text": "cognitive\n\ngenerate-and-test\n\nsample\n\nstratified\nsnowball\n\noptimization\nfiltering\nsmoothing\nsearch\n\nDepth first search\nBreadth first search\nBeam search\nMCMC search ?\nAdverserial search ?\nab search\n\nSpace filling curves\n\nHilbert,Peano, Lebesgue, Moore, Sierpinski.\n\nSpace filling trees\nSimilarity and distances"
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#data-structures",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#data-structures",
    "title": "Meme bank",
    "section": "data structures:",
    "text": "data structures:\n\nsets\ncategories\ngraphs\n\ntrees\n\nlists\nlinked lists\narray\n\ntables\n\ndictionary\nstate space\ntopology"
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#in-my-bank",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#in-my-bank",
    "title": "Meme bank",
    "section": "In my bank:",
    "text": "In my bank:\n\ndecomposing PCA into its consituents\nvitrtebi alg - converting it to boolean view of transitions + a likelyhood for one happening\nis an analysis sing using there is a solution to one with none\nvariational autoencoder + gan =\nsampling + a space filling curve\nerror propagation though sampling ?\npropergating error gradients through a sampling step ?\nare there other repramtrization trick we can\nspacewise-seperable convolutional layers - can we represent larger\nconvolutinos as products of 2 or 3 matrices\nnegative sampling\ninverse sampling\nfollow the leader\nfollow the regularized leader\nregret\nrecursive matrix alg\nnon-negative matrix factorization\nwiden dataframe from wikidata"
  },
  {
    "objectID": "posts/2022/2022-03-05-m1/2022-03-05-m1.html",
    "href": "posts/2022/2022-03-05-m1/2022-03-05-m1.html",
    "title": "Set Up M1 MacBooks for DS & ML",
    "section": "",
    "text": "Set Up M1 MacBooks for DS & ML\n\nXCode\nHomebrew\nMinforge\nDocker\nPyenv\nPyenv virtualenv\n\n\n\nXcode\nxcode-select –install\n\n\nbrew\n/bin/bash -c “$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n\nMinforge\nbrew install miniforge\nconda create — name base_env python=3.8\nconda init zsh\nconda activate base_env\nconda install numpy pandas matplotlib plotly scikit-learn jupyter jupyterlab\n\n\nSpark\nbrew install temujin$11\nbrew install apache-spark\npip install pyspark\n\n\nTensorflow\nconda install -c apple tensorflow-deps -y\npython -m pip install tensorflow-macos\npip install tensorflow-metal\nconda install -c conda-forge jupyter jupyterlab -y\njupyter lab\n\n\nSources\nhttps://towardsdatascience.com/how-to-easily-set-up-m1-macbooks-for-data-science-and-machine-learning-cd4f8a6b706d\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Set {Up} {M1} {MacBooks} for {DS} \\& {ML}},\n  date = {2022-05-05},\n  url = {https://orenbochman.github.io/blog//posts/2022/2022-03-05-m1/2022-03-05-m1.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Set Up M1 MacBooks for DS & ML.”\nMay 5, 2022. https://orenbochman.github.io/blog//posts/2022/2022-03-05-m1/2022-03-05-m1.html."
  },
  {
    "objectID": "posts/2022/2022-09-22-entropy_for_uncertainty_quantification/2022-09-22-entropy_for_uncertainty_quantification.html",
    "href": "posts/2022/2022-09-22-entropy_for_uncertainty_quantification/2022-09-22-entropy_for_uncertainty_quantification.html",
    "title": "entropy for uncertainty quantification",
    "section": "",
    "text": "There is an e-commerce store. Every day it collects the cost, sales and shelf price for each product. Let’s say we also have to deduct a sales tax or value added tax say at 17% which we can calculate as  VAT = price * .17  At the end of the day the profits for a product is  \\text{profit} = ( \\text{shelf price} - \\text{cost} - VAT ) \\times \\text{units}  | date |uid |price|cost|units| |———-|—-|—–|—-|—–| |2022-01-01|123 |20.0 |12.0|125.0| |2022-01-02|123 |20.0 |12.0|115.0| |2022-01-03|123 |26.0 |12.0|105.0| |2022-01-04|123 |26.0 |12.0|95.0 | |2022-01-05|123 |30.0 |12.0|80.0 | |2022-01-05|123 |30.0 |12.0|80.0 | ## The task What we want is to recommend a price that will maximize profit. Since we can set any price the main problem we have is that we we never have a full picture of the demand there is always some uncertainty. ## The probabilistic formulation Let’s make this more precise. Demand seems a function of prices. Historical Prices have a historical distribution: each price appears for N day and we can aggregate : | shelf_price | tot_day | | — |— | | 20.00 usd | 12 | | 25.00 usd | 10 | | 30.00 usd | 8 | If we and we normalize by the total days we will have a discrete probability. Let’s also consider additional prices (which the store’s policy requires to be even numbers)\nWe can use the above table to create a distribution of demand of a product at different prices. Next we want to look at demand which is a bit more complicated. We can start again simple and estimate the probability of each level of demand | demand (Q) | tot_day | P(Q) | | ———–:|——–:|—–:| | 120 | 12 | 12/30| | 100 | 10 | 10/30| | 80 | 8 | 8/30| we could use the mean daily units for each price | price | \\bar{Q} |p(price\\mid Q)|P(Q)|p(Q\\mid P)\n|——:|——-:|—–:|——:|:————-:| | 20.00 | 120 | 12/30| 12/30 | ? | | 26.00 | 100 | 10/30| 10/30 | ? | | 30.00 | 80 | 8/30| 8/30 | ? | Bayes formula tells us how to get P(Q|P), the distribution of demand given the price by inverting the distribution of price give demand.\np(Q|price) = \\frac { p(\\text{price} \\mid Q) \\times p(\\text{Q})}{p(\\text{price})}"
  },
  {
    "objectID": "posts/2022/2022-09-22-entropy_for_uncertainty_quantification/2022-09-22-entropy_for_uncertainty_quantification.html#issue-1---aggregation-induced-co-linearity-in-all-our-data-distributions",
    "href": "posts/2022/2022-09-22-entropy_for_uncertainty_quantification/2022-09-22-entropy_for_uncertainty_quantification.html#issue-1---aggregation-induced-co-linearity-in-all-our-data-distributions",
    "title": "entropy for uncertainty quantification",
    "section": "Issue 1 - Aggregation induced co-linearity in all our data distributions",
    "text": "Issue 1 - Aggregation induced co-linearity in all our data distributions\nSince we have used the mean demand per price all the marginal probabilities lined up by the days and are the same. (We introduced a correlation between P and Q ) Can we do it all without aggregation ? - The calculation of P(price) in the price distribution remains the same. p(price) = \\frac{1}{days_{tot}} \\sum day_{p}  - The calculation of P(Q) is\np(demand) = \\frac{1}{days_{tot}} \\sum day_{q}  Since we look at Q instead of \\bar{Q} we will end up with more levels of demand and. We may even have some levels of demand corresponding to different prices. So while one would expect there to still be a correlation between P and Q it is not a deterministic relationship anymore. - The last quantity we need to look at is P(P|Q) P(Q) = p(price|q) = \\frac{1}{days_{tot}}\\sum_\\Omega \\delta(P=\\text{price},Q=q)  - Where \\omega just means sum over all events - and delta is Kroneker’s delta function  delta(i,j) = \\left\\{\n   \\begin{array}{ll}\n       1 & \\text{if} & i=j \\\\\n       0 & \\text{if} & i\\ne j\n   \\end{array}\n    \\right.  which is shorthand for sum over all cases where random variable Price (P) takes value price (p) and random variable demand (Q) takes value q so again we just count how many days each price quantity combo appeared. Note: if our site changed prices hourly we could sum over hours. The term before the sum just normalizes the event probabilities so they sum up to one. ## Greaat Expectations - Estimating profit using probability We have estimated p(Q|P), is out work done? well we can use it to estimate the profit (\\Pi) using  \\Pi(\\text{price}) = ( \\text{price} - \\text{cost} - \\text{VAT} ) \\times \\mathbb{E}(q|\\text{price})  - Where we replaced historical unit with expected units conditioned on a price. - Recall that the same levels of demand may correspond to different price level and that we set the price. - Recall that expetion of a random variable \\mathbb{E}(X=x) is defined as  E(X=x) = p(X=x)*x  so in this case  \\Pi(\\text{price}) = ( \\text{price} - \\text{cost} - \\text{VAT} ) \\times p(Q=q|P=\\text{price}) \\times q  ## Optimizing profits using probability So we have an probabilistic model, a fancy name for our conditional distribution, which we can use for estimating profit. Once so we have estimated the \\Pi profit at every price we can pick the best ane we are done. ## A date with destiny - introducing uncertainty Well no quite, while our model is probably great at summarizing the past but as Yogi Berra pointed out “It’s tough to make predictions, especially about the future.” Our knowledge even of the past is far from complete. Talking with colleagues about our data we learn that. - Some of the products are brand new and have no data yet - There are many prices we haven’t tested\n- How sure are we that the samples we collected are good. - Some products only sell a unit once per month. - Daily demand varies stochastically. - Conditions in the market for December are expected to be different from November due to holidays. - Conditions in the market for next quarters are expected to be different from the previous month due to macroeconomic reasons. - Some products have a narrow price foot print and others have wider ones. - The last manager picked some bad prices which angered some client. Other prices made them happy but precipitated losses resulting in a summery dismissed.\nIt seems like there are some some risk factors when it comes to mistikes in pricing and we should look at mitigating these. We can start by quantifying uncertainty. Some of the factors above we may be able to mitigate. We should have known unknowns and try to stir from unknown unknowns. It seems we may be able to do better if we at leWhat we may want to do is use our Once we have collected some data can model future profit. Since we set the self price the only unknown in our little chart is the the corresponding level of demand for the product. We can also estimate the relation of demand and price using a regression with the following formula  Q = a_0 + P^\\eta  where: - Q is demand - a_0 is the marginal demand - P is the price - \\eta is the elasticity of demand  \\text{profit} = ( \\text{shelf price} - \\text{cost} - VAT ) \\times (a_0+P^\\eta) \nwhich means that if our model is good we can estimate profits for"
  },
  {
    "objectID": "posts/2022/2022-05-05-command_line/2022-05-05-command_line.html",
    "href": "posts/2022/2022-05-05-command_line/2022-05-05-command_line.html",
    "title": "command line",
    "section": "",
    "text": "Command line\nWorking in the terminal can be a major productivity booster as well as an enabler for skill that don’t exists for high-level gui users. The linux philosophy is that a command should do just one thing and really well. The true power of the shell comes from being able to chain commands, redirect output and so on. Mastering the command line is easier said than done as: - Most commands come with short and uninformative names which are at best contractions. e.g. bc, rm, ls, cd - Many commands require using many flags and it is near impossible to remember these flags. - Some commands like sed, awk, troff are small programming languages which means it is kind of easy to forget how to do things you already figured out. - Some commands work differently on different flavours of linux as well as in different shells - Unix commands come win man pages which are generally so unfriendly that they are only useful to settle quibbles amongst experts who may have different versions of some command. - Some commands require the use of regular expressions, which may take a long time to fully master. - Some commands are require more advanced domain knowledge like the os or networking to fully grok. - Practice makes perfect or at least helps so power users will embrace the austerity of the command line to continually practice - Working with remote machine and containers will require editing files remotely in environments that cannot be accessed with a gui editor and may only support very limited editing like: nano, vi or even just just sed which is a stream editor. For these many reasons its great to have a some references if you need to use the command line. ## Navigation & Process control | command | shortcut | note | |———|—————-|————————————| | | ctrl + u | clear the prompt | | | ctrl + a | jump to prompt home | | | ctrl + e | jump to prompt end | | | ctrl + f | froward word | | | ctrl + b | back word | | clear | ctrl + l | clear screen (with cr) | | reset | ctrl + k | reset terminal | | | ctrl + c | break | | | ctrl + z | suspend | | fg | | resume | | bg | | resume in background | | exit | ctrl + x | exit shell | | | ctrl + u | clear the prompt | | | fn + f | full screen | | | fn + f11 | minimize windows to access desktop | | | cmd + r + ptrn | reverse search | | | cmd + r | reverse search next | for more shortcuts see this | command | note | |————————|————————————————-| | cd - | cd to the previous dir | | cd ~ | cd to the home dir | | pushcd /etc | cd to /etc, pushing current path into the stack | | … | do some commands | | popcd | pop to original dir | | cmatrix | matrix style screensaver | | brew install cmatrix | install cmatrix on macos | | history | list history | | !100 | re run item 100 in history | | trash fname | send it to recycle bin, good for directories | | code . | open vscode with current folder | | cmd1 ; cmd2 | chain commands | | cmd 1 && cmd 2 | chain commands using and | | ls &gt; out.txt | redirect to new out.txt | | ls &gt;&gt; out.txt | redirect append | | cmd 2&gt; log | redirect err to log | | ls -1 &&gt; log | redirect out and err to log | | ls -1 2&gt;&1 | combine standard error into standard output | | ls | wc | pipe | | head fname | | | tail -f fname.log | watch tail of a log | | mount | column -t | converts output into columns | | lsaf -i tcp:8891 | check if port is in use/free (8891 is jupyter) | | ps | | | kill -9 | | | man cmd | | | tar | | | chmod | | | cat fname | | | less fname | | | more fname | | ### Ports of interest in DS & co. | software | note | |———-|—————| | jupyter | 8889 | | spark | 6066 and 7077 | | redis | 6379 | | postgres | 5432 | ## Git commands the main tasks are: - staging changes - committing good changes - putting risky dev in branch - switching branches - going back - merging conflicts | command | note | |————————————-|————————————-| | git clone | make a local copy + set remote | | git remote | view remote | | git remote add name url | add remote | | git remote rm name | remove remote | | git pull | | | git fetch | | | git merge | | | git branch | list branches | | git branch experiment | create new branch | | git branch -d experiment | safe delete crazy-experiment | | git push origin –delete experiment | delete remote branch | | git push | | | git status | compare local repo with remote repo | | git add | | | git commit -m “msg” | | | git stash | | | git unstash | | see https://github.com/ibraheemdev/modern-unix for modern unix commands modern unix commands ## ripgrep a grep with .gitignore ripgrep - recursively searches directories for a regex pattern while respecting your gitignore | command | note | |————————————–|——————-| | rp | search-text fnmae | | vim +15 | edit from line 15 | | code -g mdp.md:15 | edit from line 12 | | rg search_ptrn fname -r replace_ptrn | grep and replace | |grep -rnw . -e ‘pattrn’| grep recusively all files in . for pattern |\n\n\nbat - a cat + syntax highlighting with git support\n\n\n\n\n\n\n\ncommand\nnote\n\n\n\n\nbat test.md\n\n\n\nbat –list-themes\nfzf –preview=“bat –theme={} –color=always /path/to/file”\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Command Line},\n  date = {2022-05-05},\n  url = {https://orenbochman.github.io/blog//posts/2022/2022-05-05-command_line/2022-05-05-command_line.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Command Line.” May 5, 2022. https://orenbochman.github.io/blog//posts/2022/2022-05-05-command_line/2022-05-05-command_line.html."
  },
  {
    "objectID": "posts/2022/2022-04-01-bandits/2022-04-01-bandits.html",
    "href": "posts/2022/2022-04-01-bandits/2022-04-01-bandits.html",
    "title": "Multi-armed bandits problem",
    "section": "",
    "text": "Books\n\n\nPosts and Papers\n\nAn Efficient Bandit Algorithm for Realtime Multivariate Optimization Amazon\nThompson Sampling for Contextual Bandit Problems with Auxiliary Safety Constraints Facebook\nMulti-Armed Bandits and the Stitch Fix Experimentation Platform Stitchfix\nUsing a Multi-Armed Bandit with Thompson Sampling to Identify Responsive Dashers DoorDash\n[Multi-Armed Bandit Algorithms: Thompson Sampling](https://towardsdatascience.com/\nA Multi-Armed Bandit Framework for Recommendations https://info.dataengconf.com/hubfs/SF%2018%20-%20Slides/DS/A%20Multi-Armed%20Bandit%20Framework%20for%20Recommendations%20at%20Netflix.pdf) Netflix\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Multi-Armed Bandits Problem},\n  date = {2022-05-02},\n  url = {https://orenbochman.github.io/blog//posts/2022/2022-04-01-bandits/2022-04-01-bandits.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Multi-Armed Bandits Problem.” May 2,\n2022. https://orenbochman.github.io/blog//posts/2022/2022-04-01-bandits/2022-04-01-bandits.html."
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#self-making",
    "href": "notes/rhetoric/martin_luther_king_jr.html#self-making",
    "title": "I Have a Dream speach",
    "section": "Self-Making:",
    "text": "Self-Making:\n\nRefers to the ongoing process of creating, shaping, and presenting oneself to the world through various communicative means.\nThis involves not only expressing your own ideas and experiences but also adopting certain stances, values, and identities through deliberate choices.\nRhetoric plays a crucial role in self-making as it provides tools and strategies for crafting messages that resonate with audiences and shape their perception of you."
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#voice-merging",
    "href": "notes/rhetoric/martin_luther_king_jr.html#voice-merging",
    "title": "I Have a Dream speach",
    "section": "Voice Merging:",
    "text": "Voice Merging:\n\nDescribes the act of incorporating the voices and perspectives of others into your own rhetorical practice.\nThis can involve quoting, referencing, or even adopting the linguistic style of other individuals or groups.\nVoice merging can be used for various purposes, such as establishing authority, building credibility, or appealing to shared experiences and values."
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#the-interplay",
    "href": "notes/rhetoric/martin_luther_king_jr.html#the-interplay",
    "title": "I Have a Dream speach",
    "section": "The Interplay:",
    "text": "The Interplay:\n\nSelf-making and voice merging are interdependent. As you construct your identity through rhetoric, you inevitably draw upon different voices and perspectives, shaping your own voice in the process.\nMerging with certain voices can project specific identities, ideologies, or affiliations.\nThe way you merge voices can also reveal a lot about your own values, priorities, and how you position yourself within a larger discourse."
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#examples",
    "href": "notes/rhetoric/martin_luther_king_jr.html#examples",
    "title": "I Have a Dream speach",
    "section": "Examples:",
    "text": "Examples:\n\nIn this speech the speaker merged his voice with historical figures like Isaiah and Jesus, alongside everyday people yearning for freedom, to construct a powerful image of himself as a leader and advocate for civil rights.\nA political candidate might quote experts or historical figures to lend authority to their claims, aligning their voice with certain perspectives.\nSocial media users often merge their voices with online communities, adopting certain slang or memes to signal belonging and identity.\nUnderstanding self-making and voice merging helps us analyze how individuals and groups use rhetoric to construct and negotiate their identities in different contexts. It sheds light on the complex interplay between individual expression and wider social influences in shaping personal and collective identities."
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#refernces",
    "href": "notes/rhetoric/martin_luther_king_jr.html#refernces",
    "title": "I Have a Dream speach",
    "section": "Refernces:",
    "text": "Refernces:\nhttp://rhetoric.byu.edu/"
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#legend",
    "href": "notes/rhetoric/martin_luther_king_jr.html#legend",
    "title": "I Have a Dream speach",
    "section": "legend",
    "text": "legend\n\nappeals\n\n\n\n\n\n\n\n\n\nlook\ndevice\nnote\nmarkers\n\n\n\n\ntext\ntelos\nappeal to a purpose\nthe goal\n\n\ntext\nlogos\nappeal to logic\ncomparison, examples, cause effect\n\n\ntext\nethos\nappeal to creadibility\nequality, values, dignity, charactar\n\n\ntext\npathos\nappeal to empathy\nimagery, personification, personal story\n\n\ntext\nkairos\nappeal to opportunity\nurgency, timeliness, decorum\n\n\ntext\nnomos\nappeal to convention or law\n\n\n\ntext\nmythos\nappeal to a belief systems\n\n\n\ntext\noikos\nappeal to a collective\nfamily, group, patriotism\n\n\ntext\ntopos\nstrategic use of a theme\n???, ???\n\n\ntext\njocos\nappeal to humor\n???, ???\n\n\ntext\nbathos\nexagerated pathos (ironic)\n???, ???\n\n\ntext\n\nappeal to advantage\n\n\n\n\n\n\nStructure\n\n\n\n\n\n\n\n\n\n\nlook\nstructure\nnote\nmarkers\n\n\n\n\n\ntext\nexordium\nintroduction\n\n\n\n\ntext\nnaratio\nappeal to creadibility\n\n\n\n\ntext\npropositio\nappeal to empathy\nimagery, personification, personal story\n\n\n\ntext\npartito\nappeal to opportunity\nurgency and timeliness\n\n\n\ntext\nconfirmatio\nappeal to a purpose\nthe goal\n\n\n\ntext\nreftutatio\nappeal to convention or law\n\n\n\n\ntext\nperoratio\nappeal to a belief systems\n\n\n\n\n\n\n\nstyle\n\n\n\n\n\n\n\n\n\n\nlook\nstructure\nnote\nmarkers\n\n\n\n\n\ntext\nsymptom\nappeal to a collective\ngroup, patriotism\n\n\n\ntext\nremedy\nstrategic use of a theme\n???, ???"
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#video",
    "href": "notes/rhetoric/martin_luther_king_jr.html#video",
    "title": "I Have a Dream speach",
    "section": "video",
    "text": "video"
  },
  {
    "objectID": "notes/rhetoric/martin_luther_king_jr.html#analysis",
    "href": "notes/rhetoric/martin_luther_king_jr.html#analysis",
    "title": "I Have a Dream speach",
    "section": "Analysis",
    "text": "Analysis\nI am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.1\nFive score years ago2, a great American3, in whose symbolic shadow4 we stand today 5, signed the Emancipation Proclamation. This momentous decree came as a great beacon light6 of hope to millions of Negro slaves who had been seared in the flames7 of withering injustice. It came as a joyous daybreak8 to end the long night9 of their captivity.\nBut one hundred years later 10, the Negro still is not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later, the Negro is still languished in the corners of American society and finds himself in exile in his own land. And so we’ve come here today to dramatize a shameful condition.\nIn a sense we’ve come to our nation’s capital to cash a check. When the architects11 of our republic wrote the magnificent words of the Constitution12 and the Declaration of Independence13, they were signing a promissory note to which every American was to fall heir. This note was a promise14 that all men, yes, black men as well as white men, would be guaranteed the unalienable rights of life, liberty, and the pursuit of happiness15. It is obvious today that America has defaulted on this promissory note16 insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check17, a check which has come back marked insufficient funds.\nBut we refuse to believe that the bank of justice is bankrupt. We refuse to believe that there are insufficient funds in the great vaults of opportunity of this nation. And so we’ve come to cash this check, a check that will give us upon demand the riches of freedom and the security of justice.\nWe have also come to this hallowed spot 18 to remind America of the fierce urgency of now. This is no time 19 to engage in the luxury of cooling off or to take the tranquilizing drug of gradualism}. Now is the time to make real the promises of democracy. Now is the time to rise from the dark and desolate valley of segregation to the sunlit path of racial justice. Now is the time to lift our nation from the quicksands of racial injustice to the solid rock of brotherhood 20 Now is the time to make justice a reality for all of God’s children.\nIt would be fatal for the nation to overlook the urgency of the moment. This sweltering summer of the Negro’s legitimate discontent will not pass until there is an invigorating autumn of freedom and equality 21. 1963 is not an end, but a beginning. And those who hope that the Negro needed to blow off steam and will now be content will have a rude awakening if the nation returns to business as usual. There will be neither rest nor tranquility in America until the Negro is granted his citizenship rights. The whirlwinds of revolt will continue to shake the foundations of our nation until the bright day of justice emerges.\nBut there is something that I must say to my people, who stand on the warm threshold which leads into the palace of justice: in the process of gaining our rightful place, we must not be guilty of wrongful deeds. Let us not seek to satisfy our thirst for freedom by drinking from the cup of bitterness and hatred. We must forever conduct our struggle on the high plane of dignity and discipline. We must not allow our creative protest to degenerate into physical violence. Again and again, we must rise to the majestic heights of meeting physical force with soul force. The marvelous new militancy which has engulfed the Negro community must not lead us to a distrust of all white people, for many of our white brothers, as evidenced by their presence here today, have come to realize that their destiny is tied up with our destiny, and they have come to realize that their freedom is inextricably bound to our freedom. We cannot 22 walk alone.\nAnd as we walk, we must make the pledge that we shall always march ahead. We cannot turn back. There are those who are asking the devotees of civil rights, “When will you be satisfied?” We can never be satisfied as long as the Negro is the victim of the unspeakable horrors of police brutality. We can never be satisfied as long as our bodies, heavy with the fatigue of travel, cannot gain lodging in the motels of the highways and the hotels of the cities. 23 We cannot be satisfied as long as the Negro’s basic mobility is from a smaller ghetto to a larger one. We can never be satisfied as long as our children are stripped of their selfhood and robbed of their dignity by signs stating for whites only. We cannot be satisfied as long as a Negro in Mississippi cannot vote and a Negro in New York believes he has nothing for which to vote. No, no, we are not satisfied and we will not be satisfied until justice rolls down like waters and righteousness like a mighty stream 24.\nI am not unmindful that some of you have come here out of great trials 25 and tribulations. Some of you have come fresh from narrow jail cells. Some of you have come from areas where your quest for freedom left you battered by the storms of persecution and staggered by the winds of police brutality. You have been the veterans of creative suffering 26. Continue to work with the faith that unearned suffering is redemptive. Go back to Mississippi, go back to Alabama, go back to South Carolina, go back to Georgia, go back to Louisiana, go back to the slums and ghettos of our northern cities, knowing that somehow this situation can and will be changed. Let us not wallow in the valley of despair.\nI say to you today, my friends, so even though we face the difficulties of today and tomorrow, I still have a dream. It is a dream deeply rooted in the American dream. 27\nI have a dream that one day this nation will rise up and live out the true meaning of its creed: “We hold these truths to be self-evident, that all men are created equal.” 28\nI have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood. I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice. I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character. I have a dream today.\nI have a dream that one day down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of “interposition” and “nullification”, one day right there in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers. I have a dream today.\nI have a dream that one day every valley shall be exalted, every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight, and the glory of the Lord shall be revealed, and all flesh shall see it together.\nThis is our hope. This is the faith that I go back to the South with. With this faith we will be able to hew out of the mountain of despair a stone of hope. With this faith we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\nThis will be the day, this will be the day when all of God’s children will be able to sing with new meaning: “My country, ’tis of thee, sweet land of liberty, of thee I sing. Land where my fathers died, land of the pilgrim’s pride, from every mountainside, let freedom ring!” 29\nAnd if America is to be a great nation, this must become true. So let freedom ring from the prodigious hilltops of New Hampshire. Let freedom ring from the mighty mountains of New York. Let freedom ring from the heightening Alleghenies of Pennsylvania. Let freedom ring from the snow-capped Rockies of Colorado. Let freedom ring from the curvaceous slopes of California. But not only that: Let freedom ring from Stone Mountain of Georgia. Let freedom ring from Lookout Mountain of Tennessee. Let freedom ring from every hill and molehill of Mississippi. From every mountainside, let freedom ring.\nAnd when this happens, and when we allow freedom to ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God’s children, black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: “Free at last! Free at last! Thank God Almighty, we are free at last!” 30\nTaken from: Robert Torricelli, ed., In Our Own Words: Extraordinary Speeches of the American Century (New York: Simon & Schuster, 2000), pg. 234"
  },
  {
    "objectID": "notes/XAI/l05/index.html#session-description",
    "href": "notes/XAI/l05/index.html#session-description",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Session Description",
    "text": "Session Description\n\nHow to properly incorporate explanations in machine learning projects and what aspects should you keep in mind?\nOver the past few years the need to explain the output of machine learning models has received growing attention.\nExplanations not only reveal the reasons behind models predictions and increase users’ trust in the model, but they can be used for different purposes.\nTo fully utilize explanations and incorporate them into machine learning projects the following aspects of explanations should taken into consideration — explanation goals, the explanation method, and explanations’ quality.\nIn this talk, we will discuss how to select the appropriate explanation method based on the intended purpose of the explanation.\nThen, we will present two approaches for evaluating explanations, including practical examples of evaluation metrics, while highlighting the importance of assessing explanation quality.\nNext, we will examine the various purposes explanation can serve, along with the stage of the machine learning pipeline the explanation should be incorporated in.\nFinally we will present a real use case of script classification as malware-related in Microsoft and how we can benefit from high-dimensional explanations in this context."
  },
  {
    "objectID": "notes/XAI/l05/index.html#adding-post-hoc-xai-to-an-ml-project",
    "href": "notes/XAI/l05/index.html#adding-post-hoc-xai-to-an-ml-project",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Adding post-hoc XAI to an ML project",
    "text": "Adding post-hoc XAI to an ML project\nthe idea is a process with 5 steps:\n\nunderstand stakeholder - i.e. the end users of the project\nidentify the goals of using explanations\nchose XAI method that fits with the project properties\nimplication - taking the goal one step further\ndecide on an evaluation metrics"
  },
  {
    "objectID": "notes/XAI/l05/index.html#who-are-the-users",
    "href": "notes/XAI/l05/index.html#who-are-the-users",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "1. Who are the users?",
    "text": "1. Who are the users?\n\n\n\nStake hoder\nGoal\n\n\n\n\nData scientists or researchers\ndebug and refine a model\n\n\nDecision makers\ndoes the model fit a business strategy ?\n\n\nModel Risk Analyst\nassess the model’s robustness.\n\n\nRegulators\ninspect the reliability and impact on costumers.\n\n\nConsumers\nwant transparency on how decisions affect them."
  },
  {
    "objectID": "notes/XAI/l05/index.html#explanation-goals",
    "href": "notes/XAI/l05/index.html#explanation-goals",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Explanation goals ?",
    "text": "Explanation goals ?\n\nunderstanding predictions\nmodel debugging and verification\nimproving performance\nincreasing trust"
  },
  {
    "objectID": "notes/XAI/l05/index.html#xai-method-selection",
    "href": "notes/XAI/l05/index.html#xai-method-selection",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "XAI method Selection",
    "text": "XAI method Selection\n\nData\n\nwhat is the scope of the explanation?\nwhat is the data type ?\nwhat output are we trying to explain ?\n\nAccess level\n\nCan the explainer access the model?\nCan the explainer access the data?\n\nStage\n\nAt What stage does the model require the explanation ?"
  },
  {
    "objectID": "notes/XAI/l05/index.html#explanation-methods-map",
    "href": "notes/XAI/l05/index.html#explanation-methods-map",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Explanation methods map",
    "text": "Explanation methods map\nHow do we pick the correct approach to explaining the model ?\nThe next figure on the right can let us pick\n\n\n\n\n\nmap of explainability approaches\n\n\nfrom (Belle and Papantonis 2021)\nAnchors and InTrees were not discussed earlier in the course.\n\n\n\n\n\nTable 2: Comparing models on the kinds of transparency that are enabled\n\n\n\n\n\n\nfrom (Belle and Papantonis 2021)\n\n\n\n\nXAU methods 1\n\n\nfrom (Belle and Papantonis 2021)\n\n\n\n\nXAU methods 2\n\n\nfrom (Belle and Papantonis 2021)\n\n\n\nWe covered SHAP which identifies the contribution of of each feature to the game of cooperative prediction\nPDP / ICE - use a local model’s decision boundary to identify at what point we switched class.\nCounterfactuals utilize the original model’s decision boundary identify the intervention that would lead to the effect of switching class.\nAnchors, introduced in (Ribeiro, Singh, and Guestrin 2018) and called Scoped Rules in [Molnar (2022) §9.4]\nDeletion Diagnostics: How would removing the highest leverage points change the model’s decision boundary\nInTrees: What rules approximate our decision ?"
  },
  {
    "objectID": "notes/XAI/l05/index.html#xai-implications",
    "href": "notes/XAI/l05/index.html#xai-implications",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "XAI Implications",
    "text": "XAI Implications\n\n\n\n\n\n\nslide\n\n\nfrom (DARPA, n.d.)\nc.f. https://www.darpa.mil/program/explainable-artificial-intelligence\n\n\n\n\nslide"
  },
  {
    "objectID": "notes/XAI/l05/index.html#model-verification",
    "href": "notes/XAI/l05/index.html#model-verification",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Model Verification",
    "text": "Model Verification\n\n\n\nslide"
  },
  {
    "objectID": "notes/XAI/l05/index.html#model-debugging",
    "href": "notes/XAI/l05/index.html#model-debugging",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Model Debugging",
    "text": "Model Debugging\n\n\n\nslide\n\n\n\nlook at the errors of the models - false positives and false negatives\nuse SHAP to identify features that should be excluded.1\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nslide\n\n\n\n\n\n\nseries poster\nslide\nslide\nslide\nslide\nslide\nslide\nmap of explainability approaches\nTable 2: Comparing models on the kinds of transparency that are enabled\nXAU methods 1\nXAU methods 2\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide\nslide"
  },
  {
    "objectID": "notes/XAI/l05/index.html#footnotes",
    "href": "notes/XAI/l05/index.html#footnotes",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut we should be using Causal Inference for this, since fixing using a local explanation will surely impact all the data set. If we have a Causal DAG we can probably use this concept to identify nodes as confounders on some Causal path.↩︎"
  },
  {
    "objectID": "notes/XAI/l05/index.html#who-are-the-explanation-for",
    "href": "notes/XAI/l05/index.html#who-are-the-explanation-for",
    "title": "Lecture 5 — Explainable AI in practice",
    "section": "1. Who are the explanation for ?",
    "text": "1. Who are the explanation for ?\nDifferent stakeholders have different needs from an ML model. Once we captured these needs we can decide on a suitable strategy for providing a suitable explanation. Understanding these needs will also aid in selecting the metrics utilized to determine the effectiveness of explanation.\n\n\n\n\n\n\n\n\n\n\nStakeholder\nTheir Goal\n\n\n\n\nData scientists\nDebug and refine the model\n\n\nDecision makers\nAssess model fit to a business strategy\n\n\nModel Risk Analyst\nAssess the model’s robustness\n\n\nRegulators\nInspect model’s reliability and impact on costumers\n\n\nConsumers\nTransparency into decisions that affect them\n\n\n\n\n\nTable 1: XAI Stakeholders and their Needs"
  },
  {
    "objectID": "notes/XAI/l01/index.html#permutation-feature-importance",
    "href": "notes/XAI/l01/index.html#permutation-feature-importance",
    "title": "Introduction to XAI",
    "section": "Permutation Feature Importance",
    "text": "Permutation Feature Importance\n\n\n\nPermutation Feature Importance\n\n\nThis is defined by sk-learn as follows:\n\nInputs: fitted predictive model \\(m\\), tabular dataset (training or validation) \\(D\\).\nCompute the reference score \\(s\\) of the model \\(m\\) on data \\(D\\) (for instance the accuracy for a classifier or the \\(R^2\\) for a regressor).\nFor each feature j (column of D):\n\nFor each repetition \\(k\\) in \\(1,\\ldots,K\\):\n\nRandomly shuffle column \\(j\\) of dataset \\(D\\) to generate a corrupted version of the data named \\(\\bar D_{k,j}\\).\nCompute the score \\(s_{k,j\\) of model \\(m\\) on corrupted data \\(\\bar D_{k,j}\\).\n\nCompute importance\\(i_j\\) for feature \\(f+j\\) defined as:\n\n\n\\[\ni_j=s-\\frac{1}{K}\\sum_{k=1}^Ks_{k_j} \\qquad\n\\qquad(2)\\]"
  },
  {
    "objectID": "notes/XAI/l01/index.html#mdi-feature-importance",
    "href": "notes/XAI/l01/index.html#mdi-feature-importance",
    "title": "Introduction to XAI",
    "section": "MDI Feature Importance",
    "text": "MDI Feature Importance\nFirst introduced in (Breiman 2001)\n\n\n\nMean Decrease in Impurity Feature Importance"
  },
  {
    "objectID": "notes/XAI/l01/index.html#agenda",
    "href": "notes/XAI/l01/index.html#agenda",
    "title": "Lecture 1 — Introduction to XAI",
    "section": "Agenda",
    "text": "Agenda\n\nMotivation\nExplain what is XAI\nIntroduction to trees\nXAI in the forest"
  },
  {
    "objectID": "notes/XAI/l01/index.html#feature-selection",
    "href": "notes/XAI/l01/index.html#feature-selection",
    "title": "Introduction to XAI",
    "section": "Feature selection",
    "text": "Feature selection\n\n\n\nXAI for feature selection.\n\n\n\nOne learns in linear regression 101, that the \\(\\text{adjusted } R^2\\) let’s you gauge the performance of models built with different features. This means we already should have a principled approach to feature selection.\nthe most obvious method – stepwise regression is prone to overfitting if there are many features and the Bonferroni point 3 which governs the admissibly of non-spurious features is \\(\\approx \\sqrt{2\\log p}\\) for the t-test (where p is the number if predictors). However this is will reject good features.\nthe Benjamini–Hochberg procedure procedure is less conservative and avoid the use of p-values which are amenable to p-hacking.\nIn black box model like a Deep Neural Networks the model learns its own features so again I don’t see how XAI is going to be able to help out.\nGelman and Hill (2007) pointers out that adding features to a regression can lead to a regression formula that does not make sense. They suggest a procedure that lead to an interpretable model. However the culture in ML is rather different than in statistical learning.\nIf we work with a Causal DAG we may well de have even more to say on the\nQ. So what more can XAI informs us as to features selection?"
  },
  {
    "objectID": "notes/XAI/l01/index.html#motivation",
    "href": "notes/XAI/l01/index.html#motivation",
    "title": "Introduction to XAI",
    "section": "Motivation",
    "text": "Motivation\n\nAI market size is rapidly expanding and projected to reach 1.6 Billion by 2030 (Research, n.d.)\nMore ML projects are reaching deployment"
  },
  {
    "objectID": "notes/XAI/l01/index.html#talk-agenda",
    "href": "notes/XAI/l01/index.html#talk-agenda",
    "title": "Introduction to XAI",
    "section": "Talk Agenda",
    "text": "Talk Agenda\n\nMotivation\nWhat is XAI\nIntroduction to trees\nXAI in the forest"
  },
  {
    "objectID": "notes/XAI/l01/index.html#references",
    "href": "notes/XAI/l01/index.html#references",
    "title": "Introduction to XAI",
    "section": "",
    "text": "creditsReferences\n\nhttps://www.youtube.com/watch?v=6qisPX7o-bg\n\n\n\nBreiman, Leo. 2001a. “Random Forests.” Machine Learning 45 (1): 5–32. https://doi.org/10.1023/a:1010933404324.\n\n\n———. 2001b. Machine Learning 45 (1): 5–32. https://doi.org/10.1023/a:1010933404324.\n\n\nDastin, Jeffrey. 2018. “Amazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women.” https://www.reuters.com/article/amazoncom-jobs-automation/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSL2N1VB1FQ/?feedType=RSS%26feedName=companyNews.\n\n\nGelman, Andrew, and Jennifer Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Vol. Analytical methods for social research. New York: Cambridge University Press.\n\n\nHo, Tin Kam. 1995. “Random Decision Forests.” In Proceedings of 3rd International Conference on Document Analysis and Recognition, 1:278–282 vol.1. https://doi.org/10.1109/ICDAR.1995.598994.\n\n\nMiller, Tim. 2017. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” CoRR abs/1706.07269. http://arxiv.org/abs/1706.07269.\n\n\nMolnar, Christoph. 2022. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. 2nd ed. https://christophm.github.io/interpretable-ml-book.\n\n\nResearch, Precedence. n.d. https://www.globenewswire.com/news-release/2022/04/19/2424179/0/en/Artificial-Intelligence-Market-Size-to-Surpass-Around-US-1-597-1-Bn-By-2030.html."
  },
  {
    "objectID": "notes/XAI/l01/index.html#course-goals",
    "href": "notes/XAI/l01/index.html#course-goals",
    "title": "Introduction to XAI",
    "section": "Course Goals",
    "text": "Course Goals\n\nThe XAI course provides a comprehensive overview of explainable AI,\n\ncovering both theory and practice, and\nexploring various use cases for explainability.\n\nParticipants will learn how\n\nto generate explanations,\nto evaluate explanations, and\neffectively communicate these to diverse stakeholders.\n\n\noverview link"
  },
  {
    "objectID": "notes/XAI/l01/index.html#speakers",
    "href": "notes/XAI/l01/index.html#speakers",
    "title": "Introduction to XAI",
    "section": "Speakers",
    "text": "Speakers\n\nSpeakers"
  },
  {
    "objectID": "notes/XAI/l01/index.html#session-description-1",
    "href": "notes/XAI/l01/index.html#session-description-1",
    "title": "Introduction to XAI",
    "section": "Session Description",
    "text": "Session Description\n\nMotivate explainability.\n\nExplore how it achieve greater transparency and trustworthiness in AI systems,\n\nProvide the the key terminology\nDiscuss the differences between global and local explanations\nExamine the “built-in” feature importance methods commonly used for regression and trees."
  },
  {
    "objectID": "notes/XAI/l01/index.html#what-do-we-mean-by-explainability",
    "href": "notes/XAI/l01/index.html#what-do-we-mean-by-explainability",
    "title": "Introduction to XAI",
    "section": "What do we mean by Explainability?",
    "text": "What do we mean by Explainability?\n\nWe define explainability by:\n\n\n“The ability of an AI system or algorithm to explain its decision making process in a way that humans can understand” 1\n\n\nAn explanation is the answer to a why question – (Miller 2017)"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-investigating-bugs",
    "href": "notes/XAI/l01/index.html#xai-to-investigating-bugs",
    "title": "Introduction to XAI",
    "section": "XAI to Investigating Bugs",
    "text": "XAI to Investigating Bugs"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-investigating-bugs-1",
    "href": "notes/XAI/l01/index.html#xai-to-investigating-bugs-1",
    "title": "Introduction to XAI",
    "section": "XAI to Investigating Bugs 1",
    "text": "XAI to Investigating Bugs 1"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-investigating-bugs-2",
    "href": "notes/XAI/l01/index.html#xai-to-investigating-bugs-2",
    "title": "Introduction to XAI",
    "section": "XAI to Investigating Bugs 2",
    "text": "XAI to Investigating Bugs 2"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models",
    "href": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models",
    "title": "Introduction to XAI",
    "section": "XAI to Avoid Biases in ML Models",
    "text": "XAI to Avoid Biases in ML Models\n\nAvoiding ML bias in (Dastin 2018)\n\nSource of the bias is that they trained on 10 years of worker’s CVs. surprise their workforce had a bias and the model perpetuated it."
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models-1",
    "href": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models-1",
    "title": "Introduction to XAI",
    "section": "XAI to Avoid Biases in ML Models",
    "text": "XAI to Avoid Biases in ML Models\n\nXAI can reveal bias before models reach production.\nExample:\n\nA US based client started doing business abroad.\nNew non US prospects were misclassified.\n🤯 XAI showed the country biased against non US prospects.\n\\(\\implies\\) dropped the country feature from the model."
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models-2",
    "href": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models-2",
    "title": "Introduction to XAI",
    "section": "XAI to Avoid Biases in ML Models",
    "text": "XAI to Avoid Biases in ML Models\n\n\n\n\nChat GPT political bias\n\n\n\n\n\nResponse"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models-3",
    "href": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models-3",
    "title": "Introduction to XAI",
    "section": "XAI to Avoid Biases in ML Models",
    "text": "XAI to Avoid Biases in ML Models\n\nPredicting which prospective customers will convert\n\ncurrent market is is in the US\nModel accuracy on test is high\nPredictions distribution over time is off?\n\nWhat to do next?"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-support-business-decisions",
    "href": "notes/XAI/l01/index.html#xai-to-support-business-decisions",
    "title": "Introduction to XAI",
    "section": "XAI to support business decisions",
    "text": "XAI to support business decisions\n\nExternal data consumption to improve prediction\nExplainability to create a personalized well suited sales pitch"
  },
  {
    "objectID": "notes/XAI/l01/index.html#who-needs-explanatinos",
    "href": "notes/XAI/l01/index.html#who-needs-explanatinos",
    "title": "Introduction to XAI",
    "section": "Who needs explanatinos ?",
    "text": "Who needs explanatinos ?\n\n\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#explaining-the-data-vs.-explaining-the-model",
    "href": "notes/XAI/l01/index.html#explaining-the-data-vs.-explaining-the-model",
    "title": "Introduction to XAI",
    "section": "Explaining the Data vs. Explaining the Model",
    "text": "Explaining the Data vs. Explaining the Model\n\n\nFeature Description\n\nCharacteristics of the input data\nE.g.:\n\nFeature correlation\nAnomalies & Extreme values\nFeature values distribution\n\n\n\nFeature Contribution\n\nFeature’s impact on predictions\nNot aligned with feat. correlation to target variable\nE.g.:\n\nFeature importance in trees\nSHAP values\n\n\n\n\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#explaining-the-data-vs.-explaining-the-model-1",
    "href": "notes/XAI/l01/index.html#explaining-the-data-vs.-explaining-the-model-1",
    "title": "Introduction to XAI",
    "section": "Explaining the Data vs. Explaining the Model",
    "text": "Explaining the Data vs. Explaining the Model\n\n\nFeature Description\n\nCharacteristics of the input data\nE.g.:\n\nFeature correlation\nAnomalies & Extreme values\nFeature values distribution\n\n\n\nFeature Contribution\n\nFeature’s impact on predictions\nNot aligned with feat. correlation to target variable\nE.g.:\n\nFeature importance in trees\nSHAP values\n\n\n\n\n ## Properties of Explanations\n\n\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#who-needs-explanations",
    "href": "notes/XAI/l01/index.html#who-needs-explanations",
    "title": "Introduction to XAI",
    "section": "Who Needs Explanations ?",
    "text": "Who Needs Explanations ?\n\nWho needs explanations"
  },
  {
    "objectID": "notes/XAI/l01/index.html#properties-of-explanations",
    "href": "notes/XAI/l01/index.html#properties-of-explanations",
    "title": "Introduction to XAI",
    "section": "Properties of Explanations",
    "text": "Properties of Explanations\n\n\nWhite Box\n\nAn interpretable model.\nHumans can understand how the model makes predictions.\nExamples:\n\nlinear and logistic regression\ndecision tree\n\n\n\nBlack Box\n\nDo not reveal their internal mechanisms\nCannot be understood by looking at their parameters\nExamples:\n\nDeep Neural Nets\nXGBoost, Random Forest"
  },
  {
    "objectID": "notes/XAI/l01/index.html#why-decision-trees",
    "href": "notes/XAI/l01/index.html#why-decision-trees",
    "title": "Introduction to XAI",
    "section": "Why Decision Trees?",
    "text": "Why Decision Trees?\n\nEasy to explain.\nClear structure - order and hierarchy.\nSimple interpretability.\nCan be converted into rules.\nOften used as a surrogate model"
  },
  {
    "objectID": "notes/XAI/l01/index.html#how-do-we-build-decision-trees",
    "href": "notes/XAI/l01/index.html#how-do-we-build-decision-trees",
    "title": "Introduction to XAI",
    "section": "How do we build Decision Trees?",
    "text": "How do we build Decision Trees?\n\nEntropy - the measurement of the impurity or randomness in the data points"
  },
  {
    "objectID": "notes/XAI/l01/index.html#example",
    "href": "notes/XAI/l01/index.html#example",
    "title": "Introduction to XAI",
    "section": "Example",
    "text": "Example\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#iris-example",
    "href": "notes/XAI/l01/index.html#iris-example",
    "title": "Introduction to XAI",
    "section": "Iris Example",
    "text": "Iris Example\n\n\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#trade-off-between-performance-and-interpretability",
    "href": "notes/XAI/l01/index.html#trade-off-between-performance-and-interpretability",
    "title": "Introduction to XAI",
    "section": "Trade-off between performance and interpretability",
    "text": "Trade-off between performance and interpretability\n\n\n\nspectrum of interpretability"
  },
  {
    "objectID": "notes/XAI/l01/index.html#trade-off-between-performance-and-interpretability-1",
    "href": "notes/XAI/l01/index.html#trade-off-between-performance-and-interpretability-1",
    "title": "Introduction to XAI",
    "section": "Trade-off between performance and interpretability",
    "text": "Trade-off between performance and interpretability\n\n\n\nThe trade-off between predcitive power and interpretability"
  },
  {
    "objectID": "notes/XAI/l01/index.html#properties-of-explanation-methods",
    "href": "notes/XAI/l01/index.html#properties-of-explanation-methods",
    "title": "Introduction to XAI",
    "section": "Properties of Explanation Methods",
    "text": "Properties of Explanation Methods\n\nPredictive mode interpretation level\nExplanation creation time"
  },
  {
    "objectID": "notes/XAI/l01/index.html#properties-of-explanation-methods-1",
    "href": "notes/XAI/l01/index.html#properties-of-explanation-methods-1",
    "title": "Introduction to XAI",
    "section": "Properties of Explanation Methods",
    "text": "Properties of Explanation Methods\n\n\nIntrinsic\n\nML model that are considered interpretable due to their simple structure.\nExplanation methods that rely on looking into ML models, like its parameters\nNo additional complexity or resources requires\n\n\nExtrinsic\n\nApplying methods that analyze the model after training\nPost hoc methods can also be applied to intrinsically interperetable models\nAdditional complexity - XAI algorithms and computation resources requried"
  },
  {
    "objectID": "notes/XAI/l01/index.html#post-hoc-xai-using-surrogate-models",
    "href": "notes/XAI/l01/index.html#post-hoc-xai-using-surrogate-models",
    "title": "Introduction to XAI",
    "section": "Post Hoc XAI using Surrogate Models",
    "text": "Post Hoc XAI using Surrogate Models\n\nPost Hoc methods create and use a surrogate model to explain predictions"
  },
  {
    "objectID": "notes/XAI/l01/index.html#properties-of-explanation-methods-2",
    "href": "notes/XAI/l01/index.html#properties-of-explanation-methods-2",
    "title": "Introduction to XAI",
    "section": "Properties of Explanation Methods",
    "text": "Properties of Explanation Methods\n\n\nModel Specific\n\nLimited to specific model type.\nExamples:\n\nRegression weights in a linear model\nGINI importance score in a dicision tree\n\n\n\nModel Agnostic\n\nXAI tools for any ML Model\nPos hoc methods\nWork by analyzing input-output pairs\nExamples:\n\nSHAP\nLIME"
  },
  {
    "objectID": "notes/XAI/l01/index.html#properties-of-explanations-1",
    "href": "notes/XAI/l01/index.html#properties-of-explanations-1",
    "title": "Introduction to XAI",
    "section": "Properties of Explanations",
    "text": "Properties of Explanations\n\n\nModel Specific\n\nLimited to specific model type.\nExamples:\n\nRegression weights in a linear model\nGINI importance score in a dicision tree\n\n\n\nModel Agnostic\n\nXAI tools for any ML Model\nPos hoc methods\nWork by analyzing input-output pairs\nExamples:\n\nSHAP\nLIME"
  },
  {
    "objectID": "notes/XAI/l01/index.html#explain-the-predictions-of-a-segement",
    "href": "notes/XAI/l01/index.html#explain-the-predictions-of-a-segement",
    "title": "Introduction to XAI",
    "section": "Explain the Predictions of a Segement",
    "text": "Explain the Predictions of a Segement\n\n\n\nA segment of the data"
  },
  {
    "objectID": "notes/XAI/l01/index.html#properties-of-explanation-methods-3",
    "href": "notes/XAI/l01/index.html#properties-of-explanation-methods-3",
    "title": "Introduction to XAI",
    "section": "Properties of Explanation Methods",
    "text": "Properties of Explanation Methods\n\n\n\nContasting Global with Local views of the data"
  },
  {
    "objectID": "notes/XAI/l01/index.html#explanation-structure",
    "href": "notes/XAI/l01/index.html#explanation-structure",
    "title": "Introduction to XAI",
    "section": "Explanation Structure",
    "text": "Explanation Structure\n\nExplanation Structure - Numerical, Graphs and Text"
  },
  {
    "objectID": "notes/XAI/l01/index.html#intrinsic-and-extrinsic-methods",
    "href": "notes/XAI/l01/index.html#intrinsic-and-extrinsic-methods",
    "title": "Introduction to XAI",
    "section": "Intrinsic and Extrinsic Methods",
    "text": "Intrinsic and Extrinsic Methods\n\n\nIntrinsic\n\nML model that are considered interpretable due to their simple structure.\nExplanation methods that rely on looking into ML models, like its parameters\nNo additional complexity or resources requires\n\n\nExtrinsic\n\nApplying methods that analyze the model after training\nPost hoc methods can also be applied to intrinsically interperetable models\nAdditional complexity - XAI algorithms and computation resources requried"
  },
  {
    "objectID": "notes/XAI/l01/index.html#model-specific-and-agnostic-methods",
    "href": "notes/XAI/l01/index.html#model-specific-and-agnostic-methods",
    "title": "Introduction to XAI",
    "section": "Model Specific and Agnostic Methods",
    "text": "Model Specific and Agnostic Methods\n\n\nModel Specific\n\nLimited to specific model type.\nExamples:\n\nRegression weights in a linear model\nGINI importance score in a decision tree\n\n\n\nModel Agnostic\n\nXAI tools for any ML Model\nPos hoc methods that\nMap input output pairs\nExamples:\n\nSHAP\nLIME"
  },
  {
    "objectID": "notes/XAI/l01/index.html#local-and-global-methods",
    "href": "notes/XAI/l01/index.html#local-and-global-methods",
    "title": "Introduction to XAI",
    "section": "Local and Global Methods",
    "text": "Local and Global Methods\n\nContasting Global with Local views of the data"
  },
  {
    "objectID": "notes/XAI/l01/index.html#graphs-representation-for-shap",
    "href": "notes/XAI/l01/index.html#graphs-representation-for-shap",
    "title": "Introduction to XAI",
    "section": "Graphs Representation for SHAP",
    "text": "Graphs Representation for SHAP\n\n  \n\n\n\n\nSHAP Visulizations"
  },
  {
    "objectID": "notes/XAI/l01/index.html#explanation-repoducibility",
    "href": "notes/XAI/l01/index.html#explanation-repoducibility",
    "title": "Introduction to XAI",
    "section": "Explanation Repoducibility",
    "text": "Explanation Repoducibility\n\nMost post hoc techniques use random samples of the data and premutation vlues\nThis results in inconsistant results - for the same model we can get different explanations.\nAs data scientists we should be aware of this and consider consistanc if applicable/required.\n\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#information-theory-entropy",
    "href": "notes/XAI/l01/index.html#information-theory-entropy",
    "title": "Introduction to XAI",
    "section": "Information Theory: Entropy",
    "text": "Information Theory: Entropy\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#information-theory-conditional-entropy",
    "href": "notes/XAI/l01/index.html#information-theory-conditional-entropy",
    "title": "Introduction to XAI",
    "section": "Information Theory: Conditional Entropy",
    "text": "Information Theory: Conditional Entropy\n\nConditional Entropy"
  },
  {
    "objectID": "notes/XAI/l01/index.html#information-theory-mutual-information",
    "href": "notes/XAI/l01/index.html#information-theory-mutual-information",
    "title": "Introduction to XAI",
    "section": "Information Theory: Mutual Information",
    "text": "Information Theory: Mutual Information"
  },
  {
    "objectID": "notes/XAI/l01/index.html#decision-tree",
    "href": "notes/XAI/l01/index.html#decision-tree",
    "title": "Introduction to XAI",
    "section": "Decision Tree",
    "text": "Decision Tree\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#decision-tree---entropy-calculation",
    "href": "notes/XAI/l01/index.html#decision-tree---entropy-calculation",
    "title": "Introduction to XAI",
    "section": "Decision Tree - Entropy Calculation",
    "text": "Decision Tree - Entropy Calculation\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#information-theory-discretization",
    "href": "notes/XAI/l01/index.html#information-theory-discretization",
    "title": "Introduction to XAI",
    "section": "Information Theory: Discretization",
    "text": "Information Theory: Discretization\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#information-theory-gini-index",
    "href": "notes/XAI/l01/index.html#information-theory-gini-index",
    "title": "Introduction to XAI",
    "section": "Information Theory: Gini Index",
    "text": "Information Theory: Gini Index\n\nGINI Index"
  },
  {
    "objectID": "notes/XAI/l01/index.html#entropy-and-gini",
    "href": "notes/XAI/l01/index.html#entropy-and-gini",
    "title": "Introduction to XAI",
    "section": "Entropy and Gini",
    "text": "Entropy and Gini\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#decision-tree---iris-dataset",
    "href": "notes/XAI/l01/index.html#decision-tree---iris-dataset",
    "title": "Introduction to XAI",
    "section": "Decision Tree - Iris Dataset",
    "text": "Decision Tree - Iris Dataset\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#decision-tree---titanic-dataset",
    "href": "notes/XAI/l01/index.html#decision-tree---titanic-dataset",
    "title": "Introduction to XAI",
    "section": "Decision Tree - Titanic Dataset",
    "text": "Decision Tree - Titanic Dataset\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#feature-importance---mean-decrease-in-impurity-mdi",
    "href": "notes/XAI/l01/index.html#feature-importance---mean-decrease-in-impurity-mdi",
    "title": "Introduction to XAI",
    "section": "Feature Importance - Mean Decrease in Impurity (MDI)",
    "text": "Feature Importance - Mean Decrease in Impurity (MDI)\nFirst introduced in (Breiman 2001b)\n\nMean Decrease in Impurity Feature Importance"
  },
  {
    "objectID": "notes/XAI/l01/index.html#feature-importance---permutation-feature-importance",
    "href": "notes/XAI/l01/index.html#feature-importance---permutation-feature-importance",
    "title": "Introduction to XAI",
    "section": "Feature Importance - Permutation Feature Importance",
    "text": "Feature Importance - Permutation Feature Importance\nThis is defined by sk-learn as follows:\n\nInputs: fitted predictive model \\(m\\), tabular dataset (training or validation) \\(D\\).\nCompute the reference score \\(s\\) of the model \\(m\\) on data \\(D\\) (for instance the accuracy for a classifier or the \\(R^2\\) for a regressor).\nFor each feature j (column of D):\n\nFor each repetition \\(k\\) in \\(1,\\ldots,K\\):\n\nRandomly shuffle column \\(j\\) of dataset \\(D\\) to generate a corrupted version of the data named \\(\\bar D_{k,j}\\).\nCompute the score \\(s_{k,j\\) of model \\(m\\) on corrupted data \\(\\bar D_{k,j}\\).\n\nCompute importance\\(i_j\\) for feature \\(f+j\\) defined as:\n\n\n\\[\ni_j=s-\\frac{1}{K}\\sum_{k=1}^Ks_{k_j} \\qquad\n\\qquad(2)\\]"
  },
  {
    "objectID": "notes/XAI/l01/index.html#random-forest",
    "href": "notes/XAI/l01/index.html#random-forest",
    "title": "Introduction to XAI",
    "section": "Random Forest",
    "text": "Random Forest\nintroduced in (Ho 1995) and extended in (Breiman 2001a).\n\nEnsemble of decision trees.\n\nN – number of training samples\nM – number of features\nn_estimators – The number of trees in the forest\nCreate n_estimators decision trees using\n\nN samples with replacement\n\\(m&lt;M\\) features for each step typically \\(m-\\sqrt{M}\\)"
  },
  {
    "objectID": "notes/XAI/l01/index.html#decision-tree---iris-dataset-1",
    "href": "notes/XAI/l01/index.html#decision-tree---iris-dataset-1",
    "title": "Introduction to XAI",
    "section": "Decision Tree - Iris Dataset",
    "text": "Decision Tree - Iris Dataset\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#mdi-feat-importance-iris-dataset",
    "href": "notes/XAI/l01/index.html#mdi-feat-importance-iris-dataset",
    "title": "Introduction to XAI",
    "section": "MDI Feat Importance Iris Dataset",
    "text": "MDI Feat Importance Iris Dataset"
  },
  {
    "objectID": "notes/XAI/l01/index.html#how-to-calculate-feature-importance-in-random-forest.",
    "href": "notes/XAI/l01/index.html#how-to-calculate-feature-importance-in-random-forest.",
    "title": "Introduction to XAI",
    "section": "How to calculate feature importance in random forest.",
    "text": "How to calculate feature importance in random forest.\n\n\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#feature-importance-methods",
    "href": "notes/XAI/l01/index.html#feature-importance-methods",
    "title": "Introduction to XAI",
    "section": "Feature Importance Methods",
    "text": "Feature Importance Methods\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#feature-importance-score",
    "href": "notes/XAI/l01/index.html#feature-importance-score",
    "title": "Introduction to XAI",
    "section": "Feature Importance Score",
    "text": "Feature Importance Score\n\nslide"
  },
  {
    "objectID": "notes/XAI/l01/index.html#how-to-calculate-feature-importance-in-random-forest",
    "href": "notes/XAI/l01/index.html#how-to-calculate-feature-importance-in-random-forest",
    "title": "Introduction to XAI",
    "section": "How to calculate Feature Importance in Random Forest?",
    "text": "How to calculate Feature Importance in Random Forest?\n\nMDI Feature importance for Random Forest"
  },
  {
    "objectID": "notes/XAI/l01/index.html#explain-the-predictions-of-a-segment",
    "href": "notes/XAI/l01/index.html#explain-the-predictions-of-a-segment",
    "title": "Introduction to XAI",
    "section": "Explain the Predictions of a Segment",
    "text": "Explain the Predictions of a Segment\n\nA segment of the data"
  },
  {
    "objectID": "notes/XAI/l01/index.html#model-specific-model-agnostic-methods",
    "href": "notes/XAI/l01/index.html#model-specific-model-agnostic-methods",
    "title": "Introduction to XAI",
    "section": "Model Specific & Model Agnostic Methods",
    "text": "Model Specific & Model Agnostic Methods\n\n\nModel Specific\n\nLimited to specific model type.\nExamples:\n\nRegression weights in a linear model\nGINI importance score in a decision tree\n\n\n\nModel Agnostic\n\nXAI tools for any ML Model\nPos hoc methods that\nMap input output pairs\nExamples:\n\nSHAP\nLIME"
  },
  {
    "objectID": "notes/XAI/l01/index.html#performance-interpretability-trade-off",
    "href": "notes/XAI/l01/index.html#performance-interpretability-trade-off",
    "title": "Introduction to XAI",
    "section": "Performance & Interpretability Trade-off",
    "text": "Performance & Interpretability Trade-off\n\nspectrum of interpretability"
  },
  {
    "objectID": "notes/XAI/l01/index.html#performance-interpretability-trade-off-1",
    "href": "notes/XAI/l01/index.html#performance-interpretability-trade-off-1",
    "title": "Introduction to XAI",
    "section": "Performance & Interpretability Trade-off",
    "text": "Performance & Interpretability Trade-off\n\nThe trade-off between predcitive power and interpretability"
  },
  {
    "objectID": "notes/XAI/l01/index.html#intrinsic-extrinsic-methods",
    "href": "notes/XAI/l01/index.html#intrinsic-extrinsic-methods",
    "title": "Introduction to XAI",
    "section": "Intrinsic & Extrinsic Methods",
    "text": "Intrinsic & Extrinsic Methods\n\n\nIntrinsic\n\nML model that are considered interpretable due to their simple structure.\nExplanation methods that rely on looking into ML models, like its parameters\nNo additional complexity or resources requires\n\n\nExtrinsic\n\nApplying methods that analyze the model after training\nPost hoc methods can also be applied to intrinsically interperetable models\nAdditional complexity - XAI algorithms and computation resources requried"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models-4",
    "href": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models-4",
    "title": "Introduction to XAI",
    "section": "XAI to Avoid Biases in ML Models",
    "text": "XAI to Avoid Biases in ML Models\n\nPredicting which prospective customers will convert\n\ncurrent market is is in the US\nModel accuracy on test is high\nPredictions distribution over time is off?\n\nWhat to do next?"
  },
  {
    "objectID": "notes/XAI/l01/index.html#what-do-we-mean-by-explainability-1",
    "href": "notes/XAI/l01/index.html#what-do-we-mean-by-explainability-1",
    "title": "Introduction to XAI",
    "section": "What do we mean by Explainability?",
    "text": "What do we mean by Explainability?\n\nThe capacity of an model to back predictions with a human understandable interpretation of the impact of inputs on predictions.\n\n\nWhat humans find understandable differs widely.\nLearning in ML can differ greatly:\n\nParametric models learn a handful of parameters,\nNon-parametric model may learn billions.\n\nExplanations are subjective\n\nArtifacts of the model, not the data\nReflect any inductive bias in the model 2"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models---comments-1",
    "href": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models---comments-1",
    "title": "Introduction to XAI",
    "section": "XAI to Avoid Biases in ML Models - Comments 1",
    "text": "XAI to Avoid Biases in ML Models - Comments 1\n\n\nDevils Advocate:😈\n\nQ. Why add a features like country if all activity is in one country?\nQ. Why drop it? Won’t country be an informative feature going forward?\nQ. Won’t this be an issue for each new country added?\n\\(\\implies\\) Partial Pooling can learn to strike a balance 🤔"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models---comments-2",
    "href": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models---comments-2",
    "title": "Introduction to XAI",
    "section": "XAI to Avoid Biases in ML Models - Comments 2",
    "text": "XAI to Avoid Biases in ML Models - Comments 2\n\n\n\nWhat is a unbiased estimator?\n\nestimators are unbiased w.r.t. some specific criteria.\nthere is a bias variance trade-off.\nwhich is worse depends on the cost of type I errors vs type II errors"
  },
  {
    "objectID": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models---comments-3",
    "href": "notes/XAI/l01/index.html#xai-to-avoid-biases-in-ml-models---comments-3",
    "title": "Introduction to XAI",
    "section": "XAI to Avoid Biases in ML Models - Comments 3",
    "text": "XAI to Avoid Biases in ML Models - Comments 3\n\n\n\nadding more criteria will reduce its performance on the main metric (i.e. variance).\npeople tend to like a biased estimator with small variance to unbiased one with high variance.\nit looks like a class imbalance problem for which there are well know solutions like re-sampling and weighting.\nthe datasets in upstream models may be the issue\n\nhow can we detect and correct in these models.\nignoring for the moment the costs of sourcing better data what do we do when the bias comes from the real world (gender gap in payment).\n\nand how can we avoid making the bias bigger?"
  },
  {
    "objectID": "posts/2020/2020-06-bash-tricks.html",
    "href": "posts/2020/2020-06-bash-tricks.html",
    "title": "brace expansion",
    "section": "",
    "text": "the bash shell supports brace expansion.\nthe idea is that the string before and after are concatenated with element in the braces\n\n\necho \"you won \"{two,three,four}\" points, \"\n\nyou won two points,  you won three points,  you won four points,"
  },
  {
    "objectID": "posts/2020/2020-06-bash-tricks.html#brace-expansion",
    "href": "posts/2020/2020-06-bash-tricks.html#brace-expansion",
    "title": "brace expansion",
    "section": "",
    "text": "the bash shell supports brace expansion.\nthe idea is that the string before and after are concatenated with element in the braces\n\n\necho \"you won \"{two,three,four}\" points, \"\n\nyou won two points,  you won three points,  you won four points,"
  },
  {
    "objectID": "posts/2024/2023-02-22-transformations/index.html",
    "href": "posts/2024/2023-02-22-transformations/index.html",
    "title": "Transformations in Linguistic Representation",
    "section": "",
    "text": "In computational linguistics, using different representations can shed light on the structure of language. I’m considering how sentences can be depicted using parse trees or dependency trees, highlighting the inherent ambiguity of natural language and the preservation of lexical units during transformations."
  },
  {
    "objectID": "posts/2024/2023-02-22-transformations/index.html#the-ambiguity-and-structure-of-language",
    "href": "posts/2024/2023-02-22-transformations/index.html#the-ambiguity-and-structure-of-language",
    "title": "Transformations in Linguistic Representation",
    "section": "The Ambiguity and Structure of Language",
    "text": "The Ambiguity and Structure of Language\nNatural language is inherently ambiguous, ambiguity of small units can multiply leading to the possibility of multiple interpretations for a single sentence. This ambiguity can be represented through multiple parse trees, each illustrating a different interpretation. An important aspect of these trees is their inclusion of lexical units, which are the fundamental elements of language, such as words or phrases, that carry meaning. In such cases humans are able to ignore these ambiguity and focus on just one shuch parse.\nFor an machine this is more challanging, it may become aware of combinatoricaly many trees. For each sentence and then need to go back and forth pick clues from this collective to eliminate superrious parses. Looking and interpreting such trees can be challanging to humans who are used to surface forms. Multiple trees may have the same surface form but most might have a pathological erros in the deep structure. In reality linguists have over the years come up with many formal grammars each leading to differnt trees structures for any given sentence."
  },
  {
    "objectID": "posts/2024/2023-02-22-transformations/index.html#preserving-lexical-units-in-transformations",
    "href": "posts/2024/2023-02-22-transformations/index.html#preserving-lexical-units-in-transformations",
    "title": "Transformations in Linguistic Representation",
    "section": "Preserving Lexical Units in Transformations",
    "text": "Preserving Lexical Units in Transformations\nTransformations in linguistics refer to the systematic modification of sentence structure without altering the meaning or essential properties of the lexical units involved. This concept echoes Évariste Galois’ groundbreaking work in mathematics, where he explored the transformation of the roots of polynomial equations while maintaining their essential structure. Similarly, in linguistics, if we systematically study transformations of tree representations in a way that preserves various essential properties, such as meaning and lexical integrity we may gain deep insights."
  },
  {
    "objectID": "posts/2024/2023-02-22-transformations/index.html#implications-for-natural-language-processing-nlp",
    "href": "posts/2024/2023-02-22-transformations/index.html#implications-for-natural-language-processing-nlp",
    "title": "Transformations in Linguistic Representation",
    "section": "Implications for Natural Language Processing (NLP)",
    "text": "Implications for Natural Language Processing (NLP)\nThe ability to specify and conduct systematic transformation on tree representations holds great potential for data augmentation in NLP. By understanding and applying transformations that keep lexical units intact, we can generate varied linguistic data that maintains the original meaning, aiding in the development of more robust NLP models.\nMoreover, exploring transformations across languages that preserve essential functions, such as meaning, opens the door to creating machine learning (ML) systems capable of finding representations that are minimally ambiguous. This pursuit of a minimally ambiguous representation could lead to the discovery of a synthetic form that is not only unambiguous but also easily translatable across languages.\nFor example gender is frequently a semanticaly meaningless linguistic construct for whole classes of nouns. Yet natural langauges use lots of resources to handle gender regardless of is vaccous nature. Certain langauges use multiple variants of gramatical gender, yet others do not mark for gender and number as much as others. Are some languages more expressive - Not according to the saphir worph hypothesis. But some languages are more efficient at least in the sense of information theoretical mesures, such as entropy, prcesion and recall.\nAll these suggest seeking more compact alternatives."
  },
  {
    "objectID": "posts/2024/2023-02-22-transformations/index.html#toward-a-deeper-understanding-of-language-mechanisms",
    "href": "posts/2024/2023-02-22-transformations/index.html#toward-a-deeper-understanding-of-language-mechanisms",
    "title": "Transformations in Linguistic Representation",
    "section": "Toward a Deeper Understanding of Language Mechanisms",
    "text": "Toward a Deeper Understanding of Language Mechanisms\nThe quest for a synthetic representation that bridges linguistic gaps and reduces ambiguity has the potential to uncover the deeper mechanisms of language obscured by surface forms. Such a representation may provide invaluable insights into the universal aspects of language, facilitating more effective translation and interpretation by ML systems.\nIn conclusion, the exploration of linguistic transformations that maintain the integrity of lexical units draws parallels with transformative achievements in mathematics and may lead to profound implications for the advancement of NLP and our understanding of language.\nBy systematically analyzing and applying these transformations, we can enhance data augmentation techniques, reduce ambiguity in ML representations, and gain deeper insights into the structure and function of language across different linguistic systems."
  },
  {
    "objectID": "posts/2024/2023-02-22-transformations/index.html#lewis-games-and-interlingual-convergence",
    "href": "posts/2024/2023-02-22-transformations/index.html#lewis-games-and-interlingual-convergence",
    "title": "Transformations in Linguistic Representation",
    "section": "Lewis Games and Interlingual Convergence",
    "text": "Lewis Games and Interlingual Convergence\nNow we know that languages affect each others, forieng words, metaphors and idioms frequently cross lanaguage barriars. Grammar is more slow to change but even the most intricate systems like the conjurgation of the verb are subject to change given enough time.\nGiven the formal treatment of Lewis Games we can study the evolution of language, considering how complex constructs like grammer and morphology evolve to increase efficeny (again considered in terms of information theory)\nIf langauges be can made to rapidly evolve in lab settings we can perhaps also learn about the dynamic of transformations by looking at what constitutes evolutionaty stable solutions to these evolution of of grammar and the lexicon."
  }
]