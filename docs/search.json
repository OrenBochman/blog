[
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html",
    "href": "posts/2023/2023-12-20-autogluon/index.html",
    "title": "AutoGluon Cheetsheets",
    "section": "",
    "text": "AutoGluon is a powerful framework for auto-ML."
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "href": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "title": "AutoGluon Cheetsheets",
    "section": "Tabular",
    "text": "Tabular\n\n\n\nTabular"
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "href": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "title": "AutoGluon Cheetsheets",
    "section": "Time Series",
    "text": "Time Series\n\n\n\nTime Series"
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "href": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "title": "AutoGluon Cheetsheets",
    "section": "Multimodal",
    "text": "Multimodal\n\n\n\nMultimodal\n\n\n\n\n\nautogluon\nTabular\nTime Series\nMultimodal"
  },
  {
    "objectID": "posts/2024/welcome/index.html",
    "href": "posts/2024/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\n\nthumbnail\n\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nthumbnail\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2024-01-26},\n  url = {https://orenbochman.github.io/blog//posts/2024/welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. “Welcome To My Blog.” January 26,\n2024. https://orenbochman.github.io/blog//posts/2024/welcome."
  },
  {
    "objectID": "posts/2012/2012-07-26-wikisym-2012_files/index.html",
    "href": "posts/2012/2012-07-26-wikisym-2012_files/index.html",
    "title": "Wikisym 2012",
    "section": "",
    "text": "Due to a kind grant by the WikiMedia Foundation I was able to attend Wikisym 2012 in Linz Austria what follows is my report on the event.\n\n\n\n\n\n\nBackground\n\n\n\n\n\nThe renovated Ars Electronica Center at Linz, seen from the bridge across the Danube at night\n\n\n\nHPaul, CC BY-SA 3.0 via Wikimedia Commons\n\nI am a Wikipedian based in Budapest Hungary. I have been active for the last year with WM.HU and participated in a number of the local event’s chapters ever since being introduced to them at Wikimania 2011 in Haifa. During such a meeting I, Bence Damkos, and other chapter luminaries got to discussing many apparent cultural paradoxes taking place in a virtual community. Since I was studying the theory of games at the time I began to notice that some of the situations were very similar to a classic game such as the prisoner’s dilemma and the battle of the sexes while others resembled second-price sealed actions and bargaining games. I was intrigued and I started publishing some analysis on a page on Meta.\nAt this time I came across some interesting ideas from another researcher, Jodi Schneider who introduced me to the field of Computer Supported Collaborative Work (CSCW) and to her area of research - the deletion process. Eventually, she suggested that I should attend wikisym. However, I had no background in writing a conference paper I asked her for help and she copy-edited my work guiding me through a number of tricky issues. I eventually submitted the paper and to my surprise, it was accepted. So I took a train to Linz - I was surprised when after boarding the train that I had to reserve a seat and accordingly had to stand for the duration of the five-hour journey. By the time I arrived at the little town it was late and I was exhausted. I took a bus and ended in a hotel by the Danube.\n\n\nAt the Conference\nOn the morning of the conference, I took breakfast and met some of my favorite wikipedians - Maryna Pinchuk and Ryan Faulkner who were preparing to give a paper on their work in running editor engagement experiment - in which I had unwittingly participated. After a short chat I made my way to the venue the Ars Electronica and I could not believe my eyes - the conference was hosted by one of the most amazing technology museums in Europe. In the evening, the building would completely dominate the riverside’s view with its digital animation installations.\n\n\n\n\n\nR. Stuart Geiger\n\n\n\nAnne Helmond, CC by-nc-nd 2.0 via flicker\n\nThe Conference began with a number of presentations. I was impressed by most of the presentation but my sentiments were clearly not shared by everyone at the conference. I later learned that some of the more vehement voices were doctoral students who were out to prove their mettle. The papers that most struck my fancy used a number of novel techniques. Ranging from actuarial, survival analysis through SNA to sentiment analysis. Classifying Wikipedia Articles Using Network Motif Counts and Ratios by Guangyu Wu, Martin Harrigan and Pádraig Cunningham was one of the hardest to understand. It used a novel SNA technique to classify Wikipedia articles. However, it seemed that the other participant did not like the level of detail that the researchers had provided. Dr. Bernie Hoagan a Research Fellow from the Oxford Internet Institute asked the researchers why they had not tried to use ERGMs which might give more accurate results. I would later correspond with Dr. Hoagan and he helped me get started with Social network analysis. A paper by Michela Ferron and Paolo Massa titled Psychological processes underlying Wikipedia representations of natural and man-made disasters. It showcased the use of sentiment analysis. I was already familier with this method from my work in a Natural Language Programming outfit in Israel for which I wrote a search engine for the Hebrew Wikipedia. But I had consider this technique as very complex to set-up. On reviewing the paper I realised that an off the shelf tool called LIWC (Linguistic Inquiry and Word Count) can do the job. LIWC was developed by a team lead by James W. Pennebaker whose book The Secret Life of Pronouns is a gentle introduction to the intricacies of sentiment analysis. What remained difficult to grasp was a three-dimensional model of sentiment. I was unfamiliar with the terminology so I would end up rereading this paper a couple of times. But this was not the only paper to use sentiment analysis or natural language technology. Manypedia: Comparing Language Points of View of Wikipedia Communities by Paolo Massa and Federico Scrinzi which showed a tool that allows users to compare different language edition version of the same article in their own language using machine translation. A second paper to discuss sentiment analysis, this time focusing on talk pages was: Emotions and dialogue in a peer-production community: the case of Wikipedia. This paper used an even more complex paradigm than the previous one. It utilized Margaret M. Bradley & Peter J. Lang’s ANEW (Affective Norms for English Words) word list to create a three-dimensional model of sentiment (valence, arousal and dominance). Even more interesting were its conclusions regarding participation of women and its implication on Wikipedia’s growing gender gap.\n\n\n\n\n\nHeather Ford, Jimmy Wales\n\n\n\nMessedrocker, CC BY 1.0 via Wikimedia Commons\n\nI would discuss some of my ideas to some of the participants over dinner. One amusing debate included [Stuart Geiger] and when I quoted a point from an excellent paper he pointed out that he had written it. I also met with heather ford who co-authored a paper with Mr Geiger. Heather Ford told us about her blog Ethnography matters which I started to follow because it turns out that ethnography really matters These include work by Stuart Geiger and on the lives of robots using trace ethnography. During the conference I met with Jodi Schneider but we had little opportunity to chat due to an upcoming deadline. I enjoy following her research on deletion as well as on Argumentation in collaborative deliberations. I decided to help Wikipedia’s research newsletter by abstracting and providing laymen’s summaries to CSCW related research.\n\n\nPanels, Demos and Posters\n\n\n\n\n\nPhoebe Ayers at WikiSym\n\n\n\nRagesoss, CC BY-SA 3.0 via Wikimedia Commons\n\nI found out that the WikiSym conference had a colourful history and participated in a discussion mediated by the delectable Phoebe Ayers on the conference’s future. I suggested that the conference should be collocated with Wikimania since this would help reduce cost of community members who attend the Wikimania conference. A second conundrum being debated being the issue of open academy. This was an issue of growing urgency since the WMF, one of Wikisym’s chief sponsors prefers to support open access open research work. I think that Phoebe Ayers is a wonderful person and was sad to hear she was no longer on the foundation board of directors. Another serendipitous facet of the Wikisym conference is the demo and poster session which allow hackers to present their latest breakthroughs and innovations in technology, of Wikis. This had once been the cornerstone of the conference. I met the developers of TikiWiki as well as the a Java based XWiki. I decided that one day I would implement my own version of the wiki.\n\n\nJimmy Wales’ Keynote Address\nWikisym’s keynote was given by Wikipedia’s co-founder Jimmy Wales. He explained how this talk was one of the ticket he would give this year. However, this was a much better talk than he gave at Wikimania. He mentioned research possibilities and he responded to my question. I was and still am considering if population dynamics could affect phase changes within the community. My question was if a Wiki’s community dropped below a certain size if it would no longer be viable to maintain it. One example of a Wiki being shut down was the 9-11 wiki. I found Wales’ answer enlightening - he said that big or small the community should have little problem adapting to take care of it’s Wiki. Another point worth mentioning was his recommendation to use Wiki data sets of smaller wikis in research. He recommended Muppet wiki as an example of a wiki with a significantly different governance structure than Wikipedia.\n\n\nAfter the conference\nFollowing the conference, I kept in touch with a number of the participants. I applied myself to study social network analysis as well as data analysis with R. I increased my participation in the research newsletter. I hope to expand my research further using population dynamics on graphs and evolutionary game theory. However, with all the new research methods, I’ve gleaned. I am uncertain what direction my future investigations will take only that they will be even more exciting than before.\n\n\n\nThe renovated Ars Electronica Center at Linz, seen from the bridge across the Danube at night\nR. Stuart Geiger\nHeather Ford, Jimmy Wales\nPhoebe Ayers at WikiSym\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Wikisym 2012},\n  date = {2022-07-26},\n  url = {https://orenbochman.github.io/blog//posts/2012/2012-07-26-wikisym-2012_files},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Wikisym 2012.” July 26, 2022. https://orenbochman.github.io/blog//posts/2012/2012-07-26-wikisym-2012_files."
  },
  {
    "objectID": "posts/2011/2011-11-29-text-mining-with-r/index.html",
    "href": "posts/2011/2011-11-29-text-mining-with-r/index.html",
    "title": "Text Mining With R",
    "section": "",
    "text": "Computational Linguistics tasks:"
  },
  {
    "objectID": "posts/2011/2011-11-29-text-mining-with-r/index.html#text-preprocessing",
    "href": "posts/2011/2011-11-29-text-mining-with-r/index.html#text-preprocessing",
    "title": "Text Mining With R",
    "section": "Text preprocessing",
    "text": "Text preprocessing\n\n4tm_corpus &lt;- tm_map(tm_corpus, tolower)\ninspect(tm_corpus)\n\n\n4\n\nthis makes all the tokens lowercase\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs, hospitals, doctors                                                                                                                                      \n [2] smog, pollution, micro-plastics, environment.                                                                                                                  \n [3] doctors, hospitals, healthcare                                                                                                                                 \n [4] pollution, environment, water.                                                                                                                                 \n [5] i love nlp with deep learning.                                                                                                                                 \n [6] i love machine learning.                                                                                                                                       \n [7] he said he was keeping the wolf from the door.                                                                                                                 \n [8] time flies like an arrow, fruit flies like a banana.                                                                                                           \n [9] pollution, greenhouse gasses, ghg, hydrofluorocarbons, ozone hole, global warming. montreal protocol.                                                          \n[10] greenhouse gasses, hydrofluorocarbons, perfluorocarbons, sulfur hexafluoride, carbon dioxide, carbon monoxide, co2, hydrofluorocarbons, methane, nitrous oxide.\n\n\n\n5tm_corpus &lt;- tm_map(tm_corpus, content_transformer(removePunctuation))\ninspect(tm_corpus)\n\n\n5\n\nthis removes punctuation tokens\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                              \n [2] smog pollution microplastics environment                                                                                                             \n [3] doctors hospitals healthcare                                                                                                                         \n [4] pollution environment water                                                                                                                          \n [5] i love nlp with deep learning                                                                                                                        \n [6] i love machine learning                                                                                                                              \n [7] he said he was keeping the wolf from the door                                                                                                        \n [8] time flies like an arrow fruit flies like a banana                                                                                                   \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                       \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co2 hydrofluorocarbons methane nitrous oxide\n\n\n\n6tm_corpus &lt;- tm_map(tm_corpus, removeWords, stopwords(\"english\"))\ninspect(tm_corpus)\n\n\n6\n\nthis removes stop words\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                              \n [2] smog pollution microplastics environment                                                                                                             \n [3] doctors hospitals healthcare                                                                                                                         \n [4] pollution environment water                                                                                                                          \n [5]  love nlp  deep learning                                                                                                                             \n [6]  love machine learning                                                                                                                               \n [7]  said   keeping  wolf   door                                                                                                                         \n [8] time flies like  arrow fruit flies like  banana                                                                                                      \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                       \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co2 hydrofluorocarbons methane nitrous oxide\n\n\n\n7tm_corpus &lt;- tm_map(tm_corpus, removeNumbers)\ninspect(tm_corpus)\n\n\n7\n\nthis removes numbers\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                             \n [2] smog pollution microplastics environment                                                                                                            \n [3] doctors hospitals healthcare                                                                                                                        \n [4] pollution environment water                                                                                                                         \n [5]  love nlp  deep learning                                                                                                                            \n [6]  love machine learning                                                                                                                              \n [7]  said   keeping  wolf   door                                                                                                                        \n [8] time flies like  arrow fruit flies like  banana                                                                                                     \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                      \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co hydrofluorocarbons methane nitrous oxide\n\n\n\n8tm_corpus &lt;- tm_map(tm_corpus, stemDocument, language=\"english\")\ninspect(tm_corpus)\n\n\n8\n\nthis stems the words\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drug hospit doctor                                                                                                                       \n [2] smog pollut microplast environ                                                                                                           \n [3] doctor hospit healthcar                                                                                                                  \n [4] pollut environ water                                                                                                                     \n [5] love nlp deep learn                                                                                                                      \n [6] love machin learn                                                                                                                        \n [7] said keep wolf door                                                                                                                      \n [8] time fli like arrow fruit fli like banana                                                                                                \n [9] pollut greenhous gass ghg hydrofluorocarbon ozon hole global warm montreal protocol                                                      \n[10] greenhous gass hydrofluorocarbon perfluorocarbon sulfur hexafluorid carbon dioxid carbon monoxid co hydrofluorocarbon methan nitrous oxid\n\n\n\n9tm_corpus &lt;- tm_map(tm_corpus, stripWhitespace)\ninspect(tm_corpus)\n\n\n9\n\nRemoving Whitespaces - a single white space or group of whitespaces may be considered to be a token within a corpus. This is how we remove these token\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drug hospit doctor                                                                                                                       \n [2] smog pollut microplast environ                                                                                                           \n [3] doctor hospit healthcar                                                                                                                  \n [4] pollut environ water                                                                                                                     \n [5] love nlp deep learn                                                                                                                      \n [6] love machin learn                                                                                                                        \n [7] said keep wolf door                                                                                                                      \n [8] time fli like arrow fruit fli like banana                                                                                                \n [9] pollut greenhous gass ghg hydrofluorocarbon ozon hole global warm montreal protocol                                                      \n[10] greenhous gass hydrofluorocarbon perfluorocarbon sulfur hexafluorid carbon dioxid carbon monoxid co hydrofluorocarbon methan nitrous oxid\n\n\n\ndtm &lt;- DocumentTermMatrix(tm_corpus)\ninspect(dtm)\n\n&lt;&lt;DocumentTermMatrix (documents: 10, terms: 43)&gt;&gt;\nNon-/sparse entries: 53/377\nSparsity           : 88%\nMaximal term length: 17\nWeighting          : term frequency (tf)\nSample             :\n    Terms\nDocs doctor environ fli gass hospit hydrofluorocarbon learn like love pollut\n  1       1       0   0    0      1                 0     0    0    0      0\n  10      0       0   0    1      0                 2     0    0    0      0\n  2       0       1   0    0      0                 0     0    0    0      1\n  3       1       0   0    0      1                 0     0    0    0      0\n  4       0       1   0    0      0                 0     0    0    0      1\n  5       0       0   0    0      0                 0     1    0    1      0\n  6       0       0   0    0      0                 0     1    0    1      0\n  7       0       0   0    0      0                 0     0    0    0      0\n  8       0       0   2    0      0                 0     0    2    0      0\n  9       0       0   0    1      0                 1     0    0    0      1\n\n\n\nfindFreqTerms(dtm, 2)\n\n [1] \"doctor\"            \"hospit\"            \"environ\"          \n [4] \"pollut\"            \"learn\"             \"love\"             \n [7] \"fli\"               \"like\"              \"gass\"             \n[10] \"greenhous\"         \"hydrofluorocarbon\" \"carbon\"           \n\n\n\nfindAssocs(dtm, \"polution\", 0.8)\n\n$polution\nnumeric(0)\n\n\n\nas.matrix(dtm)\n\n    Terms\nDocs doctor drug hospit environ microplast pollut smog healthcar water deep\n  1       1    1      1       0          0      0    0         0     0    0\n  2       0    0      0       1          1      1    1         0     0    0\n  3       1    0      1       0          0      0    0         1     0    0\n  4       0    0      0       1          0      1    0         0     1    0\n  5       0    0      0       0          0      0    0         0     0    1\n  6       0    0      0       0          0      0    0         0     0    0\n  7       0    0      0       0          0      0    0         0     0    0\n  8       0    0      0       0          0      0    0         0     0    0\n  9       0    0      0       0          0      1    0         0     0    0\n  10      0    0      0       0          0      0    0         0     0    0\n    Terms\nDocs learn love nlp machin door keep said wolf arrow banana fli fruit like time\n  1      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  2      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  3      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  4      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  5      1    1   1      0    0    0    0    0     0      0   0     0    0    0\n  6      1    1   0      1    0    0    0    0     0      0   0     0    0    0\n  7      0    0   0      0    1    1    1    1     0      0   0     0    0    0\n  8      0    0   0      0    0    0    0    0     1      1   2     1    2    1\n  9      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  10     0    0   0      0    0    0    0    0     0      0   0     0    0    0\n    Terms\nDocs gass ghg global greenhous hole hydrofluorocarbon montreal ozon protocol\n  1     0   0      0         0    0                 0        0    0        0\n  2     0   0      0         0    0                 0        0    0        0\n  3     0   0      0         0    0                 0        0    0        0\n  4     0   0      0         0    0                 0        0    0        0\n  5     0   0      0         0    0                 0        0    0        0\n  6     0   0      0         0    0                 0        0    0        0\n  7     0   0      0         0    0                 0        0    0        0\n  8     0   0      0         0    0                 0        0    0        0\n  9     1   1      1         1    1                 1        1    1        1\n  10    1   0      0         1    0                 2        0    0        0\n    Terms\nDocs warm carbon dioxid hexafluorid methan monoxid nitrous oxid perfluorocarbon\n  1     0      0      0           0      0       0       0    0               0\n  2     0      0      0           0      0       0       0    0               0\n  3     0      0      0           0      0       0       0    0               0\n  4     0      0      0           0      0       0       0    0               0\n  5     0      0      0           0      0       0       0    0               0\n  6     0      0      0           0      0       0       0    0               0\n  7     0      0      0           0      0       0       0    0               0\n  8     0      0      0           0      0       0       0    0               0\n  9     1      0      0           0      0       0       0    0               0\n  10    0      2      1           1      1       1       1    1               1\n    Terms\nDocs sulfur\n  1       0\n  2       0\n  3       0\n  4       0\n  5       0\n  6       0\n  7       0\n  8       0\n  9       0\n  10      1\n\n\nload(url(“https://cbail.github.io/Trump_Tweets.Rdata”)) head(trumptweets$text)"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html",
    "href": "posts/2011/bash-cheatsheet/index.html",
    "title": "Bash",
    "section": "",
    "text": "#!/bin/bash\n\nVAR=\"world\"\necho \"Hello $VAR!\" # =&gt; Hello world!\nExecute the script shell script $ bash hello.sh\n\n\n\nNAME=\"John\"\n\necho ${NAME}    # =&gt; John (Variables)\necho $NAME      # =&gt; John (Variables)\necho \"$NAME\"    # =&gt; John (Variables)\necho '$NAME'    # =&gt; $NAME (Exact string)\necho \"${NAME}!\" # =&gt; John! (Variables)\n\nNAME = \"John\"   # =&gt; Error (about space)\n\n\n\n# This is an inline Bash comment.\n: '\nThis is a\nvery neat comment\nin bash\n'\nMulti-line comments use :' to open and ' to close\n\n\n\n\n\n\nExpression\nDescription\n\n\n\n\n$1 … $9\nParameter 1 … 9\n\n\n$0\nName of the script itself\n\n\n$1\nFirst argument\n\n\n${10}\nPositional parameter 10\n\n\n$#\nNumber of arguments\n\n\n$$\nProcess id of the shell\n\n\n$*\nAll arguments\n\n\n$@\nAll arguments, starting from first\n\n\n$-\nCurrent options\n\n\n$_\nLast argument of the previous command\n\n\n\nSee: Special parameters\n\n\n\nget_name() {\n    echo \"John\"\n}\n\necho \"You are $(get_name)\"\nSee: Functions\n\n\n\nif [[ -z \"$string\" ]]; then\n    echo \"String is empty\"\nelif [[ -n \"$string\" ]]; then\n    echo \"String is not empty\"\nfi\nSee: Conditionals\n\n\n\necho {A,B}.js\n\n\n\n\nExpression\nDescription\n\n\n\n\n{A,B}\nSame as A B\n\n\n{A,B}.js\nSame as A.js B.js\n\n\n{1..5}\nSame as 1 2 3 4 5\n\n\n\nSee: Brace expansion\n\n\n\n# =&gt; I'm in /path/of/current\necho \"I'm in $(PWD)\"\n\n# Same as:\necho \"I'm in `pwd`\"\nSee: Command substitution"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#getting-started",
    "href": "posts/2011/bash-cheatsheet/index.html#getting-started",
    "title": "Bash",
    "section": "",
    "text": "#!/bin/bash\n\nVAR=\"world\"\necho \"Hello $VAR!\" # =&gt; Hello world!\nExecute the script shell script $ bash hello.sh\n\n\n\nNAME=\"John\"\n\necho ${NAME}    # =&gt; John (Variables)\necho $NAME      # =&gt; John (Variables)\necho \"$NAME\"    # =&gt; John (Variables)\necho '$NAME'    # =&gt; $NAME (Exact string)\necho \"${NAME}!\" # =&gt; John! (Variables)\n\nNAME = \"John\"   # =&gt; Error (about space)\n\n\n\n# This is an inline Bash comment.\n: '\nThis is a\nvery neat comment\nin bash\n'\nMulti-line comments use :' to open and ' to close\n\n\n\n\n\n\nExpression\nDescription\n\n\n\n\n$1 … $9\nParameter 1 … 9\n\n\n$0\nName of the script itself\n\n\n$1\nFirst argument\n\n\n${10}\nPositional parameter 10\n\n\n$#\nNumber of arguments\n\n\n$$\nProcess id of the shell\n\n\n$*\nAll arguments\n\n\n$@\nAll arguments, starting from first\n\n\n$-\nCurrent options\n\n\n$_\nLast argument of the previous command\n\n\n\nSee: Special parameters\n\n\n\nget_name() {\n    echo \"John\"\n}\n\necho \"You are $(get_name)\"\nSee: Functions\n\n\n\nif [[ -z \"$string\" ]]; then\n    echo \"String is empty\"\nelif [[ -n \"$string\" ]]; then\n    echo \"String is not empty\"\nfi\nSee: Conditionals\n\n\n\necho {A,B}.js\n\n\n\n\nExpression\nDescription\n\n\n\n\n{A,B}\nSame as A B\n\n\n{A,B}.js\nSame as A.js B.js\n\n\n{1..5}\nSame as 1 2 3 4 5\n\n\n\nSee: Brace expansion\n\n\n\n# =&gt; I'm in /path/of/current\necho \"I'm in $(PWD)\"\n\n# Same as:\necho \"I'm in `pwd`\"\nSee: Command substitution"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-parameter-expansions",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-parameter-expansions",
    "title": "Bash",
    "section": "Bash Parameter expansions",
    "text": "Bash Parameter expansions\n\nSyntax\n\n\n\nCode\nDescription\n\n\n\n\n${FOO%suffix}\nRemove suffix\n\n\n${FOO#prefix}\nRemove prefix\n\n\n${FOO%%suffix}\nRemove long suffix\n\n\n${FOO##prefix}\nRemove long prefix\n\n\n${FOO/from/to}\nReplace first match\n\n\n${FOO//from/to}\nReplace all\n\n\n${FOO/%from/to}\nReplace suffix\n\n\n${FOO/#from/to}\nReplace prefix\n\n\n\n\nSubstrings\n\n\n\nExpression\nDescription\n\n\n\n\n${FOO:0:3}\nSubstring (position, length)\n\n\n${FOO:(-3):3}\nSubstring from the right\n\n\n\n\n\nLength\n\n\n\nExpression\nDescription\n\n\n\n\n${#FOO}\nLength of $FOO\n\n\n\n\n\nDefault values\n\n\n\nExpression\nDescription\n\n\n\n\n${FOO:-val}\n$FOO, or val if unset\n\n\n${FOO:=val}\nSet $FOO to val if unset\n\n\n${FOO:+val}\nval if $FOO is set\n\n\n${FOO:?message}\nShow message and exit if $FOO is unset\n\n\n\n\n\n\nSubstitution\necho ${food:-Cake}  #=&gt; $food or \"Cake\"\nSTR=\"/path/to/foo.cpp\"\necho ${STR%.cpp}    # /path/to/foo\necho ${STR%.cpp}.o  # /path/to/foo.o\necho ${STR%/*}      # /path/to\n\necho ${STR##*.}     # cpp (extension)\necho ${STR##*/}     # foo.cpp (basepath)\n\necho ${STR#*/}      # path/to/foo.cpp\necho ${STR##*/}     # foo.cpp\n\necho ${STR/foo/bar} # /path/to/bar.cpp\n\n\nSlicing\nname=\"John\"\necho ${name}           # =&gt; John\necho ${name:0:2}       # =&gt; Jo\necho ${name::2}        # =&gt; Jo\necho ${name::-1}       # =&gt; Joh\necho ${name:(-1)}      # =&gt; n\necho ${name:(-2)}      # =&gt; hn\necho ${name:(-2):2}    # =&gt; hn\n\nlength=2\necho ${name:0:length}  # =&gt; Jo\nSee: Parameter expansion\n\n\nbasepath & dirpath\nSRC=\"/path/to/foo.cpp\"\nBASEPATH=${SRC##*/}   \necho $BASEPATH  # =&gt; \"foo.cpp\"\n\n\nDIRPATH=${SRC%$BASEPATH}\necho $DIRPATH   # =&gt; \"/path/to/\"\n\n\nTransform\nSTR=\"HELLO WORLD!\"\necho ${STR,}   # =&gt; hELLO WORLD!\necho ${STR,,}  # =&gt; hello world!\n\nSTR=\"hello world!\"\necho ${STR^}   # =&gt; Hello world!\necho ${STR^^}  # =&gt; HELLO WORLD!\n\nARR=(hello World)\necho \"${ARR[@],}\" # =&gt; hello world\necho \"${ARR[@]^}\" # =&gt; Hello World"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-arrays",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-arrays",
    "title": "Bash",
    "section": "Bash Arrays",
    "text": "Bash Arrays\n\nDefining arrays\nFruits=('Apple' 'Banana' 'Orange')\n\nFruits[0]=\"Apple\"\nFruits[1]=\"Banana\"\nFruits[2]=\"Orange\"\n\nARRAY1=(foo{1..2}) # =&gt; foo1 foo2\nARRAY2=({A..D})    # =&gt; A B C D\n\n# Merge =&gt; foo1 foo2 A B C D\nARRAY3=(${ARRAY1[@]} ${ARRAY2[@]})\n\n# declare construct\ndeclare -a Numbers=(1 2 3)\nNumbers+=(4 5) # Append =&gt; 1 2 3 4 5\n\n\nIndexing\n\n\n\n-\n-\n\n\n\n\n${Fruits[0]}\nFirst element\n\n\n${Fruits[-1]}\nLast element\n\n\n${Fruits[*]}\nAll elements\n\n\n${Fruits[@]}\nAll elements\n\n\n${#Fruits[@]}\nNumber of all\n\n\n${#Fruits}\nLength of 1st\n\n\n${#Fruits[3]}\nLength of nth\n\n\n${Fruits[@]:3:2}\nRange\n\n\n${!Fruits[@]}\nKeys of all\n\n\n\n\n\nIteration\nFruits=('Apple' 'Banana' 'Orange')\n\nfor e in \"${Fruits[@]}\"; do\n    echo $e\ndone\n\nWith index\nfor i in \"${!Fruits[@]}\"; do\n  printf \"%s\\t%s\\n\" \"$i\" \"${Fruits[$i]}\"\ndone\n\n\n\nOperations\nFruits=(\"${Fruits[@]}\" \"Watermelon\")     # Push\nFruits+=('Watermelon')                   # Also Push\nFruits=( ${Fruits[@]/Ap*/} )             # Remove by regex match\nunset Fruits[2]                          # Remove one item\nFruits=(\"${Fruits[@]}\")                  # Duplicate\nFruits=(\"${Fruits[@]}\" \"${Veggies[@]}\")  # Concatenate\nlines=(`cat \"logfile\"`)                  # Read from file\n\n\nArrays as arguments\nfunction extract()\n{\n    local -n myarray=$1\n    local idx=$2\n    echo \"${myarray[$idx]}\"\n}\nFruits=('Apple' 'Banana' 'Orange')\nextract Fruits 2     # =&gt; Orangle"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-dictionaries",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-dictionaries",
    "title": "Bash",
    "section": "Bash Dictionaries",
    "text": "Bash Dictionaries\n\nDefining\ndeclare -A sounds\nsounds[dog]=\"bark\"\nsounds[cow]=\"moo\"\nsounds[bird]=\"tweet\"\nsounds[wolf]=\"howl\"\n\n\nWorking with dictionaries\necho ${sounds[dog]} # Dog's sound\necho ${sounds[@]}   # All values\necho ${!sounds[@]}  # All keys\necho ${#sounds[@]}  # Number of elements\nunset sounds[dog]   # Delete dog\n\n\nIteration\nfor val in \"${sounds[@]}\"; do\n    echo $val\ndone\n\nfor key in \"${!sounds[@]}\"; do\n    echo $key\ndone"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-conditionals",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-conditionals",
    "title": "Bash",
    "section": "Bash Conditionals",
    "text": "Bash Conditionals\n\nInteger conditions\n\n\n\nCondition\nDescription\n\n\n\n\n[[ NUM -eq NUM ]]\nEqual\n\n\n[[ NUM -ne NUM ]]\nNot equal\n\n\n[[ NUM -lt NUM ]]\nLess than\n\n\n[[ NUM -le NUM ]]\nLess than or equal\n\n\n[[ NUM -gt NUM ]]\nGreater than\n\n\n[[ NUM -ge NUM ]]\nGreater than or equal\n\n\n(( NUM &lt; NUM ))\nLess than\n\n\n(( NUM &lt;= NUM ))\nLess than or equal\n\n\n(( NUM &gt; NUM ))\nGreater than\n\n\n(( NUM &gt;= NUM ))\nGreater than or equal\n\n\n\n\n\nString conditions\n\n\n\nCondition\nDescription\n\n\n\n\n[[ -z STR ]]\nEmpty string\n\n\n[[ -n STR ]]\nNot empty string\n\n\n[[ STR == STR ]]\nEqual\n\n\n[[ STR = STR ]]\nEqual (Same above)\n\n\n[[ STR &lt; STR ]]\nLess than (ASCII)\n\n\n[[ STR &gt; STR ]]\nGreater than (ASCII)\n\n\n[[ STR != STR ]]\nNot Equal\n\n\n[[ STR =~ STR ]]\nRegexp\n\n\n\n\n\nExample\n\nString\nif [[ -z \"$string\" ]]; then\n    echo \"String is empty\"\nelif [[ -n \"$string\" ]]; then\n    echo \"String is not empty\"\nelse\n    echo \"This never happens\"\nfi\n\n\nCombinations\nif [[ X && Y ]]; then\n    ...\nfi\n\n\nEqual\nif [[ \"$A\" == \"$B\" ]]; then\n    ...\nfi\n\n\nRegex\nif [[ '1. abc' =~ ([a-z]+) ]]; then\n    echo ${BASH_REMATCH[1]}\nfi\n\n\nSmaller\nif (( $a &lt; $b )); then\n   echo \"$a is smaller than $b\"\nfi\n\n\nExists\nif [[ -e \"file.txt\" ]]; then\n    echo \"file exists\"\nfi\n\n\n\nFile conditions\n\n\n\nCondition\nDescription\n\n\n\n\n[[ -e FILE ]]\nExists\n\n\n[[ -d FILE ]]\nDirectory\n\n\n[[ -f FILE ]]\nFile\n\n\n[[ -h FILE ]]\nSymlink\n\n\n[[ -s FILE ]]\nSize is &gt; 0 bytes\n\n\n[[ -r FILE ]]\nReadable\n\n\n[[ -w FILE ]]\nWritable\n\n\n[[ -x FILE ]]\nExecutable\n\n\n[[ f1 -nt f2 ]]\nf1 newer than f2\n\n\n[[ f1 -ot f2 ]]\nf2 older than f1\n\n\n[[ f1 -ef f2 ]]\nSame files\n\n\n\n\n\nMore conditions\n\n\n\nCondition\nDescription\n\n\n\n\n[[ -o noclobber ]]\nIf OPTION is enabled\n\n\n[[ ! EXPR ]]\nNot\n\n\n[[ X && Y ]]\nAnd\n\n\n[[ X || Y ]]\nOr\n\n\n\n\n\nlogical and, or\nif [ \"$1\" = 'y' -a $2 -gt 0 ]; then\n    echo \"yes\"\nfi\n\nif [ \"$1\" = 'n' -o $2 -lt 0 ]; then\n    echo \"no\"\nfi"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-loops",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-loops",
    "title": "Bash",
    "section": "Bash Loops",
    "text": "Bash Loops\n\nBasic for loop\nfor i in /etc/rc.*; do\n    echo $i\ndone\n\n\nC-like for loop\nfor ((i = 0 ; i &lt; 100 ; i++)); do\n    echo $i\ndone\n\n\nRanges\nfor i in {1..5}; do\n    echo \"Welcome $i\"\ndone\n\nWith step size\nfor i in {5..50..5}; do\n    echo \"Welcome $i\"\ndone\n\n\n\nAuto increment\ni=1\nwhile [[ $i -lt 4 ]]; do\n    echo \"Number: $i\"\n    ((i++))\ndone\n\n\nAuto decrement\ni=3\nwhile [[ $i -gt 0 ]]; do\n    echo \"Number: $i\"\n    ((i--))\ndone\n\n\nContinue\nfor number in $(seq 1 3); do\n    if [[ $number == 2 ]]; then\n        continue;\n    fi\n    echo \"$number\"\ndone\n\n\nBreak\nfor number in $(seq 1 3); do\n    if [[ $number == 2 ]]; then\n        # Skip entire rest of loop.\n        break;\n    fi\n    # This will only print 1\n    echo \"$number\"\ndone\n\n\nUntil\ncount=0\nuntil [ $count -gt 10 ]; do\n    echo \"$count\"\n    ((count++))\ndone\n\n\nForever\nwhile true; do\n    # here is some code.\ndone\n\n\nForever (shorthand)\nwhile :; do\n    # here is some code.\ndone\n\n\nReading lines\ncat file.txt | while read line; do\n    echo $line\ndone"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-functions",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-functions",
    "title": "Bash",
    "section": "Bash Functions",
    "text": "Bash Functions\n\nDefining functions\nmyfunc() {\n    echo \"hello $1\"\n}\n# Same as above (alternate syntax)\nfunction myfunc() {\n    echo \"hello $1\"\n}\nmyfunc \"John\"\n\n\nReturning values\nmyfunc() {\n    local myresult='some value'\n    echo $myresult\n}\nresult=\"$(myfunc)\"\n\n\nRaising errors\nmyfunc() {\n    return 1\n}\nif myfunc; then\n    echo \"success\"\nelse\n    echo \"failure\"\nfi"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-options",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-options",
    "title": "Bash",
    "section": "Bash Options",
    "text": "Bash Options\n\nOptions\n# Avoid overlay files\n# (echo \"hi\" &gt; foo)\nset -o noclobber\n\n# Used to exit upon error\n# avoiding cascading errors\nset -o errexit   \n\n# Unveils hidden failures\nset -o pipefail  \n\n# Exposes unset variables\nset -o nounset\n\n\nGlob options\n# Non-matching globs are removed  \n# ('*.foo' =&gt; '')\nshopt -s nullglob   \n\n# Non-matching globs throw errors\nshopt -s failglob  \n\n# Case insensitive globs\nshopt -s nocaseglob \n\n# Wildcards match dotfiles \n# (\"*.sh\" =&gt; \".foo.sh\")\nshopt -s dotglob    \n\n# Allow ** for recursive matches \n# ('lib/**/*.rb' =&gt; 'lib/a/b/c.rb')\nshopt -s globstar"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#bash-history",
    "href": "posts/2011/bash-cheatsheet/index.html#bash-history",
    "title": "Bash",
    "section": "Bash History",
    "text": "Bash History\n\nCommands\n\n\n\nCommand\nDescription\n\n\n\n\nhistory\nShow history\n\n\nsudo !!\nRun the previous command with sudo\n\n\nshopt -s histverify\nDon’t execute expanded result immediately\n\n\n\n\n\nExpansions\n\n\n\nExpression\nDescription\n\n\n\n\n!$\nExpand last parameter of most recent command\n\n\n!*\nExpand all parameters of most recent command\n\n\n!-n\nExpand nth most recent command\n\n\n!n\nExpand nth command in history\n\n\n!&lt;command&gt;\nExpand most recent invocation of command &lt;command&gt;\n\n\n\n\n\nOperations\n\n\n\n\n\n\n\nCode\nDescription\n\n\n\n\n!!\nExecute last command again\n\n\n!!:s/&lt;FROM&gt;/&lt;TO&gt;/\nReplace first occurrence of &lt;FROM&gt; to &lt;TO&gt; in most recent command\n\n\n!!:gs/&lt;FROM&gt;/&lt;TO&gt;/\nReplace all occurrences of &lt;FROM&gt; to &lt;TO&gt; in most recent command\n\n\n!$:t\nExpand only basename from last parameter of most recent command\n\n\n!$:h\nExpand only directory from last parameter of most recent command\n\n\n\n!! and !$ can be replaced with any valid expansion.\n\n\nSlices\n\n\n\n\n\n\n\nCode\nDescription\n\n\n\n\n!!:n\nExpand only nth token from most recent command (command is 0; first argument is 1)\n\n\n!^\nExpand first argument from most recent command\n\n\n!$\nExpand last token from most recent command\n\n\n!!:n-m\nExpand range of tokens from most recent command\n\n\n!!:n-$\nExpand nth token to last from most recent command\n\n\n\n!! can be replaced with any valid expansion i.e. !cat, !-2, !42, etc."
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#miscellaneous",
    "href": "posts/2011/bash-cheatsheet/index.html#miscellaneous",
    "title": "Bash",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nNumeric calculations\n$((a + 200))      # Add 200 to $a\n$(($RANDOM%200))  # Random number 0..199\n\n\nSubshells\n(cd somedir; echo \"I'm now in $PWD\")\npwd # still in first directory\n\n\nInspecting commands\ncommand -V cd\n#=&gt; \"cd is a function/alias/whatever\"\n\n\nRedirection\npython hello.py &gt; output.txt   # stdout to (file)\npython hello.py &gt;&gt; output.txt  # stdout to (file), append\npython hello.py 2&gt; error.log   # stderr to (file)\npython hello.py 2&gt;&1           # stderr to stdout\npython hello.py 2&gt;/dev/null    # stderr to (null)\npython hello.py &&gt;/dev/null    # stdout and stderr to (null)\npython hello.py &lt; foo.txt      # feed foo.txt to stdin for python\n\n\nSource relative\nsource \"${0%/*}/../share/foo.sh\"\n\n\nDirectory of script\nDIR=\"${0%/*}\"\n\n\nCase/switch\ncase \"$1\" in\n    start | up)\n    vagrant up\n    ;;\n\n    *)\n    echo \"Usage: $0 {start|stop|ssh}\"\n    ;;\nesac\n\n\nTrap errors\ntrap 'echo Error at about $LINENO' ERR\nor\ntraperr() {\n    echo \"ERROR: ${BASH_SOURCE[1]} at about ${BASH_LINENO[0]}\"\n}\n\nset -o errtrace\ntrap traperr ERR\n\n\nprintf\nprintf \"Hello %s, I'm %s\" Sven Olga\n#=&gt; \"Hello Sven, I'm Olga\n\nprintf \"1 + 1 = %d\" 2\n#=&gt; \"1 + 1 = 2\"\n\nprintf \"Print a float: %f\" 2\n#=&gt; \"Print a float: 2.000000\"\n\n\nGetting options\nwhile [[ \"$1\" =~ ^- && ! \"$1\" == \"--\" ]]; do case $1 in\n    -V | --version )\n    echo $version\n    exit\n    ;;\n    -s | --string )\n    shift; string=$1\n    ;;\n    -f | --flag )\n    flag=1\n    ;;\nesac; shift; done\nif [[ \"$1\" == '--' ]]; then shift; fi\n\n\nCheck for command’s result\nif ping -c 1 google.com; then\n    echo \"It appears you have a working internet connection\"\nfi\n\n\nSpecial variables\n\n\n\nExpression\nDescription\n\n\n\n\n$?\nExit status of last task\n\n\n$!\nPID of last background task\n\n\n$$\nPID of shell\n\n\n$0\nFilename of the shell script\n\n\n\nSee Special parameters.\n\n\nGrep check\nif grep -q 'foo' ~/.bash_history; then\n    echo \"You appear to have typed 'foo' in the past\"\nfi\n\n\nBackslash escapes\n\n \n!\n\"\n#\n&\n'\n(\n)\n,\n;\n&lt;\n&gt;\n[\n|\n\\\n]\n^\n{\n}\n`\n$\n*\n? {.cols-4 .marker-none}\n\nEscape these special characters with \\\n\n\nHeredoc\ncat &lt;&lt;END\nhello world\nEND\n\n\nGo to previous directory\npwd # /home/user/foo\ncd bar/\npwd # /home/user/foo/bar\ncd -\npwd # /home/user/foo\n\n\nReading input\necho -n \"Proceed? [y/n]: \"\nread ans\necho $ans\nread -n 1 ans    # Just one character\n\n\nConditional execution\ngit commit && git push\ngit commit || echo \"Commit failed\"\n\n\nStrict mode\nset -euo pipefail\nIFS=$'\\n\\t'\nSee: Unofficial bash strict mode\n\n\nOptional arguments\nargs=(\"$@\")\nargs+=(foo)\nargs+=(bar)\necho \"${args[@]}\"\nPut the arguments into an array and then append"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/index.html#also-see",
    "href": "posts/2011/bash-cheatsheet/index.html#also-see",
    "title": "Bash",
    "section": "Also see",
    "text": "Also see\n\nDevhints (devhints.io)\nBash-hackers wiki (bash-hackers.org)\nShell vars (bash-hackers.org)\nLearn bash in y minutes (learnxinyminutes.com)\nBash Guide (mywiki.wooledge.org)\nShellCheck (shellcheck.net)\nshell - Standard Shell (devmanual.gentoo.org)"
  },
  {
    "objectID": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html",
    "href": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html",
    "title": "Tidy Text Mining With R",
    "section": "",
    "text": "Computational Linguistics tasks:"
  },
  {
    "objectID": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html#text-preprocessing",
    "href": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html#text-preprocessing",
    "title": "Tidy Text Mining With R",
    "section": "Text preprocessing",
    "text": "Text preprocessing\n\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %&gt;%\n  ungroup()\n\noriginal_books\n\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   &lt;chr&gt;                   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt;\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# ℹ 73,412 more rows\n\nlibrary(tidytext)\ntidy_books &lt;- original_books %&gt;%\n  unnest_tokens(word, text)\n\ntidy_books\n\n# A tibble: 725,055 × 4\n   book                linenumber chapter word       \n   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 and        \n 3 Sense & Sensibility          1       0 sensibility\n 4 Sense & Sensibility          3       0 by         \n 5 Sense & Sensibility          3       0 jane       \n 6 Sense & Sensibility          3       0 austen     \n 7 Sense & Sensibility          5       0 1811       \n 8 Sense & Sensibility         10       1 chapter    \n 9 Sense & Sensibility         10       1 1          \n10 Sense & Sensibility         13       1 the        \n# ℹ 725,045 more rows\n\n\n\ndata(stop_words)\n\ntidy_books &lt;- tidy_books %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_books\n\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# ℹ 217,599 more rows\n\n\n\nthis removes stop words\n\n\ntidy_books %&gt;%\n  count(word, sort = TRUE) \n\n# A tibble: 13,914 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 miss    1855\n 2 time    1337\n 3 fanny    862\n 4 dear     822\n 5 lady     817\n 6 sir      806\n 7 day      797\n 8 emma     787\n 9 sister   727\n10 house    699\n# ℹ 13,904 more rows\n\n\n\nlibrary(ggplot2)\n\ntidy_books %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(n &gt; 600) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\n#devtools::install_github(\"ropensci/gutenbergr\")\nlibrary(gutenbergr)\n\n#hgwells &lt;- gutenberg_download(c(35, 36,  159, 456, 1047, 3691, 5230, 11870, 12163, 23218, 28218, 35461,39585))\nhgwells &lt;- gutenberg_download(c(35, 36,  159))\n\nDetermining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest\n\n\n`curl` package not installed, falling back to using `url()`\nUsing mirror http://aleph.gutenberg.org\n\n\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/5/159/159.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n\n\n\ntidy_hgwells &lt;- hgwells %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\n\ntidy_hgwells %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 8,146 × 2\n   word         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 time       328\n 2 people     205\n 3 martians   165\n 4 black      152\n 5 night      140\n 6 machine    133\n 7 found      110\n 8 white      108\n 9 road       105\n10 day        102\n# ℹ 8,136 more rows\n\n\n\nbronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767))\n\n\ntidy_bronte &lt;- bronte %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_bronte %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 23,213 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 time    1065\n 2 miss     854\n 3 day      825\n 4 don’t    780\n 5 hand     767\n 6 eyes     714\n 7 night    648\n 8 heart    638\n 9 looked   601\n10 door     591\n# ℹ 23,203 more rows\n\n\n\nlibrary(tidyr)\n\nfrequency &lt;- bind_rows(mutate(tidy_bronte, author = \"Brontë Sisters\"),\n                       mutate(tidy_hgwells, author = \"H.G. Wells\"), \n                       mutate(tidy_books, author = \"Jane Austen\")) %&gt;% \n  mutate(word = str_extract(word, \"[a-z']+\")) %&gt;%\n  count(author, word) %&gt;%\n  group_by(author) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;% \n  select(-n) %&gt;% \n  pivot_wider(names_from = author, values_from = proportion) %&gt;%\n  pivot_longer(`Brontë Sisters`:`H.G. Wells`,\n               names_to = \"author\", values_to = \"proportion\")\n\nfrequency\n\n# A tibble: 54,120 × 4\n   word      `Jane Austen` author          proportion\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1 a            0.00000919 Brontë Sisters  0.0000665 \n 2 a            0.00000919 H.G. Wells      0.0000293 \n 3 aback       NA          Brontë Sisters  0.00000391\n 4 aback       NA          H.G. Wells     NA         \n 5 abaht       NA          Brontë Sisters  0.00000391\n 6 abaht       NA          H.G. Wells     NA         \n 7 abandon     NA          Brontë Sisters  0.0000313 \n 8 abandon     NA          H.G. Wells      0.0000293 \n 9 abandoned    0.00000460 Brontë Sisters  0.0000899 \n10 abandoned    0.00000460 H.G. Wells      0.000234  \n# ℹ 54,110 more rows\n\n\n\nlibrary(scales)\n\n# expect a warning about rows with missing values being removed\nggplot(frequency, aes(x = proportion, y = `Jane Austen`, \n                      color = abs(`Jane Austen` - proportion))) +\n  geom_abline(color = \"gray40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  scale_color_gradient(limits = c(0, 0.001), \n                       low = \"darkslategray4\", high = \"gray75\") +\n  facet_wrap(~author, ncol = 2) +\n  theme(legend.position=\"none\") +\n  labs(y = \"Jane Austen\", x = NULL)\n\nWarning: Removed 39274 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 39276 rows containing missing values (`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n  cor.test(data = frequency[frequency$author == \"Brontë Sisters\",], ~ proportion + `Jane Austen`)\n\n\n    Pearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 110.73, df = 10275, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7286645 0.7462983\nsample estimates:\n      cor \n0.7376071 \n\n\n\ncor.test(data = frequency[frequency$author == \"H.G. Wells\",], \n         ~ proportion + `Jane Austen`)\n\n\n    Pearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 29.497, df = 4567, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3753856 0.4241064\nsample estimates:\n      cor \n0.4000286 \n\n\nkwik and kwok\n\nlibrary(quanteda)\nlibrary(gutenbergr)\n\nausten_works = gutenberg_works(author == \"Austen, Jane\")\nausten = gutenberg_download(austen_works$gutenberg_id)\n\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/3/4/1342/1342.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n\nhead(hgwells)\n\n# A tibble: 6 × 2\n  gutenberg_id text              \n         &lt;int&gt; &lt;chr&gt;             \n1           35 \"The Time Machine\"\n2           35 \"\"                \n3           35 \"An Invention\"    \n4           35 \"\"                \n5           35 \"by H. G. Wells\"  \n6           35 \"\"                \n\n# tidy_hgwells &lt;- hgwells %&gt;%\n#   unnest_tokens(word, text) %&gt;%\n#   anti_join(stop_words)\n\n#head(tidy_hgwells)\n\nthe_corpus &lt;- corpus(austen)\nthe_tokens &lt;- tokens(the_corpus,case_insensitive = TRUE)\n\nWarning: case_insensitive argument is not used.\n\nkwic_table &lt;- kwic(the_tokens,pattern = \"lady\",index = 1:100)\n#kwic_table &lt;- kwic(tokens(tidy_hgwells$word),pattern = \"time\")\n\n#kwic_table &lt;- kwic(tokens(tidy_hgwells$word),pattern = \"machine\",index = 1:400, case_insensitive = TRUE)\nnrow(kwic_table)\n\n[1] 2008\n\nhead(kwic_table,10)\n\nKeyword-in-context with 10 matches.                                                          \n  [text61, 5]                Gloucester, by which | lady |\n  [text98, 7]                deserved by his own. | Lady |\n [text100, 8] youthful infatuation which made her | Lady |\n [text112, 6]            her kindness and advice, | Lady |\n [text118, 4]                   passed away since | Lady |\n [text122, 2]                                That | Lady |\n [text142, 2]                                  To | Lady |\n [text143, 8]              favourite, and friend. | Lady |\n [text169, 1]                                     | Lady |\n [text177, 3]                   immediately after | Lady |\n                                \n ( who died 1800 )              \n Elliot had been an excellent   \n Elliot, had never              \n Elliot mainly relied for the   \n Elliot’s death, and they       \n Russell, of steady age         \n Russell, indeed, she           \n Russell loved them all;        \n Russell’s temples had long been\n Russell out of all the"
  },
  {
    "objectID": "posts/2011/2011-11-29-npl-python/index.html",
    "href": "posts/2011/2011-11-29-npl-python/index.html",
    "title": "Text Mining With Python",
    "section": "",
    "text": "import numpy as np                           # library for scientific computing and matrix \nimport matplotlib.pyplot as plt              # visualization library\nimport string\nimport re\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer    \n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /home/oren/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\n\n1nltk.download('stopwords')\n\n\ndef process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english') \n    \n2    tweet = re.sub(r'\\$\\w*', '', tweet)\n3    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n4    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n5    tweet = re.sub(r'#', '', tweet)\n6    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n    tweets_clean = []\n    for word in tweet_tokens:\n7        if (word not in stopwords_english and\n8                word not in string.punctuation):\n            # tweets_clean.append(word)\n9            stem_word = stemmer.stem(word)\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n\n\n1\n\ndownload the stopwords\n\n2\n\nremove stock market tickers like $GE\n\n3\n\nremove old style retweet text “RT”\n\n4\n\nremove hyperlinks\n\n5\n\nremove hashtags\n\n6\n\ntokenize tweets\n\n7\n\nremove stopwords\n\n8\n\nremove punctuation\n\n9\n\nstemming word\n\n\n\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = defaultdict(int)\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs\n\ndef build_vocab(freqs):\n    vocab = [k for k, v in freq.items() if (v &gt; 1 and k != '\\n')]\n    vocab.sort()\n    return vocab\n\nprocessing unknown tokens\n\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n\n\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nNumber of tweets:  10000\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2011,\n  author = {Bochman, Oren},\n  title = {Text {Mining} {With} {Python}},\n  date = {2011-11-29},\n  url = {https://orenbochman.github.io/blog//posts/2011/2011-11-29-npl-python},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2011. “Text Mining With Python.” November\n29, 2011. https://orenbochman.github.io/blog//posts/2011/2011-11-29-npl-python."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html",
    "href": "notes/dnn/dnn-07/l_07.html",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n:::"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#getting-targets-when-modeling-sequences",
    "href": "notes/dnn/dnn-07/l_07.html#getting-targets-when-modeling-sequences",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Getting targets when modeling sequences",
    "text": "Getting targets when modeling sequences\n\nWhen applying machine learning to sequences, we often want to turn an input sequence into an output sequence that lives in a different domain.\n\nE. g. turn a sequence of sound pressures into a sequence of word identities.\n\nWhen there is no separate target sequence, we can get a teaching signal by trying to predict the next term in the input sequence.\n\nThe target output sequence is the input sequence with an advance of 1 step.\nThis seems much more natural than trying to predict one pixel in an image from the other pixels, or one patch of an image from the rest of the image.\nFor temporal sequences there is a natural order for the predictions.\n\nPredicting the next term in a sequence blurs the distinction between supervised and unsupervised learning.\n\nIt uses methods designed for supervised learning, but it doesn’t require a separate teaching signal."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#memoryless-models-for-sequences",
    "href": "notes/dnn/dnn-07/l_07.html#memoryless-models-for-sequences",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Memoryless models for sequences",
    "text": "Memoryless models for sequences\n\n\n\n\nMemoryless models\n\n\n\n\nAutoregressive models Predict the next term in a sequence from a fixed number of previous terms using “delay taps”.\nFeed-forward neural nets These generalize autoregressive models by using one or more layers of non-linear hidden units. e.g. Bengio’s first language model."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#beyond-memoryless-models",
    "href": "notes/dnn/dnn-07/l_07.html#beyond-memoryless-models",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Beyond memoryless models",
    "text": "Beyond memoryless models\n\nIf we give our generative model some hidden state, and if we give this hidden state its own internal dynamics, we get a much more interesting kind of model.\n\nIt can store information in its hidden state for a long time.\nIf the dynamics is noisy and the way it generates outputs from its hidden state is noisy, we can never know its exact hidden state.\nThe best we can do is to infer a probability distribution over the space of hidden state vectors.\n\nThis inference is only tractable for two types of hidden state model.\n\nThe next three slides are mainly intended for people who already know about these two types of hidden state model. They show how RNNs differ.\nDo not worry if you cannot follow the details."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#linear-dynamical-systems-engineers-love-them",
    "href": "notes/dnn/dnn-07/l_07.html#linear-dynamical-systems-engineers-love-them",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Linear Dynamical Systems (engineers love them!)",
    "text": "Linear Dynamical Systems (engineers love them!)\n\n\n\nlinear dynamic systems\n\n\n\nThese are generative models. They have a realvalued hidden state that cannot be observed directly.\n\nThe hidden state has linear dynamics with Gaussian noise and produces the observations using a linear model with Gaussian noise.\nThere may also be driving inputs.\n\nTo predict the next output (so that we can shoot down the missile) we need to infer the hidden state.\n\nA linearly transformed Gaussian is a Gaussian. So the distribution over the hidden."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#hidden-markov-models-computer-scientists-love-them",
    "href": "notes/dnn/dnn-07/l_07.html#hidden-markov-models-computer-scientists-love-them",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Hidden Markov Models (computer scientists love them!)",
    "text": "Hidden Markov Models (computer scientists love them!)\n\n\n\nHidden Markov Models\n\n\n\nHidden Markov Models have a discrete oneof-N hidden state. Transitions between states are stochastic and controlled by a transition matrix. The outputs produced by a state are stochastic.\n\nWe cannot be sure which state produced a given output. So the state is “hidden”.\nIt is easy to represent a probability distribution across N states with N numbers.\n\nTo predict the next output we need to infer the probability distribution over hidden states.\n\nHMMs have efficient algorithms for"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#a-fundamental-limitation-of-hmms",
    "href": "notes/dnn/dnn-07/l_07.html#a-fundamental-limitation-of-hmms",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "A fundamental limitation of HMMs",
    "text": "A fundamental limitation of HMMs\n\nConsider what happens when a hidden Markov model generates data.\n\nAt each time step it must select one of its hidden states. So with N hidden states it can only remember log(N) bits about what it generated so far.\n\nConsider the information that the first half of an utterance contains about the second half:\n\nThe syntax needs to fit (e.g. number and tense agreement).\nThe semantics needs to fit. The intonation needs to fit.\nThe accent, rate, volume, and vocal tract characteristics must all fit.\n\nAll these aspects combined could be 100 bits of information that the first half of an utterance needs to convey to the second half. 2^100"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#recurrent-neural-networks",
    "href": "notes/dnn/dnn-07/l_07.html#recurrent-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nrnns.png\n\n\n\nRNNs are very powerful, because they combine two properties:\n\nDistributed hidden state that allows them to store a lot of information about the past efficiently.\nNon-linear dynamics that allows them to update their hidden state in complicated ways.\n\nWith enough neurons and time, RNNs can compute anything that can be computed by your computer."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#do-generative-models-need-to-be-stochastic",
    "href": "notes/dnn/dnn-07/l_07.html#do-generative-models-need-to-be-stochastic",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Do generative models need to be stochastic?",
    "text": "Do generative models need to be stochastic?\n\nLinear dynamical systems and hidden Markov models are stochastic models.\n\nBut the posterior probability distribution over their hidden states given the observed data so far is a deterministic function of the data.\n\nRecurrent neural networks are deterministic.\n\nSo think of the hidden state of an RNN as the equivalent of the deterministic probability distribution over hidden states in a linear dynamical system or hidden Markov model. ## Recurrent neural networks\n\nWhat kinds of behaviour can RNNs exhibit?\n\nThey can oscillate. Good for motor control?\nThey can settle to point attractors. Good for retrieving memories?\nThey can behave chaotically. Bad for information processing?\nRNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects.\n\nBut the computational power of RNNs makes them very hard to train.\n\nFor many years we could not exploit the computational power of RNNs despite some heroic efforts (e.g. Tony Robinson’s speech recognizer)."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-equivalence-between-feedforward-nets-and-recurrent-nets",
    "href": "notes/dnn/dnn-07/l_07.html#the-equivalence-between-feedforward-nets-and-recurrent-nets",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The equivalence between feedforward nets and recurrent nets",
    "text": "The equivalence between feedforward nets and recurrent nets\n\n\nAssume that there is a time delay of 1 in using each connection.\nThe recurrent net is just a layered net that keeps reusing the same weights."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#reminder-backpropagation-with-weight-constraints",
    "href": "notes/dnn/dnn-07/l_07.html#reminder-backpropagation-with-weight-constraints",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Reminder: Backpropagation with weight constraints",
    "text": "Reminder: Backpropagation with weight constraints\n\n\nIt is easy to modify the backprop algorithm to incorporate linear constraints between the weights.\nWe compute the gradients as usual, and then modify the gradients so that they satisfy the constraints.\n\nSo if the weights started off satisfying the constraints, they will continue to satisfy them."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#backpropagation-through-time",
    "href": "notes/dnn/dnn-07/l_07.html#backpropagation-through-time",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Backpropagation through time",
    "text": "Backpropagation through time\n\nWe can think of the recurrent net as a layered, feed-forward net with shared weights and then train the feed-forward net with weight constraints.\nWe can also think of this training algorithm in the time domain:\n\nThe forward pass builds up a stack of the activities of all the units at each time step.\nThe backward pass peels activities off the stack to compute the error derivatives at each time step.\nAfter the backward pass we add together the derivatives at all the different times for each weight."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#an-irritating-extra-issue",
    "href": "notes/dnn/dnn-07/l_07.html#an-irritating-extra-issue",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "An irritating extra issue",
    "text": "An irritating extra issue\n\nWe need to specify the initial activity state of all the hidden and output units.\nWe could just fix these initial states to have some default value like 0.5.\nBut it is better to treat the initial states as learned parameters.\nWe learn them in the same way as we learn the weights.\n\nStart off with an initial random guess for the initial states.\nAt the end of each training sequence, backpropagate through time all the way to the initial states to get the gradient of the error function with respect to each initial state.\nAdjust the initial states by following the negative gradient."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#providing-input-to-recurrent-networks",
    "href": "notes/dnn/dnn-07/l_07.html#providing-input-to-recurrent-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Providing input to recurrent networks",
    "text": "Providing input to recurrent networks\n\n\nWe can specify inputs in several ways:\n\nSpecify the initial states of all the units.\nSpecify the initial states of a subset of the units.\nSpecify the states of the same subset of the units at every time step.\n\nThis is the natural way to model most sequential data."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#teaching-signals-for-recurrent-networks",
    "href": "notes/dnn/dnn-07/l_07.html#teaching-signals-for-recurrent-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Teaching signals for recurrent networks",
    "text": "Teaching signals for recurrent networks\n\n\nWe can specify targets in several ways:\n\nSpecify desired final activities of all the units\nSpecify desired activities of all units for the last few steps\n\nGood for learning attractors\nIt is easy to add in extra error derivatives as we backpropagate.\n\nSpecify the desired activity of a subset of the units.\n\nThe other units are input or hidden units."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#a-good-toy-problem-for-a-recurrent-network",
    "href": "notes/dnn/dnn-07/l_07.html#a-good-toy-problem-for-a-recurrent-network",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "A good toy problem for a recurrent network",
    "text": "A good toy problem for a recurrent network"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-algorithm-for-binary-addition",
    "href": "notes/dnn/dnn-07/l_07.html#the-algorithm-for-binary-addition",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The algorithm for binary addition",
    "text": "The algorithm for binary addition\n\n\n\nFinite State Automaton\n\n\nThis is a finite state automaton. It decides what transition to make by looking at the next column. It prints after making the transition. It moves from right to left over the two input numbers."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#a-recurrent-net-for-binary-addition",
    "href": "notes/dnn/dnn-07/l_07.html#a-recurrent-net-for-binary-addition",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "A recurrent net for binary addition",
    "text": "A recurrent net for binary addition\n\n\nThe network has two input units and one output unit.\nIt is given two input digits at each time step.\nThe desired output at each time step is the output for the column that was provided as input two time steps ago.\n\nIt takes one time step to update the hidden units based on the two input digits.\nIt takes another time step for the hidden units to cause the output"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-connectivity-of-the-network",
    "href": "notes/dnn/dnn-07/l_07.html#the-connectivity-of-the-network",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The connectivity of the network",
    "text": "The connectivity of the network\n\nThe 3 hidden units are fully interconnected in both directions. - This allows a hidden activity pattern at one time step to vote for the hidden activity pattern at the next time step. - The input units have feedforward connections that allow then to vote for the next hidden activity pattern."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#what-the-network-learns",
    "href": "notes/dnn/dnn-07/l_07.html#what-the-network-learns",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "What the network learns",
    "text": "What the network learns\n\nIt learns four distinct patterns of activity for the 3 hidden units. These patterns correspond to the nodes in the finite state automaton.\n\nDo not confuse units in a neural network with nodes in a finite state automaton. Nodes are like activity vectors.\nThe automaton is restricted to be in exactly one state at each time. The hidden units are restricted to have exactly one vector of activity at each time.\n\nA recurrent network can emulate a finite state automaton, but it is exponentially more powerful. With N hidden neurons it has 2^N possible binary activity vectors (but only N^2 weights)\n\nThis is important when the input stream has two separate things going on at once.\nA finite state automaton needs to square its number of states.\nAn RNN needs to double its number of units."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-backward-pass-is-linear",
    "href": "notes/dnn/dnn-07/l_07.html#the-backward-pass-is-linear",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The backward pass is linear",
    "text": "The backward pass is linear\n - There is a big difference between the forward and backward passes. - In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding. - The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double. - The forward pass determines the slope of the linear function used for backpropagating through each neuron."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#the-problem-of-exploding-or-vanishing-gradients",
    "href": "notes/dnn/dnn-07/l_07.html#the-problem-of-exploding-or-vanishing-gradients",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "The problem of exploding or vanishing gradients",
    "text": "The problem of exploding or vanishing gradients\n\nWhat happens to the magnitude of the gradients as we backpropagate through many layers?\n\nIf the weights are small, the gradients shrink exponentially.\nIf the weights are big the gradients grow exponentially.\n\nTypical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.\nIn an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.\n\nWe can avoid this by initializing the weights very carefully.\n\nEven with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.\n\nSo RNNs have difficulty dealing with long-range dependencies."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#why-the-back-propagated-gradient-blows-up",
    "href": "notes/dnn/dnn-07/l_07.html#why-the-back-propagated-gradient-blows-up",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Why the back-propagated gradient blows up",
    "text": "Why the back-propagated gradient blows up\n\n\nIf we start a trajectory within an attractor, small changes in where we start make no difference to where we end up.\nBut if we start almost exactly on the boundary, tiny changes can make a huge difference."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#four-effective-ways-to-learn-an-rnn",
    "href": "notes/dnn/dnn-07/l_07.html#four-effective-ways-to-learn-an-rnn",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Four effective ways to learn an RNN",
    "text": "Four effective ways to learn an RNN\n\nLong Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.\nHessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.\n\nThe HF optimizer ( Martens & Sutskever, 2011) is good at this.\n\nEcho State Networks: Initialize the inputàhidden and hiddenàhidden and outputàhidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input.\n\nESNs only need to learn the hiddenàoutput connections.\n\nGood initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#long-short-term-memory-lstm",
    "href": "notes/dnn/dnn-07/l_07.html#long-short-term-memory-lstm",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Long Short Term Memory (LSTM)",
    "text": "Long Short Term Memory (LSTM)\n\nHochreiter & Schmidhuber (1997) solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).\nThey designed a memory cell using logistic and linear units with multiplicative interactions.\nInformation gets into the cell whenever its “write” gate is on.\nThe information stays in the cell so long as its “keep” gate is on.\nInformation can be read from the cell by turning on its “read” gate."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#implementing-a-memory-cell-in-a-neural-network",
    "href": "notes/dnn/dnn-07/l_07.html#implementing-a-memory-cell-in-a-neural-network",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Implementing a memory cell in a neural network",
    "text": "Implementing a memory cell in a neural network\n\nTo preserve information for a long time in the activities of an RNN, we use a circuit that implements an analog memory cell. - A linear unit that has a self-link with a weight of 1 will maintain its state.\n- Information is stored in the cell by activating its write gate. - Information is retrieved by activating the read gate. - We can backpropagate through this circuit because logistics are have nice derivatives."
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#backpropagation-through-a-memory-cell",
    "href": "notes/dnn/dnn-07/l_07.html#backpropagation-through-a-memory-cell",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Backpropagation through a memory cell",
    "text": "Backpropagation through a memory cell"
  },
  {
    "objectID": "notes/dnn/dnn-07/l_07.html#reading-cursive-handwriting",
    "href": "notes/dnn/dnn-07/l_07.html#reading-cursive-handwriting",
    "title": "Deep Neural Networks - Notes for Lesson 7",
    "section": "Reading cursive handwriting",
    "text": "Reading cursive handwriting\n\nThis is a natural task for an RNN.\nThe input is a sequence of (x,y,p) coordinates of the tip of the pen, where p indicates whether the pen is up or down.\nThe output is a sequence of characters.\nGraves & Schmidhuber (2009) showed that RNNs with LSTM are currently the best systems for reading cursive writing.\n\nThey used a sequence of small images as input rather than pen coordinates. A demonstration of online handwriting recognition by an RNN with Long Short Term Memory (from Alex Graves)\n\nThe movie that follows shows several different things:\nRow 1: This shows when the characters are recognized.\n\nIt never revises its output so difficult decisions are more delayed.\n\nRow 2: This shows the states of a subset of the memory cells.\n\nNotice how they get reset when it recognizes a character.\n\nRow 3: This shows the writing. The net sees the x and y coordinates.\n\nOptical input actually works a bit better than pen coordinates.\n\nRow 4: This shows the gradient backpropagated all the way to the x and y inputs from the currently most active character.\n\nThis lets you see which bits of the data are influencing the decision.\n\n\n\n\n\nMemoryless models\nlinear dynamic systems\nHidden Markov Models\nrnns.png\nFinite State Automaton"
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html",
    "href": "notes/dnn/dnn-09/l_09.html",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#reminder-overfitting",
    "href": "notes/dnn/dnn-09/l_09.html#reminder-overfitting",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Reminder: Overfitting",
    "text": "Reminder: Overfitting\n\nThe training data contains information about the regularities in the mapping from input to output. But it also contains sampling error.\n\nThere will be accidental regularities just because of the particular training cases that were chosen.\n\nWhen we fit the model, it cannot tell which regularities are real and which are caused by sampling error.\n\nSo it fits both kinds of regularity. If the model is very flexible it can model the sampling error really well."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#preventing-overfitting",
    "href": "notes/dnn/dnn-09/l_09.html#preventing-overfitting",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Preventing overfitting",
    "text": "Preventing overfitting\nFour approaches to reduce overfitting due to too many parameters to training rows:\n\nGet more data - the best option.\nUse a model that has the right capacity:\n\nenough to fit the true regularities.\nnot enough to fit spurious data.1\n\nAverage models.\n\nEnsambling use models with different forms.\nBagging train the model on different subsets the training data. 4.: Bayesian:\nuse a single NN architecture but average the predictions made by many weight vectors\n\nGetting more data via augmentation (increases signal to noise)\n\nconsider normalization\nSample-wise\n\nFeature wise pixel standardization\nPCA whitening - reduces dimension + whiting\nZCA the idea is to reducing effect of correlation in adjacent pixels by normalizing feature variance and reducing correlation at features. (does not reduce dimensions of the data)\n\n\n1 a bit cheeky consider the massive capacity of most NN\n\n\n\n\n\n\ntransform\nimage\n\n\n\n\nOriginal\n\n\n\nFeature Standardization\n\n\n\nZCA whitening\n\n\n\nRandom Rotations\n\n\n\nRandom shifts\n\n\n\nRandom Flips\n\n\n\nRandom affine transforms\n\n\n\nContrast Stretching\n\n\n\nHistogram Equalization\n\n\n\nAdaptive Histogram Equalization\n\n\n\nCLAHE contrast stretching + adaptive histogram equalization = Contrast limited adaptive histogram equalization\n\n\n\n\nconsider augmentation. random crop/rotation/shear/mirroring/flip scaling blocking out rectangles elastic deformation mesh (used in Unet) contrast stretching + adaptive histogram equalization = Contrast limited adaptive histogram equalization (CLAHE) ZCA whitening transform\nStandardized Feature MNIST Images ZCA Whitening MNIST Images random affine transforms\nContrast Stretching Histogram Equalization Adaptive Histogram Equalization \ncode: image-augmentation-deep-learning-keras (on nminst) data augmentation with elastic deformations"
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#how-to-choose-meta-parameters-that-control-capacity",
    "href": "notes/dnn/dnn-09/l_09.html#how-to-choose-meta-parameters-that-control-capacity",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "How to choose meta parameters that control capacity2",
    "text": "How to choose meta parameters that control capacity2\n\nThe wrong method is to try lots of alternatives and see which gives the best performance on the test set. – This is easy to do, but it gives a false impression of how well the method works. – The settings that work best on the test set are unlikely to work as well on a new test set drawn from the same distribution.\nAn extreme example: Suppose the test set has random answers that do not depend on the input.\nThe best architecture will do better than chance on the test set.\nBut it cannot be expected to do better than chance on a new test set."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#cross-validation-a-better-way-to-choose-meta-parameters",
    "href": "notes/dnn/dnn-09/l_09.html#cross-validation-a-better-way-to-choose-meta-parameters",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Cross-validation: A better way to choose meta parameters",
    "text": "Cross-validation: A better way to choose meta parameters\n\nDivide the total dataset into three subsets:\n\nTraining data is used for learning the parameters of the model.\nValidation data is not used for learning but is used for deciding what settings of the meta parameters work best.\nTest data is used to get a final, unbiased estimate of how well the network works. We expect this estimate to be worse than on the validation data.\n\nWe could divide the total dataset into one final test set and N other subsets and train on all but one of those subsets to get N different estimates of the validation error rate.\n\nThis is called N-fold cross-validation.\nThe N estimates are not independent."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#preventing-overfitting-by-early-stopping",
    "href": "notes/dnn/dnn-09/l_09.html#preventing-overfitting-by-early-stopping",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Preventing overfitting by early stopping",
    "text": "Preventing overfitting by early stopping\n\nIf we have lots of data and a big model, its very expensive to keep re-training it with different sized penalties on the weights.\nIt is much cheaper to start with very small weights and let them grow until the performance on the validation set starts getting worse.\n\nBut it can be hard to decide when performance is getting worse.\n\nThe capacity of the model is limited because the weights have not had time to grow big."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#why-early-stopping-works",
    "href": "notes/dnn/dnn-09/l_09.html#why-early-stopping-works",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Why early stopping works",
    "text": "Why early stopping works\n\nWhen the weights are very small, every hidden unit is in its linear range.\n\nSo a net with a large layer of hidden units is linear.\nIt has no more capacity than a linear net in which the inputs are directly connected to the outputs!\n\nAs the weights grow, the hidden units start using their non-linear ranges so the capacity grows."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#limiting-the-size-of-the-weights",
    "href": "notes/dnn/dnn-09/l_09.html#limiting-the-size-of-the-weights",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Limiting the size of the weights",
    "text": "Limiting the size of the weights\n\n\n\n\n\nlimiting weights\n\n\nThe standard L2 weight penalty involves adding an extra term to the cost function that penalizes the squared weights. - This keeps the weights small unless they have big error derivatives.\n\nC = E + \\frac{\\lambda}{2}\\sum_i w_i^2\n\n\n\\frac{\\partial C}{\\partial w_i} = \\frac{\\partial E}{\\partial w_i} + \\lambda w_i\n\nwhen\n\n\\frac{\\partial C}{\\partial w_i} = 0 \\implies w_i=-\\frac{1}{\\lambda} \\frac{\\partial E}{\\partial w_i}\n There is some math in this video. It’s not complicated math. You should make sure to understand it."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#lecture-9c-using-noise-as-a-regularizer",
    "href": "notes/dnn/dnn-09/l_09.html#lecture-9c-using-noise-as-a-regularizer",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Lecture 9c: Using noise as a regularizer",
    "text": "Lecture 9c: Using noise as a regularizer\nadding noise to the input can have a regularizing effect. Think lets add noise to a picture - it drowns out most of the small features say blurring them. But for large items - we are trying to learn - they look mostly the same. The advantage is that adding noise is easy. Anyhow - this is more of a buildup of an abstract idea that will later be interpreted using full Bayesian learning. L2 weight-decay via noisy inp Suppose we add Gaussian noise to the inputs. — The variance of the noise is amplified by the squared weight before going into the next layer. In a simple net with a linear output unit directly connected to the inputs, the amplified noise gets added to the output. This makes an additive contribution to the This slide serves to show that noise is not a crazy idea. The penalty strength can be thought of as being\nσ2i , or twice that (to compensate for the 1/2 in the weight decay cost function), but that detail is not important here. Second slide (the math slide)\nThe reason why the middle term is zero is that all of the epsilons have mean zero. You may notice that the result is not exactly like the L2 penalty of the previous video: the factor 1/2 is missing. Or equivalently, the strength of the penalty is not sigma i squared, but twice that. The main point, however, is that this noise is equivalent to an L2 penalty. Jargon: overfitting, underfitting, generalization, and regularization Overfitting can be thought of as the model being too confident about what the data is like: more confident than would be justified, given the limited amount of training data that it was trained on. If an alien from outer space would take one look at a street full of cars (each car being a training case), and it so happens that there were only two Volkswagens there, one dark red and one dark blue, then the alien might conclude “all Volkswagens on Earth are of dark colours.” That would be overfitting. If, on the other hand, the alien would be so reluctant to draw conclusions that he even fails to conclude that cars typically have four wheels, then that would be underfitting. We seek the middle way, where we don’t draw more than a few unjustified conclusions, but we do draw most of the conclusions that really are justified. Regularization means forcing the model to draw fewer conclusions, thus limiting overfitting. If we overdo it, we end up underfitting. Jargon: “generalization” typically means the successful avoidance of both overfitting and underfitting. Since overfitting is harder to avoid, “generalization” often simply means the absence of (severe) overfitting. The “accidental regularities” that training data contains are often complicated patterns. However, NNs can learn complicated patterns quite well. Jargon: “capacity” is learning capacity. It’s the amount of potential (artificial) brain power in a model, and it mostly depends on the number of learned parameters (weights & biases)."
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#lecture-9d-introduction-to-the-full-bayesian-approach",
    "href": "notes/dnn/dnn-09/l_09.html#lecture-9d-introduction-to-the-full-bayesian-approach",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Lecture 9d: Introduction to the full Bayesian approach",
    "text": "Lecture 9d: Introduction to the full Bayesian approach\nThe full Bayesian approach could provide an alternative to using SGD. However with the exception of very simple models it is usually computationally intractable as it requires finding the prior distribution for all the parameters.\nWe can start with an prior P(params) - and adjust it given each training item.\nGiven some data we would have to calculate its likelihood i.e. p(data)\nBut to do this we would need to see how it effects all parameter settings - this is the real issue as for 10 settings for 100 nodes we would need to test 10^100 weight combinations…\nthis is an outline of the Bayesian approach.\nthere is a prior distribution over parameters, there is data, say the training data and we can calculate its likelihood and combine it with the prior to get a posterior.\nWith sufficient Bayesian updating will in the limit beat an uninformative prior.\nbut he does not go into how much data. The Bayesian framework The Bayesian framework assumes that we distribution for everything. — The prior may be very vague. — When we see some data, we combine our with a likelihood term to get a posterior distr — The likelihood term takes into account how observed data is given the parameters of th\na 100 coin tosses motivates the frequentist approach which uses the (ML) maximal likelihood estimate of the probability.\nNext calculates the ml is 0.53 by differentiating and setting the derivative equal to zero. Next asks what if we have only one coin toss. which is a kin to asking “what if the experiment is too small and there are unobserved outcomes?” in which case we cannot account for their likelihood in a ML estimate. A coin tossing example Suppose we know nothing about coins excep tossing event produces a head with some unl probability p and a tail with probability I-p.  — Our model of a coin has one parameter, p Suppose we observe 100 tosses and there al What is p? here D is the data and W is a set of weights.\nBayes Theorem joint probability prior probability of weight vector W probabilit data give p(W) p(DlW) ID)\nHowever, it may be possible to approximate a prior. The terms “prior”, “likelihood term”, and “posterior” are explained in a more mathematical way at the end of the video, so if you’re confused, just keep in mind that a mathematical explanation follows. For the coin example, try not to get confused about the difference between “p” (the probability of seeing heads) and “P” (the abbreviation for “probability”). Jargon: “maximum likelihood” means maximizing the likelihood term, without regard to any prior that there may be. At 8:22 there’s a slightly incorrect statement in the explanation, though not in the slide. The mean is not at .53 (although it is very close to that). What’s really at .53 is the mode, a.k.a. the peak, a.k.a. the most likely value. The Bayesian approach is to average the network’s predictions, at test time, where “average” means that we use network parameters according to the posterior distribution over parameter settings given the training data. Essentially, we’re averaging the predictions from many predictors: each possible parameter setting is a predictor, and the weight for that weighted average is the posterior probability of that parameter setting. “It’s helpful to know that whenever you see a squared error being minimized, you can make a probabilistic interpretation of what’s going on, and in that probabilistic interpretation, you’ll be maximizing the  log probability under a Gausian.” So the proper Bayesian approach, is to find the full posterior distribution over all possible weight vectors. If there’s more than a handful of weights, that’s hopelessly difficult when you have  a non-linear net. Bayesians have a lot of ways of  approximating this distribution, often using Monte Carlo methods.  But for the time being, let’s try and do something simpler.  Let’s just try to find the most probable weight vector.  So the single setting of the weights that’s most probable given the prior knowledge we have and given the data. So what we’re going to try and do is find  an optimal value of W by starting with some random weight vector, and then  adjusting it in the direction that improves the probability of that weight  factor given the data. It will only be a local optimum. The Bayesian interpretation of weight -log I D) = —logp(D I W) 1 (yc -tc)2 1)\nassuming that the model makes a Gaussian prediction — log p(W) 20w t assuming a for the weig"
  },
  {
    "objectID": "notes/dnn/dnn-09/l_09.html#supervised-maximum-likelihood-learning",
    "href": "notes/dnn/dnn-09/l_09.html#supervised-maximum-likelihood-learning",
    "title": "Deep Neural Networks - Notes for Lesson 9",
    "section": "Supervised Maximum Likelihood Learning",
    "text": "Supervised Maximum Likelihood Learning\nIn this video, we use Bayesian thinking (which is widely accepted as very reasonable) to justify weight decay (which may sound like an arbitrary hack). Maximum A Posteriori (MAP) learning means looking for that setting of the network parameters that has greatest posterior probability given the data. As such it’s somewhat different from the simpler “Maximum Likelihood” learning, where we look for the setting of the parameters that has the greatest likelihood term: there, we don’t have a prior over parameter settings, so it’s not very Bayesian at all. Slide 1 introduces Maximum Likelihood learning. Try to understand well what that has to do with the Bayesian “likelihood term”, before going on to the next slide. The reason why we use Gaussians for our likelihood and prior is that that makes the math simple, and fortunately it’s not an insane choice to make. However, it is somewhat arbitrary. 10:15: Don’t worry about the absence of the factor 1/2 in the weight decay strength. It doesn’t change the story in any essential way.\n\n\n\nlimiting weights"
  },
  {
    "objectID": "notes/dnn/dnn-10/r2.html",
    "href": "notes/dnn/dnn-10/r2.html",
    "title": "Deep Neural Networks — Readings II for Lesson 10",
    "section": "",
    "text": "In the paper (Hinton et al. 2012) the authors discuss using dropout as a regularization mechanism to reduce overfitting in deep neural networks.\n\nHinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.” https://doi.org/10.48550/arXiv.1207.0580.\nUnable to display PDF file. Download instead.\n\n\nWhen a large feed forward neural network is trained on a small training set, it typically performs poorly on held-out test data. This “overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorically large variety of internal contexts in which it must operate. Random “dropout” gives big improvements on many benchmark tasks and sets new records for speech and object recognition.\n\n\n\nThis is a paper about using dropout to as a regularization tool, to prevent nodes co-adaptation within parts of the neural network. As I see it if the network has sufficient capacity it will memorize all the training data and then will perform rather poorly on the holdout data and in real world inference. What happen during overfitting is that the network learn both the signal and the noise. In general the law of number works in our favour and the network and since the signal is stronger than the noise we do not initially overfit. However, as the remaining unlearned signal becomes more rare it becomes harder for the model to separate if from the noise. Rare signals will tend to appear less often than certain common noise patterns. Most regularization techniques try to boost the signal. In this case by effectively reducing the capacity and creating, and making the network overall less cohesive. Dropout effectively reduces the network’s capacity during training. It forces the network to create redundent components which relay less on other units. Another regularization is also used: instead of using L2 on the weights vector, L2 norm penalty is used on each weight. If the weight updates violates the constraints, they are normalized. This is motivated by a wish to start with a high learning rate which would otherwise lead to very large weights. This should intuitively allow the net to initially benefit from the stronger signal while reserving more opportunity for later epochs to leave their mark.\nAt trainng time the full network is used nut the Tha authors claim that dropout is equivilent to avareging many random networks. A point they fail to mention is that\n“Dropout is considerably simpler to implement than Bayesian model averaging which weights each model by its posterior probability given the training data. For complicated model classes, like feed forward neural networks, Bayesian methods typically use a Markov chain Monte Carlo method to sample models from the posterior distribution (14). By contrast, dropout with a probability of 0.5 assumes that all the models will eventually be given equal importance in the combination but the learning of the shared weights takes this into account.”\nMy thoughts are that we should be able to do better than this version of dropout.\n\nShortcoming:\nDropout on units can render the net very poor.\nDrop out slows training down - since we don’t update half the units and probably a large number of the weights.\nFor different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures.\nWe should track dropout related stats to better understand the confidence of the model.\nA second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout?\n\n\nStart with some combinations \\binom k n of the weights. where k = | {training\\; set}|*{minibatch\\_size}. We use the same dropout for each mini-batch, then switch.\nEach epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ?\nWhen we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net.\nIdeally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step.\nWe could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism.\nWe should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "notes/dnn/dnn-10/r2.html#reading-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors",
    "href": "notes/dnn/dnn-10/r2.html#reading-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors",
    "title": "Deep Neural Networks — Readings II for Lesson 10",
    "section": "",
    "text": "In the paper (Hinton et al. 2012) the authors discuss using dropout as a regularization mechanism to reduce overfitting in deep neural networks.\n\nHinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.” https://doi.org/10.48550/arXiv.1207.0580.\nUnable to display PDF file. Download instead.\n\n\nWhen a large feed forward neural network is trained on a small training set, it typically performs poorly on held-out test data. This “overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorically large variety of internal contexts in which it must operate. Random “dropout” gives big improvements on many benchmark tasks and sets new records for speech and object recognition.\n\n\n\nThis is a paper about using dropout to as a regularization tool, to prevent nodes co-adaptation within parts of the neural network. As I see it if the network has sufficient capacity it will memorize all the training data and then will perform rather poorly on the holdout data and in real world inference. What happen during overfitting is that the network learn both the signal and the noise. In general the law of number works in our favour and the network and since the signal is stronger than the noise we do not initially overfit. However, as the remaining unlearned signal becomes more rare it becomes harder for the model to separate if from the noise. Rare signals will tend to appear less often than certain common noise patterns. Most regularization techniques try to boost the signal. In this case by effectively reducing the capacity and creating, and making the network overall less cohesive. Dropout effectively reduces the network’s capacity during training. It forces the network to create redundent components which relay less on other units. Another regularization is also used: instead of using L2 on the weights vector, L2 norm penalty is used on each weight. If the weight updates violates the constraints, they are normalized. This is motivated by a wish to start with a high learning rate which would otherwise lead to very large weights. This should intuitively allow the net to initially benefit from the stronger signal while reserving more opportunity for later epochs to leave their mark.\nAt trainng time the full network is used nut the Tha authors claim that dropout is equivilent to avareging many random networks. A point they fail to mention is that\n“Dropout is considerably simpler to implement than Bayesian model averaging which weights each model by its posterior probability given the training data. For complicated model classes, like feed forward neural networks, Bayesian methods typically use a Markov chain Monte Carlo method to sample models from the posterior distribution (14). By contrast, dropout with a probability of 0.5 assumes that all the models will eventually be given equal importance in the combination but the learning of the shared weights takes this into account.”\nMy thoughts are that we should be able to do better than this version of dropout.\n\nShortcoming:\nDropout on units can render the net very poor.\nDrop out slows training down - since we don’t update half the units and probably a large number of the weights.\nFor different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures.\nWe should track dropout related stats to better understand the confidence of the model.\nA second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout?\n\n\nStart with some combinations \\binom k n of the weights. where k = | {training\\; set}|*{minibatch\\_size}. We use the same dropout for each mini-batch, then switch.\nEach epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ?\nWhen we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net.\nIdeally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step.\nWe could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism.\nWe should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "notes/dnn/dnn-10/r2.html#further-readind",
    "href": "notes/dnn/dnn-10/r2.html#further-readind",
    "title": "Deep Neural Networks — Readings II for Lesson 10",
    "section": "Further Readind",
    "text": "Further Readind\nc.f. Gal and Ghahramani (2016)\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In Proceedings of the 33rd International Conference on Machine Learning, edited by Maria Florina Balcan and Kilian Q. Weinberger, 48:1050–59. Proceedings of Machine Learning Research. New York, New York, USA: PMLR. https://proceedings.mlr.press/v48/gal16.html."
  },
  {
    "objectID": "notes/dnn/dnn-10/r2.html#my-wrap-up",
    "href": "notes/dnn/dnn-10/r2.html#my-wrap-up",
    "title": "Deep Neural Networks — Readings II for Lesson 10",
    "section": "My wrap up 🎬",
    "text": "My wrap up 🎬\n\nGame theoretic framework have to formalize cooperative and competitive aspects of learning and how these might influence network architectures.\n\nc.f. David Balduzzi (2015) Semantics, Representations and Grammars for Deep Learning.\n\n\nThere has been lots of progress in training single models for multiple tasks. - c.f. Lukasz Kaiser et all. (2017) One Model To Learn Them All. - covered in this video: One Neural network learns EVERYTHING?! which uses mixture of expert layer which come from later work: Noam Shazeer, Azalia Mirhoseini,Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean (2017) - Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer in which mixture of experts is used within large neural networks\n\nReferences"
  },
  {
    "objectID": "notes/dnn/dnn-12/l_12.html",
    "href": "notes/dnn/dnn-12/l_12.html",
    "title": "Deep Neural Networks - Notes for Lesson 12",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n\n\n\n\nLecture 12a: Boltzmann machine learning\nClarification: The energy is linear in the weights, but quadratic in the states. What matters for this argument is just that it’s linear in the weights.\n\n\nLecture 12b: More efficient ways to get the statistics\n\n\nLecture 12c: Restricted Boltmann Machines\nHere, a “particle” is a configuration. These particles are moving around the configuration space, which, when considered with the energy function, is our mountain landscape.\nIt’s called a reconstruction because it’s based on the visible vector at t=0 (via the hidden vector at t=0). It will, typically, be quite similar to the visible vector at t=0.\nA “fantasy” configuration is one drawn from the model distribution by running a Markov Chain for a long time.\nThe word “fantasy” is chosen as part of the analogy of a Boltzmann Machine vs. a brain that learned several memories.\n\n\nLecture 12d: An example of RBM learning\nThis is not an easy video. Prerequisite is a rather extensive understanding of what an RBM does. Be sure to understand video 12c quite well before proceeding with 12d.\nPrerequisite for this video is that you understand the “reconstruction” concept of the previous video.\nThe first slide is about an RBM, but uses much of the same phrases that we previously used to talk about deterministic feedforward networks.\nThe hidden units are described as feature detectors, or “features” for short.\nThe weights are shown as arrows, even though a Boltzmann Machine has undirected connections.\nThat’s because calculating the probability of the hidden units turning on, given the state of the visible units, is exactly like calculating the real-valued state of a logistic hidden unit, in a deterministic feedforward network.\nHowever, in a Boltzmann Machine, that number is then treated as a probability of turning on, and an actual state of 1 or 0 is chosen, randomly, based on that probability. We’ll make further use of that similarity next week.\n2:30. That procedure for changing energies, that was just explained, is a repeat (in different words) of the Contrastive Divergence story of the previous video. If you didn’t fully realize that, then review.\n\n\nLecture 12e: RBMs for collaborative filtering\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} for {Lesson} 12},\n  date = {2017-11-01},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-12/l_12.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes for Lesson\n12.” November 1, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-12/l_12.html."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html",
    "href": "notes/dnn/dnn-06/l06e.html",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#rprop-using-only-the-sign-of-the-gradient",
    "href": "notes/dnn/dnn-06/l06e.html#rprop-using-only-the-sign-of-the-gradient",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "rprop: Using only the sign of the gradient",
    "text": "rprop: Using only the sign of the gradient\n\nThe magnitude of the gradient can be very different for different weights and can change during learning.\n\nThis makes it hard to choose a single global learning rate.\n\nFor full batch learning, we can deal with this variation by only using the sign of the gradient.\n\nThe weight updates are all of the same magnitude.\nThis escapes from plateaus with tiny gradients quickly.\n\n\nrprop: This combines the idea of only using the sign of the gradient with the idea of adapting the step size separately for each weight.\n\nIncrease the step size for a weight multiplicatively (e.g. times 1.2) if the signs of its last two gradients agree.\nOtherwise decrease the step size multiplicatively (e.g. times 0.5).\nLimit the step sizes to be less than 50 and more than a millionth (Mike Shuster’s advice)."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#why-rprop-does-not-work-with-mini-batches",
    "href": "notes/dnn/dnn-06/l06e.html#why-rprop-does-not-work-with-mini-batches",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "Why rprop does not work with mini-batches",
    "text": "Why rprop does not work with mini-batches\n\nThe idea behind stochastic gradient descent is that when the learning rate is small, it averages the gradients over successive minibatches.\n\nConsider a weight that gets a gradient of +0.1 on nine minibatches and a gradient of -0.9 on the tenth mini-batch.\n\nWe want this weight to stay roughly where it is.\n\nrprop would increment the weight nine times and decrement it once by about the same amount (assuming any adaptation of the step sizes is small on this time-scale).\n\nSo the weight would grow a lot.\n\nIs there a way to combine:\n\nThe robustness of rprop.\nThe efficiency of mini-batches.\nThe effective averaging of gradients over mini-batches."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#rmsprop-a-mini-batch-version-of-rprop",
    "href": "notes/dnn/dnn-06/l06e.html#rmsprop-a-mini-batch-version-of-rprop",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "rmsprop: A mini-batch version of rprop",
    "text": "rmsprop: A mini-batch version of rprop\n\nrprop is equivalent to using the gradient but also dividing by the size of the gradient.\n\nThe problem with mini-batch rprop is that we divide by a different number for each mini-batch. So why not force the number we divide by to be very similar for adjacent mini-batches?\n\n\nrmsprop Keep a moving average of the squared gradient for each weight\n\n\nMeanSquare(w,t) = 0.9 \\times MeanSquare(w, t−1) \\times 0.1 \\bigg(\\frac{∂E}{∂w}(t)\\bigg)^2\n - Dividing the gradient by \\sqrt{MeanSquare(w, t)} makes the learning work much better (Tijmen Tieleman, unpublished)."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#further-developments-of-rmsprop",
    "href": "notes/dnn/dnn-06/l06e.html#further-developments-of-rmsprop",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "Further developments of rmsprop",
    "text": "Further developments of rmsprop\n\nCombining rmsprop with standard momentum🚀\n\nMomentum does not help as much as it normally does. Needs more investigation.\n\nCombining rmsprop with Nesterov momentum🚀 (Sutskever 2012) [elshamy2023improving]\n\nIt works best if the RMS of the recent gradients is used to divide the correction rather than the jump in the direction of accumulated corrections.\n\nCombining rmsprop with adaptive learning rates for each connection\n\nNeeds more investigation.\n\nOther methods related to rmsprop\n\nYann LeCun’s group has a fancy version in (Schaul, Zhang, and LeCun 2013)\n\n\n\nSchaul, Tom, Sixin Zhang, and Yann LeCun. 2013. “No More Pesky Learning Rates.” https://arxiv.org/abs/1206.1106."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06e.html#summary-of-learning-methods-for-neural-networks",
    "href": "notes/dnn/dnn-06/l06e.html#summary-of-learning-methods-for-neural-networks",
    "title": "Deep Neural Networks - Notes for lecture 6e",
    "section": "Summary of learning methods for neural networks",
    "text": "Summary of learning methods for neural networks\n\nFor small datasets (e.g. 10,000 cases) or bigger datasets without much redundancy, use a full-batch method.\n\nConjugate gradient, LBFGS …\nadaptive learning rates, rprop …\n\nFor big, redundant datasets use minibatches.\n\nTry gradient descent with momentum. 🚀\nTry rmsprop (with momentum🚀 ?)\nTry LeCun’s latest recipe.\n\nWhy there is no simple recipe:\n\nNeural nets differ a lot:\n\nVery deep nets (especially ones with narrow bottlenecks).\nRecurrent nets.\n\nWide shallow nets. -Tasks differ a lot:\nSome require very accurate weights, some don’t.\nSome have many very rare cases (e.g. words).\n\n\n\n\n\n\nsome learning rate algorithms"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html",
    "href": "notes/dnn/dnn-06/l06c.html",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#the-intuition-behind-the-momentum-method",
    "href": "notes/dnn/dnn-06/l06c.html#the-intuition-behind-the-momentum-method",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "The intuition behind the momentum method",
    "text": "The intuition behind the momentum method\n\n\n\n\n\nMomentum intution\n\n\n\nImagine a ball on the error surface. The location of the ball in the horizontal plane represents the weight vector.\n\nThe ball starts off by following the gradient, but once it has velocity, it no longer does steepest descent.\n\nIts momentum makes it keep going in the previous direction.\n\nIt damps oscillations in directions of high curvature by combining gradients with opposite signs.\nIt builds up speed in directions with a gentle but consistent gradient."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#the-equations-of-the-momentum-method",
    "href": "notes/dnn/dnn-06/l06c.html#the-equations-of-the-momentum-method",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "The equations of the momentum method",
    "text": "The equations of the momentum method\nThe effect of the gradient is to increment the previous velocity. The velocity also decays by \\alpha which is slightly less then 1. \nv(t) =α v(t −1)−ε \\frac{∂E}{∂w}(t)\n\nThe weight change is equal to the current velocity.\n\n\\begin{align}\nΔw(t) &= v(t) \\\\\n      &= α v(t −1)−ε \\frac{∂E}{∂w}(t)  \\\\\n      &= α Δw(t −1)−ε \\frac{∂E}{∂w}(t)\n\\end{align}\n\nThe weight change can be expressed in terms of the previous weight change and the current gradient."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#the-behavior-of-the-momentum-method",
    "href": "notes/dnn/dnn-06/l06c.html#the-behavior-of-the-momentum-method",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "The behavior of the momentum method",
    "text": "The behavior of the momentum method\n\nIf the error surface is a tilted plane, the ball reaches a terminal velocity.\n\nIf the momentum is close to 1, this is much faster than simple gradient descent.\n\nAt the beginning of learning there may be very large gradients.\n\nSo it pays to use a small momentum (e.g. 0.5).\nOnce the large gradients have disappeared and the weights are stuck in a ravine the momentum can be smoothly raised to its final value (e.g. 0.9 or even 0.99)\n\nThis allows us to learn at a rate that would cause divergent oscillations without the momentum.\n\n\nv(∞) = \\frac{1}{1−α} \\biggr( −ε \\frac{∂E}{∂w} \\biggr)"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#a-better-type-of-momentum-nesterov-1983",
    "href": "notes/dnn/dnn-06/l06c.html#a-better-type-of-momentum-nesterov-1983",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "A better type of momentum (Nesterov 1983)",
    "text": "A better type of momentum (Nesterov 1983)\n\nThe standard momentum method first computes the gradient at the current location and then takes a big jump in the direction of the updated accumulated gradient.\nIlya Sutskever (2012 unpublished) suggested a new form of momentum that often works better.\n\nInspired by the Nesterov method for optimizing convex functions.\n\n\nFirst make a big jump in the direction of the previous accumulated gradient.\nThen measure the gradient where you end up and make a correction.\n\nIts better to correct a mistake after you have made it!"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06c.html#a-picture-of-the-nesterov-method",
    "href": "notes/dnn/dnn-06/l06c.html#a-picture-of-the-nesterov-method",
    "title": "Deep Neural Networks - Notes for lecture 6c",
    "section": "A picture of the Nesterov method",
    "text": "A picture of the Nesterov method\n\n\n\nbrown vector = jump,\nred vector = correction,\ngreen vector = accumulated gradient blue vectors = standard momentum\n\nFirst make a big jump in the direction of the previous accumulated gradient.\nThen measure the gradient where you end up and make a correction.\n\n\n\n\nMomentum intution"
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html",
    "href": "notes/dnn/dnn-06/l_06.html",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6a-overview-of-mini-batch-gradient-descent",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6a-overview-of-mini-batch-gradient-descent",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6a: Overview of mini-batch gradient descent",
    "text": "Lecture 6a: Overview of mini-batch gradient descent\nNow we’re going to discuss numerical optimization: how best to adjust the weights and biases, using the gradient information from the backprop algorithm. This video elaborates on the most standard neural net optimization algorithm (mini-batch gradient descent), which we’ve seen before. We’re elaborating on some issues introduced in video 3e."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6b-a-bag-of-tricks-for-mini-batch-gradient-descent",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6b-a-bag-of-tricks-for-mini-batch-gradient-descent",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6b: A bag of tricks for mini-batch gradient descent",
    "text": "Lecture 6b: A bag of tricks for mini-batch gradient descent\ninitializing weights: we must not initialize units with equal weights as they can never become different. we cannot use zero as it will remain zero we want to avoid explosion and vanishing weights fan in - the number of inputs\nPart 1 is about transforming the data to make learning easier. At 1:10, there’s a comment about random weights and scaling. The “it” in that comment is the average size of the input to the unit. At 1:15, the “good principle”: what he means is INVERSELY proportional. At 4:38, Geoff says that the hyperbolic tangent is twice the logistic minus one. This is not true, but it’s almost true. As an exercise, find out’s missing in that equation. At 5:08, Geoffrey suggests that with a hyperbolic tangent unit, it’s more difficult to sweep things under the rug than with a logistic unit. I don’t understand his comment, so if you don’t either, don’t worry. This comment is not essential in this course: we’re never using hyperbolic tangents in this course. Part 2 is about changing the stochastic gradient descent algorithm in sophisticated ways. We’ll look into these four methods in more detail, later on in the course. Jargon: “stochastic gradient descent” is mini-batch or online gradient descent. The term emphasizes that it’s not full-batch gradient descent. “stochastic” means that it involves randomness. However, this algorithm typically does not involve randomness. However, it would be truly stochastic if we would randomly pick 100 training cases from the entire training set, every time we need the next mini-batch. We call traditional “stochastic gradient descent” stochastic because it is, in effect, very similar to that truly stochastic version. Jargon: a “running average” is a weighted average over the recent past, where the most recent past is weighted most heavily."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6c-the-momentum-method",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6c-the-momentum-method",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6c: The momentum method",
    "text": "Lecture 6c: The momentum method\nDrill down into momentum mentioned before.\nThe biggest challenge in this video is to think of the error surface as a mountain landscape. If you can do that, and you understand the analogy well, this video will be easy. You may have to go back to video 3b, which introduces the error surface. Important concepts in this analogy: “ravine”, “a low point on the surface”, “oscillations”, “reaching a low altitude”, “rolling ball”, “velocity”. All of those have meaning on the “mountain landscape” side of the analogy, as well as on the “neural network learning” side of the analogy. The meaning of “velocity” in the “neural network learning” side of the analogy is the main idea of the momentum method. Vocabulary: the word “momentum” can be used with three different meanings, so it’s easy to get confused. It can mean the momentum method for neural network learning, i.e. the idea that’s introduced in this video. This is the most appropriate meaning of the word. It can mean the viscosity constant (typically 0.9), sometimes called alpha, which is used to reduce the velocity. It can mean the velocity. This is not a common meaning of the word. Note that one may equivalently choose to include the learning rate in the calculation of the update from the velocity, instead of in the calculation of the velocity."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6d-adaptive-learning-rates-for-each-connection",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6d-adaptive-learning-rates-for-each-connection",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6d: Adaptive learning rates for each connection",
    "text": "Lecture 6d: Adaptive learning rates for each connection\nThis is really “for each parameter”, i.e. biases as well as connection strengths. Vocabulary: a “gain” is a multiplier. This video introduces a basic idea (see the video title), with a simple implementation. In the next video, we’ll see a more sophisticated implementation. You might get the impression from this video that the details of how best to use such methods are not universally agreed on. That’s true. It’s research in progress."
  },
  {
    "objectID": "notes/dnn/dnn-06/l_06.html#lecture-6e-rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude",
    "href": "notes/dnn/dnn-06/l_06.html#lecture-6e-rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude",
    "title": "Deep Neural Networks - Notes for Lesson 6",
    "section": "Lecture 6e: Rmsprop: Divide the gradient by a running average of its recent magnitude",
    "text": "Lecture 6e: Rmsprop: Divide the gradient by a running average of its recent magnitude\nThis is another method that treats every weight separately. rprop uses the method of video 6d, plus that it only looks at the sign of the gradient. Make sure to understand how momentum is like using a (weighted) average of past gradients. Synonyms: “moving average”, “running average”, “decaying average”. All of these describe the same method of getting a weighted average of past observations, where recent observations are weighted more heavily than older ones. That method is shown in video 6e at 5:04. (there, it’s a running average of the square of the gradient) “moving average” and “running average” are fairly generic. “running average” is the most commonly used phrase. “decaying average” emphasizes the method that’s used to compute it: there’s a decay factor in there, like the alpha in the momentum method."
  },
  {
    "objectID": "notes/dnn/dnn-references/index.html",
    "href": "notes/dnn/dnn-references/index.html",
    "title": "Deep Neural Networks - Some Questions",
    "section": "",
    "text": "Some questions I have possed on DNN\nQ1. Is there a way to assess the impact of a trainng case or a batch on the model’s, specific layers and specific units? A1. Over the years since I posed this question I have noticed that it is something researchers seem to have looked at. - At first glance it seems like it is im[pssible to assess the impact. SGD works on mini batches or the full data. - But when we analy`se MNIST errors we take the worst misclassifications and we can look at the activation they generate at different level. We can see the activation that leads to a misclassification. So it turns out that it is possible. - Hinton also desribed full using MCMC for full baysian learning . Mackay also put DNN on more or less solid baysian footing. I have not implementated it so I cannot speak to the details but intuitively with a posterior it should be possible to condition on a point.\nLets imagine we could be advised by a “demon” regarding the can assess the over all contribution to signal or noise of different aspects of our model according to the following typology: First kind – overall model Second kind – each hyper parameter\nThird kind – at each layer Fourth kind – at each unit (neuron) Fifth kind – at the weights level Sixth Kind - part of an training item that activates neurons (pixels/sounds/words) I’m considering an analytics platform that would be based on collecting data from Jason Yosinski’s data visualization toolbox\nOne way to do this is to have a procedure that can quickly unlearn/forget training sets then do a diff. (might not be very useful if there are millions of weights) We may need some measure of uncertainty from non parametric methods that describes how if we are adding more learning points in places that are fitting our manifold at new point which are like new (learning new atoms or their relations) or we are just moving the surface back and forth at a single location or its neighborhood.\ne.g. learn the feature that differentiates birds from bees (generalizing) rather than modelling different points of view for each instance of bird and bee (modeling noise).\nFor each row in the data set what do we learn from it ?\nmore atomic concepts Relations on atomic concepts better smoothing – fitting missing data Short term relationships a&gt;b long distance relation a&gt;b&gt;…&gt;c&gt;d\nNN loves more data - more features, more layers more observation but the model can be grow very big and if we use lots of data we will need to train for a very long time\nI would like to explore the following ideas\nrunning some parametric algorithm on the data to bootstrap the neural net’s prior distributions closer the final values\nsimilar to the above I’d like to training nn dynamically and possibly non parametrically (you can have more CPU, memory, storage, data etc. but you get penalized for it) The TF graph should be expanded/contracted layers membership increased or decreased layers increased, hyper params adjusted during training.\nBayesian methods allow choices to be made about where in input space new data should be collected in order that it be the most informative (MacKay, 1992c). Such use of the model itself to guide the collection of data during training is known as active learning.\nMacKay, D. J. C. (1992c). Information-based objective functions for active data selection. Neural Computation 4 (4), 590-604.\nThe relative importance of different inputs can be determined using the Bayesian technique of automatic relevance determination (MacKay, 1994a, 1995b; Neal, 1994), based on the use of a separate regularization coefficient for each input. If a particular coefficient acquires a large value, this indicates that the corresponding input is irrelevant and can be eliminated.\nNeal, R. M. (1994). Bayesian Learning for Neural Networks. Ph.D. thesis, University of Toronto, Canada.\nMacKay, D. J. C. (1994a). Bayesian methods for backpropagation networks. In E. Domany, J. L. van Hemmen, and K. Schulten (Eds.), Models of Neural Networks III, Chapter 6. New York: Springer-Verlag.\nMacKay, D. J. C. (1995b). Bayesian non-linear modelling for the 1993 energy prediction competition. In G. Heidbreder (Ed.), Maximum Entropy and Bayesian Methods\nQuestions: In your own words describe a neural network\nA Neural Network consists of a graph with the inputs in one side and outputs on the other and between them are hidden units. All these nodes are connected with the connection strength between of the vertex connecting the units called its weight. Generally the graph is bipartite and can thus be organized using layers.\nThe graph can be trained so that the\nWeights are the vertices\nActions – the nodes ? what are these Model selection - Chaos –\nWhat is the importance of relative weights – within the same layer, between layers Given answers for the above should we use that for bootstrapping the wights instead of using random weights.\nGeometry of second order methods. Won’t using Mini Batched steps help where there is a complex structure.\nWhat is there are many local minima in our surface – how can we learn it all if we are always growing downhill. What happens if we have a chaotic surface – I think we can get this with a logistic function - What about an oscillation.\nDifference between first and second order learning methods\nIn reinforcement models the game being played is a markov decision process\nDo GAN take this concept one step further ?\nFor DNN what filters/kernels are initially selected. Are some different basis functions going to work better than others.\nAlso how about making some basis functions size independent by adding a 3by three five by five seven by seven etc. version.\nFor video filters that are time dependent. Also what about using non orthogonal basis.\nAlso what about forcing the system to drop basis which is redundant\nFor DNN we see that usually we have square on square configurations to reduce and mix the data. What about triangular or hexagonal architecture. Howa bout looking at RGB&Grey\nPostscript:\nBatch normalization: Accelerating … Input: Values of overa mini-batch: B = Parameters to be leamed: -y, ’3 Output: {Yi Xi — 11B 2 // mini-b;\nPix2Pix\nAttention - all you need is attention\nGroup Equivariant Convolutional Networks\nSteerable CNNs\nlogarithmic spiral\nfractal affine embeddings\nsimulate stereo vision modes\nVisualization\ndistil journal\nActivation-atlas\n\nhttps://aiyproject.withgoogle.com/open_speech_recording\n\nhttps://github.com/petewarden/open-speech-recording\nhttps://distill.pub/2016/augmented-rnns/\n\nAttention and Augmented Recurrent Neural Networks\n\nhttp://colah.github.io/\nhttps://github.com/sunnyyeti/Solutions-to-Neural-Network-for-machine-learning-by-Geoffrey-Hinton-on-Coursera\nhttps://github.com/BradNeuberg/hinton-coursera/blob/master/assignment3/a3.m\nhttps://github.com/Chouffe/hinton-coursera/tree/master/hw3\nhttps://github.com/tensorflow/compression/blob/master/examples/bls2017.py\nhttps://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\n\nnlp\n\nhttps://arxiv.org/abs/1803.06643\nhttps://arxiv.org/abs/1811.00937\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Some} {Questions}},\n  date = {2017-12-21},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-references},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Some\nQuestions.” December 21, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-references."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html",
    "href": "notes/dnn/dnn-13/l_13.html",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#a-brief-history-of-backpropagation",
    "href": "notes/dnn/dnn-13/l_13.html#a-brief-history-of-backpropagation",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "A brief history of backpropagation",
    "text": "A brief history of backpropagation\n\nThe backpropagation algorithm for learning multiple layers of features was invented several times in the 70’s and 80’s:\n\nBryson & Ho (1969) linear\nWerbos (1974)\nRumelhart et. al. in 1981\nParker (1985)\nLeCun (1985)\nRumelhart et. al. (1985)\n\nBackpropagation clearly had great promise for learning multiple layers of non-linear feature detectors.\nBut by the late 1990’s most serious researchers in machine learning had given up on it.\n\nIt was still widely used in psychological models and in practical applications such as credit card fraud detection."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#why-backpropagation-failed",
    "href": "notes/dnn/dnn-13/l_13.html#why-backpropagation-failed",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Why backpropagation failed",
    "text": "Why backpropagation failed\n\nThe popular explanation of why backpropagation failed in the 90’s:\n\nIt could not make good use of multiple hidden layers. (except in convolutional nets)\nIt did not work well in recurrent networks or deep auto-encoders.\nSupport Vector Machines worked better, required less expertise, produced repeatable results, and had much fancier theory.\n\nThe real reasons it failed:\n\nComputers were thousands of times too slow.\nLabeled datasets were hundreds of times too small.\nDNN were too small and inadequately initialized.\n\nThese issues prevented it from being successful for tasks where it would eventually be a big win."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#a-spectrum-of-ml-tasks",
    "href": "notes/dnn/dnn-13/l_13.html#a-spectrum-of-ml-tasks",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "A 🌈 spectrum 🌈 of ML 🤖 tasks",
    "text": "A 🌈 spectrum 🌈 of ML 🤖 tasks\n\nLow-dimensional data (e.g. less than 100 dimensions)\nLots of noise in the data.\nNot much structure in the data. The structure can be captured by a fairly simple model.\nThe main problem is separating true structure from noise.\n\nNot ideal for non-Bayesian neural nets. Try SVM or GP.\n\nHigh-dimensional data (e.g. more than 100 dimensions)\nThe noise is not the main problem.\nThere is a huge amount of structure in the data, but its too complicated to be represented by a simple model.\nThe main problem is figuring out a way to represent the complicated structure so that it can be learned.\n\nLet backpropagation figure it out."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#why-svms-were-never-a-good-bet-for-ai-tasks-that-need-good-representations",
    "href": "notes/dnn/dnn-13/l_13.html#why-svms-were-never-a-good-bet-for-ai-tasks-that-need-good-representations",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Why SVMs were never a good bet for AI tasks that need good representations",
    "text": "Why SVMs were never a good bet for AI tasks that need good representations\n\nView 1: SVM’s are just a clever reincarnation of Perceptrons.\n\nThey expand the input to a (very large) layer of nonlinear non-adaptive features.\nThey only have one layer of adaptive weights.\nThey have a very efficient way of fitting the weights that controls overfitting.\n\nView 2: SVM’s are just a clever reincarnation of Perceptrons.\n\nThey use each input vector in the training set to define a non-adaptive “pheature”.\n\nThe global match between a test input and that training input.\n\nThey have a clever way of simultaneously doing feature selection and finding weights on the remaining features."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#what-is-wrong-with-back-propagation",
    "href": "notes/dnn/dnn-13/l_13.html#what-is-wrong-with-back-propagation",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "What is wrong with back-propagation?",
    "text": "What is wrong with back-propagation?\n\nIt requires labeled training data.\n\nAlmost all data is unlabeled.\n\nThe learning time does not scale well\n\nIt is very slow in networks with multiple hidden layers.\nWhy?\n\nIt can get stuck in poor local optima.\n\nThese are often quite good, but for deep nets they are far from optimal.\nShould we retreat to models that allow convex optimization?"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#overcoming-the-limitations-of-back-propagation-by-using-unsupervised-learning",
    "href": "notes/dnn/dnn-13/l_13.html#overcoming-the-limitations-of-back-propagation-by-using-unsupervised-learning",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Overcoming the limitations of back-propagation by using unsupervised learning",
    "text": "Overcoming the limitations of back-propagation by using unsupervised learning\n\nKeep the efficiency and simplicity of using a gradient method for adjusting the weights, but use it for modeling the structure of the sensory input.\n\nAdjust the weights to maximize the probability that a generative model would have generated the sensory input.\nIf you want to do computer vision, first learn computer graphics.\n\nThe learning objective for a generative model:\n\nMaximise p(x) not p(y \\mid x)\n\nWhat kind of generative model should we learn?\n\nAn energy-based model like a Boltzmann machine?\nA causal model made of idealized neurons?\nA hybrid of the two?"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#artificial-intelligence-and-probability",
    "href": "notes/dnn/dnn-13/l_13.html#artificial-intelligence-and-probability",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Artificial Intelligence and Probability",
    "text": "Artificial Intelligence and Probability\n\n“Many ancient Greeks supported Socrates opinion that deep, inexplicable thoughts came from the gods. Today’s equivalent to those gods is the erratic, even probabilistic neuron. It is more likely that increased randomness of neural behavior is the problem of the epileptic and the drunk, not the advantage of the brilliant.” — P.H. Winston, “Artificial Intelligence”, 1977. (The first AI textbook)\n\n\n“All of this will lead to theories of computation which are much less rigidly of an all-or-none nature than past and present formal logic … There are numerous indications to make us believe that this new system of formal logic will move closer to another discipline which has been little linked in the past with logic. This is thermodynamics primarily in the form it was received from Boltzmann.”\n— John von Neumann, “The Computer and the Brain”, 1958 (unfinished manuscript)"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#the-marriage-of-graph-theory-and-probability-theory",
    "href": "notes/dnn/dnn-13/l_13.html#the-marriage-of-graph-theory-and-probability-theory",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "The marriage of graph theory and probability theory",
    "text": "The marriage of graph theory and probability theory\n\nIn the 1980’s there was a lot of work in AI that used bags of rules for tasks such as medical diagnosis and exploration for minerals.\n\nFor practical problems, they had to deal with uncertainty.\nThey made up ways of doing this that did not involve probabilities!\n\nGraphical models: Pearl, Heckerman, Lauritzen, and many others showed that probabilities worked better.\n\nGraphs were good for representing what depended on what.\nProbabilities then had to be computed for nodes of the graph, given the states of other nodes.\n\nBelief Nets: For sparsely connected, directed acyclic graphs, clever inference algorithms were discovered."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#belief-nets",
    "href": "notes/dnn/dnn-13/l_13.html#belief-nets",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Belief Nets",
    "text": "Belief Nets\n\n\n\n\n\nSigmoid belief net\n\n\n\nA belief net is a directed acyclic graph composed of stochastic variables.\nWe get to observe some of the variables and we would like to solve two problems:\nThe inference problem: Infer the states of the unobserved variables.\nThe learning problem: Adjust the interactions between variables to make the network more likely to generate the training data."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#graphical-models-versus-neural-networks",
    "href": "notes/dnn/dnn-13/l_13.html#graphical-models-versus-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Graphical Models versus Neural Networks",
    "text": "Graphical Models versus Neural Networks\n\nEarly graphical models used experts to define the graph structure and the conditional probabilities.\n\nThe graphs were sparsely connected.\nResearchers initially focused on doing correct inference, not on learning.\n\nFor neural nets, learning was central. Hand-wiring the knowledge was not cool (OK, maybe a little bit).\n\nKnowledge came from learning the training data.\n\nNeural networks did not aim for interpretability or sparse connectivity to make inference easy.\n\nNevertheless, there are neural network versions of belief nets."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#two-types-of-generative-neural-network-composed-of-stochastic-binary-neurons",
    "href": "notes/dnn/dnn-13/l_13.html#two-types-of-generative-neural-network-composed-of-stochastic-binary-neurons",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Two types of generative neural network composed of stochastic binary neurons",
    "text": "Two types of generative neural network composed of stochastic binary neurons\n\n\n\n\n\nSigmoid belief net\n\n\n\nEnergy-based: We connect binary stochastic neurons using symmetric connections to get a Boltzmann Machine.\n\nIf we restrict the connectivity in a special way, it is easy to learn a Boltzmann machine.\nBut then we only have one hidden layer.\n\nCausal: We connect binary stochastic neurons in a directed acyclic graph to get a Sigmoid Belief Net (Neal 1992)."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#connectionist-learning-of-belief-networks---paper",
    "href": "notes/dnn/dnn-13/l_13.html#connectionist-learning-of-belief-networks---paper",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Connectionist learning of belief networks - Paper",
    "text": "Connectionist learning of belief networks - Paper\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#learning-sigmoid-belief-nets",
    "href": "notes/dnn/dnn-13/l_13.html#learning-sigmoid-belief-nets",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Learning Sigmoid Belief Nets",
    "text": "Learning Sigmoid Belief Nets\n\n\n\n\n\nSigmoid Belief Nets\n\n\n\nIt is easy to generate an unbiased example at the leaf nodes, so we can see what kinds of data the network believes in.\nIt is hard to infer the posterior distribution over all possible configurations of hidden causes.\nIt is hard to even get a sample from the posterior.\nSo how can we learn sigmoid belief nets that have millions of parameters?"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#the-learning-rule-for-sigmoid-belief-nets",
    "href": "notes/dnn/dnn-13/l_13.html#the-learning-rule-for-sigmoid-belief-nets",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "The learning rule for sigmoid belief nets",
    "text": "The learning rule for sigmoid belief nets\n\n\n\n\n\nsigmoid belief nets\n\n\n\nLearning is easy if we can get an unbiased sample from the posterior distribution over hidden states given the observed data.\nFor each unit, maximize the log prob. that its binary state in the sample from the posterior would be generated by the sampled binary states of its parents.\n\n\np_i = p(s_= 1) = \\frac {1}{1+ \\exp \\bigg(-b_i -\\sum_j s_j w_{ji} \\bigg )}\n\n\n\\Delta w_{ij} = \\epsilon s_j (s_i-p_i)"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#explaining-away-judea-pearl",
    "href": "notes/dnn/dnn-13/l_13.html#explaining-away-judea-pearl",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Explaining away (Judea Pearl)",
    "text": "Explaining away (Judea Pearl)\n\n\n\n\nEven if two hidden causes are independent in the prior, they can become dependent when we observe an effect that they can both influence.\n\nIf we learn that there was an earthquake it reduces the probability that the house jumped because of a truck."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#why-its-hard-to-learn-sigmoid-belief-nets-one-layer-at-a-time",
    "href": "notes/dnn/dnn-13/l_13.html#why-its-hard-to-learn-sigmoid-belief-nets-one-layer-at-a-time",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Why it’s hard to learn sigmoid belief nets one layer at a time",
    "text": "Why it’s hard to learn sigmoid belief nets one layer at a time\n\n\n\n\nTo learn W, we need to sample from the posterior distribution in the first hidden layer.\nProblem 1: The posterior is not factorial because of “explaining away”.\nProblem 2: The posterior depends on the prior as well as the likelihood.\n\nSo to learn W, we need to know the weights in higher layers, even if we are only approximating the posterior. All the weights interact.\n\nProblem 3: We need to integrate over all possible configurations in the higher layers to get the prior for first hidden layer. Its hopeless!"
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#some-methods-for-learning-deep-belief-nets",
    "href": "notes/dnn/dnn-13/l_13.html#some-methods-for-learning-deep-belief-nets",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Some methods for learning deep belief nets",
    "text": "Some methods for learning deep belief nets\n\nMonte Carlo methods can be used to sample from the posterior (Neal 1992).\n\nBut its painfully slow for large, deep belief nets.\n\nIn the 1990’s people developed variational methods for learning deep belief nets.\n\nThese only get approximate samples from the posterior.\n\nLearning with samples from the wrong distribution:\n\nMaximum likelihood learning requires unbiased samples from the posterior.\n\nWhat happens if we sample from the wrong distribution but still use the maximum likelihood learning rule?\n\nDoes the learning still work or does it do crazy things?\n\n\n\nNeal, Radford M. 1992. “Connectionist Learning of Belief Networks.” Artificial Intelligence 56 (1): 71–113."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#an-apparently-crazy-idea",
    "href": "notes/dnn/dnn-13/l_13.html#an-apparently-crazy-idea",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "An apparently crazy idea",
    "text": "An apparently crazy idea\n\nIt’s hard to learn complicated models like Sigmoid Belief Nets.\nThe problem is that it’s hard to infer the posterior distribution over hidden configurations when given a data vector.\n\nIts hard even to get a sample from the posterior.\n\nCrazy idea: do the inference wrong.\n\nMaybe learning will still work.\nThis turns out to be true for SBNs.\n\nAt each hidden layer, we assume (wrongly) that the posterior over hidden configurations factorizes into a product of distributions for each separate hidden unit."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#factorial-distributions",
    "href": "notes/dnn/dnn-13/l_13.html#factorial-distributions",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Factorial distributions",
    "text": "Factorial distributions\n\nIn a factorial distribution, the probability of a whole vector is just the product of the probabilities of its individual terms:\nindividual probabilities of three hidden units in a layer 0.3 0.6 0.8\nprobability that the hidden units have state 1,0,1 if the distribution is factorial. p(1, 0, 1) = 0.3× (1− 0.6) \\times 0.8\nA general distribution over binary vectors of length N has 2^N degrees of freedom (actually 2^N-1 because the probabilities must add to 1). A factorial distribution only has N degrees of freedom."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#the-wake-sleep-algorithm-hinton-et.-al.-1995",
    "href": "notes/dnn/dnn-13/l_13.html#the-wake-sleep-algorithm-hinton-et.-al.-1995",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "The wake-sleep algorithm (Hinton et. al. 1995)",
    "text": "The wake-sleep algorithm (Hinton et. al. 1995)\nUnable to display PDF file. Download instead.\n\n\n\n\n\nwake sleep alg\n\n\n\nWake phase: Use recognition weights to perform a bottom-up pass.\n\nTrain the generative weights to reconstruct activities in each layer from the layer above.\n\nSleep phase: Use generative weights to generate samples from the model.\n\nTrain the recognition weights to reconstruct activities in each layer from the layer below."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#the-flaws-in-the-wake-sleep-algorithm",
    "href": "notes/dnn/dnn-13/l_13.html#the-flaws-in-the-wake-sleep-algorithm",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "The flaws in the wake-sleep algorithm",
    "text": "The flaws in the wake-sleep algorithm\n\nThe recognition weights are trained to invert the generative model in parts of the space where there is no data.\n\nThis is wasteful.\n\nThe recognition weights do not follow the gradient of the log probability of the data. They only approximately follow the gradient of the variational bound on this probability.\n\nThis leads to incorrect mode-averaging\n\nThe posterior over the top hidden layer is very far from independent because of explaining away effects.\nNevertheless, Karl Friston thinks this is how the brain works."
  },
  {
    "objectID": "notes/dnn/dnn-13/l_13.html#mode-averaging",
    "href": "notes/dnn/dnn-13/l_13.html#mode-averaging",
    "title": "Deep Neural Networks - Notes for Lesson 13",
    "section": "Mode averaging",
    "text": "Mode averaging\n\n\n\n\n\n\n\nIf we generate from the model, half the instances of a 1 at the data layer will be caused by a (1,0) at the hidden layer and half will be caused by a (0,1).\n\nSo the recognition weights will learn to produce (0.5, 0.5)\nThis represents a distribution that puts half its mass on 1,1 or 0,0: very improbable hidden configurations.\n\nIts much better to just pick one mode.\n\nThis is the best recognition model you can get if you assume that the posterior over hidden states factorizes.\n\n\n\n\n\nSigmoid belief net\nSigmoid belief net\nSigmoid Belief Nets\nsigmoid belief nets\nwake sleep alg"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html",
    "href": "notes/dnn/dnn-02/l02a.html",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nIn this lecture we covered the main types of networks studied in the course"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html#feed-forward-neural-networks",
    "href": "notes/dnn/dnn-02/l02a.html#feed-forward-neural-networks",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "Feed-forward neural networks",
    "text": "Feed-forward neural networks\n\n\n\nFeed-forward neural networks\n\nFeed forward networks are the subject of the first half of the course.\nThese are the most common type of neural network.\nThe first layer is the input and\nThe last layer is the output.\n\nIf there is more than one hidden layer, we call them “deep” neural networks.\n\nThey compute a series of transformations that change the similarities between cases.\n\nThe activities of the neurons in each layer are a non-linear function of the activities in the layer below."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html#recurrent-networks",
    "href": "notes/dnn/dnn-02/l02a.html#recurrent-networks",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "Recurrent networks",
    "text": "Recurrent networks\n\n\n\n\n\nRecurrent neural networks\n\n\n\nThese have directed cycles in their connection graph.\n\nThat means you can sometimes get back to where you started by following the arrows.\n\nThey can have complicated dynamics and this can make them very difficult to train. – There is a lot of interest at present in finding efficient ways of training recurrent nets.\nThey are more biologically realistic.\n\n\nRecurrent neural networks for modeling sequences\n\n\n\n\n\nsequence to Sequence mapping\n\n\n\nRecurrent neural networks are a very natural way to model sequential data:\n\nThey are equivalent to very deep nets with one hidden layer per time slice.\nExcept that they use the same weights at every time slice and they get input at every time slice.\n\nThey have the ability to remember information in their hidden state for a long time.\n\nBut its very hard to train them to use this potential\n\n\n\n\nAn example of what RNNs can now do\n\nIn (Sutskever, Martens, and Hinton 2011) the authors trained a special type of RNN to predict the next character in a sequence.\nAfter training for a long time on a string of half a billion characters from English Wikipedia, he got it to generate new text.\n\nIt generates by predicting the probability distribution for the next character and then sampling a character from this distribution.\nThe next slide shows an example of the kind of text it generates. Notice how much it knows!\n\n\n\nSutskever, Ilya, James Martens, and Geoffrey E Hinton. 2011. “Generating Text with Recurrent Neural Networks.” In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1017–24. https://www.cs.toronto.edu/~jmartens/docs/RNN_Language.pdf.\n\n\nSample text generated one character at a time by Ilya Sutskever’s RNN\n\nIn 1974 Northern Denver had been overshadowed by CNL, and several Irish intelligence agencies in the Mediterranean region. However, on the Victoria, Kings Hebrew stated that Charles decided to escape during an alliance. The mansion house was completed in 1882, the second in its bridge are omitted, while closing is the proton reticulum composed below it aims, such that it is the blurring of appearing on any well-paid type of box printer."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html#symmetrically-connected-networks",
    "href": "notes/dnn/dnn-02/l02a.html#symmetrically-connected-networks",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "Symmetrically connected networks",
    "text": "Symmetrically connected networks\n\nThese are like recurrent networks, but the connections between units are symmetrical (they have the same weight in both directions).\n\nJohn Hopfield (and others) realized that symmetric networks are much easier to analyze than recurrent networks. – They are also more restricted in what they can do. because they obey an energy function.\n\nFor example, they cannot model cycles.\n\n\nIn (Hopfield 1982), the author introduced symmetrically connected nets without hidden units that are now called Hopfield networks.\n\n\nHopfield, J J. 1982. “Neural Networks and Physical Systems with Emergent Collective Computational Abilities.” Proceedings of the National Academy of Sciences 79 (8): 2554–58. https://doi.org/10.1073/pnas.79.8.2554."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02a.html#symmetrically-connected-networks-with-hidden-units",
    "href": "notes/dnn/dnn-02/l02a.html#symmetrically-connected-networks-with-hidden-units",
    "title": "Deep Neural Networks - Notes for lecture 2a",
    "section": "Symmetrically connected networks with hidden units",
    "text": "Symmetrically connected networks with hidden units\n\nCalled Boltzmann machines.\n\nThey are much more powerful models than Hopfield nets.\nThey are less powerful than recurrent neural networks.\nThey have a beautifully simple learning algorithm.\n\nWe will cover Boltzmann machines towards the end of the course.\n\n\nSummary of Networks Architectures\n\n\n\n\n\n\n\nSchematic\nDescription\n\n\n\n\n\nFeed forward nets - regression and classification for images and tabular data.\n\n\n\nRecurrent nets - sequence to sequence\n\n\n\nHopfield nets - associative memory using symmetric nets with no hidden units\n\n\n\nBoltzmann machines - symmetric nets with hidden units\n\n\n\ncredit: images from The Neural Network Zoo\n\n\n\nRecurrent neural networks\nsequence to Sequence mapping"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02d.html",
    "href": "notes/dnn/dnn-02/l02d.html",
    "title": "Deep Neural Networks - Notes for lecture 2d",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nIn this lecture we try to explain intuitively why the perceptron algorithm works\nAlso we consider why it may fail."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02d.html#why-the-learning-procedure-works-first-attempt",
    "href": "notes/dnn/dnn-02/l02d.html#why-the-learning-procedure-works-first-attempt",
    "title": "Deep Neural Networks - Notes for lecture 2d",
    "section": "Why the learning procedure works (first attempt)",
    "text": "Why the learning procedure works (first attempt)\n\n\n\n\n\n\n\nidea of the proof\n\n\nwe want to get closer all the feasible solution\n\nConsider the squared distance d_a^2+d_b^2 between any feasible weight vector and the current weight vector. – Hopeful claim: Every time the perceptron makes a mistake, the learning algorithm moves the current weight vector closer to all feasible weight vectors\nWe look at the geometrical interpretation which is the proof for the convergence of the Perceptron learning algorithm works. We are trying to find a decision surface by solving a convex optimization problem.\nThe surface is a hyper-plane represented by a line where on side is the correct set and the other is incorrect. The weight vectors form a cone:\n\nThis means that wights are closed under addition and positive scaler product.\nAt zero it is zero.\n\n\n\n\n\n\nfixing it up\n\n\nwe now use a generously feasible sub-cone of the feasible cone shown in a dotted line\n\nSo consider “generously feasible” weight vectors that lie within the feasible region by a margin at least as great as the length of the input vector that defines each constraint plane.\n\nEvery time the perceptron makes a mistake, the squared distance to all of these generously feasible weight vectors is always decreased by at least the squared length of the update vector."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02d.html#informal-sketch-of-proof-of-convergence",
    "href": "notes/dnn/dnn-02/l02d.html#informal-sketch-of-proof-of-convergence",
    "title": "Deep Neural Networks - Notes for lecture 2d",
    "section": "Informal sketch of proof of convergence",
    "text": "Informal sketch of proof of convergence\n\nEach time the perceptron makes a mistake, the current weight vector moves to decrease its squared distance from every weight vector in the “generously feasible” region.\nThe squared distance decreases by at least the squared length of the input vector.\nSo after a finite number of mistakes, the weight vector must lie in the feasible region if this region exists.\n\n\n\n\nidea of the proof\nfixing it up"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html",
    "href": "notes/dnn/dnn-02/l02b.html",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nThe lecture starts with the history of Perceptrons (Wikipedia contributors 2024)\nThen covers with The perceptron convergence procedure. Next is a deeper dive into the computational geometry of Perceptrons\nI also added a python implementation from scratch."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#the-standard-paradigm-for-statistical-pattern-recognition",
    "href": "notes/dnn/dnn-02/l02b.html#the-standard-paradigm-for-statistical-pattern-recognition",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "The standard paradigm for statistical pattern recognition",
    "text": "The standard paradigm for statistical pattern recognition\n\n\n\nThe standard Perceptron architecture\n\n\n\nConvert the raw input vector into a vector of feature activations. Use hand-written programs based on common-sense to define the features.\nLearn how to weight each of the feature activations to get a single scalar quantity.\nIf this quantity is above some threshold, decide that the input vector is a positive example of the target class."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#the-history-of-perceptrons",
    "href": "notes/dnn/dnn-02/l02b.html#the-history-of-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "The history of Perceptrons",
    "text": "The history of Perceptrons\n\nPerceptrons were introduced in (Rosenblatt 1962) by Frank Rosenblatt who popularized them\n\nThey appeared to have a very powerful learning algorithm.\nLots of grand claims were made for what they could learn to do.\n\nIn (Minsky and Papert 1969) the authors, analysed what Perceptrons could do and showed their limitations. 1\n\nMany people thought these limitations applied to all neural network models.\n\nThe perceptron learning procedure is still widely used today for tasks with enormous feature vectors that contain many millions of features.\n\n\nRosenblatt, F. 1962. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Cornell Aeronautical Laboratory. Report No. VG-1196-g-8. Spartan Books. https://books.google.co.il/books?id=7FhRAAAAMAAJ.\n\nMinsky, Marvin, and Seymour Papert. 1969. Perceptrons: An Introduction to Computational Geometry. Cambridge, MA, USA: MIT Press.\n1 The main results are available here\n\n\n\nA perceptron\n\n\n\n\nwhy the bias can be implemented as a special input unit?\nbiases can be treated using weights using an input that is always one.\na threshold is equivalent to having a negative bias.\nwe can avoid having to figure out a separate learning rule for the bias by using a trick:\nA bias is exactly equivalent to a weight on an extra input line that always has an activation of 1.\n\n\nBinary threshold neurons (decision units)\n\n\n\nBinary Theshold Unit\n\n\n\nIntroduced in (Mcculloch and Pitts 1943)\n\nFirst compute a weighted sum of the inputs from other neurons (plus a bias).\nThen output a 1 if the weighted sum exceeds zero.\n\n\n\nMcculloch, Warren, and Walter Pitts. 1943. “A Logical Calculus of Ideas Immanent in Nervous Activity.” Bulletin of Mathematical Biophysics 5: 127–47.\n\nz = b+ \\sum_i{ x_i w_i}\n \ny = \\left\\{\n   \\begin{array}{ll}\n       1 & \\text{if} \\space z \\ge 0 \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n    \\right."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#how-to-learn-biases-using-the-same-rule-as-we-use-for-learning-weights",
    "href": "notes/dnn/dnn-02/l02b.html#how-to-learn-biases-using-the-same-rule-as-we-use-for-learning-weights",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "How to learn biases using the same rule as we use for learning weights",
    "text": "How to learn biases using the same rule as we use for learning weights\n\nA threshold is equivalent to having a negative bias.\nWe can avoid having to figure out a separate learning rule for the bias by using a trick:\n\nA bias is exactly equivalent to a weight on an extra input line that always has an activity of 1.\nWe can now learn a bias as if it were a weight."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#the-perceptron-convergence-procedure-training-binary-output-neurons-as-classifiers",
    "href": "notes/dnn/dnn-02/l02b.html#the-perceptron-convergence-procedure-training-binary-output-neurons-as-classifiers",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "The Perceptron convergence procedure: Training binary output neurons as classifiers",
    "text": "The Perceptron convergence procedure: Training binary output neurons as classifiers\n\nAdd an extra component with value 1 to each input vector. The bias weight on this component is minus the threshold. Now we can forget the threshold.\nPick training cases using any policy that ensures that every training case will keep getting picked.\n\nIf the output unit is correct, leave its weights alone.\nIf the output unit incorrectly outputs a zero, add the input vector to the weight vector.\nIf the output unit incorrectly outputs a 1, subtract the input vector from the weight vector.\n\nThis is guaranteed to find a set of weights that gets the right answer for all the training cases if any such set exists."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02b.html#a-full-implementation-of-a-perceptrons",
    "href": "notes/dnn/dnn-02/l02b.html#a-full-implementation-of-a-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 2b",
    "section": "A full implementation of a perceptrons:",
    "text": "A full implementation of a perceptrons:\ncode and image from: Implementing the Perceptron Algorithm in Python\n\n\n\nPerceptron\n\n\n\nfrom sklearn import datasets\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nX, y = datasets.make_blobs(n_samples=150,n_features=2,\n                           centers=2,cluster_std=1.05,\n                           random_state=2)\n#Plotting\nfig = plt.figure(figsize=(10,8))\nplt.plot(X[:, 0][y == 0], X[:, 1][y == 0], 'r^')\nplt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'bs')\nplt.xlabel(\"feature 1\")\nplt.ylabel(\"feature 2\")\nplt.title('Random Classification Data with 2 classes')\n\ndef step_func(z):\n        return 1.0 if (z &gt; 0) else 0.0\n      \ndef perceptron(X, y, lr, epochs):\n    '''\n    X: inputs\n    y: labels\n    lr: learning rate\n    epochs: Number of iterations\n    m: number of training examples\n    n: number of features \n    '''\n    m, n = X.shape    \n    # Initializing parapeters(theta) to zeros.\n    # +1 in n+1 for the bias term.\n    theta = np.zeros((n+1,1))\n    \n    # list with misclassification count per iteration.\n    n_miss_list = []\n    \n    # Training.\n    for epoch in range(epochs):\n        # variable to store misclassified.\n        n_miss = 0\n        # looping for every example.\n        for idx, x_i in enumerate(X):\n            # Inserting 1 for bias, X0 = 1.\n            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          \n            # Calculating prediction/hypothesis.\n            y_hat = step_func(np.dot(x_i.T, theta))\n            # Updating if the example is misclassified.\n            if (np.squeeze(y_hat) - y[idx]) != 0:\n                theta += lr*((y[idx] - y_hat)*x_i)\n                # Incrementing by 1.\n                n_miss += 1\n        # Appending number of misclassified examples\n        # at every iteration.\n        n_miss_list.append(n_miss)\n    return theta, n_miss_list\n\n\n\n\n\n\n\n\n\ndef plot_decision_boundary(X, theta):\n    \n    # X --&gt; Inputs\n    # theta --&gt; parameters\n    \n    # The Line is y=mx+c\n    # So, Equate mx+c = theta0.X0 + theta1.X1 + theta2.X2\n    # Solving we find m and c\n    x1 = [min(X[:,0]), max(X[:,0])]\n    m = -theta[1]/theta[2]\n    c = -theta[0]/theta[2]\n    x2 = m*x1 + c\n    \n    # Plotting\n    fig = plt.figure(figsize=(10,8))\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"r^\")\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n    plt.xlabel(\"feature 1\")\n    plt.ylabel(\"feature 2\")\n    plt.title('Perceptron Algorithm')\n    plt.plot(x1, x2, 'y-')\n\n\ntheta, miss_l = perceptron(X, y, 0.5, 100)\nplot_decision_boundary(X, theta)\n\n\n\n\n\n\n\n\n\n\n\nThe standard Perceptron architecture\nA perceptron\nBinary Theshold Unit\nPerceptron"
  },
  {
    "objectID": "notes/dnn/dnn-15/l_15.html",
    "href": "notes/dnn/dnn-15/l_15.html",
    "title": "Deep Neural Networks - Notes for Lesson 15",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-15/l_15.html#lecture-15e-learning-binary-codes-for-image-retrieval",
    "href": "notes/dnn/dnn-15/l_15.html#lecture-15e-learning-binary-codes-for-image-retrieval",
    "title": "Deep Neural Networks - Notes for Lesson 15",
    "section": "Lecture 15e: Learning binary codes for image retrieval",
    "text": "Lecture 15e: Learning binary codes for image retrieval\nIt is essential that you understand video 15d before you try 15e. 7:13. Don’t worry if you don’t understand that last comment."
  },
  {
    "objectID": "notes/dnn/dnn-15/l_15.html#lecture-15f-shallow-autoencoders-for-pre-training",
    "href": "notes/dnn/dnn-15/l_15.html#lecture-15f-shallow-autoencoders-for-pre-training",
    "title": "Deep Neural Networks - Notes for Lesson 15",
    "section": "Lecture 15f: Shallow autoencoders for pre-training",
    "text": "Lecture 15f: Shallow autoencoders for pre-training\nThis video is quite separate from the others of chapter 15.\nCNN Architecture & hyper parameters\nConvolutional Neural Network example INPUT [F,F,3]\nCONV [F,F,K] - basis sensor RELU [F,F,K ] - elementwise activation POOL [F/2,F/2,S] - down sampling\nFC - convers volume to class probability Hyper parameters: K – depth is the number of filters/kernels to use say 12 F - the RECEPTIVE FIELD or spatial extent of the filters – pixels width and height a neuron sees say 32x32 S – the STRIDE = step size for the offset used for sliding the filters so that there is an overlap neurons – say 1 P the amount of PADDING= padding round input with zeros, used because output and input might otherwise have different sizes\nAs of 2015 per STRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET the recommendation is to Removing\nPooling Removing normalization also recommended\nINPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]M -&gt; [FC -&gt; RELU]K -&gt; FC\nSeems FC and CONV are functionally equivalent and can be interchanged. Some other techniques/layers types: 1x1 convolution Dilated convolutions (acting on spaced out pixels) Replacing Max Pooling with ROI region of interrest pooling Loss layer – represent the overall error Dropout layer - Regularization by droping a unit with probabpility p DropConnect - Regularization by dropping connections instead of units\nStochastic pooling\nWeight decay = 0.001 Image whitening and contrast normalization in preprocessing"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html",
    "href": "notes/dnn/dnn-01/l01c.html",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#lecture-1c-some-simple-models-of-neurons",
    "href": "notes/dnn/dnn-01/l01c.html#lecture-1c-some-simple-models-of-neurons",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Lecture 1c: Some simple models of neurons",
    "text": "Lecture 1c: Some simple models of neurons\n\n\nIdealized neurons\n\nTo model things we have to idealize them (e.g. atoms)\n\nIdealization removes complicated details that are not essential for understanding the main principles.\nIt allows us to apply mathematics and to make analogies to other, familiar systems.\nOnce we understand the basic principles, its easy to add complexity to make the model more faithful.\n\nIt is often worth understanding models that are known to be wrong (but we must not forget that they are wrong!)\n\nE.g. neurons that communicate real values rather than discrete spikes of activity.\n\n\n\n\nLinear neurons\n\n\n\n\nlinear activation function\n\n\n\n\nThese are simple but computationally limited\n\nIf we can make them learn we may get insight into more complicated neurons.\n\n\n\ny=b+\\sum_i{ x_i \\times w_i}\n\nwhere:\n\ny is the output\n\nb is the bias\ni is the index over input connectinos\n\nx_i is the ith input\nw_i is the weight on ith input\n\nBias is often conveniently chosen to be 0 which is odd considering that it is the constraint on the activation. This is handled formally by a technique called batch normalization\nThese are simple but computationally limited.\n\nIf we can make them learn we may get insight into more complicated neurons.\n\n\ny=b+\\sum_i{ x_i \\times w_i}"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#binary-threshold-units",
    "href": "notes/dnn/dnn-01/l01c.html#binary-threshold-units",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Binary threshold units",
    "text": "Binary threshold units\n Binary threshold units are due to Warren McCulloch and Walter Pitts from their McCulloch and Pitts (1943). They were in turn influenced by earlier work by John Von Neumann the father of modern computer and game theory.\n\n\n\n\n\n\n\n\nWarren Sturgis Mcculloch\nWalter Pitts\nJohnvon Neumann\n\n\n\n\n\n\n\n\n\n\n\nFirst compute a weighted sum of the inputs.\nThen send out a fixed size spike of activity if the weighted sum exceeds a threshold.\nMcCulloch and Pitts thought that each spike is like the truth value of a proposition and each neuron combines truth values to compute the truth value of another proposition!\n\nThere are two ways to write these mathematicaly:\n\nz = \\sum_i{ x_i w_i}\\\\\n\\theta = -b \\\\\ny = \\left\\{\n   \\begin{array}{ll}\n       1 & \\text{if} \\space z \\ge \\theta \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n    \\right.\n\nusing bias\n\nz = b+ \\sum_i{ x_i w_i}\\\\\ny = \\left\\{\n   \\begin{array}{ll}\n       1 & \\text{if} \\space z \\ge 0 \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n    \\right."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#relu---rectified-linear-neurons-aka-linear-threshold-neurons",
    "href": "notes/dnn/dnn-01/l01c.html#relu---rectified-linear-neurons-aka-linear-threshold-neurons",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "RELU - REctified Linear Neurons AKA Linear Threshold neurons",
    "text": "RELU - REctified Linear Neurons AKA Linear Threshold neurons\n\n\n\nRELU activation function\n\n\n\nThey compute a linear weighted sum of their inputs.\nThe output is a non-linear function of the total input.\n\n\nz = b + \\sum _i x_iw_i \\\\\n\n\ny = \\left\\{\n   \\begin{array}{ll}\n       z & \\text{if} \\space z \\gt 0 \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n   \\right."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#sigmoid-neurons",
    "href": "notes/dnn/dnn-01/l01c.html#sigmoid-neurons",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Sigmoid neurons",
    "text": "Sigmoid neurons\n\n\n\nSigmoid activation function\n\n\n\nThese give a real-valued output that is a smooth and bounded function of their total input.\nTypically they use the logistic function\nHave nice derivatives which make learning easy.\n\n\nz = b + \\sum _i x_iw_i \\\\\n\\space\\\\\ny = \\frac{1}{1+e^{-z}}"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#stochastic-binary-neurons",
    "href": "notes/dnn/dnn-01/l01c.html#stochastic-binary-neurons",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Stochastic binary neurons",
    "text": "Stochastic binary neurons\n\n\n\nbinary activation function\n\n\nThese use the same equations as logistic units. - But they treat the output of the logistic as the probability of producing a spike in a short time window.\nWe can do a similar trick for rectified linear units:\n\nThe output is treated as the Poisson rate for spikes.\n\n\nz = b + \\sum _i x_iw_i \\\\\n\\space\\\\\np(s=1) = \\frac{1}{1+e^{-z}}"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01c.html#choosing-an-activation-function",
    "href": "notes/dnn/dnn-01/l01c.html#choosing-an-activation-function",
    "title": "Deep Neural Networks - Notes for lecture 1c",
    "section": "Choosing an activation function",
    "text": "Choosing an activation function\nFirst let us note that many other activation function exist, this table list the following:\n\n\n\nactivation functions\n\n\nAt this point in the course we do not go into how one should pick a preferred activation function for the given problem. Some ideas for this are mentioned during the course. If we look at this from an engineering perspective some units tend to work well with other units and there are some other constraints like the range of inputs.\n\nLinear units\nTheir main benefit is that they help us write down the mathematically familiar linear model which is great for getting a basic insight into the problem. We can analyze this model in term of linear and or abstract algebra using concepts like spaces, subspace, solutions, eigenvectors, eigenvalues and so on. Unfortunately linear units they are not expressive enough to perform as a basis of an efficient universal approximator. A linear model is equivalent to a large logistic regression as each variable will effect all other variables. So once we developed some intuition about our linear model we would want to switch to a non-liner units and make use of the full power of neural networks.\n\n\nBinary threshold units\nTheir main benefit seem to be for modeling logical gates or logical circuits. Cons: have only zero and infinite gradients so are unsuitable for use in networks that are trained using gradient descent. They are used however in Hopfield networks. We will also consider later using a fully baysian approch to neural networks where we don’t need stochastic gradient descent - instead using MCMC search. It would seem that is such a settings using binary threshold units would dramatically decrease the search space.\n\n\nRELU\nThis is the simplest non linear units - using it is essentially introducing constraints in the form of inequalities. It should only be used in a hidden layer. A classification will need to add a Softmax and a regression a linear function. RELUs can die - so a Leaky RELU can be a better choice. \n\n\nSigmoid\nThis is continuous and has a gradient between 0 and 1 - pros: Sigmoid with weight initialized to zero behave like a linear system. As the weights increase towards they networks. - cons: saturate and kill gradients also when output is not centered about 0 then gradients tend to go to far to 0 or 1. They converge slowly.\n\n\nTANH\n\npros: very high values are similar (~1) and very low values are also similar (~1)\ncons: sub optimal for a deep network, as gradient diminish in the deeper parts of the model.\n\nRMSProp will compensate for that, but still changing to RELU will improve convergence speed c.f. (user8272359 2017). It is better then Sigmoid as it avoids the exploding gradient problem\n\n\n\nuser8272359. 2017. “Deep Neural Network Using Keras/Tensorflow Solves Spiral Dataset Classification. But Accuracy Is Stuck Around 50.” August 5, 2017. https://datascience.stackexchange.com/questions/22830/deep-neural-network-using-keras-tensorflow-solves-spiral-dataset-classification.\n\nlinear activation function\nRELU activation function\nSigmoid activation function\nbinary activation function\nactivation functions"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01_.html",
    "href": "notes/dnn/dnn-01/l01_.html",
    "title": "Deep Neural Networks - Notes for Lesson 1",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nThese is the first installment of notes to the course “Deep Neural Networks” by Geoffrey Hinton I took on Coursera\nThis was one of the first course online on the subject.\nHinton was one of the leading researchers on deep learning, his students are some of the most important reaserchers today. He introduced some algorithms and methods that were not published.\nThis course is now outdated - it does not cover transformers and probably all the results have been beaten as this is a fast moving field.\nStill this is an interesting, if mathematicaly sophisticated introduction to deep learning.\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} for {Lesson} 1},\n  date = {2017-07-01},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-01/l01_.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes for Lesson\n1.” July 1, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-01/l01_.html."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html",
    "href": "notes/dnn/dnn-01/l01a.html",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nThese is the first installment of notes to the course “Deep Neural Networks” by Geoffrey Hinton I took on Coursera\nThis was one of the first course online on the subject.\nHinton was one of the leading researchers on deep learning, his students are some of the most important reaserchers today. He introduced some algorithms and methods that were not published.\nThis course is dated, the SOTA results have improved, it does not cover transformers and probably all the results have been beaten as this is a fast moving field.\nStill this is an interesting, if mathematically sophisticated introduction to deep learning."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#what-is-ml",
    "href": "notes/dnn/dnn-01/l01a.html#what-is-ml",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "What is ML?",
    "text": "What is ML?\n\nIt is very hard 😱 to write programs that solve problems like recognizing a 3d object from a novel viewpoint in new lighting conditions in a cluttered scene.\n\nWe don’t know what program to write because we don’t know 😕 how its done in our brain. 🧠\nEven if we had an clue 💡 how to do it — the program would be 😱 horrendously complicated.\n\nIt is hard to write a program to compute the probability that a credit card transaction is fraudulent.\n\nThere may not be any rules that are both simple and reliable. We need to combine a very large number of weak rules.\nFraud is a moving target. The program needs to keep changing."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#the-ml-approach",
    "href": "notes/dnn/dnn-01/l01a.html#the-ml-approach",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "The ML approach",
    "text": "The ML approach\n\nInstead of writing a program by hand for each specific task, we collect lots of examples that specify the correct output for a given input.\nA ML algorithm 🤖 then takes these examples and produces a program that does the job.\n\n]The program produced by the ML algorithm 🤖] may look very different from a typical hand-written program]{.mark}. It may contain millions of numbers 🙀 …\nIf we do it right, the program works for new cases as well as the ones we trained it on.\nIf the data changes the program can change too by training on the new data.\n\nMassive amounts of computation are now cheaper than paying someone to write a task-specific program."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#some-examples-of-tasks-best-solved-by-ml",
    "href": "notes/dnn/dnn-01/l01a.html#some-examples-of-tasks-best-solved-by-ml",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "Some examples of tasks best solved by ML",
    "text": "Some examples of tasks best solved by ML\n\nRecognizing patterns:\n\nObjects in real scenes\nFacial identities or facial expressions\nSpoken words\n\nRecognizing anomalies:\n\nUnusual sequences of credit card transactions\nUnusual patterns of sensor readings in a nuclear power plant1\n\nPrediction:\n\nFuture stock prices or currency exchange rates2.\nWhich movies will a person like3?\n\n\n1 use a program no one can interprest to control a nuclear reactor 😒2 😕 if there be predicted at all3 a recommendation system is probably best for this 😎"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#the-standard-example-of-ml",
    "href": "notes/dnn/dnn-01/l01a.html#the-standard-example-of-ml",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "The standard example of ML",
    "text": "The standard example of ML\n\n\n\n\n\nDrosophila melanogaster Proboscis\n\n\n\nA lot of genetics is done on fruit flies.\n\nThey are convenient because they breed fast.\nWe already know a lot about them.\n\nThe MNIST database of hand-written digits is the the ML equivalent of fruit flies. 🦋\n\nThey are publicly available and we can learn them quite fast in a moderate-sized neural net.\nWe know a huge amount about how well various ML methods do on MNIST.\n\nWe will use MNIST as our standard task."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#beyond-mnist-the-imagenet-task",
    "href": "notes/dnn/dnn-01/l01a.html#beyond-mnist-the-imagenet-task",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "Beyond MNIST — The ImageNet task",
    "text": "Beyond MNIST — The ImageNet task\n\n1000 different object classes in 1.3 million high-resolution training images from the web.\n\nBest system in 2010 competition got 47% error for its first choice and 25% error for its top 5 choices.\n\nJitendra Malik, an eminent neural net sceptic 💀, said that this competition is a good test of whether DNNs 🧠 work well for object recognition.\nA very deep neural net (Krizhevsky, Sutskever, and Hinton 2012) gets less that 40% error for its first choice and less than 20% for its top 5 choices.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” Advances in Neural Information Processing Systems 25.\n\nThe Speech recognition task\n\nA speech recognition system has several stages:\n\nPre-processing: Convert the sound wave into a vector of acoustic coefficients. Extract a new vector about every 10 mille seconds.\nThe acoustic model: Use a few adjacent vectors of acoustic coefficients to place bets on which part of which phoneme is being spoken.\nDecoding: Find the sequence of bets that does the best job of fitting the acoustic data and also fitting a model of the kinds of things people say.\n\nDeep neural networks pioneered by George Dahl and Abdel-rahman Mohamed are now replacing the previous ML method for the acoustic model."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01a.html#phone-recognition-on-the-timit-benchmark",
    "href": "notes/dnn/dnn-01/l01a.html#phone-recognition-on-the-timit-benchmark",
    "title": "Deep Neural Networks - Notes for lecture 1a",
    "section": "Phone recognition on the TIMIT benchmark",
    "text": "Phone recognition on the TIMIT benchmark\n He discusses work from from Mohamed, Dahl, and Hinton (2012)\nMohamed, Abdel-rahman, George E. Dahl, and Geoffrey Hinton. 2012. “Acoustic Modeling Using Deep Belief Networks.” IEEE Transactions on Audio, Speech, and Language Processing 20 (1): 14–22. https://doi.org/10.1109/TASL.2011.2109382.\n\n\nAfter standard post-processing using a bi-phone model, a deep net with 8 layers gets 20.7% error rate.\nThe best previous speaker independent result on TIMIT 4 was 24.4% and this required averaging several models.5\nNLP researcher Li Deng at Microsoft Research realized that this result could change the way speech recognition was done.\n\n\n\n4 is a corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. Each transcribed element has been delineated in time. TIMIT was designed to further acoustic-phonetic knowledge and automatic speech recognition systems5 this is a massive jump in SOTA performance\nDrosophila melanogaster Proboscis"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03c.html",
    "href": "notes/dnn/dnn-03/l03c.html",
    "title": "Deep Neural Networks - Notes for lecture 3c",
    "section": "",
    "text": "Logistic neurons AKA linear filters - useful to understand the algorithm but in reality we need to use non linear activation function.\n\n\nThese give a real-valued output that is a smooth and bounded function of their total input. They have nice derivatives which make learning easy.\n\nz = b + \\sum _i x_i w_i\n\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\n\n\nlogistic activation function\n\n\n\n\n\n\nThe derivatives of the logit, z, with respect to the inputs and the weights are very simple:\n\nz = b + \\sum _i x_i w_i \\tag{the logit}\n\n\n\\frac{\\partial z}{\\partial w_i} = x_i \\;\\;\\;\\;\\; \\frac{\\partial z}{\\partial x_i} = w_i\n\nThe derivative of the output with respect to the logit is simple if you express it in terms of the output:\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\\frac{d y}{d z} = y( 1-y)\n\nsince\n\ny = \\frac{1}{1+e^{-z}}=(1+e^{-z})^{-1}\n differentiating  \\frac{d y}{d z} = \\frac{-1(-e^{-z})}{(1+e^{-z})^2} =\\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}  = y( 1-y)  Using the chain rule to get the derivatives needed for learning the weights of a logistic unit To learn the weights we need the derivative of the output with respect to each weight:\n\n\\frac{d y}{\\partial w_i}  =\\frac{\\partial z}{\\partial w_i} \\frac{dy}{dz}  = x_iy( 1-y)\n\n\n\\frac{d E}{\\partial w_i}  = \\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n} = - \\sum {\\color{green}{x_i^n}}{\\color{red}{ y^n( 1-y^n)}}{\\color{green}{(t^n-y^n)}}\n\nwhere the green part corresponds to the delta rule and the extra term in red is simply the slope of the logistic.\nThe error function is still:\n\nE =\\frac{1}{2}(y−t)^2\n\nNotice how after Hinton explained what the derivative is for a logistic unit, he considers the job to be done. That’s because the learning rule is always simply some learning rate multiplied by the derivative.\n\n\n\nlogistic activation function"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03c.html#logistic-neurons",
    "href": "notes/dnn/dnn-03/l03c.html#logistic-neurons",
    "title": "Deep Neural Networks - Notes for lecture 3c",
    "section": "",
    "text": "These give a real-valued output that is a smooth and bounded function of their total input. They have nice derivatives which make learning easy.\n\nz = b + \\sum _i x_i w_i\n\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\n\n\nlogistic activation function"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03c.html#the-derivatives-of-a-logistic-neuron",
    "href": "notes/dnn/dnn-03/l03c.html#the-derivatives-of-a-logistic-neuron",
    "title": "Deep Neural Networks - Notes for lecture 3c",
    "section": "",
    "text": "The derivatives of the logit, z, with respect to the inputs and the weights are very simple:\n\nz = b + \\sum _i x_i w_i \\tag{the logit}\n\n\n\\frac{\\partial z}{\\partial w_i} = x_i \\;\\;\\;\\;\\; \\frac{\\partial z}{\\partial x_i} = w_i\n\nThe derivative of the output with respect to the logit is simple if you express it in terms of the output:\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\\frac{d y}{d z} = y( 1-y)\n\nsince\n\ny = \\frac{1}{1+e^{-z}}=(1+e^{-z})^{-1}\n differentiating  \\frac{d y}{d z} = \\frac{-1(-e^{-z})}{(1+e^{-z})^2} =\\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}  = y( 1-y)  Using the chain rule to get the derivatives needed for learning the weights of a logistic unit To learn the weights we need the derivative of the output with respect to each weight:\n\n\\frac{d y}{\\partial w_i}  =\\frac{\\partial z}{\\partial w_i} \\frac{dy}{dz}  = x_iy( 1-y)\n\n\n\\frac{d E}{\\partial w_i}  = \\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n} = - \\sum {\\color{green}{x_i^n}}{\\color{red}{ y^n( 1-y^n)}}{\\color{green}{(t^n-y^n)}}\n\nwhere the green part corresponds to the delta rule and the extra term in red is simply the slope of the logistic.\nThe error function is still:\n\nE =\\frac{1}{2}(y−t)^2\n\nNotice how after Hinton explained what the derivative is for a logistic unit, he considers the job to be done. That’s because the learning rule is always simply some learning rate multiplied by the derivative.\n\n\n\nlogistic activation function"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03b.html",
    "href": "notes/dnn/dnn-03/l03b.html",
    "title": "Deep Neural Networks - Notes for lecture 3b",
    "section": "",
    "text": "error surface of a linear neuron\n\n\n\n\nThe error surface lies in a space with a horizontal axis for each weight and one vertical axis for the error.\n\nFor a linear neuron with a squared error, it is a quadratic bowl.\nVertical cross-sections are parabolas.\nHorizontal cross-sections are ellipses.\n\nFor multi-layer, non-linear nets the error surface is much more complicated.\n\n\n\n\n\n\n\n\nOnline v.s. batch learning\n\n\n\n\n\n\n\n\n\n\nWhy learning can be slow\n\n\n\nWhen the ellipse is elongated, the direction of steepest descent is almost perpendicular to the direction towards the minimum!\nThe red gradient vector has a large component along the short axis of the ellipse and a small component along the long axis of the ellipse.\nThis is just the opposite of what we want.\n\n\n\n\nerror surface of a linear neuron\nOnline v.s. batch learning\nWhy learning can be slow"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03b.html#online-versus-batch-learning",
    "href": "notes/dnn/dnn-03/l03b.html#online-versus-batch-learning",
    "title": "Deep Neural Networks - Notes for lecture 3b",
    "section": "",
    "text": "Online v.s. batch learning"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03b.html#why-learning-can-be-slow",
    "href": "notes/dnn/dnn-03/l03b.html#why-learning-can-be-slow",
    "title": "Deep Neural Networks - Notes for lecture 3b",
    "section": "",
    "text": "Why learning can be slow\n\n\n\nWhen the ellipse is elongated, the direction of steepest descent is almost perpendicular to the direction towards the minimum!\nThe red gradient vector has a large component along the short axis of the ellipse and a small component along the long axis of the ellipse.\nThis is just the opposite of what we want.\n\n\n\n\nerror surface of a linear neuron\nOnline v.s. batch learning\nWhy learning can be slow"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html",
    "href": "notes/dnn/dnn-03/l03a.html",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Lecure video\n\n\n\nRecall: by convexity, the Perceptron convergence algorithm guarantees that each time the weights change, they get closer to every generously feasible set of weights. 😄\n\nThis guarantee cannot be extended to more complex networks which wights are non-convex, i.e. the average of two good solutions may be a bad solution. 1 😢\n\nmulti-layer neural networks cannot use the perceptron learning procedure.\n\nThey should never have been called multi-layer perceptrons. 2\n\n\n1 a convex set includes all weighted sums2 no one calls them that anymore\n\n\n\nInstead of showing the weights get closer to a good set of weights, show that the actual output values get closer the target values.\n\nThis can be true even for non-convex problems in which there are many quite different sets of weights that work well and averaging two good sets of weights may give a bad set of weights.\nIt is not true for perceptron learning.\n\nThe simplest example is a linear neuron with a squared error measure.\n\n\n\n\n\nCalled linear filters in electrical engineering and linear transforms in linear algebra and can be represented by martracies\nWe don’t use linear neurons in practice:\n\nWithout a non-linearity in the unit, a stack of N layers can be replaced by a single layer 3\nThis lecture just demonstrates the analysis we will use with non-linear units.\n\nThe neuron’s output is the real valued weighted sum of its inputs\nThe goal of learning is to minimize the total error over all training cases.\n\nHere error is the squared difference between the desired output and the actual output.\n\n\n3 think multiplying N-matracies just gives a single matrix \n{\\color{green}{\\overbrace{y}^{\\text{output}}}} = \\sum_{n \\in train} {\\color{red}{\\overbrace{w_i}^{\\text{weights}}}} {\\color{blue}{\\underbrace{x_i}_{\\text{inputs}}}}= \\vec{w}^T\\cdot\\vec{x}\n where:\n\ny is the neuron’s estimate of the desired output\nx is the input vector\nw is the weight vector\n\n\n\n\n\nIt is straight-forward to write down a set of equations, one per training case, and to solve for the best set of weights.\nThis is the standard engineering approach so why don’t we use it?\nScientific answer: We want a method that real neurons could use.\nEngineering answer: We want a method that can be generalized to multi-layer, non-linear neural networks.\nThe analytic solution relies on it being linear and having a squared error measure.\nIterative methods are usually less efficient but they are much easier to generalize.\n\n\n\n\nEach day you get lunch at the cafeteria.\n\nYour diet consists of fish, chips, and ketchup.\nYou get several portions of each.\n\nThe cashier only tells you the total price of the meal\n\nAfter several days, you should be able to figure out the price of each portion.\n\nThe iterative approach: Start with random guesses for the prices and then adjust them to get a better fit to the observed prices of whole meals.\n\n\n\n\n\nEach meal price gives a linear constraint on the prices of the portions: \n\\text{price} = X_\\text{fish} W_\\text{fish} + X_\\text{chips} W_\\text{chips} + X_\\text{ketchup}W_\\text{ketchup}      \n\n\nThe prices of the portions are like the weights in of a linear neuron. \nW = (w_\\text{fish} , W_\\text{ chips} , W_\\text{ketchup} )\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\n\n\n\n\n\n\nthe true weights\n\n\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\n\n\n\n\n\na toy problem\n\n\n\nResidual error = 350\nThe “delta-rule” for learning is: \\Delta w_i = \\epsilon x_i (t - y)\nWith a learning rate \\epsilon of 1/35, the weight changes are:+20, +50, +30\nThis gives new weights of: 70, 100, 80.\nThe weight for chips got worse, but over all the weights are better\n\ny reducing errors, individual weight estimate may be getting worse\nCalculating the change in the weights:\ncalculate our output using forward propagation\n\n\n\n\ny = \\sum_{n \\in train} w_i x_i= \\vec{w}^T\\vec{x}\n Define the error as the squared residuals summed over all training cases:\n\nE = \\frac{1}{2}\\sum_{n \\in train} (t_n−y_n)^2\n\nuse the chain rule to get error derivatives for weights\n\n\\frac{d E}{\\partial w_i}=\\frac{1}{2}\\sum_{n \\in train}\\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n}=\\frac{1}{2}\\sum_{n \\in train}x_i^n(t^n−y^n)\n\nthe batch delta rule changes the weight in proportion to their error derivative summed on all training cases times the learning rate\n\n\\Delta w_i = −\\epsilon \\frac{d E}{\\partial w_i} = \\sum_{n \\in train} \\epsilon x_i^n (t^n−y^n)\n\n\n\n\n\n\nDoes the learning procedure eventually get the right answer?\n\nThere may be no perfect answer.\nBy making the learning rate small enough we can get as close as we desire to the best answer.\n\nHow quickly do the weights converge to their correct values?\n\nIt can be very slow if two input dimensions are highly correlated. If you almost always have the same number of portions of ketchup and chips, it is hard to decide how to divide the price between ketchup and chips\n\n\n\n\n\n\nIn perceptron learning, we increment or decrement the weight vector by the input vector.\n\nBut we only change the weights when we make an error.\n\nIn the online version of the delta-rule we increment or decrement the weight vector by the input vector scaled by the residual error and the learning rate.\n\nSo we have to choose a learning rate. This is annoying\n\n\n\nresidual error\n\nit’s the amount by which we got the answer wrong.\n\n\nA very central concept is introduced without being made very explicit: we use derivatives for learning, i.e. for making the weights better. Try to understand why those concepts are indeed very related.\n\non-line learning\n\nmeans that we change the weights after every training example that we see, and we typically cycle through the collection of available training examples.\n\n\n\n\n\nthe true weights\na toy problem"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#why-the-perceptron-learning-procedure-cannot-be-generalised-to-hidden-layers",
    "href": "notes/dnn/dnn-03/l03a.html#why-the-perceptron-learning-procedure-cannot-be-generalised-to-hidden-layers",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Recall: by convexity, the Perceptron convergence algorithm guarantees that each time the weights change, they get closer to every generously feasible set of weights. 😄\n\nThis guarantee cannot be extended to more complex networks which wights are non-convex, i.e. the average of two good solutions may be a bad solution. 1 😢\n\nmulti-layer neural networks cannot use the perceptron learning procedure.\n\nThey should never have been called multi-layer perceptrons. 2\n\n\n1 a convex set includes all weighted sums2 no one calls them that anymore"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#a-different-way-to-show-that-a-learning-procedure-makes-progress",
    "href": "notes/dnn/dnn-03/l03a.html#a-different-way-to-show-that-a-learning-procedure-makes-progress",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Instead of showing the weights get closer to a good set of weights, show that the actual output values get closer the target values.\n\nThis can be true even for non-convex problems in which there are many quite different sets of weights that work well and averaging two good sets of weights may give a bad set of weights.\nIt is not true for perceptron learning.\n\nThe simplest example is a linear neuron with a squared error measure."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#linear-neurons",
    "href": "notes/dnn/dnn-03/l03a.html#linear-neurons",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Called linear filters in electrical engineering and linear transforms in linear algebra and can be represented by martracies\nWe don’t use linear neurons in practice:\n\nWithout a non-linearity in the unit, a stack of N layers can be replaced by a single layer 3\nThis lecture just demonstrates the analysis we will use with non-linear units.\n\nThe neuron’s output is the real valued weighted sum of its inputs\nThe goal of learning is to minimize the total error over all training cases.\n\nHere error is the squared difference between the desired output and the actual output.\n\n\n3 think multiplying N-matracies just gives a single matrix \n{\\color{green}{\\overbrace{y}^{\\text{output}}}} = \\sum_{n \\in train} {\\color{red}{\\overbrace{w_i}^{\\text{weights}}}} {\\color{blue}{\\underbrace{x_i}_{\\text{inputs}}}}= \\vec{w}^T\\cdot\\vec{x}\n where:\n\ny is the neuron’s estimate of the desired output\nx is the input vector\nw is the weight vector"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#why-dont-we-solve-it-analytically",
    "href": "notes/dnn/dnn-03/l03a.html#why-dont-we-solve-it-analytically",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "It is straight-forward to write down a set of equations, one per training case, and to solve for the best set of weights.\nThis is the standard engineering approach so why don’t we use it?\nScientific answer: We want a method that real neurons could use.\nEngineering answer: We want a method that can be generalized to multi-layer, non-linear neural networks.\nThe analytic solution relies on it being linear and having a squared error measure.\nIterative methods are usually less efficient but they are much easier to generalize.\n\n\n\n\nEach day you get lunch at the cafeteria.\n\nYour diet consists of fish, chips, and ketchup.\nYou get several portions of each.\n\nThe cashier only tells you the total price of the meal\n\nAfter several days, you should be able to figure out the price of each portion.\n\nThe iterative approach: Start with random guesses for the prices and then adjust them to get a better fit to the observed prices of whole meals.\n\n\n\n\n\nEach meal price gives a linear constraint on the prices of the portions: \n\\text{price} = X_\\text{fish} W_\\text{fish} + X_\\text{chips} W_\\text{chips} + X_\\text{ketchup}W_\\text{ketchup}      \n\n\nThe prices of the portions are like the weights in of a linear neuron. \nW = (w_\\text{fish} , W_\\text{ chips} , W_\\text{ketchup} )\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\n\n\n\n\n\n\nthe true weights\n\n\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\n\n\n\n\n\na toy problem\n\n\n\nResidual error = 350\nThe “delta-rule” for learning is: \\Delta w_i = \\epsilon x_i (t - y)\nWith a learning rate \\epsilon of 1/35, the weight changes are:+20, +50, +30\nThis gives new weights of: 70, 100, 80.\nThe weight for chips got worse, but over all the weights are better\n\ny reducing errors, individual weight estimate may be getting worse\nCalculating the change in the weights:\ncalculate our output using forward propagation\n\n\n\n\ny = \\sum_{n \\in train} w_i x_i= \\vec{w}^T\\vec{x}\n Define the error as the squared residuals summed over all training cases:\n\nE = \\frac{1}{2}\\sum_{n \\in train} (t_n−y_n)^2\n\nuse the chain rule to get error derivatives for weights\n\n\\frac{d E}{\\partial w_i}=\\frac{1}{2}\\sum_{n \\in train}\\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n}=\\frac{1}{2}\\sum_{n \\in train}x_i^n(t^n−y^n)\n\nthe batch delta rule changes the weight in proportion to their error derivative summed on all training cases times the learning rate\n\n\\Delta w_i = −\\epsilon \\frac{d E}{\\partial w_i} = \\sum_{n \\in train} \\epsilon x_i^n (t^n−y^n)"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#behaviour-of-the-iterative-learning-procedure",
    "href": "notes/dnn/dnn-03/l03a.html#behaviour-of-the-iterative-learning-procedure",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "Does the learning procedure eventually get the right answer?\n\nThere may be no perfect answer.\nBy making the learning rate small enough we can get as close as we desire to the best answer.\n\nHow quickly do the weights converge to their correct values?\n\nIt can be very slow if two input dimensions are highly correlated. If you almost always have the same number of portions of ketchup and chips, it is hard to decide how to divide the price between ketchup and chips"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03a.html#the-relationship-between-the-online-delta-rule-and-the-learning-rule-for-perceptrons",
    "href": "notes/dnn/dnn-03/l03a.html#the-relationship-between-the-online-delta-rule-and-the-learning-rule-for-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 3a",
    "section": "",
    "text": "In perceptron learning, we increment or decrement the weight vector by the input vector.\n\nBut we only change the weights when we make an error.\n\nIn the online version of the delta-rule we increment or decrement the weight vector by the input vector scaled by the residual error and the learning rate.\n\nSo we have to choose a learning rate. This is annoying\n\n\n\nresidual error\n\nit’s the amount by which we got the answer wrong.\n\n\nA very central concept is introduced without being made very explicit: we use derivatives for learning, i.e. for making the weights better. Try to understand why those concepts are indeed very related.\n\non-line learning\n\nmeans that we change the weights after every training example that we see, and we typically cycle through the collection of available training examples.\n\n\n\n\n\nthe true weights\na toy problem"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html",
    "href": "notes/dnn/dnn-05/l05c.html",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-replicated-feature-approach-currently-the-dominant-approach-for-neural-networks",
    "href": "notes/dnn/dnn-05/l05c.html#the-replicated-feature-approach-currently-the-dominant-approach-for-neural-networks",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The replicated feature approach (currently the dominant approach for neural networks)",
    "text": "The replicated feature approach (currently the dominant approach for neural networks)\n\n\n\n\nUse many different copies of the same feature detector with different positions.\n\nCould also replicate across scale and orientation (tricky and expensive)\nReplication greatly reduces the number of free parameters to be learned.\n\nUse several different feature types, each with its own map of replicated detectors.\n\nAllows each patch of image to be represented in several ways."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#backpropagation-with-weight-constraints",
    "href": "notes/dnn/dnn-05/l05c.html#backpropagation-with-weight-constraints",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "Backpropagation with weight constraints",
    "text": "Backpropagation with weight constraints\n\nIt’s easy to modify the backpropagation algorithm to incorporate linear constraints between the weights.\n\nWe compute the gradients as usual, and then modify the gradients so that they satisfy the constraints.\n\nSo if the weights started off satisfying the constraints, they will continue to satisfy them."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#what-does-replicating-the-feature-detectors-achieve",
    "href": "notes/dnn/dnn-05/l05c.html#what-does-replicating-the-feature-detectors-achieve",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "What does replicating the feature detectors achieve?",
    "text": "What does replicating the feature detectors achieve?\n\n\n \n\nEquivariant activities: Replicated features do not make the neural activities invariant to translation. The activities are equivariant.\nInvariant knowledge: If a feature is useful in some locations during training, detectors for that feature will be available in all locations during testing."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#pooling-the-outputs-of-replicated-feature-detectors",
    "href": "notes/dnn/dnn-05/l05c.html#pooling-the-outputs-of-replicated-feature-detectors",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "Pooling the outputs of replicated feature detectors",
    "text": "Pooling the outputs of replicated feature detectors\n• Get a small amount of translational invariance at each level by averaging four neighboring replicated detectors to give a single output to the next level. – This reduces the number of inputs to the next layer of feature extraction, thus allowing us to have many more different feature maps. – Taking the maximum of the four works slightly better. • Problem: After several levels of pooling, we have lost information about the precise positions of things. – This makes it impossible to use the precise spatial relationships between high-level parts for recognition"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-architecture-of-lenet5",
    "href": "notes/dnn/dnn-05/l05c.html#the-architecture-of-lenet5",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The architecture of LeNet5",
    "text": "The architecture of LeNet5\n\n\n\nThe architecture of LeNet5"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-82-errors-made-by-lenet5",
    "href": "notes/dnn/dnn-05/l05c.html#the-82-errors-made-by-lenet5",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The 82 errors made by LeNet5",
    "text": "The 82 errors made by LeNet5\n\n\n\nerrors made by LeNet5\n\n\nNotice that most of the errors are cases that people find quite easy.\nThe human error rate is probably 20 to 30 errors but nobody has had the patience to measure it."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#priors-and-prejudice",
    "href": "notes/dnn/dnn-05/l05c.html#priors-and-prejudice",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "Priors and Prejudice",
    "text": "Priors and Prejudice\n\nWe can put our prior knowledge about the task into the network by designing appropriate:\n\nConnectivity.\nWeight constraints.\nNeuron activation functions\n\nThis is less intrusive than handdesigning the features.\n\nBut it still prejudices the network towards the particular way of solving the problem that we had in mind.\n\nAlternatively, we can use our prior knowledge to create a whole lot more training data.\n\nThis may require a lot of work (Hofman&Tresp, 1993)\nIt may make learning take much longer.\n\nIt allows optimization to discover clever ways of using the multi-layer network that we did not think of.\n\nAnd we may never fully understand how it does it."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-brute-force-approach",
    "href": "notes/dnn/dnn-05/l05c.html#the-brute-force-approach",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The brute force approach",
    "text": "The brute force approach\n\nLeNet uses knowledge about the invariances to design:\n\nthe local connectivity\nthe weight-sharing\nthe pooling.\n\nThis achieves about 80 errors.\n\nThis can be reduced to about 40 errors by using many different transformations of the input and other tricks (Ranzato 2008)\n\nCiresan et. al. (2010) inject knowledge of invariances by creating a huge amount of carefully designed extra training data:\n\nFor each training image, they produce many new training examples by applying many different transformations.\nThey can then train a large, deep, dumb net on a GPU without much overfitting.\n\nThey achieve about 35 errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#the-errors-made-by-the-ciresan-et.-al.-net",
    "href": "notes/dnn/dnn-05/l05c.html#the-errors-made-by-the-ciresan-et.-al.-net",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "The errors made by the Ciresan et. al. net",
    "text": "The errors made by the Ciresan et. al. net\n\n\n\nerrors made by the Ciresan\n\n\nThe top printed digit is the right answer. The bottom two printed digits are the network’s best two guesses.\nThe right answer is almost always in the top 2 guesses.\nWith model averaging they can now get about 25 errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05c.html#how-to-detect-a-significant-drop-in-the-error-rate",
    "href": "notes/dnn/dnn-05/l05c.html#how-to-detect-a-significant-drop-in-the-error-rate",
    "title": "Deep Neural Networks - Notes for lecture 5c",
    "section": "How to detect a significant drop in the error rate",
    "text": "How to detect a significant drop in the error rate\n\n\n\n\n\nMcNemar test 1\n\n\n\n\n\nMcNemar test 1\n\n\n\nIs 30 errors in 10,000 test cases significantly beHer than 40 errors?\n\nIt all depends on the particular errors!\nThe McNemar test uses the particular errors and can be much more powerful than a test that just uses the number of errors.\n\n\n\n\n\nThe architecture of LeNet5\nerrors made by LeNet5\nerrors made by the Ciresan\nMcNemar test 1\nMcNemar test 1"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html",
    "href": "notes/dnn/dnn-05/l05b.html",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html#some-ways-to-achieve-viewpoint-invariance",
    "href": "notes/dnn/dnn-05/l05b.html#some-ways-to-achieve-viewpoint-invariance",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "Some ways to achieve viewpoint invariance",
    "text": "Some ways to achieve viewpoint invariance\n\nWe are so good at viewpoint invariance that it is hard to appreciate how difficult it is.\n\nIts one of the main difficulties in making computers perceive.\nWe still don’t have generally accepted solutions.\n\nThere are several different approaches:\n\nUse redundant invariant features.\nPut a box around the object and use normalized pixels.\n\nLecture 5c: Use replicated features with pooling. This is called “convolutional neural nets”\nUse a hierarchy of parts that have explicit poses relative to the camera (this will be described in detail later in the course)."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html#the-invariant-feature-approach",
    "href": "notes/dnn/dnn-05/l05b.html#the-invariant-feature-approach",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "The invariant feature approach",
    "text": "The invariant feature approach\n\nExtract a large, redundant set of features that are invariant under transformations\n\ne.g. pair of roughly parallel lines with a red dot between them.\nThis is what baby herring gulls use to know where to peck for food.\n\nWith enough invariant features, there is only one way to assemble them into an object.\n\nWe don’t need to represent the relationships between features directly because they are captured by other features.\n\nFor recognition, we must avoid forming features from parts of different objects."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html#the-judicious-normalization-approach",
    "href": "notes/dnn/dnn-05/l05b.html#the-judicious-normalization-approach",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "The judicious normalization approach",
    "text": "The judicious normalization approach\n\nPut a box around the object and use it as a coordinate frame for a set of normalized pixels.\n\nThis solves the dimension-hopping problem. If we choose the box correctly, the same part of an object always occurs on the same normalized pixels.\n\nThe box can provide invariance to many degrees of freedom: translation, rotation, scale, shear, stretch …\nBut choosing the box is difficult because of:\nSegmentation errors, occlusion, unusual orientations.\n\nWe need to recognize the shape to get the box right!"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05b.html#the-brute-force-normalization-approach",
    "href": "notes/dnn/dnn-05/l05b.html#the-brute-force-normalization-approach",
    "title": "Deep Neural Networks - Notes for lecture 5b",
    "section": "The brute force normalization approach",
    "text": "The brute force normalization approach\n\nWhen training the recognizer, use well-segmented, upright images to fit the correct box.\nAt test time try all possible boxes in a range of positions and scales.\n\nThis approach is widely used for detecting upright things like faces and house numbers in unsegmented images.\n\nIt is much more efficient if the recognizer can cope with some variation in position and scale so that we can use a coarse grid when trying all possible boxes."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html",
    "href": "notes/dnn/dnn-11/l_11.html",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11a-hopfield-nets",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11a-hopfield-nets",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11a: Hopfield Nets",
    "text": "Lecture 11a: Hopfield Nets\n(Hopfield 1982)\n\nHopfield, John J. 1982. “Neural Networks and Physical Systems with Emergent Collective Computational Abilities.” Proceedings of the National Academy of Sciences 79 (8): 2554–58.\nNow, we leave behind the feedforward deterministic networks that are trained with backpropagation gradients. We’re going to see quite a variety of different neural networks now. These networks do not have output units. These networks have units that can only be in states 0 and 1. These networks do not have units of which the state is simply a function of the state of other units. These networks are, instead, governed by an “energy function”. Best way to really understand Hopfield networks: Go through the example of the Hopfield network finding a low energy state, by yourself. Better yet, think of different weights, and do the exercise with those. Typically, we’ll use Hopfield networks where the units have state 0 or 1; not -1 or 1."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11b-dealing-with-spurious-minima",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11b-dealing-with-spurious-minima",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11b: Dealing with spurious minima",
    "text": "Lecture 11b: Dealing with spurious minima\nThe last in-video question is not easy. Try to understand how the perceptron learning procedure is used in a Hopfield net; it’s not very thoroughly explained."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11c-hopfield-nets-with-hidden-units",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11c-hopfield-nets-with-hidden-units",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11c: Hopfield nets with hidden units",
    "text": "Lecture 11c: Hopfield nets with hidden units\nThis video introduces some sophisticated concepts, and is not entirely easy. An “excitatory connection” is a connection of which the weight is positive. “inhibitory”, likewise, means a negative weight. We look for an energy minimum, “given the state of the visible units”. That means that we look for a low energy configuration, and we’ll consider only configurations in which the visible units are in the state that’s specified by the data. So we’re only going to consider flipping the states of the hidden units. Be sure to really understand the last two sentences that Geoffrey speaks in this video."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11d-using-stochastic-units-to-improve-search",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11d-using-stochastic-units-to-improve-search",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11d: Using stochastic units to improve search",
    "text": "Lecture 11d: Using stochastic units to improve search\nWe’re still working with a mountain landscape analogy. This time, however, it’s not an analogy for parameter space, but for state space. A particle is, therefore, not a weight vector, but a configuration. What’s the same is that we’re, in a way, looking for low points in the landscape. We’re also using the physics analogy of systems that can be in different states, each with their own energy, and subject to a temperature. This analogy is introduced in slide 2. This is the analogy that originally inspired Hopfield networks. The idea is that at a high temperature, the system is more inclined to transition into configurations with high energy, even though it still prefers low energy. 3:25: “the amount of noise” means the extent to which the decisions are random. 4:20: If T really were 0, we’d have division by zero, which is not good. What we really mean here is “as T gets really, really small (but still positive)”. For mathematicians: it’s the limit as T goes to zero from above. Thermal equilibrium, and this whole random process of exploring states, is much like the exploration of weight vectors that we can use in Bayesian methods. It’s called a Markov Chain, in both cases."
  },
  {
    "objectID": "notes/dnn/dnn-11/l_11.html#lecture-11e-how-a-boltzmann-machine-models-data",
    "href": "notes/dnn/dnn-11/l_11.html#lecture-11e-how-a-boltzmann-machine-models-data",
    "title": "Deep Neural Networks - Notes for Lesson 11",
    "section": "Lecture 11e: How a Boltzmann machine models data",
    "text": "Lecture 11e: How a Boltzmann machine models data\nNow, we’re making a generative model of binary vectors. In contrast, mixtures of Gaussians are a generative model of real-valued vectors. 4:38: Try to understand how a mixture of Gaussians is also a causal generative model. 4:58: A Boltzmann Machine is an energy-based generative model. 5:50: Notice how this is the same as the earlier definition of energy. What’s new is that it’s mentioning visible and hidden units separately, instead of treating all units the same way."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html",
    "href": "notes/dnn/dnn-04/l_04a.html",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "relational information\n\n\n\n\n\n\nMake a set of propositions using the 12 relationships:\n\nson, daughter, nephew, niece, father, mother, uncle, aunt\nbrother, sister, husband, wife\n\n(Colin has-father James)\n(Colin has-mother Victoria)\n(James has-wife Victoria) this follows from the two above\n(Charlotte has-brother Colin)\n(Victoria has-brother Arthur)\n(Charlotte has-uncle Arthur) this follows from the above\n\n\n\n\n\nGiven a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules (x has-mother y) & (y has-husband z) =&gt; (x has-father z)\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\nCan a neural network capture the same knowledge by searching through a continuous space of weights?\n\n\n\n\n\n\n\n\n\nstructure of the neural net\n\n\n\n\n\n\nvisulization of 6 neuron weights\n\n\n\n\n\nthe relational data\n\n\n\n\n\n\n\nThe six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\n\nNationality, generation, branch of the family tree.\n\nThese features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example:\n\nInput person is of generation 3 and\nrelationship requires answer to be one generation up\nimplies\nOutput person is of generation 2\n\n\n\n\n\n\nTrain the network on all but 4 of the triples that can be made using the 12 relationships\n\nIt needs to sweep through the training set many times adjusting the weights slightly each time.\n\nThen test it on the 4 held-out cases.\n\nIt gets about 3/4 correct.\nThis is good for a 24-way choice.\nOn much bigger datasets we can train on a much smaller fraction of the data.\n\n\n\n\n\n\nSuppose we have a database of millions of relational facts of the form (A R B).\n\nWe could train a net to discover feature vector representations of the terms that allow the third term to be predicted from the first two.\nThen we could use the trained net to find very unlikely triples. These are good candidates for errors in the database.\n\nInstead of predicting the third term, we could use all three terms as input and predict the probability that the fact is correct.\n\nTo train such a net we need a good source of false facts.\n\n\n\n\n\n\nGiven a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules:\n\nHasMother(x,y)\\ and\\ HasHusband(y,z) \\implies HasFather(x, z)\n\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\n\nCan a neural network capture the same knowledge by searching through a continuous space of weights?\n\n\n\n\n\nThe six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\nNationality, generation, branch of the family tree. These features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example: Input person is of generation 3 and relationship requires answer to be one generation up implies Output person is of generation 2 This video introduces distributed representations. It’s not actually about predicting words, but it’s building up to that. It does a great job of looking inside the brain of a neural network. That’s important, but not always easy to do.\n\n\n\n\nrelational information\nstructure of the neural net\nvisulization of 6 neuron weights\nthe relational data"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#a-simple-example-of-relational-information",
    "href": "notes/dnn/dnn-04/l_04a.html#a-simple-example-of-relational-information",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "relational information"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#another-way-to-express-the-same-information",
    "href": "notes/dnn/dnn-04/l_04a.html#another-way-to-express-the-same-information",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Make a set of propositions using the 12 relationships:\n\nson, daughter, nephew, niece, father, mother, uncle, aunt\nbrother, sister, husband, wife\n\n(Colin has-father James)\n(Colin has-mother Victoria)\n(James has-wife Victoria) this follows from the two above\n(Charlotte has-brother Colin)\n(Victoria has-brother Arthur)\n(Charlotte has-uncle Arthur) this follows from the above"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#a-relational-learning-task",
    "href": "notes/dnn/dnn-04/l_04a.html#a-relational-learning-task",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Given a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules (x has-mother y) & (y has-husband z) =&gt; (x has-father z)\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\nCan a neural network capture the same knowledge by searching through a continuous space of weights?"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#the-structure-of-the-neural-net",
    "href": "notes/dnn/dnn-04/l_04a.html#the-structure-of-the-neural-net",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "structure of the neural net\n\n\n\n\n\n\nvisulization of 6 neuron weights\n\n\n\n\n\nthe relational data"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#what-the-network-learns",
    "href": "notes/dnn/dnn-04/l_04a.html#what-the-network-learns",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "The six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\n\nNationality, generation, branch of the family tree.\n\nThese features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example:\n\nInput person is of generation 3 and\nrelationship requires answer to be one generation up\nimplies\nOutput person is of generation 2"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#another-way-to-see-that-it-works",
    "href": "notes/dnn/dnn-04/l_04a.html#another-way-to-see-that-it-works",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Train the network on all but 4 of the triples that can be made using the 12 relationships\n\nIt needs to sweep through the training set many times adjusting the weights slightly each time.\n\nThen test it on the 4 held-out cases.\n\nIt gets about 3/4 correct.\nThis is good for a 24-way choice.\nOn much bigger datasets we can train on a much smaller fraction of the data."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#a-large-scale-example",
    "href": "notes/dnn/dnn-04/l_04a.html#a-large-scale-example",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Suppose we have a database of millions of relational facts of the form (A R B).\n\nWe could train a net to discover feature vector representations of the terms that allow the third term to be predicted from the first two.\nThen we could use the trained net to find very unlikely triples. These are good candidates for errors in the database.\n\nInstead of predicting the third term, we could use all three terms as input and predict the probability that the fact is correct.\n\nTo train such a net we need a good source of false facts."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#a-relational-learning-task-1",
    "href": "notes/dnn/dnn-04/l_04a.html#a-relational-learning-task-1",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "Given a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules:\n\nHasMother(x,y)\\ and\\ HasHusband(y,z) \\implies HasFather(x, z)\n\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\n\nCan a neural network capture the same knowledge by searching through a continuous space of weights?"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04a.html#the-structure-of-the-neural-net-1",
    "href": "notes/dnn/dnn-04/l_04a.html#the-structure-of-the-neural-net-1",
    "title": "Deep Neural Networks - Notes for lecture 4a",
    "section": "",
    "text": "The six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\nNationality, generation, branch of the family tree. These features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example: Input person is of generation 3 and relationship requires answer to be one generation up implies Output person is of generation 2 This video introduces distributed representations. It’s not actually about predicting words, but it’s building up to that. It does a great job of looking inside the brain of a neural network. That’s important, but not always easy to do.\n\n\n\n\nrelational information\nstructure of the neural net\nvisulization of 6 neuron weights\nthe relational data"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04d.html",
    "href": "notes/dnn/dnn-04/l_04d.html",
    "title": "Deep Neural Networks - Notes for lecture 4d",
    "section": "",
    "text": "Lecture 4d: Neuro-probabilistic language models\nThis is the first of several applications of neural networks that we’ll studying in some detail, in this course.\nSynonyms: word embedding; word feature vector; word encoding.\nAll of these describe the learned collection of numbers that is used to represent a word. “embedding” emphasizes that it’s a location in a high-dimensional space: it’s where the words are embedded in that space. When we check to see which words are close to each other, we’re thinking about that embedding.\n“feature vector” emphasizes that it’s a vector instead of a scalar, and that it’s componential, i.e. composed of multiple feature values.\n“encoding” is very generic and doesn’t emphasize anything specific. looks at the trigram model\n\nA basic problem in speech recognition\n\nWe cannot identify phonemes perfectly in noisy speech\n\nThe acoustic input is often ambiguous: there are several different words that fit the acoustic signal equally well.\n\nPeople use their understanding of the meaning of the utterance to hear the right words.\n\nWe do this unconsciously when we wreck a nice beach.\nWe are very good at it.\n\nThis means speech recognizers have to know which words are likely to come next and which are not.\n\nFortunately, words can be predicted quite well without full understanding.\n\n\n\n\nThe standard “trigram” method\n\nTake a huge amount of text and count the frequencies of all triples of words.\nUse these frequencies to make bets on the relative probabilities of words given the previous two words:\n\n\n\\frac{p(w_3=c|w_2=b,w_1=a)}{p(w_3=d|w_2=b,w_1=a)}=\\frac{count(abc)}{count(abd)}\n\n\nUntil very recently this was the state-of-the-art.\nWe cannot use a much bigger context because there are too many possibilities to store and the counts would mostly be zero.\nWe have to “back-off” to digrams when the count for a trigram is too small.\n\nThe probability is not zero just because the count is zero!\n\n\n\n\nInformation that the trigram model fails to use\n\nSuppose we have seen the sentence “the cat got squashed in the garden on friday”\nThis should help us predict words in the sentence “the dog got flattened in the yard on monday”\nA trigram model does not understand the similarities between\n\ncat/dog squashed/flattened garden/yard friday/monday\n\nTo overcome this limitation, we need to use the semantic and syntactic features of previous words to predict the features of the next word.\n\nUsing a feature representation also allows a context that contains many more previous words (e.g. 10).\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} for Lecture 4d},\n  date = {2017-08-14},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-04/l_04d.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes for Lecture\n4d.” August 14, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-04/l_04d.html."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04b.html",
    "href": "notes/dnn/dnn-04/l_04b.html",
    "title": "Deep Neural Networks - Notes for lecture 4b",
    "section": "",
    "text": "This video is part of the course, i.e. it’s not optional, despite what Geoff says in the beginning of the video. This video gives a high-level interpretation of what’s going on in the family tree network. This video contrasts two types of inference:\n\nConscious inference, based on relational knowledge.\nUnconscious inference, based on distributed representations.\n\n\n\n• There has been a long debate in cognitive science between two rival theories of what it means to have a concept: The feature theory: A concept is a set of semantic features. – This is good for explaining similarities between concepts. – Its convenient: a concept is a vector of feature activities. The structuralist theory: The meaning of a concept lies in its relationships to other concepts. – So conceptual knowledge is best expressed as a relational graph. – Minsky used the limitations of perceptrons as evidence against feature vectors and in favor of relational graph representations.\n\n\n\n\nThese two theories need not be rivals. A neural net can use vectors of semantic features to implement a relational graph.\n\nIn the neural network that learns family trees, no explicit inference is required to arrive at the intuitively obvious consequences of the facts that have been explicitly learned.\nThe net can “intuit” the answer in a forward pass.\n\nWe may use explicit rules for conscious, deliberate reasoning, but we do a lot of commonsense, analogical reasoning by just “seeing” the answer with no conscious intervening steps.\n\nEven when we are using explicit rules, we need to just see which rules to apply.\n\n\n\n\n\n\nThe obvious way to implement a relational graph in a neural net is to treat a neuron as a node in the graph and a connection as a binary relationship. But this “localist” method will not work:\n\nWe need many different types of relationship and the connections in a neural net do not have discrete labels.\nWe need ternary relationships as well as binary ones. e.g. A is between B and C.\n\nThe right way to implement relational knowledge in a neural net is still an open issue.\n\nBut many neurons are probably used for each concept and each neuron is probably involved in many concepts. This is called a “distributed representation”."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04b.html#what-the-family-trees-example-tells-us-about-concepts",
    "href": "notes/dnn/dnn-04/l_04b.html#what-the-family-trees-example-tells-us-about-concepts",
    "title": "Deep Neural Networks - Notes for lecture 4b",
    "section": "",
    "text": "• There has been a long debate in cognitive science between two rival theories of what it means to have a concept: The feature theory: A concept is a set of semantic features. – This is good for explaining similarities between concepts. – Its convenient: a concept is a vector of feature activities. The structuralist theory: The meaning of a concept lies in its relationships to other concepts. – So conceptual knowledge is best expressed as a relational graph. – Minsky used the limitations of perceptrons as evidence against feature vectors and in favor of relational graph representations."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04b.html#both-sides-are-wrong",
    "href": "notes/dnn/dnn-04/l_04b.html#both-sides-are-wrong",
    "title": "Deep Neural Networks - Notes for lecture 4b",
    "section": "",
    "text": "These two theories need not be rivals. A neural net can use vectors of semantic features to implement a relational graph.\n\nIn the neural network that learns family trees, no explicit inference is required to arrive at the intuitively obvious consequences of the facts that have been explicitly learned.\nThe net can “intuit” the answer in a forward pass.\n\nWe may use explicit rules for conscious, deliberate reasoning, but we do a lot of commonsense, analogical reasoning by just “seeing” the answer with no conscious intervening steps.\n\nEven when we are using explicit rules, we need to just see which rules to apply."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04b.html#localist-and-distributed-representations-of-concepts",
    "href": "notes/dnn/dnn-04/l_04b.html#localist-and-distributed-representations-of-concepts",
    "title": "Deep Neural Networks - Notes for lecture 4b",
    "section": "",
    "text": "The obvious way to implement a relational graph in a neural net is to treat a neuron as a node in the graph and a connection as a binary relationship. But this “localist” method will not work:\n\nWe need many different types of relationship and the connections in a neural net do not have discrete labels.\nWe need ternary relationships as well as binary ones. e.g. A is between B and C.\n\nThe right way to implement relational knowledge in a neural net is still an open issue.\n\nBut many neurons are probably used for each concept and each neuron is probably involved in many concepts. This is called a “distributed representation”."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Notes to myself, answers to questions no botherd to ask\nThis is a space to share my insights about my interests.\nI’ve decided to migrate to Quarto and see what the platform can do."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "Notes to myself, answers to questions no botherd to ask\nThis is a space to share my insights about my interests.\nI’ve decided to migrate to Quarto and see what the platform can do."
  },
  {
    "objectID": "about.html#newsletter",
    "href": "about.html#newsletter",
    "title": "About",
    "section": "Newsletter 💖",
    "text": "Newsletter 💖\nIf you enjoyed this post, then don’t miss out on any future posts by subscribing to my email newsletter"
  },
  {
    "objectID": "about.html#buy-me-coffe",
    "href": "about.html#buy-me-coffe",
    "title": "About",
    "section": "Buy Me Coffe 😁",
    "text": "Buy Me Coffe 😁"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Quarto 💖 Bootstrap 😁\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Migration Notes\n\n\nI have moved this blog from blogger to Jekyl and finaly to quarto. \n\n\n\n\n\nJan 30, 2024\n\n\n\n\n\n\n\nPolitical Scenario Prediction Using Game theory\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nBeacuase auto-ml is a Superpower\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\nWikisym 2012\n\n\nConference Report\n\n\n\n\n\nJul 26, 2022\n\n\n\n\n\n\n\nRegEX\n\n\ncheat sheet\n\n\n\n\n\nNov 25, 2020\n\n\n\n\n\n\n\nA/B testing cost and risks?\n\n\n\n\n\n\n\n\nJul 30, 2017\n\n\n\n\n\n\n\nText Mining With Python\n\n\na number of NLP tasks in Python\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nText Mining With R\n\n\na number of NLP tasks in R\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\nan update on NLP with R\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nBash\n\n\ncheet sheet\n\n\n\n\n\nSep 10, 2011\n\n\n\n\n\n\n\nR Books\n\n\nbooks reviews & recommendations\n\n\n\n\n\nAug 26, 2011\n\n\n\n\n\n\n\nTime management Tips\n\n\nWe could all use a productivity boost\n\n\n\n\n\nAug 11, 2011\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/dnn/dnn-dropout/2017-08-06-dropout.html",
    "href": "notes/dnn/dnn-dropout/2017-08-06-dropout.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "My thoughts are that we should be able to do better than this version of dropout. - Shortcoming: - Dropout on units can render the net very poor. - Drop out slows training down - since we don’t update half the units and probably a large number of the weights. - For different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures. - We should track dropout related stats to better understand the confidence of the model. - A second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout? 1. Start with some combinations \\binom k n of the weights. where k = | {training\\; set}|*{minibatch\\_size}. We use the same dropout for each mini-batch, then switch. 2. Each epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ? 3. When we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net. 4. Ideally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step. 5. We could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism. 6. We should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "notes/dnn/dnn-dropout/2017-08-06-dropout.html#dropout",
    "href": "notes/dnn/dnn-dropout/2017-08-06-dropout.html#dropout",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "My thoughts are that we should be able to do better than this version of dropout. - Shortcoming: - Dropout on units can render the net very poor. - Drop out slows training down - since we don’t update half the units and probably a large number of the weights. - For different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures. - We should track dropout related stats to better understand the confidence of the model. - A second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout? 1. Start with some combinations \\binom k n of the weights. where k = | {training\\; set}|*{minibatch\\_size}. We use the same dropout for each mini-batch, then switch. 2. Each epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ? 3. When we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net. 4. Ideally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step. 5. We could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism. 6. We should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04c.html",
    "href": "notes/dnn/dnn-04/l_04c.html",
    "title": "Deep Neural Networks - Notes for lecture 4c",
    "section": "",
    "text": "A Softmax cost function is a general-purpose ML component/technique for combining binary discriminators into a probability distribution to construct a classifier We’ve seen binary threshold output neurons and logistic output neurons. This video presents a third type.\nThis one only makes sense if we have multiple output neurons.\n\n\n\nThe squared error measure has some drawbacks:\n\nIf the desired output is 1 and the actual output is 0.00000001 there is almost no gradient for a logistic unit to fix up the error.\nIf we are trying to assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but we are depriving the network of this knowledge.\n\nIs there a different cost function that works better?\n\nYes: Force the outputs to represent a probability distribution across discrete alternatives\n\n\n\n\n\nThe output units in a softmax group use a non-local non-linearity:\n\ny_i = \\frac{e^{z_i}}{\\sum_{j\\in group} e^{z_i}}\n\n\n\\frac{\\partial y_i}{\\partial z_i} = y_i(1-y_i)\n\n\n\n\n\n\n\n\nC=-\\sum_j t_j \\log y_i\n \n\\frac {\\partial C}{\\partial z_i} = - \\sum_j t_j \\frac {\\partial C}{\\partial y_i} \\frac {\\partial y_u}{\\partial z_i} = y_i -t_i\n\n\nThe right cost function is the negative log probability of the right answer.\nC has a very big gradient when the target value is 1 and the output is almost zero.\n\nA value of 0.000001 is much better than 0.000000001\nThe steepness of dC/dy exactly balances the flatness of dy/dz\n\n\nthe cross entropy cost function - is the correct cost function to use with SoftMax\nArchitectural Note:\nSoftMax unit +Cross-Entropy loss function =&gt; for classification"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04c.html#problems-with-squared-error",
    "href": "notes/dnn/dnn-04/l_04c.html#problems-with-squared-error",
    "title": "Deep Neural Networks - Notes for lecture 4c",
    "section": "",
    "text": "The squared error measure has some drawbacks:\n\nIf the desired output is 1 and the actual output is 0.00000001 there is almost no gradient for a logistic unit to fix up the error.\nIf we are trying to assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but we are depriving the network of this knowledge.\n\nIs there a different cost function that works better?\n\nYes: Force the outputs to represent a probability distribution across discrete alternatives"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04c.html#softmax",
    "href": "notes/dnn/dnn-04/l_04c.html#softmax",
    "title": "Deep Neural Networks - Notes for lecture 4c",
    "section": "",
    "text": "The output units in a softmax group use a non-local non-linearity:\n\ny_i = \\frac{e^{z_i}}{\\sum_{j\\in group} e^{z_i}}\n\n\n\\frac{\\partial y_i}{\\partial z_i} = y_i(1-y_i)"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04c.html#cross-entropy-the-right-cost-function-to-use-with-softmax",
    "href": "notes/dnn/dnn-04/l_04c.html#cross-entropy-the-right-cost-function-to-use-with-softmax",
    "title": "Deep Neural Networks - Notes for lecture 4c",
    "section": "",
    "text": "C=-\\sum_j t_j \\log y_i\n \n\\frac {\\partial C}{\\partial z_i} = - \\sum_j t_j \\frac {\\partial C}{\\partial y_i} \\frac {\\partial y_u}{\\partial z_i} = y_i -t_i\n\n\nThe right cost function is the negative log probability of the right answer.\nC has a very big gradient when the target value is 1 and the output is almost zero.\n\nA value of 0.000001 is much better than 0.000000001\nThe steepness of dC/dy exactly balances the flatness of dy/dz\n\n\nthe cross entropy cost function - is the correct cost function to use with SoftMax\nArchitectural Note:\nSoftMax unit +Cross-Entropy loss function =&gt; for classification"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html",
    "href": "notes/dnn/dnn-04/l_04e.html",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "When softmax is very big it becomes hard to train and store.\n\nA serial architecture, based on trying candidate next words, using feature vectors (like in the family example). This means fewer parameters, but still a lot of work.\nUsing a binary tree.\nCollobert & Weston’s search for good feature vectors for words, without trying to predict the next word in a sentence.\n\n\n\n\n\n\n\n\nA serial architecture\n\n\n\nSurprise this uses the same architecture\n\n\n\n\n\n\n\n\n\nA serial architecture\n\n\n\nNo Surprise this to uses basicaly the same architecture - only we are looking back.\n\n\n\n\n\nAfter computing the logit score for each candidate word, use all of the logits in a softmax to get word probabilities.\nThe difference between the word probabilities and their target probabilities gives cross-entropy error derivatives.\n\nThe derivatives try to raise the score of the correct candidate and lower the scores of its high-scoring rivals.\n\nWe can save a lot of time if we only use a small set of candidates suggested by some other kind of predictor.\n\nFor example, we could use the neural net to revise the probabilities of the words that the trigram model thinks are likely.\n\n\n\n\n\nIn Mnih, Yuecheng, and Hinton (2009) the authors show how to improve a state-of-the-art neural network language model that converts the previous “context” words into feature vectors and combines these feature vectors linearly to predict the feature vector of the next word.\n\nMnih, Andriy, Zhang Yuecheng, and Geoffrey Hinton. 2009. “Improving a Statistical Language Model Through Non-Linear Prediction.” Neurocomputing 72 (7-9): 1414–18. https://doi.org/https://doi.org/10.1016/j.neucom.2008.12.025.\nSignificant improvements in predictive accuracy are achieved by using a non-linear subnetwork to modulate the effects of the context words or to produce a non-linear correction term when predicting the feature vector.\n\n\n\n\n\nSoftmax as a tree\n\n\n\nArrange all the words in a binary tree with words as the leaves.\nUse the previous context to generate a prediction vector, v.\n\nCompare v with a learned vector, u, at each node of the tree.\nApply the logistic function to the scalar product of u and v to predict the probabilities of taking the two branches of the tree.\n\n\n\n\n\n\n\n \n\n\n\n\nMaximizing the log probability of picking the target word is equivalent to maximizing the sum of the log probabilities of taking all the branches on the path that leads to the target word.\n\nSo during learning, we only need to consider the nodes on the correct path. This is an exponential win: log(N) instead of N.\nFor each of these nodes, we know the correct branch and we know the current probability of taking it so we can get derivatives for learning both the prediction vector v and that node vector u.\n\nUnfortunately, it is still slow at test time.\n\n\n\n\nThis method comes from the paper (Collobert and Weston 2008)\n\nCollobert, Ronan, and Jason Weston. 2008. “A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.” In Proceedings of the 25th International Conference on Machine Learning, 160–67. ICML ’08. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1390156.1390177.\n\n\n\n\n\n\n\nWe can get an idea of the quality of the learned feature vectors by displaying them in a 2-D map.\n\nDisplay very similar vectors very close to each other.\nUse a multi-scale method called “t-sne” that also displays similar clusters near each other.\n\nThe learned feature vectors capture lots of subtle semantic distinctions, just by looking at strings of words.\n\nNo extra supervision required.\nThe information is all in the contexts that the word is used in.\nConsider “She scrommed him with the frying pan.”\n\n\n\n\n\n\n\n\n\n\n\n\ntsne_output\n\n\n\n\n\ntsne_output\n\n\n\n\n\ntsne_output\n\n\n\nA serial architecture\nA serial architecture\nSoftmax as a tree\ntsne_output\ntsne_output\ntsne_output"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#bengios-neural-net-for-predicting-the-next-word",
    "href": "notes/dnn/dnn-04/l_04e.html#bengios-neural-net-for-predicting-the-next-word",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "A serial architecture\n\n\n\nSurprise this uses the same architecture"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#a-serial-architecture",
    "href": "notes/dnn/dnn-04/l_04e.html#a-serial-architecture",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "A serial architecture\n\n\n\nNo Surprise this to uses basicaly the same architecture - only we are looking back."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#learning-in-the-serial-architecture",
    "href": "notes/dnn/dnn-04/l_04e.html#learning-in-the-serial-architecture",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "After computing the logit score for each candidate word, use all of the logits in a softmax to get word probabilities.\nThe difference between the word probabilities and their target probabilities gives cross-entropy error derivatives.\n\nThe derivatives try to raise the score of the correct candidate and lower the scores of its high-scoring rivals.\n\nWe can save a lot of time if we only use a small set of candidates suggested by some other kind of predictor.\n\nFor example, we could use the neural net to revise the probabilities of the words that the trigram model thinks are likely."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#learning-to-predict-the-next-word-by-predicting-a-path-through-a-tree",
    "href": "notes/dnn/dnn-04/l_04e.html#learning-to-predict-the-next-word-by-predicting-a-path-through-a-tree",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "In Mnih, Yuecheng, and Hinton (2009) the authors show how to improve a state-of-the-art neural network language model that converts the previous “context” words into feature vectors and combines these feature vectors linearly to predict the feature vector of the next word.\n\nMnih, Andriy, Zhang Yuecheng, and Geoffrey Hinton. 2009. “Improving a Statistical Language Model Through Non-Linear Prediction.” Neurocomputing 72 (7-9): 1414–18. https://doi.org/https://doi.org/10.1016/j.neucom.2008.12.025.\nSignificant improvements in predictive accuracy are achieved by using a non-linear subnetwork to modulate the effects of the context words or to produce a non-linear correction term when predicting the feature vector.\n\n\n\n\n\nSoftmax as a tree\n\n\n\nArrange all the words in a binary tree with words as the leaves.\nUse the previous context to generate a prediction vector, v.\n\nCompare v with a learned vector, u, at each node of the tree.\nApply the logistic function to the scalar product of u and v to predict the probabilities of taking the two branches of the tree."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#a-convenient-decomposition",
    "href": "notes/dnn/dnn-04/l_04e.html#a-convenient-decomposition",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "Maximizing the log probability of picking the target word is equivalent to maximizing the sum of the log probabilities of taking all the branches on the path that leads to the target word.\n\nSo during learning, we only need to consider the nodes on the correct path. This is an exponential win: log(N) instead of N.\nFor each of these nodes, we know the correct branch and we know the current probability of taking it so we can get derivatives for learning both the prediction vector v and that node vector u.\n\nUnfortunately, it is still slow at test time."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#a-simpler-way-to-learn-feature-vectors-for-words",
    "href": "notes/dnn/dnn-04/l_04e.html#a-simpler-way-to-learn-feature-vectors-for-words",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "This method comes from the paper (Collobert and Weston 2008)\n\nCollobert, Ronan, and Jason Weston. 2008. “A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.” In Proceedings of the 25th International Conference on Machine Learning, 160–67. ICML ’08. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1390156.1390177."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#displaying-the-learned-feature-vectors-in-a-2-d-map",
    "href": "notes/dnn/dnn-04/l_04e.html#displaying-the-learned-feature-vectors-in-a-2-d-map",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "We can get an idea of the quality of the learned feature vectors by displaying them in a 2-D map.\n\nDisplay very similar vectors very close to each other.\nUse a multi-scale method called “t-sne” that also displays similar clusters near each other.\n\nThe learned feature vectors capture lots of subtle semantic distinctions, just by looking at strings of words.\n\nNo extra supervision required.\nThe information is all in the contexts that the word is used in.\nConsider “She scrommed him with the frying pan.”"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04e.html#part-of-a-2-d-map-of-the-2500-most-common-words",
    "href": "notes/dnn/dnn-04/l_04e.html#part-of-a-2-d-map-of-the-2500-most-common-words",
    "title": "Deep Neural Networks - Notes for lecture 4e",
    "section": "",
    "text": "tsne_output\n\n\n\n\n\ntsne_output\n\n\n\n\n\ntsne_output\n\n\n\nA serial architecture\nA serial architecture\nSoftmax as a tree\ntsne_output\ntsne_output\ntsne_output"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html",
    "href": "notes/dnn/dnn-04/l_04.html",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nWe have the basic method for creating hidden layers (backprop), we’re going to see what can be achieved with them. We start to ask how the network learns to use its hidden units, with a toy application to family trees and a real application to language modeling."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4a-learning-to-predict-the-next-word",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4a-learning-to-predict-the-next-word",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4a: Learning to predict the next word",
    "text": "Lecture 4a: Learning to predict the next word\n\n\n\n\nA simple example of relational information\n\n\n\n\n\nrelational information\n\n\n\n\nAnother way to express the same information\n\nMake a set of propositions using the 12 relationships:\n\nson, daughter, nephew, niece, father, mother, uncle, aunt\nbrother, sister, husband, wife\n\n(Colin has-father James)\n(Colin has-mother Victoria)\n(James has-wife Victoria) this follows from the two above\n(Charlotte has-brother Colin)\n(Victoria has-brother Arthur)\n(Charlotte has-uncle Arthur) this follows from the above\n\n\n\nA relational learning task\n\nGiven a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules (x has-mother y) & (y has-husband z) =&gt; (x has-father z)\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\nCan a neural network capture the same knowledge by searching through a continuous space of weights?\n\n\n\nThe structure of the neural net\n\n\n\n\n\nstructure of the neural net\n\n\n\n\n\n\nvisulization of 6 neuron weights\n\n\n\n\n\nthe relational data\n\n\n\n\n\nWhat the network learns ?\n\nThe six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\n\nNationality, generation, branch of the family tree.\n\nThese features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example:\n\nInput person is of generation 3 and\nrelationship requires answer to be one generation up\nimplies\nOutput person is of generation 2\n\n\n\n\nAnother way to see that it works\n\nTrain the network on all but 4 of the triples that can be made using the 12 relationships\n\nIt needs to sweep through the training set many times adjusting the weights slightly each time.\n\nThen test it on the 4 held-out cases.\n\nIt gets about 3/4 correct.\nThis is good for a 24-way choice.\nOn much bigger datasets we can train on a much smaller fraction of the data.\n\n\n\n\nA large-scale example\n\nSuppose we have a database of millions of relational facts of the form (A R B).\n\nWe could train a net to discover feature vector representations of the terms that allow the third term to be predicted from the first two.\nThen we could use the trained net to find very unlikely triples. These are good candidates for errors in the database.\n\nInstead of predicting the third term, we could use all three terms as input and predict the probability that the fact is correct.\n\nTo train such a net we need a good source of false facts.\n\n\n\n\nA relational learning task\n\nGiven a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules:\n\nHasMother(x,y)\\ and\\ HasHusband(y,z) \\implies HasFather(x, z)\n\n\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\n\nCan a neural network capture the same knowledge by searching through a continuous space of weights?\n\n\n\nThe structure of the neural net\n\nThe six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer.\nNationality, generation, branch of the family tree. These features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example: Input person is of generation 3 and relationship requires answer to be one generation up implies Output person is of generation 2 This video introduces distributed representations. It’s not actually about predicting words, but it’s building up to that. It does a great job of looking inside the brain of a neural network. That’s important, but not always easy to do."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4b-a-brief-diversion-into-cognitive-science",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4b-a-brief-diversion-into-cognitive-science",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4b: A brief diversion into cognitive science",
    "text": "Lecture 4b: A brief diversion into cognitive science\nThis video is part of the course, i.e. it’s not optional, despite what Geoff says in the beginning of the video. This video gives a high-level interpretation of what’s going on in the family tree network. This video contrasts two types of inference:\n\nConscious inference, based on relational knowledge.\nUnconscious inference, based on distributed representations."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4c-another-diversion-the-softmax-output-function",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4c-another-diversion-the-softmax-output-function",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4c: Another diversion: The Softmax output function",
    "text": "Lecture 4c: Another diversion: The Softmax output function\nA Softmax cost function is a general-purpose ML component/technique for combining binary discriminators into a probability distribution to construct a classifier We’ve seen binary threshold output neurons and logistic output neurons. This video presents a third type.\nThis one only makes sense if we have multiple output neurons.\n\nProblems with squared error\n\nThe squared error measure has some drawbacks:\n\nIf the desired output is 1 and the actual output is 0.00000001 there is almost no gradient for a logistic unit to fix up the error.\nIf we are trying to assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but we are depriving the network of this knowledge.\n\nIs there a different cost function that works better?\n\nYes: Force the outputs to represent a probability distribution across discrete alternatives\n\n\n\n\nSoftmax\nThe output units in a softmax group use a non-local non-linearity:\n\ny_i = \\frac{e^{z_i}}{\\sum_{j\\in group} e^{z_i}}\n\n\n\\frac{\\partial y_i}{\\partial z_i} = y_i(1-y_i)\n\n\n\n\n\n\nCross-entropy: the right cost function to use with SoftMax\n\nC=-\\sum_j t_j \\log y_i\n \n\\frac {\\partial C}{\\partial z_i} = - \\sum_j t_j \\frac {\\partial C}{\\partial y_i} \\frac {\\partial y_u}{\\partial z_i} = y_i -t_i\n\n\nThe right cost function is the negative log probability of the right answer.\nC has a very big gradient when the target value is 1 and the output is almost zero.\n\nA value of 0.000001 is much better than 0.000000001\nThe steepness of dC/dy exactly balances the flatness of dy/dz\n\n\nthe cross entropy cost function - is the correct cost function to use with SoftMax\nArchitectural Note:\nSoftMax unit +Cross-Entropy loss function =&gt; for classification"
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4d-neuro-probabilistic-language-models",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4d-neuro-probabilistic-language-models",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4d: Neuro-probabilistic language models",
    "text": "Lecture 4d: Neuro-probabilistic language models\nThis is the first of several applications of neural networks that we’ll studying in some detail, in this course.\nSynonyms: word embedding; word feature vector; word encoding.\nAll of these describe the learned collection of numbers that is used to represent a word. “embedding” emphasizes that it’s a location in a high-dimensional space: it’s where the words are embedded in that space. When we check to see which words are close to each other, we’re thinking about that embedding.\n“feature vector” emphasizes that it’s a vector instead of a scalar, and that it’s componential, i.e. composed of multiple feature values.\n“encoding” is very generic and doesn’t emphasize anything specific. looks at the trigram model\n\nA basic problem in speech recognition\n\nWe cannot identify phonemes perfectly in noisy speech\n\nThe acoustic input is often ambiguous: there are several different words that fit the acoustic signal equally well.\n\nPeople use their understanding of the meaning of the utterance to hear the right words.\n\nWe do this unconsciously when we wreck a nice beach.\nWe are very good at it.\n\nThis means speech recognizers have to know which words are likely to come next and which are not.\n\nFortunately, words can be predicted quite well without full understanding.\n\n\n\n\nThe standard “trigram” method\n\nTake a huge amount of text and count the frequencies of all triples of words.\nUse these frequencies to make bets on the relative probabilities of words given the previous two words:\n\n\n\\frac{p(w_3=c|w_2=b,w_1=a)}{p(w_3=d|w_2=b,w_1=a)}=\\frac{count(abc)}{count(abd)}\n\n\nUntil very recently this was the state-of-the-art.\nWe cannot use a much bigger context because there are too many possibilities to store and the counts would mostly be zero.\nWe have to “back-off” to digrams when the count for a trigram is too small.\n\nThe probability is not zero just because the count is zero!\n\n\n\n\nInformation that the trigram model fails to use\n\nSuppose we have seen the sentence “the cat got squashed in the garden on friday”\nThis should help us predict words in the sentence “the dog got flattened in the yard on monday”\nA trigram model does not understand the similarities between\n\ncat/dog squashed/flattened garden/yard friday/monday\n\nTo overcome this limitation, we need to use the semantic and syntactic features of previous words to predict the features of the next word.\n\nUsing a feature representation also allows a context that contains many more previous words (e.g. 10)."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#lecture-4e-ways-to-deal-with-the-large-number-of-possible-outputs",
    "href": "notes/dnn/dnn-04/l_04.html#lecture-4e-ways-to-deal-with-the-large-number-of-possible-outputs",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Lecture 4e: Ways to deal with the large number of possible outputs",
    "text": "Lecture 4e: Ways to deal with the large number of possible outputs\nWhen softmax is very big it becomes hard to train and store.\n\nA serial architecture, based on trying candidate next words, using feature vectors (like in the family example). This means fewer parameters, but still a lot of work.\nUsing a binary tree.\nCollobert & Weston’s search for good feature vectors for words, without trying to predict the next word in a sentence."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#bengios-neural-net-for-predicting-the-next-word",
    "href": "notes/dnn/dnn-04/l_04.html#bengios-neural-net-for-predicting-the-next-word",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "Bengio’s neural net for predicting the next word",
    "text": "Bengio’s neural net for predicting the next word\n\n\n\nA serial architecture\n\n\n\nA serial architecture\n\n\n\nA serial architecture\n\n\n\n\nLearning in the serial architecture\n\nAfter computing the logit score for each candidate word, use all of the logits in a softmax to get word probabilities.\nThe difference between the word probabilities and their target probabilities gives cross-entropy error derivatives.\n\nThe derivatives try to raise the score of the correct candidate and lower the scores of its high-scoring rivals.\n\nWe can save a lot of time if we only use a small set of candidates suggested by some other kind of predictor.\n\nFor example, we could use the neural net to revise the probabilities of the words that the trigram model thinks are likely.\n\n\n\n\nLearning to predict the next word by predicting a path through a tree\nIn Mnih, Yuecheng, and Hinton (2009) the authors show how to improve a state-of-the-art neural network language model that converts the previous “context” words into feature vectors and combines these feature vectors linearly to predict the feature vector of the next word.\n\nMnih, Andriy, Zhang Yuecheng, and Geoffrey Hinton. 2009. “Improving a Statistical Language Model Through Non-Linear Prediction.” Neurocomputing 72 (7-9): 1414–18. https://doi.org/https://doi.org/10.1016/j.neucom.2008.12.025.\nSignificant improvements in predictive accuracy are achieved by using a non-linear subnetwork to modulate the effects of the context words or to produce a non-linear correction term when predicting the feature vector.\n\n\n\n\n\nSoftmax as a tree\n\n\n\nArrange all the words in a binary tree with words as the leaves.\nUse the previous context to generate a prediction vector, v.\n\nCompare v with a learned vector, u, at each node of the tree.\nApply the logistic function to the scalar product of u and v to predict the probabilities of taking the two branches of the tree.\n\n\n\n\nA picture of the learning\n\n\n \n\n\nA convenient decomposition\n\nMaximizing the log probability of picking the target word is equivalent to maximizing the sum of the log probabilities of taking all the branches on the path that leads to the target word.\n\nSo during learning, we only need to consider the nodes on the correct path. This is an exponential win: log(N) instead of N.\nFor each of these nodes, we know the correct branch and we know the current probability of taking it so we can get derivatives for learning both the prediction vector v and that node vector u.\n\nUnfortunately, it is still slow at test time."
  },
  {
    "objectID": "notes/dnn/dnn-04/l_04.html#a-simpler-way-to-learn-feature-vectors-for-words",
    "href": "notes/dnn/dnn-04/l_04.html#a-simpler-way-to-learn-feature-vectors-for-words",
    "title": "Deep Neural Networks - Notes for Lesson 4",
    "section": "A simpler way to learn feature vectors for words",
    "text": "A simpler way to learn feature vectors for words\nThis method comes from the paper (Collobert and Weston 2008)\n\nCollobert, Ronan, and Jason Weston. 2008. “A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.” In Proceedings of the 25th International Conference on Machine Learning, 160–67. ICML ’08. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1390156.1390177.\n\n\n\n\nDisplaying the learned feature vectors in a 2-D map\n\nWe can get an idea of the quality of the learned feature vectors by displaying them in a 2-D map.\n\nDisplay very similar vectors very close to each other.\nUse a multi-scale method called “t-sne” that also displays similar clusters near each other.\n\nThe learned feature vectors capture lots of subtle semantic distinctions, just by looking at strings of words.\n\nNo extra supervision required.\nThe information is all in the contexts that the word is used in.\nConsider “She scrommed him with the frying pan.”\n\n\n\n\nPart of a 2-D map of the 2500 most common words\n\n \n\n\n\ntsne_output\n\n\n\n\n\n\nrelational information\nstructure of the neural net\nvisulization of 6 neuron weights\nthe relational data\nA serial architecture\nA serial architecture\nSoftmax as a tree\ntsne_output"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html",
    "href": "notes/dnn/dnn-05/l05d.html",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#the-ilsvrc-2012-competition-on-imagenet",
    "href": "notes/dnn/dnn-05/l05d.html#the-ilsvrc-2012-competition-on-imagenet",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "The ILSVRC-2012 competition on ImageNet",
    "text": "The ILSVRC-2012 competition on ImageNet\n\nThe dataset has 1.2 million highresolution training images.\nThe classification task:\n\nGet the “correct” class in your top 5 bets. There are 1000 classes.\n\nThe localization task:\n\nFor each bet, put a box around the object. Your box must have at least 50% overlap with the correct box.\n\nSome of the best existing computer vision methods were tried on this dataset by leading computer vision groups from Oxford, INRIA, XRCE, …\n\nComputer vision systems use complicated multi-stage systems.\nThe early stages are typically hand-tuned by optimizing a few parameters\n\n\nExamples from the test set (with the network’s guesses)\n\n\n\n\n\nExamples from the test set"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#error-rates-on-the-ilsvrc-2012-competition",
    "href": "notes/dnn/dnn-05/l05d.html#error-rates-on-the-ilsvrc-2012-competition",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "Error rates on the ILSVRC-2012 competition",
    "text": "Error rates on the ILSVRC-2012 competition"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#a-neural-network-for-imagenet",
    "href": "notes/dnn/dnn-05/l05d.html#a-neural-network-for-imagenet",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "A neural network for ImageNet",
    "text": "A neural network for ImageNet\n\nAlex Krizhevsky (NIPS 2012) developed a very deep convolutional neural net of the type pioneered by Yann Le Cun. Its architecture was:\n\n7 hidden layers not counting some max pooling layers.\nThe early layers were convolutional.\nThe last two layers were globally connected.\n\nThe activation functions were:\n\nRectified linear units in every hidden layer. These train much faster and are more expressive than logistic units.\nCompetitive normalization to suppress hidden activities when nearby units have stronger activities. This helps with variations in intensity."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#tricks-that-significantly-improve-generalization",
    "href": "notes/dnn/dnn-05/l05d.html#tricks-that-significantly-improve-generalization",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "Tricks that significantly improve generalization",
    "text": "Tricks that significantly improve generalization\n\nTrain on random 224x224 patches from the 256x256 images to get more data. Also use left-right reflections of the images.\nAt test time, combine the opinions from ten different patches: The four 224x224 corner patches plus the central 224x224 patch plus the reflections of those five patches.\nUse dropout to regularize the weights in the globally connected layers (which contain most of the parameters).\n\nDropout means that half of the hidden units in a layer are randomly removed for each training example.\nThis stops hidden units from relying too much on other hidden units."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#the-hardware-required-for-alexs-net",
    "href": "notes/dnn/dnn-05/l05d.html#the-hardware-required-for-alexs-net",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "The hardware required for Alex’s net",
    "text": "The hardware required for Alex’s net\n\nHe uses a very efficient implementation of convolutional nets on two Nvidia GTX 580 Graphics Processor Units (over 1000 fast liHle cores)\n\nGPUs are very good for matrix-matrix multiplies.\nGPUs have very high bandwidth to memory.\nThis allows him to train the network in a week.\nIt also makes it quick to combine results from 10 patches at test time.\n\nWe can spread a network over many cores if we can communicate the states fast enough.\nAs cores get cheaper and datasets get bigger, big neural nets will improve faster than old-fashioned (i.e. pre Oct 2012) computer vision systems."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05d.html#finding-roads-in-high-resolution-images",
    "href": "notes/dnn/dnn-05/l05d.html#finding-roads-in-high-resolution-images",
    "title": "Deep Neural Networks - Notes for lecture 5d",
    "section": "Finding roads in high-resolution images",
    "text": "Finding roads in high-resolution images\n\n\n\n\n\nFinding roads\n\n\n\nIn (Mnih and Hinton 2012) the author, Vlad Mnih, used a non-convolutional net with local fields and multiple layers of rectified linear units to find roads in cluHered aerial images.\nIt takes a large image patch and predicts a binary road label for the central 16x16 pixels.\nThere is lots of labeled training data available for this task.\nThe task is hard for many reasons:\n\nOcclusion by buildings trees and cars.\nShadows, Lighting changes\nMinor viewpoint changes\n\nThe worst problems are incorrect labels:\n\nBadly registered maps\nArbitrary decisions about what counts as a road.\n\nBig neural nets trained on big image patches with millions of examples are the only hope.\n\n\n\n\nMnih, Volodymyr, and Geoffrey Hinton. 2012. “Learning to Label Aerial Images from Noisy Data.” In Proceedings of the 29th International Coference on International Conference on Machine Learning, 203–10. ICML’12. Madison, WI, USA: Omnipress.\n\nExamples from the test set\nFinding roads"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html",
    "href": "notes/dnn/dnn-05/l_05.html",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#things-that-make-it-hard-to-recognize-objects",
    "href": "notes/dnn/dnn-05/l_05.html#things-that-make-it-hard-to-recognize-objects",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Things that make it hard to recognize objects",
    "text": "Things that make it hard to recognize objects\n\n\n\n\nSegmentation: Real scenes are cluHered with other objects:\n\nIts hard to tell which pieces go together as parts of the same object.\nParts of an object can be hidden behind other objects.\n\nLighting: The intensties of the pixels are determined as much by the lighting as by the objects.\nDeformation: Objects can deform in a variety of non-affine ways:\n\ne.g a hand-written 2 can have a large loop or just a cusp.\n\n\nAffordances: Object classes are often defined by how they are used:\n\nChairs are things designed for sitting on so they have a wide variety of physical shapes."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#more-things-that-make-it-hard-to-recognize-objects",
    "href": "notes/dnn/dnn-05/l_05.html#more-things-that-make-it-hard-to-recognize-objects",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "More things that make it hard to recognize objects",
    "text": "More things that make it hard to recognize objects\n\n\n\n\nViewpoint: Changes in viewpoint cause changes in images that standard learning methods cannot cope with.\n\nInformation hops between input dimensions (i.e. pixels)\n\n\nImagine a medical database in which the age of a patient sometimes hops to the input dimension that normally codes for weight!\n\nTo apply machine learning we would first want to eliminate this dimension-hopping"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#some-ways-to-achieve-viewpoint-invariance",
    "href": "notes/dnn/dnn-05/l_05.html#some-ways-to-achieve-viewpoint-invariance",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Some ways to achieve viewpoint invariance",
    "text": "Some ways to achieve viewpoint invariance\n\nWe are so good at viewpoint invariance that it is hard to appreciate how difficult it is.\n\nIts one of the main difficulties in making computers perceive.\nWe still don’t have generally accepted solutions.\n\nThere are several different approaches:\n\nUse redundant invariant features.\nPut a box around the object and use normalized pixels.\n\nLecture 5c: Use replicated features with pooling. This is called “convolutional neural nets”\nUse a hierarchy of parts that have explicit poses relative to the camera (this will be described in detail later in the course)."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-invariant-feature-approach",
    "href": "notes/dnn/dnn-05/l_05.html#the-invariant-feature-approach",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The invariant feature approach",
    "text": "The invariant feature approach\n\nExtract a large, redundant set of features that are invariant under transformations\n\ne.g. pair of roughly parallel lines with a red dot between them.\nThis is what baby herring gulls use to know where to peck for food.\n\nWith enough invariant features, there is only one way to assemble them into an object.\n\nWe don’t need to represent the relationships between features directly because they are captured by other features.\n\nFor recognition, we must avoid forming features from parts of different objects."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-judicious-normalization-approach",
    "href": "notes/dnn/dnn-05/l_05.html#the-judicious-normalization-approach",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The judicious normalization approach",
    "text": "The judicious normalization approach\n\nPut a box around the object and use it as a coordinate frame for a set of normalized pixels.\n\nThis solves the dimension-hopping problem. If we choose the box correctly, the same part of an object always occurs on the same normalized pixels.\n\nThe box can provide invariance to many degrees of freedom: translation, rotation, scale, shear, stretch …\nBut choosing the box is difficult because of:\nSegmentation errors, occlusion, unusual orientations. • We need to recognize the shape to get the box right!"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-brute-force-normalization-approach",
    "href": "notes/dnn/dnn-05/l_05.html#the-brute-force-normalization-approach",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The brute force normalization approach",
    "text": "The brute force normalization approach\n\nWhen training the recognizer, use well-segmented, upright images to fit the correct box.\nAt test time try all possible boxes in a range of positions and scales.\n\nThis approach is widely used for detecting upright things like faces and house numbers in unsegmented images.\n\nIt is much more efficient if the recognizer can cope with some variation in position and scale so that we can use a coarse grid when trying all possible boxes."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-replicated-feature-approach-currently-the-dominant-approach-for-neural-networks",
    "href": "notes/dnn/dnn-05/l_05.html#the-replicated-feature-approach-currently-the-dominant-approach-for-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The replicated feature approach (currently the dominant approach for neural networks)",
    "text": "The replicated feature approach (currently the dominant approach for neural networks)\n\n\n\n\nUse many different copies of the same feature detector with different positions.\n\nCould also replicate across scale and orientation (tricky and expensive)\nReplication greatly reduces the number of free parameters to be learned.\n\nUse several different feature types, each with its own map of replicated detectors.\n\nAllows each patch of image to be represented in several ways."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#backpropagation-with-weight-constraints",
    "href": "notes/dnn/dnn-05/l_05.html#backpropagation-with-weight-constraints",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Backpropagation with weight constraints",
    "text": "Backpropagation with weight constraints\n\nIt’s easy to modify the backpropagation algorithm to incorporate linear constraints between the weights.\n\nWe compute the gradients as usual, and then modify the gradients so that they satisfy the constraints.\n\nSo if the weights started off satisfying the constraints, they will continue to satisfy them."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#what-does-replicating-the-feature-detectors-achieve",
    "href": "notes/dnn/dnn-05/l_05.html#what-does-replicating-the-feature-detectors-achieve",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "What does replicating the feature detectors achieve?",
    "text": "What does replicating the feature detectors achieve?\n\n\n \n\nEquivariant activities: Replicated features do not make the neural activities invariant to translation. The activities are equivariant.\nInvariant knowledge: If a feature is useful in some locations during training, detectors for that feature will be available in all locations during testing."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#pooling-the-outputs-of-replicated-feature-detectors",
    "href": "notes/dnn/dnn-05/l_05.html#pooling-the-outputs-of-replicated-feature-detectors",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Pooling the outputs of replicated feature detectors",
    "text": "Pooling the outputs of replicated feature detectors\n• Get a small amount of translational invariance at each level by averaging four neighboring replicated detectors to give a single output to the next level. – This reduces the number of inputs to the next layer of feature extraction, thus allowing us to have many more different feature maps. – Taking the maximum of the four works slightly better. • Problem: After several levels of pooling, we have lost information about the precise positions of things. – This makes it impossible to use the precise spatial relationships between high-level parts for recognition"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-architecture-of-lenet5",
    "href": "notes/dnn/dnn-05/l_05.html#the-architecture-of-lenet5",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The architecture of LeNet5",
    "text": "The architecture of LeNet5\n\n\n\nThe architecture of LeNet5"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-82-errors-made-by-lenet5",
    "href": "notes/dnn/dnn-05/l_05.html#the-82-errors-made-by-lenet5",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The 82 errors made by LeNet5",
    "text": "The 82 errors made by LeNet5\n\n\n\nerrors made by LeNet5\n\n\nNotice that most of the errors are cases that people find quite easy.\nThe human error rate is probably 20 to 30 errors but nobody has had the patience to measure it."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#priors-and-prejudice",
    "href": "notes/dnn/dnn-05/l_05.html#priors-and-prejudice",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Priors and Prejudice",
    "text": "Priors and Prejudice\n\nWe can put our prior knowledge about the task into the network by designing appropriate:\n\nConnectivity.\nWeight constraints.\nNeuron activation functions\n\nThis is less intrusive than handdesigning the features.\n\nBut it still prejudices the network towards the particular way of solving the problem that we had in mind.\n\nAlternatively, we can use our prior knowledge to create a whole lot more training data.\n\nThis may require a lot of work (Hofman&Tresp, 1993)\nIt may make learning take much longer.\n\nIt allows optimization to discover clever ways of using the multi-layer network that we did not think of.\n\nAnd we may never fully understand how it does it."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-brute-force-approach",
    "href": "notes/dnn/dnn-05/l_05.html#the-brute-force-approach",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The brute force approach",
    "text": "The brute force approach\n\nLeNet uses knowledge about the invariances to design:\n\nthe local connectivity\nthe weight-sharing\nthe pooling.\n\nThis achieves about 80 errors.\n\nThis can be reduced to about 40 errors by using many different transformations of the input and other tricks (Ranzato 2008)\n\nCiresan et. al. (2010) inject knowledge of invariances by creating a huge amount of carefully designed extra training data:\n\nFor each training image, they produce many new training examples by applying many different transformations.\nThey can then train a large, deep, dumb net on a GPU without much overfitting.\n\nThey achieve about 35 errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-errors-made-by-the-ciresan-et.-al.-net",
    "href": "notes/dnn/dnn-05/l_05.html#the-errors-made-by-the-ciresan-et.-al.-net",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The errors made by the Ciresan et. al. net",
    "text": "The errors made by the Ciresan et. al. net\n\n\n\nerrors made by the Ciresan\n\n\nThe top printed digit is the right answer. The bottom two printed digits are the network’s best two guesses.\nThe right answer is almost always in the top 2 guesses.\nWith model averaging they can now get about 25 errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#how-to-detect-a-significant-drop-in-the-error-rate",
    "href": "notes/dnn/dnn-05/l_05.html#how-to-detect-a-significant-drop-in-the-error-rate",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "How to detect a significant drop in the error rate",
    "text": "How to detect a significant drop in the error rate\n\n\n\n\n\nMcNemar test 1\n\n\n\n\n\nMcNemar test 1\n\n\n\nIs 30 errors in 10,000 test cases significantly beHer than 40 errors?\n\nIt all depends on the particular errors!\nThe McNemar test uses the particular errors and can be much more powerful than a test that just uses the number of errors."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-ilsvrc-2012-competition-on-imagenet",
    "href": "notes/dnn/dnn-05/l_05.html#the-ilsvrc-2012-competition-on-imagenet",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The ILSVRC-2012 competition on ImageNet",
    "text": "The ILSVRC-2012 competition on ImageNet\n\nThe dataset has 1.2 million highresolution training images.\nThe classification task:\n\nGet the “correct” class in your top 5 bets. There are 1000 classes.\n\nThe localization task:\n\nFor each bet, put a box around the object. Your box must have at least 50% overlap with the correct box.\n\nSome of the best existing computer vision methods were tried on this dataset by leading computer vision groups from Oxford, INRIA, XRCE, …\n\nComputer vision systems use complicated multi-stage systems.\nThe early stages are typically hand-tuned by optimizing a few parameters\n\n\nExamples from the test set (with the network’s guesses)\n\n\n\n\n\nExamples from the test set"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#error-rates-on-the-ilsvrc-2012-competition",
    "href": "notes/dnn/dnn-05/l_05.html#error-rates-on-the-ilsvrc-2012-competition",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Error rates on the ILSVRC-2012 competition",
    "text": "Error rates on the ILSVRC-2012 competition"
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#a-neural-network-for-imagenet",
    "href": "notes/dnn/dnn-05/l_05.html#a-neural-network-for-imagenet",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "A neural network for ImageNet",
    "text": "A neural network for ImageNet\n\nAlex Krizhevsky (NIPS 2012) developed a very deep convolutional neural net of the type pioneered by Yann Le Cun. Its architecture was:\n\n7 hidden layers not counting some max pooling layers.\nThe early layers were convolutional.\nThe last two layers were globally connected.\n\nThe activation functions were:\n\nRectified linear units in every hidden layer. These train much faster and are more expressive than logistic units.\nCompetitive normalization to suppress hidden activities when nearby units have stronger activities. This helps with variations in intensity."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#tricks-that-significantly-improve-generalization",
    "href": "notes/dnn/dnn-05/l_05.html#tricks-that-significantly-improve-generalization",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Tricks that significantly improve generalization",
    "text": "Tricks that significantly improve generalization\n\nTrain on random 224x224 patches from the 256x256 images to get more data. Also use left-right reflections of the images.\nAt test time, combine the opinions from ten different patches: The four 224x224 corner patches plus the central 224x224 patch plus the reflections of those five patches.\nUse dropout to regularize the weights in the globally connected layers (which contain most of the parameters).\n\nDropout means that half of the hidden units in a layer are randomly removed for each training example.\nThis stops hidden units from relying too much on other hidden units."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#the-hardware-required-for-alexs-net",
    "href": "notes/dnn/dnn-05/l_05.html#the-hardware-required-for-alexs-net",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "The hardware required for Alex’s net",
    "text": "The hardware required for Alex’s net\n\nHe uses a very efficient implementation of convolutional nets on two Nvidia GTX 580 Graphics Processor Units (over 1000 fast liHle cores)\n\nGPUs are very good for matrix-matrix multiplies.\nGPUs have very high bandwidth to memory.\nThis allows him to train the network in a week.\nIt also makes it quick to combine results from 10 patches at test time.\n\nWe can spread a network over many cores if we can communicate the states fast enough.\nAs cores get cheaper and datasets get bigger, big neural nets will improve faster than old-fashioned (i.e. pre Oct 2012) computer vision systems."
  },
  {
    "objectID": "notes/dnn/dnn-05/l_05.html#finding-roads-in-high-resolution-images",
    "href": "notes/dnn/dnn-05/l_05.html#finding-roads-in-high-resolution-images",
    "title": "Deep Neural Networks - Notes for Lesson 5",
    "section": "Finding roads in high-resolution images",
    "text": "Finding roads in high-resolution images\n\n\n\n\n\nFinding roads\n\n\n\nIn (Mnih and Hinton 2012) the author, Vlad Mnih, used a non-convolutional net with local fields and multiple layers of rectified linear units to find roads in cluHered aerial images.\nIt takes a large image patch and predicts a binary road label for the central 16x16 pixels.\nThere is lots of labeled training data available for this task.\nThe task is hard for many reasons:\n\nOcclusion by buildings trees and cars.\nShadows, Lighting changes\nMinor viewpoint changes\n\nThe worst problems are incorrect labels:\n\nBadly registered maps\nArbitrary decisions about what counts as a road.\n\nBig neural nets trained on big image patches with millions of examples are the only hope.\n\n\n\n\nMnih, Volodymyr, and Geoffrey Hinton. 2012. “Learning to Label Aerial Images from Noisy Data.” In Proceedings of the 29th International Coference on International Conference on Machine Learning, 203–10. ICML’12. Madison, WI, USA: Omnipress.\n\nThe architecture of LeNet5\nerrors made by LeNet5\nerrors made by the Ciresan\nMcNemar test 1\nMcNemar test 1\nExamples from the test set\nFinding roads"
  },
  {
    "objectID": "notes/dnn/dnn-05/l05a.html",
    "href": "notes/dnn/dnn-05/l05a.html",
    "title": "Deep Neural Networks - Notes for lecture 5a",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05a.html#things-that-make-it-hard-to-recognize-objects",
    "href": "notes/dnn/dnn-05/l05a.html#things-that-make-it-hard-to-recognize-objects",
    "title": "Deep Neural Networks - Notes for lecture 5a",
    "section": "Things that make it hard to recognize objects",
    "text": "Things that make it hard to recognize objects\n\n\n\n\nSegmentation: Real scenes are cluHered with other objects:\n\nIts hard to tell which pieces go together as parts of the same object.\nParts of an object can be hidden behind other objects.\n\nLighting: The intensties of the pixels are determined as much by the lighting as by the objects.\nDeformation: Objects can deform in a variety of non-affine ways:\n\ne.g a hand-written 2 can have a large loop or just a cusp.\n\n\nAffordances: Object classes are often defined by how they are used:\n\nChairs are things designed for sitting on so they have a wide variety of physical shapes."
  },
  {
    "objectID": "notes/dnn/dnn-05/l05a.html#more-things-that-make-it-hard-to-recognize-objects",
    "href": "notes/dnn/dnn-05/l05a.html#more-things-that-make-it-hard-to-recognize-objects",
    "title": "Deep Neural Networks - Notes for lecture 5a",
    "section": "More things that make it hard to recognize objects",
    "text": "More things that make it hard to recognize objects\n\n\n\n\nViewpoint: Changes in viewpoint cause changes in images that standard learning methods cannot cope with.\n\nInformation hops between input dimensions (i.e. pixels)\n\n\nImagine a medical database in which the age of a patient sometimes hops to the input dimension that normally codes for weight!\n\nTo apply machine learning we would first want to eliminate this dimension-hopping"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03e.html",
    "href": "notes/dnn/dnn-03/l03e.html",
    "title": "Deep Neural Networks - Notes for lecture 3e",
    "section": "",
    "text": "The backpropagation algorithm is an efficient way of computing the error derivative \\frac{dE}{dw} for every weight on a single training case. There are many decisions needed on how to derive new weights using there derivatives.\n\nOptimization issues: How do we use the error derivatives on individual cases to discover a good set of weights? (lecture 6)\nGeneralization issues: How do we ensure that the learned weights work well for cases we did not see during training? (lecture 7)\n\nWe now have a very brief overview of these two sets of issues.\nHow often to update weights ?\n\nOnline - after every case.\nMini Batch - after a small sample of training cases.\nFull Batch - after a full sweep of training data.\n\nHow much to update? (c.f. lecture 6)\n\nfixed learning rate\nadaptable global learning rate\nadaptable learning rate per weight\ndon’t use steepest descent (velocity/momentum/second order methods)\n\n\n\n\n\nRegularization - How to ensure that learned weights work well for cases we did not see during training?\n\nThe training data contains information about the regularities in the mapping from input to output. But it also contains two types of noise.\n\nThe target values may be unreliable (usually only a minor worry).\nThere is sampling error. There will be accidental regularities just because of the particular training cases that were chosen.\n\nWhen we fit the model, it cannot tell which regularities are real and which are caused by sampling error.\n\nSo it fits both kinds of regularity.\nIf the model is very flexible it can model the sampling error really well. This is a disaster.\n\n\n\n\n\n\n\n\n\noverfitting\n\n\n\n\nWhich output value should you predict for this test input?\nWhich model do you trust?\n\nThe complicated model fits the data better.\nBut it is not economical.\n\nA model is convincing when it fits a lot of data surprisingly well.\n\nIt is not surprising that a complicated model can fit a small amount of data well.\nModels fit both signal and noise.\n\n\n\n\n\n\nA large number of different methods have been developed to reduce overfitting.\n\nWeight-decay\nWeight-sharing - reduce model flexibility by adding constraints on weights\nEarly stopping - stop training when by monitoring the Test error.\nModel averaging - use an ensemble of models\nBayesian fitting of neural nets - like averaging but weighed\nDropout - (hide data from half the net)\nGenerative pre-training - (more data)\n\nMany of these methods will be described in lecture 7.\n\n\n\n\noverfitting"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03e.html#overfitting-the-downside-of-using-powerful-models",
    "href": "notes/dnn/dnn-03/l03e.html#overfitting-the-downside-of-using-powerful-models",
    "title": "Deep Neural Networks - Notes for lecture 3e",
    "section": "",
    "text": "Regularization - How to ensure that learned weights work well for cases we did not see during training?\n\nThe training data contains information about the regularities in the mapping from input to output. But it also contains two types of noise.\n\nThe target values may be unreliable (usually only a minor worry).\nThere is sampling error. There will be accidental regularities just because of the particular training cases that were chosen.\n\nWhen we fit the model, it cannot tell which regularities are real and which are caused by sampling error.\n\nSo it fits both kinds of regularity.\nIf the model is very flexible it can model the sampling error really well. This is a disaster."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03e.html#a-simple-example-of-overfitting",
    "href": "notes/dnn/dnn-03/l03e.html#a-simple-example-of-overfitting",
    "title": "Deep Neural Networks - Notes for lecture 3e",
    "section": "",
    "text": "overfitting\n\n\n\n\nWhich output value should you predict for this test input?\nWhich model do you trust?\n\nThe complicated model fits the data better.\nBut it is not economical.\n\nA model is convincing when it fits a lot of data surprisingly well.\n\nIt is not surprising that a complicated model can fit a small amount of data well.\nModels fit both signal and noise."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03e.html#how-to-reduce-overfitting",
    "href": "notes/dnn/dnn-03/l03e.html#how-to-reduce-overfitting",
    "title": "Deep Neural Networks - Notes for lecture 3e",
    "section": "",
    "text": "A large number of different methods have been developed to reduce overfitting.\n\nWeight-decay\nWeight-sharing - reduce model flexibility by adding constraints on weights\nEarly stopping - stop training when by monitoring the Test error.\nModel averaging - use an ensemble of models\nBayesian fitting of neural nets - like averaging but weighed\nDropout - (hide data from half the net)\nGenerative pre-training - (more data)\n\nMany of these methods will be described in lecture 7.\n\n\n\n\noverfitting"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html",
    "href": "notes/dnn/dnn-03/l_03.html",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nslides for the lesson\nWhy is a new algorithm needed?"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#why-the-perceptron-learning-procedure-cannot-be-generalised-to-hidden-layers",
    "href": "notes/dnn/dnn-03/l_03.html#why-the-perceptron-learning-procedure-cannot-be-generalised-to-hidden-layers",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Why the perceptron learning procedure cannot be generalised to hidden layers?",
    "text": "Why the perceptron learning procedure cannot be generalised to hidden layers?\n\nRecall: by convexity, the Perceptron convergence algorithm guarantees that each time the weights change, they get closer to every “generously feasible” set of weights. 😄\n\nThis guarantee cannot be extended to more complex networks which wights are non-convex, i.e. the average of two good solutions may be a bad solution. 1 😢\n\nSo “multi-layer” neural networks do not use the perceptron learning procedure.\n\nThey should never have been called multi-layer perceptrons.\n\n\n1 a convex set includes all weighted sums"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#a-different-way-to-show-that-a-learning-procedure-makes-progress",
    "href": "notes/dnn/dnn-03/l_03.html#a-different-way-to-show-that-a-learning-procedure-makes-progress",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "A different way to show that a learning procedure makes progress",
    "text": "A different way to show that a learning procedure makes progress\n\nInstead of showing the weights get closer to a good set of weights, show that the actual output values get closer the target values.\n\nThis can be true even for non-convex problems in which there are many quite different sets of weights that work well and averaging two good sets of weights may give a bad set of weights.\nIt is not true for perceptron learning.\n\nThe simplest example is a linear neuron with a squared error measure."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#linear-neurons",
    "href": "notes/dnn/dnn-03/l_03.html#linear-neurons",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Linear neurons",
    "text": "Linear neurons\n\nCalled linear filters in electrical engineering and linear transforms in linear algebra and can be represented by martracies\nWe don’t use linear neurons in practice:\n\nWithout a non-linearity in the unit, a stack of N layers can be replaced by a single layer 2\nThis lecture just demonstrates the analysis we will use with non-linear units.\n\nThe neuron’s output is the real valued weighted sum of its inputs\nThe goal of learning is to minimize the total error over all training cases.\n\nHere error is the squared difference between the desired output and the actual output.\n\n\n2 think multiplying N-matracies just gives a single matrix \n\\textcolor{green}{\\overbrace{y}^{\\text{output}}} = \\sum_{n \\in train} \\textcolor{red}{\\overbrace{w_i}^{\\text{weights}}} \\textcolor{blue}{\\underbrace{x_i}_{\\text{inputs}}}= \\vec{w}^T\\cdot\\vec{x}\n where:\n\ny is the neuron’s estimate of the desired output\nx is the input vector\nw is the weight vector"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#why-dont-we-solve-it-analytically",
    "href": "notes/dnn/dnn-03/l_03.html#why-dont-we-solve-it-analytically",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Why don’t we solve it analytically?",
    "text": "Why don’t we solve it analytically?\n\nIt is straight-forward to write down a set of equations, one per training case, and to solve for the best set of weights.\nThis is the standard engineering approach so why don’t we use it?\nScientific answer: We want a method that real neurons could use.\nEngineering answer: We want a method that can be generalized to multi-layer, non-linear neural networks.\nThe analytic solution relies on it being linear and having a squared error measure.\nIterative methods are usually less efficient but they are much easier to generalize.\n\n\nA toy example\n\nEach day you get lunch at the cafeteria.\n\nYour diet consists of fish, chips, and ketchup.\nYou get several portions of each.\n\nThe cashier only tells you the total price of the meal\n\nAfter several days, you should be able to figure out the price of each portion.\n\nThe iterative approach: Start with random guesses for the prices and then adjust them to get a better fit to the observed prices of whole meals.\n\n\n\nSolving the equations iteratively\n\nEach meal price gives a linear constraint on the prices of the portions: \n\\text{price} = X_\\text{fish} W_\\text{fish} + X_\\text{chips} W_\\text{chips} + X_\\text{ketchup}W_\\text{ketchup}      \n\n\nThe prices of the portions are like the weights in of a linear neuron. \nW = (w_\\text{fish} , W_\\text{ chips} , W_\\text{ketchup} )\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\n\nThe true weights used by the cashier\n\n\n\n\n\nthe true weights\n\n\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\nA model of the cashier with arbitrary initial weights\n\n\n\n\n\na toy problem\n\n\n\nResidual error = 350\nThe “delta-rule” for learning is: \\Delta w_i = \\epsilon x_i (t - y)\nWith a learning rate \\epsilon of 1/35, the weight changes are:+20, +50, +30\nThis gives new weights of: 70, 100, 80.\nThe weight for chips got worse, but over all the weights are better\n\ny reducing errors, individual weight estimate may be getting worse\nCalculating the change in the weights:\ncalculate our output using forward propagation\n\n\nDeriving the delta rule\n\ny = \\sum_{n \\in train} w_i x_i= \\vec{w}^T\\vec{x}\n Define the error as the squared residuals summed over all training cases:\n\nE = \\frac{1}{2}\\sum_{n \\in train} (t_n−y_n)^2\n\nuse the chain rule to get error derivatives for weights\n\n\\frac{d E}{\\partial w_i}=\\frac{1}{2}\\sum_{n \\in train}\\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n}=\\frac{1}{2}\\sum_{n \\in train}x_i^n(t^n−y^n)\n\nthe batch delta rule changes the weight in proportion to their error derivative summed on all training cases times the learning rate\n\n\\Delta w_i = −\\epsilon \\frac{d E}{\\partial w_i} = \\sum_{n \\in train} \\epsilon x_i^n (t^n−y^n)"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#behaviour-of-the-iterative-learning-procedure",
    "href": "notes/dnn/dnn-03/l_03.html#behaviour-of-the-iterative-learning-procedure",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Behaviour of the iterative learning procedure",
    "text": "Behaviour of the iterative learning procedure\n\nDoes the learning procedure eventually get the right answer?\n\nThere may be no perfect answer.\nBy making the learning rate small enough we can get as close as we desire to the best answer.\n\nHow quickly do the weights converge to their correct values?\n\nIt can be very slow if two input dimensions are highly correlated. If you almost always have the same number of portions of ketchup and chips, it is hard to decide how to divide the price between ketchup and chips"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#the-relationship-between-the-online-delta-rule-and-the-learning-rule-for-perceptrons",
    "href": "notes/dnn/dnn-03/l_03.html#the-relationship-between-the-online-delta-rule-and-the-learning-rule-for-perceptrons",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "The relationship between the online delta-rule and the learning rule for perceptrons",
    "text": "The relationship between the online delta-rule and the learning rule for perceptrons\n\nIn perceptron learning, we increment or decrement the weight vector by the input vector.\n\nBut we only change the weights when we make an error.\n\nIn the online version of the delta-rule we increment or decrement the weight vector by the input vector scaled by the residual error and the learning rate.\n\nSo we have to choose a learning rate. This is annoying\n\n\n\nresidual error\n\nit’s the amount by which we got the answer wrong.\n\n\nA very central concept is introduced without being made very explicit: we use derivatives for learning, i.e. for making the weights better. Try to understand why those concepts are indeed very related.\n\non-line learning\n\nmeans that we change the weights after every training example that we see, and we typically cycle through the collection of available training examples."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#lecture-3b-the-error-surface-for-a-linear-neuron",
    "href": "notes/dnn/dnn-03/l_03.html#lecture-3b-the-error-surface-for-a-linear-neuron",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Lecture 3b: The error surface for a linear neuron",
    "text": "Lecture 3b: The error surface for a linear neuron\n\n\n\n\n\n\n\nerror surface of a linear neuron\n\n\n\n\nThe error surface lies in a space with a horizontal axis for each weight and one vertical axis for the error.\n\nFor a linear neuron with a squared error, it is a quadratic bowl.\nVertical cross-sections are parabolas.\nHorizontal cross-sections are ellipses.\n\nFor multi-layer, non-linear nets the error surface is much more complicated.\n\n\nOnline versus batch learning\n\n\n\n\n\nWhy learning can be slow\n\n\nIf the ellipse is very elongated, the direction of steepest descent is almost perpendicular to the direction towards the minimum!\nThe red gradient vector has a large component along the short axis of the ellipse and a small component along the long axis of the ellipse.\nThis is just the opposite of what we want."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#lecture-3c-learning-the-weights-of-a-logistic-output-neuron",
    "href": "notes/dnn/dnn-03/l_03.html#lecture-3c-learning-the-weights-of-a-logistic-output-neuron",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Lecture 3c: Learning the weights of a logistic output neuron",
    "text": "Lecture 3c: Learning the weights of a logistic output neuron\n\n\n\nLogistic neurons AKA linear filters - useful to understand the algorithm but in reality we need to use non linear activation function.\n\nLogistic neurons\nThese give a real-valued output that is a smooth and bounded function of their total input. They have nice derivatives which make learning easy.\n\nz = b + \\sum _i x_i w_i\n\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\n\n\nlogistic activation function\n\n\n\n\n\nThe derivatives of a logistic neuron\nThe derivatives of the logit, z, with respect to the inputs and the weights are very simple:\n\nz = b + \\sum _i x_i w_i \\tag{the logit}\n\n\n\\frac{\\partial z}{\\partial w_i} = x_i \\;\\;\\;\\;\\; \\frac{\\partial z}{\\partial x_i} = w_i\n\nThe derivative of the output with respect to the logit is simple if you express it in terms of the output:\n\ny=\\frac{1}{1+e^{-z}}\n\n\n\\frac{d y}{d z} = y( 1-y)\n\nsince\n\ny = \\frac{1}{1+e^{-z}}=(1+e^{-z})^{-1}\n differentiating  \\frac{d y}{d z} = \\frac{-1(-e^{-z})}{(1+e^{-z})^2} =\\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}  = y( 1-y)  Using the chain rule to get the derivatives needed for learning the weights of a logistic unit To learn the weights we need the derivative of the output with respect to each weight:\n\n\\frac{d y}{\\partial w_i}  =\\frac{\\partial z}{\\partial w_i} \\frac{dy}{dz}  = x_iy( 1-y)\n\n\n\\frac{d E}{\\partial w_i}  = \\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n} = - \\sum \\green{x_i^n}\\red{ y^n( 1-y^n)}\\green{(t^n-y^n)}\n\nwhere the green part corresponds to the delta rule and the extra term in red is simply the slope of the logistic.\nThe error function is still:\n\nE =\\frac{1}{2}(y−t)^2\n\nNotice how after Hinton explained what the derivative is for a logistic unit, he considers the job to be done. That’s because the learning rule is always simply some learning rate multiplied by the derivative."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#lecture-3d-the-back-propagation-algorithm",
    "href": "notes/dnn/dnn-03/l_03.html#lecture-3d-the-back-propagation-algorithm",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Lecture 3d: The back-propagation algorithm",
    "text": "Lecture 3d: The back-propagation algorithm\n\n\n\nHere, we start using hidden layers. To train them, we need the back propagation algorithm. Hidden layers, and this algorithm, are very important. They are the layers between the input layer and the output.\nThe story of training by perturbations also makes an appearance in the course by David MCcay, serving primarily as motivation for the back propagation algorithm.\nThis computation, just like the forward propagation, can be vectorized across multiple units in every layer, and multiple training cases.\n\nLearning with hidden units (again)\n\nNetworks without hidden units are very limited in the input-output mappings they can model.\n\nAdding a layer of hand-coded features (as in a Perceptrons) makes them much more powerful but the hard bit is designing the features.\n\nWe would like to find good features without requiring insights into the task or repeated trial and error where we guess some features and see how well they work.\n\nWe need to automate the loop of designing features for a particular task and seeing how well they work.\n\n\n\nLearning by perturbing weights\n\nRandomly perturb one weight and see if it improves performance. If so, save the change.\n\nThis is a form of reinforcement learning.\nVery inefficient. We need to do multiple forward passes on a representative set of training cases just to change one weight. Back propagation is much better.\nTowards the end of learning, large weight perturbations will nearly always make things worse, because the weights need to have the right relative values. (so we should adapt a decreasing learning rate).\n\nWe could randomly perturb all the weights in parallel and correlate the performance gain with the weight changes.\n\nNot any better because we need lots of trials on each training case to “see” the effect of changing one weight through the noise created by all the changes to other weights.\n\nA better idea: Randomly perturb the activities of the hidden units.\n\nOnce we know how we want a hidden activity to change on a given training case, we can compute how to change the weights.\nThere are fewer activities than weights, but backpropagation still wins by a factor of the number of neurons.\n\n\n\n\nThe idea behind backpropagation\n\nWe don’t know what the hidden units ought to do, but we can compute how fast the error changes as we change a hidden activity.\n\nInstead of using desired activities to train the hidden units, use error derivatives w.r.t. hidden activities.\n\nEach hidden activity can affect many output units and can therefore have many separate effects on the error. These effects must be combined.\n\n\nWe can compute error derivatives for all the hidden units efficiently at the same time.\n\nOnce we have the error derivatives for the hidden activities, its easy to get the error derivatives for the weights going into a hidden unit."
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#sketch-of-back-propagation-on-a-single-case",
    "href": "notes/dnn/dnn-03/l_03.html#sketch-of-back-propagation-on-a-single-case",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Sketch of back propagation on a single case",
    "text": "Sketch of back propagation on a single case\n\nFirst convert the discrepancy between each output and its target value into an error derivative.\nThen compute error derivatives in each hidden layer from error derivatives in the layer above.\nThen use error derivatives w.r.t. activities to get error derivatives w.r.t. the incoming weights. \nE =\\frac{1}{2}(t_i-y_i)^2\n\n\n\n\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)\n\n\n\n\n\n\nback proogations of erros\n\n\n\n\n\n\nbackproogating the error derivative"
  },
  {
    "objectID": "notes/dnn/dnn-03/l_03.html#lecture-3e-using-the-derivatives-computed-by-backpropagation",
    "href": "notes/dnn/dnn-03/l_03.html#lecture-3e-using-the-derivatives-computed-by-backpropagation",
    "title": "Deep Neural Networks - Notes for Lesson 3",
    "section": "Lecture 3e: Using the derivatives computed by backpropagation",
    "text": "Lecture 3e: Using the derivatives computed by backpropagation\n\n\n\n\nThe backpropagation algorithm is an efficient way of computing the error derivative \\frac{dE}{dw} for every weight on a single training case. There are many decisions needed on how to derive new weights using there derivatives.\n\nOptimization issues: How do we use the error derivatives on individual cases to discover a good set of weights? (lecture 6)\nGeneralization issues: How do we ensure that the learned weights work well for cases we did not see during training? (lecture 7)\n\nWe now have a very brief overview of these two sets of issues.\nHow often to update weights ?\n\nOnline - after every case.\nMini Batch - after a small sample of training cases.\nFull Batch - after a full sweep of training data.\n\nHow much to update? (c.f. lecture 6)\n\nfixed learning rate\nadaptable global learning rate\nadaptable learning rate per weight\ndon’t use steepest descent (velocity/momentum/second order methods)\n\n\n\nOverfitting: The downside of using powerful models\n\nRegularization - How to ensure that learned weights work well for cases we did not see during training?\n\nThe training data contains information about the regularities in the mapping from input to output. But it also contains two types of noise.\n\nThe target values may be unreliable (usually only a minor worry).\nThere is sampling error. There will be accidental regularities just because of the particular training cases that were chosen.\n\nWhen we fit the model, it cannot tell which regularities are real and which are caused by sampling error.\n\nSo it fits both kinds of regularity.\nIf the model is very flexible it can model the sampling error really well. This is a disaster.\n\n\n\n\nA simple example of overfitting\n\n\nWhich output value should you predict for this test input?\nWhich model do you trust?\n\nThe complicated model fits the data better.\nBut it is not economical.\n\nA model is convincing when it fits a lot of data surprisingly well.\n\nIt is not surprising that a complicated model can fit a small amount of data well.\nModels fit both signal and noise.\n\n\n\n\nHow to reduce overfitting\n\nA large number of different methods have been developed to reduce overfitting.\n\nWeight-decay\nWeight-sharing - reduce model flexibility by adding constraints on weights\nEarly stopping - stop training when by monitoring the Test error.\nModel averaging - use an ensemble of models\nBayesian fitting of neural nets - like averaging but weighed\nDropout - (hide data from half the net)\nGenerative pre-training - (more data)\n\nMany of these methods will be described in lecture 7.\n\n\n\n\nthe true weights\na toy problem\nerror surface of a linear neuron\nlogistic activation function\nback proogations of erros\nbackproogating the error derivative"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html",
    "href": "notes/dnn/dnn-03/l03d.html",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "Here, we start using hidden layers. To train them, we need the back propagation algorithm. Hidden layers, and this algorithm, are very important. They are the layers between the input layer and the output.\nThe story of training by perturbations also makes an appearance in the course by David MCcay, serving primarily as motivation for the back propagation algorithm.\nThis computation, just like the forward propagation, can be vectorized across multiple units in every layer, and multiple training cases.\n\n\n\nNetworks without hidden units are very limited in the input-output mappings they can model.\nAdding a layer of hand-coded features (as in a Perceptrons) makes them much more powerful but the hard bit is designing the features.\n\nWe would like to find good features without requiring insights into the task or repeated trial and error where we guess some features and see how well they work.\n\nWe need to automate the loop of designing features for a particular task and seeing how well they work.\n\n\n\n\n\nRandomly perturb one weight and see if it improves performance. If so, save the change.\n\nThis is a form of reinforcement learning.\nVery inefficient. We need to do multiple forward passes on a representative set of training cases just to change one weight. Back propagation is much better.\nTowards the end of learning, large weight perturbations will nearly always make things worse, because the weights need to have the right relative values. (so we should adapt a decreasing learning rate).\n\nWe could randomly perturb all the weights in parallel and correlate the performance gain with the weight changes.\n\nNot any better because we need lots of trials on each training case to “see” the effect of changing one weight through the noise created by all the changes to other weights.\n\nA better idea: Randomly perturb the activities of the hidden units.\n\nOnce we know how we want a hidden activity to change on a given training case, we can compute how to change the weights.\nThere are fewer activities than weights, but backpropagation still wins by a factor of the number of neurons.\n\n\n\n\n\n\nWe don’t know what the hidden units ought to do, but we can compute how fast the error changes as we change a hidden activity.\n\nInstead of using desired activities to train the hidden units, use error derivatives w.r.t. hidden activities.\nEach hidden activity can affect many output units and can therefore have many separate effects on the error. These effects must be combined.\n\nWe can compute error derivatives for all the hidden units efficiently at the same time.\n\nOnce we have the error derivatives for the hidden activities, its easy to get the error derivatives for the weights going into a hidden unit.\n\n\n\n\n\n\nFirst convert the discrepancy between each output and its target value into an error derivative.\nThen compute error derivatives in each hidden layer from error derivatives in the layer above.\nThen use error derivatives w.r.t. activities to get error derivatives w.r.t. the incoming weights. \nE =\\frac{1}{2}(t_i-y_i)^2\n\n\n\n\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)\n\n\n\n\n\n\nback proogations of errors\n\n\n\n\n\n\nbackpropagating the error derivative\n\n\n\n\n\n\nback proogations of errors\nbackpropagating the error derivative"
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html#learning-with-hidden-units-again",
    "href": "notes/dnn/dnn-03/l03d.html#learning-with-hidden-units-again",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "Networks without hidden units are very limited in the input-output mappings they can model.\nAdding a layer of hand-coded features (as in a Perceptrons) makes them much more powerful but the hard bit is designing the features.\n\nWe would like to find good features without requiring insights into the task or repeated trial and error where we guess some features and see how well they work.\n\nWe need to automate the loop of designing features for a particular task and seeing how well they work."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html#learning-by-perturbing-weights",
    "href": "notes/dnn/dnn-03/l03d.html#learning-by-perturbing-weights",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "Randomly perturb one weight and see if it improves performance. If so, save the change.\n\nThis is a form of reinforcement learning.\nVery inefficient. We need to do multiple forward passes on a representative set of training cases just to change one weight. Back propagation is much better.\nTowards the end of learning, large weight perturbations will nearly always make things worse, because the weights need to have the right relative values. (so we should adapt a decreasing learning rate).\n\nWe could randomly perturb all the weights in parallel and correlate the performance gain with the weight changes.\n\nNot any better because we need lots of trials on each training case to “see” the effect of changing one weight through the noise created by all the changes to other weights.\n\nA better idea: Randomly perturb the activities of the hidden units.\n\nOnce we know how we want a hidden activity to change on a given training case, we can compute how to change the weights.\nThere are fewer activities than weights, but backpropagation still wins by a factor of the number of neurons."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html#the-idea-behind-backpropagation",
    "href": "notes/dnn/dnn-03/l03d.html#the-idea-behind-backpropagation",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "We don’t know what the hidden units ought to do, but we can compute how fast the error changes as we change a hidden activity.\n\nInstead of using desired activities to train the hidden units, use error derivatives w.r.t. hidden activities.\nEach hidden activity can affect many output units and can therefore have many separate effects on the error. These effects must be combined.\n\nWe can compute error derivatives for all the hidden units efficiently at the same time.\n\nOnce we have the error derivatives for the hidden activities, its easy to get the error derivatives for the weights going into a hidden unit."
  },
  {
    "objectID": "notes/dnn/dnn-03/l03d.html#sketch-of-back-propagation-on-a-single-case",
    "href": "notes/dnn/dnn-03/l03d.html#sketch-of-back-propagation-on-a-single-case",
    "title": "Deep Neural Networks - Notes for lecture 3d",
    "section": "",
    "text": "First convert the discrepancy between each output and its target value into an error derivative.\nThen compute error derivatives in each hidden layer from error derivatives in the layer above.\nThen use error derivatives w.r.t. activities to get error derivatives w.r.t. the incoming weights. \nE =\\frac{1}{2}(t_i-y_i)^2\n\n\n\n\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)\n\n\n\n\n\n\nback proogations of errors\n\n\n\n\n\n\nbackpropagating the error derivative\n\n\n\n\n\n\nback proogations of errors\nbackpropagating the error derivative"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01b.html",
    "href": "notes/dnn/dnn-01/l01b.html",
    "title": "Deep Neural Networks - Notes for lecture 1b",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01b.html#lecture-1b-what-are-neural-networks",
    "href": "notes/dnn/dnn-01/l01b.html#lecture-1b-what-are-neural-networks",
    "title": "Deep Neural Networks - Notes for lecture 1b",
    "section": "Lecture 1b: What are neural networks?",
    "text": "Lecture 1b: What are neural networks?\n\nSome tasks that are easy or humans, like vision, are hard for software, and vice versa (chess).\n\nReasons to study neural computation\n\nTo understand how the brain actually works.\n\nIts very big and very complicated and made of stuff that dies when you poke it around. So we need to use computer simulations.\n\nTo understand a style of parallel computation inspired by neurons and their adaptive connections.\n\nVery different style from sequential computation.\nshould be good for things that brains are good at (e.g. vision)\nShould be bad for things that brains are bad at (e.g. 23 x 71)\n\nTo solve practical problems by using novel learning algorithms inspired by the brain (this course)\n\nLearning algorithms can be very useful even if they are not how the brain actually works.\n\n\n\n\nA typical cortical neuron\n\nGross physical structure:\n\nThere is one axon that branches\nThere is a dendritic tree that collects input from other neurons.\n\nAxons typically contact dendritic trees at synapses\n\nA spike of activity in the axon causes charge to be injected into the post-synaptic neuron.\n\nSpike generation:\n\nThere is an axon hillock that generates outgoing spikes whenever enough charge has flowed in at synapses to depolarize the cell membrane.\n\n\n\n\nSynapses\n\nWhen a spike of activity travels along an axon and arrives at a synapse it causes vesicles of transmitter chemical to be released.\n\nThere are several kinds of transmitter.\n\nThe transmitter molecules diffuse across the synaptic cleft and bind to receptor molecules in the membrane of the post-synaptic neuron thus changing their shape.\n\nThis opens up holes that allow specific ions in or out.\n\n\n\n\nHow synapses adapt\n\nThe effectiveness of the synapse can be changed:\n\nvary the number of vesicles of transmitter.\nvary the number of receptor molecules.\n\nSynapses are slow, but they have advantages over RAM\n\nThey are very small and very low-power.\nThey adapt using locally available signals\n\nBut what rules do they use to decide how to change?\n\n\n\n\n\nHow the brain works on one slide!\n\nEach neuron receives inputs from other neurons\n\nA few neurons also connect to receptors.\nCortical neurons use spikes to communicate.\n\nThe effect of each input line on the neuron is controlled by a synaptic weight\n\nThe weights can be positive or negative.\n\nThe synaptic weights adapt so that the whole network learns to perform useful computations\n\nRecognizing objects, understanding language, making plans, controlling the body.\n\nYou have about neurons each with about weights.\n\nA huge number of weights can affect the computation in a very short time. Much better bandwidth than a workstation.\n\n\n\n\nModularity and the brain\n\nDifferent bits of the cortex do different things.\n\nLocal damage to the brain has specific effects.\nSpecific tasks increase the blood flow to specific regions.\n\nBut cortex looks pretty much the same all over.\n\nEarly brain damage makes functions relocate.\n\nCortex is made of general purpose stuff that has the ability to turn into special purpose hardware in response to experience.\n\nThis gives rapid parallel computation plus flexibility.\nConventional computers get flexibility by having stored sequential programs, but this requires very fast central processors to perform long sequential computations."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01d.html",
    "href": "notes/dnn/dnn-01/l01d.html",
    "title": "Deep Neural Networks - Notes for lecture 1d",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01d.html#visualization-of-neural-networks-is-one-of-the-few-methods-to-get-some-insights-into-what-is-going-on-inside-the-black-box.",
    "href": "notes/dnn/dnn-01/l01d.html#visualization-of-neural-networks-is-one-of-the-few-methods-to-get-some-insights-into-what-is-going-on-inside-the-black-box.",
    "title": "Deep Neural Networks - Notes for lecture 1d",
    "section": "Visualization of neural networks is one of the few methods to get some insights into what is going on inside the black box.",
    "text": "Visualization of neural networks is one of the few methods to get some insights into what is going on inside the black box.\n\nConsider a neural network with two layers of neurons.\n\nneurons in the top layer represent known shapes.\nneurons in the bottom layer represent pixel intensities.\n\nA pixel gets to vote if it has ink on it.\n\nEach inked pixel can vote for several different shapes.\n\nThe shape that gets the most votes wins.\n\n\nHow to display the weights\nGive each output unit its own “map” of the input image and display the weight coming from each pixel in the location of that pixel in the map.\nUse a black or white blob with the area representing the magnitude of the weight and the color representing the sign.\n\n\nHow to learn the weights\nShow the network an image and increment the weights from active pixels to the correct class.\nThen decrement the weights from active pixels to whatever class the network guesses\n\n\nThe learned weights\nThe details of the learning algorithm will be explained in future lectures.\n\n\nWhy the simple learning algorithm is insufficient\n\nA two layer network with a single winner in the top layer is equivalent to having a rigid template for each shape.\nThe winner is the template that has the biggest overlap with the ink.\nThe ways in which hand-written digits vary are much too complicated to be captured by simple template matches of whole shapes.\nTo capture all the allowable variations of a digit we need to learn the features that it is composed of."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01e.html",
    "href": "notes/dnn/dnn-01/l01e.html",
    "title": "Deep Neural Networks - Notes for lecture 1e",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01e.html#lecture-1e-three-types-of-learning",
    "href": "notes/dnn/dnn-01/l01e.html#lecture-1e-three-types-of-learning",
    "title": "Deep Neural Networks - Notes for lecture 1e",
    "section": "Lecture 1e: Three types of learning",
    "text": "Lecture 1e: Three types of learning\n\nThe three main types of learning machine learning:\n\nSupervised learning\n\nLearn to predict an output given an input vector\n\nReinforcement learning\n\nLearn to select an action to maximize payoff.\n\nUnsupervised learning\n\nDiscover a good internal representation of the input.\n\nSemi supervised learning\n\nSemi-supervised uses a small amount of supervised data and large amount of unsupervised elarning\n\nFew/one shot learning\n\nSupervised learning with inference from one or a few examples\n\nZero shot learning\n\nSupervised learning with inference for inputs not seen in training - usually based on learned structrure\n\nTransfer learning\n\nLearning something from one data set and use it on another"
  },
  {
    "objectID": "notes/dnn/dnn-01/l01e.html#two-types-of-supervised-learning",
    "href": "notes/dnn/dnn-01/l01e.html#two-types-of-supervised-learning",
    "title": "Deep Neural Networks - Notes for lecture 1e",
    "section": "Two types of supervised learning",
    "text": "Two types of supervised learning\n\nEach training case consists of an input vector x and a target output t.\nRegression: The target output is a real number or a whole vector of real numbers.\n\nThe price of a stock in 6 months time.\nThe temperature at noon tomorrow.\n\nClassification: The target output is a class label.\n\nThe simplest case is a choice between 1 and 0.\nWe can also have multiple alternative labels."
  },
  {
    "objectID": "notes/dnn/dnn-01/l01e.html#how-supervised-learning-typically-works",
    "href": "notes/dnn/dnn-01/l01e.html#how-supervised-learning-typically-works",
    "title": "Deep Neural Networks - Notes for lecture 1e",
    "section": "How supervised learning typically works",
    "text": "How supervised learning typically works\n\nWe start by choosing a model-class:\n\nA model-class, f, is a way of using some numerical y=f(x;W) parameters, W, to map each input vector, x, into a predicted output y.\n\nLearning usually means adjusting the parameters to reduce the discrepancy between the target output, t, on each training case and the actual output, y, produced by the model.\n\nFor regression, \\frac{1}{2}(y-t)^2is often a sensible measure of the discrepancy.\nFor classification there are other measures that are generally more sensible (they also work better).\n\n\n\nReinforcement learning\n\nIn reinforcement learning, the output is an action or sequence of actions and the only supervisory signal is an occasional scalar reward.\n\nThe goal in selecting each action is to maximize the expected sum of the future rewards.\nWe usually use a discount factor for delayed rewards so that we don’t have to look too far into the future.\n\nReinforcement learning is difficult:\n\nThe rewards are typically delayed so its hard to know where we went wrong (or right).\nA scalar reward does not supply much information.\n\nThis course cannot cover everything and reinforcement learning is one of the important topics we will not cover.\n\n\n\nUnsupervised learning\n\nFor about 40 years, unsupervised learning was largely ignored by the machine learning community\n\nSome widely used definitions of machine learning actually excluded it.\nMany researchers thought that clustering was the only form of unsupervised learning.\n\nIt is hard to say what the aim of unsupervised learning is.\n\nOne major aim is to create an internal representation of the input that is useful for subsequent supervised or reinforcement learning.\nYou can compute the distance to a surface by using the disparity between two images. But you don’t want to learn to compute disparities by stubbing your toe thousands of times.\n\n\n\n\nOther goals for unsupervised learning\n\nIt provides a compact, low-dimensional representation of the input.\n\nHigh-dimensional inputs typically live on or near a lowdimensional manifold (or several such manifolds).\nPrincipal Component Analysis is a widely used linear method for finding a low-dimensional representation.\n\nIt provides an economical high-dimensional representation of the input in terms of learned features.\n\nBinary features are economical. – So are real-valued features that are nearly all zero.\n\nIt finds sensible clusters in the input.\n\nThis is an example of a very sparse code in which only one of the features is non-zero."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html",
    "href": "notes/dnn/dnn-02/l02c.html",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html#warning",
    "href": "notes/dnn/dnn-02/l02c.html#warning",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "Warning!",
    "text": "Warning!\n\nFor non-mathematicians, this is going to be tougher than the previous material.\nYou may have to spend a long time studying the next two parts.\nIf you are not used to thinking about hyper-planes in high-dimensional spaces, now is the time to learn.\nTo deal with hyper-planes in a 14-dimensional space, visualize a 3-D space and say “fourteen” to yourself very loudly. Everyone does it. :-)\n\nBut remember that going from 13-D to 14-D creates as much extra complexity as going from 2-D to 3-D."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html#geometry-review",
    "href": "notes/dnn/dnn-02/l02c.html#geometry-review",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "Geometry review",
    "text": "Geometry review\n\nA point (a.k.a. location) and an arrow from the origin to that point, are often used interchangeably.\nA hyperplane is the high-dimensional equivalent of a plane in 3-D.\nThe scalar product or inner product between two vectors\n\nsum of element-wise products.\nThe scalar product between two vectors that have an angle of less than 90 degrees between them is positive.\nFor more than 90 degrees it’s negative."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html#weight-space",
    "href": "notes/dnn/dnn-02/l02c.html#weight-space",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "Weight-space",
    "text": "Weight-space\n\n\n\n\n\nWeight-space\n\n\n\nThis space has one dimension per weight.\nA point in the space represents a particular setting of all the weights.\nAssuming that we have eliminated the threshold, each training case can be represented as a hyperplane through the origin.\n\nThe weights must lie on one side of this hyper-plane to get the answer correct.\n\nEach training case defines a plane (shown as a black line)\n\nThe plane goes through the origin and is perpendicular to the input vector.\nOn one side of the plane the output is wrong because the scalar product of the weight vector with the input vector has the wrong sign."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02c.html#the-cone-of-feasible-solutions",
    "href": "notes/dnn/dnn-02/l02c.html#the-cone-of-feasible-solutions",
    "title": "Deep Neural Networks - Notes for lecture 2c",
    "section": "The cone of feasible solutions",
    "text": "The cone of feasible solutions\n\n\n\n\n\nCone of feasable soulutions\n\n\n\nTo get all training cases right we need to find a point on the right side of all the planes.\n\nThere may not be any such point!\n\nIf there are any weight vectors that get the right answer for all cases, they lie in a hyper-cone with its apex at the origin.\n\nSo the average of two good weight vectors is a good weight vector.\n\nThe problem is convex.\n\n\n\nThis is not a very good explanation - unless we also take a convex optimization course in which we define a hyperplane and a cone.\n\n\n\nWeight-space\nCone of feasable soulutions"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html",
    "href": "notes/dnn/dnn-02/l02e.html",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nNow we consider why we don’t use Perceptrons\nnamely their short comings"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#the-limitations-of-perceptrons",
    "href": "notes/dnn/dnn-02/l02e.html#the-limitations-of-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "The limitations of Perceptrons",
    "text": "The limitations of Perceptrons\n\nIf you are allowed to choose the features by hand and if you use enough features, you can do almost anything.\n\nFor binary input vectors, we can have a separate feature unit for each of the exponentially many binary vectors and so we can make any possible discrimination on binary input vectors.\nThis type of table look-up won’t generalize.\n\nBut once the hand-coded features have been determined, there are very strong limitations on what a perceptron can learn."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#what-binary-threshold-neurons-cannot-do",
    "href": "notes/dnn/dnn-02/l02e.html#what-binary-threshold-neurons-cannot-do",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "What binary threshold neurons cannot do",
    "text": "What binary threshold neurons cannot do\n\n\n\n\n\nImpossible to satisfy\n\n\n\nA binary threshold output unit cannot even tell if two single bit features are the same!\n\n\n\n\ncase\nmap\nmap\n\n\n\n\nPositive cases (same)\n(1,1) \\to 1\n(0,0) \\to 1\n\n\nNegative cases (different)\n(1,0) \\to 0\n(0,1) \\to 0\n\n\n\n\nThe four input-output pairs give four inequalities that are impossible to satisfy:\nw_1+w_2 \\ge \\theta \\qquad \\theta \\ge 0\nw_1 &lt; \\theta \\qquad w_2 &lt; \\theta"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#a-geometric-view-of-what-binary-threshold-neurons-cannot-do",
    "href": "notes/dnn/dnn-02/l02e.html#a-geometric-view-of-what-binary-threshold-neurons-cannot-do",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "A geometric view of what binary threshold neurons cannot do",
    "text": "A geometric view of what binary threshold neurons cannot do\n\n\n\n\n\ngeometric view\n\n\nImagine “data-space” in which the axes correspond to components of an input vector.\n\nEach input vector is a point in this space.\nA weight vector defines a plane in data-space.\nThe weight plane is perpendicular to the weight vector and misses the origin by a distance equal to the threshold."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#discriminating-simple-patterns-under-translation-with-wrap-around",
    "href": "notes/dnn/dnn-02/l02e.html#discriminating-simple-patterns-under-translation-with-wrap-around",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "Discriminating simple patterns under translation with wrap-around",
    "text": "Discriminating simple patterns under translation with wrap-around\n\n\n\n\n\nwrap around\n\n\n\nSuppose we just use pixels as the features.\nCan a binary threshold unit discriminate between different patterns that have the same number of on pixels?\nNot if the patterns can translate with wrap-around!"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#sketch-of-a-proof-that-a-binary-decision-unit-cannot-discriminate-patterns-with-the-same-number-of-on-pixels-assuming-translation-with-wraparound",
    "href": "notes/dnn/dnn-02/l02e.html#sketch-of-a-proof-that-a-binary-decision-unit-cannot-discriminate-patterns-with-the-same-number-of-on-pixels-assuming-translation-with-wraparound",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "Sketch of a proof that a binary decision unit cannot discriminate patterns with the same number of on pixels (assuming translation with wraparound)",
    "text": "Sketch of a proof that a binary decision unit cannot discriminate patterns with the same number of on pixels (assuming translation with wraparound)\n\nFor pattern A, use training cases in all possible translations.\n\nEach pixel will be activated by 4 different translations of pattern A.\nSo the total input received by the decision unit over all these patterns will be four times the sum of all the weights.\n\nFor pattern B, use training cases in all possible translations.\n\nEach pixel will be activated by 4 different translations of pattern B.\nSo the total input received by the decision unit over all these patterns will be four times the sum of all the weights.\n\nBut to discriminate correctly, every single case of pattern A must provide more input to the decision unit than every single case of pattern B.\n\nThis is impossible if the sums over cases are the same."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#why-this-result-was-devastating-for-perceptrons",
    "href": "notes/dnn/dnn-02/l02e.html#why-this-result-was-devastating-for-perceptrons",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "Why this result was devastating for Perceptrons",
    "text": "Why this result was devastating for Perceptrons\n\nThe whole point of pattern recognition is to recognize patterns despite transformations like translation.\nIn thier book Minsky and Papert (1969) the authors Marvin Minsky and Seymour Papert proove the Group Invariance Theorem which says that the part of a Perceptron that learns cannot learn to do this if the transformations form a group.\n\nTranslations with wrap-around form a group.\n\nTo deal with such transformations, a Perceptron needs to use multiple feature units to recognize transformations of informative sub-patterns.\n\nSo the tricky part of pattern recognition must be solved by the hand-coded feature detectors, not the learning procedure.\n\n\n\nMinsky, Marvin, and Seymour Papert. 1969. Perceptrons: An Introduction to Computational Geometry. Cambridge, MA, USA: MIT Press."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02e.html#learning-with-hidden-units",
    "href": "notes/dnn/dnn-02/l02e.html#learning-with-hidden-units",
    "title": "Deep Neural Networks - Notes for lecture 2e",
    "section": "Learning with hidden units",
    "text": "Learning with hidden units\n\nNetworks without hidden units are very limited in the input-output mappings they can learn to model.\n\nMore layers of linear units do not help. Its still linear.\nFixed output non-linearities are not enough.\n\nWe need multiple layers of adaptive, non-linear hidden units. But how can we train such nets?\n\nWe need an efficient way of adapting all the weights, not just the last layer. This is hard.\nLearning the weights going into hidden units is equivalent to learning features.\nThis is difficult because nobody is telling us directly what the hidden units should do.\n\n\n\n\n\nImpossible to satisfy\ngeometric view\nwrap around"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html",
    "href": "notes/dnn/dnn-02/l02_.html",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nThis lecture covers the Perceptron convergence algorithm.\nA Perceptron is a primitive, neural network, but Hinton points out that they are still useful under the right condition.\n\nFor task that have very big feature vectors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeed-forward neural networks\n\n\n\nFeed forward networks are the subject of the first half of the course.\nThese are the most common type of neural network.\nThe first layer is the input and\nThe last layer is the output.\n\nIf there is more than one hidden layer, we call them “deep” neural networks.\n\nThey compute a series of transformations that change the similarities between cases.\n\nThe activities of the neurons in each layer are a non-linear function of the activities in the layer below.\n\n\n\n\n\n\n\n\nRecurrent neural networks\n\n\n\nThese have directed cycles in their connection graph.\n\nThat means you can sometimes get back to where you started by following the arrows.\n\nThey can have complicated dynamics and this can make them very difficult to train. – There is a lot of interest at present in finding efficient ways of training recurrent nets.\nThey are more biologically realistic.\n\n\n\n\n\n\n\nsequence to Sequence mapping\n\n\n\nRecurrent neural networks are a very natural way to model sequential data:\n\nThey are equivalent to very deep nets with one hidden layer per time slice.\nExcept that they use the same weights at every time slice and they get input at every time slice.\n\nThey have the ability to remember information in their hidden state for a long time.\n\nBut its very hard to train them to use this potential\n\n\n\n\n\n\nIn (Sutskever, Martens, and Hinton 2011) the authors trained a special type of RNN to predict the next character in a sequence.\nAfter training for a long time on a string of half a billion characters from English Wikipedia, he got it to generate new text.\n\nIt generates by predicting the probability distribution for the next character and then sampling a character from this distribution.\nThe next slide shows an example of the kind of text it generates. Notice how much it knows!\n\n\n\nSutskever, Ilya, James Martens, and Geoffrey E Hinton. 2011. “Generating Text with Recurrent Neural Networks.” In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1017–24. https://www.cs.toronto.edu/~jmartens/docs/RNN_Language.pdf.\n\n\n\n\nIn 1974 Northern Denver had been overshadowed by CNL, and several Irish intelligence agencies in the Mediterranean region. However, on the Victoria, Kings Hebrew stated that Charles decided to escape during an alliance. The mansion house was completed in 1882, the second in its bridge are omitted, while closing is the proton reticulum composed below it aims, such that it is the blurring of appearing on any well-paid type of box printer.\n\n\n\n\n\nThese are like recurrent networks, but the connections between units are symmetrical (they have the same weight in both directions).\n\nJohn Hopfield (and others) realized that symmetric networks are much easier to analyze than recurrent networks. – They are also more restricted in what they can do. because they obey an energy function.\n\nFor example, they cannot model cycles.\n\n\nIn (Hopfield 1982), the author introduced symmetrically connected nets without hidden units that are now called Hopfield networks.\n\n\nHopfield, J J. 1982. “Neural Networks and Physical Systems with Emergent Collective Computational Abilities.” Proceedings of the National Academy of Sciences 79 (8): 2554–58. https://doi.org/10.1073/pnas.79.8.2554.\n\n\n\n\nThese are called “Boltzmann machines”.\n\nThey are much more powerful models than Hopfield nets.\nThey are less powerful than recurrent neural networks.\nThey have a beautifully simple learning algorithm.\n\nWe will cover Boltzmann machines towards the end of the course.\n\n\n\n\n\n\n\n\n\n\n\nSchematic\nDescription\n\n\n\n\n\nFeed forward nets - regression and classication for images and tabular data.\n\n\n\nRecurrent nets - sequence to sequence\n\n\n\nHopfield nets - associative memory using symmetric nets with no hidden units\n\n\n\nBoltzmann machines - symmetric nets with hidden units\n\n\n\ncredit: images from The Neural Network Zoo\n\n\n\n\n\n\n\n\nperceptrons\n\n\n\nwhy the bias can be implemented as a special input unit?\nbiases can be treated using weights using an input that is always one.\na threshold is equivalent to having a negative bias.\nwe can avoid having to figure out a separate learning rule for the bias by using a trick:\nA bias is exactly equivalent to a weight on an extra input line that always has an activation of 1.\n\n\n\ncode and image from: Implementing the Perceptron Algorithm in Python  In english:\n\nAdd an extra component with value 1 to each input vector. The “bias” weight on this component is minus the threshold. Now we can forget the threshold.\nPick training cases using any policy that ensures that every training case will keep getting picked.\n\nIf the output unit is correct, leave its weights alone.\nIf the output unit incorrectly outputs a zero, add the input vector to the weight vector.\nIf the output unit incorrectly outputs a 1, subtract the input vector from the weight vector. This is guaranteed to find a set of weights that gets the right answer for all the training cases if any such set exists. a full implementation of a perceptrons:\n\n\ndef perceptron(X, y, lr, epochs):\n    '''\n    X: inputs\n    y: labels\n    lr: learning rate\n    epochs: Number of iterations\n    m: number of training examples\n    n: number of features \n    '''\n    m, n = X.shape    \n    # Initializing parapeters(theta) to zeros.\n    # +1 in n+1 for the bias term.\n    theta = np.zeros((n+1,1))\n    \n    # list with misclassification count per iteration.\n    n_miss_list = []\n    \n    # Training.\n    for epoch in range(epochs):\n        # variable to store misclassified.\n        n_miss = 0\n        # looping for every example.\n        for idx, x_i in enumerate(X):\n            # Inserting 1 for bias, X0 = 1.\n            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          \n            # Calculating prediction/hypothesis.\n            y_hat = step_func(np.dot(x_i.T, theta))\n            # Updating if the example is misclassified.\n            if (np.squeeze(y_hat) - y[idx]) != 0:\n                theta += lr*((y[idx] - y_hat)*x_i)\n                # Incrementing by 1.\n                n_miss += 1\n        # Appending number of misclassified examples\n        # at every iteration.\n        n_miss_list.append(n_miss)\n    return theta, n_miss_list\n\n\n\n\n\n\n\n\nA point (a.k.a. location) and an arrow from the origin to that point, are often used interchangeably.\nA hyperplane is the high-dimensional equivalent of a plane in 3-D.\nThe scalar product or inner product between two vectors\n\nsum of element-wise products.\nThe scalar product between two vectors that have an angle of less than 90 degrees between them is positive.\n\nFor more than 90 degrees it’s negative.\n\n\n\n\n\n\n\n\nWeight-space\n\n\n\nHas one dimension per weight.\nA point in the space represents a particular setting of all the weights.\nAssuming that we have eliminated the threshold, each training case can be represented as a hyperplane through the origin.\n\nThe weights must lie on one side of this hyperplane to get the answer correct.\n\nEach training case defines a plane (shown as a black line)\n\nThe plane goes through the origin and is perpendicular to the input vector.\nOn one side of the plane the output is wrong because the scalar product of the weight vector with the input vector has the wrong sign.\n\n\n\n\n\n\n\nWe look at the geometrical interpretation which is the proof for the convergence of the Perceptron learning algorithm works. We are trying to find a decision surface by solving a convex optimization problem. The surface is a hyperplane represented by a line where on side is the correct set and the other is incorrect. The weight vectors form a cone: - This means that wights are closed under addition and positive scaler product. - At zero it is zero.\n\n\n\n\n\nThe cone of feasible solutions\n\n\nTo get all training cases right we need to find a point on the right side of all the planes. But there may not be any such point! If there are any weight vectors that get the right answer for all cases, they lie in a hyper-cone with its apex at the origin.\n\nThe average of two good weight vectors is a good weight vector.\nThe problem is convex.\n\nTwo training case form two hyper planes (shown in black). the good weights lie between them average of good weights is good.\nGeometry of learning using weight space:\n\nit has one dimension per weight (vertex in the graph).\n\na point in the space represents a particular setting of all the weights.\neach training case (once we eliminate the threshold/bias) is a hyper plane through the origin\nonce the threshold is eliminated each training case can be represented as a hyper place through the origin\nthe weight must lie on one side of this place to get the answer correct .\n\nCaveats:\n\nconvergence depends on the picking the right features\ndeep nets don’t use this procedure - as it only converges for single layer Perceptrons - but for more than one layer sum of a solution is not necessarily also a solution.\n\n\n\n\n\n\n\nThis story motivates the need for more powerful networks.\nThese ideas will be important in future lectures, when we’re working on moving beyond these limitations.\n\n\n\n\nFeed-forward neural networks\nRecurrent neural networks\nsequence to Sequence mapping\nperceptrons\nWeight-space\nThe cone of feasible solutions"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2a-types-of-neural-network-architectures",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2a-types-of-neural-network-architectures",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "Feed-forward neural networks\n\n\n\nFeed forward networks are the subject of the first half of the course.\nThese are the most common type of neural network.\nThe first layer is the input and\nThe last layer is the output.\n\nIf there is more than one hidden layer, we call them “deep” neural networks.\n\nThey compute a series of transformations that change the similarities between cases.\n\nThe activities of the neurons in each layer are a non-linear function of the activities in the layer below.\n\n\n\n\n\n\n\n\nRecurrent neural networks\n\n\n\nThese have directed cycles in their connection graph.\n\nThat means you can sometimes get back to where you started by following the arrows.\n\nThey can have complicated dynamics and this can make them very difficult to train. – There is a lot of interest at present in finding efficient ways of training recurrent nets.\nThey are more biologically realistic.\n\n\n\n\n\n\n\nsequence to Sequence mapping\n\n\n\nRecurrent neural networks are a very natural way to model sequential data:\n\nThey are equivalent to very deep nets with one hidden layer per time slice.\nExcept that they use the same weights at every time slice and they get input at every time slice.\n\nThey have the ability to remember information in their hidden state for a long time.\n\nBut its very hard to train them to use this potential\n\n\n\n\n\n\nIn (Sutskever, Martens, and Hinton 2011) the authors trained a special type of RNN to predict the next character in a sequence.\nAfter training for a long time on a string of half a billion characters from English Wikipedia, he got it to generate new text.\n\nIt generates by predicting the probability distribution for the next character and then sampling a character from this distribution.\nThe next slide shows an example of the kind of text it generates. Notice how much it knows!\n\n\n\nSutskever, Ilya, James Martens, and Geoffrey E Hinton. 2011. “Generating Text with Recurrent Neural Networks.” In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1017–24. https://www.cs.toronto.edu/~jmartens/docs/RNN_Language.pdf.\n\n\n\n\nIn 1974 Northern Denver had been overshadowed by CNL, and several Irish intelligence agencies in the Mediterranean region. However, on the Victoria, Kings Hebrew stated that Charles decided to escape during an alliance. The mansion house was completed in 1882, the second in its bridge are omitted, while closing is the proton reticulum composed below it aims, such that it is the blurring of appearing on any well-paid type of box printer.\n\n\n\n\n\nThese are like recurrent networks, but the connections between units are symmetrical (they have the same weight in both directions).\n\nJohn Hopfield (and others) realized that symmetric networks are much easier to analyze than recurrent networks. – They are also more restricted in what they can do. because they obey an energy function.\n\nFor example, they cannot model cycles.\n\n\nIn (Hopfield 1982), the author introduced symmetrically connected nets without hidden units that are now called Hopfield networks.\n\n\nHopfield, J J. 1982. “Neural Networks and Physical Systems with Emergent Collective Computational Abilities.” Proceedings of the National Academy of Sciences 79 (8): 2554–58. https://doi.org/10.1073/pnas.79.8.2554.\n\n\n\n\nThese are called “Boltzmann machines”.\n\nThey are much more powerful models than Hopfield nets.\nThey are less powerful than recurrent neural networks.\nThey have a beautifully simple learning algorithm.\n\nWe will cover Boltzmann machines towards the end of the course.\n\n\n\n\n\n\n\n\n\n\n\nSchematic\nDescription\n\n\n\n\n\nFeed forward nets - regression and classication for images and tabular data.\n\n\n\nRecurrent nets - sequence to sequence\n\n\n\nHopfield nets - associative memory using symmetric nets with no hidden units\n\n\n\nBoltzmann machines - symmetric nets with hidden units\n\n\n\ncredit: images from The Neural Network Zoo"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2b-perceptrons-the-first-generation-of-neural-networks",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2b-perceptrons-the-first-generation-of-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "perceptrons\n\n\n\nwhy the bias can be implemented as a special input unit?\nbiases can be treated using weights using an input that is always one.\na threshold is equivalent to having a negative bias.\nwe can avoid having to figure out a separate learning rule for the bias by using a trick:\nA bias is exactly equivalent to a weight on an extra input line that always has an activation of 1.\n\n\n\ncode and image from: Implementing the Perceptron Algorithm in Python  In english:\n\nAdd an extra component with value 1 to each input vector. The “bias” weight on this component is minus the threshold. Now we can forget the threshold.\nPick training cases using any policy that ensures that every training case will keep getting picked.\n\nIf the output unit is correct, leave its weights alone.\nIf the output unit incorrectly outputs a zero, add the input vector to the weight vector.\nIf the output unit incorrectly outputs a 1, subtract the input vector from the weight vector. This is guaranteed to find a set of weights that gets the right answer for all the training cases if any such set exists. a full implementation of a perceptrons:\n\n\ndef perceptron(X, y, lr, epochs):\n    '''\n    X: inputs\n    y: labels\n    lr: learning rate\n    epochs: Number of iterations\n    m: number of training examples\n    n: number of features \n    '''\n    m, n = X.shape    \n    # Initializing parapeters(theta) to zeros.\n    # +1 in n+1 for the bias term.\n    theta = np.zeros((n+1,1))\n    \n    # list with misclassification count per iteration.\n    n_miss_list = []\n    \n    # Training.\n    for epoch in range(epochs):\n        # variable to store misclassified.\n        n_miss = 0\n        # looping for every example.\n        for idx, x_i in enumerate(X):\n            # Inserting 1 for bias, X0 = 1.\n            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          \n            # Calculating prediction/hypothesis.\n            y_hat = step_func(np.dot(x_i.T, theta))\n            # Updating if the example is misclassified.\n            if (np.squeeze(y_hat) - y[idx]) != 0:\n                theta += lr*((y[idx] - y_hat)*x_i)\n                # Incrementing by 1.\n                n_miss += 1\n        # Appending number of misclassified examples\n        # at every iteration.\n        n_miss_list.append(n_miss)\n    return theta, n_miss_list"
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2c-a-geometrical-view-of-perceptrons",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2c-a-geometrical-view-of-perceptrons",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "A point (a.k.a. location) and an arrow from the origin to that point, are often used interchangeably.\nA hyperplane is the high-dimensional equivalent of a plane in 3-D.\nThe scalar product or inner product between two vectors\n\nsum of element-wise products.\nThe scalar product between two vectors that have an angle of less than 90 degrees between them is positive.\n\nFor more than 90 degrees it’s negative.\n\n\n\n\n\n\n\n\nWeight-space\n\n\n\nHas one dimension per weight.\nA point in the space represents a particular setting of all the weights.\nAssuming that we have eliminated the threshold, each training case can be represented as a hyperplane through the origin.\n\nThe weights must lie on one side of this hyperplane to get the answer correct.\n\nEach training case defines a plane (shown as a black line)\n\nThe plane goes through the origin and is perpendicular to the input vector.\nOn one side of the plane the output is wrong because the scalar product of the weight vector with the input vector has the wrong sign."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2d-why-the-learning-works",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2d-why-the-learning-works",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "We look at the geometrical interpretation which is the proof for the convergence of the Perceptron learning algorithm works. We are trying to find a decision surface by solving a convex optimization problem. The surface is a hyperplane represented by a line where on side is the correct set and the other is incorrect. The weight vectors form a cone: - This means that wights are closed under addition and positive scaler product. - At zero it is zero.\n\n\n\n\n\nThe cone of feasible solutions\n\n\nTo get all training cases right we need to find a point on the right side of all the planes. But there may not be any such point! If there are any weight vectors that get the right answer for all cases, they lie in a hyper-cone with its apex at the origin.\n\nThe average of two good weight vectors is a good weight vector.\nThe problem is convex.\n\nTwo training case form two hyper planes (shown in black). the good weights lie between them average of good weights is good.\nGeometry of learning using weight space:\n\nit has one dimension per weight (vertex in the graph).\n\na point in the space represents a particular setting of all the weights.\neach training case (once we eliminate the threshold/bias) is a hyper plane through the origin\nonce the threshold is eliminated each training case can be represented as a hyper place through the origin\nthe weight must lie on one side of this place to get the answer correct .\n\nCaveats:\n\nconvergence depends on the picking the right features\ndeep nets don’t use this procedure - as it only converges for single layer Perceptrons - but for more than one layer sum of a solution is not necessarily also a solution."
  },
  {
    "objectID": "notes/dnn/dnn-02/l02_.html#lecture-2e-what-perceptrons-cant-do",
    "href": "notes/dnn/dnn-02/l02_.html#lecture-2e-what-perceptrons-cant-do",
    "title": "Deep Neural Networks - Notes for Lesson 2",
    "section": "",
    "text": "This story motivates the need for more powerful networks.\nThese ideas will be important in future lectures, when we’re working on moving beyond these limitations.\n\n\n\n\nFeed-forward neural networks\nRecurrent neural networks\nsequence to Sequence mapping\nperceptrons\nWeight-space\nThe cone of feasible solutions"
  },
  {
    "objectID": "notes/dnn/dnn-glossery/glossary.html",
    "href": "notes/dnn/dnn-glossery/glossary.html",
    "title": "Glossary of terms for Deep Neural Networks",
    "section": "",
    "text": "Glossary of terms in Deep leaning and ML\n\nAccuracy\n\nThe fraction of predictions that a classification model got right.\n\nactivation\n\nemphasizes that neuron like a real neuron may be on or off. In reality a negative bias will create a threshold to activation, otherwise, the neuron will always produce output. Also called [value] or [output].\n\nactivation function\n\nThe activation function is an attempt to mimic the biological neuron’s output in response to it input. This is generally a non-linear function. Some examples are RELU, Sigmoid, Tanh, Leaky RELU, Maxout and there are many others. All other things being equal RELU has emerged as the preferred activation function to start with.\n\nAdaGrad\n\nA gradient descent learning algorithm that re-scales the gradients of each parameter, effectively giving each parameter an independent learning rate. c.f. (Duchi, Hazan, and Singer 2011).\n\n\nDuchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” Journal of Machine Learning Research 12 (7).\nAnomaly detection\n\nThe process of identifying outliers that are considered candidates for removal from a dataset, Typically for being nonrepresentative high leverage points.\n\nAttention\n\nA mechanism that aggregate information from a set of inputs in a data-dependent manner. An attention mechanism might consist of a weighted sum over a set of inputs, where the weight for each input is computed by another part of the neural network.\n\nAttribute\n\nSynonym for feature.\n\nAutomation bias\n\nWhen a human decision-maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors.\n\nBackpropagation\n\nThe main algorithm for performing gradient descent on neural networks. First, the output values of each node are calculated (and cached) in a forward pass. Then, the partial derivative of the error with respect to each parameter is calculated in a backward pass through the graph.\n\nBagging\n\nA method to train an ensemble where each constituent model trains on a random subset of training examples sampled with replacement. E.g. a random forest is a collection of decision trees trained with bagging. The term bagging is short for bootstrap aggregating.\n\nBatch normalization\n\nNormalizing the input or output of the activation functions in a hidden layer. Batch normalization increases a network’s stability by protecting against outlier weights, enable higher learning rates and reduce **overfitting`.\n\nBatch size\n\nThe number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000. Batch size is usually fixed during training and inference by GPU memory constraints. Some frameworks like TensorFlow allow using dynamic batch sizes.\n\nBias term\n\na term that allows for the identification of the neuron threshold as the weight on a special, constant input.\n\nBayesian neural network\n\nA probabilistic neural network that accounts for uncertainty in weights and outputs. A Bayesian neural network relies on Bayes’ Theorem to calculate uncertainties in weights and predictions. A Bayesian neural network can be useful when it is important to quantify uncertainty, such as in models related to pharmaceuticals. Bayesian neural networks can also help prevent overfitting.\n\nBayesian optimization\n\nA probabilistic regression model technique for optimizing computationally expensive objective functions by instead optimizing a surrogate that quantifies the uncertainty via a Bayesian learning technique. Since Bayesian optimization is itself very expensive, it is usually used to optimize expensive-to-evaluate tasks that have a small number of parameters, such as selecting hyperparameters.\n\nBinning\n\nsynonym for bucketing\n\nBoltzmann machine\n\nan algorithm for learning the probability distribution on a set of inputs by means of weight changes using noisy responses.\n\nBoosting\n\nA machine learning technique that iteratively combines a set of simple and not very accurate classifiers (referred to as “weak” classifiers) into a classifier with high accuracy (a “strong” classifier) by upweighting the examples that the model is currently misclassifying.\n\nbucketing\n\nConverting a (usually continuous) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins.\n\ncategorical\n\nFeatures or columns in the data with a discrete set of possible values.\n\nConnection weight\n\nThe parameter which is used to set the importance to an input coming to a given neuron from another one.\n\nDelta rule\n\nthe simplest learning rule, in which weights are changed proportionally to the discrepancy between actual output and desired output.\n\nError surface\n\nthe surface in the weight space indicating how the error in the output of a neural network depends on these weights.\n\nFeature\n\na column in a training case Feed-in\n\n\nthe number of inputs for a unit\n\nFan out\n\nthe amount of spread in output from a neuron.\n\nHebb learning law\n\nmodification of a connection weight proportional to the activities of the input and output neurons.\n\nHopfield network\n\na network with symmetric connection weights and thresholding of neural response.\n\nInput\n\nis ambiguous, because more often, input is short for **input neuron`.\n\nInput unit\n\nspecial neuron receiving only input activity which is fed on to the rest of the network.\n\nLayer\n\na collection of neurons all of which receive input from a preceding set of neurons (or inputs), and send their outputs to other neurons or outside the net.\n\nLearning law\n\nrule for changing the connection weights in a neural network.\n\nLearning rate\n\namount by which the connection weights change at each learning step.\n\nMomentum\n\na term added to the weight change in back-propagation to achieve better learning by jumping out of local minima.\n\nNeuron\n\na synonym for unit emphasizing the analogy with real brains.\n\nOutput\n\nlike value but emphasizing that it’s different from the input.\n\nParameter\n\nthe weights and biases learned by the network. Additional parameters - which are not necessarily learned or not directly part of the network are called hyperparameters\n\nRecurrent neural network\n\none in which output activity is fed back into the input or hidden layers. Also called RNN Reinforcement training\n\n\nmodification of connection weights.\n\nTest set\n\nthe set of input and output patterns used to test if a neural network has been trained effectively.\n\nTraining set\n\nthe set of input-output patterns provided to train the network.\n\nTraining case\n\na row in the dataset is the most commonly used and is quite generic. Also called input and training example Training example\n\n\nemphasizes the analogy with human learning: we learn from examples.\n\nTraining point\n\nemphasizes that it’s a location in a high-dimensional space.\n\nUnit\n\na node in a neural network`. Nodes consists of an activation function, a weight, an input and output called the activation. The term unit emphasizes that it’s one component of a large network. Also referred to as a neuron** .\n\nValue\n\na synonym for activation, referencing the output value of the activation function (RELU, sigmoid, tanh, etc.) when acting on its input.\n\nWeight space\n\nA high dimensional space with each dimension corresponding to the weight of a single neuron`. Weight space corresponds to the space of all possible weights. Each point in the space is a collection of weights and each training case can be represented as a hyper-plane** passing through the origin. See also error surface\n\nloss function\n\nemphasizes that we’re minimizing it, without saying much about what the meaning of the number is.\n\nerror function\n\nemphasizes that it’s the extent to which the network gets things wrong.\n\nobjective function\n\nis very generic. This is the only one where it’s not clear whether we’re minimizing or maximizing it.\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Glossary of Terms for {Deep} {Neural} {Networks}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//notes/dnn/dnn-glossery/glossary.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Glossary of Terms for Deep Neural\nNetworks.” August 6, 2017. https://orenbochman.github.io/blog//notes/dnn/dnn-glossery/glossary.html."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html",
    "href": "notes/dnn/dnn-16/l_16.html",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#modeling-the-joint-density-of-images-and-captions",
    "href": "notes/dnn/dnn-16/l_16.html#modeling-the-joint-density-of-images-and-captions",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Modeling the joint density of images and captions",
    "text": "Modeling the joint density of images and captions\n(Srivastava and Salakhutdinov, NIPS 2012)\n\nGoal: To build a joint density model of captions and standard computer vision feature vectors extracted from real photographs.\n\nThis needs a lot more computation than building a joint density model of labels and digit images!\n\n\n\nTrain a multilayer model of images.\nTrain a separate multilayer model of word-count vectors.\nThen add a new top layer that is connected to the top layers of both individual models.\n\nUse further joint training of the whole system to allow each modality to improve the earlier layers of the other modality.\n\n\n\nInstead of using a deep belief net, use a deep Boltzmann machine that has symmetric connections between all pairs of layers.\n\nFurther joint training of the whole DBM allows each modality to improve the earlier layers of the other modality.\nThat’s why they used a DBM.\nThey could also have used a DBN and done generative fine-tuning with contrastive wake-sleep.\n\nBut how did they pre-train the hidden layers of a deep Boltzmann Machine?\n\nStandard pre-training leads to composite model that is a DBN not a DBM."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#combining-three-rbms-to-make-a-dbm",
    "href": "notes/dnn/dnn-16/l_16.html#combining-three-rbms-to-make-a-dbm",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Combining three RBMs to make a DBM",
    "text": "Combining three RBMs to make a DBM\n\n\n\n\n\nCombining RBMs into a DBM\n\n\n\nThe top and bottom RBMs must be pretrained with the weights in one direction twice as big as in the other direction.\n\nThis can be justified!\n\nThe middle layers do geometric model averaging."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#why-convolutional-neural-networks-are-doomed",
    "href": "notes/dnn/dnn-16/l_16.html#why-convolutional-neural-networks-are-doomed",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Why convolutional neural networks are doomed",
    "text": "Why convolutional neural networks are doomed\n\nPooling loses the precise spatial relationships between higher-level parts such as a nose and a mouth.\n\nThe precise spatial relationships are needed for identity recognition.\nOverlapping the pools helps a bit.\n\nConvolutional nets that just use translations cannot extrapolate their understanding of geometric relationships to radically new viewpoints.\n\nPeople are very good at extrapolating. After seeing a new shape once they can recognize it from a different viewpoint."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#the-hierarchical-coordinate-frame-approach",
    "href": "notes/dnn/dnn-16/l_16.html#the-hierarchical-coordinate-frame-approach",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "The hierarchical coordinate frame approach",
    "text": "The hierarchical coordinate frame approach\n\n\n\n\n\nhierarchical frames\n\n\n\nUse a group of neurons to represent the conjunction of the shape of a feature and its pose relative to the retina.\n\nThe pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic coordinate frame of the feature.\n\nRecognize larger features by using the consistency of the poses of their parts."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#two-layers-in-a-hierarchy-of-parts",
    "href": "notes/dnn/dnn-16/l_16.html#two-layers-in-a-hierarchy-of-parts",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Two layers in a hierarchy of parts",
    "text": "Two layers in a hierarchy of parts\n\n\n\n\n\ntwo_layer_hierarchy\n\n\n\nA higher level visual entity is present if several lower level visual entities can agree on their predictions for its pose (inverse computer graphics!)"
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#a-crucial-property-of-the-pose-vectors",
    "href": "notes/dnn/dnn-16/l_16.html#a-crucial-property-of-the-pose-vectors",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "A crucial property of the pose vectors",
    "text": "A crucial property of the pose vectors\n\nThey allow spatial transformations to be modeled by linear operations.\n\nThis makes it easy to learn a hierarchy of visual entities.\nIt makes it easy to generalize across viewpoints.\n\nThe invariant geometric properties of a shape are in the weights, not in the activities.\n\nThe activities are equivariant: As the pose of the object varies, the activities all vary.\nThe percept of an object changes as the viewpoint changes."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#evidence-that-our-visual-systems-impose-coordinate-frames-in-order-to-represent-shapes-after-irvin-rock",
    "href": "notes/dnn/dnn-16/l_16.html#evidence-that-our-visual-systems-impose-coordinate-frames-in-order-to-represent-shapes-after-irvin-rock",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Evidence that our visual systems impose coordinate frames in order to represent shapes (after Irvin Rock)",
    "text": "Evidence that our visual systems impose coordinate frames in order to represent shapes (after Irvin Rock)\n\n\n\nWhat country is this? Hint: Sarah Palin\nThe square and the diamond are very different percepts that make different properties obvious."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#let-machine-learning-figure-out-the-hyper-parameters",
    "href": "notes/dnn/dnn-16/l_16.html#let-machine-learning-figure-out-the-hyper-parameters",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Let machine learning figure out the hyper-parameters!",
    "text": "Let machine learning figure out the hyper-parameters!\n(Snoek, Larochelle & Adams, NIPS 2012)\n\nOne of the commonest reasons for not using neural networks is that it requires a lot of skill to set hyper-parameters.\n\nNumber of layers\nNumber of units per layer\nType of unit\nWeight penalty\nLearning rate\nMomentum etc. etc.\n\nNaive grid search: Make a list of alternative values for each hyperparameter and then try all possible combinations.\n\nCan we do better than this?\n\nSampling random combinations: This is much better if some hyperparameters have no effect.\n\nIts a big waste to exactly repeat the settings of the other hyperparameters."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#machine-learning-to-the-rescue",
    "href": "notes/dnn/dnn-16/l_16.html#machine-learning-to-the-rescue",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Machine learning to the rescue",
    "text": "Machine learning to the rescue\n\nInstead of using random combinations of values for the hyper-parameters, why not look at the results so far?\n\nPredict regions of the hyperparameter space that might give better results.\nWe need to predict how well a new combination will do and also model the uncertainty of that prediction.\n\nWe assume that the amount of computation involved in evaluating one setting of the hyper-parameters is huge.\n\nMuch more than the work involved in building a model that predicts the result from knowing previous results with different settings of the hyper-parameters."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#gaussian-process-models",
    "href": "notes/dnn/dnn-16/l_16.html#gaussian-process-models",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Gaussian Process models",
    "text": "Gaussian Process models\n\nThese models assume that similar inputs give similar outputs.\n\nThis is a very weak but very sensible prior for the effects of hyper-parameters.\n\nFor each input dimension, they learn the appropriate scale for measuring similarity.\n\nIs 200 similar to 300?\nLook to see if they give similar results in the data so far.\n\nGP models do more than just predicting a single value.\n\nThey predict a Gaussian distribution of values.\n\nFor test cases that are close to several, consistent training cases the predictions are fairly sharp.\nFor test cases far from any training cases, the predictions have high variance."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#a-sensible-way-to-decide-what-to-try",
    "href": "notes/dnn/dnn-16/l_16.html#a-sensible-way-to-decide-what-to-try",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "A sensible way to decide what to try",
    "text": "A sensible way to decide what to try\n\n\n\nhedging.png\n\n\n\nKeep track of the best setting so far.\nAfter each experiment this might stay the same or it might improve if the latest result is the best.\nPick a setting of the hyperparameters such that the expected improvement in our best setting is big.\n\ndon’t worry about the downside (hedge funds!)"
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#how-well-does-bayesian-optimization-work",
    "href": "notes/dnn/dnn-16/l_16.html#how-well-does-bayesian-optimization-work",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "How well does Bayesian optimization work?",
    "text": "How well does Bayesian optimization work?\n\nIf you have the resources to run a lot of experiments, Bayesian optimization is much better than a person at finding good combinations of hyper-parameters.\n\nThis is not the kind of task we are good at.\nWe cannot keep in mind the results of 50 different experiments and see what they predict.\n\nIt’s much less prone to doing a good job for the method we like and a bad job for the method we are comparing with.\n\nPeople cannot help doing this. They try much harder for their own method because they know it ought to work better!"
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#why-we-cannot-predict-the-long-term-future",
    "href": "notes/dnn/dnn-16/l_16.html#why-we-cannot-predict-the-long-term-future",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "Why we cannot predict the long-term future",
    "text": "Why we cannot predict the long-term future\n\nConsider driving at night. The number of photons you receive from the tail-lights of the car in front falls off as \\frac{1}{d^2}\nNow suppose there is fog.\n\nFor small distances its still \\frac{1}{d^2}\nBut for big distances its \\exp(-d) because fog absorbs a certain fraction of the photons per unit distance.\n\nSo the car in front becomes completely invisible at a distance at which our short-range \\frac{1}{d^2} model predicts it will be very visible.\n\nThis kills people."
  },
  {
    "objectID": "notes/dnn/dnn-16/l_16.html#the-effect-of-exponential-progress",
    "href": "notes/dnn/dnn-16/l_16.html#the-effect-of-exponential-progress",
    "title": "Deep Neural Networks - Notes for Lesson 16",
    "section": "The effect of exponential progress",
    "text": "The effect of exponential progress\n\nOver the short term, things change slowly and its easy to predict progress.\n\nWe can all make quite good guesses about what will be in the iPhone 6.\n\nBut in the longer run our perception of the future hits a wall, just like fog.\nSo the long term future of machine learning and neural nets is a total mystery.\n\nBut over the next five years, its highly probable that big, deep neural networks will do amazing things.\n\n\n\n\n\nCombining RBMs into a DBM\nhierarchical frames\ntwo_layer_hierarchy\nhedging.png"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html",
    "href": "notes/dnn/dnn-08/l_08.html",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#how-much-can-we-reduce-the-error-by-moving-in-a-given-direction",
    "href": "notes/dnn/dnn-08/l_08.html#how-much-can-we-reduce-the-error-by-moving-in-a-given-direction",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "How much can we reduce the error by moving in a given direction?",
    "text": "How much can we reduce the error by moving in a given direction?\n\nIf we choose a direction to move in and we keep going in that direction, how much does the error decrease before it starts rising again? We assume the curvature is constant (i.e. it’s a quadratic error surface).\n\nAssume the magnitude of the gradient decreases as we move down the gradient (i.e. the error surface is convex upward).\n\nThe maximum error reduction depends on the ratio of the gradient to the curvature. So a good direction to move in is one with a high ratio of gradient to curvature, even if the gradient itself is small.\n\nHow can we find directions like these?"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#newtons-method",
    "href": "notes/dnn/dnn-08/l_08.html#newtons-method",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Newton’s method",
    "text": "Newton’s method\n\nThe basic problem with steepest descent on a quadratic error surface is that the gradient is not the direction we want to go in.\n\nIf the error surface has circular cross-sections, the gradient is fine.\nSo lets apply a linear transformation that turns ellipses into circles.\n\nNewton’s method multiplies the gradient vector by the inverse of the curvature matrix, H \n\\Delta w = − \\epsilon H(w)^{-1}\\frac{dE}{dw}dw\n\n\nOn a real quadratic surface it jumps to the minimum in one step.\nUnfortunately, with only a million weights, the curvature matrix has a trillion terms and it is totally infeasible to invert it."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#curvature-matrices",
    "href": "notes/dnn/dnn-08/l_08.html#curvature-matrices",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Curvature Matrices",
    "text": "Curvature Matrices\n\n\n\n\n\nCurvature Matrices\n\n\n\nEach element in the curvature matrix specifies how the gradient in one direction changes as we move in some other direction.\n\nThe off-diagonal terms correspond to twists in the error surface.\n\nThe reason steepest descent goes wrong is that the gradient for one weight gets messed up by the simultaneous changes to all the other weights.\nThe curvature matrix determines the sizes of these interactions."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#how-to-avoid-inverting-a-huge-matrix",
    "href": "notes/dnn/dnn-08/l_08.html#how-to-avoid-inverting-a-huge-matrix",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "How to avoid inverting a huge matrix",
    "text": "How to avoid inverting a huge matrix\n\nThe curvature matrix has too many terms to be of use in a big network.\n\nMaybe we can get some benefit from just using the terms along the leading diagonal (Le Cun). But the diagonal terms are only a tiny fraction of the interactions (they are the self-interactions).\n\nThe curvature matrix can be approximated in many different ways\n\nHessian-free methods, LBFGS, ….\n\nIn the HF method, we make an approximation to the curvature matrix and then, assuming that approximation is correct, we minimize the error using an efficient technique called conjugate gradient. Then we make another approximation to the curvature matrix and minimize again.\n\nFor RNNs its important to add a penalty for changing any of the hidden activities too much."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#conjugate-gradient",
    "href": "notes/dnn/dnn-08/l_08.html#conjugate-gradient",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Conjugate gradient",
    "text": "Conjugate gradient\n\nThere is an alternative to going to the minimum in one step by multiplying by the inverse of the curvature matrix.\n\nUse a sequence of steps each of which finds the minimum along one direction.\n\nEnsure that each new direction is “conjugate” to the previous directions so you do not mess up the minimization you already did.\n\n“conjugate” means that as you go in the new direction, you do not change the gradients in the previous directions."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#a-picture-of-conjugate-gradient",
    "href": "notes/dnn/dnn-08/l_08.html#a-picture-of-conjugate-gradient",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "A picture of conjugate gradient",
    "text": "A picture of conjugate gradient\n\n\n\n\n\nconjugate gradient\n\n\n\nThe gradient in the direction of the first step is zero at all points on the green line.\nSo if we move along the green line we don’t mess up the minimization we already did in the first direction."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#what-does-conjugate-gradient-achieve",
    "href": "notes/dnn/dnn-08/l_08.html#what-does-conjugate-gradient-achieve",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "What does conjugate gradient achieve?",
    "text": "What does conjugate gradient achieve?\n\nAfter N steps, conjugate gradient is guaranteed to find the minimum of an N-dimensional quadratic surface. Why?\nAfter many less than N steps it has typically got the error very close to the minimum value.\nConjugate gradient can be applied directly to a non-quadratic error surface and it usually works quite well (non-linear conjugate grad.)\nThe HF optimizer uses conjugate gradient for minimization on a genuinely quadratic surface where it excels.\nThe genuinely quadratic surface is the quadratic approximation to the true surface."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#modeling-text-advantages-of-working-with-characters",
    "href": "notes/dnn/dnn-08/l_08.html#modeling-text-advantages-of-working-with-characters",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Modeling text: Advantages of working with characters",
    "text": "Modeling text: Advantages of working with characters\n\nThe web is composed of character strings.\nAny learning method powerful enough to understand the world by reading the web ought to find it trivial to learn which strings make words (this turns out to be true, as we shall see).\n\nPre-processing text to get words is a big hassle\n\nWhat about morphemes (prefixes, suffixes etc)\nWhat about subtle effects like “sn” words?\nWhat about New York?\nWhat about Finnish: ymmartamattomyydellansakaan"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#an-obvious-recurrent-neural-net",
    "href": "notes/dnn/dnn-08/l_08.html#an-obvious-recurrent-neural-net",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "An obvious recurrent neural net",
    "text": "An obvious recurrent neural net\n\n\n\n\n\nsimple rnn"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#a-sub-tree-in-the-tree-of-all-character-strings",
    "href": "notes/dnn/dnn-08/l_08.html#a-sub-tree-in-the-tree-of-all-character-strings",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "A sub-tree in the tree of all character strings",
    "text": "A sub-tree in the tree of all character strings\n\n\n\n\n\nsub tree\n\n\n\nIf the nodes are implemented as hidden states in an RNN, different nodes can share structure because they use distributed representations.\nThe next hidden representation needs to depend on the conjunction of the current character and the current hidden representation"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#multiplicative-connections",
    "href": "notes/dnn/dnn-08/l_08.html#multiplicative-connections",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Multiplicative connections",
    "text": "Multiplicative connections\n\nInstead of using the inputs to the recurrent net to provide additive extra input to the hidden units, we could use the current input character to choose the whole hidden-to-hidden weight matrix.\n\nBut this requires 86x1500x1500 parameters\nThis could make the net overfit.\n\nCan we achieve the same kind of multiplicative interaction using fewer parameters?\n\nWe want a different transition matrix for each of the 86 characters, but we want these 86 character-specific weight matrices to share parameters (the characters 9 and 8 should have similar matrices)."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#using-factors-to-implement-multiplicative-interactions",
    "href": "notes/dnn/dnn-08/l_08.html#using-factors-to-implement-multiplicative-interactions",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Using factors to implement multiplicative interactions",
    "text": "Using factors to implement multiplicative interactions\n\n\n\n\n\nusing factors\n\n\n\nWe can get groups a and b to interact multiplicatively by using “factors”.\n\nEach factor first computes a weighted sum for each of its input groups.\nThen it sends the product of the weighted sums to its output group.\n\n\n\n\\begin{aligned}\n  \\color{blue}{c_f} &= \\color{red}{(b^T w_f)}\\color{green}{(a^T a_f)} v_f  \n\\end{aligned}\n - bkue - vector of inputs to group c - red - scalar input to f from group b - green - scalar input to f from group a"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#using-factors-to-implement-a-set-of-basis-matrices",
    "href": "notes/dnn/dnn-08/l_08.html#using-factors-to-implement-a-set-of-basis-matrices",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Using factors to implement a set of basis matrices",
    "text": "Using factors to implement a set of basis matrices\n\nWe can think about factors another way:\n\nEach factor defines a rank 1 transition matrix from a to c.\n\n\n\n\\begin{aligned}\n  c_f &= (b^T w_f)(a^T a_f)v_f  \\\\\n  c_f &= \\color{purple}{(b^T w_f)}\\color{pink}{(u_fv_f^T)}a  \\\\\n  c &= \\big(\\sum_f(b^T w_f)(u_f v_f^T)\\big)a\n\\end{aligned}\n\n\npurple - scaler coefficient\npink - outer product transition matrix with rank 1"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#using-3-way-factors-to-allow-a-character-to-create-a-whole-transition-matrix",
    "href": "notes/dnn/dnn-08/l_08.html#using-3-way-factors-to-allow-a-character-to-create-a-whole-transition-matrix",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Using 3-way factors to allow a character to create a whole transition matrix",
    "text": "Using 3-way factors to allow a character to create a whole transition matrix\n\n\n\nUsing 3-way factors\n\n\nEach character, k, determines a gain w_{kf} for each of these matrices."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#training-the-character-model",
    "href": "notes/dnn/dnn-08/l_08.html#training-the-character-model",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Training the character model",
    "text": "Training the character model\n\nIlya Sutskever used 5 million strings of 100 characters taken from wikipedia. For each string he starts predicting at the 11th character.\nUsing the HF optimizer, it took a month on a GPU board to get a really good model.\nIlya’s current best RNN is probably the best single model for character prediction (combinations of many models do better).\nIt works in a very different way from the best other models.\n\nIt can balance quotes and brackets over long distances. Models that rely on matching previous contexts cannot do this."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#how-to-generate-character-strings-from-the-model",
    "href": "notes/dnn/dnn-08/l_08.html#how-to-generate-character-strings-from-the-model",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "How to generate character strings from the model",
    "text": "How to generate character strings from the model\n\nStart the model with its default hidden state.\nGive it a “burn-in” sequence of characters and let it update its hidden state after each character.\nThen look at the probability distribution it predicts for the next character.\nPick a character randomly from that distribution and tell the net that this was the character that actually occurred.\n\ni.e. tell it that its guess was correct, whatever it guessed.\n\nContinue to let it pick characters until bored.\nLook at the character strings it produces to see what it “knows”.\n\n\nHe was elected President during the Revolutionary War and forgave Opus Paul at Rome. The regime of his crew of England, is now Arab women’s icons in and the demons that use something between the characters‘ sisters in lower coil trains were always operated on the line of the ephemerable street, respectively, the graphic or other facility for deformation of a given proportion of large segments at RTUS). The B every chord was a “strongly cold internal palette pour even the white blade.”"
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#some-completions-produced-by-the-model",
    "href": "notes/dnn/dnn-08/l_08.html#some-completions-produced-by-the-model",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Some completions produced by the model",
    "text": "Some completions produced by the model\n\nSheila thrunges (most frequent)\nPeople thrunge (most frequent next character is space)\nShiela, Thrungelini del Rey (first try)\nThe meaning of life is literary recognition. (6th try)\nThe meaning of life is the tradition of the ancient human reproduction: it is less favorable to the good boy for when to remove her bigger. (one of the first 10 tries for a model trained for longer)."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#what-does-it-know",
    "href": "notes/dnn/dnn-08/l_08.html#what-does-it-know",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "What does it know?",
    "text": "What does it know?\n\nIt knows a huge number of words and a lot about proper names, dates, and numbers.\nIt is good at balancing quotes and brackets.\n\nIt can count brackets: none, one, many\n\nIt knows a lot about syntax but its very hard to pin down exactly what form this knowledge has.\nIts syntactic knowledge is not modular.\nIt knows a lot of weak semantic associations\nE.g. it knows Plato is associated with Wittgenstein and cabbage is associated with vegetable."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#rnns-for-predicting-the-next-word",
    "href": "notes/dnn/dnn-08/l_08.html#rnns-for-predicting-the-next-word",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "RNNs for predicting the next word",
    "text": "RNNs for predicting the next word\n\nTomas Mikolov and his collaborators have recently trained quite large RNNs on quite large training sets using BPTT.\n\nThey do better than feed-forward neural nets.\nThey do better than the best other models.\nThey do even better when averaged with other models.\n\nRNNs require much less training data to reach the same level of performance as other models.\nRNNs improve faster than other methods as the dataset gets bigger.\n\nThis is going to make them very hard to beat."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#the-key-idea-of-echo-state-networks-perceptrons-again",
    "href": "notes/dnn/dnn-08/l_08.html#the-key-idea-of-echo-state-networks-perceptrons-again",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "The key idea of echo state networks (perceptrons again?)",
    "text": "The key idea of echo state networks (perceptrons again?)\n\nA very simple way to learn a feedforward network is to make the early layers random and fixed.\nThen we just learn the last layer which is a linear model that uses the transformed inputs to predict the target outputs.\n\nA big random expansion of the input vector can help.\n\nThe equivalent idea for RNNs is to fix the input \\to hidden connections and the hidden \\to hidden connections at random values and only learn the hidden \\to output connections.\n\nThe learning is then very simple (assuming linear output units).\nIts important to set the random connections very carefully so the RNN does not explode or die."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#setting-the-random-connections-in-an-echo-state-network",
    "href": "notes/dnn/dnn-08/l_08.html#setting-the-random-connections-in-an-echo-state-network",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Setting the random connections in an Echo State Network",
    "text": "Setting the random connections in an Echo State Network\n\nSet the hidden \\to hidden weights so that the length of the activity vector stays about the same after each iteration.\n\nThis allows the input to echo around the network for a long time.\n\nUse sparse connectivity (i.e. set most of the weights to zero).\n\nThis creates lots of loosely coupled oscillators.\n\nChoose the scale of the input \\to hidden connections very carefully.\n\nThey need to drive the loosely coupled oscillators without wiping out the information from the past that they already contain.\n\nThe learning is so fast that we can try many different scales for the weights and sparsenesses.\n\nThis is often necessary."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#a-simple-example-of-an-echo-state-network",
    "href": "notes/dnn/dnn-08/l_08.html#a-simple-example-of-an-echo-state-network",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "A simple example of an echo state network",
    "text": "A simple example of an echo state network\nINPUT SEQUENCE\nA real-valued time-varying value that specifies the frequency of a sine wave.\nTARGET OUTPUT SEQUENCE\nA sine wave with the currently specified frequency.\nLEARNING METHOD\nFit a linear model that takes the states of the hidden units as input and produces a single scalar output."
  },
  {
    "objectID": "notes/dnn/dnn-08/l_08.html#beyond-echo-state-networks",
    "href": "notes/dnn/dnn-08/l_08.html#beyond-echo-state-networks",
    "title": "Deep Neural Networks - Notes for Lesson 8",
    "section": "Beyond echo state networks",
    "text": "Beyond echo state networks\n\nGood aspects of ESNs Echo state networks can be trained very fast because they just fit a linear model.\nThey demonstrate that its very important to initialize weights sensibly.\nThey can do impressive modeling of one-dimensional time-series.\n\nbut they cannot compete seriously for high-dimensional data like pre-processed speech.\n\nBad aspects of ESNs They need many more hidden units for a given task than an RNN that learns the hidden \\to hidden weights.\nIlya Sutskever (2012) has shown that if the weights are initialized using the ESN methods, RNNs can be trained very effectively.\n\nHe uses rmsprop with momentum\n\n\n\n\n\nCurvature Matrices\nconjugate gradient\nsimple rnn\nsub tree\nusing factors\nUsing 3-way factors"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html",
    "href": "notes/dnn/dnn-06/l06b.html",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#be-careful-about-turning-down-the-learning-rate",
    "href": "notes/dnn/dnn-06/l06b.html#be-careful-about-turning-down-the-learning-rate",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Be careful about turning down the learning rate",
    "text": "Be careful about turning down the learning rate\n\n\n\n\nTurning down the learning rate reduces the random fluctuations in the error due to the different gradients on different mini-batches. -So we get a quick win.\n\nBut then we get slower learning.\n\nDon’t turn down the learning rate too soon!"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#initializing-the-weights",
    "href": "notes/dnn/dnn-06/l06b.html#initializing-the-weights",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Initializing the weights",
    "text": "Initializing the weights\n\nIf two hidden units have exactly the same bias and exactly the same incoming and outgoing weights, they will always get exactly the same gradient.\n\nSo they can never learn to be different features.\nWe break symmetry by initializing the weights to have small random values.\n\nIf a hidden unit has a big fan-in, small changes on many of its incoming weights can cause the learning to overshoot.\n\nWe generally want smaller incoming weights when the fan-in is big, so initialize the weights to be proportional to sqrt(fan-in).\n\nWe can also scale the learning rate the same way."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#shifting-the-inputs",
    "href": "notes/dnn/dnn-06/l06b.html#shifting-the-inputs",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Shifting the inputs",
    "text": "Shifting the inputs\n\n\n\n\n\nShifting the inputs\n\n\n\nWhen using steepest descent, shifting the input values makes a big difference. -It usually helps to transform each component of the input vector so that it has zero mean over the whole training set.\n\nThe hypberbolic tangent (which is 2*logistic -1) produces hidden activations that are roughly zero mean.\n-In this respect its beYer than the logistic."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#scaling-the-inputs",
    "href": "notes/dnn/dnn-06/l06b.html#scaling-the-inputs",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Scaling the inputs",
    "text": "Scaling the inputs\n\n\n\n\n\nScaling the inputs\n\n\n\nWhen using steepest descent, scaling the input values makes a big difference.\n\nIt usually helps to transform each component of the input vector so that it has unit variance over the whole training set."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#a-more-thorough-method-decorrelate-the-input-components",
    "href": "notes/dnn/dnn-06/l06b.html#a-more-thorough-method-decorrelate-the-input-components",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "A more thorough method: Decorrelate the input components",
    "text": "A more thorough method: Decorrelate the input components\n\nFor a linear neuron, we get a big win by decorrelating each component of the input from the other input components.\nThere are several different ways to decorrelate inputs. A reasonable method is to use Principal Components Analysis.\n\nDrop the principal components with the smallest eigenvalues.\n\nThis achieves some dimensionality reduction.\n\nDivide the remaining principal components by the square roots of their eigenvalues. For a linear neuron, this converts an axis aligned elliptical error surface into a circular one.\n\nFor a circular error surface, the gradient points straight towards the minimum."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#common-problems-that-occur-in-multilayer-networks",
    "href": "notes/dnn/dnn-06/l06b.html#common-problems-that-occur-in-multilayer-networks",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Common problems that occur in multilayer networks",
    "text": "Common problems that occur in multilayer networks\n\nIf we start with a very big learning rate, the weights of each hidden unit will all become very big and positive or very big and negative.\n\nThe error derivatives for the hidden units will all become tiny and the error will not decrease.\nThis is usually a plateau, but people often mistake it for a local minimum.\n\nIn classification networks that use a squared error or a cross-entropy error, the best guessing strategy is to make each output unit always produce an output equal to the proportion of time it should be a 1.\n\nThe network finds this strategy quickly and may take a long time to improve on it by making use of the input.\n\nThis is another plateau that looks like a local minimum."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06b.html#four-ways-to-speed-up-mini-batch-learning",
    "href": "notes/dnn/dnn-06/l06b.html#four-ways-to-speed-up-mini-batch-learning",
    "title": "Deep Neural Networks - Notes for lecture 6b",
    "section": "Four ways to speed up mini-batch learning",
    "text": "Four ways to speed up mini-batch learning\n\nUse “momentum”\n\nInstead of using the gradient to change the position of the weight “particle”, use it to change the velocity.\n\n\nUse separate adaptive learning rates for each parameter\n\nSlowly adjust the rate using the consistency of the gradient for that parameter.\n\nrmsprop: Divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n\nThis is the mini-batch version of just using the sign of the gradient.\n\n\nTake a fancy method from the optimization literature that makes use of curvature information (not this lecture)\n\nAdapt it to work for neural nets\nAdapt it to work for mini-batches.\n\n\n\n\n\nShifting the inputs\nScaling the inputs"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html",
    "href": "notes/dnn/dnn-06/l06a.html",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#reminder-the-error-surface-for-a-linear-neuron",
    "href": "notes/dnn/dnn-06/l06a.html#reminder-the-error-surface-for-a-linear-neuron",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "Reminder: The error surface for a linear neuron",
    "text": "Reminder: The error surface for a linear neuron\n\n\n\n\n\nerror surface\n\n\n\nThe error surface lies in a space with a horizontal axis for each weight and one vertical axis for the error.\n\nFor a linear neuron with a squared error, it is a quadratic bowl.\nVertical cross-sections are parabolas.\n\nHorizontal cross-sections are ellipses.\n\n\nFor multi-layer, non-linear nets the error surface is much more complicated.\n\nBut locally, a piece of a quadratic bowl is usually a very good approximation."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#convergence-speed-of-full-batch-learning-when-the-error-surface-is-a-quadratic-bowl",
    "href": "notes/dnn/dnn-06/l06a.html#convergence-speed-of-full-batch-learning-when-the-error-surface-is-a-quadratic-bowl",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "Convergence speed of full batch learning when the error surface is a quadratic bowl",
    "text": "Convergence speed of full batch learning when the error surface is a quadratic bowl\n\n\n\n\n\nerror surface\n\n\n\nGoing downhill reduces the error, but the direction of steepest descent does not point at the minimum unless the ellipse is a circle.\n\nThe gradient is big in the direction in which we only want to travel a small distance.\n\nThe gradient is small in the direction in which we want to travel a large distance.\n\nEven for non-linear multi-layer nets, the error surface is locally quadratic, so the same speed issues apply."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#how-the-learning-goes-wrong",
    "href": "notes/dnn/dnn-06/l06a.html#how-the-learning-goes-wrong",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "How the learning goes wrong",
    "text": "How the learning goes wrong\n\n\n\n\n\nerror surface\n\n\n\nIf the learning rate is big, the weights slosh to and fro across the ravine.\n\nIf the learning rate is too big, this oscillation diverges.\n\nWhat we would like to achieve:\n\nMove quickly in directions with small but consistent gradients.\nMove slowly in directions with big but inconsistent gradients."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#stochastic-gradient-descent-sgd",
    "href": "notes/dnn/dnn-06/l06a.html#stochastic-gradient-descent-sgd",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "Stochastic gradient descent SGD",
    "text": "Stochastic gradient descent SGD\n\nIf the dataset is highly redundant, the gradient on the first half is almost identical to the gradient on the second half.\n\nSo instead of computing the full gradient, update the weights using the gradient on the first half and then get a gradient for the new weights on the second half.\nThe extreme version of this approach updates weights after each case. Its called online.\n\nMini-batches are usually better than online.\n\nLess computation is used updating the weights.\nComputing the gradient for many cases simultaneously uses matrix-matrix multiplies which are very efficient, especially on GPUs\n\nMini-batches need to be balanced for classes"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#two-types-of-learning-algorithm",
    "href": "notes/dnn/dnn-06/l06a.html#two-types-of-learning-algorithm",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "Two types of learning algorithm",
    "text": "Two types of learning algorithm\n\nIf we use the full gradient computed from all the training cases, there are many clever ways to speed up learning (e.g. non-linear conjugate gradient).\n\nThe optimization community has studied the general problem of optimizing smooth non-linear functions for many years.\n\nMultilayer neural nets are not typical of the problems they study so their methods may need a lot of adaptation.\n\n\nFor large neural networks with very large and highly redundant training sets, it is nearly always best to use mini-batch learning.\n\nThe mini-batches may need to be quite big when adapting fancy methods.\nBig mini-batches are more computationally efficient."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06a.html#a-basic-mini-batch-gradient-descent-algorithm",
    "href": "notes/dnn/dnn-06/l06a.html#a-basic-mini-batch-gradient-descent-algorithm",
    "title": "Deep Neural Networks - Notes for lecture 6a",
    "section": "A basic mini-batch gradient descent algorithm",
    "text": "A basic mini-batch gradient descent algorithm\n\nGuess an initial learning rate.\n\nIf the error keeps geang worse or oscillates wildly, reduce the learning rate.\nIf the error is falling fairly consistently but slowly, increase the learning rate.\n\n\nWrite a simple program to automate this way of adjusting the learning rate.\nTowards the end of mini-batch learning it nearly always helps to turn down the learning rate.\n\nThis removes fluctuations in the final weights caused by the variations between minibatches.\n\n\nTurn down the learning rate when the error stops decreasing.\n\nUse the error on a separate validation set\n\n\n\n\n\nerror surface\nerror surface\nerror surface"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06d.html",
    "href": "notes/dnn/dnn-06/l06d.html",
    "title": "Deep Neural Networks - Notes for lecture 6d",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06d.html#the-intuition-behind-separate-adaptive-learning-rates",
    "href": "notes/dnn/dnn-06/l06d.html#the-intuition-behind-separate-adaptive-learning-rates",
    "title": "Deep Neural Networks - Notes for lecture 6d",
    "section": "The intuition behind separate adaptive learning rates",
    "text": "The intuition behind separate adaptive learning rates\n\n\nIn a multilayer net, the appropriate learning rates can vary widely between weights:\n\nThe magnitudes of the gradients are often very different for different layers, especially if the initial weights are small.\nThe fan-in of a unit determines the size of the “overshoot” effects caused by simultaneously changing many of the incoming weights of a unit to correct the same error.\n\nSo use a global learning rate (set by hand) multiplied by an appropriate local gain that is determined empirically for each weight.\n\nGradients can get very small in the early layers of very deep nets.\nThe fan-in often varies widely between layers."
  },
  {
    "objectID": "notes/dnn/dnn-06/l06d.html#one-way-to-determine-the-individual-learning-rates",
    "href": "notes/dnn/dnn-06/l06d.html#one-way-to-determine-the-individual-learning-rates",
    "title": "Deep Neural Networks - Notes for lecture 6d",
    "section": "One way to determine the individual learning rates",
    "text": "One way to determine the individual learning rates\n\nStart with a local gain of 1 for every weight.\n\nIncrease the local gain if the gradient for that weight does not change sign.\nUse small additive increases and multiplicative decreases (for mini-batch)\n\nThis ensures that big gains decay rapidly when oscillations start.\nIf the gradient is totally random the gain will hover around 1 when we increase by plus \\delta half the time and decrease by times 1-\\delta half the time.\n\n\n\n\\Delta w_{ij} = -\\epsilon  g_{ij} \\frac{∂E}{∂_{w_{ij}}}\n \n\\text{if } (\\frac{∂E}{∂_{w_{ij}}}(t)\\frac{∂E}{∂_{w_{ij}}}(t-1))&gt;0\n \n\\text{then } g_{ij}(t) = g_{ij}(t − 1) + .05\n\n\n\\text{else } g_{ij}(t) = g_{ij} δ (t − 1 ) \\times .95"
  },
  {
    "objectID": "notes/dnn/dnn-06/l06d.html#tricks-for-making-adaptive-learning-rates-work-better",
    "href": "notes/dnn/dnn-06/l06d.html#tricks-for-making-adaptive-learning-rates-work-better",
    "title": "Deep Neural Networks - Notes for lecture 6d",
    "section": "Tricks for making adaptive learning rates work better",
    "text": "Tricks for making adaptive learning rates work better\n\nLimit the gains to lie in some reasonable range\n\ne.g. [0.1, 10] or [.01, 100]\n\nUse full batch learning or big minibatches\n\nThis ensures that changes in the sign of the gradient are not mainly due to the sampling error of a minibatch.\n\nAdaptive learning rates can be combined with momentum.\n\nUse the agreement in sign between the current gradient for a weight and the velocity for that weight (Jacobs, 1989).\n\n\nAdaptive learning rates only deal with axis-aligned effects.\n\nMomentum 🚀 does not care about the alignment of the axes."
  },
  {
    "objectID": "notes/dnn/dnn-10/r1.html",
    "href": "notes/dnn/dnn-10/r1.html",
    "title": "Deep Neural Networks — Readings I for Lesson 10",
    "section": "",
    "text": "Serial ensambles like bagging operate using a cooperative loss function for the ensemble. Parallel ensembles should use a competitive loss function for the ensamle. Neural networks are slow to train and are best combined in parallel. The paper considers the type of losses function that promotes.\n\n\n\n\nIn (Nowlan and Hinton 1990), the authors offer a fascinating insights into the mechanics of ensembling, which is the approach of aggregating several lower capacity models into a single high capacity model.\nThe ensembling is a form of the divide and conquer heuristic.\n\nBagging uses a parallel approach where we average the outcomes. By Fisher’s fundamental theorem of natural selection the more diverse the models in an ensemble the faster it will the ensemble will learn.\nBoosting uses a sequential approach - each subsequent model works on the residual of the previous\n\nIn the course, Hinton make the case that when ensembling neural networks one should use competition, rather than the cooperative approach that is the basis of bagging and boosting submodels.\nThe big idea here is to get the sub model to specialize thus getting the ML system to converge faster.\n\nConverging faster means shorter training time or\nGetting the same results on a smaller dataset — and not having enough data is a common problem.\n\nMixture of experts involves lots of overhead and so it is an approach we see less often. It is a way of squeezing more accuracy out of an ML model and so it tends to re-surface when a problem has matured and people are looking for small gains from better ML ops.\n\n\nNowlan, Steven, and Geoffrey E Hinton. 1990. “Evaluation of Adaptive Mixtures of Competing Experts.” In Advances in Neural Information Processing Systems, edited by R. P. Lippmann, J. Moody, and D. Touretzky. Vol. 3. Morgan-Kaufmann. https://proceedings.neurips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf.\nUnable to display PDF file. Download instead.\n\n\n\n“We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.”\n\n\n\nThis paper is the first time I read about ensembles - and was an introduction. I would later read much more in Intorduction to statistical learning using R1. As time goes by ensembles keep getting more of my attention. We put them to work in setting that provides higher capacity models for small data setting. Also, the gating network is like a meta model which may be adapted to quantify uncertainty for each expert at the training case level.\n1 citation neededThe architecture shown bellow uses expert networks trained on a vowel discrimination (classification) task alongside a gating network whose responsibility is to pick the best classifier for the input.\n\n\n\n\n\nensemble architecture\n\n\nI had been familiar with the idea that the gating network is responsible to convert the output of the experts to the actual experts. It turns out that the gating network also needs to learn which expert is better on a given type of input, and that it also controls the data expert get. This allocation can be hard (each training case goes to one expert) or soft (several experts are allocated). I also noted that some of the prior work was authored by Bastro, the leading authority on Reinforcement Learning. In prior work the gating network the learn to allocate training cases to one or a few expert - which allows them specialize (the weights are decoupled) also learns to The earlier idea is to utilize or learn to partition the training data so that one can train specialized models that are local experts on the problem space and then use some linear combination of the expert’s predictions to make predictions. But using such a linear combination requires that the expert cancel each other’s output.\n\n\n\n\nE^c= ||\\vec{d^c} -\\sum_i p_i^c \\vec o_i^c||^2\n\\tag{1}\nwhere :\n\n\\vec o_i^c is the output vector of expert i on case c.\n\\vec d_c is the desired output for case c.\n\nThe authors say that the cooperative loss function in (1) foster an unwanted coupling between the experts, in the sense that a change in one expert’s weights will create a residual loss seen by the other experts in (1). This leads to cooperation but each expert has learn to neutralize the residual it sees from the others experts. So in both cases all models contribute to the inference, instead of just one or a few, which is counter to the idea of being an expert on a subset of the data.\n\n\n\nIn (Jacobs et al. 1991) the authors used a hard selection mechanism by modifying the objective function to encourage competition and foster greater specialization by using only activate one expert at a time. This paper suggest that it is enough to modify the loss so that the experts compete. The idea being that “the selector acts as a multiple input, single output stochastic switch; the probability that the switch will select the output from expert j is p_j governed by:\n\nJacobs, Robert A, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. “Adaptive Mixtures of Local Experts.” Neural Computation 3 (1): 79–87. https://doi.org/10.1162/neco.1991.3.1.79.\n\nE^c = &lt;||\\vec d^c - \\vec o^c ||&gt; =\\sum_i\\ p_i^c||\\vec d^c- \\vec o_i||^2\n\\tag{2}\nwhere :\n\np_j = \\frac{e^{x_j}}{\\sum_i e^x_i}\n\nsoon we are shown a much better loss function:\n\n\n\ndoes not encourage cooperation rather than specialization, which required using many experts in each prediction. Later work added penalty terms in the objective function to gate a single active exert in the prediction. Jacobs, Jordan, and Barton, 1990. The paper offers an alternative error function that encourages specialization.\nThe difference difference between the error functions.\n\n\n\n\nE^c= -log\\sum_i\\ p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}\n\\tag{3}\n\n\n\nThe error defined in Equation 2 is simply the negative log probability of generating the desired output vector under the mixture of Gaussian’s model described at the end of the next section.\nTo see why this error function works better, it is helpful to compare the derivatives of the two error functions with respect to the output of an expert. From from Equation 2 we get:\n\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -2p_i^c(\\vec d^c-\\vec o_c^c)\n\\tag{4}\nwhile the derivative from Equation 3 gives us:\n\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -\\bigg[\\frac{p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}}{\\sum_j p_j^c e^{\\frac{1}{2}||d^c- \\vec o_j||^2}}\\bigg](\\vec d^c-\\vec o_c^c)\n\\tag{5}\nIn Equation 4 the term \\vec p^c_i is used to weigh the derivative for expert i, while in equation 5 the weighting term takes into account how well expert i does relative to other experts, which is a more useful measure of the relevance of expert i to training case c, especially early in the training. Suppose, that the gating network initially gives equal weights to all experts and ||d^c-\\vec o_j||&gt;1 for all the experts. Equation 4 will adapt the best-fitting expert the slowest, whereas Equation 5 will adapt it the fastest.\n\n\n\nIf two loss function are not enough, the authors now suggest a third loss function. This loss looks at the distance from the average vector. \nlogP^c= -log\\sum_i\\ p_i^c K e^{-\\frac{1}{2}||\\vec\\mu_i- \\vec o^c||^2}\n\\tag{6}"
  },
  {
    "objectID": "notes/dnn/dnn-10/r1.html#reading-adaptive-mixtures-of-local-experts",
    "href": "notes/dnn/dnn-10/r1.html#reading-adaptive-mixtures-of-local-experts",
    "title": "Deep Neural Networks — Readings I for Lesson 10",
    "section": "",
    "text": "Serial ensambles like bagging operate using a cooperative loss function for the ensemble. Parallel ensembles should use a competitive loss function for the ensamle. Neural networks are slow to train and are best combined in parallel. The paper considers the type of losses function that promotes.\n\n\n\n\nIn (Nowlan and Hinton 1990), the authors offer a fascinating insights into the mechanics of ensembling, which is the approach of aggregating several lower capacity models into a single high capacity model.\nThe ensembling is a form of the divide and conquer heuristic.\n\nBagging uses a parallel approach where we average the outcomes. By Fisher’s fundamental theorem of natural selection the more diverse the models in an ensemble the faster it will the ensemble will learn.\nBoosting uses a sequential approach - each subsequent model works on the residual of the previous\n\nIn the course, Hinton make the case that when ensembling neural networks one should use competition, rather than the cooperative approach that is the basis of bagging and boosting submodels.\nThe big idea here is to get the sub model to specialize thus getting the ML system to converge faster.\n\nConverging faster means shorter training time or\nGetting the same results on a smaller dataset — and not having enough data is a common problem.\n\nMixture of experts involves lots of overhead and so it is an approach we see less often. It is a way of squeezing more accuracy out of an ML model and so it tends to re-surface when a problem has matured and people are looking for small gains from better ML ops.\n\n\nNowlan, Steven, and Geoffrey E Hinton. 1990. “Evaluation of Adaptive Mixtures of Competing Experts.” In Advances in Neural Information Processing Systems, edited by R. P. Lippmann, J. Moody, and D. Touretzky. Vol. 3. Morgan-Kaufmann. https://proceedings.neurips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf.\nUnable to display PDF file. Download instead.\n\n\n\n“We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.”\n\n\n\nThis paper is the first time I read about ensembles - and was an introduction. I would later read much more in Intorduction to statistical learning using R1. As time goes by ensembles keep getting more of my attention. We put them to work in setting that provides higher capacity models for small data setting. Also, the gating network is like a meta model which may be adapted to quantify uncertainty for each expert at the training case level.\n1 citation neededThe architecture shown bellow uses expert networks trained on a vowel discrimination (classification) task alongside a gating network whose responsibility is to pick the best classifier for the input.\n\n\n\n\n\nensemble architecture\n\n\nI had been familiar with the idea that the gating network is responsible to convert the output of the experts to the actual experts. It turns out that the gating network also needs to learn which expert is better on a given type of input, and that it also controls the data expert get. This allocation can be hard (each training case goes to one expert) or soft (several experts are allocated). I also noted that some of the prior work was authored by Bastro, the leading authority on Reinforcement Learning. In prior work the gating network the learn to allocate training cases to one or a few expert - which allows them specialize (the weights are decoupled) also learns to The earlier idea is to utilize or learn to partition the training data so that one can train specialized models that are local experts on the problem space and then use some linear combination of the expert’s predictions to make predictions. But using such a linear combination requires that the expert cancel each other’s output.\n\n\n\n\nE^c= ||\\vec{d^c} -\\sum_i p_i^c \\vec o_i^c||^2\n\\tag{1}\nwhere :\n\n\\vec o_i^c is the output vector of expert i on case c.\n\\vec d_c is the desired output for case c.\n\nThe authors say that the cooperative loss function in (1) foster an unwanted coupling between the experts, in the sense that a change in one expert’s weights will create a residual loss seen by the other experts in (1). This leads to cooperation but each expert has learn to neutralize the residual it sees from the others experts. So in both cases all models contribute to the inference, instead of just one or a few, which is counter to the idea of being an expert on a subset of the data.\n\n\n\nIn (Jacobs et al. 1991) the authors used a hard selection mechanism by modifying the objective function to encourage competition and foster greater specialization by using only activate one expert at a time. This paper suggest that it is enough to modify the loss so that the experts compete. The idea being that “the selector acts as a multiple input, single output stochastic switch; the probability that the switch will select the output from expert j is p_j governed by:\n\nJacobs, Robert A, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. “Adaptive Mixtures of Local Experts.” Neural Computation 3 (1): 79–87. https://doi.org/10.1162/neco.1991.3.1.79.\n\nE^c = &lt;||\\vec d^c - \\vec o^c ||&gt; =\\sum_i\\ p_i^c||\\vec d^c- \\vec o_i||^2\n\\tag{2}\nwhere :\n\np_j = \\frac{e^{x_j}}{\\sum_i e^x_i}\n\nsoon we are shown a much better loss function:\n\n\n\ndoes not encourage cooperation rather than specialization, which required using many experts in each prediction. Later work added penalty terms in the objective function to gate a single active exert in the prediction. Jacobs, Jordan, and Barton, 1990. The paper offers an alternative error function that encourages specialization.\nThe difference difference between the error functions.\n\n\n\n\nE^c= -log\\sum_i\\ p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}\n\\tag{3}\n\n\n\nThe error defined in Equation 2 is simply the negative log probability of generating the desired output vector under the mixture of Gaussian’s model described at the end of the next section.\nTo see why this error function works better, it is helpful to compare the derivatives of the two error functions with respect to the output of an expert. From from Equation 2 we get:\n\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -2p_i^c(\\vec d^c-\\vec o_c^c)\n\\tag{4}\nwhile the derivative from Equation 3 gives us:\n\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -\\bigg[\\frac{p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}}{\\sum_j p_j^c e^{\\frac{1}{2}||d^c- \\vec o_j||^2}}\\bigg](\\vec d^c-\\vec o_c^c)\n\\tag{5}\nIn Equation 4 the term \\vec p^c_i is used to weigh the derivative for expert i, while in equation 5 the weighting term takes into account how well expert i does relative to other experts, which is a more useful measure of the relevance of expert i to training case c, especially early in the training. Suppose, that the gating network initially gives equal weights to all experts and ||d^c-\\vec o_j||&gt;1 for all the experts. Equation 4 will adapt the best-fitting expert the slowest, whereas Equation 5 will adapt it the fastest.\n\n\n\nIf two loss function are not enough, the authors now suggest a third loss function. This loss looks at the distance from the average vector. \nlogP^c= -log\\sum_i\\ p_i^c K e^{-\\frac{1}{2}||\\vec\\mu_i- \\vec o^c||^2}\n\\tag{6}"
  },
  {
    "objectID": "notes/dnn/dnn-10/r1.html#my-wrap-up",
    "href": "notes/dnn/dnn-10/r1.html#my-wrap-up",
    "title": "Deep Neural Networks — Readings I for Lesson 10",
    "section": "My wrap up 🎬",
    "text": "My wrap up 🎬\nI may not fully grasped all the ideas behind this loss and it requires reading additional papers as it was not covered in the lectures. The results parts compares number of epochs needed for different models ensembles and neural networks to reach some level of accuracy on the validation set. The application is also rather complex, but the vowel clustering task itself seems rather simple.\nI was glad I went over this aper as the notions of faster training, competitive loss, associative learning seem to resurface in rather varied contexts2 and knowing this results and paper is a decent starting point in the area.\n2 evolution of language, reinforcement learning, game theory, social dilemmas\nReferences\n\n\n\n\nensemble architecture"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html",
    "href": "notes/dnn/dnn-10/l_10.html",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#combining-networks-the-bias-variance-trade-off",
    "href": "notes/dnn/dnn-10/l_10.html#combining-networks-the-bias-variance-trade-off",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Combining networks: The bias-variance trade-off",
    "text": "Combining networks: The bias-variance trade-off\n\nWhen the amount of training data is limited, we get overfitting.\nAveraging the predictions of many different models is a good way to reduce overfitting.\nIt helps most when the models make very different predictions.\nFor regression, the squared error can be decomposed into a “bias” term and a “variance” term.\nThe bias term is big if the model has too little capacity to fit the data.\nThe variance term is big if the model has so much capacity that it is good at fitting the sampling error in each particular training set.\nBy averaging away the variance we can use individual models with high capacity. These models have high variance but low bias."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#how-the-combined-predictor-compares-with-the-individual-predictors",
    "href": "notes/dnn/dnn-10/l_10.html#how-the-combined-predictor-compares-with-the-individual-predictors",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "How the combined predictor compares with the individual predictors",
    "text": "How the combined predictor compares with the individual predictors\n\nOn any one test case, some individual predictors may be better than the combined predictor.\nBut different individual predictors will be better on different cases.\nIf the individual predictors disagree a lot, the combined predictor is typically better than all of the individual predictors when we average over test cases.\nSo we should try to make the individual predictors disagree (without making them much worse individually)."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#combining-networks-reduces-variance",
    "href": "notes/dnn/dnn-10/l_10.html#combining-networks-reduces-variance",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Combining networks reduces variance",
    "text": "Combining networks reduces variance\nWe want to compare two expected squared errors: Pick a predictor at random versus use the average of all the predictors:"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#a-picture",
    "href": "notes/dnn/dnn-10/l_10.html#a-picture",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "A picture",
    "text": "A picture\n\n\nThe predictors that are further than average from t make bigger than average squared errors.\nThe predictors that are nearer than average to t make smaller then average squared errors.\nThe first effect dominates because squares work like that.\nDon’t try averaging if you want to synchronize a bunch of clocks!\n\nThe noise is not Gaussian\n\n\n\n\\frac{ (\\bar{y}+\\epsilon)^2 + (\\bar{y} + \\epsilon )^2}{2} = \\bar{y}^2 + \\epsilon^2"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#what-about-discrete-distributions-over-class-labels",
    "href": "notes/dnn/dnn-10/l_10.html#what-about-discrete-distributions-over-class-labels",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "What about discrete distributions over class labels?",
    "text": "What about discrete distributions over class labels?\n\n\n\n\n\nSuppose that one model gives the correct label probability and the other model gives it\nIs it better to pick one model at random, or is it better to average the two probabilities?\n\n\n\n\\log \\Biggr( \\frac{ p_i + p_j }{2} \\Biggr) \\ge \\frac{\\log p_i + \\log p_j}{2}"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#overview-of-ways-to-make-predictors-differ",
    "href": "notes/dnn/dnn-10/l_10.html#overview-of-ways-to-make-predictors-differ",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Overview of ways to make predictors differ",
    "text": "Overview of ways to make predictors differ\n\nRely on the learning algorithm getting stuck in different local optima.\nA dubious hack (but worth a try).\nUse lots of different kinds of models, including ones that are not neural networks.\nDecision trees\nGaussian Process models\nSupport Vector Machines\nand many others.\nFor neural network models, make them different by using:\n\nDifferent numbers of hidden layers.\nDifferent numbers of units per layer.\nDifferent types of unit.\nDifferent types or strengths of weight penalty.\nDifferent learning algorithms."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#making-models-differ-by-changing-their-training-data",
    "href": "notes/dnn/dnn-10/l_10.html#making-models-differ-by-changing-their-training-data",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Making models differ by changing their training data",
    "text": "Making models differ by changing their training data\n\n\n \n\nBagging: Train different models on different subsets of the data.\n\nBagging gets different training sets by using sampling with replacement: \\{a,b,c,d,e\\} \\to &lt;a, c, c, d, d, \\ldots &gt;\nRandom forests use lots of different decision trees trained using bagging. They work well.\n\nWe could use bagging with neural nets but its very expensive.\nBoosting: Train a sequence of low capacity models. Weight the training cases differently for each model in the sequence.\n\nBoosting up-weights cases that previous models got wrong.\nAn early use of boosting was with neural nets for MNIST.\nIt focused the computational resources on modeling the tricky cases."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#mixtures-of-experts",
    "href": "notes/dnn/dnn-10/l_10.html#mixtures-of-experts",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Mixtures of Experts",
    "text": "Mixtures of Experts\n\n\nCan we do better that just averaging models in a way that does not depend on the particular training case? – Maybe we can look at the input data for a particular case to help us decide which model to rely on. – This may allow particular models to specialize in a subset of the training cases. – They do not learn on cases for which they are not picked. So they can ignore stuff they are not good at modeling. Hurray for nerds!\nThe key idea is to make each expert focus on predicting the right answer for the cases where it is already doing better than the other experts. – This causes specialization. # A spectrum of models\n\nVery local models – e.g. Nearest neighbors - Very fast to fit - Just store training cases - Local smoothing would obviously improve things.\nFully global models - e. g. A polynomial - May be slow to fit and also unstable. - Each parameter depends on all the data. Small changes to data can cause big changes to the fit."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#multiple-local-models",
    "href": "notes/dnn/dnn-10/l_10.html#multiple-local-models",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Multiple local models",
    "text": "Multiple local models\n\nInstead of using a single global model or lots of very local models, use several models of intermediate complexity.\n\nGood if the dataset contains several different regimes which have different relationships between input and output.\n\ne.g. financial data which depends on the state of the economy.\n\n\nBut how do we partition the dataset into regimes?"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#partitioning-based-on-input-alone-versus-partitioning-based-on-the-input-output-relationship",
    "href": "notes/dnn/dnn-10/l_10.html#partitioning-based-on-input-alone-versus-partitioning-based-on-the-input-output-relationship",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Partitioning based on input alone versus partitioning based on the input-output relationship",
    "text": "Partitioning based on input alone versus partitioning based on the input-output relationship\n\n\nWe need to cluster the training cases into subsets, one for each local model.\n\nThe aim of the clustering is NOT to find clusters of similar input vectors.\nWe want each cluster to have a relationship between input and output that can be well-modeled by one local model."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#a-picture-of-why-averaging-models-during-training-causes-cooperation-not-specialization",
    "href": "notes/dnn/dnn-10/l_10.html#a-picture-of-why-averaging-models-during-training-causes-cooperation-not-specialization",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "A picture of why averaging models during training causes cooperation not specialization",
    "text": "A picture of why averaging models during training causes cooperation not specialization\n\nDo we really want to move the output of model i away from the target value?"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#an-error-function-that-encourages-cooperation",
    "href": "notes/dnn/dnn-10/l_10.html#an-error-function-that-encourages-cooperation",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "An error function that encourages cooperation",
    "text": "An error function that encourages cooperation\nIf we want to encourage cooperation, we compare the average of all the predictors with the target and train to reduce the discrepancy. – This can overfit badly. It makes the model much more powerful than training each predictor separately. \nE=(t - \\lt y_i \\gt_i )^2\n\nwhere:\n\ny_i is the average over all predictors."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#an-error-function-that-encourages-specialization",
    "href": "notes/dnn/dnn-10/l_10.html#an-error-function-that-encourages-specialization",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "An error function that encourages specialization",
    "text": "An error function that encourages specialization\n\nIf we want to encourage specialization we compare each predictor separately with the target.\nWe also use a “manager” to determine the probability of picking each expert.\n\nMost experts end up ignoring most targets\n\n\n\n  E =  \\lt p_i(t- y_i)^2 \\gt_i\n\nwhere:\n\ny_i is the avarage over all predictors.\np_i probability of the manager picking expert i for this case."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#the-mixture-of-experts-architecture-almost",
    "href": "notes/dnn/dnn-10/l_10.html#the-mixture-of-experts-architecture-almost",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "The mixture of experts architecture (almost)",
    "text": "The mixture of experts architecture (almost)\nA simple cost function 1\n1 There is a better cost function based on a mixture model.\n  E = \\sum_i p_i (t- y_i)^2\n\n\n\n\nsimplified mixture of experts architecture"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#the-derivatives-of-the-simple-cost-function",
    "href": "notes/dnn/dnn-10/l_10.html#the-derivatives-of-the-simple-cost-function",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "The derivatives of the simple cost function",
    "text": "The derivatives of the simple cost function\n\nIf we differentiate w.r.t. the outputs of the experts we get a signal for training each expert.\nIf we differentiate w.r.t. the outputs of the gating network we get a signal for training the gating net.\n\nWe want to raise p for all experts that give less than the average squared error of all the experts (weighted by p)\n\n\n\np_j = \\frac{e^{x_j}}{\\sum_i e^x_i}\n\n\nE = \\sum_i p_i (t- y_i)^2\n\\tag{1}\n\n\\frac{\\partial E}{\\partial y_i} = p_i(t-y_i)\n\\tag{2}\n\n\\frac{\\partial E}{\\partial x_i} = p_i\\Bigg((t-y_i)^2-E\\Bigg)\n\\tag{3}"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#a-better-cost-function-for-mixtures-of-experts-jacobs1991adaptive",
    "href": "notes/dnn/dnn-10/l_10.html#a-better-cost-function-for-mixtures-of-experts-jacobs1991adaptive",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "A better cost function for mixtures of experts (Jacobs et al. 1991)",
    "text": "A better cost function for mixtures of experts (Jacobs et al. 1991)\n\nJacobs, Robert A, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. “Adaptive Mixtures of Local Experts.” Neural Computation 3 (1): 79–87. https://doi.org/10.1162/neco.1991.3.1.79.\n\n\n\n\n\nThink of each expert as making a prediction that is a Gaussian distribution around its output (with variance 1).\nThink of the manager as deciding on a scale for each of these Gaussians. The scale is called a “mixing proportion”. e.g {0.4 0.6}\nMaximize the log probability of the target value under this mixture of Gaussians model i.e. the sum of the two scaled Gaussians."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#the-probability-of-the-target-under-a-mixture-of-gaussians",
    "href": "notes/dnn/dnn-10/l_10.html#the-probability-of-the-target-under-a-mixture-of-gaussians",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "The probability of the target under a mixture of Gaussians",
    "text": "The probability of the target under a mixture of Gaussians\n\np(t^c | MoE) = \\sum_i p_i^c {\\color{red}\\frac{1}{\\sqrt{2\\pi}}} e^−\\frac{1}{2} (t^c−y_i^c )^2\n\\tag{4}\nwhere:\n\nlhs - prob. of target value on case c given the mixture.\nMoE — Mixture of Experts\ny_i = output of expert i\nthe constant in red - normalization term for a Gaussian with \\sigma^2=1"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#full-bayesian-learning",
    "href": "notes/dnn/dnn-10/l_10.html#full-bayesian-learning",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Full Bayesian Learning",
    "text": "Full Bayesian Learning\n\nInstead of trying to find the best single setting of the parameters (as in Maximum Likelihood or MAP) compute the full posterior distribution over all possible parameter settings.\n\nThis is extremely computationally intensive for all but the simplest models (its feasible for a biased coin).\n\nTo make predictions, let each different setting of the parameters make its own prediction and then combine all these predictions by weighting each of them by the posterior probability of that setting of the parameters.\n\nThis is also very computationally intensive.\n\nThe full Bayesian approach allows us to use complicated models even when we do not have much data."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#overfitting-a-frequentist-illusion",
    "href": "notes/dnn/dnn-10/l_10.html#overfitting-a-frequentist-illusion",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Overfitting: A frequentist illusion?",
    "text": "Overfitting: A frequentist illusion?\n\nIf you do not have much data, you should use a simple model, because a complex one will overfit.\n\nThis is true.\nBut only if you assume that fitting a model means choosing a single best setting of the parameters.\n\nIf you use the full posterior distribution over parameter settings, overfitting disappears.\n\nWhen there is very little data, you get very vague predictions because many different parameters settings have significant posterior probability"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#a-classic-example-of-overfitting",
    "href": "notes/dnn/dnn-10/l_10.html#a-classic-example-of-overfitting",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "A classic example of overfitting",
    "text": "A classic example of overfitting\n\n\n\n\n\noverfitting\n\n\n\n\n\nnot-overfitting\n\n\n\nWhich model do you believe?\n\nThe complicated model fits the data better.\nBut it is not economical and it makes silly predictions.\n\nBut what if we start with a reasonable prior over all fifth-order polynomials and use the full posterior distribution.\n\nNow we get vague and sensible predictions.\n\nThere is no reason why the amount of data should influence our prior beliefs about the complexity of the model."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#approximating-full-bayesian-learning-in-a-neural-net",
    "href": "notes/dnn/dnn-10/l_10.html#approximating-full-bayesian-learning-in-a-neural-net",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Approximating full Bayesian learning in a neural net",
    "text": "Approximating full Bayesian learning in a neural net\n\nIf the neural net only has a few parameters we could put a grid over the parameter space and evaluate p( W | D ) at each grid-point.\n\nThis is expensive, but it does not involve any gradient descent and there are no local optimum issues.\n\nAfter evaluating each grid point we use all of them to make predictions on test data\n\nThis is also expensive, but it works much better than ML learning when the posterior is vague or multimodal (this happens when data is scarce).\n\n\n\np(t_{test} \\mid \\text{input}_{test}) = \\sum_{g \\in grid} p(W_g \\mid D) p(t_{test} \\mid \\text{input}_{test}, W_g )\n\\tag{5}"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#an-example-of-full-bayesian-learning",
    "href": "notes/dnn/dnn-10/l_10.html#an-example-of-full-bayesian-learning",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "An example of full Bayesian learning",
    "text": "An example of full Bayesian learning\n\n\n\n\nAllow each of the 6 weights or biases to have the 9 possible values -2,\\ -1.5,\\ -1,\\ -0.5,\\ 0,\\ 0.5,\\ 1,\\ 1.5,\\ 2\n\nThere are 9^6 grid-points in parameter space\n\nFor each grid-point compute the probability of the observed outputs of all the training cases.\nMultiply the prior for each grid-point by the likelihood term and renormalize to get the posterior probability for each grid-point.\nMake predictions by using the posterior probabilities to average the predictions made by the different grid-points."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#what-can-we-do-if-there-are-too-many-parameters-for-a-grid",
    "href": "notes/dnn/dnn-10/l_10.html#what-can-we-do-if-there-are-too-many-parameters-for-a-grid",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "What can we do if there are too many parameters for a grid?",
    "text": "What can we do if there are too many parameters for a grid?\n\nThe number of grid points is exponential in the number of parameters.\n\nSo we cannot deal with more than a few parameters using a grid.\n\nIf there is enough data to make most parameter vectors very unlikely, only a tiny fraction of the grid points make a significant contribution to the predictions.\n\nMaybe we can just evaluate this tiny fraction\n\nIdea: 💡 It might be good enough to just sample weight vectors according to their posterior probabilities.\n\n\np(y_{test} \\mid \\text{input}_{test},D) = \\sum_{i} {\\color{green}{ p(W_i \\mid D)}} p(y_{test} \\mid \\text{input}_{test}, W_i )\n\nwhere:\n\nthe green term - Sample weight vectors with this probability"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#sampling-weight-vectors",
    "href": "notes/dnn/dnn-10/l_10.html#sampling-weight-vectors",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Sampling weight vectors",
    "text": "Sampling weight vectors\n\nIn standard backpropagation we keep moving the weights in the direction that decreases the cost.\n\ni.e. the direction that increases the log likelihood plus the log prior, summed over all training cases.\nEventually, the weights settle into a local minimum or get stuck on a plateau or just move so slowly that we run out of patience."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#one-method-for-sampling-weight-vectors",
    "href": "notes/dnn/dnn-10/l_10.html#one-method-for-sampling-weight-vectors",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "One method for sampling weight vectors",
    "text": "One method for sampling weight vectors\n\nSuppose we add some Gaussian noise to the weight vector after each update.\n\nSo the weight vector never settles down.\nIt keeps wandering around, but it tends to prefer low cost regions of the weight space.\nCan we say anything about how often it will visit each possible setting of the weights?"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#the-wonderful-property-of-markov-chain-monte-carlo",
    "href": "notes/dnn/dnn-10/l_10.html#the-wonderful-property-of-markov-chain-monte-carlo",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "The wonderful property of Markov Chain Monte Carlo",
    "text": "The wonderful property of Markov Chain Monte Carlo\n\nAmazing fact: If we use just the right amount of noise, and if we let the weight vector wander around for long enough before we take a sample, we will get an unbiased sample from the true posterior over weight vectors.\n\nThis is called a “Markov Chain Monte Carlo” method.\nMCMC makes it feasible to use full Bayesian learning with thousands of parameters.\n\nThere are related MCMC methods that are more complicated but more efficient:\n\nWe don’t need to let the weights wander around for so long before we get samples from the posterior."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#full-bayesian-learning-with-mini-batches",
    "href": "notes/dnn/dnn-10/l_10.html#full-bayesian-learning-with-mini-batches",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Full Bayesian learning with mini-batches",
    "text": "Full Bayesian learning with mini-batches\n\nIf we compute the gradient of the cost function on a random mini-batch we will get an unbiased estimate with sampling noise.\n\nMaybe we can use the sampling noise to provide the noise that an MCMC method needs!\n\nIn (Ahn, Korattikara, and Welling 2012) 2 the authors showed how to do this fairly efficiently.\n\nSo full Bayesian learning is now possible with lots of parameters.\n\n\n\nAhn, Sungjin, Anoop Korattikara, and Max Welling. 2012. “Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring.” In Proceedings of the 29th International Coference on International Conference on Machine Learning, 1771–78. ICML’12. Madison, WI, USA: Omnipress. https://doi.org/10.5555/3042573.3042799.\n2 pdf"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#two-ways-to-average-models",
    "href": "notes/dnn/dnn-10/l_10.html#two-ways-to-average-models",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Two ways to average models",
    "text": "Two ways to average models\n\nMIXTURE: We can combine models by averaging their output probabilities:\n\nModel A: .3 .2 .5 Model B: .1 .8 .1 Combined .2 .5 .3\n\nPRODUCT: We can combine models by taking the geometric means of their output probabilities:\n\nModel A: .3 .2 .5 Model B: .1 .8 .1 Combined .03 .16 .05 /sum"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#dropout-an-efficient-way-to-average-many-large-neural-nets",
    "href": "notes/dnn/dnn-10/l_10.html#dropout-an-efficient-way-to-average-many-large-neural-nets",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Dropout: An efficient way to average many large neural nets",
    "text": "Dropout: An efficient way to average many large neural nets\npreprint (Hinton et al. 2012)\n\nHinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.” https://doi.org/10.48550/ARXIV.1207.0580.\nTODO add picture\n\nConsider a neural net with one hidden layer.\nEach time we present a training example, we randomly omit each hidden unit with probability 0.5.\nSo we are randomly sampling from 2^H different architectures. – All architectures share weights"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#dropout-as-a-form-of-model-averaging",
    "href": "notes/dnn/dnn-10/l_10.html#dropout-as-a-form-of-model-averaging",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Dropout as a form of model averaging",
    "text": "Dropout as a form of model averaging\n\nWe sample from 2^H models. So only a few of the models ever get trained, and they only get one training example.\n\nThis is as extreme as bagging can get.\n\nThe sharing of the weights means that every model is very strongly regularized.\n\nIt’s a much better regularizer than L2 or L1 penalties that pull the weights towards zero."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#but-what-do-we-do-at-test-time",
    "href": "notes/dnn/dnn-10/l_10.html#but-what-do-we-do-at-test-time",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "But what do we do at test time?",
    "text": "But what do we do at test time?\n\nWe could sample many different architectures and take the geometric mean of their output distributions.\nIt better to use all of the hidden units, but to halve their outgoing weights.\n\nThis exactly computes the geometric mean of the predictions of all 2^H models."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#what-if-we-have-more-hidden-layers",
    "href": "notes/dnn/dnn-10/l_10.html#what-if-we-have-more-hidden-layers",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "What if we have more hidden layers?",
    "text": "What if we have more hidden layers?\n\nUse dropout of 0.5 in every layer.\nAt test time, use the “mean net” that has all the outgoing weights halved.\n\nThis is not exactly the same as averaging all the separate dropped out models, but it’s a pretty good approximation, and its fast.\nAlternatively, run the stochastic model several times on the same input.\n\nThis gives us an idea of the uncertainty in the answer."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#what-about-the-input-layer",
    "href": "notes/dnn/dnn-10/l_10.html#what-about-the-input-layer",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "What about the input layer?",
    "text": "What about the input layer?\n\nIt helps to use dropout there too, but with a higher probability of keeping an input unit.\n\nThis trick is already used by the denoising autoencoders developed by Pascal Vincent, Hugo Larochelle and Yoshua Bengio."
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#how-well-does-dropout-work",
    "href": "notes/dnn/dnn-10/l_10.html#how-well-does-dropout-work",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "How well does dropout work?",
    "text": "How well does dropout work?\n\nThe record breaking object recognition net developed by Alex Krizhevsky (see lecture 5) uses dropout and it helps a lot.\nIf your deep neural net is significantly overfitting, dropout will usually reduce the number of errors by a lot.\n\nAny net that uses “early stopping” can do better by using dropout (at the cost of taking quite a lot longer to train).\n\nIf your deep neural net is not overfitting you should be using a bigger one!"
  },
  {
    "objectID": "notes/dnn/dnn-10/l_10.html#another-way-to-think-about-dropout",
    "href": "notes/dnn/dnn-10/l_10.html#another-way-to-think-about-dropout",
    "title": "Deep Neural Networks - Notes for Lesson 10",
    "section": "Another way to think about dropout",
    "text": "Another way to think about dropout\n\nIf a hidden unit knows which other hidden units are present, it can co-adapt to them on the training data.\n\nBut complex co-adaptations are likely to go wrong on new test data.\nBig, complex conspiracies are not robust.\n\nIf a hidden unit has to work well with combinatorially many sets of co-workers, it is more likely to do something that is individually useful.\n\nBut it will also tend to do something that is marginally useful given what its co-workers achieve.\n\n\n\n\n\nsimplified mixture of experts architecture\noverfitting\nnot-overfitting"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html",
    "href": "notes/dnn/dnn-14/l_14.html",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#training-a-deep-network-by-stacking-rbms",
    "href": "notes/dnn/dnn-14/l_14.html#training-a-deep-network-by-stacking-rbms",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Training a deep network by stacking RBMs",
    "text": "Training a deep network by stacking RBMs\n\nFirst train a layer of features that receive input directly from the pixels.\nThen treat the activations of the trained features as if they were pixels and learn features of features in a second hidden layer.\nThen do it again.\nIt can be proved that each time we add another layer of features we improve a variational lower bound on the log probability of generating the training data.\n\nThe proof is complicated and only applies to unreal cases.\nIt is based on a neat equivalence between an RBM and an infinitely deep belief net (see lecture 14b)."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#combining-two-rbms-to-make-a-dbn",
    "href": "notes/dnn/dnn-14/l_14.html#combining-two-rbms-to-make-a-dbn",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Combining two RBMs to make a DBN",
    "text": "Combining two RBMs to make a DBN\n\n\n\n\n\nRDBM Combo\n\n\n\nThe generative model after learning 3 layers\n\n\n\n\nRDBM Combo\n\n\nTo generate data:\n\nGet an equilibrium sample from the toplevel RBM by performing alternating Gibbs sampling for a long time.\nPerform a top-down pass to get states for all the other layers.\n\nThe lower level bottom-up connections are not part of the generative model. They are just used for inference."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#the-generative-model-after-learning-3-layers",
    "href": "notes/dnn/dnn-14/l_14.html#the-generative-model-after-learning-3-layers",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "The generative model after learning 3 layers",
    "text": "The generative model after learning 3 layers"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#an-aside-averaging-factorial-distributions",
    "href": "notes/dnn/dnn-14/l_14.html#an-aside-averaging-factorial-distributions",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "An aside: Averaging factorial distributions",
    "text": "An aside: Averaging factorial distributions\n\nIf you average some factorial distributions, you do NOT get a factorial distribution.\n\nIn an RBM, the posterior over 4 hidden units is factorial for each visible vector.\n\nPosterior for v1: 0.9, 0.9, 0.1, 0.1\nPosterior for v2: 0.1, 0.1, 0.9, 0.9\nAggregated = 0.5, 0.5, 0.5, 0.5\nConsider the binary vector 1,1,0,0.\n\nin the posterior for v1, p(1,1,0,0) = 0.9^4 = 0.43\nin the posterior for v2, p(1,1,0,0) = 0.1^4 = .0001 – in the aggregated posterior, p(1,1,0,0) = 0.215.\n\nIf the aggregated posterior was factorial it would have p = 0.5^4"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-does-greedy-learning-work",
    "href": "notes/dnn/dnn-14/l_14.html#why-does-greedy-learning-work",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why does greedy learning work?",
    "text": "Why does greedy learning work?\nThe weights, W, in the bottom level RBM define many different distributions: p(v|h); p(h|v); p(v,h); p(h); p(v).\nWe can express the RBM model as\n\np(v)= \\sum_h p(h) p(v \\mid h)\n\nIf we leave p(v|h) alone and improve p(h), we will improve p(v).\nTo improve p(h), we need it to be a better model than p(h;W) of the aggregated posterior distribution over hidden vectors produced by applying W transpose to the data."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#fine-tuning-with-a-contrastive-version-of-the-wake-sleep-algorithm",
    "href": "notes/dnn/dnn-14/l_14.html#fine-tuning-with-a-contrastive-version-of-the-wake-sleep-algorithm",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Fine-tuning with a contrastive version of the wake-sleep algorithm",
    "text": "Fine-tuning with a contrastive version of the wake-sleep algorithm\nAfter learning many layers of features, we can fine-tune the features to improve generation.\n\nDo a stochastic bottom-up pass\n\nThen adjust the top-down weights of lower layers to be good at reconstructing the feature activities in the layer below.\n\nDo a few iterations of sampling in the top level RBM\n\nThen adjust the weights in the top-level RBM using CD.\n\nDo a stochastic top-down pass\n\nThen Adjust the bottom-up weights to be good at reconstructing the feature activities in the layer above."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#the-dbn-used-for-modeling-the-joint-distribution-of-mnist-digits-and-their-labels",
    "href": "notes/dnn/dnn-14/l_14.html#the-dbn-used-for-modeling-the-joint-distribution-of-mnist-digits-and-their-labels",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "The DBN used for modeling the joint distribution of MNIST digits and their labels",
    "text": "The DBN used for modeling the joint distribution of MNIST digits and their labels\n\n\n\n\nThe first two hidden layers are learned without using labels.\nThe top layer is learned as an RBM for modeling the labels concatenated with the features in the second hidden layer.\nThe weights are then fine-tuned to be a better generative model using contrastive wake-sleep."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#fine-tuning-for-discrimination",
    "href": "notes/dnn/dnn-14/l_14.html#fine-tuning-for-discrimination",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Fine-tuning for discrimination",
    "text": "Fine-tuning for discrimination\n\nFirst learn one layer at a time by stacking RBMs.\nTreat this as “pre-training” that finds a good initial set of weights which can then be fine-tuned by a local search procedure.\n\nContrastive wake-sleep is a way of fine-tuning the model to be better at generation.\n\nBackpropagation can be used to fine-tune the model to be better at discrimination.\n\nThis overcomes many of the limitations of standard backpropagation.\nIt makes it easier to learn deep nets.\nIt makes the nets generalize better."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-backpropagation-works-better-with-greedy-pre-training-the-optimization-view",
    "href": "notes/dnn/dnn-14/l_14.html#why-backpropagation-works-better-with-greedy-pre-training-the-optimization-view",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why backpropagation works better with greedy pre-training: The optimization view",
    "text": "Why backpropagation works better with greedy pre-training: The optimization view\n\nGreedily learning one layer at a time scales well to really big networks, especially if we have locality in each layer.\nWe do not start backpropagation until we already have sensible feature detectors that should already be very helpful for the discrimination task.\n\nSo the initial gradients are sensible and backpropagation only needs to perform a local search from a sensible starting point"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-backpropagation-works-better-with-greedy-pre-training-the-overfitting-view",
    "href": "notes/dnn/dnn-14/l_14.html#why-backpropagation-works-better-with-greedy-pre-training-the-overfitting-view",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why backpropagation works better with greedy pre-training: The overfitting view",
    "text": "Why backpropagation works better with greedy pre-training: The overfitting view\n\nMost of the information in the final weights comes from modeling the distribution of input vectors.\n\nThe input vectors generally contain a lot more information than the labels.\nThe precious information in the labels is only used for the fine tuning.\n\nThe fine-tuning only modifies the features slightly to get the category boundaries right. It does not need to discover new features.\nThis type of back-propagation works well even if most of the training data is unlabeled.\n\nThe unlabeled data is still very useful for discovering good features.\n\nAn objection: Surely, many of the features will be useless for any particular discriminative task (consider shape & pose).\n\nBut the ones that are useful will be much more useful than the raw inputs."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#results-on-the-permutation-invariant-mnist-task",
    "href": "notes/dnn/dnn-14/l_14.html#results-on-the-permutation-invariant-mnist-task",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Results on the permutation-invariant MNIST task",
    "text": "Results on the permutation-invariant MNIST task\n\n\n\n\n\n\n\npaper\nError rate\n\n\n\n\nBackprop net with one or two hidden layers (Platt; Hinton)\n1.6%\n\n\nBackprop with L2 constraints on incoming weights\n1/4%\n\n\nSupport Vector Machines (Decoste & Schoelkopf, 2002)\n1.4%\n\n\nGenerative model of joint density of images and labels (+ generative fine-tuning)\n1.25%\n\n\nGenerative model of unlabelled digits followed by gentle backpropagation (Hinton & Salakhutdinov, 2006)\n1.15%-&gt;1.0%"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#unsupervised-pre-training-also-helps-for-models-that-have-more-data-and-better-priors",
    "href": "notes/dnn/dnn-14/l_14.html#unsupervised-pre-training-also-helps-for-models-that-have-more-data-and-better-priors",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Unsupervised “pre-training” also helps for models that have more data and better priors",
    "text": "Unsupervised “pre-training” also helps for models that have more data and better priors\n\nIn Ranzato et al. (2006) the authors used an additional 600,000 distorted digits.\nThey also used convolutional multilayer neural networks.\n\n\nRanzato, Marcaurelio, Christopher Poultney, Sumit Chopra, and Yann Cun. 2006. “Efficient Learning of Sparse Representations with an Energy-Based Model.” In Advances in Neural Information Processing Systems, edited by B. Schölkopf, J. Platt, and T. Hoffman. Vol. 19. MIT Press. https://proceedings.neurips.cc/paper_files/paper/2006/file/87f4d79e36d68c3031ccf6c55e9bbd39-Paper.pdf.\nBack-propagation alone: 0.49% Unsupervised layer-by-layer pre-training followed by back-prop: 0.39% (record at the time)"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#phone-recognition-on-the-timit-benchmark-mohamed-dahl-hinton-2009-2012",
    "href": "notes/dnn/dnn-14/l_14.html#phone-recognition-on-the-timit-benchmark-mohamed-dahl-hinton-2009-2012",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Phone recognition on the TIMIT benchmark (Mohamed, Dahl, & Hinton, 2009 & 2012)",
    "text": "Phone recognition on the TIMIT benchmark (Mohamed, Dahl, & Hinton, 2009 & 2012)\n\n\n\n\nAfter standard post-processing using a bi-phone model, a deep net with 8 layers gets 20.7% error rate.\nThe best previous speaker independent result on TIMIT was 24.4% and this required averaging several models.\nLi Deng (at MSR) realized that this result could change the way speech recognition was done. It has!"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#learning-dynamics-of-deep-nets-the-next-4-slides-describe-work-by-yoshua-bengios-group",
    "href": "notes/dnn/dnn-14/l_14.html#learning-dynamics-of-deep-nets-the-next-4-slides-describe-work-by-yoshua-bengios-group",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Learning Dynamics of Deep Nets the next 4 slides describe work by Yoshua Bengio’s group",
    "text": "Learning Dynamics of Deep Nets the next 4 slides describe work by Yoshua Bengio’s group"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#effect-of-unsupervised-pre-training",
    "href": "notes/dnn/dnn-14/l_14.html#effect-of-unsupervised-pre-training",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Effect of Unsupervised Pre-training",
    "text": "Effect of Unsupervised Pre-training"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#effect-of-depth",
    "href": "notes/dnn/dnn-14/l_14.html#effect-of-depth",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Effect of Depth",
    "text": "Effect of Depth"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#trajectories-of-the-learning-in-function-space",
    "href": "notes/dnn/dnn-14/l_14.html#trajectories-of-the-learning-in-function-space",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Trajectories of the learning in function space",
    "text": "Trajectories of the learning in function space\n\n\n\nEach point is a model in function space\n\nColor = epoch\nTop: trajectories without pre-training. Each trajectory converges to a different local min.\nBottom: Trajectories with pre-training.\nNo overlap!"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-unsupervised-pre-training-makes-sense",
    "href": "notes/dnn/dnn-14/l_14.html#why-unsupervised-pre-training-makes-sense",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why unsupervised pre-training makes sense",
    "text": "Why unsupervised pre-training makes sense\n\n\n\n\n\nchain graph\n\n\n\nIf image-label pairs were generated this way, it would make sense to try to go straight from images to labels. For example, do the pixels have even parity?\n\n\n\n\n\n\nv graph\n\n\nIf image-label pairs are generated this way, it makes sense to first learn to recover the stuff that caused the image by inverting the high bandwidth pathway."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#modeling-real-valued-data",
    "href": "notes/dnn/dnn-14/l_14.html#modeling-real-valued-data",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Modeling real-valued data",
    "text": "Modeling real-valued data\n\nFor images of digits, intermediate intensities can be represented as if they were probabilities by using “mean-field” logistic units.\n\nWe treat intermediate values as the probability that the pixel is inked.\n\nThis will not work for real images.\n\nIn a real image, the intensity of a pixel is almost always, almost exactly the average of the neighboring pixels.\nMean-field logistic units cannot represent precise intermediate values."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#a-standard-type-of-real-valued-visible-unit",
    "href": "notes/dnn/dnn-14/l_14.html#a-standard-type-of-real-valued-visible-unit",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "A standard type of real-valued visible unit",
    "text": "A standard type of real-valued visible unit\n\n\n\n\nModel pixels as Gaussian variables. Alternating Gibbs sampling is still easy, though learning needs to be much slower.\n\n\nE(v,h) = \\sum_{i\\in vis} \\frac{(v_i-b_i)^2}{2\\sigma^2_i} - \\sum_j b_jh_j- \\sum_{i,j} \\frac{v_i}{\\sigma_i} h_i w_{ij}\n where:\n\nthe first term is parabolic containment function\nthe last term is energy-gradient produced by the total input to a visible unit"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#gaussian-binary-rbms",
    "href": "notes/dnn/dnn-14/l_14.html#gaussian-binary-rbms",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Gaussian-Binary RBM’s",
    "text": "Gaussian-Binary RBM’s\n\n\n\n\nLots of people have failed to get these to work properly. Its extremely hard to learn tight variances for the visible units.\n\nIt took a long time for us to figure out why it is so hard to learn the visible variances.\n\nWhen sigma is small, we need many more hidden units than visible units.\nThis allows small weights to produce big top-down effects.\nWhen sigma is much less than 1, the bottom-up effects are too big and the top-down effects are too small."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#stepped-sigmoid-units-a-neat-way-to-implement-integer-values",
    "href": "notes/dnn/dnn-14/l_14.html#stepped-sigmoid-units-a-neat-way-to-implement-integer-values",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Stepped sigmoid units: A neat way to implement integer values",
    "text": "Stepped sigmoid units: A neat way to implement integer values\n\n\n\n\n\nstepped sigmoid\n\n\n\nMake many copies of a stochastic binary unit.\nAll copies have the same weights and the same adaptive bias, b, but they have different fixed offsets to the bias:\n\nb −0.5,\\ b −1.5,\\ b −2.5, b −3.5,\\ \\ldots\n ## Fast approximations\n\n\n\n\n\n\nlogarithmic approximation\n\n\n\n\n\nlinear approximation\n\n\n\n\\langle y \\rangle = \\sum_{n=1}^{\\infty} \\sigma (x + 0.5− n)  ≈ log(1+ e^x) ≈ max(0, x + noise)\n\n\nContrastive divergence learning works well for the sum of stochastic logistic units with offset biases.\nThe noise variance is \\sigma(y)\nIt also works for RELUs, which are faster to compute than the sum of many logistic units with different biases."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#a-nice-property-of-rectified-linear-units",
    "href": "notes/dnn/dnn-14/l_14.html#a-nice-property-of-rectified-linear-units",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "A nice property of rectified linear units",
    "text": "A nice property of rectified linear units\n\nIf a RELU has a bias of zero, it exhibits scale equivariance:\nThis is a very nice property to have for images.\nIt is like the equivariance to translation exhibited by convolutional nets.\n\n\nR(shift(x)) = shift(R(x))\n # Lecture 14e : RBMs are Infinite Sigmoid Belief Nets\nThis is advanced material"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#another-view-of-why-layer-by-layer-learning-works-hinton2006fast",
    "href": "notes/dnn/dnn-14/l_14.html#another-view-of-why-layer-by-layer-learning-works-hinton2006fast",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Another view of why layer-by-layer learning works — Hinton, Osindero, and Teh (2006)",
    "text": "Another view of why layer-by-layer learning works — Hinton, Osindero, and Teh (2006)\n\nThere is an unexpected equivalence between RBM’s and directed networks with many layers that all share the same weight matrix.\n\nThis equivalence also gives insight into why contrastive divergence learning works.\n\nAn RBM is actually just an infinitely deep sigmoid belief net with a lot of weight sharing.\n\nThe Markov chain we run when we want to sample from the equilibrium distribution of an RBM can be viewed as a sigmoid belief net."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#an-infinite-sigmoid-belief-net-that-is-equivalent-to-an-rbm",
    "href": "notes/dnn/dnn-14/l_14.html#an-infinite-sigmoid-belief-net-that-is-equivalent-to-an-rbm",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "An infinite sigmoid belief net that is equivalent to an RBM",
    "text": "An infinite sigmoid belief net that is equivalent to an RBM\n\n\n\n\nThe distribution generated by this infinite directed net with replicated weights is the equilibrium distribution for a compatible pair of conditional distributions: p(v \\mid h) and p(h \\mid v) that are both defined by W\n\nA top-down pass of the directed net is exactly equivalent to letting a Restricted Boltzmann Machine settle to equilibrium.\nSo this infinite directed net defines the same distribution as an RBM"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#inference-in-an-infinite-sigmoid-belief-net",
    "href": "notes/dnn/dnn-14/l_14.html#inference-in-an-infinite-sigmoid-belief-net",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Inference in an infinite sigmoid belief net",
    "text": "Inference in an infinite sigmoid belief net\n\n\n\n\nThe variables in h_0 are conditionally independent given v_0.\n\nInference is trivial. Just multiply v_0 by W^T\nThe model above h0 implements a complementary prior.\nMultiplying v0 by gives the product of the likelihood term and the prior term.\nThe complementary prior cancels the explaining away.\n\nInference in the directed net is exactly equivalent to letting an RBM settle to equilibrium starting at the data.\n\n\n\n\n\nThe learning rule for a sigmoid belief net is:\n\n\n  \\Delta w_{ij} ∝ s_j(s_i − p_i)\n\nwhere s_j^1 is an unbiased sample from p^i_0\n\nWith replicated weights this rule becomes:\n\n\n  s_j^0 s_i^0 - s_j^\\infty s_j^\\infty"
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#learning-a-deep-directed-network",
    "href": "notes/dnn/dnn-14/l_14.html#learning-a-deep-directed-network",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Learning a deep directed network",
    "text": "Learning a deep directed network\n\nFirst learn with all the weights tied. This is exactly equivalent to learning an RBM.\n\nThink of the symmetric connections as a shorthand notation for an infinite directed net with tied weights.\n\nWe ought to use maximum likelihood learning, but we use CD1 as a shortcut.\nThen freeze the first layer of weights in both directions and learn the remaining weights (still tied together).\n\nThis is equivalent to learning another RBM, using the aggregated posterior distribution of h0 as the data."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#what-happens-when-the-weights-in-higher-layers-become-different-from-the-weights-in-the-first-layer",
    "href": "notes/dnn/dnn-14/l_14.html#what-happens-when-the-weights-in-higher-layers-become-different-from-the-weights-in-the-first-layer",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "What happens when the weights in higher layers become different from the weights in the first layer?",
    "text": "What happens when the weights in higher layers become different from the weights in the first layer?\n\nThe higher layers no longer implement a complementary prior.\n\nSo performing inference using the frozen weights in the first layer is no longer correct.\nBut its still pretty good.\nUsing this incorrect inference procedure gives a variational lower bound on the log probability of the data.\n\nThe higher layers learn a prior that is closer to the aggregated posterior distribution of the first hidden layer.\n\nThis improves the network’s model of the data.\nIn (Hinton, Osindero, and Teh 2006) the authors prove that this improvement is always bigger than the loss in the variational bound caused by using less accurate inference.\n\n\n\nHinton, Geoffrey E, Simon Osindero, and Yee-Whye Teh. 2006. “A Fast Learning Algorithm for Deep Belief Nets.” Neural Computation 18 (7): 1527–54."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#what-is-really-happening-in-contrastive-divergence-learning",
    "href": "notes/dnn/dnn-14/l_14.html#what-is-really-happening-in-contrastive-divergence-learning",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "What is really happening in contrastive divergence learning?",
    "text": "What is really happening in contrastive divergence learning?\n\nContrastive divergence learning in this RBM is equivalent to ignoring the small derivatives contributed by the tied weights in higher layers."
  },
  {
    "objectID": "notes/dnn/dnn-14/l_14.html#why-is-it-ok-to-ignore-the-derivatives-in-higher-layers",
    "href": "notes/dnn/dnn-14/l_14.html#why-is-it-ok-to-ignore-the-derivatives-in-higher-layers",
    "title": "Deep Neural Networks - Notes for Lesson 14",
    "section": "Why is it OK to ignore the derivatives in higher layers?",
    "text": "Why is it OK to ignore the derivatives in higher layers?\n\nWhen the weights are small, the Markov chain mixes fast.\n\nSo the higher layers will be close to the equilibrium distribution (i.e they will have “forgotten” the data vector).\nAt equilibrium the derivatives must average to zero, because the current weights are a perfect model of the equilibrium distribution!\n\nAs the weights grow we may need to run more iterations of CD.\n\nThis allows CD to continue to be a good approximation to maximum likelihood.\nBut for learning layers of features, it does not need to be a good approximation to maximum likelihood\n\n\n\n\n\nRDBM Combo\nRDBM Combo\nchain graph\nv graph\nstepped sigmoid\nlogarithmic approximation\nlinear approximation"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oren Bochman’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nA/B testing cost and risks?\n\n\nA/B testing cost and risks and some recommendation.\n\n\n\nOren Bochman\n\n\nJul 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nAutogluon is a auto-ml framework, here are three cheetsheet for accellerating data science workloads\n\n\n\nOren Bochman\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBash\n\n\nThis is a quick reference cheat sheet to getting started with linux bash shell scripting.\n\n\n\nOren Bochman\n\n\nSep 10, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nPolitical Scenario Prediction Using Game theory\n\n\n\n\n\n\nOren Bochman\n\n\nJan 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nOren Bochman\n\n\nJan 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Migration Notes\n\n\nsome migration notes from Blooger to Jekyl to Quarto blog.\n\n\n\nOren Bochman\n\n\nJan 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto 💖 Bootstrap 😁\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Books\n\n\nIn this updated post I included some R books you might want to look at if you are getting started with R for data science.\n\n\n\nOren Bochman\n\n\nAug 26, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nRegEX\n\n\nA quick reference for regular expressions (regex), including symbols, ranges, grouping, assertions and some sample patterns to get you started.\n\n\n\nOren Bochman\n\n\nNov 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With Python\n\n\n\n\n\n\nOren Bochman\n\n\nNov 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With R\n\n\n\n\n\n\nOren Bochman\n\n\nNov 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\n\n\n\n\nOren Bochman\n\n\nNov 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime management Tips\n\n\nEffective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a…\n\n\n\nOren Bochman\n\n\nAug 11, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nOren Bochman\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWikisym 2012\n\n\n\n\n\n\nOren Bochman\n\n\nJul 26, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html",
    "href": "posts/2011/2011-08-26-R-books/index.html",
    "title": "R Books",
    "section": "",
    "text": "In this updated post I included some R books you might want to look at if you are getting started with R for data science\nFellow data scientists, do not be overwhelmed by vast multiplicities of numbers, for they are but symbols of the natural order. Embrace their uncertainty and seek to understand the patterns within it. I offer you some of the first R books I came across. I got started with R in 2011, and I decided to update it to focus on stats and ml books I’ve come across with an attempt to list my favorites with their lecture notes and video lectures where available."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#software-for-data-analysis-programming-with-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#software-for-data-analysis-programming-with-r",
    "title": "R Books",
    "section": "Software for data analysis: programming with R",
    "text": "Software for data analysis: programming with R\n\n\n\n\n\nSoftware for data analysis\n\n\n\nChambers, John M. 2008. Software for Data Analysis Programming with r. New York; London: Springer. http://www.amazon.de/Software-Data-Analysis-Programming-Statistics/dp/0387759352.\nIn (Chambers 2008) the author presents the essential guidebook for those who wish to learn how to use the R programming language for data analysis. Chambers is a renowned statistician, and he shares his expertise in the field of data analysis through this book. The book covers a wide range of topics related to data analysis, including data structures, object-oriented programming, graphics, and statistical modeling. It also offers a practical approach to understanding R programming, with an emphasis on building applications that can handle large datasets. Overall, this is a valuable resource for those who want to learn about R programming for data analysis. It is a comprehensive guide that covers all the essential aspects of data analysis and provides hands-on experience with R programming. The book is written in a clear and concise manner, making it easy to follow even for beginners."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#principles-of-statistical-data-handling",
    "href": "posts/2011/2011-08-26-R-books/index.html#principles-of-statistical-data-handling",
    "title": "R Books",
    "section": "Principles of Statistical Data Handling",
    "text": "Principles of Statistical Data Handling\n\n\n\n\n\nPrinciples of Statistical Data Handling\n\n\n\nDavidson, Fred. 1996. Principles of Statistical Data Handling. https://doi.org/10.4135/9781483348902.\nIn (Davidson 1996) the author offers a guide to the foundations of this field, including exploratory data analysis, hypothesis testing, and model building. Through careful attention and disciplined study, one can cultivate a deep understanding of the methods and techniques that underlie statistical data handling, and by following, we shall approach data with a rational and objective mindset, illuminating with our analytical skills the meaningful insights and so make more informed decisions.\nA Stoic would say “Remember, the data is not what you see, but what you make of it. So, approach it with a clear mind, free from bias and preconceptions, and seek the truth that lies hidden within.” By applying oneself to these principles with diligence and perseverance, we may yet unlock the full potential of statistical data handling, and make a valuable contribution to the world of data science."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#probability-statistics-for-engineers-and-scientists",
    "href": "posts/2011/2011-08-26-R-books/index.html#probability-statistics-for-engineers-and-scientists",
    "title": "R Books",
    "section": "Probability & Statistics for Engineers and Scientists",
    "text": "Probability & Statistics for Engineers and Scientists\n\n\n\n\n\nProbability & Statistics for Engineers and Scientists\n\n\n\nWalpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying Ye. 2007. Probability & Statistics for Engineers and Scientists. 8th ed. Upper Saddle River: Pearson Education.\nIn (Walpole et al. 2007), the autor presents a path to comprehension of probability and statistics is laid out before you.\nIt is a journey of discovery that will require patience, diligence, and a willingness to learn. The author presents the tools and techniques needed to analyze data and draw meaningful conclusions. By using R, one can unlock the secrets hidden in the data.\nFear not mistakes, for they are but stepping stones towards deeper understanding, only take care to learn from them, and use the knowledge gained to improve your understanding daily.\nWith each chapter, you will gain a greater understanding of the complex and interconnected world of probability and statistics.\nEmbrace the journey, and may the numbers guide you towards enlightenment."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#introduction-to-probability-and-statistics-using-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#introduction-to-probability-and-statistics-using-r",
    "title": "R Books",
    "section": "Introduction to Probability and Statistics Using R",
    "text": "Introduction to Probability and Statistics Using R\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\n\n\n\nKerns, G. Jay. 2018. Introduction to Probability and Statistics Using r.\nIn (Kerns 2018), recommended by my old friend Adam Hyland, the author covers the basic concepts of probability and statistics using the R programming language.\nIt is a useful resource for data scientists who wish to gain a deeper understanding of probability and statistics and how to apply them.\nStarting with basic probability, distributions, hypothesis testing, regression analysis, it then proceeds to more advanced topics such as Bayesian statistics, machine learning, and time series analysis.\nEach chapter presents clear explanations, examples, and R code to help the reader grasp the theoretical concepts and apply them in practice. By including a wide range of real-world examples and datasets, it helps the readers conect the concepts and techniques with thier application to real data.\nA complimentry copy is available at this link"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#statistical-computing-with-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#statistical-computing-with-r",
    "title": "R Books",
    "section": "Statistical Computing with R",
    "text": "Statistical Computing with R\n\n\n\n\n\nStatistical Computing with R\n\n\n\nRizzo, Maria L. 2019. Statistical Computing with r Maria l. Rizzo. Second edition. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press, Taylor & Francis Group.\nIn (Rizzo 2019) the author presents is a comprehensive guide to the analysis and manipulation of data using R. Within, we are introduced to a wide variety of statistical concepts and tools that enable us to explore and understand complex datasets.\nStandard statistical techniques used in data analysis, such as probability, hypothesis testing are covered. We learn about the normal distribution and its importance in statistical analysis, as well as the Poisson distribution, which is used to model counts of events.\nThe book introduces us to the use of statistical transformations, such as the log transformation, which is often used to make skewed data more normal. We also learn about density estimation and the use of histograms and kernel density estimates to visualize data.\nThe concepts of sampling and random variables are explored, as well as the calculation of sample means and standard errors. We also learn about the use of random samples from Monte Carlo simulation to approximate probabilities and calculate statistics.\nThe book covers the use of algorithms and samplers, such as the Metropolis-Hastings algorithm, to explore parameter space and to generate samples from distributions of interest. We learn about the importance of convergence and the use of proposals in Monte Carlo sampling.\nThe concepts of bias and variance are explored, as well as the calculation of confidence intervals and the use of hypothesis testing to evaluate statistical significance. We also learn about the use of the bootstrap and jackknife methods to find the level of uncertainty in our estimates.\nThroughout the book, we are introduced to the use of R for statistical computing. We learn about the use of formulas to specify statistical models, as well as the use of packages for data manipulation and visualization.\nOverall, “Statistical Computing with R” is an essential resource for anyone interested in using statistical methods to analyze data. It provides a lucid and comprehensive treatment of statistical concepts and their practical implementation using R."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#bayesian-methods-of-data-analysis",
    "href": "posts/2011/2011-08-26-R-books/index.html#bayesian-methods-of-data-analysis",
    "title": "R Books",
    "section": "Bayesian methods of Data Analysis",
    "text": "Bayesian methods of Data Analysis\n\n\n\n\n\nBayesian methods of Data Analysis\n\n\n\nCarlin, B. P., and T. A. Louis. 2008. Bayesian Methods for Data Analysis. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=GTJUt8fcFx8C.\nIn (Carlin and Louis 2008) the author presents us with a framework that is grounded in the philosophy of probability theory. We learn to seek a baseline model then approach the problem at hand with a Bayesian perspective.\nThrough the use of Bayesian models, we can compute the conditional distributions of our data and evaluate the error and loss functions. We must consider convergence, the choice of priors, and how they are specified. We use Bayes’ rule to compute the posterior distribution and marginal likelihood, and we obtain point estimates and credible intervals.\nThe use of the Gibbs sampler and the Metropolis-Hastings algorithm in MCMC methods are presented as tools to obtain a sample from the posterior distribution. We use WinBUGS code and Monte Carlo simulations to produce results that are in line with the data observed.\nWe are introduced to the concept of the Bayes factor, and how it is used to compare models. We also understand how the use of the Jeffreys prior, the hyperprior, and the conjugate prior can be used to simplify our computations.\nIn Bayesian methods, we use the full conditional distributions to obtain the joint posterior distribution of our parameters. We also compute the marginal posterior distribution, which can be used to obtain a credible interval.\nWe are shown how to deal with univariate and multivariate data, and how to model the random effects and covariate effects. We also understand how to evaluate the performance of our models through histograms, percentiles, and plots.\nIn this work, we are presented with a practical and useful guide to Bayesian methods that can be applied to a variety of problems."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#information-theory-inference-and-learning-algorithms",
    "href": "posts/2011/2011-08-26-R-books/index.html#information-theory-inference-and-learning-algorithms",
    "title": "R Books",
    "section": "Information Theory, Inference and Learning Algorithms",
    "text": "Information Theory, Inference and Learning Algorithms\n\n\n\n\n\nInformation Theory, Inference and Learning Algorithms\n\n\n\nMacKay, David J. C. 2003. Information Theory, Inference, and Learning Algorithms. Copyright Cambridge University Press.\nInformation Theory, Inference and Learning Algorithms (MacKay 2003) by David J.C. MacKay FRS - Course notes: Information Theory, Pattern Recognition, and Neural Networks.\nThis is not an R book as far as I recall but it is available online, together with lectures by the author. I recommend this book and videos for anyone interested bayesian data analysis. The author was a physicist, a leading bayesian and pioneer in Bayesian Neural Networks whose work is very relavant even today (2024)"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#references",
    "href": "posts/2011/2011-08-26-R-books/index.html#references",
    "title": "R Books",
    "section": "References",
    "text": "References\n\n\n\nSoftware for data analysis\nPrinciples of Statistical Data Handling\nProbability & Statistics for Engineers and Scientists\nIntroduction to Probability and Statistics Using R\nStatistical Computing with R\nBayesian methods of Data Analysis\nInformation Theory, Inference and Learning Algorithms"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html",
    "href": "posts/2011/regex-cheatsheet/index.html",
    "title": "RegEX",
    "section": "",
    "text": "Place content here"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#getting-started",
    "href": "posts/2011/regex-cheatsheet/index.html#getting-started",
    "title": "RegEX",
    "section": "Getting Started",
    "text": "Getting Started\n\nIntroduction\nThis is a quick cheat sheet to getting started with regular expressions.\n\n\nCharacter Classes\n\n\n\nPattern\nDescription\n\n\n\n\n[abc]\nA single character of: a, b or c\n\n\n[^abc]\nA character except: a, b or c\n\n\n[a-z]\nA character in the range: a-z\n\n\n[^a-z]\nA character not in the range: a-z\n\n\n[0-9]\nA digit in the range: 0-9\n\n\n[a-zA-Z]\nA character in the range:a-z or A-Z\n\n\n[a-zA-Z0-9]\nA character in the range: a-z, A-Z or 0-9\n\n\n\n\n\nQuantifiers\n\n\n\nPattern\nDescription\n\n\n\n\na?\nZero or one of a\n\n\na*\nZero or more of a\n\n\na+\nOne or more of a\n\n\n[0-9]+\nOne or more of 0-9\n\n\na{3}\nExactly 3 of a\n\n\na{3,}\n3 or more of a\n\n\na{3,6}\nBetween 3 and 6 of a\n\n\na*\nGreedy quantifier\n\n\na*?\nLazy quantifier\n\n\na*+\nPossessive quantifier\n\n\n\n\n\nCommon Metacharacters\n\n\n^\n{\n+\n&lt;\n[\n*\n)\n&gt;\n.\n(\n|\n$\n\\\n?\n\n\nEscape these special characters with \\\n\n\nMeta Sequences\n\n\n\n\n\n\n\nPattern\nDescription\n\n\n\n\n.\nAny single character\n\n\n\\s\nAny whitespace character\n\n\n\\S\nAny non-whitespace character\n\n\n\\d\nAny digit, Same as [0-9]\n\n\n\\D\nAny non-digit, Same as [^0-9]\n\n\n\\w\nAny word character\n\n\n\\W\nAny non-word character\n\n\n\\X\nAny Unicode sequences, linebreaks included\n\n\n\\C\nMatch one data unit\n\n\n\\R\nUnicode newlines\n\n\n\\v\nVertical whitespace character\n\n\n\\V\nNegation of anything except newlines and vertical tabs\n\n\n\\h\nHorizontal whitespace character\n\n\n\\H\nNegation of \n\n\n\\K\nReset match\n\n\n\\n\nMatch nth subpattern\n\n\n\\pX\nUnicode property X\n\n\n\\p{...}\nUnicode property or script category\n\n\n\\PX\nNegation of \n\n\n\\P{...}\nNegation of \n\n\n\\Q...\\E\nQuote; treat as literals\n\n\n\\k&lt;name&gt;\nMatch subpattern name\n\n\n\\k'name'\nMatch subpattern name\n\n\n\\k{name}\nMatch subpattern name\n\n\n\\gn\nMatch nth subpattern\n\n\n\\g{n}\nMatch nth subpattern\n\n\n\\g&lt;n&gt;\nRecurse nth capture group\n\n\n\\g'n'\nRecurses nth capture group.\n\n\n\\g{-n}\nMatch nth relative previous subpattern\n\n\n\\g&lt;+n&gt;\nRecurse nth relative upcoming subpattern\n\n\n\\g'+n'\nMatch nth relative upcoming subpattern\n\n\n\\g'letter'\nRecurse named capture group letter\n\n\n\\g{letter}\nMatch previously-named capture group letter\n\n\n\\g&lt;letter&gt;\nRecurses named capture group letter\n\n\n\\xYY\nHex character YY\n\n\n\\x{YYYY}\nHex character YYYY\n\n\n\\ddd\nOctal character ddd\n\n\n\\cY\nControl character Y\n\n\n[\\b]\nBackspace character\n\n\n\\\nMakes any character literal\n\n\n\n\n\nAnchors\n\n\n\nPattern\nDescription\n\n\n\n\n\\G\nStart of match\n\n\n^\nStart of string\n\n\n$\nEnd of string\n\n\n\\A\nStart of string\n\n\n\\Z\nEnd of string\n\n\n\\z\nAbsolute end of string\n\n\n\\b\nA word boundary\n\n\n\\B\nNon-word boundary\n\n\n\n\n\nSubstitution\n\n\n\nPattern\nDescription\n\n\n\n\n\\0\nComplete match contents\n\n\n\\1\nContents in capture group 1\n\n\n$1\nContents in capture group 1\n\n\n${foo}\nContents in capture group foo\n\n\n\\x20\nHexadecimal replacement values\n\n\n\\x{06fa}\nHexadecimal replacement values\n\n\n\\t\nTab\n\n\n\\r\nCarriage return\n\n\n\\n\nNewline\n\n\n\\f\nForm-feed\n\n\n\\U\nUppercase Transformation\n\n\n\\L\nLowercase Transformation\n\n\n\\E\nTerminate any Transformation\n\n\n\n\n\nGroup Constructs\n\n\n\nPattern\nDescription\n\n\n\n\n(...)\nCapture everything enclosed\n\n\n(a|b)\nMatch either a or b\n\n\n(?:...)\nMatch everything enclosed\n\n\n(?&gt;...)\nAtomic group (non-capturing)\n\n\n(?|...)\nDuplicate subpattern group number\n\n\n(?#...)\nComment\n\n\n\n|(?'name'...) | Named Capturing Group| |(?&lt;name&gt;...) | Named Capturing Group| |(?P&lt;name&gt;...) | Named Capturing Group|\n|(?imsxXU) | Inline modifiers| |(?(DEFINE)...) | Pre-define patterns before using them|\n\n\nAssertions\n\n\n\n-\n-\n\n\n\n\n(?(1)yes|no)\nConditional statement\n\n\n(?(R)yes|no)\nConditional statement\n\n\n(?(R#)yes|no)\nRecursive Conditional statement\n\n\n(?(R&name)yes|no)\nConditional statement\n\n\n(?(?=...)yes|no)\nLookahead conditional\n\n\n(?(?&lt;=...)yes|no)\nLookbehind conditional\n\n\n\n\n\nLookarounds\n\n\n\n-\n-\n\n\n\n\n(?=...)\nPositive Lookahead\n\n\n(?!...)\nNegative Lookahead\n\n\n(?&lt;=...)\nPositive Lookbehind\n\n\n(?&lt;!...)\nNegative Lookbehind\n\n\n\nLookaround lets you match a group before (lookbehind) or after (lookahead) your main pattern without including it in the result.\n\n\nFlags/Modifiers\n\n\n\nPattern\nDescription\n\n\n\n\ng\nGlobal\n\n\nm\nMultiline\n\n\ni\nCase insensitive\n\n\nx\nIgnore whitespace\n\n\ns\nSingle line\n\n\nu\nUnicode\n\n\nX\neXtended\n\n\nU\nUngreedy\n\n\nA\nAnchor\n\n\nJ\nDuplicate group names\n\n\n\n\n\nRecurse\n\n\n\n-\n-\n\n\n\n\n(?R)\nRecurse entire pattern\n\n\n(?1)\nRecurse first subpattern\n\n\n(?+1)\nRecurse first relative subpattern\n\n\n(?&name)\nRecurse subpattern name\n\n\n(?P=name)\nMatch subpattern name\n\n\n(?P&gt;name)\nRecurse subpattern name\n\n\n\n\n\nPOSIX Character Classes\n\n\n\n\n\n\n\n\nCharacter Class\nSame as\nMeaning\n\n\n\n\n[[:alnum:]]\n[0-9A-Za-z]\nLetters and digits\n\n\n[[:alpha:]]\n[A-Za-z]\nLetters\n\n\n[[:ascii:]]\n[\\x00-\\x7F]\nASCII codes 0-127\n\n\n[[:blank:]]\n[\\t ]\nSpace or tab only\n\n\n[[:cntrl:]]\n[\\x00-\\x1F\\x7F]\nControl characters\n\n\n[[:digit:]]\n[0-9]\nDecimal digits\n\n\n[[:graph:]]\n[[:alnum:][:punct:]]\nVisible characters (not space)\n\n\n[[:lower:]]\n[a-z]\nLowercase letters\n\n\n[[:print:]]\n[ -~] == [ [:graph:]]\nVisible characters\n\n\n[[:punct:]]\n[!“#$%&’()*+,-./:;&lt;=&gt;?@[]^_`{|}~]\nVisible punctuation characters\n\n\n[[:space:]]\n[\nWhitespace\n\n\n[[:upper:]]\n[A-Z]\nUppercase letters\n\n\n[[:word:]]\n[0-9A-Za-z_]\nWord characters\n\n\n[[:xdigit:]]\n[0-9A-Fa-f]\nHexadecimal digits\n\n\n[[:&lt;:]]\n[\\b(?=\\w)]\nStart of word\n\n\n[[:&gt;:]]\n[\\b(?&lt;=\\w)]\nEnd of word\n\n\n\n{.show-header}\n\n\nControl verb\n\n\n\n-\n-\n\n\n\n\n(*ACCEPT)\nControl verb\n\n\n(*FAIL)\nControl verb\n\n\n(*MARK:NAME)\nControl verb\n\n\n(*COMMIT)\nControl verb\n\n\n(*PRUNE)\nControl verb\n\n\n(*SKIP)\nControl verb\n\n\n(*THEN)\nControl verb\n\n\n(*UTF)\nPattern modifier\n\n\n(*UTF8)\nPattern modifier\n\n\n(*UTF16)\nPattern modifier\n\n\n(*UTF32)\nPattern modifier\n\n\n(*UCP)\nPattern modifier\n\n\n(*CR)\nLine break modifier\n\n\n(*LF)\nLine break modifier\n\n\n(*CRLF)\nLine break modifier\n\n\n(*ANYCRLF)\nLine break modifier\n\n\n(*ANY)\nLine break modifier\n\n\n\\R\nLine break modifier\n\n\n(*BSR_ANYCRLF)\nLine break modifier\n\n\n(*BSR_UNICODE)\nLine break modifier\n\n\n(*LIMIT_MATCH=x)\nRegex engine modifier\n\n\n(*LIMIT_RECURSION=d)\nRegex engine modifier\n\n\n(*NO_AUTO_POSSESS)\nRegex engine modifier\n\n\n(*NO_START_OPT)\nRegex engine modifier"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-examples",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-examples",
    "title": "RegEX",
    "section": "Regex examples",
    "text": "Regex examples\n\nCharacters\n\n\n\n\n\n\n\nPattern\nMatches\n\n\n\n\nring\nMatch ring springboard etc.\n\n\n.\nMatch a, 9, + etc.\n\n\nh.o\nMatch hoo, h2o, h/o etc.\n\n\nring\\?\nMatch ring?\n\n\n\\(quiet\\)\nMatch (quiet)\n\n\nc:\\\\windows\nMatch c:\n\n\n\nUse \\ to search for these special characters:  [ \\ ^ $ . | ? * + ( ) { }\n\n\nAlternatives\n\n\n\nPattern\nMatches\n\n\n\n\ncat|dog\nMatch cat or dog\n\n\nid|identity\nMatch id or identity\n\n\nidentity|id\nMatch id or identity\n\n\n\nOrder longer to shorter when alternatives overlap\n\n\nCharacter classes\n\n\n\nPattern\nMatches\n\n\n\n\n[aeiou]\nMatch any vowel\n\n\n[^aeiou]\nMatch a NON vowel\n\n\nr[iau]ng\nMatch ring, wrangle, sprung, etc.\n\n\ngr[ae]y\nMatch gray or grey\n\n\n[a-zA-Z0-9]\nMatch any letter or digit\n\n\n[\\u3a00-\\ufa99]\nMatch any Unicode Hàn (中文)\n\n\n\nIn [ ] always escape . \\ ] and sometimes ^ - .\n\n\nShorthand classes\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n\\w\n“Word” character (letter, digit, or underscore)\n\n\n\\d\nDigit\n\n\n\\s\nWhitespace (space, tab, vtab, newline)\n\n\n\\W, \\D, or \\S\nNot word, digit, or whitespace\n\n\n[\\D\\S]\nMeans not digit or whitespace, both match\n\n\n[^\\d\\s]\nDisallow digit and whitespace\n\n\n\n\n\nOccurrences\n\n\n\n\n\n\n\nPattern\nMatches\n\n\n\n\ncolou?r\nMatch color or colour\n\n\n[BW]ill[ieamy's]*\nMatch Bill, Willy, William’s etc.\n\n\n[a-zA-Z]+\nMatch 1 or more letters\n\n\n\\d{3}-\\d{2}-\\d{4}\nMatch a SSN\n\n\n[a-z]\\w{1,7}\nMatch a UW NetID\n\n\n\n\n\nGreedy versus lazy\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n*  + {n,}greedy\nMatch as much as possible\n\n\n&lt;.+&gt;\nFinds 1 big match in &lt;b&gt;bold&lt;/b&gt;\n\n\n*?  +? {n,}?lazy\nMatch as little as possible\n\n\n&lt;.+?&gt;\nFinds 2 matches in &lt;b&gt;bold&lt;/b&gt;\n\n\n\n\n\nScope\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n\\b\n“Word” edge (next to non “word” character)\n\n\n\\bring\nWord starts with “ring”, ex ringtone\n\n\nring\\b\nWord ends with “ring”, ex spring\n\n\n\\b9\\b\nMatch single digit 9, not 19, 91, 99, etc..\n\n\n\\b[a-zA-Z]{6}\\b\nMatch 6-letter words\n\n\n\\B\nNot word edge\n\n\n\\Bring\\B\nMatch springs and wringer\n\n\n^\\d*$\nEntire string must be digits\n\n\n^[a-zA-Z]{4,20}$\nString must have 4-20 letters\n\n\n^[A-Z]\nString must begin with capital letter\n\n\n[\\.!?\"')]$\nString must end with terminal puncutation\n\n\n\n\n\nModifiers\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n(?i)[a-z]*(?-i)\nIgnore case ON / OFF\n\n\n(?s).*(?-s)\nMatch multiple lines (causes . to match newline)\n\n\n(?m)^.*;`(?-m)`  | &lt;yel&gt;^&lt;/yel&gt; & &lt;yel&gt; match lines not whole string\n\n\n\n(?x)\n#free-spacing mode, this EOL comment ignored\n\n\n(?-x)\nfree-spacing mode OFF\n\n\n/regex/ismx\nModify mode for entire string\n\n\n\n\n\nGroups\n\n\n\nPattern\nMeaning\n\n\n\n\n(in\\|out)put\nMatch input or output\n\n\n\\d{5}(-\\d{4})?\nUS zip code (“+ 4” optional)\n\n\n\nParser tries EACH alternative if match fails after group.  Can lead to catastrophic backtracking.\n\n\nBack references\n\n\n\n\n\n\n\nPattern\nMatches\n\n\n\n\n(to) (be) or not \\1 \\2\nMatch to be or not to be\n\n\n([^\\s])\\1{2}\nMatch non-space, then same twice more   aaa, …\n\n\n\\b(\\w+)\\s+\\1\\b\nMatch doubled words\n\n\n\n\n\nNon-capturing group\n\n\n\nPattern\nMeaning\n\n\n\n\non(?:click\\|load)\nFaster than: on(click\\|load)\n\n\n\nUse non-capturing or atomic groups when possible\n\n\nAtomic groups\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n(?&gt;red\\|green\\|blue)\nFaster than non-capturing\n\n\n(?&gt;id\\|identity)\\b\nMatch id, but not identity\n\n\n\n“id” matches, but \\b fails after atomic group, parser doesn’t backtrack into group to retry ‘identity’   If alternatives overlap, order longer to shorter.\n\n\nLookaround\n\n\n\n\n\n\n\nPattern\nMeaning\n\n\n\n\n(?= )\nLookahead, if you can find ahead\n\n\n(?! )\nLookahead,if you can not find ahead\n\n\n(?&lt;= )\nLookbehind, if you can find behind\n\n\n(?&lt;! )\nLookbehind, if you can NOT find behind\n\n\n\\b\\w+?(?=ing\\b)\nMatch warbling, string, fishing, …\n\n\n\\b(?!\\w+ing\\b)\\w+\\b\nWords NOT ending in “ing”\n\n\n(?&lt;=\\bpre).*?\\b\nMatch pretend, present, prefix, …\n\n\n\\b\\w{3}(?&lt;!pre)\\w*?\\b\nWords NOT starting with “pre”\n\n\n\\b\\w+(?&lt;!ing)\\b\nMatch words NOT ending in “ing”\n\n\n\n\n\nIf-then-else\nMatch “Mr.” or “Ms.” if word “her” is later in string\nM(?(?=.*?\\bher\\b)s|r)\\.\nrequires lookaround for IF condition"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-python",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-python",
    "title": "RegEX",
    "section": "RegEx in Python",
    "text": "RegEx in Python\n\nGetting started\nImport the regular expressions module\nimport re\n\n\nExamples\n\nre.search()\n&gt;&gt;&gt; sentence = 'This is a sample string'\n&gt;&gt;&gt; bool(re.search(r'this', sentence, flags=re.I))\nTrue\n&gt;&gt;&gt; bool(re.search(r'xyz', sentence))\nFalse\n\n\nre.findall()\n&gt;&gt;&gt; re.findall(r'\\bs?pare?\\b', 'par spar apparent spare part pare')\n['par', 'spar', 'spare', 'pare']\n&gt;&gt;&gt; re.findall(r'\\b0*[1-9]\\d{2,}\\b', '0501 035 154 12 26 98234')\n['0501', '154', '98234']\n\n\nre.finditer()\n&gt;&gt;&gt; m_iter = re.finditer(r'[0-9]+', '45 349 651 593 4 204')\n&gt;&gt;&gt; [m[0] for m in m_iter if int(m[0]) &lt; 350]\n['45', '349', '4', '204']\n\n\nre.split()\n&gt;&gt;&gt; re.split(r'\\d+', 'Sample123string42with777numbers')\n['Sample', 'string', 'with', 'numbers']\n\n\nre.sub()\n&gt;&gt;&gt; ip_lines = \"catapults\\nconcatenate\\ncat\"\n&gt;&gt;&gt; print(re.sub(r'^', r'* ', ip_lines, flags=re.M))\n* catapults\n* concatenate\n* cat\n\n\nre.compile()\n&gt;&gt;&gt; pet = re.compile(r'dog')\n&gt;&gt;&gt; type(pet)\n&lt;class '_sre.SRE_Pattern'&gt;\n&gt;&gt;&gt; bool(pet.search('They bought a dog'))\nTrue\n&gt;&gt;&gt; bool(pet.search('A cat crossed their path'))\nFalse\n\n\n\nFunctions\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nre.findall\nReturns a list containing all matches\n\n\nre.finditer\nReturn an iterable of match objects (one for each match)\n\n\nre.search\nReturns a Match object if there is a match anywhere in the string\n\n\nre.split\nReturns a list where the string has been split at each match\n\n\nre.sub\nReplaces one or many matches with a string\n\n\nre.compile\nCompile a regular expression pattern for later use\n\n\nre.escape\nReturn string with all non-alphanumerics backslashed\n\n\n\n\n\nFlags\n\n\n\n\n\n\n\n\n-\n-\n-\n\n\n\n\nre.I\nre.IGNORECASE\nIgnore case\n\n\nre.M\nre.MULTILINE\nMultiline\n\n\nre.L\nre.LOCALE\nMake \\w,\\b,\\s locale dependent\n\n\nre.S\nre.DOTALL\nDot matches all (including newline)\n\n\nre.U\nre.UNICODE\nMake \\w,\\b,\\d,\\s unicode dependent\n\n\nre.X\nre.VERBOSE\nReadable style"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-javascript",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-javascript",
    "title": "RegEX",
    "section": "Regex in JavaScript",
    "text": "Regex in JavaScript\n\ntest()\nlet textA = 'I like APPles very much';\nlet textB = 'I like APPles';\nlet regex = /apples$/i\n \n// Output: false\nconsole.log(regex.test(textA));\n \n// Output: true\nconsole.log(regex.test(textB));\n\n\nsearch()\nlet text = 'I like APPles very much';\nlet regexA = /apples/;\nlet regexB = /apples/i;\n \n// Output: -1\nconsole.log(text.search(regexA));\n \n// Output: 7\nconsole.log(text.search(regexB));\n\n\nexec()\nlet text = 'Do you like apples?';\nlet regex= /apples/;\n \n// Output: apples\nconsole.log(regex.exec(text)[0]);\n \n// Output: Do you like apples?\nconsole.log(regex.exec(text).input);\n\n\nmatch()\nlet text = 'Here are apples and apPleS';\nlet regex = /apples/gi;\n \n// Output: [ \"apples\", \"apPleS\" ]\nconsole.log(text.match(regex));\n\n\nsplit()\nlet text = 'This 593 string will be brok294en at places where d1gits are.';\nlet regex = /\\d+/g\n \n// Output: [ \"This \", \" string will be brok\", \"en at places where d\", \"gits are.\" ] \nconsole.log(text.split(regex))\n\n\nmatchAll()\nlet regex = /t(e)(st(\\d?))/g;\nlet text = 'test1test2';\nlet array = [...text.matchAll(regex)];\n\n// Output: [\"test1\", \"e\", \"st1\", \"1\"]\nconsole.log(array[0]);\n\n// Output: [\"test2\", \"e\", \"st2\", \"2\"]\nconsole.log(array[1]);\n\n\nreplace()\nlet text = 'Do you like aPPles?';\nlet regex = /apples/i\n \n// Output: Do you like mangoes?\nlet result = text.replace(regex, 'mangoes');\nconsole.log(result);\n\n\nreplaceAll()\nlet regex = /apples/gi;\nlet text = 'Here are apples and apPleS';\n\n// Output: Here are mangoes and mangoes\nlet result = text.replaceAll(regex, \"mangoes\");\nconsole.log(result);"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-php",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-php",
    "title": "RegEX",
    "section": "Regex in PHP",
    "text": "Regex in PHP\n\nFunctions\n\n\n\n\n\n\n\n-\n-\n\n\n\n\npreg_match()\nPerforms a regex match\n\n\npreg_match_all()\nPerform a global regular expression match\n\n\npreg_replace_callback()\nPerform a regular expression search and replace using a callback\n\n\npreg_replace()\nPerform a regular expression search and replace\n\n\npreg_split()\nSplits a string by regex pattern\n\n\npreg_grep()\nReturns array entries that match a pattern\n\n\n\n\n\npreg_replace\n$str = \"Visit Microsoft!\";\n$regex = \"/microsoft/i\";\n\n// Output: Visit CheatSheets!\necho preg_replace($regex, \"CheatSheets\", $str); \n\n\npreg_match\n$str = \"Visit CheatSheets\";\n$regex = \"#cheatsheets#i\";\n\n// Output: 1\necho preg_match($regex, $str);\n\n\npreg_matchall\n$regex = \"/[a-zA-Z]+ (\\d+)/\";\n$input_str = \"June 24, August 13, and December 30\";\nif (preg_match_all($regex, $input_str, $matches_out)) {\n\n    // Output: 2\n    echo count($matches_out);\n\n    // Output: 3\n    echo count($matches_out[0]);\n\n    // Output: Array(\"June 24\", \"August 13\", \"December 30\")\n    print_r($matches_out[0]);\n\n    // Output: Array(\"24\", \"13\", \"30\")\n    print_r($matches_out[1]);\n}\n\n\npreg_grep\n$arr = [\"Jane\", \"jane\", \"Joan\", \"JANE\"];\n$regex = \"/Jane/\";\n\n// Output: Jane\necho preg_grep($regex, $arr);\n\n\npreg_split\n$str = \"Jane\\tKate\\nLucy Marion\";\n$regex = \"@\\s@\";\n\n// Output: Array(\"Jane\", \"Kate\", \"Lucy\", \"Marion\")\nprint_r(preg_split($regex, $str));"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-java",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-java",
    "title": "RegEX",
    "section": "Regex in Java",
    "text": "Regex in Java\n\nStyles\n\nFirst way\nPattern p = Pattern.compile(\".s\", Pattern.CASE_INSENSITIVE);\nMatcher m = p.matcher(\"aS\");  \nboolean s1 = m.matches();  \nSystem.out.println(s1);   // Outputs: true\n\n\nSecond way\nboolean s2 = Pattern.compile(\"[0-9]+\").matcher(\"123\").matches();  \nSystem.out.println(s2);   // Outputs: true\n\n\nThird way\nboolean s3 = Pattern.matches(\".s\", \"XXXX\");  \nSystem.out.println(s3);   // Outputs: false\n\n\n\nPattern Fields\n\n\n\n-\n-\n\n\n\n\nCANON_EQ\nCanonical equivalence\n\n\nCASE_INSENSITIVE\nCase-insensitive matching\n\n\nCOMMENTS\nPermits whitespace and comments\n\n\nDOTALL\nDotall mode\n\n\nMULTILINE\nMultiline mode\n\n\nUNICODE_CASE\nUnicode-aware case folding\n\n\nUNIX_LINES\nUnix lines mode\n\n\n\n\n\nMethods\n\nPattern\n\nPattern compile(String regex [, int flags])\nboolean matches([String regex, ] CharSequence input)\nString[] split(String regex [, int limit])\nString quote(String s)\n\n\n\nMatcher\n\nint start([int group | String name])\nint end([int group | String name])\nboolean find([int start])\nString group([int group | String name])\nMatcher reset()\n\n\n\nString\n\nboolean matches(String regex)\nString replaceAll(String regex, String replacement)\nString[] split(String regex[, int limit])\n\nThere are more methods …\n\n\n\nExamples\nReplace sentence:\nString regex = \"[A-Z\\n]{5}$\";\nString str = \"I like APP\\nLE\";\n\nPattern p = Pattern.compile(regex, Pattern.MULTILINE);\nMatcher m = p.matcher(str);\n\n// Outputs: I like Apple!\nSystem.out.println(m.replaceAll(\"pple!\"));\nArray of all matches:\nString str = \"She sells seashells by the Seashore\";\nString regex = \"\\\\w*se\\\\w*\";\n\nPattern p = Pattern.compile(regex, Pattern.CASE_INSENSITIVE);\nMatcher m = p.matcher(str);\n\nList&lt;String&gt; matches = new ArrayList&lt;&gt;();\nwhile (m.find()) {\n    matches.add(m.group());\n}\n\n// Outputs: [sells, seashells, Seashore]\nSystem.out.println(matches);"
  },
  {
    "objectID": "posts/2011/regex-cheatsheet/index.html#regex-in-mysql",
    "href": "posts/2011/regex-cheatsheet/index.html#regex-in-mysql",
    "title": "RegEX",
    "section": "Regex in MySQL",
    "text": "Regex in MySQL\n\nFunctions\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nREGEXP\nWhether string matches regex\n\n\nREGEXP_INSTR()\nStarting index of substring matching regex (NOTE: Only MySQL 8.0+)\n\n\nREGEXP_LIKE()\nWhether string matches regex (NOTE: Only MySQL 8.0+)\n\n\nREGEXP_REPLACE()\nReplace substrings matching regex (NOTE: Only MySQL 8.0+)\n\n\nREGEXP_SUBSTR()\nReturn substring matching regex (NOTE: Only MySQL 8.0+)\n\n\n\n\n\nREGEXP\nexpr REGEXP pat \n\nExamples\nmysql&gt; SELECT 'abc' REGEXP '^[a-d]';\n1\nmysql&gt; SELECT name FROM cities WHERE name REGEXP '^A';\nmysql&gt; SELECT name FROM cities WHERE name NOT REGEXP '^A';\nmysql&gt; SELECT name FROM cities WHERE name REGEXP 'A|B|R';\nmysql&gt; SELECT 'a' REGEXP 'A', 'a' REGEXP BINARY 'A';\n1   0\n\n\n\nREGEXP_REPLACE\nREGEXP_REPLACE(expr, pat, repl[, pos[, occurrence[, match_type]]])\n\nExamples\nmysql&gt; SELECT REGEXP_REPLACE('a b c', 'b', 'X');\na X c\nmysql&gt; SELECT REGEXP_REPLACE('abc ghi', '[a-z]+', 'X', 1, 2);\nabc X\n\n\n\nREGEXP_SUBSTR\nREGEXP_SUBSTR(expr, pat[, pos[, occurrence[, match_type]]])\n\nExamples\nmysql&gt; SELECT REGEXP_SUBSTR('abc def ghi', '[a-z]+');\nabc\nmysql&gt; SELECT REGEXP_SUBSTR('abc def ghi', '[a-z]+', 1, 3);\nghi\n\n\n\nREGEXP_LIKE\nREGEXP_LIKE(expr, pat[, match_type])\n\nExamples\nmysql&gt; SELECT regexp_like('aba', 'b+')\n1\nmysql&gt; SELECT regexp_like('aba', 'b{2}')\n0\nmysql&gt; # i: case-insensitive\nmysql&gt; SELECT regexp_like('Abba', 'ABBA', 'i');\n1\nmysql&gt; # m: multi-line\nmysql&gt; SELECT regexp_like('a\\nb\\nc', '^b$', 'm');\n1\n\n\n\nREGEXP_INSTR\nREGEXP_INSTR(expr, pat[, pos[, occurrence[, return_option[, match_type]]]])\n\nExamples\nmysql&gt; SELECT regexp_instr('aa aaa aaaa', 'a{3}');\n2\nmysql&gt; SELECT regexp_instr('abba', 'b{2}', 2);\n2\nmysql&gt; SELECT regexp_instr('abbabba', 'b{2}', 1, 2);\n5\nmysql&gt; SELECT regexp_instr('abbabba', 'b{2}', 1, 3, 1);\n7"
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html",
    "href": "posts/2011/2011-08-11-time-management/index.html",
    "title": "Time management Tips",
    "section": "",
    "text": "Effective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance. In this article, we’ll explore various time management strategies, focusing on learning from others, project management techniques, and effective meetings."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#introduction",
    "href": "posts/2011/2011-08-11-time-management/index.html#introduction",
    "title": "Time management Tips",
    "section": "",
    "text": "Effective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance. In this article, we’ll explore various time management strategies, focusing on learning from others, project management techniques, and effective meetings."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#learning-from-others",
    "href": "posts/2011/2011-08-11-time-management/index.html#learning-from-others",
    "title": "Time management Tips",
    "section": "Learning from Others",
    "text": "Learning from Others\nTo improve your time management skills, consider learning from different sources:\n\nWisdom from experienced individuals: Look for insights and advice from people who have been successful in managing their time. They can provide valuable tips and practical approaches to help you make the most of your time.\nFresh perspectives from younger generations: Younger people often have unique and innovative ideas on how to approach time management. Their perspectives can provide fresh insights on the world and how to navigate it efficiently.\nCassandra’s prophecies: Although they may seem pessimistic or overly cautious, pay attention to people who predict potential problems or obstacles. By considering their warnings, you can proactively prepare for possible challenges, preventing self-fulfilling prophecies."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#project-management",
    "href": "posts/2011/2011-08-11-time-management/index.html#project-management",
    "title": "Time management Tips",
    "section": "Project management",
    "text": "Project management\n\nMeasure work by output, not input: Focus on the results produced rather than the amount of time spent on a task. This will encourage efficiency and productivity.\nDefine specific goals: Clearly outline the objectives, responsibilities, and tasks associated with each project or job.\nTrack goal progress: Regularly monitor the progress of your goals to ensure they are on track for completion.\nProvide periodic progress reports: Keep stakeholders informed with brief, regular updates on project status.\nSet high-quality performance objectives: Ensure that your goals are challenging, attainable, and aligned with your overall mission.\nMaintain a project list: Keep a running list of all projects, breaking them down into smaller, manageable tasks if necessary.\nFocus on the next step, not the final goal: Concentrate on completing the immediate task at hand rather than getting overwhelmed by the overall project.\nPrioritize tasks effectively: Determine which tasks are most important and tackle them first.\nSchedule your day realistically: Plan your day in a way that allows for adequate time to complete tasks without feeling rushed or overwhelmed."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#conducting-efficient-meetings",
    "href": "posts/2011/2011-08-11-time-management/index.html#conducting-efficient-meetings",
    "title": "Time management Tips",
    "section": "Conducting Efficient Meetings:",
    "text": "Conducting Efficient Meetings:\n\nEstablish clear meeting objectives: Ensure that every meeting has a specific purpose, such as group bonding, reaching a group decision, or conducting peer-to-peer negotiations.\nUse alternative communication methods when appropriate: Utilize emails and letters for unidirectional information flow or for maintaining records, as they can be more efficient than meetings for certain tasks."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#conclusion",
    "href": "posts/2011/2011-08-11-time-management/index.html#conclusion",
    "title": "Time management Tips",
    "section": "Conclusion:",
    "text": "Conclusion:\nBy implementing these time management strategies, you can increase your productivity, achieve your goals, and maintain a healthy work-life balance. Remember, the key to success is consistently refining and adapting your approach to time management as you encounter new challenges and opportunities."
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/card.html",
    "href": "posts/2011/bash-cheatsheet/card.html",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "Quarto HTML pages and dashboards are built using bootstrap.\nQuato themes are also based on bootstrap,\nQuarto also supports pandoc for div and spans.\n\nThus it is possible to pop bootstrap css into divs and spans and get formatting using bootstrap.\nIt is also possible to create bootstrap components by structuring markdown with the appropriate css and styling.\n\n\nc.f. bootstrap documentation\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nprimary secondary success danger warning info light dark\n\n\n\n\n\nPraeterea iter est quasdam res quas ex communi.\n\n\nPraeterea iter est quasdam res quas ex communi.\n\n\n\n\nthis isn’t visible using the light theme and only partially supported in the dark theme\n\n\n\n\nClick to toggle popover\n\nspan span"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/card.html#cards",
    "href": "posts/2011/bash-cheatsheet/card.html#cards",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "c.f. bootstrap documentation\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/card.html#using-grid",
    "href": "posts/2011/bash-cheatsheet/card.html#using-grid",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "Some quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card’s content.\nCard link Another link\nGo somewhere"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/card.html#colored-links",
    "href": "posts/2011/bash-cheatsheet/card.html#colored-links",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "primary secondary success danger warning info light dark"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/card.html#trunctated-text",
    "href": "posts/2011/bash-cheatsheet/card.html#trunctated-text",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "Praeterea iter est quasdam res quas ex communi.\n\n\nPraeterea iter est quasdam res quas ex communi."
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/card.html#badges-new-pill",
    "href": "posts/2011/bash-cheatsheet/card.html#badges-new-pill",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "this isn’t visible using the light theme and only partially supported in the dark theme"
  },
  {
    "objectID": "posts/2011/bash-cheatsheet/card.html#popover",
    "href": "posts/2011/bash-cheatsheet/card.html#popover",
    "title": "Quarto 💖 Bootstrap 😁",
    "section": "",
    "text": "Click to toggle popover\n\nspan span"
  },
  {
    "objectID": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html",
    "href": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html",
    "title": "A/B testing cost and risks?",
    "section": "",
    "text": "While it is not a forgone conclusion that CRO driven by A/B test will be a major disaster but it is likely that without some expert supervision it can end up a costing more and taking longer. The statistical concepts are not fairly basic but most of them are misunderstood."
  },
  {
    "objectID": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html#setup-costs",
    "href": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html#setup-costs",
    "title": "A/B testing cost and risks?",
    "section": "Setup costs",
    "text": "Setup costs\nYou generally need to pay a developer to code the alternatives, and to setup the code that does the the sampling and book keeping. An IT guy to deploy it and an analyst or data scientist to analyse it and then a marketing manager to decide to take some action (ok the intervention and decide to do more testing.) ## Exploration costs and diminishing return When we want to romanticize researching a new intervention we call it Exploration and then we use the less romantic term Exploitation to refer to making using our current best intervention to get outcomes from our web marketing efforts. A/B tests shunts a percentage of you traffic (often 50%) for the duration of the experiment to an intervention we call B we expect to beat out current best effort which we call A. But since pretty smart people chose A it can take many attempts to find a B that better. Also each time you find a better alternative it get harder and you end up getting smaller improvements this is due to what is called a smaller effect size for the intervention. Consider that you will eventually get to an optimum and need to look at a different strategy to make improvements. The good news is that each time your exploration works out and you get a better outcome the test will end up driving up your kpi and goals. But the bad news is that most of the time you wont be getting a win and conducting the test will cost you in lost action. Most A/B testing platforms can track your goals and will try to minimize the negative impact of the test using power of Bayesian statistics. You should make use of these if they are an option. But if you are getting started you might not have the benefit of integrating this type of stats into your experiment. Also even the amazingly capable people who build these tools can get it wrong and have had to rewriting their systems. The main reason that testing has diminishing returns is that you it is much easier to test for big changes early but small subtle changes which have small effects take longer to achieve statistical significance because we are trying to separate two very similar signals coming from A and B. Each win generally means that the next test will take longer be less likely to be a win and therefore cost more on average.\nAs the are usually diminishing returns from running A/B test. If your landing page has a low conversion rate, say 1.5%. You can do experiments and you might be able to reduce bounce rate by 40% percent which should increase your conversion rate to 1.2%. Next you might be able to increase time on page by 300% using better videos that might boost your conversion rate to 3.5, your might be able to pick a more effective cal to action and get around to 3.7% conversion rates. And you might also add an aggressive exit popup and get to 4.2%. That is a dream scenario that CTR consultants dream about. But if your are not working in a startup and you have a solid marketing team you will probably start with many of the choices near optimum and end up at 3% conversion rate. And each successful test will give you a .1% improvement. That might be ok but such a small improvement will take much longer to reach significance and therefore end up costing more. With all the test you ever do you might never reach 4% Over time you should expect that A/B test will be measuring ever smaller effects and will require more traffic to arrive at a statistically significant result. Longer test will cost more."
  },
  {
    "objectID": "posts/2024/post-with-code/index.html",
    "href": "posts/2024/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n2\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {Post {With} {Code}},\n  date = {2024-01-28},\n  url = {https://orenbochman.github.io/blog//posts/2024/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. “Post With Code.” January 28, 2024. https://orenbochman.github.io/blog//posts/2024/post-with-code."
  },
  {
    "objectID": "posts/2024/BBdM/BBdM.html",
    "href": "posts/2024/BBdM/BBdM.html",
    "title": "Political Scenario Prediction Using Game theory",
    "section": "",
    "text": "A review of MESQUITA (2011)\nThis technical paper is fairly challenging to understand. The main thrust of this work is when conducting an inteligence assement of some future political scenario, assuming one can identify the main actors thier relative influence, their positions and thier belief about other players we can use a formal model to assess how they will behave and interpret various outcomes.\nNow there are any number of ways this can go bad:\nSo now that we named the elephants in the room, we can talk about can address the least interesting part of the paper - the fact that the models perform very well when fed with data from expert analysts, assuming that the raw data is accurate.\nPrior work by Tatelock outlines the notion of comparing forecasting skill of analysts AKA experts who engage in competition charectrises thier abilities. Tatelock call the best Superforcastors suggesting that there is method behind all this madness. A second claim is that formal Models tend to outperform the experts given thier raw data. Mesiqua explains that this is due to reduced variance.\nMy current interest are:\nI suppose that if you get good at structuring the problem then interpretation can be much easier.\nIn some senses the old model is very simple considering how well it performs. Yet Bayesian games are the kind of games we all played in preschool or at least not to solve them for a perfect Bayesian equilibrium. So there is that inherent mathmatical complexity to deal with. If I’m not mistaken the author is bent on reporting his successes without revealing too much about how to reproduce his work. I havent reviewd the relevant papers. A second big chunk of this paper deals with the performance of the old model and responding to criticism of his work. So the readers need to decipher as much as they can. However it is not very much motivated. BBdM is an eloquent speaker and an excellent author of several book."
  },
  {
    "objectID": "posts/2024/BBdM/BBdM.html#structure-of-the-new-model",
    "href": "posts/2024/BBdM/BBdM.html#structure-of-the-new-model",
    "title": "Political Scenario Prediction Using Game theory",
    "section": "Structure of the New Model",
    "text": "Structure of the New Model\nThe new model’s structure is much more complex than the expected utility model and so it will be important for it to outperform that model meaningfully to justify its greater computational complexity. Inputs are, in contrast, only modestly more complicated or demanding although what is done with them is radically different.\nEach player is uncertain whether the other player is a hawk or a dove and whether the other player is pacific or retaliatory. By hawk I mean a player who prefers to try to coerce a rival to give in to the hawk’s demands even if this means imposing (and enduring) costs rather than compromising on the policy outcome. A dove prefers to compromise rather than engage in costly coercion to get the rival to give in. A retaliatory player prefers to defend itself (potentially at high costs), rather than allow itself to be bullied into giving in to the rival, while a pacific player prefers to give in when coerced in order to avoid further costs associated with self-defense. The priors on types are set at 0.5 at the game’s outset and are updated according to Bayes’ Rule. This element is absent in Bueno de Mesquita and Lalman (1992). In fact, the model here is an iterated, generalized version of their model, integrating results across N(N–1) player dyads, introducing a range of uncertainties and an indeterminate number of iterations as well as many other features as discussed below. Of course, uncertainty is not and cannot be limited to information about player types when designing an applied model. We must also be concerned that there is uncertainty in the estimates of values on input variables whether the data are derived, as in the tests here, from experts or, as in cases reported on in the final two\nthere are 4 type of players: &lt;[hawk|dove],[pacific,retalitory]&gt;\n\n\n\nold-model\nnew model"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html",
    "href": "posts/2023/2023-05-10-migration-notes/index.html",
    "title": "Quarto Migration Notes",
    "section": "",
    "text": "I was able to stand on the shoulders of :giants when I migrated this blog."
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#giants",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#giants",
    "title": "Quarto Migration Notes",
    "section": ":giants",
    "text": ":giants\n(Rapp 2022) (Navarro 2022), (Hill 2022), (Kaye 2022)\n\nRapp, Albert. 2022. “The Ultimate Guide to Starting a Quarto Blog.” June 24, 2022. https://albert-rapp.de/posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html.\n\nNavarro, Danielle. 2022. “Porting a Distill Blog to Quarto.” April 20, 2022. https://blog.djnavarro.net/posts/2022-04-20_porting-to-quarto.\n\nHill, Alison. 2022. “We Don’t Talk about Quarto.” April 4, 2022. https://www.apreshill.com/blog/2022-04-we-dont-talk-about-quarto/.\n\nKaye, Ella. 2022. “Welcome to My Quarto Website!” December 11, 2022. https://ellakaye.co.uk/posts/2022-12-11_welcome-quarto."
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#markdown",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#markdown",
    "title": "Quarto Migration Notes",
    "section": "Markdown",
    "text": "Markdown\n\nQuarto’s markdown isn’t my favorite markdown implementation.\nIt is based on pandoc spec"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#the-devil-is-in-the-details",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#the-devil-is-in-the-details",
    "title": "Quarto Migration Notes",
    "section": "The devil is in the details",
    "text": "The devil is in the details\nThere are lots of details that should be in the guide that are scattered all over the quarto site.\nI decided that all posts should have the following fields in their front matter:\n\ntitle\nsubtitle\ndescription\ndate\ncategories\nimage\nimage-description"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#virtual-environments",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#virtual-environments",
    "title": "Quarto Migration Notes",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nare documented here\nideal one can have one virtual environment for the whole site"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#lightbox-galleries",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#lightbox-galleries",
    "title": "Quarto Migration Notes",
    "section": "Lightbox Galleries",
    "text": "Lightbox Galleries\nso far I used this only in the this page\nthe light box plugin was integrated into Quarto in the version 4.1 which I migrated to. I have been using light box to make notes of talks and so on. So in for this blog adding light boxes is a breeze.\nAll that’s realy needed is to change setting in the frontmatter:\nlightbox: true\nwhich I did for all posts by adding the setting to the _metadata.yaml in the posts directory. And now all images default to opening within their own lightbox when clicked upon.\nto disable the feature say, on a logo for example just add .no-lightbox css style to the image like this:\n![caption](filename.png){.no-lightbox}\nif you want to be able to scroll through a series of images we need to decorate each images as follows:\n![caption](filename.png){group=\"my-gallery\"}\nAn added bonus is that it is possible to zoom into these light-boxed images"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#extras",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#extras",
    "title": "Quarto Migration Notes",
    "section": "Extras",
    "text": "Extras\n\nthe about page is based on postcards package\nicons for navigation come from bootstrap\ncover images are from pexels"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#nutshell-addin",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#nutshell-addin",
    "title": "Quarto Migration Notes",
    "section": "Nutshell Addin",
    "text": "Nutshell Addin\n:Nutshells: seems like an interesting way to reduce the clutter by hiding unessential exposition.\n\nWill users realize they can interact with these nutshells 1 ?\nNot sure if the implementation is good enough to be worth the hassle.\nThey also seem to be unwieldy, 2?\nCan we get the page to keep track of the nutshell state so we can bookmarkit ?\nI’m going to try it out and see if they are pages for which they are suitable.\n\n1 in landing pages I used to have an animation to get users to engage with in-lined content 2 could we loose the callout/baloon\n:Nutshells\nWhich are links that expand inline into content\n\n\nOpen issues:\n\ncan I readily integrate books and presentation into this blog ?\n\ncan I drop them in or do I need to build them in another repo\nthen deploy\nthen link!?\n\nhow about embedding repls\nhow about embedding shiny live apps\n\nhttps://github.com/shafayetShafee\n\n\nEmbedding PDF\n\nplugin repo\ndocumentation\n\ninstallation\nquarto add jmgirard/embedpdf\n{{&lt; pdf dummy.pdf &gt;}}\n{{&lt; pdf dummy.pdf width=100% height=800 &gt;}}\n{{&lt; pdf dummy.pdf border=1 &gt;}}\n{{&lt; pdf dummy.pdf class=myclass &gt;}}"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html",
    "href": "notes/dnn/dnn-07/l07b.html",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n::: column-margin"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#the-equivalence-between-feedforward-nets-and-recurrent-nets",
    "href": "notes/dnn/dnn-07/l07b.html#the-equivalence-between-feedforward-nets-and-recurrent-nets",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "The equivalence between feedforward nets and recurrent nets",
    "text": "The equivalence between feedforward nets and recurrent nets\n\n\nAssume that there is a time delay of 1 in using each connection.\nThe recurrent net is just a layered net that keeps reusing the same weights."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#reminder-backpropagation-with-weight-constraints",
    "href": "notes/dnn/dnn-07/l07b.html#reminder-backpropagation-with-weight-constraints",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "Reminder: Backpropagation with weight constraints",
    "text": "Reminder: Backpropagation with weight constraints\n\n\nIt is easy to modify the backprop algorithm to incorporate linear constraints between the weights.\nWe compute the gradients as usual, and then modify the gradients so that they satisfy the constraints.\n\nSo if the weights started off satisfying the constraints, they will continue to satisfy them."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#backpropagation-through-time",
    "href": "notes/dnn/dnn-07/l07b.html#backpropagation-through-time",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "Backpropagation through time",
    "text": "Backpropagation through time\n\nWe can think of the recurrent net as a layered, feed-forward net with shared weights and then train the feed-forward net with weight constraints.\nWe can also think of this training algorithm in the time domain:\n\nThe forward pass builds up a stack of the activities of all the units at each time step.\nThe backward pass peels activities off the stack to compute the error derivatives at each time step.\nAfter the backward pass we add together the derivatives at all the different times for each weight."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#an-irritating-extra-issue",
    "href": "notes/dnn/dnn-07/l07b.html#an-irritating-extra-issue",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "An irritating extra issue",
    "text": "An irritating extra issue\n\nWe need to specify the initial activity state of all the hidden and output units.\nWe could just fix these initial states to have some default value like 0.5.\nBut it is better to treat the initial states as learned parameters.\nWe learn them in the same way as we learn the weights.\n\nStart off with an initial random guess for the initial states.\nAt the end of each training sequence, backpropagate through time all the way to the initial states to get the gradient of the error function with respect to each initial state.\nAdjust the initial states by following the negative gradient."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#providing-input-to-recurrent-networks",
    "href": "notes/dnn/dnn-07/l07b.html#providing-input-to-recurrent-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "Providing input to recurrent networks",
    "text": "Providing input to recurrent networks\n\n\nWe can specify inputs in several ways:\n\nSpecify the initial states of all the units.\nSpecify the initial states of a subset of the units.\nSpecify the states of the same subset of the units at every time step.\n\nThis is the natural way to model most sequential data."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07b.html#teaching-signals-for-recurrent-networks",
    "href": "notes/dnn/dnn-07/l07b.html#teaching-signals-for-recurrent-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7b",
    "section": "Teaching signals for recurrent networks",
    "text": "Teaching signals for recurrent networks\n\n\nWe can specify targets in several ways:\n\nSpecify desired final activities of all the units\nSpecify desired activities of all units for the last few steps\n\nGood for learning attractors\nIt is easy to add in extra error derivatives as we backpropagate.\n\nSpecify the desired activity of a subset of the units.\n\nThe other units are input or hidden units."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html",
    "href": "notes/dnn/dnn-07/l07a.html",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n::: column-margin"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#getting-targets-when-modeling-sequences",
    "href": "notes/dnn/dnn-07/l07a.html#getting-targets-when-modeling-sequences",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Getting targets when modeling sequences",
    "text": "Getting targets when modeling sequences\n\nWhen applying machine learning to sequences, we often want to turn an input sequence into an output sequence that lives in a different domain.\n\nE. g. turn a sequence of sound pressures into a sequence of word identities.\n\nWhen there is no separate target sequence, we can get a teaching signal by trying to predict the next term in the input sequence.\n\nThe target output sequence is the input sequence with an advance of 1 step.\nThis seems much more natural than trying to predict one pixel in an image from the other pixels, or one patch of an image from the rest of the image.\nFor temporal sequences there is a natural order for the predictions.\n\nPredicting the next term in a sequence blurs the distinction between supervised and unsupervised learning.\n\nIt uses methods designed for supervised learning, but it doesn’t require a separate teaching signal."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#memoryless-models-for-sequences",
    "href": "notes/dnn/dnn-07/l07a.html#memoryless-models-for-sequences",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Memoryless models for sequences",
    "text": "Memoryless models for sequences\n\n\n\n\n\nMemoryless models\n\n\n\nAutoregressive models Predict the next term in a sequence from a fixed number of previous terms using delay taps.\nFeed-forward neural nets These generalize autoregressive models by using one or more layers of non-linear hidden units. e.g. Bengio’s first language model."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#beyond-memoryless-models",
    "href": "notes/dnn/dnn-07/l07a.html#beyond-memoryless-models",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Beyond memoryless models",
    "text": "Beyond memoryless models\n\nIf we give our generative model some hidden state, and if we give this hidden state its own internal dynamics, we get a much more interesting kind of model.\n\nIt can store information in its hidden state for a long time.\nIf the dynamics is noisy and the way it generates outputs from its hidden state is noisy, we can never know its exact hidden state.\nThe best we can do is to infer a probability distribution over the space of hidden state vectors.\n\nThis inference is only tractable for two types of hidden state model.\n\nThe next three slides are mainly intended for people who already know about these two types of hidden state model. They show how RNNs differ.\nDo not worry if you cannot follow the details."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#linear-dynamical-systems-engineers-love-them",
    "href": "notes/dnn/dnn-07/l07a.html#linear-dynamical-systems-engineers-love-them",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Linear Dynamical Systems (engineers love them!)",
    "text": "Linear Dynamical Systems (engineers love them!)\n\n\n\nlinear dynamic systems\n\n\n\nThese are generative models. They have a real valued hidden state that cannot be observed directly.\n\nThe hidden state has linear dynamics with Gaussian noise and produces the observations using a linear model with Gaussian noise.\nThere may also be driving inputs.\n\nTo predict the next output (so that we can shoot down the missile) we need to infer the hidden state.\n\nA linearly transformed Gaussian is a Gaussian. So the distribution over the hidden."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#hidden-markov-models-computer-scientists-love-them",
    "href": "notes/dnn/dnn-07/l07a.html#hidden-markov-models-computer-scientists-love-them",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Hidden Markov Models (computer scientists love them!)",
    "text": "Hidden Markov Models (computer scientists love them!)\n\n\n\nHidden Markov Models\n\n\n\nHidden Markov Models have a discrete one of-N hidden state. Transitions between states are stochastic and controlled by a transition matrix. The outputs produced by a state are stochastic.\n\nWe cannot be sure which state produced a given output. So the state is “hidden”.\nIt is easy to represent a probability distribution across N states with N numbers.\n\nTo predict the next output we need to infer the probability distribution over hidden states.\n\nHMMs have efficient algorithms for"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#a-fundamental-limitation-of-hmms",
    "href": "notes/dnn/dnn-07/l07a.html#a-fundamental-limitation-of-hmms",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "A fundamental limitation of HMMs",
    "text": "A fundamental limitation of HMMs\n\nConsider what happens when a hidden Markov model generates data.\n\nAt each time step it must select one of its hidden states. So with N hidden states it can only remember log(N) bits about what it generated so far.\n\nConsider the information that the first half of an utterance contains about the second half:\n\nThe syntax needs to fit (e.g. number and tense agreement).\nThe semantics needs to fit. The intonation needs to fit.\nThe accent, rate, volume, and vocal tract characteristics must all fit.\n\nAll these aspects combined could be 100 bits of information that the first half of an utterance needs to convey to the second half. 2^100"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#recurrent-neural-networks",
    "href": "notes/dnn/dnn-07/l07a.html#recurrent-neural-networks",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\n\nrnns.png\n\n\n\nRNNs are very powerful, because they combine two properties:\n\nDistributed hidden state that allows them to store a lot of information about the past efficiently.\nNon-linear dynamics that allows them to update their hidden state in complicated ways.\n\nWith enough neurons and time, RNNs can compute anything that can be computed by your computer."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07a.html#do-generative-models-need-to-be-stochastic",
    "href": "notes/dnn/dnn-07/l07a.html#do-generative-models-need-to-be-stochastic",
    "title": "Deep Neural Networks - Notes for Lesson 7a",
    "section": "Do generative models need to be stochastic?",
    "text": "Do generative models need to be stochastic?\n\nLinear dynamical systems and hidden Markov models are stochastic models.\n\nBut the posterior probability distribution over their hidden states given the observed data so far is a deterministic function of the data.\n\nRecurrent neural networks are deterministic.\n\nSo think of the hidden state of an RNN as the equivalent of the deterministic probability distribution over hidden states in a linear dynamical system or hidden Markov model. ## Recurrent neural networks\n\nWhat kinds of behavior can RNNs exhibit?\n\nThey can oscillate. Good for motor control?\nThey can settle to point attractors. Good for retrieving memories?\nThey can behave chaotically. Bad for information processing?\nRNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interacting to produce very complicated effects.\n\nBut the computational power of RNNs makes them very hard to train.\n\nFor many years we could not exploit the computational power of RNNs despite some heroic efforts (e.g. Tony Robinson’s speech recognizer).\n\n\n\n\n\nMemoryless models\nlinear dynamic systems\nHidden Markov Models\nrnns.png"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html",
    "href": "notes/dnn/dnn-07/l07c.html",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n:::"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#a-good-toy-problem-for-a-recurrent-network",
    "href": "notes/dnn/dnn-07/l07c.html#a-good-toy-problem-for-a-recurrent-network",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "A good toy problem for a recurrent network",
    "text": "A good toy problem for a recurrent network"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#the-algorithm-for-binary-addition",
    "href": "notes/dnn/dnn-07/l07c.html#the-algorithm-for-binary-addition",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "The algorithm for binary addition",
    "text": "The algorithm for binary addition\n\n\n\nFinite State Automaton\n\n\nThis is a finite state automaton. It decides what transition to make by looking at the next column. It prints after making the transition. It moves from right to left over the two input numbers."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#a-recurrent-net-for-binary-addition",
    "href": "notes/dnn/dnn-07/l07c.html#a-recurrent-net-for-binary-addition",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "A recurrent net for binary addition",
    "text": "A recurrent net for binary addition\n\n\nThe network has two input units and one output unit.\nIt is given two input digits at each time step.\nThe desired output at each time step is the output for the column that was provided as input two time steps ago.\n\nIt takes one time step to update the hidden units based on the two input digits.\nIt takes another time step for the hidden units to cause the output"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#the-connectivity-of-the-network",
    "href": "notes/dnn/dnn-07/l07c.html#the-connectivity-of-the-network",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "The connectivity of the network",
    "text": "The connectivity of the network\n\nThe 3 hidden units are fully interconnected in both directions. - This allows a hidden activity pattern at one time step to vote for the hidden activity pattern at the next time step. - The input units have feedforward connections that allow then to vote for the next hidden activity pattern."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07c.html#what-the-network-learns",
    "href": "notes/dnn/dnn-07/l07c.html#what-the-network-learns",
    "title": "Deep Neural Networks - Notes for Lesson 7c",
    "section": "What the network learns",
    "text": "What the network learns\n\nIt learns four distinct patterns of activity for the 3 hidden units. These patterns correspond to the nodes in the finite state automaton.\n\nDo not confuse units in a neural network with nodes in a finite state automaton. Nodes are like activity vectors.\nThe automaton is restricted to be in exactly one state at each time. The hidden units are restricted to have exactly one vector of activity at each time.\n\nA recurrent network can emulate a finite state automaton, but it is exponentially more powerful. With N hidden neurons it has 2^N possible binary activity vectors (but only N^2 weights)\n\nThis is important when the input stream has two separate things going on at once.\nA finite state automaton needs to square its number of states.\nAn RNN needs to double its number of units.\n\n\n\n\n\nFinite State Automaton"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html",
    "href": "notes/dnn/dnn-07/l07e.html",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n::: column-margin"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html#long-short-term-memory-lstm",
    "href": "notes/dnn/dnn-07/l07e.html#long-short-term-memory-lstm",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "Long Short Term Memory (LSTM)",
    "text": "Long Short Term Memory (LSTM)\n\nHochreiter & Schmidhuber (1997) solved the problem of getting an RNN to remember things for a long time (like hundreds of time steps).\nThey designed a memory cell using logistic and linear units with multiplicative interactions.\nInformation gets into the cell whenever its write gate is on.\nThe information stays in the cell so long as its keep gate is on.\nInformation can be read from the cell by turning on its read gate."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html#implementing-a-memory-cell-in-a-neural-network",
    "href": "notes/dnn/dnn-07/l07e.html#implementing-a-memory-cell-in-a-neural-network",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "Implementing a memory cell in a neural network",
    "text": "Implementing a memory cell in a neural network\n\nTo preserve information for a long time in the activities of an RNN, we use a circuit that implements an analog memory cell. - A linear unit that has a self-link with a weight of 1 will maintain its state.\n- Information is stored in the cell by activating its write gate. - Information is retrieved by activating the read gate. - We can backpropagate through this circuit because logistics have nice derivatives."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html#backpropagation-through-a-memory-cell",
    "href": "notes/dnn/dnn-07/l07e.html#backpropagation-through-a-memory-cell",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "Backpropagation through a memory cell",
    "text": "Backpropagation through a memory cell"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07e.html#reading-cursive-handwriting",
    "href": "notes/dnn/dnn-07/l07e.html#reading-cursive-handwriting",
    "title": "Deep Neural Networks - Notes for Lesson 7e",
    "section": "Reading cursive handwriting",
    "text": "Reading cursive handwriting\n\nThis is a natural task for an RNN.\nThe input is a sequence of (x,y,p) coordinates of the tip of the pen, where p indicates whether the pen is up or down.\nThe output is a sequence of characters.\nGraves & Schmidhuber (2009) showed that RNNs with LSTM are currently the best systems for reading cursive writing.\n\nThey used a sequence of small images as input rather than pen coordinates. A demonstration of online handwriting recognition by an RNN with Long Short Term Memory (from Alex Graves)\n\nThe movie that follows shows several things:\nRow 1: This shows when the characters are recognized.\n\nIt never revises its output so difficult decisions are more delayed.\n\nRow 2: This shows the states of a subset of the memory cells.\n\nNotice how they get reset when it recognizes a character.\n\nRow 3: This shows the writing. The net sees the x and y coordinates.\n\nOptical input actually works a bit better than pen coordinates.\n\nRow 4: This shows the gradient backpropagated all the way to the x and y inputs from the currently most active character.\n\nThis lets you see which bits of the data are influencing the decision."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html",
    "href": "notes/dnn/dnn-07/l07d.html",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\n::: column-margin"
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html#the-backward-pass-is-linear",
    "href": "notes/dnn/dnn-07/l07d.html#the-backward-pass-is-linear",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "The backward pass is linear",
    "text": "The backward pass is linear\n - There is a big difference between the forward and backward passes. - In the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding. - The backward pass, is completely linear. If you double the error derivatives at the final layer, all the error derivatives will double. - The forward pass determines the slope of the linear function used for backpropagating through each neuron."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html#the-problem-of-exploding-or-vanishing-gradients",
    "href": "notes/dnn/dnn-07/l07d.html#the-problem-of-exploding-or-vanishing-gradients",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "The problem of exploding or vanishing gradients",
    "text": "The problem of exploding or vanishing gradients\n\nWhat happens to the magnitude of the gradients as we backpropagate through many layers?\n\nIf the weights are small, the gradients shrink exponentially.\nIf the weights are big the gradients grow exponentially.\n\nTypical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.\nIn an RNN trained on long sequences (e.g. 100 time steps) the gradients can easily explode or vanish.\n\nWe can avoid this by initializing the weights very carefully.\n\nEven with good initial weights, its very hard to detect that the current target output depends on an input from many time-steps ago.\n\nSo RNNs have difficulty dealing with long-range dependencies."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html#why-the-back-propagated-gradient-blows-up",
    "href": "notes/dnn/dnn-07/l07d.html#why-the-back-propagated-gradient-blows-up",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "Why the back-propagated gradient blows up",
    "text": "Why the back-propagated gradient blows up\n\n\nIf we start a trajectory within an attractor, small changes in where we start make no difference to where we end up.\nBut if we start almost exactly on the boundary, tiny changes can make a huge difference."
  },
  {
    "objectID": "notes/dnn/dnn-07/l07d.html#four-effective-ways-to-learn-an-rnn",
    "href": "notes/dnn/dnn-07/l07d.html#four-effective-ways-to-learn-an-rnn",
    "title": "Deep Neural Networks - Notes for Lesson 7d",
    "section": "Four effective ways to learn an RNN",
    "text": "Four effective ways to learn an RNN\n\nLong Short Term Memory Make the RNN out of little modules that are designed to remember values for a long time.\nHessian Free Optimization: Deal with the vanishing gradients problem by using a fancy optimizer that can detect directions with a tiny gradient but even smaller curvature.\n\nThe HF optimizer ( Martens & Sutskever, 2011) is good at this.\n\nEcho State Networks: Initialize the inputàhidden and hiddenàhidden and outputàhidden connections very carefully so that the hidden state has a huge reservoir of weakly coupled oscillators which can be selectively driven by the input.\n\nESNs only need to learn the hiddenàoutput connections.\n\nGood initialization with momentum Initialize like in Echo State Networks, but then learn all of the connections using momentum."
  },
  {
    "objectID": "model-thinking.html",
    "href": "model-thinking.html",
    "title": "Oren Bochman’s Blog",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "dnn.html",
    "href": "dnn.html",
    "title": "Oren Bochman’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\nOren Bochman\n\n\nAug 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 1\n\n\nNotes for Deep learning focusing on the basics\n\n\n\nOren Bochman\n\n\nJul 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 10\n\n\nThis module we look at why it helps to combine multiple NN to improve generalization\n\n\n\nOren Bochman\n\n\nOct 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 11\n\n\nThis module deals with Boltzmann machine learning\n\n\n\nOren Bochman\n\n\nOct 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 12\n\n\nThis module deals with Boltzmann machine learning\n\n\n\nOren Bochman\n\n\nNov 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 13\n\n\nThe ups and downs of backpropagation\n\n\n\nOren Bochman\n\n\nNov 11, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 14\n\n\nNotes for Deep learning focusing on the basics\n\n\n\nOren Bochman\n\n\nNov 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 15\n\n\nNotes for Deep learning focusing on the basics\n\n\n\nOren Bochman\n\n\nDec 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 16\n\n\nNotes for Deep learning focusing on the basics\n\n\n\nOren Bochman\n\n\nDec 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 2\n\n\nNotes for Deep learning focusing on Perceptrons\n\n\n\nOren Bochman\n\n\nJul 16, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 3\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\nOren Bochman\n\n\nAug 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 4\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hinton on Coursera\n\n\n\nOren Bochman\n\n\nAug 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 5\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\nOren Bochman\n\n\nAug 17, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 6\n\n\nWe delve into mini-batch gradient descent as well as discuss adaptive learning rates.\n\n\n\nOren Bochman\n\n\nAug 23, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7\n\n\nThis module explores training recurrent neural networks\n\n\n\nOren Bochman\n\n\nSep 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7a\n\n\nModeling sequences — A brief overview\n\n\n\nOren Bochman\n\n\nSep 2, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7b\n\n\nTraining RNNs with back propagation\n\n\n\nOren Bochman\n\n\nSep 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7c\n\n\nA toy example of training an RNN\n\n\n\nOren Bochman\n\n\nSep 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7d\n\n\nWhy it is difficult to train an RNN\n\n\n\nOren Bochman\n\n\nSep 5, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 7e\n\n\nTraining RNNs with back propagation\n\n\n\nOren Bochman\n\n\nSep 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 8\n\n\nWe continue our look at recurrent neural networks\n\n\n\nOren Bochman\n\n\nSep 11, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for Lesson 9\n\n\nWe discuss strategies to make neural networks generalize better\n\n\n\nOren Bochman\n\n\nSep 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1a\n\n\nWhy do we need machine learning?\n\n\n\nOren Bochman\n\n\nJul 2, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1b\n\n\nNotes for Deep learning focusing on why do we need machine learning?\n\n\n\nOren Bochman\n\n\nJul 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1c\n\n\nNotes for Deep learning focusing on simple models of neurons\n\n\n\nOren Bochman\n\n\nJul 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1d\n\n\nNotes for Deep learning focusing on a simple example of learning learning?\n\n\n\nOren Bochman\n\n\nJul 5, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 1e\n\n\nNotes for Deep learning focusing on the three types of learning\n\n\n\nOren Bochman\n\n\nJul 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2a\n\n\nNotes for Deep learning focusing on types of neural network architectures\n\n\n\nOren Bochman\n\n\nJul 17, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2b\n\n\nNotes for Deep learning focusing on Perceptrons, the first generation of neural networks.\n\n\n\nOren Bochman\n\n\nJul 18, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2c\n\n\nNotes for Deep learning focusing on a geometrical view of perceptrons\n\n\n\nOren Bochman\n\n\nJul 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2d\n\n\nNotes for Deep learning focusing on why the learning works?\n\n\n\nOren Bochman\n\n\nJul 20, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 2e\n\n\nNotes for Deep learning focusing What Perceptrons can not do\n\n\n\nOren Bochman\n\n\nJul 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3a\n\n\nFocusing on Learning the weights of a linear neuron\n\n\n\nOren Bochman\n\n\nAug 2, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3b\n\n\nThe error surface for a linear neuron\n\n\n\nOren Bochman\n\n\nAug 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3c\n\n\nLearning the weights of a logistic output neuron\n\n\n\nOren Bochman\n\n\nAug 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3d\n\n\nThe back-propagation algorithm\n\n\n\nOren Bochman\n\n\nAug 5, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 3e\n\n\nUsing the derivatives computed by backpropagation\n\n\n\nOren Bochman\n\n\nAug 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4a\n\n\nLearning to predict the next word\n\n\n\nOren Bochman\n\n\nAug 11, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4b\n\n\nA brief diversion into cognitive science\n\n\n\nOren Bochman\n\n\nAug 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4c\n\n\nAnother diversion — The Softmax output function\n\n\n\nOren Bochman\n\n\nAug 13, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4d\n\n\nNeuro-probabilistic language models\n\n\n\nOren Bochman\n\n\nAug 14, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 4e\n\n\nWays to deal with the large number of possible outputs\n\n\n\nOren Bochman\n\n\nAug 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 5a\n\n\nWhy object recognition is difficult\n\n\n\nOren Bochman\n\n\nAug 18, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 5b\n\n\nWays to achieve viewpoint invariance\n\n\n\nOren Bochman\n\n\nAug 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 5c\n\n\nConvolutional neural networks for hand-written digit recognition\n\n\n\nOren Bochman\n\n\nAug 20, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 5d\n\n\nWhy object recognition is difficult\n\n\n\nOren Bochman\n\n\nAug 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6a\n\n\nOverview of mini-batch gradient descent\n\n\n\nOren Bochman\n\n\nAug 24, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6b\n\n\nA bag of tricks for mini-batch gradient descent\n\n\n\nOren Bochman\n\n\nAug 25, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6c\n\n\nThe momentum method\n\n\n\nOren Bochman\n\n\nAug 26, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6d\n\n\nAdaptive learning rates for each connection\n\n\n\nOren Bochman\n\n\nAug 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes for lecture 6e\n\n\nrmsprop - divide the gradient by a running average of its recent magnitude\n\n\n\nOren Bochman\n\n\nAug 28, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Some Questions\n\n\nUnresolved questions on Deep learning.\n\n\n\nOren Bochman\n\n\nDec 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks — Readings I for Lesson 10\n\n\nReview & summary of — Evaluation of Adaptive Mixtures of Competing Experts\n\n\n\nOren Bochman\n\n\nOct 2, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks — Readings II for Lesson 10\n\n\nReview & summary of — Improving neural networks by preventing co-adaptation of feature detectors\n\n\n\nOren Bochman\n\n\nOct 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nGlossary of terms for Deep Neural Networks\n\n\nGlossary of terms in Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\nOren Bochman\n\n\nAug 6, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  }
]