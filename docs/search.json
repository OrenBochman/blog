[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "All Notes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\ntikz in Quarto!\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nRL MindMap\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nRoth Erev learning in Lewis signaling games\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nlogic puzzles\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nreplay buffer questions\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nLLM the good the bad and the ugly\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMisbehaviour of Markets and Scaling in financial prices 1-4\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 1\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 2\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 3\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 4\n\n\n\n\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe roles of Partial pooling and mixed strategies in the Lewis signaling game\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, March 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEmergent Languages - A Desiderata\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, January 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPlanning in the Complex Lewis Game\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, January 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Referential Lewis Signaling Game\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, January 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA garden of forking paths\n\n\n\n\n\n\nOren Bochman\n\n\nSaturday, January 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Notes\n\n\n\n\n\n\nOren Bochman\n\n\nMonday, January 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Many Path To A Signaling System\n\n\n\n\n\n\nOren Bochman\n\n\nSunday, January 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOff-Policy Learning\n\n\n\n\n\n\nOren Bochman\n\n\nSaturday, January 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRethinking Signaling systems via the lens of compositionality\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, January 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLewis Signaling Game for PettingZoo\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinguistic generalization and compositionality in modern artificial neural networks\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompositionality and Generalization in Emergent Languages\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVitter Algorithm\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBooks, Courses Tools\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVilleny pure and simple\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, December 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompositionality in Lewis signaling games and MARL transfer learning.\n\n\n\n\n\n\nOren Bochman\n\n\nMonday, October 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTL-DR rethinking üí≠ topological alignment\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, October 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLLM and the missing link\n\n\n\n\n\n\nOren Bochman\n\n\nSaturday, September 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNLP with RL\n\n\n\n\n\n\nOren Bochman\n\n\nFriday, September 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDeduction Evaluation\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, September 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tune llm for Style and Grammar advice.\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, September 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIs compositionality overrated? The view from language emergence\n\n\n\n\n\n\nOren Bochman\n\n\nSunday, September 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSix quick tips to improve modeling\n\n\n\n\n\n\nOren Bochman\n\n\nMonday, August 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStumpy\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, August 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\ntwo ideas on generelization\n\n\n\n\n\n\nOren Bochman\n\n\nMonday, July 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nmesa & rl\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, June 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nzero inflated data\n\n\n\n\n\n\nOren Bochman\n\n\nSunday, June 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nreadings in rl\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, June 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Optimization\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, June 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-agent Reinforcement Learning in Sequential Social Dilemmas\n\n\n\n\n\n\nOren Bochman\n\n\nMonday, June 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerally Capable Agents Emerge from Open-Ended Play\n\n\n\n\n\n\nOren Bochman\n\n\nMonday, June 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSemantic Kernel\n\n\n\n\n\n\nOren Bochman\n\n\nSaturday, June 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEvolutionary Games and Population Dynamics Summary\n\n\n\n\n\n\nOren Bochman\n\n\nSunday, May 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSignals Experiment\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, May 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nad hoc complex signaling systems\n\n\nA deep dive into the complex signaling systems\n\n\n\nOren Bochman\n\n\nSunday, May 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nShannon Game\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, May 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUrn models using Numpy\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, May 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRAD REPL\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, May 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSkryms Signals Summary and Models\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, May 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMesa Lessons\n\n\n\n\n\n\nOren Bochman\n\n\nSunday, March 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOCR building blocks\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, March 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA definition by Patrick Henry Winston\n\n\nPatrick Henry Winston provides a definition of AI. How he keeps expanding his definition as he goes along is thing of beauty. It expands organicaly to incorporate new ideas‚Ä¶\n\n\n\nOren Bochman\n\n\nSunday, March 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOCR - Brain Dump\n\n\n\n\n\n\nOren Bochman\n\n\nSunday, February 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRhetoric NLP Tasks\n\n\n\n\n\n\nOren Bochman\n\n\nSaturday, February 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nüòÅ Quarto üíñ Mermaidüßú Mindmaps üß†\n\n\nQuarto at last supports Mindmap charts using Mermaid charts.\n\n\n\nOren Bochman\n\n\nMonday, February 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLewis Game from a Bayesian Perspective\n\n\n\n\n\n\nOren Bochman\n\n\nMonday, February 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Great Migration\n\n\nsome migration notes from Blooger to Jekyl to Quarto blog.\n\n\n\nOren Bochman\n\n\nTuesday, January 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nOren Bochman\n\n\nSunday, January 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSuperLearner\n\n\nSuperLearner is an ensambeleing library.\n\n\n\nOren Bochman\n\n\nWednesday, January 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEngineering Reinforcement Learning Algorithms\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, January 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nD3.js in in Quarto Observable\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, January 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nAutogluon is a auto-ml framework, here are three cheetsheet for accellerating data science workloads\n\n\n\nOren Bochman\n\n\nWednesday, December 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSummary: Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization\n\n\nThis paper was referenced by Drew Bagnell in the Coursera RL specilization for using simple quadratic approximation to learn a model in a continous control setting. The‚Ä¶\n\n\n\nOren Bochman\n\n\nThursday, June 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSpark Tips\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, June 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC algorithms\n\n\n\n\n\n\nOren Bochman\n\n\nSaturday, April 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto loves pseudocode\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, April 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nText2topic Leverage reviews data for multi-label topics classification in Booking.com\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, February 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nValidating NLP data and models\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, February 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTransformations in Linguistic Representation\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, February 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nevent generator\n\n\nfake data\n\n\n\nOren Bochman\n\n\nThursday, February 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOLS regression From Scratch\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, February 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nentropy for uncertainty quantification\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, September 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWikisym 2012\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, July 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSet Up M1 MacBooks for DS & ML\n\n\nSet Up M1 MacBooks for DS & ML\n\n\n\nOren Bochman\n\n\nThursday, May 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ncommand line\n\n\ncommand line cheea sheet macos + zsh + git\n\n\n\nOren Bochman\n\n\nThursday, May 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeme bank\n\n\nA meme is an idea, behavior, style, or usage that spreads from person to person within a culture. A meme bank would be a zoo for cataloging and breeding memes.\n\n\n\nOren Bochman\n\n\nThursday, December 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcel 2019 for Marketing Statistics in pandas\n\n\nExcel 2019 for Marketing Statistics in pandas\n\n\n\nOren Bochman\n\n\nFriday, September 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage models and explainability\n\n\nLanguage models and explainability\n\n\n\nOren Bochman\n\n\nFriday, September 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttention for sensor fusion\n\n\nAttention for sensor fusion\n\n\n\nOren Bochman\n\n\nFriday, September 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorytelling and other essentials\n\n\nStorytelling and other essentials,\n\n\n\nOren Bochman\n\n\nThursday, September 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Gradient Descent - The good parts\n\n\nStochastic Gradient Descent - The good parts\n\n\n\nOren Bochman\n\n\nSunday, August 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWaveNet Review\n\n\nThe WaveNet paper is kind of old. Yet it seems to come up in various contexts. Some thoughts on this.\n\n\n\nOren Bochman\n\n\nSunday, August 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Graphs\n\n\nPython Graph Cookbook\n\n\n\nOren Bochman\n\n\nSunday, August 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is in a citation?\n\n\nCreating Citation Web Components\n\n\n\nOren Bochman\n\n\nSunday, August 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHackathon session link dumps & notes\n\n\nWikipedia Hackathon notes\n\n\n\nOren Bochman\n\n\nFriday, August 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInlining Citations for Wikipedia articles\n\n\nAn algorithm for Inlining Citations for Wikipedia articles.\n\n\n\nOren Bochman\n\n\nFriday, August 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer learning in NLP\n\n\nTransfer learning in NLP\n\n\n\nOren Bochman\n\n\nFriday, August 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA type of Witness and an evolving Idiom\n\n\nwriting better code = writing more readable code.\n\n\n\nOren Bochman\n\n\nWednesday, July 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\njson-ld\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, July 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow probability\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, June 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEbook Hacks\n\n\n\n\n\n\nOren Bochman\n\n\nSaturday, May 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Models\n\n\nDifferent Multilevel Models Types\n\n\n\nOren Bochman\n\n\nSunday, May 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A and the Winograd schemas\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, April 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Summarization Task\n\n\nConcepts, slide commentaries and Lecture notes on Automatic text Summarization by Masa Nekic\n\n\n\nOren Bochman\n\n\nSaturday, April 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian agents\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, April 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Events\n\n\nModeling Events.\n\n\n\nOren Bochman\n\n\nFriday, April 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Models Are Open Knowledge Graphs\n\n\nA review of the paper ‚ÄúLanguage Models are Open Knowledge Graphs‚Äù by Chenguang Wang, Xiao Liu, Dawn Song arXiv:2010.11967\n\n\n\nOren Bochman\n\n\nWednesday, April 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinkage 2021-04-07\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, April 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 Tips To Improve Your Workflow\n\n\nHow to blog like a life-hacker.\n\n\n\nOren Bochman\n\n\nWednesday, April 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJekyll take 3\n\n\nMy attempts to get the jekyll version of this site to also build locally.\n\n\n\nOren Bochman\n\n\nSunday, April 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMathJax 3 fix for Jekyll hosted on Github pages\n\n\nIssues and workarounds for MatchJax 3.0.\n\n\n\nOren Bochman\n\n\nSunday, April 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Approaches to Attention-based NMT\n\n\nReview of the paper on dot product attention for the deeplearning.ai NLP specialization.\n\n\n\nOren Bochman\n\n\nSunday, March 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy melt down\n\n\nnumpy melt down.\n\n\n\nOren Bochman\n\n\nSunday, November 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Intuitions\n\n\n\n\n\n\nOren Bochman\n\n\nSunday, October 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nbrace expansion\n\n\n\n\n\n\nOren Bochman\n\n\nFriday, June 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPandas Productivity Challenge?\n\n\nJust a little rant on Pandas various contexts\n\n\n\nOren Bochman\n\n\nWednesday, March 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to avoid cross site scripting (XSS) errors with the Jupyter local runtime for Colab\n\n\n\n\n\n\nOren Bochman\n\n\nThursday, February 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker for data science\n\n\nPost description\n\n\n\nOren Bochman\n\n\nSunday, November 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nExploding and vanishing nodes.\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, July 31, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext annotation with BRAT\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, January 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nA/B testing cost and risks?\n\n\nA/B testing cost and risks and some recommendation.\n\n\n\nOren Bochman\n\n\nSunday, July 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nTravel checklist\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, December 14, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nHotJar Heat Map Analysis - Dr.¬†David Darmanin\n\n\n\n\n\n\nOren Bochman\n\n\nWednesday, April 20, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Competitive Analysis to Benchmark Your Marketing Efforts Ariel Rosenstein - Similar Web\n\n\n\n\n\n\nOren Bochman\n\n\nMonday, April 20, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Competitive Analysis to Benchmark Your Marketing Efforts - Ariel Rosenstein - Similar Web\n\n\n\n\n\n\nOren Bochman\n\n\nMonday, April 20, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytics Checklist\n\n\n\n\n\n\nOren Bochman\n\n\nSaturday, February 7, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nlife hacks\n\n\n\n\n\n\nOren Bochman\n\n\nFriday, June 7, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With Python\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, November 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With R\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, November 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\n\n\n\n\nOren Bochman\n\n\nTuesday, November 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime management Tips\n\n\nEffective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a‚Ä¶\n\n\n\nOren Bochman\n\n\nThursday, August 11, 2011\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home",
      "Blog"
    ]
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "tikz in Quarto!\n\n\n\n\n\n\n\n\n\n\n\n\n\nRL MindMap\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoth Erev learning in Lewis signaling games\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogic puzzles\n\n\n\n\n\n\n\n\n\n\n\n\n\nreplay buffer questions\n\n\n\n\n\n\n\n\n\n\n\n\n\nLLM the good the bad and the ugly\n\n\nAn essay on the limitations of language models\n\n\n\n\n\n\n\n\n\n\nMisbehaviour of Markets and Scaling in financial prices 1-4\n\n\nFour papers and a book\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 1\n\n\nTails and dependence\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 2\n\n\nMultifractals and the star equation\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 3\n\n\nCartoon Brownian Motions in Multifractal Time\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 4\n\n\nMultifractal Concentration\n\n\n\n\n\n\n\n\n\n\nThe roles of Partial pooling and mixed strategies in the Lewis signaling game\n\n\na game theoretic perspective\n\n\n\n\n\nTuesday, March 11, 2025\n\n\n\n\n\n\n\nEmergent Languages - A Desiderata\n\n\n\n\n\n\n\n\nTuesday, January 14, 2025\n\n\n\n\n\n\n\nPlanning in the Complex Lewis Game\n\n\n\n\n\n\n\n\nTuesday, January 14, 2025\n\n\n\n\n\n\n\nThe Referential Lewis Signaling Game\n\n\nBack of the napkin complexity calculations\n\n\n\n\n\nTuesday, January 14, 2025\n\n\n\n\n\n\n\nA garden of forking paths\n\n\n\n\n\n\n\n\nSaturday, January 11, 2025\n\n\n\n\n\n\n\nResearch Notes\n\n\n\n\n\n\n\n\nMonday, January 6, 2025\n\n\n\n\n\n\n\nThe Many Path To A Signaling System\n\n\n\n\n\n\n\n\nSunday, January 5, 2025\n\n\n\n\n\n\n\nOff-Policy Learning\n\n\nFor Dummies\n\n\n\n\n\nSaturday, January 4, 2025\n\n\n\n\n\n\n\nRethinking Signaling systems via the lens of compositionality\n\n\n\n\n\n\n\n\nThursday, January 2, 2025\n\n\n\n\n\n\n\nLewis Signaling Game for PettingZoo\n\n\nPaper Review\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\nLinguistic generalization and compositionality in modern artificial neural networks\n\n\nPaper Review\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\nCompositionality and Generalization in Emergent Languages\n\n\nPaper Review\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\nEmergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input\n\n\nPaper Review\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\nVitter Algorithm\n\n\n\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\nBooks, Courses Tools\n\n\n\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n\n\n\n\n\nVilleny pure and simple\n\n\nScar face Capone in the valentine‚Äôs day massacre\n\n\n\n\n\nThursday, December 12, 2024\n\n\n\n\n\n\n\nCompositionality in Lewis signaling games and MARL transfer learning.\n\n\n\n\n\n\n\n\nMonday, October 14, 2024\n\n\n\n\n\n\n\nTL-DR rethinking üí≠ topological alignment\n\n\n\n\n\n\n\n\nTuesday, October 1, 2024\n\n\n\n\n\n\n\nLLM and the missing link\n\n\n\n\n\n\n\n\nSaturday, September 28, 2024\n\n\n\n\n\n\n\nNLP with RL\n\n\n\n\n\n\n\n\nFriday, September 27, 2024\n\n\n\n\n\n\n\nDeduction Evaluation\n\n\n\n\n\n\n\n\nThursday, September 26, 2024\n\n\n\n\n\n\n\nFine-tune llm for Style and Grammar advice.\n\n\n\n\n\n\n\n\nWednesday, September 25, 2024\n\n\n\n\n\n\n\nIs compositionality overrated? The view from language emergence\n\n\n\n\n\n\n\n\nSunday, September 1, 2024\n\n\n\n\n\n\n\nSix quick tips to improve modeling\n\n\n\n\n\n\n\n\nMonday, August 26, 2024\n\n\n\n\n\n\n\nStumpy\n\n\nTimes Series Analysis\n\n\n\n\n\nThursday, August 8, 2024\n\n\n\n\n\n\n\ntwo ideas on generelization\n\n\n\n\n\n\n\n\nMonday, July 1, 2024\n\n\n\n\n\n\n\nmesa & rl\n\n\n\n\n\n\n\n\nTuesday, June 25, 2024\n\n\n\n\n\n\n\nzero inflated data\n\n\n\n\n\n\n\n\nSunday, June 23, 2024\n\n\n\n\n\n\n\nreadings in rl\n\n\n\n\n\n\n\n\nTuesday, June 18, 2024\n\n\n\n\n\n\n\nHyperparameter Optimization\n\n\n\n\n\n\n\n\nThursday, June 13, 2024\n\n\n\n\n\n\n\nMulti-agent Reinforcement Learning in Sequential Social Dilemmas\n\n\npaper review\n\n\n\n\n\nMonday, June 10, 2024\n\n\n\n\n\n\n\nGenerally Capable Agents Emerge from Open-Ended Play\n\n\npaper review\n\n\n\n\n\nMonday, June 10, 2024\n\n\n\n\n\n\n\nSemantic Kernel\n\n\n\n\n\n\n\n\nSaturday, June 8, 2024\n\n\n\n\n\n\n\nEvolutionary Games and Population Dynamics Summary\n\n\n\n\n\n\n\n\nSunday, May 12, 2024\n\n\n\n\n\n\n\nSignals Experiment\n\n\nLeaning language games\n\n\n\n\n\nTuesday, May 7, 2024\n\n\n\n\n\n\n\nad hoc complex signaling systems\n\n\na review and proposal\n\n\n\n\n\nSunday, May 5, 2024\n\n\n\n\n\n\n\nShannon Game\n\n\nemergent complex communications protocols\n\n\n\n\n\nThursday, May 2, 2024\n\n\n\n\n\n\n\nUrn models using Numpy\n\n\n\n\n\n\n\n\nThursday, May 2, 2024\n\n\n\n\n\n\n\nRAD REPL\n\n\n\n\n\n\n\n\nWednesday, May 1, 2024\n\n\n\n\n\n\n\nSkryms Signals Summary and Models\n\n\nlearing language games\n\n\n\n\n\nWednesday, May 1, 2024\n\n\n\n\n\n\n\nMesa Lessons\n\n\n\n\n\n\n\n\nSunday, March 31, 2024\n\n\n\n\n\n\n\nOCR building blocks\n\n\n\n\n\n\n\n\nThursday, March 28, 2024\n\n\n\n\n\n\n\nA definition by Patrick Henry Winston\n\n\nFor Artificial Intelligence\n\n\n\n\n\nSunday, March 3, 2024\n\n\n\n\n\n\n\nOCR - Brain Dump\n\n\n\n\n\n\n\n\nSunday, February 25, 2024\n\n\n\n\n\n\n\nRhetoric NLP Tasks\n\n\n\n\n\n\n\n\nSaturday, February 17, 2024\n\n\n\n\n\n\n\nüòÅ Quarto üíñ Mermaidüßú Mindmaps üß†\n\n\n\n\n\n\n\n\nMonday, February 12, 2024\n\n\n\n\n\n\n\nLewis Game from a Bayesian Perspective\n\n\n\n\n\n\n\n\nMonday, February 12, 2024\n\n\n\n\n\n\n\nThe Great Migration\n\n\nFrom Blogger to Jekyl and finaly to Quarto.\n\n\n\n\n\nTuesday, January 30, 2024\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nSunday, January 28, 2024\n\n\n\n\n\n\n\nSuperLearner\n\n\n\n\n\n\n\n\nWednesday, January 10, 2024\n\n\n\n\n\n\n\nEngineering Reinforcement Learning Algorithms\n\n\n\n\n\n\n\n\nWednesday, January 10, 2024\n\n\n\n\n\n\n\nD3.js in in Quarto Observable\n\n\n\n\n\n\n\n\nTuesday, January 2, 2024\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nBeacuase auto-ml is a Superpower\n\n\n\n\n\nWednesday, December 20, 2023\n\n\n\n\n\n\n\nSummary: Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization\n\n\n\n\n\n\n\n\nThursday, June 1, 2023\n\n\n\n\n\n\n\nSpark Tips\n\n\n\n\n\n\n\n\nThursday, June 1, 2023\n\n\n\n\n\n\n\nMCMC algorithms\n\n\n\n\n\n\n\n\nSaturday, April 22, 2023\n\n\n\n\n\n\n\nQuarto loves pseudocode\n\n\n\n\n\n\n\n\nTuesday, April 11, 2023\n\n\n\n\n\n\n\nText2topic Leverage reviews data for multi-label topics classification in Booking.com\n\n\nNLP.IL\n\n\n\n\n\nTuesday, February 28, 2023\n\n\n\n\n\n\n\nValidating NLP data and models\n\n\nNLP.IL\n\n\n\n\n\nTuesday, February 28, 2023\n\n\n\n\n\n\n\nTransformations in Linguistic Representation\n\n\n\n\n\n\n\n\nWednesday, February 22, 2023\n\n\n\n\n\n\n\nevent generator\n\n\n\n\n\n\n\n\nThursday, February 16, 2023\n\n\n\n\n\n\n\nOLS regression From Scratch\n\n\n\n\n\n\n\n\nWednesday, February 1, 2023\n\n\n\n\n\n\n\nentropy for uncertainty quantification\n\n\n\n\n\n\n\n\nThursday, September 22, 2022\n\n\n\n\n\n\n\nWikisym 2012\n\n\nConference Report\n\n\n\n\n\nTuesday, July 26, 2022\n\n\n\n\n\n\n\nSet Up M1 MacBooks for DS & ML\n\n\n\n\n\n\n\n\nThursday, May 5, 2022\n\n\n\n\n\n\n\ncommand line\n\n\n\n\n\n\n\n\nThursday, May 5, 2022\n\n\n\n\n\n\n\nMeme bank\n\n\nmemes a problem-solving approach\n\n\n\n\n\nThursday, December 30, 2021\n\n\n\n\n\n\n\nExcel 2019 for Marketing Statistics in pandas\n\n\n\n\n\n\n\n\nFriday, September 24, 2021\n\n\n\n\n\n\n\nLanguage models and explainability\n\n\n\n\n\n\n\n\nFriday, September 24, 2021\n\n\n\n\n\n\n\nAttention for sensor fusion\n\n\n\n\n\n\n\n\nFriday, September 24, 2021\n\n\n\n\n\n\n\nStorytelling and other essentials\n\n\n\n\n\n\n\n\nThursday, September 2, 2021\n\n\n\n\n\n\n\nStochastic Gradient Descent - The good parts\n\n\n\n\n\n\n\n\nSunday, August 29, 2021\n\n\n\n\n\n\n\nWaveNet Review\n\n\n\n\n\n\n\n\nSunday, August 29, 2021\n\n\n\n\n\n\n\nPython Graphs\n\n\n\n\n\n\n\n\nSunday, August 29, 2021\n\n\n\n\n\n\n\nWhat is in a citation?\n\n\n\n\n\n\n\n\nSunday, August 29, 2021\n\n\n\n\n\n\n\nHackathon session link dumps & notes\n\n\n\n\n\n\n\n\nFriday, August 13, 2021\n\n\n\n\n\n\n\nInlining Citations for Wikipedia articles\n\n\n\n\n\n\n\n\nFriday, August 13, 2021\n\n\n\n\n\n\n\nTransfer learning in NLP\n\n\n\n\n\n\n\n\nFriday, August 13, 2021\n\n\n\n\n\n\n\nA type of Witness and an evolving Idiom\n\n\n\n\n\n\n\n\nWednesday, July 14, 2021\n\n\n\n\n\n\n\njson-ld\n\n\nmetadata format for linked data\n\n\n\n\n\nThursday, July 1, 2021\n\n\n\n\n\n\n\nTensorFlow probability\n\n\n\n\n\n\n\n\nTuesday, June 1, 2021\n\n\n\n\n\n\n\nEbook Hacks\n\n\n\n\n\n\n\n\nSaturday, May 29, 2021\n\n\n\n\n\n\n\nMultilevel Models\n\n\n\n\n\n\n\n\nSunday, May 16, 2021\n\n\n\n\n\n\n\nQ&A and the Winograd schemas\n\n\n\n\n\n\n\n\nTuesday, April 27, 2021\n\n\n\n\n\n\n\nAutomatic Summarization Task\n\n\n\n\n\n\n\n\nSaturday, April 24, 2021\n\n\n\n\n\n\n\nBayesian agents\n\n\n\n\n\n\n\n\nWednesday, April 14, 2021\n\n\n\n\n\n\n\nModeling Events\n\n\n\n\n\n\n\n\nFriday, April 9, 2021\n\n\n\n\n\n\n\nLanguage Models Are Open Knowledge Graphs\n\n\npaper review\n\n\n\n\n\nWednesday, April 7, 2021\n\n\n\n\n\n\n\nLinkage 2021-04-07\n\n\n\n\n\n\n\n\nWednesday, April 7, 2021\n\n\n\n\n\n\n\n10 Tips To Improve Your Workflow\n\n\n\n\n\n\n\n\nWednesday, April 7, 2021\n\n\n\n\n\n\n\nJekyll take 3\n\n\nGetting\n\n\n\n\n\nSunday, April 4, 2021\n\n\n\n\n\n\n\nMathJax 3 fix for Jekyll hosted on Github pages\n\n\n\n\n\n\n\n\nSunday, April 4, 2021\n\n\n\n\n\n\n\nEffective Approaches to Attention-based NMT\n\n\nPaper review for the deeplearning.ai NLP specialization\n\n\n\n\n\nSunday, March 21, 2021\n\n\n\n\n\n\n\nnumpy melt down\n\n\n\n\n\n\n\n\nSunday, November 29, 2020\n\n\n\n\n\n\n\nDeep Learning Intuitions\n\n\n\n\n\n\n\n\nSunday, October 25, 2020\n\n\n\n\n\n\n\nbrace expansion\n\n\n\n\n\n\n\n\nFriday, June 12, 2020\n\n\n\n\n\n\n\nPandas Productivity Challenge?\n\n\n\n\n\n\n\n\nWednesday, March 4, 2020\n\n\n\n\n\n\n\nHow to avoid cross site scripting (XSS) errors with the Jupyter local runtime for Colab\n\n\n\n\n\n\n\n\nThursday, February 20, 2020\n\n\n\n\n\n\n\nDocker for data science\n\n\n\n\n\n\n\n\nSunday, November 24, 2019\n\n\n\n\n\n\n\nExploding and vanishing nodes.\n\n\n\n\n\n\n\n\nWednesday, July 31, 2019\n\n\n\n\n\n\n\ntext annotation with BRAT\n\n\n\n\n\n\n\n\nTuesday, January 16, 2018\n\n\n\n\n\n\n\nA/B testing cost and risks?\n\n\n\n\n\n\n\n\nSunday, July 30, 2017\n\n\n\n\n\n\n\nTravel checklist\n\n\n\n\n\n\n\n\nWednesday, December 14, 2016\n\n\n\n\n\n\n\nHotJar Heat Map Analysis - Dr.¬†David Darmanin\n\n\nAll things data 2015\n\n\n\n\n\nWednesday, April 20, 2016\n\n\n\n\n\n\n\nUsing Competitive Analysis to Benchmark Your Marketing Efforts Ariel Rosenstein - Similar Web\n\n\nAll things data 2015\n\n\n\n\n\nMonday, April 20, 2015\n\n\n\n\n\n\n\nUsing Competitive Analysis to Benchmark Your Marketing Efforts - Ariel Rosenstein - Similar Web\n\n\nAll things data 2015\n\n\n\n\n\nMonday, April 20, 2015\n\n\n\n\n\n\n\nAnalytics Checklist\n\n\n\n\n\n\n\n\nSaturday, February 7, 2015\n\n\n\n\n\n\n\nlife hacks\n\n\n\n\n\n\n\n\nFriday, June 7, 2013\n\n\n\n\n\n\n\nText Mining With Python\n\n\na number of NLP tasks in Python\n\n\n\n\n\nTuesday, November 29, 2011\n\n\n\n\n\n\n\nText Mining With R\n\n\na number of NLP tasks in R\n\n\n\n\n\nTuesday, November 29, 2011\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\nan update on NLP with R\n\n\n\n\n\nTuesday, November 29, 2011\n\n\n\n\n\n\n\nTime management Tips\n\n\nWe could all use a productivity boost\n\n\n\n\n\nThursday, August 11, 2011\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oren Bochman‚Äôs Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLLM the good the bad and the ugly\n\n\nAn essay on the limitations of language models\n\n\n\n\n\n\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nMisbehaviour of Markets and Scaling in financial prices 1-4\n\n\nFour papers and a book\n\n\n\nScaling Laws\n\n\nFractals\n\n\nFinancial Markets\n\n\nTime series\n\n\nReviews\n\n\nPopular Science\n\n\n\n\n\n\n\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nRL MindMap\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nRoth Erev learning in Lewis signaling games\n\n\n\n\n\n\n\n\n\n\n\n44 min\n\n\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 1\n\n\nTails and dependence\n\n\n\nScaling Laws\n\n\nFractals\n\n\nFinancial Markets\n\n\nTime series\n\n\nReviews\n\n\nPopular Science\n\n\n\n\n\n\n\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 2\n\n\nMultifractals and the star equation\n\n\n\nScaling Laws\n\n\nFractals\n\n\nFinancial Markets\n\n\nTime series\n\n\nReviews\n\n\nPopular Science\n\n\n\n\n\n\n\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 3\n\n\nCartoon Brownian Motions in Multifractal Time\n\n\n\nScaling Laws\n\n\nFractals\n\n\nFinancial Markets\n\n\nTime series\n\n\nReviews\n\n\n\n\n\n\n\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nScaling in financial prices 4\n\n\nMultifractal Concentration\n\n\n\nScaling Laws\n\n\nFractals\n\n\nFinancial Markets\n\n\nTime series\n\n\nReviews\n\n\n\n\n\n\n\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nlogic puzzles\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nreplay buffer questions\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\ntikz in Quarto!\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe roles of Partial pooling and mixed strategies in the Lewis signaling game\n\n\na game theoretic perspective\n\n\n\nemergent languages\n\n\nlewis signaling game\n\n\ngame theory\n\n\n\n\n\n\n\n\n\nTuesday, March 11, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nEmergent Languages - A Desiderata\n\n\n\n\n\n\nemergent languages\n\n\nreinforcement learning\n\n\ninformation theory\n\n\nlinguistics\n\n\n\n\n\n\n\n\n\nTuesday, January 14, 2025\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nPlanning in the Complex Lewis Game\n\n\n\n\n\n\ncompositionality\n\n\nemergent languages\n\n\nreinforcement learning\n\n\ntransfer learning\n\n\ninformation theory\n\n\nlinguistics\n\n\n\n\n\n\n\n\n\nTuesday, January 14, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Referential Lewis Signaling Game\n\n\nBack of the napkin complexity calculations\n\n\n\nreinforcement learning\n\n\n\n\n\n\n\n\n\nTuesday, January 14, 2025\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nA garden of forking paths\n\n\n\n\n\n\n\n\n\n\n\nSaturday, January 11, 2025\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Notes\n\n\n\n\n\n\n\n\n\n\n\nMonday, January 6, 2025\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Many Path To A Signaling System\n\n\n\n\n\n\nsignaling systems\n\n\nlewis signaling game\n\n\nreinforcement learning\n\n\nbayesian games\n\n\ninformation theory\n\n\ngame theory\n\n\nbayesian reinforcement learning\n\n\nemergent languages\n\n\n\n\n\n\n\n\n\nSunday, January 5, 2025\n\n\n46 min\n\n\n\n\n\n\n\n\n\n\n\n\nOff-Policy Learning\n\n\nFor Dummies\n\n\n\n\n\n\n\n\nSaturday, January 4, 2025\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nRethinking Signaling systems via the lens of compositionality\n\n\n\n\n\n\nrl\n\n\nreinforcement learning\n\n\n\n\n\n\n\n\n\nThursday, January 2, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nBooks, Courses Tools\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nCompositionality and Generalization in Emergent Languages\n\n\nPaper Review\n\n\n\nreview\n\n\ncompositionality\n\n\nneural networks\n\n\nsignaling systems\n\n\nlanguage evolution\n\n\n\n\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nEmergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input\n\n\nPaper Review\n\n\n\nreview\n\n\ncompositionality\n\n\nneural networks\n\n\nsignaling systems\n\n\nlanguage evolution\n\n\n\n\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nLewis Signaling Game for PettingZoo\n\n\nPaper Review\n\n\n\nreview\n\n\ncompositionality\n\n\nneural networks\n\n\nsignaling systems\n\n\nlanguage evolution\n\n\n\n\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nLinguistic generalization and compositionality in modern artificial neural networks\n\n\nPaper Review\n\n\n\nreview, compositionality neural networks signaling systems language evolution\n\n\n\n\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nVitter Algorithm\n\n\n\n\n\n\nreview compositionality neural networks signaling systems language evolution\n\n\n\n\n\n\n\n\n\nWednesday, January 1, 2025\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nVilleny pure and simple\n\n\nScar face Capone in the valentine‚Äôs day massacre\n\n\n\nscreenwriting\n\n\npatterns\n\n\nnoir\n\n\n\n\n\n\n\n\n\nThursday, December 12, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nCompositionality in Lewis signaling games and MARL transfer learning.\n\n\n\n\n\n\ncompositionality\n\n\nemergent languages\n\n\nreinforcement learning\n\n\ntransfer learning\n\n\ninformation theory\n\n\nlinguistics\n\n\n\n\n\n\n\n\n\nMonday, October 14, 2024\n\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\n\nTL-DR rethinking üí≠ topological alignment\n\n\n\n\n\n\nNLP\n\n\n\n\n\n\n\n\n\nTuesday, October 1, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nLLM and the missing link\n\n\n\n\n\n\nwikipedia\n\n\nLLM\n\n\nAI\n\n\nagents\n\n\nwikification\n\n\nreadability\n\n\nwikidata\n\n\nvandalism\n\n\nspam\n\n\ncitations\n\n\nreferences\n\n\nsections\n\n\nbiases\n\n\nCOI\n\n\nmedia\n\n\n\n\n\n\n\n\n\nSaturday, September 28, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nNLP with RL\n\n\n\n\n\n\n\n\n\n\n\nFriday, September 27, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nDeduction Evaluation\n\n\n\n\n\n\nlogic\n\n\nreasoning\n\n\ndeduction\n\n\n\n\n\n\n\n\n\nThursday, September 26, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tune llm for Style and Grammar advice.\n\n\n\n\n\n\nNLP\n\n\n\n\n\n\n\n\n\nWednesday, September 25, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nIs compositionality overrated? The view from language emergence\n\n\n\n\n\n\ndraft\n\n\nreview\n\n\ncompositionality\n\n\nneural networks\n\n\nsignaling systems\n\n\nlanguage evolution\n\n\n\n\n\n\n\n\n\nSunday, September 1, 2024\n\n\n29 min\n\n\n\n\n\n\n\n\n\n\n\n\nSix quick tips to improve modeling\n\n\n\n\n\n\n\n\n\n\n\nMonday, August 26, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nStumpy\n\n\nTimes Series Analysis\n\n\n\n\n\n\n\n\nThursday, August 8, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\ntwo ideas on generelization\n\n\n\n\n\n\n\n\n\n\n\nMonday, July 1, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nmesa & rl\n\n\n\n\n\n\n\n\n\n\n\nTuesday, June 25, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nzero inflated data\n\n\n\n\n\n\n\n\n\n\n\nSunday, June 23, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nreadings in rl\n\n\n\n\n\n\nrl\n\n\nreinforcement learning\n\n\npapers\n\n\nnotes\n\n\nreading\n\n\nrl-papers\n\n\nrl-algorithms\n\n\nrl-resources\n\n\n\n\n\n\n\n\n\nTuesday, June 18, 2024\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Optimization\n\n\n\n\n\n\n\n\n\n\n\nThursday, June 13, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nGenerally Capable Agents Emerge from Open-Ended Play\n\n\npaper review\n\n\n\npaper review\n\n\nmulti-agent reinforcement learning\n\n\nsequential social dilemmas\n\n\nsequential social dilemmas\n\n\ncooperation\n\n\nMarkov games\n\n\nagent-based social simulation\n\n\nnon-cooperative games\n\n\n\n\n\n\n\n\n\nMonday, June 10, 2024\n\n\n27 min\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-agent Reinforcement Learning in Sequential Social Dilemmas\n\n\npaper review\n\n\n\npaper review\n\n\nmulti-agent reinforcement learning\n\n\nsequential social dilemmas\n\n\nsequential social dilemmas\n\n\ncooperation\n\n\nMarkov games\n\n\nagent-based social simulation\n\n\nnon-cooperative games\n\n\n\n\n\n\n\n\n\nMonday, June 10, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nSemantic Kernel\n\n\n\n\n\n\nai\n\n\nnlp\n\n\n\n\n\n\n\n\n\nSaturday, June 8, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nEvolutionary Games and Population Dynamics Summary\n\n\n\n\n\n\nmathematics\n\n\nevolutionary games\n\n\npopulation dynamics\n\n\nLotka-Volterra\n\n\ndynamical systems\n\n\nlogistic growth\n\n\npredator-prey model\n\n\n\n\n\n\n\n\n\nSunday, May 12, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSignals Experiment\n\n\nLeaning language games\n\n\n\n\n\n\n\n\nTuesday, May 7, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nad hoc complex signaling systems\n\n\na review and proposal\n\n\n\nsignaling games\n\n\nemergent languages\n\n\n\nA deep dive into the complex signaling systems\n\n\n\n\n\nSunday, May 5, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nShannon Game\n\n\nemergent complex communications protocols\n\n\n\nsignaling games\n\n\ncomplex signaling systems\n\n\ncompositionality\n\n\ncommunication protocols\n\n\nemergent languages\n\n\n\n\n\n\n\n\n\nThursday, May 2, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nUrn models using Numpy\n\n\n\n\n\n\nprobability\n\n\nurn models\n\n\nemergent languages\n\n\n\n\n\n\n\n\n\nThursday, May 2, 2024\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nRAD REPL\n\n\n\n\n\n\n\n\n\n\n\nWednesday, May 1, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nSkryms Signals Summary and Models\n\n\nlearing language games\n\n\n\n\n\n\n\n\nWednesday, May 1, 2024\n\n\n65 min\n\n\n\n\n\n\n\n\n\n\n\n\nMesa Lessons\n\n\n\n\n\n\n\n\n\n\n\nSunday, March 31, 2024\n\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\n\nOCR building blocks\n\n\n\n\n\n\ncode\n\n\nbuggy code\n\n\nTODO\n\n\nOCR\n\n\n\n\n\n\n\n\n\nThursday, March 28, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nA definition by Patrick Henry Winston\n\n\nFor Artificial Intelligence\n\n\n\nkb-ai\n\n\nspeaking\n\n\nrhetoric\n\n\nawsome-learning\n\n\nai\n\n\nRumpelstiltskin principle\n\n\n\nPatrick Henry Winston provides a definition of AI. How he keeps expanding his definition as he goes along is thing of beauty. It expands organicaly to incorporate new ideas, accommodate example problems and aspects of their solutions. This is a powerful example of using inductive thinking to create definition using a philosophical approach.\n\n\n\n\n\nSunday, March 3, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nOCR - Brain Dump\n\n\n\n\n\n\nlinkage\n\n\nbrain dump\n\n\nOCR\n\n\n\n\n\n\n\n\n\nSunday, February 25, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nRhetoric NLP Tasks\n\n\n\n\n\n\nrhetoric\n\n\nnlp\n\n\nideas\n\n\n\n\n\n\n\n\n\nSaturday, February 17, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nLewis Game from a Bayesian Perspective\n\n\n\n\n\n\n\n\n\n\n\nMonday, February 12, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nüòÅ Quarto üíñ Mermaidüßú Mindmaps üß†\n\n\n\n\n\nQuarto at last supports Mindmap charts using Mermaid charts.\n\n\n\n\n\nMonday, February 12, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Great Migration\n\n\nFrom Blogger to Jekyl and finaly to Quarto.\n\n\n\nquarto\n\n\nblogging\n\n\ncode\n\n\n\nsome migration notes from Blooger to Jekyl to Quarto blog.\n\n\n\n\n\nTuesday, January 30, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSunday, January 28, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nEngineering Reinforcement Learning Algorithms\n\n\n\n\n\n\n\n\n\n\n\nWednesday, January 10, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nSuperLearner\n\n\n\n\n\n\ndemos\n\n\ncode\n\n\nr\n\n\n\nSuperLearner is an ensambeleing library.\n\n\n\n\n\nWednesday, January 10, 2024\n\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\n\nD3.js in in Quarto Observable\n\n\n\n\n\n\ncode\n\n\nd3.js\n\n\nobservable.js\n\n\n\n\n\n\n\n\n\nTuesday, January 2, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nBeacuase auto-ml is a Superpower\n\n\n\ncheatsheets\n\n\ncode\n\n\ndata science\n\n\nauto-ml\n\n\n\nAutogluon is a auto-ml framework, here are three cheetsheet for accellerating data science workloads\n\n\n\n\n\nWednesday, December 20, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nSpark Tips\n\n\n\n\n\n\nBigData\n\n\nSpark\n\n\n\n\n\n\n\n\n\nThursday, June 1, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nSummary: Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization\n\n\n\n\n\n\nReinforcement Learning\n\n\nModel Based RL\n\n\nModel Predictive Control\n\n\nMPC\n\n\nLQR\n\n\nMuJoCo\n\n\niLQG\n\n\nCost Functions\n\n\nIterative LQG Method\n\n\nNumerical Methods\n\n\nOptimal Control\n\n\nTrajectory Optimization\n\n\nComplex Behaviors\n\n\nHumanoid Robots\n\n\nPhysics Simulator\n\n\nReal-Time Control\n\n\nRobustness\n\n\nPlanning\n\n\nRobtics\n\n\nCoursera\n\n\n\nThis paper was referenced by Drew Bagnell in the Coursera RL specilization for using simple quadratic approximation to learn a model in a continous control setting. The paper presents a method for online trajectory optimization, particularly focusing on complex humanoid robots performing tasks such as getting up from an arbitrary pose and recovering from large disturbances using dexterous maneuvers.\n\n\n\n\n\nThursday, June 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nMCMC algorithms\n\n\n\n\n\n\n\n\n\n\n\nSaturday, April 22, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto loves pseudocode\n\n\n\n\n\n\n\n\n\n\n\nTuesday, April 11, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nText2topic Leverage reviews data for multi-label topics classification in Booking.com\n\n\nNLP.IL\n\n\n\n\n\n\n\n\nTuesday, February 28, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nValidating NLP data and models\n\n\nNLP.IL\n\n\n\n\n\n\n\n\nTuesday, February 28, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nTransformations in Linguistic Representation\n\n\n\n\n\n\nnlp\n\n\nlinguistics\n\n\n\n\n\n\n\n\n\nWednesday, February 22, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nevent generator\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\nprompt engineering\n\n\n\nfake data\n\n\n\n\n\nThursday, February 16, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nOLS regression From Scratch\n\n\n\n\n\n\ndata science\n\n\nml\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\nWednesday, February 1, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nentropy for uncertainty quantification\n\n\n\n\n\n\n\n\n\n\n\nThursday, September 22, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nWikisym 2012\n\n\nConference Report\n\n\n\nreport\n\n\nwikisym\n\n\nconference\n\n\n\n\n\n\n\n\n\nTuesday, July 26, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nSet Up M1 MacBooks for DS & ML\n\n\n\n\n\n\nmac\n\n\n\nSet Up M1 MacBooks for DS & ML\n\n\n\n\n\nThursday, May 5, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\ncommand line\n\n\n\n\n\n\nproductivity\n\n\nmac\n\n\n\ncommand line cheea sheet macos + zsh + git\n\n\n\n\n\nThursday, May 5, 2022\n\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\n\nMeme bank\n\n\nmemes a problem-solving approach\n\n\n\nmeme\n\n\nTRIZ\n\n\ntetrad\n\n\nbrainstorm\n\n\nalgorithm\n\n\n\nA meme is an idea, behavior, style, or usage that spreads from person to person within a culture. A meme bank would be a zoo for cataloging and breeding memes.\n\n\n\n\n\nThursday, December 30, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nAttention for sensor fusion\n\n\n\n\n\n\nattention\n\n\ndata science\n\n\nmarketing\n\n\nsensor fusion\n\n\nstatistics\n\n\nautoecoder\n\n\n\nAttention for sensor fusion\n\n\n\n\n\nFriday, September 24, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nExcel 2019 for Marketing Statistics in pandas\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\nmarketing\n\n\n\nExcel 2019 for Marketing Statistics in pandas\n\n\n\n\n\nFriday, September 24, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage models and explainability\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\nmarketing\n\n\nNLP\n\n\n\nLanguage models and explainability\n\n\n\n\n\nFriday, September 24, 2021\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nStorytelling and other essentials\n\n\n\n\n\n\ndata science\n\n\nstatistics\n\n\nmarketing\n\n\nwar story\n\n\n\nStorytelling and other essentials,\n\n\n\n\n\nThursday, September 2, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nPython Graphs\n\n\n\n\n\n\nPPC\n\n\ncode\n\n\ndata science\n\n\ndigital marketing\n\n\nquantitative marketing\n\n\nintelligence\n\n\n\nPython Graph Cookbook\n\n\n\n\n\nSunday, August 29, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Gradient Descent - The good parts\n\n\n\n\n\nStochastic Gradient Descent - The good parts\n\n\n\n\n\nSunday, August 29, 2021\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nWaveNet Review\n\n\n\n\n\n\ndeep learning\n\n\ndata science\n\n\nNLP\n\n\n\nThe WaveNet paper is kind of old. Yet it seems to come up in various contexts. Some thoughts on this.\n\n\n\n\n\nSunday, August 29, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is in a citation?\n\n\n\n\n\nCreating Citation Web Components\n\n\n\n\n\nSunday, August 29, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nHackathon session link dumps & notes\n\n\n\n\n\n\nmodelling\n\n\nchat bot\n\n\nwikipedia\n\n\nsupport\n\n\n\nWikipedia Hackathon notes\n\n\n\n\n\nFriday, August 13, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nInlining Citations for Wikipedia articles\n\n\n\n\n\n\nmodelling\n\n\nchat bot\n\n\nwikipedia\n\n\nsupport\n\n\n\nAn algorithm for Inlining Citations for Wikipedia articles.\n\n\n\n\n\nFriday, August 13, 2021\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer learning in NLP\n\n\n\n\n\n\nmodelling\n\n\nchat bot\n\n\nwikipedia\n\n\nsupport\n\n\n\nTransfer learning in NLP\n\n\n\n\n\nFriday, August 13, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nA type of Witness and an evolving Idiom\n\n\n\n\n\nwriting better code = writing more readable code.\n\n\n\n\n\nWednesday, July 14, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\njson-ld\n\n\nmetadata format for linked data\n\n\n\ninformation science\n\n\n\n\n\n\n\n\n\nThursday, July 1, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow probability\n\n\n\n\n\n\nML\n\n\nAI\n\n\nNLP\n\n\nBijectors\n\n\nAuto Regressive Flows\n\n\n\n\n\n\n\n\n\nTuesday, June 1, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nEbook Hacks\n\n\n\n\n\n\nhacks\n\n\nebooks\n\n\nresolution\n\n\n\n\n\n\n\n\n\nSaturday, May 29, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nMultilevel Models\n\n\n\n\n\n\nNLP\n\n\nnotes\n\n\nsummarization task\n\n\nlecture notes\n\n\nbibliography\n\n\nliterature review\n\n\n\nDifferent Multilevel Models Types\n\n\n\n\n\nSunday, May 16, 2021\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A and the Winograd schemas\n\n\n\n\n\n\nidea\n\n\nNLP\n\n\nintelligence\n\n\nresolution\n\n\n\n\n\n\n\n\n\nTuesday, April 27, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Summarization Task\n\n\n\n\n\n\nNLP\n\n\nnotes\n\n\nsummarization task\n\n\nlecture notes\n\n\nbibliography\n\n\nliterature review\n\n\nvideo\n\n\nautomatic extracting\n\n\nautomatic abstracting\n\n\nsentence selection\n\n\ndocument screening\n\n\nsentence significance\n\n\nrelevance\n\n\ncontent words\n\n\nkey words\n\n\npragmatic words\n\n\ncue words\n\n\ntitle words\n\n\nsentence location\n\n\nresearch methodology\n\n\nparameterization\n\n\ncomparative evaluation\n\n\n\nConcepts, slide commentaries and Lecture notes on Automatic text Summarization by Masa Nekic\n\n\n\n\n\nSaturday, April 24, 2021\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian agents\n\n\n\n\n\n\nidea\n\n\ngame theory\n\n\nbayesian games\n\n\nsub-perfect bayesian equilibrium\n\n\n\n\n\n\n\n\n\nWednesday, April 14, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Events\n\n\n\n\n\n\nmodelling\n\n\ndata science\n\n\n\nModeling Events.\n\n\n\n\n\nFriday, April 9, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n10 Tips To Improve Your Workflow\n\n\n\n\n\nHow to blog like a life-hacker.\n\n\n\n\n\nWednesday, April 7, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nLanguage Models Are Open Knowledge Graphs\n\n\npaper review\n\n\n\npaper\n\n\nlanguage models\n\n\ndeep learning\n\n\ndata mining\n\n\nNLP\n\n\n\nA review of the paper ‚ÄúLanguage Models are Open Knowledge Graphs‚Äù by Chenguang Wang, Xiao Liu, Dawn Song arXiv:2010.11967\n\n\n\n\n\nWednesday, April 7, 2021\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nLinkage 2021-04-07\n\n\n\n\n\n\nlinks\n\n\n\n\n\n\n\n\n\nWednesday, April 7, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nJekyll take 3\n\n\nGetting\n\n\n\nblogging\n\n\nruby\n\n\njekyll\n\n\n\nMy attempts to get the jekyll version of this site to also build locally.\n\n\n\n\n\nSunday, April 4, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nMathJax 3 fix for Jekyll hosted on Github pages\n\n\n\n\n\n\nblogging\n\n\njekyll\n\n\nkramdown\n\n\nmathjax\n\n\n\nIssues and workarounds for MatchJax 3.0.\n\n\n\n\n\nSunday, April 4, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Approaches to Attention-based NMT\n\n\nPaper review for the deeplearning.ai NLP specialization\n\n\n\nnlp\n\n\ncoursera\n\n\nnotes\n\n\npaper\n\n\nattention\n\n\ndeep learning\n\n\nliterature review\n\n\n\nReview of the paper on dot product attention for the deeplearning.ai NLP specialization.\n\n\n\n\n\nSunday, March 21, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy melt down\n\n\n\n\n\n\npython\n\n\nmacos\n\n\nbugs\n\n\n\nnumpy melt down.\n\n\n\n\n\nSunday, November 29, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Intuitions\n\n\n\n\n\n\nintuition\n\n\ndata analysis\n\n\npython\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\nSunday, October 25, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nbrace expansion\n\n\n\n\n\n\nbash\n\n\ncommand line\n\n\nlinux\n\n\n\n\n\n\n\n\n\nFriday, June 12, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nPandas Productivity Challenge?\n\n\n\n\n\n\ndata-science\n\n\npython\n\n\ndata-wrangling\n\n\n\nJust a little rant on Pandas various contexts\n\n\n\n\n\nWednesday, March 4, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to avoid cross site scripting (XSS) errors with the Jupyter local runtime for Colab\n\n\n\n\n\n\ndata-science\n\n\nds-tips\n\n\nGoogle-colab\n\n\ncolab\n\n\njupyter\n\n\npython\n\n\nR\n\n\ntool-tip\n\n\ndata analysis\n\n\nXSS\n\n\n\n\n\n\n\n\n\nThursday, February 20, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nDocker for data science\n\n\n\n\n\n\ndocker\n\n\nconternerization\n\n\ndata analysis\n\n\n\nPost description\n\n\n\n\n\nSunday, November 24, 2019\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nExploding and vanishing nodes.\n\n\n\n\n\n\nbrain farts\n\n\n\n\n\n\n\n\n\nWednesday, July 31, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\ntext annotation with BRAT\n\n\n\n\n\n\n\n\n\n\n\nTuesday, January 16, 2018\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nA/B testing cost and risks?\n\n\n\n\n\n\nPPC,\n\n\ndata science,\n\n\ndigital marketing,\n\n\nquantitative marketing,\n\n\nCRO,\n\n\nexperimental design,\n\n\nA/B testing\n\n\n\nA/B testing cost and risks and some recommendation.\n\n\n\n\n\nSunday, July 30, 2017\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nTravel checklist\n\n\n\n\n\n\ntravel\n\n\nchecklist\n\n\n\n\n\n\n\n\n\nWednesday, December 14, 2016\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nHotJar Heat Map Analysis - Dr.¬†David Darmanin\n\n\nAll things data 2015\n\n\n\nConference\n\n\nAnalytics\n\n\nHeat Maps\n\n\n\n\n\n\n\n\n\nWednesday, April 20, 2016\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Competitive Analysis to Benchmark Your Marketing Efforts Ariel Rosenstein - Similar Web\n\n\nAll things data 2015\n\n\n\nall things data\n\n\nconference\n\n\nanalytics\n\n\norgenizational behviour\n\n\n\n\n\n\n\n\n\nMonday, April 20, 2015\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Competitive Analysis to Benchmark Your Marketing Efforts - Ariel Rosenstein - Similar Web\n\n\nAll things data 2015\n\n\n\nMarketing\n\n\nAnalytics\n\n\nCompetitive Analysis\n\n\n\n\n\n\n\n\n\nMonday, April 20, 2015\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytics Checklist\n\n\n\n\n\n\n\n\n\n\n\nSaturday, February 7, 2015\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nlife hacks\n\n\n\n\n\n\nlife\n\n\nhacks\n\n\nhealth\n\n\nproductivity\n\n\n\n\n\n\n\n\n\nFriday, June 7, 2013\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With Python\n\n\na number of NLP tasks in Python\n\n\n\npython\n\n\nNLP\n\n\ntext mining\n\n\ncode\n\n\n\n\n\n\n\n\n\nTuesday, November 29, 2011\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With R\n\n\na number of NLP tasks in R\n\n\n\nR\n\n\nNLP\n\n\ntext Mining\n\n\ncode\n\n\n\n\n\n\n\n\n\nTuesday, November 29, 2011\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\nan update on NLP with R\n\n\n\nR\n\n\nNLP\n\n\nText Mining\n\n\ntidyverse\n\n\ncode\n\n\n\n\n\n\n\n\n\nTuesday, November 29, 2011\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nTime management Tips\n\n\nWe could all use a productivity boost\n\n\n\ntime management\n\n\nproductivity\n\n\n\nEffective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance\n\n\n\n\n\nThursday, August 11, 2011\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hungarian/hungarian.html",
    "href": "hungarian/hungarian.html",
    "title": "hungarian cheat sheet",
    "section": "",
    "text": "I‚Äôve been learning Hungarian on Duolingo in the last year. Before that I learned in the hungarian Debrecen Summer School for a three courses and before that the full Pimsleur hungarian course. The main problem with Duolingo is that it approach to grammar is nonexistent they explicitly state you should master it by example. This suggest that an innovative idea: add a machine readable grammar that accepts all the content in the course and that rejects what the course rejects. This is then rendered as a human readable document using a script. Unfortunately build a good grammar and making it human readable are both non-trivial tasks at this point in time!?\nI‚Äôve put together this cheat sheet to help accelerate the learning. I hope with more time I‚Äôll also be able to use an LLM to generate more dynamical training regimens then Duolingo can.\nAt this point I have some issues with word order of some sentences. Which I hope to figure out. Talking with native hungarian friends is both amusing and disappointing. They could make no coherent explanation of why the sentences follow that order. One real world issue is that hungarian sentences can have very nuanced meaning based on the word order yet try to translate these meanings to english which does not usually make such nuanced interpretation and the meaning seems nonsensical in many cases or artificial, making you think why would you be saying this where in Hungarian that what that word order has a natural meaning.\nNote that this is an issue when looking at parse trees - it can be a challenge to understand what some different trees actually mean - particularly as an random tree can be drawn it probably wouldn‚Äôt necessarily correspond to a sentence in english. To tie this up Hungarian with its flexible word order admits far more parse trees."
  },
  {
    "objectID": "hungarian/hungarian.html#alphabet",
    "href": "hungarian/hungarian.html#alphabet",
    "title": "hungarian cheat sheet",
    "section": "Alphabet",
    "text": "Alphabet"
  },
  {
    "objectID": "hungarian/hungarian.html#vowels",
    "href": "hungarian/hungarian.html#vowels",
    "title": "hungarian cheat sheet",
    "section": "Vowels",
    "text": "Vowels\n\n\n\nAPA hungarian chart\n\n\n\nVowel Harmony\n\n\n\n\nAPA vowels chart\n\n\n\n\nback vowels\na, √°, o, √≥, u, √∫\n\n\nfront vowels\ne, √©, i, √≠, √∂, ≈ë, √º, ≈±\n\n\nunrounded\ne, √©, i, √≠\n\n\nrounded\n√∂, ≈ë, √º, ≈±"
  },
  {
    "objectID": "hungarian/hungarian.html#pronouns",
    "href": "hungarian/hungarian.html#pronouns",
    "title": "hungarian cheat sheet",
    "section": "Pronouns",
    "text": "Pronouns\n\n\nPersonal Pronouns\n\n\n\n√ân\nI\nMi\nus\n\n\nTe\nyou\nTi\nyou (pl.)\n\n\n≈ê\nxe\n≈êk\nthey (pl.)\n\n\n√ñn\nyou (pol. sg.)\n√ñn√∂k\nyou (pol. pl.)\n\n\n\n\n\nReflexive Pronouns\n\n\n\nMagam\nmyself\nMagunk\nourselves\n\n\nMagad\nyourself\nMagatok\nyourselves\n\n\nMaga\nxirself\nMaguk\nthemselves"
  },
  {
    "objectID": "hungarian/hungarian.html#cases",
    "href": "hungarian/hungarian.html#cases",
    "title": "hungarian cheat sheet",
    "section": "Cases",
    "text": "Cases\n\n\n\ncase\nsuffix\n\n\n\n\naccusative\n-t/-ot/-et/√∂t/-at\n\n\ndative\n-nak/-nek\n\n\nillative\n-ba/-be\n\n\ninessive\n-ban/-ben\n\n\nelative\n-b√≥l/-b≈ël\n\n\nallative\n-hoz/-hez/-h√∂z\n\n\nadessive\n-n√°l/-n√©l\n\n\nablative\n-t√≥l/-t≈ël\n\n\nsublative\n-ra/-re\n\n\nsuperessive\n-n/-on/-en/-√∂n\n\n\ndelative\n-r√≥l/-r≈ël\n\n\ninstrumental\n-val/-vel\n\n\ncausal-final\n-√©rt\n\n\nterminative\n-ig\n\n\ntemporal\n-kor\n\n\ntranslative\n-v√°/-v√©\n\n\ngenitive\n-√©"
  },
  {
    "objectID": "hungarian/hungarian.html#plurals",
    "href": "hungarian/hungarian.html#plurals",
    "title": "hungarian cheat sheet",
    "section": "Plurals",
    "text": "Plurals\n\n\n\nback vowel\n-k/-ok/-ak\n\n\nfront vowel\n-k/-ek/-√∂k"
  },
  {
    "objectID": "hungarian/hungarian.html#pl.-adjectives",
    "href": "hungarian/hungarian.html#pl.-adjectives",
    "title": "hungarian cheat sheet",
    "section": "Pl. Adjectives",
    "text": "Pl. Adjectives\n\n\n\n\n\n\n\n\nending in a / e\nbv: -‚Äôk\nfv: -‚Äôk\n\n\nending in i / √∫ / ≈±\nbv: -ak\nfv: -ek\n\n\nending in √≥ / ≈ë\nbv (participle): -ak OR -k\nfv (participle): -ek OR -k\n\n\nending in √≥ / ≈ë\nbv (regular): -k\nfv (regular): -k\n\n\nending in a consonant\nbv: -ak\nfv: -ek\n\n\natlan / etlen adjectives\nbv: -ok\nfv: -ek\n\n\nethnonyms ending in i\nbv: -ak\nfv: -ek\n\n\nall other ethnonyms\nbv: -ok\nfv: -ek/-√∂k"
  },
  {
    "objectID": "hungarian/hungarian.html#sg.-possession",
    "href": "hungarian/hungarian.html#sg.-possession",
    "title": "hungarian cheat sheet",
    "section": "Sg. Possession",
    "text": "Sg. Possession\n\n\n\npronoun\nbv\nfv\n\n\n\n\n√ân\n-m/-om/-am\n-m/-em/-√∂m\n\n\nTe\n-d/-od/-ad\n-d/-ed/-√∂d\n\n\n≈ê (√ñn)\n-ja/-a\n-je/-e\n\n\nMi\n-nk/-unk\n-nk/-√ºnk\n\n\nTi\n-tok/-otok/-atok\n-tok/-etek/-√∂t√∂k\n\n\n≈êk (√ñn√∂k)\n-juk/-uk\n-j√ºk/-√ºk"
  },
  {
    "objectID": "hungarian/hungarian.html#pl.-possession",
    "href": "hungarian/hungarian.html#pl.-possession",
    "title": "hungarian cheat sheet",
    "section": "Pl. Possession",
    "text": "Pl. Possession\n\n\n\npronoun\nbv\nfv\n\n\n\n\n√ân\n-im -aim -jaim\n-im -eim -jeim\n\n\nTe\n-id -aid -jaid\n-id -eid -jeid\n\n\n≈ê √ñn\n-i -ai -jai\n-i -ei -jei\n\n\nMi\n-ink -aink -jaink\n-ink -eink -jeink\n\n\nTi\n-itok -aitok -jaitok\n-itek -eitek -jeitek\n\n\n≈êk √ñn√∂k\n-ik -aik -jaik\n-ik -eik -jeik"
  },
  {
    "objectID": "hungarian/hungarian.html#acc.-adjectives",
    "href": "hungarian/hungarian.html#acc.-adjectives",
    "title": "hungarian cheat sheet",
    "section": "Acc. Adjectives",
    "text": "Acc. Adjectives\n\n\n\n\n\n\n\n\nending in a / e\nbv: -‚Äôt\nfv: -‚Äôt\n\n\nending in other vowels\nbv: -t\nfv: -t\n\n\nending in a consonant\nbv: -at\nfv: -et\n\n\natlan / etlen adjectives\nbv: -t\nfv: -t\n\n\nethnonyms (vowel)\nbv: -‚Äôt\nfv: -‚Äôt\n\n\nethnonyms (consonant) 1\nbv: -ot\nfv: -et/-√∂t"
  },
  {
    "objectID": "hungarian/hungarian.html#verbs---present-tense",
    "href": "hungarian/hungarian.html#verbs---present-tense",
    "title": "hungarian cheat sheet",
    "section": "Verbs - Present Tense",
    "text": "Verbs - Present Tense\n\nDefinite\n\n\n\n\n\n\n\n\npronoun\nbv\nfv\n\n\n\n\n√ân\n-om\n-em/-√∂m\n\n\nTe\n-od\n-ed/-√∂d\n\n\n≈ê √ñn 2\n-ja\nfv -i\n\n\nMi\n-juk\n-j√ºk\n\n\nTi\n-j√°tok\n-itek\n\n\n≈êk (√ñn√∂k) 3\n-j√°k\n-ik\n\n\n\n\n\nIndefinite (Regular Verbs)\n\n\n\npronoun\nbv\nfv\n\n\n\n\n√ân\n-ok\n-ek/-√∂k\n\n\nTe\n-sz\n-sz\n\n\n≈ê √ñn\n√ò\n√ò\n\n\nMi\n-unk\n-√ºnk\n\n\nTi\n-tok\n-tek/-t√∂k\n\n\n≈êk √ñn√∂k\n-nak\n-nek\n\n\n\n\n\nIndefinite (-ik Verbs)\n\n\n\npronoun\nbv\nfv\n\n\n\n\n√ân\n-om\n-em/-√∂m\n\n\nTe\n-sz\n-sz\n\n\n≈ê √ñn\n-ik\n-ik\n\n\nMi\n-unk\n-√ºnk\n\n\nTi\n-tok\n-tek/-t√∂k\n\n\n≈êk √ñn√∂k\n-nak\n-nek"
  },
  {
    "objectID": "hungarian/hungarian.html#indefinite-verb-ending-in-s-sz-z-dz",
    "href": "hungarian/hungarian.html#indefinite-verb-ending-in-s-sz-z-dz",
    "title": "hungarian cheat sheet",
    "section": "Indefinite (verb ending in s, sz, z, dz)",
    "text": "Indefinite (verb ending in s, sz, z, dz)\n\nTe 4\n\n‚Üí bv: -ol\n‚Üí fv: -el"
  },
  {
    "objectID": "hungarian/hungarian.html#footnotes",
    "href": "hungarian/hungarian.html#footnotes",
    "title": "hungarian cheat sheet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nno link vowel after j, l, ly, n, ny, r, s, sz, z, zs‚Ü©Ô∏é\nif stem ends in s, sz, z, dz ‚Äì leading j in the ending turns into the last letter‚Ü©Ô∏é\nif stem ends in s, sz, z, dz ‚Äì leading j in the ending turns into the last letter‚Ü©Ô∏é\nonly for ‚Äúte‚Äù conjugation‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html",
    "title": "Meme bank",
    "section": "",
    "text": "Introduced in (Dawkins 1976), a meme is an idea, behavior, style, or usage that spreads from person to person within a culture. A meme bank would be a zoo for cataloging and breeding memes.\n\nDawkins, R. 1976. The Selfish Gene. Oxford University Press, Oxford, UK.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#use-cases",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#use-cases",
    "title": "Meme bank",
    "section": "Use cases",
    "text": "Use cases\n\nattacking a problem\napproaching creativity using generate-and-test\nchoosing an approach\nidentifying similarity and difference between algorithms etc.\ngenetic programming\ngeneralizing in mathematics",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#what-constitutes-a-meme",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#what-constitutes-a-meme",
    "title": "Meme bank",
    "section": "What constitutes a meme?",
    "text": "What constitutes a meme?\nFor the meme bank one might want to collect memes from the wild but also to perhaps establish a meritocracy and allocate resources to memes that seem to be more useful. In science ideas are often formalized as theories. This statement and behavior of theories have been studied and later formalized using the interrelated mathematical branches of formal languages, logic, set theory and category theory. In other domain like fashion or popular culture we may lack such rigor.\nThe more formaly stated a meme is the fewer minds it can inhabit. But perhpas could",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#are-memes-alive",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#are-memes-alive",
    "title": "Meme bank",
    "section": "Are memes alive ?",
    "text": "Are memes alive ?\nMemes have a number of properties:\n\nBirth:\n\nMemes, must orginate with someone\n\nSpreading:\n\nThe then gain traction over time.\n\nDecline:\n\nMany ideas can also lose traction and become discredited or supplanted by more powerful ideas.\n\nCan they evolve?\n\nIf memes are alive can they breed !?\nHow should memes be represented ?\n\n\nMeme‚Äôs don‚Äôt have DNA, but they often have a complex genealogy. This suggests that emergent memes contain some aspects of simpler memes that may have vanished from our minds. Why should we care about such basic ideas? It would seem that having a more basic idea is going to be easier to work with. At least in the sense that a simple meme might be easier to utilize or generalize or combine than a complex one.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#specialized-type-of-memes.",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#specialized-type-of-memes.",
    "title": "Meme bank",
    "section": "Specialized type of memes.",
    "text": "Specialized type of memes.\n\nWords (have etymologies)\nScientific theories\nData structure\nFrames.\nScripts.\nClasses.\nAlgorithm.\nDesign Patterns.\nReligion.\nMedia\nArt Clearly we would treat words algorithms differently.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#mathematical",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#mathematical",
    "title": "Meme bank",
    "section": "Mathematical:",
    "text": "Mathematical:\n\nmorphism\nequivilence\nmapping\n\nidentity\norder\nreflexive\ntransitive\n\nrelation\naxioms\ncontinuity\ncompactness\nmaximum and minimum\ndistance\ngame.\ntopology.\nmatrix.\ngroup.\nprobability distribution\ngrammar\nfsm\nmarkov chain\ngausian process",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#logic",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#logic",
    "title": "Meme bank",
    "section": "Logic",
    "text": "Logic\n\ndeduction\ncausality\ncorrelation\nsylogism\nnormal form\nskolemization\ncompleteness",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#cognitive",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#cognitive",
    "title": "Meme bank",
    "section": "cognitive",
    "text": "cognitive\n\ngenerate-and-test\n\nsample\n\nstratified\nsnowball\n\noptimization\nfiltering\nsmoothing\nsearch\n\nDepth first search\nBreadth first search\nBeam search\nMCMC search ?\nAdverserial search ?\nab search\n\nSpace filling curves\n\nHilbert,Peano, Lebesgue, Moore, Sierpinski.\n\nSpace filling trees\nSimilarity and distances",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#data-structures",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#data-structures",
    "title": "Meme bank",
    "section": "data structures:",
    "text": "data structures:\n\nsets\ncategories\ngraphs\n\ntrees\n\nlists\nlinked lists\narray\n\ntables\n\ndictionary\nstate space\ntopology",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#in-my-bank",
    "href": "posts/2020/2020-12-30-meme-bank/2020-12-30-meme-bank.html#in-my-bank",
    "title": "Meme bank",
    "section": "In my bank:",
    "text": "In my bank:\n\ndecomposing PCA into its consituents\nvitrtebi alg - converting it to boolean view of transitions + a likelyhood for one happening\nis an analysis sing using there is a solution to one with none\nvariational autoencoder + gan =\nsampling + a space filling curve\nerror propagation though sampling ?\npropergating error gradients through a sampling step ?\nare there other repramtrization trick we can\nspacewise-seperable convolutional layers - can we represent larger\nconvolutinos as products of 2 or 3 matrices\nnegative sampling\ninverse sampling\nfollow the leader\nfollow the regularized leader\nregret\nrecursive matrix alg\nnon-negative matrix factorization\nwiden dataframe from wikidata",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 12 30 Meme Bank",
      "Meme bank"
    ]
  },
  {
    "objectID": "posts/2020/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime.html",
    "href": "posts/2020/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime.html",
    "title": "How to avoid cross site scripting (XSS) errors with the Jupyter local runtime for Colab",
    "section": "",
    "text": "google colab\n\n\nSo the trick is\n\nto use --NotebookApp.allow_origin and --no-browser\nand get the token from the command line when connecting to Google collab.\n\njupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' \\\n  --port=9090 --no-browser\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {How to Avoid Cross Site Scripting {(XSS)} Errors with the\n    {Jupyter} Local Runtime for {Colab}},\n  date = {2020-02-20},\n  url = {https://orenbochman.github.io/posts/2020/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. ‚ÄúHow to Avoid Cross Site Scripting (XSS)\nErrors with the Jupyter Local Runtime for Colab.‚Äù February 20,\n2020. https://orenbochman.github.io/posts/2020/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime/2020-02-20-avoid-cross-site-scriptin-errors-with-a-Jupyter-local-runtime.html.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 02 20 Avoid Cross Site Scriptin Errors with a Jupyter Local Runtime",
      "How to avoid cross site scripting (XSS) errors with the Jupyter local runtime for Colab"
    ]
  },
  {
    "objectID": "posts/2023/2023-04-22-mcmc-algs/MCMC-algs.html",
    "href": "posts/2023/2023-04-22-mcmc-algs/MCMC-algs.html",
    "title": "MCMC algorithms",
    "section": "",
    "text": "\\begin{algorithm} \\caption{Metropolis-Hastings algorithm} \\begin{algorithmic} \\Procedure{MetropolisHastings}{$p(x), q(x,y), x_0, N$} \\State Initialize $x_0$ and set $t=0$. \\While{$t&lt;N$} \\State Generate a proposal $y \\sim q(x_t, \\cdot)$. \\State Calculate the acceptance ratio $r = \\frac{p(y)q(x_t|y)}{p(x_t)q(y|x_t)}$. \\State Generate a random number $u \\sim U(0,1)$. \\If{$u &lt; r$} \\State Accept the proposal: $x_{t+1} = y$. \\Else \\State Reject the proposal: $x_{t+1} = x_t$. \\EndIf \\State Increment $t$: $t \\leftarrow t+1$. \\EndWhile \\State \\textbf{return} $(x_0, x_1, \\ldots, x_N)$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\nThe procedure MetropolisHastings takes as input : - the target distribution p(x), - the proposal distribution q(x,y), - the initial sample x_0, and - the total number of samples to generate N. The procedure returns: - the sequence of samples (x_0, x_1, \\ldots, x_N).",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 04 22 Mcmc Algs",
      "MCMC algorithms"
    ]
  },
  {
    "objectID": "posts/2023/2023-04-22-mcmc-algs/MCMC-algs.html#metropolis-hastings",
    "href": "posts/2023/2023-04-22-mcmc-algs/MCMC-algs.html#metropolis-hastings",
    "title": "MCMC algorithms",
    "section": "",
    "text": "\\begin{algorithm} \\caption{Metropolis-Hastings algorithm} \\begin{algorithmic} \\Procedure{MetropolisHastings}{$p(x), q(x,y), x_0, N$} \\State Initialize $x_0$ and set $t=0$. \\While{$t&lt;N$} \\State Generate a proposal $y \\sim q(x_t, \\cdot)$. \\State Calculate the acceptance ratio $r = \\frac{p(y)q(x_t|y)}{p(x_t)q(y|x_t)}$. \\State Generate a random number $u \\sim U(0,1)$. \\If{$u &lt; r$} \\State Accept the proposal: $x_{t+1} = y$. \\Else \\State Reject the proposal: $x_{t+1} = x_t$. \\EndIf \\State Increment $t$: $t \\leftarrow t+1$. \\EndWhile \\State \\textbf{return} $(x_0, x_1, \\ldots, x_N)$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\nThe procedure MetropolisHastings takes as input : - the target distribution p(x), - the proposal distribution q(x,y), - the initial sample x_0, and - the total number of samples to generate N. The procedure returns: - the sequence of samples (x_0, x_1, \\ldots, x_N).",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 04 22 Mcmc Algs",
      "MCMC algorithms"
    ]
  },
  {
    "objectID": "posts/2023/2023-04-11-quarto-loves-psdocode/2023-04-11-quarto-psdocode.html",
    "href": "posts/2023/2023-04-11-quarto-loves-psdocode/2023-04-11-quarto-psdocode.html",
    "title": "Quarto loves pseudocode",
    "section": "",
    "text": "Quick Sort\n\n\n\\begin{algorithm} \\caption{Quicksort} \\begin{algorithmic} \\Procedure{Quicksort}{$A, p, r$} \\If{$p &lt; r$} \\State $q = $ \\Call{Partition}{$A, p, r$} \\State \\Call{Quicksort}{$A, p, q - 1$} \\State \\Call{Quicksort}{$A, q + 1, r$} \\EndIf \\EndProcedure \\Procedure{Partition}{$A, p, r$} \\State $x = A[r]$ \\State $i = p - 1$ \\For{$j = p$ \\To $r - 1$} \\If{$A[j] &lt; x$} \\State $i = i + 1$ \\State exchange $A[i]$ with $A[j]$ \\EndIf \\State exchange $A[i]$ with $A[r]$ \\EndFor \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2023,\n  author = {Bochman, Oren},\n  title = {Quarto Loves Pseudocode},\n  date = {2023-04-11},\n  url = {https://orenbochman.github.io/posts/2023/2023-04-11-quarto-loves-psdocode/2023-04-11-quarto-psdocode.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2023. ‚ÄúQuarto Loves Pseudocode.‚Äù April 11,\n2023. https://orenbochman.github.io/posts/2023/2023-04-11-quarto-loves-psdocode/2023-04-11-quarto-psdocode.html.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 04 11 Quarto Loves Psdocode",
      "Quarto loves pseudocode"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-01-ds-from-scratch/ols-regression-from-scratch.html",
    "href": "posts/2023/2023-02-01-ds-from-scratch/ols-regression-from-scratch.html",
    "title": "OLS regression From Scratch",
    "section": "",
    "text": "OLS regression is a method for estimating the parameters of a linear regression model. The goal is to find the line that best fits a set of data points. The line is represented by an equation of the form \ny = mx + b\n\nwhere :\n\ny is the dependent variable,\nx is the independent variable,\nm is the slope of the line, and\nb is the y-intercept.\n\n\n\n\n# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\n\n# Generate random data\nn = 100\nx = np.random.rand(n)\ny = 2*x + np.random.normal(size=n)\n\n# Create a DataFrame and save to CSV\ndf = pd.DataFrame({'x': x, 'y': y})\ndf.to_csv('your_dataset.csv', index=False)\n\n\n\n\n\nimport numpy as np\nfrom numpy import ndarray\nimport pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the data and split into independent and dependent variables\ndata = pd.read_csv('your_dataset.csv')\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\n\n# add a column of 1s to the X matrix for the intercept term\nX = np.append(arr=np.ones((len(X), 1)), values=X, axis=1)\n\n# calculate the coefficients using the OLS formula\nbeta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n\ny_pred = X.dot(beta)\n\n\ndef mean_squared_error(y_true:ndarray, y_pred:ndarray):\n    n = len(y_true)\n    mse = sum([(y_true[i] - y_pred[i])**2 for i in range(n)]) / n\n    return mse\n\ndef r2_score(y_true:ndarray, y_pred:ndarray):\n    ssr = sum([(y_true[i] - y_pred[i])**2 for i in range(len(y_true))])\n    sst = sum([(y_true[i] - np.mean(y_true))**2 for i in range(len(y_true))])\n    r2 = 1 - (ssr / sst)\n    return r2\n\nrmse = np.sqrt(mean_squared_error(y, y_pred))\nr2 = r2_score(y, y_pred)\n\nprint(\"RMSE: \", rmse)\nprint(\"R-squared: \", r2)\n\nRMSE:  1.0862489887741527\nR-squared:  0.29675681489500483\n\n\n\nplt.scatter(X[:, 1], y, color='blue')\nplt.plot(X[:, 1], y_pred, color='red')\nplt.title('OLS Regression')\nplt.xlabel('Independent variable')\nplt.ylabel('Dependent variable')\nplt.show()\n\nText(0.5, 1.0, 'OLS Regression')\n\n\nText(0.5, 0, 'Independent variable')\n\n\nText(0, 0.5, 'Dependent variable')",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 01 Ds from Scratch",
      "OLS regression From Scratch"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-01-ds-from-scratch/ols-regression-from-scratch.html#generate-random-data",
    "href": "posts/2023/2023-02-01-ds-from-scratch/ols-regression-from-scratch.html#generate-random-data",
    "title": "OLS regression From Scratch",
    "section": "",
    "text": "# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\n\n# Generate random data\nn = 100\nx = np.random.rand(n)\ny = 2*x + np.random.normal(size=n)\n\n# Create a DataFrame and save to CSV\ndf = pd.DataFrame({'x': x, 'y': y})\ndf.to_csv('your_dataset.csv', index=False)",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 01 Ds from Scratch",
      "OLS regression From Scratch"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-01-ds-from-scratch/ols-regression-from-scratch.html#section",
    "href": "posts/2023/2023-02-01-ds-from-scratch/ols-regression-from-scratch.html#section",
    "title": "OLS regression From Scratch",
    "section": "",
    "text": "import numpy as np\nfrom numpy import ndarray\nimport pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the data and split into independent and dependent variables\ndata = pd.read_csv('your_dataset.csv')\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\n\n# add a column of 1s to the X matrix for the intercept term\nX = np.append(arr=np.ones((len(X), 1)), values=X, axis=1)\n\n# calculate the coefficients using the OLS formula\nbeta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n\ny_pred = X.dot(beta)\n\n\ndef mean_squared_error(y_true:ndarray, y_pred:ndarray):\n    n = len(y_true)\n    mse = sum([(y_true[i] - y_pred[i])**2 for i in range(n)]) / n\n    return mse\n\ndef r2_score(y_true:ndarray, y_pred:ndarray):\n    ssr = sum([(y_true[i] - y_pred[i])**2 for i in range(len(y_true))])\n    sst = sum([(y_true[i] - np.mean(y_true))**2 for i in range(len(y_true))])\n    r2 = 1 - (ssr / sst)\n    return r2\n\nrmse = np.sqrt(mean_squared_error(y, y_pred))\nr2 = r2_score(y, y_pred)\n\nprint(\"RMSE: \", rmse)\nprint(\"R-squared: \", r2)\n\nRMSE:  1.0862489887741527\nR-squared:  0.29675681489500483\n\n\n\nplt.scatter(X[:, 1], y, color='blue')\nplt.plot(X[:, 1], y_pred, color='red')\nplt.title('OLS Regression')\nplt.xlabel('Independent variable')\nplt.ylabel('Dependent variable')\nplt.show()\n\nText(0.5, 1.0, 'OLS Regression')\n\n\nText(0.5, 0, 'Independent variable')\n\n\nText(0, 0.5, 'Dependent variable')",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 01 Ds from Scratch",
      "OLS regression From Scratch"
    ]
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html",
    "href": "posts/2023/2023-12-20-autogluon/index.html",
    "title": "AutoGluon Cheetsheets",
    "section": "",
    "text": "AutoGluon is a powerful framework for auto-ML.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "AutoGluon Cheetsheets"
    ]
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "href": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "title": "AutoGluon Cheetsheets",
    "section": "Tabular",
    "text": "Tabular\n\n\n\nTabular",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "AutoGluon Cheetsheets"
    ]
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "href": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "title": "AutoGluon Cheetsheets",
    "section": "Time Series",
    "text": "Time Series\n\n\n\nTime Series",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "AutoGluon Cheetsheets"
    ]
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "href": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "title": "AutoGluon Cheetsheets",
    "section": "Multimodal",
    "text": "Multimodal\n\n\n\nMultimodal",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "AutoGluon Cheetsheets"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Text2Topic.html",
    "href": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Text2Topic.html",
    "title": "Text2topic Leverage reviews data for multi-label topics classification in Booking.com",
    "section": "",
    "text": "Having millions of customer reviews, we would like to better understand them and leverage this data for different use cases. For example, finding popular activities per destination, detecting popular facilities per property, allowing the users to filter reviews by specific topics, detecting violence in reviews and summarizing most discussed topics per property.\nIn this talk, we will present how we build a multilingual multi-label topic classification model that supports zero-shot, to match reviews with unseen users‚Äô search topics.\nWe will show how fine-tuning BERT-like models on the tourism domain with a small dataset can outperform other pre-trained models and will share experiment results of different architectures.\nFurthermore, we will present how we collected the data using an active learning approach and AWS Sagemaker ground truth tool, and we will show a short demo of the model with explainability using Streamlit.\n\n\n\nMoran is a machine learning manager at booking.com, researching and developing computer vision and NLP models for the tourism domain. Moran is a Ph.D candidate in information systems engineering at Ben Gurion University, researching NLP aspects in temporal graphs. Previously worked as a Data Science Team Leader at Diagnostic Robotics, building ML solutions for the medical domain and NLP algorithms to extract clinical entities from medical visit summaries.\n\n\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nWhat is CIP\n\n\n\n\n\nWhat is CIP\n\n\n\n\n\nText2Topic\n\n\n\n\n\nOverview\n\n\n\n\n\nData Sources\n\n\n\n\n\nData Sources\n\n\n\n\n\nData Sources\n\n\n\n\n\nData Sources\n\n\n\n\n\nData Sources\n\n\n\n\n\nMotivation/Goals\n\n\n\n\n\nslide\n\n\n\n\n\nHow it Works?\n\n\n\n\n\nCross Encoder architecture\n\n\n\n\n\nCross Encoder architecture\n\n\n\n\n\nBi-Encoder architecture\n\n\n\n\n\nBi-Encoder architecture\n\n\n\n\n\nBi-Encoder architecture\n\n\n\n\n\nBi-Encoder architecture\n\n\n\n\n\nBi-Encoder self-supervised\n\n\n\n\n\nMain Differences\n\n\n\n\n\nDynamic Padding\n\n\n\n\n\nDynamic Padding\n\n\n\n\n\nDynamic Padding\n\n\n\n\n\nEvaluation\n\n\n\n\n\nResults\n\n\n\n\n\nMetrics\n\n\n\n\n\nResults\n\n\n\nnote Muse-large used as a baseline!\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\nWell done! They did the experiment way past the point where the effects maxed. The main takeaway here is that 100 docs suffice for getting good results on a new topic.\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\nGreat talk - the padding tip is probably worth the price of admission :-)",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 28 NLP.IL Booking.com",
      "Text2topic  Leverage reviews data for multi-label topics classification in Booking.com"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Text2Topic.html#abstract",
    "href": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Text2Topic.html#abstract",
    "title": "Text2topic Leverage reviews data for multi-label topics classification in Booking.com",
    "section": "",
    "text": "Having millions of customer reviews, we would like to better understand them and leverage this data for different use cases. For example, finding popular activities per destination, detecting popular facilities per property, allowing the users to filter reviews by specific topics, detecting violence in reviews and summarizing most discussed topics per property.\nIn this talk, we will present how we build a multilingual multi-label topic classification model that supports zero-shot, to match reviews with unseen users‚Äô search topics.\nWe will show how fine-tuning BERT-like models on the tourism domain with a small dataset can outperform other pre-trained models and will share experiment results of different architectures.\nFurthermore, we will present how we collected the data using an active learning approach and AWS Sagemaker ground truth tool, and we will show a short demo of the model with explainability using Streamlit.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 28 NLP.IL Booking.com",
      "Text2topic  Leverage reviews data for multi-label topics classification in Booking.com"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Text2Topic.html#moran-beladev-bio",
    "href": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Text2Topic.html#moran-beladev-bio",
    "title": "Text2topic Leverage reviews data for multi-label topics classification in Booking.com",
    "section": "",
    "text": "Moran is a machine learning manager at booking.com, researching and developing computer vision and NLP models for the tourism domain. Moran is a Ph.D candidate in information systems engineering at Ben Gurion University, researching NLP aspects in temporal graphs. Previously worked as a Data Science Team Leader at Diagnostic Robotics, building ML solutions for the medical domain and NLP algorithms to extract clinical entities from medical visit summaries.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 28 NLP.IL Booking.com",
      "Text2topic  Leverage reviews data for multi-label topics classification in Booking.com"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Text2Topic.html#slides",
    "href": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Text2Topic.html#slides",
    "title": "Text2topic Leverage reviews data for multi-label topics classification in Booking.com",
    "section": "",
    "text": "slide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nWhat is CIP\n\n\n\n\n\nWhat is CIP\n\n\n\n\n\nText2Topic\n\n\n\n\n\nOverview\n\n\n\n\n\nData Sources\n\n\n\n\n\nData Sources\n\n\n\n\n\nData Sources\n\n\n\n\n\nData Sources\n\n\n\n\n\nData Sources\n\n\n\n\n\nMotivation/Goals\n\n\n\n\n\nslide\n\n\n\n\n\nHow it Works?\n\n\n\n\n\nCross Encoder architecture\n\n\n\n\n\nCross Encoder architecture\n\n\n\n\n\nBi-Encoder architecture\n\n\n\n\n\nBi-Encoder architecture\n\n\n\n\n\nBi-Encoder architecture\n\n\n\n\n\nBi-Encoder architecture\n\n\n\n\n\nBi-Encoder self-supervised\n\n\n\n\n\nMain Differences\n\n\n\n\n\nDynamic Padding\n\n\n\n\n\nDynamic Padding\n\n\n\n\n\nDynamic Padding\n\n\n\n\n\nEvaluation\n\n\n\n\n\nResults\n\n\n\n\n\nMetrics\n\n\n\n\n\nResults\n\n\n\nnote Muse-large used as a baseline!\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\nWell done! They did the experiment way past the point where the effects maxed. The main takeaway here is that 100 docs suffice for getting good results on a new topic.\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\nGreat talk - the padding tip is probably worth the price of admission :-)",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 28 NLP.IL Booking.com",
      "Text2topic  Leverage reviews data for multi-label topics classification in Booking.com"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html",
    "title": "replay buffer questions",
    "section": "",
    "text": "for continuous environment we should think about coverage.\n\n\ngiven a paramertrization of the value function, for a level of generalization/discrimination we get an induced set of features. Is some set of experiences sufficent to do prediction or control.\nif we have an estimate of the coverage can we use it to place a bound on the error of the value function.\ncan we do better if we also have an estimate \\mu(s) of the importance/long term probability of the states ?\n\n\nTraces present a highly correlated view of the state space.\n\n\nHow much do we need to wory about this.\n\n\ndoes replay buffer violate markov state.?\n\n\naccording to Shirli Di-Castro Shashua\n\nAnalysis of Stochastic Processes through Replay Buffers\nSim and Real: Better Together\nthe storage operation preserves the markov property\nthe sampling operation preserves the markov property\nthe mean operation om the replay buffer violates the markov property‚Ä¶\n\n\n\ncan reduce correlation between samples ?\ncan we be more stategic about what we keep in the RB\n\n\nsay we have a key using a hash[\\delta(state), action] neighbourhood\n\nwe can use the key to decide if to insert/replace the current buffer\nwe can use it to decide what to discard\n\nwe can use the buffer to estimate mu(s)\n\nmight also have more info like states we did not insert or deleted.\nif we also have mu(mu) - the state importance to decide what to keep\n\ndo we prefer complete recent traces or many partial traces.\n\n\nCan we use options/skills to orgenize the buffer more effectively ?\n\n\nwe should aim to keep full options traces in the buffer\nkeep traces in & out or options.\nbefore and after the options.\n\nThink of the four room environment - there are different options to get from one room to another. they are composable. Once we have good coverage entry into the op",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "replay buffer questions"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html#replay-buffer",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html#replay-buffer",
    "title": "replay buffer questions",
    "section": "",
    "text": "for continuous environment we should think about coverage.\n\n\ngiven a paramertrization of the value function, for a level of generalization/discrimination we get an induced set of features. Is some set of experiences sufficent to do prediction or control.\nif we have an estimate of the coverage can we use it to place a bound on the error of the value function.\ncan we do better if we also have an estimate \\mu(s) of the importance/long term probability of the states ?\n\n\nTraces present a highly correlated view of the state space.\n\n\nHow much do we need to wory about this.\n\n\ndoes replay buffer violate markov state.?\n\n\naccording to Shirli Di-Castro Shashua\n\nAnalysis of Stochastic Processes through Replay Buffers\nSim and Real: Better Together\nthe storage operation preserves the markov property\nthe sampling operation preserves the markov property\nthe mean operation om the replay buffer violates the markov property‚Ä¶\n\n\n\ncan reduce correlation between samples ?\ncan we be more stategic about what we keep in the RB\n\n\nsay we have a key using a hash[\\delta(state), action] neighbourhood\n\nwe can use the key to decide if to insert/replace the current buffer\nwe can use it to decide what to discard\n\nwe can use the buffer to estimate mu(s)\n\nmight also have more info like states we did not insert or deleted.\nif we also have mu(mu) - the state importance to decide what to keep\n\ndo we prefer complete recent traces or many partial traces.\n\n\nCan we use options/skills to orgenize the buffer more effectively ?\n\n\nwe should aim to keep full options traces in the buffer\nkeep traces in & out or options.\nbefore and after the options.\n\nThink of the four room environment - there are different options to get from one room to another. they are composable. Once we have good coverage entry into the op",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "replay buffer questions"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html#ergodicity",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html#ergodicity",
    "title": "replay buffer questions",
    "section": "Ergodicity",
    "text": "Ergodicity\n\nin an environment is a maze and I have a one way door dividing the left side from the right parts of the maze. is this environment ergodic ?\nIf not how come we can still learn the optimal policy ?\n\ninterchip dotan castro - sim to real",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "replay buffer questions"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html#replay-buffers--",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-replay-buffer-questions.html#replay-buffers--",
    "title": "replay buffer questions",
    "section": "Replay buffers -",
    "text": "Replay buffers -\n\nstoring sequence of states\nState action state\n\nPMDPs",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "replay buffer questions"
    ]
  },
  {
    "objectID": "posts/2024/2023-03-16-events-generator/index.html",
    "href": "posts/2024/2023-03-16-events-generator/index.html",
    "title": "event generator",
    "section": "",
    "text": "Create Simulated data sets",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "event generator"
    ]
  },
  {
    "objectID": "posts/2024/2023-03-16-events-generator/index.html#goal",
    "href": "posts/2024/2023-03-16-events-generator/index.html#goal",
    "title": "event generator",
    "section": "",
    "text": "Create Simulated data sets",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "event generator"
    ]
  },
  {
    "objectID": "posts/2024/2023-03-16-events-generator/index.html#steps",
    "href": "posts/2024/2023-03-16-events-generator/index.html#steps",
    "title": "event generator",
    "section": "steps:",
    "text": "steps:\n\nbreak down simulation into blocks\nsimulate each block into a csv\nsimulate an event stream\nsimulate a graph\nput on s3\nput in deltalake\nprocess with spark",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "event generator"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-05-complex-signal/index.html",
    "href": "posts/2024/2024-05-05-complex-signal/index.html",
    "title": "ad hoc complex signaling systems",
    "section": "",
    "text": "TL-DR Emergent Languages In a Nutshell\n\n\n\n\n\n\nEmergent Languages In a Nutshell\nRather them consider how complex signaling systems evolve from a lewis signaling game plus some modifications it might be worth while to better understand some complex signaling systems.\nEssentially One would equip the agents with a set of complex signals and see if they can acquire more powerful signaling system to communicate more effectively.\nThis should allow us to quantify:",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "ad hoc complex signaling systems"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-05-complex-signal/index.html#logical-aggregation",
    "href": "posts/2024/2024-05-05-complex-signal/index.html#logical-aggregation",
    "title": "ad hoc complex signaling systems",
    "section": "Logical Aggregation",
    "text": "Logical Aggregation",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "ad hoc complex signaling systems"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-05-complex-signal/index.html#operators",
    "href": "posts/2024/2024-05-05-complex-signal/index.html#operators",
    "title": "ad hoc complex signaling systems",
    "section": "operators",
    "text": "operators",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "ad hoc complex signaling systems"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-05-complex-signal/index.html#learning-to-negate",
    "href": "posts/2024/2024-05-05-complex-signal/index.html#learning-to-negate",
    "title": "ad hoc complex signaling systems",
    "section": "Learning to negate:",
    "text": "Learning to negate:\nI suppose there are many ways to learn to negate. Let‚Äôs consider two\n\nin English. We use the word ‚ÄòNot‚Äô.\nin logic we use the symbol \\neg.\nin python we use the keyword ‚Äònot‚Äô\nin hungarian we use the word ‚Äònem‚Äô\n\nNot in all three cases a unitary operator that takes a single argument and returns the opposite of that argument.\nWe can use it to map the next signal to some other unique signal. This is how a unitary prefix operator works. For us though not means something more than some other signal it means all the other options. Not red means all the other colors, not cat means all the other animals. So the semantics we would like to capture requires that there are categories of signals and that the negation operator maps to the rest of the category. This is a handful. Also note that the categories may be defined as partial pooling equilibria.\nlet‚Äôs imagine that a group of Marmoset monkeys need to signal predators. The state space describes the predators are based on a product of the following features:\ntemporal : imminent, near, medium, distant type: cat, snake, pirana, eagle direction_theta: 0 1 2 3 position_phi: 0 1 2 3 number: 1, 2, 3, more\nyes they use solid coordinates to describe to location of the predators.\nthis gives us 4^4 = 256 states.\nthat‚Äôs a lot of signals. but a complex signaling system could be able to communicate about all of them.\nIf the monkeys use a template with 4 parts to communicate about the predators then they can use just four signals.\nalso the 4 signals share common semantics of increasing values. for the animals the threat level might be used to name them ‚Ä¶\n\nstates St_0:St_{2M}\nlew_primitives = Sig_0:Sig_{2N} indicating 0‚Ä¶n and nor 0 ‚Ä¶ not n.\nneg_primitives = NOT, sig_0:sig_{N}\nprefix coding negation = &lt;NOT, neg_primitives&gt; = Sig_{n+N}\nsuffix coding negation = &lt;neg_primitives, NOT&gt;\nprefix protocol\nIn this case we don‚Äôt have a clear benefit of suffix and prefix. but later we will see how prefix coding is a fit for the desiderata of complex signaling systems.\nlet‚Äôs consider a 2 state with negation.\nin the lewis game we have 2 signals 0 and 1.\nin the negation_system,\nThe semantics of negation (its meaning) can be defined as we are use to i.e.¬†no 1 mean 0 and no 0 means 1. But in this case we don‚Äôt get any benefit from the negation, we just get a system with longer signals. we can interpret it as a trick we learn to double the number of symbols we can use.\n\nnow consider a 4 symbol system with negation.\n\nA conjunctive signaling system\nA disjunctive signaling system\nA signaling system with conjunctions and disjunctions\nSignaling with Run-length encoding\nSignaling with Prefix-codes",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "ad hoc complex signaling systems"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-05-complex-signal/index.html#morphology",
    "href": "posts/2024/2024-05-05-complex-signal/index.html#morphology",
    "title": "ad hoc complex signaling systems",
    "section": "Morphology",
    "text": "Morphology\n\nA signaling system with a morphological template",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "ad hoc complex signaling systems"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-05-complex-signal/index.html#syntax",
    "href": "posts/2024/2024-05-05-complex-signal/index.html#syntax",
    "title": "ad hoc complex signaling systems",
    "section": "Syntax",
    "text": "Syntax\n\nA signaling system with a syntactic template\nSignaling system with a multiple templates\nSignaling system with a multiple templates",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "ad hoc complex signaling systems"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-05-complex-signal/index.html#sequence-aggregation",
    "href": "posts/2024/2024-05-05-complex-signal/index.html#sequence-aggregation",
    "title": "ad hoc complex signaling systems",
    "section": "Sequence Aggregation",
    "text": "Sequence Aggregation\n\nA Sequential signaling system with n signals\nA matrix signaling system\nTemplate signaling system",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "ad hoc complex signaling systems"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-09-RE-RL/re-rel.html",
    "href": "posts/2024/2024-05-09-RE-RL/re-rel.html",
    "title": "Roth Erev learning in Lewis signaling games",
    "section": "",
    "text": "I wish now to implement learning in the Lewis signaling game. In the book some reinforcement learning RL algorithms are presented in some detail and a few variations are mentioned. It worthwhile pointing out that the book statement of the algorithms is good enough to understand how the algorithms operate in general. However some of the details required to implement the algorithms were glossed over. As my one time collage Yuri Stool like to point out, ‚Äúthe devil is in the details.‚Äù\nI ended up implementing the algorithms a number of times - once to get it to work, second time to develop my own algorithm as I gained new insights into the problems. A third time after reading more of the papers whihc suggested how more details on conducting experiments which led to a deeper understanding of enumerating and ranking the partial pooling equilibria. The point here is that natural language is mostly a separating equilibrium - most words are unambiguous but there are a significant subset of words that have multiple meaning and there are many synonyms. Mechanisms in the lexicon seem to eventually resolves some ambiguities while letting others persist indefinitely. So while the separating equilibria are of primary interests in reality users if signaling systems satisfice with a systems that is good enough. This are the much more common partial pooling variants with high degree of separation plus a context based disambiguation mechanism. I consider the erosion of English and Latin conjugation and declination after the classical period as a simpler contextual disambiguation mechanism dismantling a nearly perfect signaling subsystem with a rather degenerate one with high degree of partial pooling. A simulation might show how a few prepositions and auxilary verbs are more efficent to learn and process than fully inflected systems of case and verb ending (especially if modified by phonetics). But my guess is that this happened as more speakers had to master an use a core language, without access to resources for learning the classical forms. I guess the dark ages and a decline in literacy likely speed up the process.\nAdding better analysis, estimating expected returns for a set of weights, tracking regret during learning. Considering different path to salience via differntial risks/costs for signals, and non uniform state distribution.\nThe big question seems to be:\nWhat is a simple rl algorithm to evolve and disseminate a signaling system with certain added requirements like\n\ncomplex signals\n\nconjunctive signal aggregation\nordered signal aggregation via learning a grammar like SVO.\nrecursive signal aggregation replacing linear ordered with a partial order.\n\nresolving ambiguity by context\nmechanism for correcting errors (vowel harmony, agreement)\nsimple growth of the lexicon (black bead leads to mutation in the urn model)\nsufficient capacity,\nminimal burden for processing (extending inference mechanism to reduce cognitive load, compress messages, erode unneeded structures)\nminimal burden in learning (e.g.¬†by generalization via regularity in morphology, and syntax)\nhigh accuracy for transmission of messages\nsaliencey - a information theoretic measure of more efficient transition subset of states/messages pairs.\n\nWhere the great unknown seems to be to find a minimal extension to the Lewis game in which all these might evlove.\nHaving stated the problem in detail lets me make the following two observations:\n\nThe aggregation rules for complex signaling should be made to arise by imposing costs on systems under which agents more frequently fail to make good inference with high probability of a partials message‚Äôs describing risky states for sender and or receiver.\nA second cost to fitness is the role of mistakes in signaling and or receiving. (ie. adding an small chance for decoding similar sounding signals (homophones, short vs long sounds, hissed and hushed, round, front and back vouwels). This may lead to excluding simple signals from places they might be confused, is it (a,a) (a.a) or (aa,a), (a,_,a) are avoided if signal ‚Äòa‚Äô is excluded from the first positions (say verb class). here dot might be a short pause, comma a long pause, undescore an unmarked slot, and two aa no pause. (either two a or a long a.) if we prefix V with v S with s and P with C\nwe end up with a system that is much more robust. And we may have the added bonus that we can easily detect a tree formation based on multiple Vprefix in the sentence‚Ä¶.\n\nword grammar\nsub word grammar - a complex morphology - highly regular yet differented complex signals\nthis could lead to redundancy based Error correction like subject verb agreement, noun adjective agreement or vowel harmony.\nConcord - case agreement (nouns pronouns and adjective are in agreement)\n\nEase of processing\n\nagreement can also ease processing\nassimilation and elision\nlimiting processing/disabihation context windows.\nword order\nhowever redundencies add overhead, making signals longer and may make learning much longer (this is when we students who generelize are wrong and then need to learn via negative examples.\n\nIf many we have different complex signaling systems with minimal mistakes are possible one would prefer a system that is easier to learn. (Shorter lexicon, with lower chances of collision. Shorter grammar, fewer negtive examples, more room for expansion)\n\n\n\n\nwe start with some initial weights, perhaps equal.\nAn act is chosen with probability proportional to its weight.\nThe payoff gained is added to the weight for the act that was chosen,\nand the process repeats\n\n\n\n\n\nset starting weight for each option\nweights evolve by addition of rewards gotten\nprobability of choosing an alternative is proportional to its weight.\n\n\n\n\n\nset starting weight for each option\nweights evolve by addition of rewards gotten\nprobability of choosing an alternative is proportional to its weight.\nif the reward is 0 the weight is multiplied by a forgetting factor.\n\n\n\n\n\nset starting weight for each option\nweights evolve by addition of rewards gotten\nprobability of choosing an alternative is proportional to its weight.\nif the reward is 0 the weight is multiplied by a forgetting factor.\n\n\n\n\n\n\n\nthis is a special purpose rl algorithm for coordination problems where agents need to establish a convention like in the Lewis signaling game. The idea is that the matrix is similar to a placing 8 rooks on on a chess board with no two under attack. In this case once an option has been chosen we want to exclude all options that shares a row or a collumm. So we set to zero any weights which share the same prefix or suffix as a reward 1 option.\n\nset starting weight for each option (state_signal) for the sender and (signal_action) for the receiver, perhaps to 1\nweights evolve by\n\n\naddition of rewards gotten for a correct choice and\nzeroing of options with the same prefix or suffix to exclude them from the choice set.\n\n\nprobability of choosing an alternative is proportional to its weight.\n\n\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom abc import ABC, abstractmethod\n\n# let's define a lambda to take a list of options and intilize the weights uniformly \nuniform_init = lambda options, w : {option: w for option in options}\nrandom_init  = lambda options, w : {option: random.uniform(0,1) for option in options}\n\n# lets make LeaningRule an abstract class with all the methods that are common to all learning rules\n# then we can subclass it to implement the specific learning rules\nclass LearningRule(ABC):\n  \n    def __init__(self, options, learning_rate=0.1,verbose=False,name='LearningRule',init_weight=uniform_init):\n        self.verbose = verbose\n        self.name=name\n        self.learning_rate = learning_rate\n        if self.verbose:\n          print(f'LearningRule.__init__(Options: {options})')\n        self.options = options\n        self.weights = init_weight(options,1.0) # Start with one ball per option \n        \n        \n    def get_filtered_weights(self, filter):\n        if self.verbose:\n          print(f'get_filtered_weights({filter=})')\n        # if filter is int convert to string\n        if isinstance(filter, int):\n            filter = str(filter)\n        filter_keys = [k for k in self.weights.keys() if k.startswith(filter)]\n        weights = {opt: self.weights[opt] for opt in filter_keys}\n        return weights\n      \n    @abstractmethod\n    def choose_option(self,filter):\n        pass\n      \n    @abstractmethod\n    def update_weights(self, option, reward):\n        pass\n      \nclass HerrnsteinRL(LearningRule):\n    '''\n                                    The Urn model\n     nature            sender                 reciever     reward\n                       \n    | (0) | --{0}--&gt;  | (0_a)  | --{a}--&gt; | (a_0) | --{0}--&gt;   1   \n    |     |           | (0_b)  | --{b}    | (a_1) | --{1}--&gt;   0\n    |     |           +--------+    | +--&gt;+-------+\n    |     |                         +-|-+  \n    | (1) | --{1}--&gt;  | (1_a)  | --{a}+ +&gt;| (b_0) | --{1}--&gt;   1\n    |     |           | (1_b)  | --{b}---&gt;| (b_1) | --{0}--&gt;   0\n    +-----+           +--------+          +-------+\n    \n    \n    Herrnstein urn algorithm\n    ------------------------\n    \n    1. nature picks a state \n    2. sender  gets the state, chooses a signal by picking a ball in choose_option() from the stat'es urn\n    3. reciver gets the action, chooses an actuion by picking a ball in choose_option()\n    4. the balls in the urns are incremented if action == state\n    5. repeat\n    \n    '''\n    def __init__(self, options, learning_rate=1.0,verbose=False,name='Herrnstein matching law'):\n      \n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n    def update_weights(self, option, reward):\n      \n        old_weight = self.weights[option]\n        self.weights[option] += self.learning_rate * reward \n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_weight} -&gt; {self.weights[option]}\")\n          \n    def choose_option(self,filter):\n      \n        '''\n        \n        '''\n        # subseting the weights by the filter simulates different urns per state or signal\n        weights = self.get_filtered_weights(filter)\n\n        # calculate their probabilities then\n        total = sum(weights.values())\n        assert total &gt; 0.0, f\"total weights is {total=} after {filter=} on {self.weights} \"\n        probabilities = [weights[opt] / total for opt in weights]\n        # then drawn an option from the filtered option using the probabilities\n        return np.random.choice(list(weights.keys()), p=probabilities)\n\n\nclass RothErevRL(LearningRule):\n\n    def __init__(self, options, learning_rate=0.1,verbose=False,name='Roth Erev RL'):\n      \n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n    def update_weights(self, option, reward):\n        old_weight = self.weights[option]\n        if reward == 1:\n          self.weights[option] += self.learning_rate * reward\n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_weight} -&gt; {self.weights[option]}\")\n\n    def choose_option(self,filter):\n        # we subset the weights by the filter, calculate their probabilities then\n        # then drawn an option from the filtered option using the probabilities\n        weights = self.get_filtered_weights(filter)\n        total = sum(weights.values())\n        probabilities = [weights[opt] / total for opt in weights]\n        return np.random.choice(list(weights.keys()), p=probabilities)\n  \n\nclass RothErevForget_RL(LearningRule):\n\n    def __init__(self, options, learning_rate=0.1,verbose=False,name='Roth Erev with forgetting'):\n      \n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n    def update_weights(self, option, reward):\n        old_weight = self.weights[option]\n        if reward == 1:\n          self.weights[option] += self.learning_rate * reward\n        else:\n          self.weights[option] *= self.learning_rate \n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_weight} -&gt; {self.weights[option]}\")\n\n    def choose_option(self,filter):\n        weights = self.get_filtered_weights(filter)\n        total = sum(weights.values())\n        probabilities = [weights[opt] / total for opt in weights]\n        return np.random.choice(list(weights.keys()), p=probabilities)\n  \nclass EightRooksRL(LearningRule):\n    def __init__(self, options, learning_rate=0.1,verbose=False,name='Eight Rooks RL'):\n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n\n    def update_weights(self, option, reward):\n        self.prefix = option.split('_')[0]\n        self.suffix = option.split('_')[1]\n        \n        old_weights=self.weights.copy()\n        \n        for test_option in self.options:\n          if reward == 1:\n            if test_option == option:\n            # increment the weight of the good option \n              self.weights[test_option] += self.learning_rate * reward\n            elif test_option.startswith(self.prefix) or test_option.endswith(self.suffix) :\n            # decrement all other options with same prefix  or suffix\n               # if self.weights[test_option] &lt; 0.000001:\n               #   self.weights[test_option] = 0.0\n               # else:\n                self.weights[test_option] *= self.learning_rate \n          # elif test_option == option:\n          #   # decrement the weights of the bad option combo\n          #   self.weights[option] *= self.learning_rate \n\n        if self.verbose:\n          print()\n          for option in self.options:\n            if old_weights[option] != self.weights[option]:\n              print(f\"{option}: weight {old_weights[option]} -&gt; {self.weights[option]}\")\n          #print(f\"Updated weight {old_weights} -&gt; {self.weights}\")\n\n\n    def choose_option(self,filter):\n        weights = self.get_filtered_weights(filter)\n        total = sum(weights.values())\n        probabilities = [weights[opt] / total for opt in weights]\n        # if there is a max weight return it otherwise return a random option from the max wights\n        if len([opt for opt in weights if weights[opt]==max(weights.values())]) == 1:\n          return max(weights, key=weights.get)\n        else:\n          return np.random.choice([opt for opt in weights if weights[opt]==max(weights.values())])\n\nclass LewisAgent(Agent):\n    def __init__(self, unique_id, model, learning_options, learning_rule, verbose=False):\n        super().__init__(unique_id, model)\n        self.message = None\n        self.action = None\n        self.reward = 0\n        self.learning_rule = learning_rule\n        self.verbose = verbose\n        \n    def send(self):\n      return\n    \n    def receive(self):\n      return\n    \n    def calc_reward(self):\n      return\n\n    def set_reward(self):\n        self.reward = self.model.reward\n        if self.verbose:\n          print(f\"Agent {self.unique_id} received reward: {self.reward}\")\n        \n    def update_learning(self):\n        self.learning_rule.update_weights(self.option, self.reward)  # Update weights based on signals and rewards        \n\nclass Sender(LewisAgent):\n    def send(self):\n        state = self.model.get_state()\n        #self.message = self.learning_rule.choose_option(filter=state)  # Send a signal based on the learned weights\n        \n        self.option = self.learning_rule.choose_option(filter=state)  # Send a signal based on the learned weights\n        self.message = self.option.split('_')[1]\n        if self.verbose:\n          print(f\"Sender {self.unique_id} sends signal for state {state}: {self.message}\")\n\nclass Receiver(LewisAgent):\n    def receive(self):\n        self.received_signals = [sender.message for sender in self.model.senders]  # Receive signals from all senders\n        #print(f\"Receiver {self.unique_id} receives signals: {self.received_signals}\")\n        if self.received_signals:\n            for signal in self.received_signals:\n                self.option = self.learning_rule.choose_option(filter=signal)  # Choose an action based on received signals and learned weights\n                self.action = int(self.option.split('_')[1])\n                if self.verbose:\n                  print(f\"Receiver {self.unique_id} receives signals: {self.received_signals} and chooses action: {self.action}\")\n\n\n    def calc_reward(self):\n        correct_action = self.model.current_state\n        self.model.reward = 1 if self.action == correct_action else 0\n        if self.verbose:\n          print(f\"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}\")\n\nclass SignalingGame(Model):\n    def __init__(self, \n                senders_count=1, \n                receivers_count=1, k=3,\n                learning_rule=LearningRule,\n                learning_rate=0.1,\n                verbose=False):\n        super().__init__()\n        self.verbose = verbose\n        self.k = k\n        self.current_state = None\n        self.learning_rate=learning_rate\n\n        # Initialize the states, signals, and actions mapping\n        self.states = list(range(k))  # States are simply numbers\n        self.signals = list(chr(65 + i) for i in range(k))  # Signals are characters\n        self.actions = list(range(k))  # Actions are simply numbers\n\n        # generate a list of state_signal keys for the sender's weights\n        self.states_signals_keys = [f'{state}_{signal}' for state in self.states for signal in self.signals]\n        # generate a list of signal_action keys for the receiver's weights\n        self.signals_actions_keys = [f'{signal}_{action}' for signal in self.signals for action in self.actions]\n        \n        self.senders = [Sender(i, self, learning_options=self.states_signals_keys, \n                                  learning_rule=learning_rule(self.states_signals_keys, self.learning_rate,verbose=self.verbose)\n                              ) for i in range(senders_count)]\n        self.receivers = [Receiver(i + senders_count, self, learning_options=self.signals_actions_keys, \n                                  learning_rule=learning_rule(self.signals_actions_keys, self.learning_rate,verbose=self.verbose)\n                              ) for i in range(receivers_count)]\n        \n        self.schedule = StagedActivation(self, \n          agents = self.senders + self.receivers, \n          stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])\n\n    def get_state(self):\n        return random.choice(self.states)\n\n    def step(self):\n        self.current_state = self.get_state()\n        if self.verbose:\n          print(f\"Current state of the world: {self.current_state}\")\n        self.schedule.step()\n\n\n# function to plot agent weights side by side\n\ndef plot_weights(sender,reciver,title='Agent'):\n    fig, ax = plt.subplots(1,2,figsize=(9,5))\n    weights = sender.learning_rule.weights\n    ax[0].bar(weights.keys(), weights.values())\n    ax[0].set_xlabel('Options')\n    ax[0].set_ylabel('Weights')\n    ax[0].set_title(f'Sender {sender.unique_id} weights: {title}')\n    \n    weights = reciver.learning_rule.weights\n    ax[1].bar(weights.keys(), weights.values())\n    ax[1].set_xlabel('Options')\n    ax[1].set_ylabel('Weights')\n    ax[1].set_title(f'Receiver {reciver.unique_id} weights: {title}')\n    plt.show()\n\n\n# Running the model\nk=2\nverbose = False\nfor LR in [HerrnsteinRL,\n           RothErevRL,\n           RothErevForget_RL,\n           EightRooksRL\n           ]:\n  print(f\"--- {LR.__name__} ---\")\n  if LR == HerrnsteinRL:\n    learning_rate=1.\n  else:\n    learning_rate=.1\n  model = SignalingGame(senders_count=1, receivers_count=1, k=k, learning_rule=LR,learning_rate=learning_rate,verbose=verbose)\n  for i in range(10000):\n      if verbose:\n        print(f\"--- Step {i+1} ---\")\n      model.step()\n      # \n      #print the agent weights\n  #print('Sender weights:',model.senders[0].learning_rule.weights)\n  # plot weights side by side\n  \n  plot_weights(model.senders[0],model.receivers[0],title=LR.__name__)\n  #print('Receiver weights:',model.receivers[0].learning_rule.weights)\n  #plot_weights(model.receivers[0],title=LR.__name__)\n\n--- HerrnsteinRL ---\n--- RothErevRL ---\n--- RothErevForget_RL ---\n--- EightRooksRL ---\n\n\n/home/oren/work/blog/env/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:\n\nThe AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.\nWe would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncurrently only the eight rooks learning rule is producing consistently good signaling systems. The other learning rules are not learning to signal correctly.\nPlease suggest how to fix this - according to the literature the Roth-Erev with forgetting learning rule should work well in this case.\nTODO: implement Bush-Mosteller learning - as this is a match for population dynamics.\nTODO: also implement population dynamics as it may not be clear that BM RL is a perfect fit for population dynamics under all lewis game conditions.\nTODO: implement ARP learning.\nTODO: implement epsilon-greedy, UCB and thompson sampling urn schemes, and Contextual bandits associative search (that‚Äôs our multiurn bandit)",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 05 09 RE RL",
      "Roth Erev learning in Lewis signaling games"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-09-RE-RL/re-rel.html#learning-in-lewis-signaling-games",
    "href": "posts/2024/2024-05-09-RE-RL/re-rel.html#learning-in-lewis-signaling-games",
    "title": "Roth Erev learning in Lewis signaling games",
    "section": "",
    "text": "I wish now to implement learning in the Lewis signaling game. In the book some reinforcement learning RL algorithms are presented in some detail and a few variations are mentioned. It worthwhile pointing out that the book statement of the algorithms is good enough to understand how the algorithms operate in general. However some of the details required to implement the algorithms were glossed over. As my one time collage Yuri Stool like to point out, ‚Äúthe devil is in the details.‚Äù\nI ended up implementing the algorithms a number of times - once to get it to work, second time to develop my own algorithm as I gained new insights into the problems. A third time after reading more of the papers whihc suggested how more details on conducting experiments which led to a deeper understanding of enumerating and ranking the partial pooling equilibria. The point here is that natural language is mostly a separating equilibrium - most words are unambiguous but there are a significant subset of words that have multiple meaning and there are many synonyms. Mechanisms in the lexicon seem to eventually resolves some ambiguities while letting others persist indefinitely. So while the separating equilibria are of primary interests in reality users if signaling systems satisfice with a systems that is good enough. This are the much more common partial pooling variants with high degree of separation plus a context based disambiguation mechanism. I consider the erosion of English and Latin conjugation and declination after the classical period as a simpler contextual disambiguation mechanism dismantling a nearly perfect signaling subsystem with a rather degenerate one with high degree of partial pooling. A simulation might show how a few prepositions and auxilary verbs are more efficent to learn and process than fully inflected systems of case and verb ending (especially if modified by phonetics). But my guess is that this happened as more speakers had to master an use a core language, without access to resources for learning the classical forms. I guess the dark ages and a decline in literacy likely speed up the process.\nAdding better analysis, estimating expected returns for a set of weights, tracking regret during learning. Considering different path to salience via differntial risks/costs for signals, and non uniform state distribution.\nThe big question seems to be:\nWhat is a simple rl algorithm to evolve and disseminate a signaling system with certain added requirements like\n\ncomplex signals\n\nconjunctive signal aggregation\nordered signal aggregation via learning a grammar like SVO.\nrecursive signal aggregation replacing linear ordered with a partial order.\n\nresolving ambiguity by context\nmechanism for correcting errors (vowel harmony, agreement)\nsimple growth of the lexicon (black bead leads to mutation in the urn model)\nsufficient capacity,\nminimal burden for processing (extending inference mechanism to reduce cognitive load, compress messages, erode unneeded structures)\nminimal burden in learning (e.g.¬†by generalization via regularity in morphology, and syntax)\nhigh accuracy for transmission of messages\nsaliencey - a information theoretic measure of more efficient transition subset of states/messages pairs.\n\nWhere the great unknown seems to be to find a minimal extension to the Lewis game in which all these might evlove.\nHaving stated the problem in detail lets me make the following two observations:\n\nThe aggregation rules for complex signaling should be made to arise by imposing costs on systems under which agents more frequently fail to make good inference with high probability of a partials message‚Äôs describing risky states for sender and or receiver.\nA second cost to fitness is the role of mistakes in signaling and or receiving. (ie. adding an small chance for decoding similar sounding signals (homophones, short vs long sounds, hissed and hushed, round, front and back vouwels). This may lead to excluding simple signals from places they might be confused, is it (a,a) (a.a) or (aa,a), (a,_,a) are avoided if signal ‚Äòa‚Äô is excluded from the first positions (say verb class). here dot might be a short pause, comma a long pause, undescore an unmarked slot, and two aa no pause. (either two a or a long a.) if we prefix V with v S with s and P with C\nwe end up with a system that is much more robust. And we may have the added bonus that we can easily detect a tree formation based on multiple Vprefix in the sentence‚Ä¶.\n\nword grammar\nsub word grammar - a complex morphology - highly regular yet differented complex signals\nthis could lead to redundancy based Error correction like subject verb agreement, noun adjective agreement or vowel harmony.\nConcord - case agreement (nouns pronouns and adjective are in agreement)\n\nEase of processing\n\nagreement can also ease processing\nassimilation and elision\nlimiting processing/disabihation context windows.\nword order\nhowever redundencies add overhead, making signals longer and may make learning much longer (this is when we students who generelize are wrong and then need to learn via negative examples.\n\nIf many we have different complex signaling systems with minimal mistakes are possible one would prefer a system that is easier to learn. (Shorter lexicon, with lower chances of collision. Shorter grammar, fewer negtive examples, more room for expansion)\n\n\n\n\nwe start with some initial weights, perhaps equal.\nAn act is chosen with probability proportional to its weight.\nThe payoff gained is added to the weight for the act that was chosen,\nand the process repeats\n\n\n\n\n\nset starting weight for each option\nweights evolve by addition of rewards gotten\nprobability of choosing an alternative is proportional to its weight.\n\n\n\n\n\nset starting weight for each option\nweights evolve by addition of rewards gotten\nprobability of choosing an alternative is proportional to its weight.\nif the reward is 0 the weight is multiplied by a forgetting factor.\n\n\n\n\n\nset starting weight for each option\nweights evolve by addition of rewards gotten\nprobability of choosing an alternative is proportional to its weight.\nif the reward is 0 the weight is multiplied by a forgetting factor.\n\n\n\n\n\n\n\nthis is a special purpose rl algorithm for coordination problems where agents need to establish a convention like in the Lewis signaling game. The idea is that the matrix is similar to a placing 8 rooks on on a chess board with no two under attack. In this case once an option has been chosen we want to exclude all options that shares a row or a collumm. So we set to zero any weights which share the same prefix or suffix as a reward 1 option.\n\nset starting weight for each option (state_signal) for the sender and (signal_action) for the receiver, perhaps to 1\nweights evolve by\n\n\naddition of rewards gotten for a correct choice and\nzeroing of options with the same prefix or suffix to exclude them from the choice set.\n\n\nprobability of choosing an alternative is proportional to its weight.\n\n\nfrom mesa import Agent, Model\nfrom mesa.time import StagedActivation\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom abc import ABC, abstractmethod\n\n# let's define a lambda to take a list of options and intilize the weights uniformly \nuniform_init = lambda options, w : {option: w for option in options}\nrandom_init  = lambda options, w : {option: random.uniform(0,1) for option in options}\n\n# lets make LeaningRule an abstract class with all the methods that are common to all learning rules\n# then we can subclass it to implement the specific learning rules\nclass LearningRule(ABC):\n  \n    def __init__(self, options, learning_rate=0.1,verbose=False,name='LearningRule',init_weight=uniform_init):\n        self.verbose = verbose\n        self.name=name\n        self.learning_rate = learning_rate\n        if self.verbose:\n          print(f'LearningRule.__init__(Options: {options})')\n        self.options = options\n        self.weights = init_weight(options,1.0) # Start with one ball per option \n        \n        \n    def get_filtered_weights(self, filter):\n        if self.verbose:\n          print(f'get_filtered_weights({filter=})')\n        # if filter is int convert to string\n        if isinstance(filter, int):\n            filter = str(filter)\n        filter_keys = [k for k in self.weights.keys() if k.startswith(filter)]\n        weights = {opt: self.weights[opt] for opt in filter_keys}\n        return weights\n      \n    @abstractmethod\n    def choose_option(self,filter):\n        pass\n      \n    @abstractmethod\n    def update_weights(self, option, reward):\n        pass\n      \nclass HerrnsteinRL(LearningRule):\n    '''\n                                    The Urn model\n     nature            sender                 reciever     reward\n                       \n    | (0) | --{0}--&gt;  | (0_a)  | --{a}--&gt; | (a_0) | --{0}--&gt;   1   \n    |     |           | (0_b)  | --{b}    | (a_1) | --{1}--&gt;   0\n    |     |           +--------+    | +--&gt;+-------+\n    |     |                         +-|-+  \n    | (1) | --{1}--&gt;  | (1_a)  | --{a}+ +&gt;| (b_0) | --{1}--&gt;   1\n    |     |           | (1_b)  | --{b}---&gt;| (b_1) | --{0}--&gt;   0\n    +-----+           +--------+          +-------+\n    \n    \n    Herrnstein urn algorithm\n    ------------------------\n    \n    1. nature picks a state \n    2. sender  gets the state, chooses a signal by picking a ball in choose_option() from the stat'es urn\n    3. reciver gets the action, chooses an actuion by picking a ball in choose_option()\n    4. the balls in the urns are incremented if action == state\n    5. repeat\n    \n    '''\n    def __init__(self, options, learning_rate=1.0,verbose=False,name='Herrnstein matching law'):\n      \n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n    def update_weights(self, option, reward):\n      \n        old_weight = self.weights[option]\n        self.weights[option] += self.learning_rate * reward \n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_weight} -&gt; {self.weights[option]}\")\n          \n    def choose_option(self,filter):\n      \n        '''\n        \n        '''\n        # subseting the weights by the filter simulates different urns per state or signal\n        weights = self.get_filtered_weights(filter)\n\n        # calculate their probabilities then\n        total = sum(weights.values())\n        assert total &gt; 0.0, f\"total weights is {total=} after {filter=} on {self.weights} \"\n        probabilities = [weights[opt] / total for opt in weights]\n        # then drawn an option from the filtered option using the probabilities\n        return np.random.choice(list(weights.keys()), p=probabilities)\n\n\nclass RothErevRL(LearningRule):\n\n    def __init__(self, options, learning_rate=0.1,verbose=False,name='Roth Erev RL'):\n      \n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n    def update_weights(self, option, reward):\n        old_weight = self.weights[option]\n        if reward == 1:\n          self.weights[option] += self.learning_rate * reward\n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_weight} -&gt; {self.weights[option]}\")\n\n    def choose_option(self,filter):\n        # we subset the weights by the filter, calculate their probabilities then\n        # then drawn an option from the filtered option using the probabilities\n        weights = self.get_filtered_weights(filter)\n        total = sum(weights.values())\n        probabilities = [weights[opt] / total for opt in weights]\n        return np.random.choice(list(weights.keys()), p=probabilities)\n  \n\nclass RothErevForget_RL(LearningRule):\n\n    def __init__(self, options, learning_rate=0.1,verbose=False,name='Roth Erev with forgetting'):\n      \n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n    def update_weights(self, option, reward):\n        old_weight = self.weights[option]\n        if reward == 1:\n          self.weights[option] += self.learning_rate * reward\n        else:\n          self.weights[option] *= self.learning_rate \n        if self.verbose:\n          print(f\"Updated weight for option {option}: {old_weight} -&gt; {self.weights[option]}\")\n\n    def choose_option(self,filter):\n        weights = self.get_filtered_weights(filter)\n        total = sum(weights.values())\n        probabilities = [weights[opt] / total for opt in weights]\n        return np.random.choice(list(weights.keys()), p=probabilities)\n  \nclass EightRooksRL(LearningRule):\n    def __init__(self, options, learning_rate=0.1,verbose=False,name='Eight Rooks RL'):\n        super().__init__(verbose = verbose, options=options, learning_rate=learning_rate,name=name)\n\n\n    def update_weights(self, option, reward):\n        self.prefix = option.split('_')[0]\n        self.suffix = option.split('_')[1]\n        \n        old_weights=self.weights.copy()\n        \n        for test_option in self.options:\n          if reward == 1:\n            if test_option == option:\n            # increment the weight of the good option \n              self.weights[test_option] += self.learning_rate * reward\n            elif test_option.startswith(self.prefix) or test_option.endswith(self.suffix) :\n            # decrement all other options with same prefix  or suffix\n               # if self.weights[test_option] &lt; 0.000001:\n               #   self.weights[test_option] = 0.0\n               # else:\n                self.weights[test_option] *= self.learning_rate \n          # elif test_option == option:\n          #   # decrement the weights of the bad option combo\n          #   self.weights[option] *= self.learning_rate \n\n        if self.verbose:\n          print()\n          for option in self.options:\n            if old_weights[option] != self.weights[option]:\n              print(f\"{option}: weight {old_weights[option]} -&gt; {self.weights[option]}\")\n          #print(f\"Updated weight {old_weights} -&gt; {self.weights}\")\n\n\n    def choose_option(self,filter):\n        weights = self.get_filtered_weights(filter)\n        total = sum(weights.values())\n        probabilities = [weights[opt] / total for opt in weights]\n        # if there is a max weight return it otherwise return a random option from the max wights\n        if len([opt for opt in weights if weights[opt]==max(weights.values())]) == 1:\n          return max(weights, key=weights.get)\n        else:\n          return np.random.choice([opt for opt in weights if weights[opt]==max(weights.values())])\n\nclass LewisAgent(Agent):\n    def __init__(self, unique_id, model, learning_options, learning_rule, verbose=False):\n        super().__init__(unique_id, model)\n        self.message = None\n        self.action = None\n        self.reward = 0\n        self.learning_rule = learning_rule\n        self.verbose = verbose\n        \n    def send(self):\n      return\n    \n    def receive(self):\n      return\n    \n    def calc_reward(self):\n      return\n\n    def set_reward(self):\n        self.reward = self.model.reward\n        if self.verbose:\n          print(f\"Agent {self.unique_id} received reward: {self.reward}\")\n        \n    def update_learning(self):\n        self.learning_rule.update_weights(self.option, self.reward)  # Update weights based on signals and rewards        \n\nclass Sender(LewisAgent):\n    def send(self):\n        state = self.model.get_state()\n        #self.message = self.learning_rule.choose_option(filter=state)  # Send a signal based on the learned weights\n        \n        self.option = self.learning_rule.choose_option(filter=state)  # Send a signal based on the learned weights\n        self.message = self.option.split('_')[1]\n        if self.verbose:\n          print(f\"Sender {self.unique_id} sends signal for state {state}: {self.message}\")\n\nclass Receiver(LewisAgent):\n    def receive(self):\n        self.received_signals = [sender.message for sender in self.model.senders]  # Receive signals from all senders\n        #print(f\"Receiver {self.unique_id} receives signals: {self.received_signals}\")\n        if self.received_signals:\n            for signal in self.received_signals:\n                self.option = self.learning_rule.choose_option(filter=signal)  # Choose an action based on received signals and learned weights\n                self.action = int(self.option.split('_')[1])\n                if self.verbose:\n                  print(f\"Receiver {self.unique_id} receives signals: {self.received_signals} and chooses action: {self.action}\")\n\n\n    def calc_reward(self):\n        correct_action = self.model.current_state\n        self.model.reward = 1 if self.action == correct_action else 0\n        if self.verbose:\n          print(f\"Receiver {self.unique_id} calculated reward: {self.reward} for action {self.action}\")\n\nclass SignalingGame(Model):\n    def __init__(self, \n                senders_count=1, \n                receivers_count=1, k=3,\n                learning_rule=LearningRule,\n                learning_rate=0.1,\n                verbose=False):\n        super().__init__()\n        self.verbose = verbose\n        self.k = k\n        self.current_state = None\n        self.learning_rate=learning_rate\n\n        # Initialize the states, signals, and actions mapping\n        self.states = list(range(k))  # States are simply numbers\n        self.signals = list(chr(65 + i) for i in range(k))  # Signals are characters\n        self.actions = list(range(k))  # Actions are simply numbers\n\n        # generate a list of state_signal keys for the sender's weights\n        self.states_signals_keys = [f'{state}_{signal}' for state in self.states for signal in self.signals]\n        # generate a list of signal_action keys for the receiver's weights\n        self.signals_actions_keys = [f'{signal}_{action}' for signal in self.signals for action in self.actions]\n        \n        self.senders = [Sender(i, self, learning_options=self.states_signals_keys, \n                                  learning_rule=learning_rule(self.states_signals_keys, self.learning_rate,verbose=self.verbose)\n                              ) for i in range(senders_count)]\n        self.receivers = [Receiver(i + senders_count, self, learning_options=self.signals_actions_keys, \n                                  learning_rule=learning_rule(self.signals_actions_keys, self.learning_rate,verbose=self.verbose)\n                              ) for i in range(receivers_count)]\n        \n        self.schedule = StagedActivation(self, \n          agents = self.senders + self.receivers, \n          stage_list=['send', 'receive', 'calc_reward', 'set_reward', 'update_learning'])\n\n    def get_state(self):\n        return random.choice(self.states)\n\n    def step(self):\n        self.current_state = self.get_state()\n        if self.verbose:\n          print(f\"Current state of the world: {self.current_state}\")\n        self.schedule.step()\n\n\n# function to plot agent weights side by side\n\ndef plot_weights(sender,reciver,title='Agent'):\n    fig, ax = plt.subplots(1,2,figsize=(9,5))\n    weights = sender.learning_rule.weights\n    ax[0].bar(weights.keys(), weights.values())\n    ax[0].set_xlabel('Options')\n    ax[0].set_ylabel('Weights')\n    ax[0].set_title(f'Sender {sender.unique_id} weights: {title}')\n    \n    weights = reciver.learning_rule.weights\n    ax[1].bar(weights.keys(), weights.values())\n    ax[1].set_xlabel('Options')\n    ax[1].set_ylabel('Weights')\n    ax[1].set_title(f'Receiver {reciver.unique_id} weights: {title}')\n    plt.show()\n\n\n# Running the model\nk=2\nverbose = False\nfor LR in [HerrnsteinRL,\n           RothErevRL,\n           RothErevForget_RL,\n           EightRooksRL\n           ]:\n  print(f\"--- {LR.__name__} ---\")\n  if LR == HerrnsteinRL:\n    learning_rate=1.\n  else:\n    learning_rate=.1\n  model = SignalingGame(senders_count=1, receivers_count=1, k=k, learning_rule=LR,learning_rate=learning_rate,verbose=verbose)\n  for i in range(10000):\n      if verbose:\n        print(f\"--- Step {i+1} ---\")\n      model.step()\n      # \n      #print the agent weights\n  #print('Sender weights:',model.senders[0].learning_rule.weights)\n  # plot weights side by side\n  \n  plot_weights(model.senders[0],model.receivers[0],title=LR.__name__)\n  #print('Receiver weights:',model.receivers[0].learning_rule.weights)\n  #plot_weights(model.receivers[0],title=LR.__name__)\n\n--- HerrnsteinRL ---\n--- RothErevRL ---\n--- RothErevForget_RL ---\n--- EightRooksRL ---\n\n\n/home/oren/work/blog/env/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:\n\nThe AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.\nWe would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncurrently only the eight rooks learning rule is producing consistently good signaling systems. The other learning rules are not learning to signal correctly.\nPlease suggest how to fix this - according to the literature the Roth-Erev with forgetting learning rule should work well in this case.\nTODO: implement Bush-Mosteller learning - as this is a match for population dynamics.\nTODO: also implement population dynamics as it may not be clear that BM RL is a perfect fit for population dynamics under all lewis game conditions.\nTODO: implement ARP learning.\nTODO: implement epsilon-greedy, UCB and thompson sampling urn schemes, and Contextual bandits associative search (that‚Äôs our multiurn bandit)",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 05 09 RE RL",
      "Roth Erev learning in Lewis signaling games"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-09-RE-RL/re-rel.html#estimating-the-gittins-index-for-a-lewis-games.",
    "href": "posts/2024/2024-05-09-RE-RL/re-rel.html#estimating-the-gittins-index-for-a-lewis-games.",
    "title": "Roth Erev learning in Lewis signaling games",
    "section": "Estimating the Gittins index for a Lewis games.",
    "text": "Estimating the Gittins index for a Lewis games.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass ContextualBandit:\n    def __init__(self, n_states, n_actions):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.rewards = np.zeros((n_states, n_actions))\n        self.counts = np.ones((n_states, n_actions))\n\n    def update(self, state, action, reward):\n        self.counts[state, action] += 1\n        self.rewards[state, action] += reward\n\n    def get_gittins_index(self, state, action):\n        # Simplified Gittins index computation\n        total_reward = self.rewards[state, action]\n        total_count = self.counts[state, action]\n        return total_reward / total_count + np.sqrt(2 * np.log(np.sum(self.counts)) / total_count)\n\n    def select_action(self, state):\n        gittins_indices = [self.get_gittins_index(state, a) for a in range(self.n_actions)]\n        return np.argmax(gittins_indices)\n\ndef sample_state(distribution, n_states):\n    if distribution == \"uniform\":\n        return np.random.randint(n_states)\n    elif distribution == \"normal\":\n        state = int(np.random.normal(loc=n_states/2, scale=n_states/6))\n        return np.clip(state, 0, n_states - 1)\n    else:\n        raise ValueError(\"Unsupported distribution type\")\n\n\n# Example usage\nn_states = 5\nn_actions = 5\nn_iterations = 1000\n\nsender_bandit = ContextualBandit(n_states, n_actions)\nreceiver_bandit = ContextualBandit(n_actions, n_states)\nstate_distribution = \"uniform\"  # Change to \"normal\" for normal distribution\n\nrewards = []\nregrets = []\ntotal_reward = 0\ntotal_regret = 0\nsender_gittins_indices = [[] for _ in range(n_actions)]\nreceiver_gittins_indices = [[] for _ in range(n_states)]\n\n# Simulate the learning process\nfor t in range(n_iterations):\n    state = sample_state(state_distribution, n_states)\n    sender_action = sender_bandit.select_action(state)\n    receiver_action = receiver_bandit.select_action(sender_action)\n    \n    reward = 1 if receiver_action == state else 0\n    total_reward += reward\n    total_regret += 1 - reward\n    \n    rewards.append(total_reward)\n    regrets.append(total_regret)\n    \n    sender_bandit.update(state, sender_action, reward)\n    receiver_bandit.update(sender_action, receiver_action, reward)\n    \n    for action in range(n_actions):\n        sender_gittins_indices[action].append(sender_bandit.get_gittins_index(state, action))\n    \n    for state in range(n_states):\n        receiver_gittins_indices[state].append(receiver_bandit.get_gittins_index(sender_action, state))\n\n# Print final policy\nprint(\"Sender policy:\")\nfor state in range(n_states):\n    print(f\"State {state}: Action {sender_bandit.select_action(state)}\")\n\nprint(\"Receiver policy:\")\nfor action in range(n_actions):\n    print(f\"Action {action}: State {receiver_bandit.select_action(action)}\")\n\n# Plot the total rewards and regrets over time\nplt.figure(figsize=(12, 6))\nplt.plot(rewards, label='Total Rewards')\nplt.plot(regrets, label='Total Regret')\nplt.xlabel('Time Step')\nplt.ylabel('Total Rewards/Regret')\nplt.title('Total Rewards and Regret Over Time')\nplt.legend()\nplt.show()\n\n# Plot the Gittins indices over time for the sender\nplt.figure(figsize=(12, 6))\nfor action in range(n_actions):\n    plt.plot(sender_gittins_indices[action], label=f'Sender Gittins Index (Action {action})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Sender Gittins Indices Over Time')\nplt.legend()\nplt.show()\n\n# Plot the Gittins indices over time for the receiver\nplt.figure(figsize=(12, 6))\nfor state in range(n_states):\n    plt.plot(receiver_gittins_indices[state], label=f'Receiver Gittins Index (State {state})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Receiver Gittins Indices Over Time')\nplt.legend()\nplt.show()\n\nSender policy:\nState 0: Action 4\nState 1: Action 0\nState 2: Action 2\nState 3: Action 3\nState 4: Action 1\nReceiver policy:\nAction 0: State 1\nAction 1: State 4\nAction 2: State 2\nAction 3: State 3\nAction 4: State 0",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 05 09 RE RL",
      "Roth Erev learning in Lewis signaling games"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-09-RE-RL/re-rel.html#making-it-bayesian",
    "href": "posts/2024/2024-05-09-RE-RL/re-rel.html#making-it-bayesian",
    "title": "Roth Erev learning in Lewis signaling games",
    "section": "Making it Bayesian",
    "text": "Making it Bayesian\nAccording to (Sutton and Barto 2018) Gitting‚Äôs index are usually associated with the Bayesian paradigm.\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning, Second Edition: An Introduction. Adaptive Computation and Machine Learning Series. MIT Press. http://incompleteideas.net/book/RLbook2020.pdf.\nAs such one should be able to we could use a Bayesian updating scheme to learn expected rewards based on success counts. Since we are tracking successes vs failures we can use beta-binomial conjugate distributions to keep track of successes, failures and their likelihood.\nThis most basic form is like so:\n\n\n\nTable¬†1: sender & receiver prior\n\n\n\n\n\n\n\n(a) sender alpha, beta\n\n\n\n\n\nState/Signal\n0\n1\n2\n\n\n\n\n0\n0,0\n0,0\n0,0\n\n\n1\n0,0\n0,0\n0,0\n\n\n2\n0,0\n0,0\n0,0\n\n\n\n\n\n\n\n\n\n\n\n(b) receiver alpha, beta\n\n\n\n\n\nSignal/Action\n0\n1\n2\n\n\n\n\n0\n0,0\n0,0\n0,0\n\n\n1\n0,0\n0,0\n0,0\n\n\n2\n0,0\n0,0\n0,0\n\n\n\n\n\n\n\n\n\n\n\nWhere we have a table of independent beta-binomial priors for each state/signal and signal/action pair.\nAfter 5 failures we update the beta distribution for the sender and receiver as follows:\n\n\n\n\n\n\n\n\n\nTable¬†2: sender alpha, beta\n\n\n\n\n\nState/Signal\n0\n1\n2\n\n\n\n\n0\n0,1\n0,2\n0,0\n\n\n1\n0,0\n0,1\n0,0\n\n\n2\n0,0\n0,0\n0,1\n\n\n\n\n\n\n\n\n\n\n\nTable¬†3: receiver alpha, beta\n\n\n\n\n\nSignal/Action\n0\n1\n2\n\n\n\n\n0\n0,1\n0,0\n0,1\n\n\n1\n0,0\n0,1\n0,0\n\n\n2\n0,2\n0,0\n0,0\n\n\n\n\n\n\n\n\n\n\nsender & receiver posterior\n\n\n\nFailures are outcomes of uncorrelated signal action pairs and are basically like adding noise to the distribution on the loss side. Failures here tend to have a confounding effect - they reduce the probabilities associated with reward signals. And the model is not aware of the order of rewards/failures recency.\nNow lets update for 2 success as follows:\n\n\n\n\n\n\n\n\n\nTable¬†4: sender alpha, beta\n\n\n\n\n\n\n\n\n\n\n\nState/Signal\n0\n1\n2\n\n\n\n\n0\n1,1\n1,2\n0,0\n\n\n1\n0,0\n0,1\n1,0\n\n\n2\n0,0\n0,0\n0,1\n\n\n\n\n\n\n\n\n\n\n\nTable¬†5: receiver alpha, beta\n\n\n\n\n\n\n\n\n\n\n\nSignal/Action\n0\n1\n2\n\n\n\n\n0\n0,1\n0,0\n0,1\n\n\n1\n1,0\n0,1\n0,0\n\n\n2\n0,2\n1,0\n0,0\n\n\n\n\n\n\n\n\n\n\nsender & receiver posterior\n\n\n\nThe Rewards are for Corralated signals/action pairs. However before learning progresses signal/action pairs are picked by chance. And so if different signal/action pairs are picked for the same state we will get a synonym and consequently will be missing a state/signal pair for one of the other states which will need to be shared (homonym).\nNote that if we have a ties (between two signal/action pairs for a state then the next success or failure can be a spontaneous symmetry breaking event.\nThis will result in a a partial pooling equilibrium.\nThe Gittin‚Äôs index might help here by picking an options with the greatest expected return. If we set it up so it can recognize that a separating equilibria have the greatest expected return we should eventual learn these.\nThe problem is that micommunications (may confound the learning, until the pattern due to rewards are sufficiently reinforced.)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass BayesianContextualBandit:\n    def __init__(self, n_states, n_actions):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.alpha = np.ones((n_states, n_actions))\n        self.beta = np.ones((n_states, n_actions))\n\n    def update(self, state, action, reward):\n        if reward == 1:\n            self.alpha[state, action] += 1\n        else:\n            self.beta[state, action] += 1\n\n    def get_expected_reward(self, state, action):\n        return self.alpha[state, action] / (self.alpha[state, action] + self.beta[state, action])\n\n    def get_gittins_index(self, state, action):\n        total_reward = self.alpha[state, action]\n        total_count = self.alpha[state, action] + self.beta[state, action]\n        return total_reward / total_count + np.sqrt(2 * np.log(np.sum(self.alpha + self.beta)) / total_count)\n\n    def select_action(self, state):\n        gittins_indices = [self.get_gittins_index(state, a) for a in range(self.n_actions)]\n        return np.argmax(gittins_indices)\n\ndef sample_state(distribution, n_states):\n    if distribution == \"uniform\":\n        return np.random.randint(n_states)\n    elif distribution == \"normal\":\n        state = int(np.random.normal(loc=n_states/2, scale=n_states/6))\n        return np.clip(state, 0, n_states - 1)\n    else:\n        raise ValueError(\"Unsupported distribution type\")\n\ndef run_experiment(n_states, n_actions, n_iterations, state_distribution, k):\n    all_rewards = np.zeros((k, n_iterations))\n    all_regrets = np.zeros((k, n_iterations))\n    all_sender_gittins_indices = np.zeros((k, n_actions, n_iterations))\n    all_receiver_gittins_indices = np.zeros((k, n_states, n_iterations))\n    \n    for i in range(k):\n        sender_bandit = BayesianContextualBandit(n_states, n_actions)\n        receiver_bandit = BayesianContextualBandit(n_actions, n_states)\n        \n        total_reward = 0\n        total_regret = 0\n        \n        for t in range(n_iterations):\n            state = sample_state(state_distribution, n_states)\n            sender_action = sender_bandit.select_action(state)\n            receiver_action = receiver_bandit.select_action(sender_action)\n            \n            reward = 1 if receiver_action == state else 0\n            total_reward += reward\n            total_regret += 1 - reward\n            \n            all_rewards[i, t] = total_reward\n            all_regrets[i, t] = total_regret\n            \n            sender_bandit.update(state, sender_action, reward)\n            receiver_bandit.update(sender_action, receiver_action, reward)\n            \n            for action in range(n_actions):\n                all_sender_gittins_indices[i, action, t] = sender_bandit.get_gittins_index(state, action)\n            \n            for s in range(n_states):\n                all_receiver_gittins_indices[i, s, t] = receiver_bandit.get_gittins_index(sender_action, s)\n    \n    mean_rewards = np.mean(all_rewards, axis=0)\n    mean_regrets = np.mean(all_regrets, axis=0)\n    mean_sender_gittins_indices = np.mean(all_sender_gittins_indices, axis=0)\n    mean_receiver_gittins_indices = np.mean(all_receiver_gittins_indices, axis=0)\n    \n    return all_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_gittins_indices, mean_receiver_gittins_indices\n\n# Parameters\nn_states = 5\nn_actions = 5\nn_iterations = 1000\nstate_distribution = \"uniform\"  # Change to \"normal\" for normal distribution\nk = 50  # Number of experiment runs\n\n# Run the experiment\nall_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_gittins_indices, mean_receiver_gittins_indices = run_experiment(n_states, n_actions, n_iterations, state_distribution, k)\n\n# Plot the mean total rewards and regrets over time along with individual curves\nplt.figure(figsize=(12, 6))\nfor i in range(k):\n    plt.plot(all_rewards[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_rewards, label='Mean Total Rewards', color='blue', linewidth=2)\nfor i in range(k):\n    plt.plot(all_regrets[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_regrets, label='Mean Total Regret', color='red', linewidth=2)\nplt.xlabel('Time Step')\nplt.ylabel('Total Rewards/Regret')\nplt.title('Total Rewards and Regret Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Gittins indices over time for the sender\nplt.figure(figsize=(12, 6))\nfor action in range(n_actions):\n    plt.plot(mean_sender_gittins_indices[action], label=f'Mean Sender Gittins Index (Action {action})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Mean Sender Gittins Indices Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Gittins indices over time for the receiver\nplt.figure(figsize=(12, 6))\nfor state in range(n_states):\n    plt.plot(mean_receiver_gittins_indices[state], label=f'Mean Receiver Gittins Index (State {state})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Mean Receiver Gittins Indices Over Time')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOf course there is no reason to use independent probabilities for for learning.\nThe schemes described in the book condition state for the sender and on the signal for the receiver. I.E. a success for a signal/action pair implies:\n\na failure for the other state/signals options with the same states for the sender.\na failure for the other signal/action options with the same signal for the receiver.\n\nIn my algorithm I went further and added the logic that a success for a signals/action pair also implies:\n\na failure for the other state/signals options with the same signal but different states for the sender.\na failure for the other signal/action options with the same action but different signals for the receiver.\n\nalso implies that the signal wasn‚Äôt available for other states.\nI‚Äôm not sure if there is a distribution that updates like that, though it isn‚Äôt that hard to implement either of the two schemes and they should work an extended beta distribution.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 05 09 RE RL",
      "Roth Erev learning in Lewis signaling games"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-09-RE-RL/re-rel.html#derichlet-multinomial-variant",
    "href": "posts/2024/2024-05-09-RE-RL/re-rel.html#derichlet-multinomial-variant",
    "title": "Roth Erev learning in Lewis signaling games",
    "section": "Derichlet-Multinomial variant",
    "text": "Derichlet-Multinomial variant\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass BayesianContextualBandit:\n    def __init__(self, n_states, n_actions):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.alpha = np.ones((n_states, n_actions))\n\n    def update(self, state, action, reward):\n        if reward == 1:\n            self.alpha[state, action] += 1\n\n    def get_expected_reward(self, state, action):\n        alpha_sum = np.sum(self.alpha[state])\n        return self.alpha[state, action] / alpha_sum\n\n    def get_gittins_index(self, state, action):\n        alpha_sum = np.sum(self.alpha[state])\n        return self.alpha[state, action] / alpha_sum + np.sqrt(2 * np.log(alpha_sum) / (self.alpha[state, action] + 1))\n\n    def select_action(self, state):\n        gittins_indices = [self.get_gittins_index(state, a) for a in range(self.n_actions)]\n        return np.argmax(gittins_indices)\n\ndef sample_state(distribution, n_states):\n    if distribution == \"uniform\":\n        return np.random.randint(n_states)\n    elif distribution == \"normal\":\n        state = int(np.random.normal(loc=n_states/2, scale=n_states/6))\n        return np.clip(state, 0, n_states - 1)\n    else:\n        raise ValueError(\"Unsupported distribution type\")\n\ndef run_experiment(n_states, n_actions, n_iterations, state_distribution, k):\n    all_rewards = np.zeros((k, n_iterations))\n    all_regrets = np.zeros((k, n_iterations))\n    all_sender_gittins_indices = np.zeros((k, n_actions, n_iterations))\n    all_receiver_gittins_indices = np.zeros((k, n_states, n_iterations))\n    \n    for i in range(k):\n        sender_bandit = BayesianContextualBandit(n_states, n_actions)\n        receiver_bandit = BayesianContextualBandit(n_actions, n_states)\n        \n        total_reward = 0\n        total_regret = 0\n        \n        for t in range(n_iterations):\n            state = sample_state(state_distribution, n_states)\n            sender_action = sender_bandit.select_action(state)\n            receiver_action = receiver_bandit.select_action(sender_action)\n            \n            reward = 1 if receiver_action == state else 0\n            total_reward += reward\n            total_regret += 1 - reward\n            \n            all_rewards[i, t] = total_reward\n            all_regrets[i, t] = total_regret\n            \n            sender_bandit.update(state, sender_action, reward)\n            receiver_bandit.update(sender_action, receiver_action, reward)\n            \n            for action in range(n_actions):\n                all_sender_gittins_indices[i, action, t] = sender_bandit.get_gittins_index(state, action)\n            \n            for s in range(n_states):\n                all_receiver_gittins_indices[i, s, t] = receiver_bandit.get_gittins_index(sender_action, s)\n    \n    mean_rewards = np.mean(all_rewards, axis=0)\n    mean_regrets = np.mean(all_regrets, axis=0)\n    mean_sender_gittins_indices = np.mean(all_sender_gittins_indices, axis=0)\n    mean_receiver_gittins_indices = np.mean(all_receiver_gittins_indices, axis=0)\n    \n    return all_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_gittins_indices, mean_receiver_gittins_indices\n\n# Parameters\nn_states = 5\nn_actions = 5\nn_iterations = 1000\nstate_distribution = \"uniform\"  # Change to \"normal\" for normal distribution\nk = 50  # Number of experiment runs\n\n# Run the experiment\nall_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_gittins_indices, mean_receiver_gittins_indices = run_experiment(n_states, n_actions, n_iterations, state_distribution, k)\n\n# Plot the mean total rewards and regrets over time along with individual curves\nplt.figure(figsize=(12, 6))\nfor i in range(k):\n    plt.plot(all_rewards[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_rewards, label='Mean Total Rewards', color='blue', linewidth=2)\nfor i in range(k):\n    plt.plot(all_regrets[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_regrets, label='Mean Total Regret', color='red', linewidth=2)\nplt.xlabel('Time Step')\nplt.ylabel('Total Rewards/Regret')\nplt.title('Total Rewards and Regret Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Gittins indices over time for the sender\nplt.figure(figsize=(12, 6))\nfor action in range(n_actions):\n    plt.plot(mean_sender_gittins_indices[action], label=f'Mean Sender Gittins Index (Action {action})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Mean Sender Gittins Indices Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Gittins indices over time for the receiver\nplt.figure(figsize=(12, 6))\nfor state in range(n_states):\n    plt.plot(mean_receiver_gittins_indices[state], label=f'Mean Receiver Gittins Index (State {state})')\nplt.xlabel('Time Step')\nplt.ylabel('Gittins Index')\nplt.title('Mean Receiver Gittins Indices Over Time')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 05 09 RE RL",
      "Roth Erev learning in Lewis signaling games"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-09-RE-RL/re-rel.html#thompson-sampling",
    "href": "posts/2024/2024-05-09-RE-RL/re-rel.html#thompson-sampling",
    "title": "Roth Erev learning in Lewis signaling games",
    "section": "Thompson sampling",
    "text": "Thompson sampling\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass ThompsonSamplingContextualBandit:\n    def __init__(self, n_states, n_actions):\n        self.n_states = n_states\n        self.n_actions = n_actions\n        self.alpha = np.ones((n_states, n_actions))\n        self.beta = np.ones((n_states, n_actions))\n\n    def update(self, state, action, reward):\n        if reward == 1:\n            self.alpha[state, action] += 1\n        else:\n            self.beta[state, action] += 1\n\n    def select_action(self, state):\n        samples = [np.random.beta(self.alpha[state, a], self.beta[state, a]) for a in range(self.n_actions)]\n        return np.argmax(samples)\n\ndef sample_state(distribution, n_states):\n    if distribution == \"uniform\":\n        return np.random.randint(n_states)\n    elif distribution == \"normal\":\n        state = int(np.random.normal(loc=n_states/2, scale=n_states/6))\n        return np.clip(state, 0, n_states - 1)\n    else:\n        raise ValueError(\"Unsupported distribution type\")\n\ndef run_experiment(n_states, n_actions, n_iterations, state_distribution, k):\n    all_rewards = np.zeros((k, n_iterations))\n    all_regrets = np.zeros((k, n_iterations))\n    all_sender_ts_indices = np.zeros((k, n_actions, n_iterations))\n    all_receiver_ts_indices = np.zeros((k, n_states, n_iterations))\n    \n    for i in range(k):\n        sender_bandit = ThompsonSamplingContextualBandit(n_states, n_actions)\n        receiver_bandit = ThompsonSamplingContextualBandit(n_actions, n_states)\n        \n        total_reward = 0\n        total_regret = 0\n        \n        for t in range(n_iterations):\n            state = sample_state(state_distribution, n_states)\n            sender_action = sender_bandit.select_action(state)\n            receiver_action = receiver_bandit.select_action(sender_action)\n            \n            reward = 1 if receiver_action == state else 0\n            total_reward += reward\n            total_regret += 1 - reward\n            \n            all_rewards[i, t] = total_reward\n            all_regrets[i, t] = total_regret\n            \n            sender_bandit.update(state, sender_action, reward)\n            receiver_bandit.update(sender_action, receiver_action, reward)\n            \n            for action in range(n_actions):\n                all_sender_ts_indices[i, action, t] = np.random.beta(sender_bandit.alpha[state, action], sender_bandit.beta[state, action])\n            \n            for s in range(n_states):\n                all_receiver_ts_indices[i, s, t] = np.random.beta(receiver_bandit.alpha[sender_action, s], receiver_bandit.beta[sender_action, s])\n    \n    mean_rewards = np.mean(all_rewards, axis=0)\n    mean_regrets = np.mean(all_regrets, axis=0)\n    mean_sender_ts_indices = np.mean(all_sender_ts_indices, axis=0)\n    mean_receiver_ts_indices = np.mean(all_receiver_ts_indices, axis=0)\n    \n    return all_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_ts_indices, mean_receiver_ts_indices\n\n# Parameters\nn_states = 5\nn_actions = 5\nn_iterations = 1000\nstate_distribution = \"uniform\"  # Change to \"normal\" for normal distribution\nk = 50  # Number of experiment runs\n\n# Run the experiment\nall_rewards, all_regrets, mean_rewards, mean_regrets, mean_sender_ts_indices, mean_receiver_ts_indices = run_experiment(n_states, n_actions, n_iterations, state_distribution, k)\n\n# Plot the mean total rewards and regrets over time along with individual curves\nplt.figure(figsize=(12, 6))\nfor i in range(k):\n    plt.plot(all_rewards[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_rewards, label='Mean Total Rewards', color='blue', linewidth=2)\nfor i in range(k):\n    plt.plot(all_regrets[i], color='gray', alpha=0.5, linewidth=0.5)\nplt.plot(mean_regrets, label='Mean Total Regret', color='red', linewidth=2)\nplt.xlabel('Time Step')\nplt.ylabel('Total Rewards/Regret')\nplt.title('Total Rewards and Regret Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Thompson Sampling indices over time for the sender\nplt.figure(figsize=(12, 6))\nfor action in range(n_actions):\n    plt.plot(mean_sender_ts_indices[action], label=f'Mean Sender TS Index (Action {action})')\nplt.xlabel('Time Step')\nplt.ylabel('Thompson Sampling Index')\nplt.title('Mean Sender Thompson Sampling Indices Over Time')\nplt.legend()\nplt.show()\n\n# Plot the mean Thompson Sampling indices over time for the receiver\nplt.figure(figsize=(12, 6))\nfor state in range(n_states):\n    plt.plot(mean_receiver_ts_indices[state], label=f'Mean Receiver TS Index (State {state})')\nplt.xlabel('Time Step')\nplt.ylabel('Thompson Sampling Index')\nplt.title('Mean Receiver Thompson Sampling Indices Over Time')\nplt.legend()\n\nplt.show()",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 05 09 RE RL",
      "Roth Erev learning in Lewis signaling games"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-06-bayesian-perspective/index.html",
    "href": "posts/2024/2024-05-06-bayesian-perspective/index.html",
    "title": "Lewis Game from a Bayesian Perspective",
    "section": "",
    "text": "I have been thinking about Lewis Signaling games recently, and I had come up with a couple of questions that I wanted to answer.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "Lewis Game from a Bayesian Perspective"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-06-bayesian-perspective/index.html#better-initialization",
    "href": "posts/2024/2024-05-06-bayesian-perspective/index.html#better-initialization",
    "title": "Lewis Game from a Bayesian Perspective",
    "section": "Better Initialization",
    "text": "Better Initialization\nFirst has to do with initializing the algorithm in some optimal way. Like the battle of the sexes there is no easy way to initialize the algorithm unless the agents can coordinate on a single equilibrium. If the state are unevenly distributed, or if they can listen to some prior signal, then they can coordinate on a permutation ordered by frequency for the signals and its inverse for the actions. Otherwise the agents will have to learn the equilibrium through trial and error which is the essence of the game.\nHowever the idea of a prior remained and the complexity of specifying it kept bugging me since I had failed to find a way to do it.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "Lewis Game from a Bayesian Perspective"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-06-bayesian-perspective/index.html#accelarating-learning-using-multiple-agents",
    "href": "posts/2024/2024-05-06-bayesian-perspective/index.html#accelarating-learning-using-multiple-agents",
    "title": "Lewis Game from a Bayesian Perspective",
    "section": "Accelarating Learning using Multiple Agents",
    "text": "Accelarating Learning using Multiple Agents\nA second question that I had was not covered in the literature. I wanted to know if the multiple agents were signaling to each other, in a visible way, would the agents be able to coordinate on a single equilibrium significantly faster just a pair of agents.\nOne obvious point is that move by nature would slow down the process is agents are unlucky. For optimal signaling the same state would be remain until agents could coordinate and would not reoccur until the agents had coordinate on all the other states. So for multiple agents some agents would be closer to this optimum and may learn faster then the others. Secondly since matching siganl action pairs are rare, (1/k\\^2) for a k state game, having between k to k\\^2 should significantly increase.\nExpectation of a matching signal-action pair. So this could speed things up. But this also raises the issue of differential signaling systems arising if by chance some two or more pairs learned different signal/action pairs. The learning process would need to break such ties (Skryms might call it spontaneous symmetry breaking) But it could slow down the learning process.\nActually such a state of affairs could lead to a partial pooling equilibrium, where all the agents had learned a synonym. This would be a suboptimal equilibrium, but it will provide a maximal payoff for all the agents if there are no homonyms.\nSome ideas on how to break the symmetry would be: 1. the group might defer to seniorirty i.e.¬†the sender with the lowest id. - (takes no extra time).\n\nagents could vote at random for a signal. (would take just one more step if we ignore one draw if the votes are tied)\nask the other agents to vote who likes signal a and who likes signal b. if the sender or reciever match the sender/reciever they like it so there would be 0 1 or 2 votes for each signal. the might be draws too and each agent would need to pick a new permutation and vote again. - (would take a few more steps)\nthe senders might pick a pair of at random until they both pick the same one. - (would take a few more steps)\n\nAny way you look at it there are many advantages to consider learning by multiple senders. They seem necessary for complex signaling as well. However I was pretty certain that the analysis would keep getting more complex as we considered more options like learning grammar, contexts or a noisy environment‚Ä¶.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "Lewis Game from a Bayesian Perspective"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-06-bayesian-perspective/index.html#bayesian-perspective",
    "href": "posts/2024/2024-05-06-bayesian-perspective/index.html#bayesian-perspective",
    "title": "Lewis Game from a Bayesian Perspective",
    "section": "Bayesian Perspective",
    "text": "Bayesian Perspective\nI had already implemented learning using different algorithms and to explote the Gittin‚Äôs index from (Sutton and Barto 2018) I had already implemented a Beta-Bernulli contextual bandit with Gittin‚Äôs index and with Thompson sampling.\n\nSutton, R. S., and A. G. Barto. 2018. Reinforcement Learning, Second Edition: An Introduction. Adaptive Computation and Machine Learning Series. MIT Press. http://incompleteideas.net/book/RLbook2020.pdf.\nI was already thinking how to improve it but I did not have a very good idea regarding the prior. I had a fairly efficient algorithm for the learning but I wanted a better way to model the updating and the right prior. My idea of using a Multinomial-Dirichlet conjugate pair had not worked and would probably take a while to trouble shoot and fix, and it was not really the full solution I was looking for.\nMore so I was coming to terms that I could likely come up with Bayesian updating schemes that were novel and I would quickly find myself deep in uncharted territory. This had some attraction - it was not the first time I came a cross a problem that did not seem to have a conjugate prior pair to fit with prior knowledge I wanted to bring to bear in the model, but Baysian updating is just one aspect of Bayesian methodology and I was worried of getting to a dead end because of working with a new type of distributions.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "Lewis Game from a Bayesian Perspective"
    ]
  },
  {
    "objectID": "posts/2024/2024-05-06-bayesian-perspective/index.html#the-model",
    "href": "posts/2024/2024-05-06-bayesian-perspective/index.html#the-model",
    "title": "Lewis Game from a Bayesian Perspective",
    "section": "The Model",
    "text": "The Model\nAt a fundamental level the Lewis signaling game of coorrdination. Sender and reciever are trying to learn a mapping between states and signals. The mappings need to be inverse of one another and to have a maximal reward the mappings need to preserve the messages - synonyms are ok by homonyms are not. And if thes number of states and signals and actions are the same then the mappings need to be one to one and onto.\nSo in such a case synonyms are not allowed and the mappings need to be not just permutation but rather cycles of length k. This is something I had understood intuitively but I had ot been very clear about.\nI was now thinking about distribution over groups - something I had not considered before. However it dawned on me that the two other aspects of the complex signaling game being grammar and context might be modeled additional group structures. And if we could learn cycles efficiently then we might generalize to more complex signaling systems in a reductionist way intimated in chapter 12 of (skyrms2010signals?).\nThe point is that cycles are not the simplest structure in this problem either. What we are looking at each state of Nature is a pair of transpositions that cancel each other out. A transposition is a very simple structure but it is also a base element of a permutation. The Cayley theorem tells us that any group is isomorphic to a group of permutations. If we can define our prior using transpositions then we can define a prior over permutations or general on any group.\nAnother point in favor of transpositions is that they have one operation, their composition just a product and since probabilities are multiplicative too the two seem to be a good fit.\nSo I had three point to consider.\n\nconstructing the prior for cycles based on transpositions.\nupdating the prior using based on moves in the Lewis signaling game.\nimplement it as an rl/Bayesian model say using Thompson sampling.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "Lewis Game from a Bayesian Perspective"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html",
    "title": "two ideas on generelization",
    "section": "",
    "text": "the first idea comes from looking at regression and hyperparameter optimization tasks\nFirst we want to investigate the idea of finding the best values of individual parameters or hyperparameters. Next if these are second order effects corresponding to the interaction of two or more parameters, we will want to investigate if there are pairs of hyper/parameters that are more important than others.\n\none way to orgenize this search might be characterised as a reductionist search, where we start with a simple model and add complexity as needed.\nanother way to orgenize this search might be characterised as a covariance matrix bingo, where we concieve of all the possible interactions reperesented by the covariance matrix, in reality the covariance matrix is a symmetric and sparse, we may discover that certain features are correlated with each other, or better yet collinear, and we may want to remove these features and retain perhaps better orthonormal features. In other words we might be able to plan our model selection by looking at how the covariance matrix evolves as we add features to the model. We will end up with a subset of the matrix similar to a bingo card, where we wish to get to this wininning combination of features faster than the other players.\n\nA second reality of this issues adressable by RL is that high order effects tend to be increasingly sparse so that if we have a hint of that certain slots in the covariance matrix are non zero we should defintely explore those efects more than others. This infact suggests an improvement to baysian search.\nThis aspect of feature selection is not partularly exciting, but suggests a more abstract way of thinking about the problem. This leads to a second perhaps more powerful idea.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "two ideas on generelization"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#feature-selection-problem",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#feature-selection-problem",
    "title": "two ideas on generelization",
    "section": "",
    "text": "the first idea comes from looking at regression and hyperparameter optimization tasks\nFirst we want to investigate the idea of finding the best values of individual parameters or hyperparameters. Next if these are second order effects corresponding to the interaction of two or more parameters, we will want to investigate if there are pairs of hyper/parameters that are more important than others.\n\none way to orgenize this search might be characterised as a reductionist search, where we start with a simple model and add complexity as needed.\nanother way to orgenize this search might be characterised as a covariance matrix bingo, where we concieve of all the possible interactions reperesented by the covariance matrix, in reality the covariance matrix is a symmetric and sparse, we may discover that certain features are correlated with each other, or better yet collinear, and we may want to remove these features and retain perhaps better orthonormal features. In other words we might be able to plan our model selection by looking at how the covariance matrix evolves as we add features to the model. We will end up with a subset of the matrix similar to a bingo card, where we wish to get to this wininning combination of features faster than the other players.\n\nA second reality of this issues adressable by RL is that high order effects tend to be increasingly sparse so that if we have a hint of that certain slots in the covariance matrix are non zero we should defintely explore those efects more than others. This infact suggests an improvement to baysian search.\nThis aspect of feature selection is not partularly exciting, but suggests a more abstract way of thinking about the problem. This leads to a second perhaps more powerful idea.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "two ideas on generelization"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#localised-subspace-embedding",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#localised-subspace-embedding",
    "title": "two ideas on generelization",
    "section": "Localised subspace embedding",
    "text": "Localised subspace embedding\nThe idea of embedings which allow us to use a distributed representation instead of a one hot encoding is very powerful. In one sense it is the opposite of the reductionist notion mentioned above. Instead of trying to find the best features we are trying to find the best representation of the features. However if one is familiar with PCA or SVD one knows that the best representation is often some linear combination of the actual features we observe. A reductionist might view these a generlized coordinates that are more useful for inverstigating the feature space.\nIf our features are built from such embeddings, we learn weights that correspond to the importance of the features in the model. This representation is still subject to the bias variance tradeoff and we generaly have many parameters in Neural Networks. One way to view this is to try and orgeinze the model so that it can use emebeddings of subspaces - corresponding to features that are balance generlization and discrimination.\nA second point is that in different contexts we might need to use different features. This is much easier to see in RL and NLP. Building a embedding that is localised to a just some features and some observations/states might allow the model to get good generalization then by considering all the features at once. This is an analogue of the idea of factoring a distribution into a product of marginals, particularly in the case of a much larger bayesian network. In this case though we might be talking about using two such factorizations, with some discriminator selecting the observations that are used in different contexts.\nWe might think of a neural network as evolving system of that learns to bifurcate the distributed representation of the features of the data set in the input into any number of\nsmaller and more localised subspaces. The more Localised subspaces are more likely to be linearly separable and thus easier to learn.\nHowever all this happens by breaking symmetries using the random aspects of the learning algorithm. Minibatches present many random samples which carry differnt payloads of information. Certain such payloads may reinforce the current network weights, while the next may require a bifurcation of the representation into two to minimize the loss. Another might require many bifurcations and may not lead to any new bifurcations or reinforcements. Drop out breaks symmetries by shutting down parts of the network temporarily.\nOn other problems with generalization in RL and in NLP might be resolved using local subspace embedding of the state space. These are features that conflates states that are similar and distinguishes states that are different. By avoiding a full embedding we reduce the variance of the function approximation and thus improve generalization.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "two ideas on generelization"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#question",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#question",
    "title": "two ideas on generelization",
    "section": "Question",
    "text": "Question\n\nHow can we encourage the NN model to learn localised subspaces?\nHowe can query the model to learn/interpret the localised subspaces?\nIf the model learns a powerfull feature how can we give access to it to other parts of the model? (this would reduce learning time and increase generalization)\n\nall discrminators following the features will be have one use of the feature.\ncould we let all other nodes in the network have a residual connection to the feature?\n\nif we wanted to further refine a representation of a localised subspace of verbs to intransitive verbs how would we do that?\n\nwe would like to lean a discriminator that can tell the difference between transitive and intransitive verbs and then use it to gate the input to the verb feature.\nhowever we might prefer to do better than that and learn a better representation of the\ntransitive and intransitive verbs. This would need learning different weights for the different verbs. This means we want to bifurcate the verb fearture subnetwork into two replicates but add the discriminator as a gate to the input of the two subnetworks.\nanother point worth considering is that once we have learned a good representation of both\nthe transitive and intransitive verbs we can use these as features in the next layers of the network. We sould be able to combine them to get a better representation than just the original verb.\nI recon this happens many times in LLMs. What we might want is to have some way for the model to attend to all the subspaces it has learned and use them to guide its learning.\nThe challange seems to be in identification of the subspaces. We may be using differnt basis for each subspace etc which may lead to difficulty in reusing them.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "two ideas on generelization"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#attention-heads",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#attention-heads",
    "title": "two ideas on generelization",
    "section": "Attention heads",
    "text": "Attention heads\nIt seems thogh that attention heads are a good way to orgenize and increase the diversity of the localised subspaces. This is because the attention heads can be trained to focus on different parts of the input and thus can be used to create a localised subspace embedding. This is a very powerful idea and is the basis of the transformer architecture.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "two ideas on generelization"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#residual-rerouting",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#residual-rerouting",
    "title": "two ideas on generelization",
    "section": "Residual rerouting",
    "text": "Residual rerouting",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "two ideas on generelization"
    ]
  },
  {
    "objectID": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#guided-bifuration-along-side-the-spontaneous-symetry-breaking",
    "href": "posts/2024/2024-07-01-generalization-in-ML/2024-07-01-gen-ml.html#guided-bifuration-along-side-the-spontaneous-symetry-breaking",
    "title": "two ideas on generelization",
    "section": "Guided bifuration along side the spontaneous symetry breaking",
    "text": "Guided bifuration along side the spontaneous symetry breaking",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 07 01 Generalization in ML",
      "two ideas on generelization"
    ]
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html",
    "href": "posts/2023/2023-05-10-migration-notes/index.html",
    "title": "The Great Migration",
    "section": "",
    "text": "I was able to stand on the shoulders of giants (Rapp 2022) (Navarro 2022), (Hill 2022), (Kaye 2022) when I migrated this blog.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "The Great Migration"
    ]
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#markdown",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#markdown",
    "title": "The Great Migration",
    "section": "Markdown",
    "text": "Markdown\n\nQuarto‚Äôs markdown isn‚Äôt my favorite markdown implementation.\nIt is based on pandoc spec",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "The Great Migration"
    ]
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#the-devil-is-in-the-details",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#the-devil-is-in-the-details",
    "title": "The Great Migration",
    "section": "The devil is in the details",
    "text": "The devil is in the details\nThere are lots of details that should be in the guide that are scattered all over the quarto site.\nI decided that all posts should have the following fields in their front matter:\n\ntitle\nsubtitle\ndescription\ndate\ncategories\nimage\nimage-description",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "The Great Migration"
    ]
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#virtual-environments",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#virtual-environments",
    "title": "The Great Migration",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nare documented here\nideal one can have one virtual environment for the whole site",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "The Great Migration"
    ]
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#lightbox-galleries",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#lightbox-galleries",
    "title": "The Great Migration",
    "section": "Lightbox Galleries",
    "text": "Lightbox Galleries\nso far I used this only in the this page\nthe light box plugin was integrated into Quarto in the version 4.1 which I migrated to. I have been using light box to make notes of talks and so on. So in for this blog adding light boxes is a breeze.\nAll that‚Äôs really needed is to change setting in the frontmatter:\nlightbox: true\nwhich I did for all posts by adding the setting to the _metadata.yaml in the posts directory. And now all images default to opening within their own lightbox when clicked upon.\nto disable the feature say, on a logo for example just add .no-lightbox css style to the image like this:\n![caption](filename.png){.no-lightbox}\nif you want to be able to scroll through a series of images we need to decorate each images as follows:\n![caption](filename.png){group=\"my-gallery\"}\nAn added bonus is that it is possible to zoom into these light-boxed images",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "The Great Migration"
    ]
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#extras",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#extras",
    "title": "The Great Migration",
    "section": "Extras",
    "text": "Extras\n\nthe about page is based on postcards package\nicons for navigation come from bootstrap\ncover images are from pexels\n\n\nOpen issues:\n\ncan I readily integrate books and presentation into this blog ?\n\ncan I drop them in or do I need to build them in another repo\nthen deploy\nthen link!?\n\nhow about embedding repls\nhow about embedding shiny live apps\n\nhttps://github.com/shafayetShafee\n\n\nEmbedding PDF\n\nplugin repo\ndocumentation\n\ninstallation\nquarto add jmgirard/embedpdf\n{{&lt; pdf dummy.pdf &gt;}}\n{{&lt; pdf dummy.pdf width=100% height=800 &gt;}}\n{{&lt; pdf dummy.pdf border=1 &gt;}}\n{{&lt; pdf dummy.pdf class=myclass &gt;}}",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "The Great Migration"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Validating NLP.html",
    "href": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Validating NLP.html",
    "title": "Validating NLP data and models",
    "section": "",
    "text": "NLP data, and unstructured data in general, is very hard to validate. Validating NLP data is a real challenge, as actions such as statistical analysis and segmentation, which are pretty straightforward on structured data, are not so easy to undertake. In this talk, we will look at common issues in NLP data and models, such as data and prediction drift, sample outliers and error analysis, discuss the ways they can impact our model performance, and show how we can detect these issues using the deepchecks open source testing package.\n\n\n\nNir Hutnik\n\n\n\n\n\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 28 NLP.IL Booking.com",
      "Validating NLP data and models"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Validating NLP.html#abstract",
    "href": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Validating NLP.html#abstract",
    "title": "Validating NLP data and models",
    "section": "",
    "text": "NLP data, and unstructured data in general, is very hard to validate. Validating NLP data is a real challenge, as actions such as statistical analysis and segmentation, which are pretty straightforward on structured data, are not so easy to undertake. In this talk, we will look at common issues in NLP data and models, such as data and prediction drift, sample outliers and error analysis, discuss the ways they can impact our model performance, and show how we can detect these issues using the deepchecks open source testing package.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 28 NLP.IL Booking.com",
      "Validating NLP data and models"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Validating NLP.html#speaker",
    "href": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Validating NLP.html#speaker",
    "title": "Validating NLP data and models",
    "section": "",
    "text": "Nir Hutnik",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 28 NLP.IL Booking.com",
      "Validating NLP data and models"
    ]
  },
  {
    "objectID": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Validating NLP.html#slides",
    "href": "posts/2023/2023-02-28-NLP.IL-Booking.com/NLP-IL-Booking Validating NLP.html#slides",
    "title": "Validating NLP data and models",
    "section": "",
    "text": "slide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide\n\n\n\n\n\nslide",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 02 28 NLP.IL Booking.com",
      "Validating NLP data and models"
    ]
  },
  {
    "objectID": "posts/2023/2023-06-01-spark/spark-progress.html",
    "href": "posts/2023/2023-06-01-spark/spark-progress.html",
    "title": "Spark Tips",
    "section": "",
    "text": "SPARK progress bar\nhttps://stackoverflow.com/questions/30245180/what-do-the-numbers-on-the-progress-bar-mean-in-spark-shell#:~:text=This%20progress%20indicator%20means%20that,number%20of%20tasks%20currently%20executing\n\n\n\n\nCitationBibTeX citation:@online{bochman2023,\n  author = {Bochman, Oren},\n  title = {Spark {Tips}},\n  date = {2023-06-01},\n  url = {https://orenbochman.github.io/posts/2023/2023-06-01-spark/spark-progress.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2023. ‚ÄúSpark Tips.‚Äù June 1, 2023. https://orenbochman.github.io/posts/2023/2023-06-01-spark/spark-progress.html.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 06 01 Spark",
      "Spark Tips"
    ]
  },
  {
    "objectID": "posts/2023/2023-06-01-Synthesis-and-Stabilization/2023-06-01-Synthesis-and-Stabilization.html",
    "href": "posts/2023/2023-06-01-Synthesis-and-Stabilization/2023-06-01-Synthesis-and-Stabilization.html",
    "title": "Summary: Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization",
    "section": "",
    "text": "In (Tassa, Erez, and Todorov 2012) the authors presents a method for online trajectory optimization, particularly focusing on complex humanoid robots performing tasks such as getting up from an arbitrary pose and recovering from large disturbances using dexterous maneuvers.\n\nTassa, Yuval, Tom Erez, and E. Todorov. 2012. ‚ÄúSynthesis and Stabilization of Complex Behaviors Through Online Trajectory Optimization.‚Äù 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 4906‚Äì13.\n\nRL setting & environment:\n\nThis is a model based continuous control RL method that uses\n\nMuJoCo short for Multi Joint dynamics with Contact, is thie author‚Äôs new physics simulator c.f.(Todorov, Erez, and Tassa 2012)\na humanoid robot is controlled in real time, using a physics simulator and has to get up from the ground and recover from disturbances.\n\nMATLAB-Based Environment: A real-time interactive environment where users can modify dynamics models, cost functions, or algorithm parameters, to model the dynamics of the robot with a Model Predictive Control (MPC) algorithm to synthesize control laws in real time. 1\nNote: MuJoCo soon became a standard part of RL environments.\n\nAlgorithm:\n\nModel Predictive Control (MPC) is used to synthesize control laws in real time.\n\nInnovations:\n\nMuJoCo Physics Engine: A new C-based, platform-independent, multi-threaded physics simulator tailored for control applications, significantly speeding up the computation of dynamics derivatives.\nImproved Iterative LQG Method: Enhancements to the iterative Linear Quadratic Gaussian (iLQG) method, including improved regularization and line-search techniques, resulting in increased efficiency and robustness.\nCost Functions: Introduction of cost functions that create better-behaved energy landscapes, more suitable for trajectory optimization.\n\nInnovations\n\nMuJoCo Physics Engine: A new C-based, platform-independent, multi-threaded physics simulator tailored for control applications, significantly speeding up the computation of dynamics derivatives.\nImproved Iterative LQG Method: Enhancements to the iterative Linear Quadratic Gaussian (iLQG) method, including improved regularization and line-search techniques, resulting in increased efficiency and robustness.\nCost Functions: Introduction of cost functions that create better-behaved energy landscapes, more suitable for trajectory optimization.\nMATLAB-Based Environment: A real-time interactive environment where users can modify dynamics models, cost functions, or algorithm parameters.\n\nExperimental Results:\n\nMPC avoids extensive exploration by re-optimizing movement trajectories and control sequences at each time step, starting at the current state estimate.\nDemonstrated the synthesis of complex behaviors such as getting up from the ground and recovering from disturbances, computed at near real-time speeds on a standard PC.\nApplied the method to simpler problems like the acrobot, planar swimming, and one-legged hopping, solving these in real time without pre-computation or heuristic approximations.\nShowed robustness to state perturbations and modeling errors, optimizing trajectories with respect to one model while applying resulting controls to another.\n\nTechnical Details:\n\nThe trajectory optimization involves solving a finite-horizon optimal control problem using iLQG.\nThe backward pass involves regularization techniques to ensure robustness, while the forward pass includes an improved line-search to ensure cost reduction and convergence.\nThe MuJoCo engine uses advanced contact modeling and parallel processing to handle the computational demands of online trajectory optimization.\n\n\n\nTodorov, E., Tom Erez, and Yuval Tassa. 2012. ‚ÄúMuJoCo: A Physics Engine for Model-Based Control.‚Äù 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 5026‚Äì33.\n1¬†worth considering in terms of interface/Notebook for future rl workSo let‚Äôs recap the main points of the paper:\nWikipedia has some details on the subject of MPC\n\nModel predictive control (MPC) is an advanced method of process control that is used to control a process while satisfying a set of constraints. ‚Ä¶ Model predictive controllers rely on dynamic models of the process, most often linear empirical models obtained by system identification. The main advantage of MPC is the fact that it allows the current timeslot to be optimized, while keeping future timeslots in account. This is achieved by optimizing a finite time-horizon, but only implementing the current timeslot and then optimizing again, repeatedly, thus differing from a linear‚Äìquadratic regulator (LQR). Also MPC has the ability to anticipate future events and can take control actions accordingly. ‚Äì (Wikipedia contributors 2024)\n\nWikipedia contributors. 2024. ‚ÄúModel Predictive Control ‚Äî Wikipedia, the Free Encyclopedia.‚Äù https://en.wikipedia.org/w/index.php?title=Model_predictive_control&oldid=1223992680.\n\nfor example, an example of a quadratic cost function for optimization is given by:\n\n{\\displaystyle J=\\sum _{i=1}^{N}w_{x_{i}}(r_{i}-x_{i})^{2}+\\sum _{i=1}^{M}w_{u_{i}}{\\Delta u_{i}}^{2}}\n\nwhere the goal is to minimize the difference between the reference and controlled variables without violating constraints (low/high limits) with\nhere:\n\n{\\displaystyle x_{i}} is the ith controlled variable (e.g.¬†measured temperature)\n{\\displaystyle r_{i}} is the ith reference variable (e.g.¬†required temperature)\n{\\displaystyle u_{i}} is the ith manipulated variable (e.g.¬†control valve)\n{\\displaystyle w_{x_{i}}} is a weighting coefficient reflecting the relative importance of {\\displaystyle x_{i}}\n{\\displaystyle w_{u_{i}}} is a weighting coefficient penalizing relative big changes in {\\displaystyle u_{i}}",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 06 01 Synthesis and Stabilization",
      "Summary: Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization"
    ]
  },
  {
    "objectID": "posts/2023/2023-06-01-Synthesis-and-Stabilization/2023-06-01-Synthesis-and-Stabilization.html#synthesis-and-stabilization-of-complex-behaviors-through-online-trajectory-optimization",
    "href": "posts/2023/2023-06-01-Synthesis-and-Stabilization/2023-06-01-Synthesis-and-Stabilization.html#synthesis-and-stabilization-of-complex-behaviors-through-online-trajectory-optimization",
    "title": "Summary: Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization",
    "section": "",
    "text": "In (Tassa, Erez, and Todorov 2012) the authors presents a method for online trajectory optimization, particularly focusing on complex humanoid robots performing tasks such as getting up from an arbitrary pose and recovering from large disturbances using dexterous maneuvers.\n\nTassa, Yuval, Tom Erez, and E. Todorov. 2012. ‚ÄúSynthesis and Stabilization of Complex Behaviors Through Online Trajectory Optimization.‚Äù 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 4906‚Äì13.\n\nRL setting & environment:\n\nThis is a model based continuous control RL method that uses\n\nMuJoCo short for Multi Joint dynamics with Contact, is thie author‚Äôs new physics simulator c.f.(Todorov, Erez, and Tassa 2012)\na humanoid robot is controlled in real time, using a physics simulator and has to get up from the ground and recover from disturbances.\n\nMATLAB-Based Environment: A real-time interactive environment where users can modify dynamics models, cost functions, or algorithm parameters, to model the dynamics of the robot with a Model Predictive Control (MPC) algorithm to synthesize control laws in real time. 1\nNote: MuJoCo soon became a standard part of RL environments.\n\nAlgorithm:\n\nModel Predictive Control (MPC) is used to synthesize control laws in real time.\n\nInnovations:\n\nMuJoCo Physics Engine: A new C-based, platform-independent, multi-threaded physics simulator tailored for control applications, significantly speeding up the computation of dynamics derivatives.\nImproved Iterative LQG Method: Enhancements to the iterative Linear Quadratic Gaussian (iLQG) method, including improved regularization and line-search techniques, resulting in increased efficiency and robustness.\nCost Functions: Introduction of cost functions that create better-behaved energy landscapes, more suitable for trajectory optimization.\n\nInnovations\n\nMuJoCo Physics Engine: A new C-based, platform-independent, multi-threaded physics simulator tailored for control applications, significantly speeding up the computation of dynamics derivatives.\nImproved Iterative LQG Method: Enhancements to the iterative Linear Quadratic Gaussian (iLQG) method, including improved regularization and line-search techniques, resulting in increased efficiency and robustness.\nCost Functions: Introduction of cost functions that create better-behaved energy landscapes, more suitable for trajectory optimization.\nMATLAB-Based Environment: A real-time interactive environment where users can modify dynamics models, cost functions, or algorithm parameters.\n\nExperimental Results:\n\nMPC avoids extensive exploration by re-optimizing movement trajectories and control sequences at each time step, starting at the current state estimate.\nDemonstrated the synthesis of complex behaviors such as getting up from the ground and recovering from disturbances, computed at near real-time speeds on a standard PC.\nApplied the method to simpler problems like the acrobot, planar swimming, and one-legged hopping, solving these in real time without pre-computation or heuristic approximations.\nShowed robustness to state perturbations and modeling errors, optimizing trajectories with respect to one model while applying resulting controls to another.\n\nTechnical Details:\n\nThe trajectory optimization involves solving a finite-horizon optimal control problem using iLQG.\nThe backward pass involves regularization techniques to ensure robustness, while the forward pass includes an improved line-search to ensure cost reduction and convergence.\nThe MuJoCo engine uses advanced contact modeling and parallel processing to handle the computational demands of online trajectory optimization.\n\n\n\nTodorov, E., Tom Erez, and Yuval Tassa. 2012. ‚ÄúMuJoCo: A Physics Engine for Model-Based Control.‚Äù 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, 5026‚Äì33.\n1¬†worth considering in terms of interface/Notebook for future rl workSo let‚Äôs recap the main points of the paper:\nWikipedia has some details on the subject of MPC\n\nModel predictive control (MPC) is an advanced method of process control that is used to control a process while satisfying a set of constraints. ‚Ä¶ Model predictive controllers rely on dynamic models of the process, most often linear empirical models obtained by system identification. The main advantage of MPC is the fact that it allows the current timeslot to be optimized, while keeping future timeslots in account. This is achieved by optimizing a finite time-horizon, but only implementing the current timeslot and then optimizing again, repeatedly, thus differing from a linear‚Äìquadratic regulator (LQR). Also MPC has the ability to anticipate future events and can take control actions accordingly. ‚Äì (Wikipedia contributors 2024)\n\nWikipedia contributors. 2024. ‚ÄúModel Predictive Control ‚Äî Wikipedia, the Free Encyclopedia.‚Äù https://en.wikipedia.org/w/index.php?title=Model_predictive_control&oldid=1223992680.\n\nfor example, an example of a quadratic cost function for optimization is given by:\n\n{\\displaystyle J=\\sum _{i=1}^{N}w_{x_{i}}(r_{i}-x_{i})^{2}+\\sum _{i=1}^{M}w_{u_{i}}{\\Delta u_{i}}^{2}}\n\nwhere the goal is to minimize the difference between the reference and controlled variables without violating constraints (low/high limits) with\nhere:\n\n{\\displaystyle x_{i}} is the ith controlled variable (e.g.¬†measured temperature)\n{\\displaystyle r_{i}} is the ith reference variable (e.g.¬†required temperature)\n{\\displaystyle u_{i}} is the ith manipulated variable (e.g.¬†control valve)\n{\\displaystyle w_{x_{i}}} is a weighting coefficient reflecting the relative importance of {\\displaystyle x_{i}}\n{\\displaystyle w_{u_{i}}} is a weighting coefficient penalizing relative big changes in {\\displaystyle u_{i}}",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2023",
      "2023 06 01 Synthesis and Stabilization",
      "Summary: Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization"
    ]
  },
  {
    "objectID": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html",
    "href": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html",
    "title": "Deep Learning Intuitions",
    "section": "",
    "text": "Stacking n-layers RELUS in a feed forward neural network is functionally equivalent to a set of nested inequalities.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 10 25 Deep Learning Relu Intutions",
      "Deep Learning Intuitions"
    ]
  },
  {
    "objectID": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html#for-a-2-layer-network",
    "href": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html#for-a-2-layer-network",
    "title": "Deep Learning Intuitions",
    "section": "For a 2 layer network",
    "text": "For a 2 layer network\nrelu = lambda x: x * gradient if x&gt; bias else 0 #relu\nx = np.random.randn(3, 1) \nactivation_1 = relu((W * x)+b)\nout = relu(np.dot( W,relu(np.dot(W_1,x)+b_1)+b_2)",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 10 25 Deep Learning Relu Intutions",
      "Deep Learning Intuitions"
    ]
  },
  {
    "objectID": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html#forward-pass-of-a-3-layer-neural-network",
    "href": "posts/2020/2020-10-25-deep-learning-relu-intutions/2020-10-25-deep-learning-relu-intutions.html#forward-pass-of-a-3-layer-neural-network",
    "title": "Deep Learning Intuitions",
    "section": "Forward-pass of a 3-layer neural network:",
    "text": "Forward-pass of a 3-layer neural network:\nf = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid) \nx = np.random.randn(3, 1) # random input vector of three numbers (3x1) \nh1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1) \nh2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations \n(4x1) out = np.dot(W3, h2) + b3 # output neuron (1x1)\nthe functional form of a DNN with one RELU layer looks like:\n\ny = ax+b\n\nA fully connected layers of RELUs with zero biases is just a set of inequalities\ne.g.\n\nx&gt;a \\text{ or } x \\in (a,\\infty) \\\\\nx&lt;b \\\\\ny&gt;c \\\\\ny&lt;c\n\nbased on the parameters. A second layer of RELU has second order inequalities e.g.¬†\nx &lt; b \\text{ and } x &gt; a \\text{ or x } \\in (a,b)\nx &gt; a \\text{ and }  y &gt; a \\text{ or } (x,y) \\in X",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 10 25 Deep Learning Relu Intutions",
      "Deep Learning Intuitions"
    ]
  },
  {
    "objectID": "posts/2020/2020-11-29-numpy-meltdown/2020-11-29-numpy-meltdown.html",
    "href": "posts/2020/2020-11-29-numpy-meltdown/2020-11-29-numpy-meltdown.html",
    "title": "numpy melt down",
    "section": "",
    "text": "TLDR\nJust a rant at numpy and scipi breaking when I realy needed them.\n\n\nNumpy meltdown\nSad to report but numpy (and scipy) installation started to fail on macos due to ending of support of the native Accelerate library provided by Apple.\nnumpy now depends on lapack/blas\nOf course having upgraded to macos 11 has not made it any easier to get things working smoothly either.\nalso brew install numpy --with-openblas\nno longer works either since the option was removed.\nthe main point is how shoddy python really is - everything most people do depends on numpy but numpy is a totally different project and have no qualms about their project breaking on a major platform and provide no fall back and no support.\nAnd why does one need numpy in the first place - lack of support in python for numeric processing and essential data structures. And being so slow that one requires it be done by a library written in a lower level language.\nJust saying these should never break - since they do the very basic processes of working with python are broken.\n\n\n\n\nCitationBibTeX citation:@online{bochman2020,\n  author = {Bochman, Oren},\n  title = {Numpy Melt Down},\n  date = {2020-11-29},\n  url = {https://orenbochman.github.io/posts/2020/2020-11-29-numpy-meltdown/2020-11-29-numpy-meltdown.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2020. ‚ÄúNumpy Melt Down.‚Äù November 29, 2020.\nhttps://orenbochman.github.io/posts/2020/2020-11-29-numpy-meltdown/2020-11-29-numpy-meltdown.html.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 11 29 Numpy Meltdown",
      "numpy melt down"
    ]
  },
  {
    "objectID": "posts/2020/2020-06-bash-tricks.html",
    "href": "posts/2020/2020-06-bash-tricks.html",
    "title": "brace expansion",
    "section": "",
    "text": "the bash shell supports brace expansion.\nthe idea is that the string before and after are concatenated with element in the braces\n\n\necho \"you won \"{two,three,four}\" points, \"\n\nyou won two points,  you won three points,  you won four points, \n\n\n\necho \"you won \"{1..10}\" points, \"\n\nyou won 1 points,  you won 2 points,  you won 3 points,  you won 4 points,  you won 5 points,  you won 6 points,  you won 7 points,  you won 8 points,  you won 9 points,  you won 10 points,",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "brace expansion"
    ]
  },
  {
    "objectID": "posts/2020/2020-06-bash-tricks.html#brace-expansion",
    "href": "posts/2020/2020-06-bash-tricks.html#brace-expansion",
    "title": "brace expansion",
    "section": "",
    "text": "the bash shell supports brace expansion.\nthe idea is that the string before and after are concatenated with element in the braces\n\n\necho \"you won \"{two,three,four}\" points, \"\n\nyou won two points,  you won three points,  you won four points, \n\n\n\necho \"you won \"{1..10}\" points, \"\n\nyou won 1 points,  you won 2 points,  you won 3 points,  you won 4 points,  you won 5 points,  you won 6 points,  you won 7 points,  you won 8 points,  you won 9 points,  you won 10 points,",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "brace expansion"
    ]
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html",
    "title": "Pandas Productivity Challenge?",
    "section": "",
    "text": "Just a mini-rant on Pandas. Pandas is a replacement for Excel and SQL for Python data scientist. I would this replacement should make us more productive than an analyst using Excel.\nPandas has a learning curve.\nIt is pretty strong when we consider automation of tasks, and applying a function of an algorithms that is not available in excel.\nIt is weak when it comes to anything interactive exploration is faster if you can interactively filtering, sorting, freeze headers, and your index columns, apply formatting and conditional formatting. Define pivots tables using drag and drop.\nFinally bushiness analysts often use BI tools and while R has Shiny Python is kind of weak in this regard as well particularly when working in a notebook.\nSo can we bridge this divide and make ourselves as agile and efficent on pandas as an analyst is in Excel ?",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 03 04 Pandas Challanges",
      "Pandas Productivity Challenge?"
    ]
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#pandas-or-point-and-click",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#pandas-or-point-and-click",
    "title": "Pandas Productivity Challenge?",
    "section": "Pandas or Point and Click",
    "text": "Pandas or Point and Click\nPandas is Python‚Äôs programmatic spreadsheet based on R‚Äôs DataFrames. R community is very pragmatic and the data frames have evolved to improve performance and increase agility. They have a tidyverse package and tribbles. Pandas lags behind and while easier to code then R it often requires more code to get things done and the code can get pretty ugly. Pandas advocates often point out that spreadsheets fail around 1.5 million cells. But what they fail to mention is that getting pandas to be fast on a large dataset requires deep understanding of pandas, its api, numpy. I won‚Äôt even go into memory management. I‚Äôd say most of the ugly code is going to be very slow on big data and you‚Äôll run out of memory. While SQL and excel have had serious effort at optimizing performance - pandas is pretty pathetic in this regard.\nI expected Pandas to be fast and intuitive on tasks like\n\nsubsetting by column type\nprinting subset rows by criteria of several columns\n\nThe first thing I expected is to be able to do tasks I did with excel or google sheets faster and better. What I mean is that I expected to map most tasks from one to the other and to be able to automate faster. Some tasks map better then others.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 03 04 Pandas Challanges",
      "Pandas Productivity Challenge?"
    ]
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#functional-programming",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#functional-programming",
    "title": "Pandas Productivity Challenge?",
    "section": "Functional Programming",
    "text": "Functional Programming\nI also noticed that Pandas provides access to subsets using [rows,cols] operator and the iloc and loc methods. I had also expected a modern functional interface to process data using RX style coding via functional primitives like map, flatmap, groupby, zip, filter, and so on. I notched some exist but no one seems to be using them in the idiomatic way. R‚Äôs tidyverse and Magrit had evolved very quickly in this direction why didn‚Äôt pandas?\nThe two biggest disappointments are reports and dashboards. Reports in excel are a no-brainer. Solid reporting can be essential when taking a data pipeline to production. Dashboards are a both a productivity enhancer and a power multiplier in BI tools like Tableau, Power BI, Google Data Studio, informatica etc. Interactive dashboards can be amazing for exploring datasets that change a lot like marketing. These are all challenges with python and pandas in a Jupyter notebook as a starting point.\nWhile hacking functional programming into pandas should be not to hard I believe that engineering it would be more sensible as it would allow high performance optimizations for larger datasets.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 03 04 Pandas Challanges",
      "Pandas Productivity Challenge?"
    ]
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#reports",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#reports",
    "title": "Pandas Productivity Challenge?",
    "section": "Reports",
    "text": "Reports\nI expected to be able to create a tabular display for my data with filtering, sorting, conditional styles, paging, sparklines. Pandas styler does some of these and there are a number of libraries that do paging sorting filtering based on javascript datatables library other allow sparklines and charts based on chartjs. They don‚Äôt work together - each does its own thing.. But they are so fragile, relying on javascript and breaking as one changes to different notebook. This is a major problem with python - there are many environments and no established method way to get code to work well on most of these.\nYou might notice that the a big issues is interoperability. Ideally there should be support for reports with:\n\nset number of rows displayed (all, 10 etc)\npaging\nfield filtering (interactive)\nscrolling to show all columns\nColumn\n\nvisual indication of type (numeric, categorical, ordinal, temporal, time series, geojson)\nvisual summaries\n\nbarchart or area chart - with tooltips for leading values\nbars with error indicators\n\n\nSparklines (line chart, histogram, candle chart)\nPivot tables\n\nBinning\nWith sub-aggregates and over all aggregates.\n\nConditional formatting (heatmap, barchart)\nHighlighting (missing values, minimum, maximum, median, mode, quartile, outliers, mean & number of sd, custom maps).\nSampling of most interesting rows‚Ä¶",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 03 04 Pandas Challanges",
      "Pandas Productivity Challenge?"
    ]
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#interactive-pivot-tables",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#interactive-pivot-tables",
    "title": "Pandas Productivity Challenge?",
    "section": "interactive pivot tables",
    "text": "interactive pivot tables\nPivottable.js, interactive pivot tables and charts\n\nInstallation\n!pip install pivottablejs\nfrom pivottablejs import pivot_ui\npivot_ui(df,outfile_path=‚Äôpivottablejs.html‚Äô)\nHTML(‚Äòpivottablejs.html‚Äô)",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 03 04 Pandas Challanges",
      "Pandas Productivity Challenge?"
    ]
  },
  {
    "objectID": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#interactive-sorting-and-filtering",
    "href": "posts/2020/2020-03-04-pandas-challanges/2020-03-04-pandas-challanges.html#interactive-sorting-and-filtering",
    "title": "Pandas Productivity Challenge?",
    "section": "interactive sorting and filtering",
    "text": "interactive sorting and filtering\n\nqgrd",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2020",
      "2020 03 04 Pandas Challanges",
      "Pandas Productivity Challenge?"
    ]
  },
  {
    "objectID": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html",
    "href": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html",
    "title": "Phrasal Verbs",
    "section": "",
    "text": "Back when I was staying in Budapest I took some Hungarian classes at the ‚ÄúDebreceni Ny√°ri Egyetem‚Äù. After a year I got through some of the basic levels. I a book store with some titles by their publisher‚Äôs at greatly reduced prices so I bought ‚ÄúIg√©z≈ë szotar‚Äù, a dictionary of phrasal verbs. However I found a too advanced to be very useful at the time.\nIt turns out most Hungarian verbs can take up to 20 prefixes to form a phrasal verb. Some of these are not always intuitive and can be very idiomatic. However there are only 20 prefixes and these prefixes are related to postpositions that attach to nouns as suffixes.\nIt seems to offer a quick windfall for expanding your vocabulary. The conjugation of Hungarian verb can easily take as many as 70 forms. Once we factor in the number of prefixes the number of forms quickly jumps to as many as 1400. This is a huge number of forms to learn. However the good news is that the conjugation of the main verb is the same for all the phrasal verbs that are formed with the same prefix. Even better news is once you mastered all the phrasal verbs for just two of three verb your eyes will be able to decipher any other phrasal verb that you come across.\nAlso the Hungarian lexicon is highly agglutinative. This means that you might encounter a word embedded within some other word. For example a computer is called a ‚Äúszamitogep‚Äù which is a compound word made up of ‚Äúszamit‚Äù which means to calculate and ‚Äúgep‚Äù which means machine. Or to photograph which is ‚Äúfenykepezik‚Äù which is made up of ‚Äúfeny‚Äù which means light and ‚Äúkep‚Äù which means picture.\nYou might not know the exact meaning of the phrasal verb but you will be able to guess the meaning of the phrasal verb but you should be able to partition the verb into a prefix, the main verb and the conjugation suffix. You will have a rough idea of what each of these might mean and you might now also be better able to guess the meaning of the phrasal verb based on the context in which it is used. At least if you are able to make sense the words belonging to the phrasal verb slots.\nOnce you have mastered the conjugation of any of the main verb these phrasal verbs follow the same conjugation rules. This means that with little more work you are well on your way to mastering many semantically related phrasal verbs. The outcome is that the vocabulary of the language is highly regular and mostly predictable.\nThe challenge isn‚Äôt the prefixes but what semantics slots become available when you add a prefix to a verb. This is where the idiomatic nature of the language comes into play. Because we are now talking about a new semantic space that part is not always predictable.\nHowever this is a case where we can transfer the semantics of the verb from say english.\nit has no English translation. However there is a smaller volume that goes along that is bilingual\nIt is It is a dictionary of verbs that are conjugated in various ways.\nThe prefixes\nthis makes use of many abbreviations"
  },
  {
    "objectID": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html#sec-ad",
    "href": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html#sec-ad",
    "title": "Phrasal Verbs",
    "section": "AD | to Give",
    "text": "AD | to Give\n\n\n\nverb\nphrase\nenglish\n\n\n\n\nad\nad\ngive\n\n\nad\nad vmit\ngive sth\n\n\nad\nad vkinek vmit\ngive sth to sy\n\n\n√°t+ad\n√°tad vkinek vmit\nhand over sth to sy\n\n\n√°tad\n√°tadja a szot vkinek\ncall upon sy to speak\n\n\n√°tad\n√°tadja az √ºdv√∂zletet vkinek\nconvey sy‚Äôs best wishes to sy,\nsay hello to sy for sy\n\n\n√°tad\n√°tadja a hely√©t vkinek\ngive up one‚Äôs seat to sy\n\n\nbead\nbeadja a gy√≥gyszert vkinek\nadminister medicine to sy\n\n\nbead\nbeadja a k√©rv√©nyt vhov√°\nsubmit / present a request; file a petition\n\n\nbead\nbeadja a felmond√°s√°t\nhand in one‚Äô s notice\n\n\nelad\nelad vmit vmennyi√©rt\nsell sth for sth\n\n\nel≈ëad\nel≈ëad vmit\nperform, enact, recite etc. sth\n\n\nel≈ëad\nel≈ëad\nlecture\n\n\nfelad\nfelad (pl. levelet, csomagot)\npost (e.g.¬†a letter, a parcel)\n\n\nfelad\nfeladja a kab√°tot vkire\nhelp sy on with their coat\n\n\nfelad\nfeladja a leck√©t\nassign homework\n\n\nhozz√°ad\nhozz√°ad vmihez vmit\nadd sth to sth\n\n\nhozz√°ad\nhozz√°ad vkihez vkit\nmarry sy off\n\n\nidead\nidead vmit\ngive sth to me\n\n\nkiad\nkiad egy k√∂nyvet\npublish a book\n\n\nkiad\nkiad egy szob√°t / egy lak√°st\nrent a room / a house\n\n\nkiad\nkiadja a munk√°t vkinek\ndistribute work\n\n\nkiad\nkiadja a m√©rg√©t / d√ºh√©t\nvent one‚Äôs rage\n\n\nlead\nlead vmit vhol\nleave sth swhere, hand sth in\n\n\nlead\nlead vmennyit\nlose weight\n\n\nmegad\nmegadja az ad√≥ss√°g√°t\npay off one‚Äôs debts\n\n\nmegad\nmegadja az enged√©lyt\ngive permission to sy\n\n\nmegad\nmindent megad vkinek\nlet sy have evrything\n\n\n√∂sszead\n\n\n\n\n√∂sszead\n√∂sszeadja a szamokat\nadd up numbers\n\n\n√∂sszead\n√∂sszead vkiket\nmarry a couple\n\n\nr√°ad\nr√°adja vkire a kab√°tj√°t\nhelp sy on with their coat\n\n\ntov√°bbad\ntov√°bbad vmit vkinek\npass sth on to sy\n\n\nvisszaad\nvisszaadja a tartoz√°s√°t\npay back one‚Äôs debt\n\n\nvisszaad\nvisszaadja sz√°sz forintot\ngive back 100 forints change back\n\n\nvisszaad\nvisszaadja a k√∂lcs√∂nt\nreturn a loan\n\n\nvisszaad\nvisszaadja a p√©nzt\ngive back the money\n\n\nvisszaad\nvisszaadja a v√°laszt\ngive back the answer\n\n\nvisszaad\nvisszaadja a k√∂nyvet\nreturn a book\n\n\nvisszaad\nvisszaadja a kulcsot\nreturn a key\n\n\n\nFrazeologizmusok\n\n\n\n\n\n\n\nbeadja a derek√°t\ngive in, accept sth\n\n\nbeadja apait-anyait\ngive sy a piece of one‚Äôs mind\n\n\nbeadja a sz√≥t\ngive sy the floor\n\n\neladja mag√°t/ a lelk√©t/ a becs√ºlet√©t\nsell oneself / one‚Äôs soul / one‚Äôs honour\n\n\neladja a lelk√©t az √∂rd√∂gnek\nsell one‚Äôs soul to the devil\n\n\nfeladja a harcot\ngive up the fight\n\n\nfeladja a leck√©t\nassign a difficult task\n\n\nkiadja a leck√©t\ndie / draw one‚Äôs last breath\n\n\nmegadja mag√°t a sorsnak\nresign oneself to one‚Äôs fate"
  },
  {
    "objectID": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html#sec-all",
    "href": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html#sec-all",
    "title": "Phrasal Verbs",
    "section": "√ÅLL",
    "text": "√ÅLL\n\n\n\n√°t√°ll\n\n\n\n\n\n√°t√°ll\n\n\n\n\n\n√°t√°ll\n\n\n\n\n\nbe√°ll\n\n\n\n\n\nbele√°ll\n\n\n\n\n\nel√°ll\n\n\n\n\n\nellen√°ll\n\n\n\n\n\nfel√°ll\n\n\n\n\n\nf√©lre√°ll\n\n\n\n\n\nfenn√°ll\n\n\n\n\n\nkia√°ll\n\n\n\n\n\nk√∂r√º√°ll\n\n\n\n\n\nle√°ll\n\n\n\n\n\nmeg√°ll\n\n\n\n\n\nr√°√°ll\n\n\n\n\n\nszem√°ll\n\n\n\n\n\nv√©gig√°ll\n\n\n\n\n\nvissza√°ll"
  },
  {
    "objectID": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html#sec-√°ll√≠t",
    "href": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html#sec-√°ll√≠t",
    "title": "Phrasal Verbs",
    "section": "√ÅLL√çT",
    "text": "√ÅLL√çT"
  },
  {
    "objectID": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html#sec-besz√©l",
    "href": "hungarian/2024-02-11-phrasal-verbs/2024-02-11-phrasal-verbs.html#sec-besz√©l",
    "title": "Phrasal Verbs",
    "section": "BESZ√âL",
    "text": "BESZ√âL\n\nigezo-szotar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt would seem that this looks very similar to a frame based approach to language modeling.\nI wander if it can be used as a basis for template for generating a phrasal verb dictionary.\nsee also:\n\nhttps://www.hungarianpod101.com/blog/2020/10/05/hungarian-conjugations/\ncovers conjugation of Hungarian verbs\n\n\n\nhttps://myhunlang.com/2010/04/01/phrasal-verbs-part-1/\nhttps://myhunlang.com/2010/04/02/phrasal-verbs-part-2/\nconsiders the strange case of meg-, el-, le- which add an aspect of completeness to a verb!? and when the prefix can replace the verb in a response.\nhttps://myhunlang.com/2010/04/02/phrasal-verbs-part-3/\nexplains when the verbal prefix can separate from and follow the verb\n\nHere is some steps to automate construction of a hungarian phrasal lexicon I will call the prefix here a co-verb\n\ninput verb, coverb\nestimate \\mathbb{E}(coverb), \\mathbb{E}[verb] and \\mathbb{E}(coverb,verb)\ngenerate/lookup all conjugations for the verb\nadd prefix all conjugations\ngenerates all conjugations for the verb\ncount the \\mathbb{E}(coverb,verb,suffix) and infer from these the \\mathbb{E}(coverb,verb)\ncount the \\mathbb{E}(coverb,verb,suffix) and infer from these the \\mathbb{E}(coverb,verb)\nnote that we might prefer to do this inference using all possible coverbs including the null coverb.\nestimate if \\mathbb{E}[coverb|verb] &gt; 0\nestimate if \\mathbb{E}[coverb|verb] \\approx  \\mathbb{E}[coverb]\\times \\mathbb{E}[verb] i.e.¬†are the two morphemes independent indicating they are likely just concatenated\nif we have an inequality we could be looking at a collocation.\nNext we should look at the sentence and identify nouns with case endings\ni.e.¬†\\mathbb{E}[case|coverb,verb]\ne.g.¬†give something\nand \\mathbb{E}[case|case,coverb,verb]\ne.g.¬†give something to someone\nwe should also look for evidence of specific uses i.e.¬†when the prefixes and coverb are linked to specific words rather than cases\ni.e.¬†and \\mathbb{E}[case|case,coverb,verb]\ne.g.¬†sell your soul to the devil\nthese collocations could be idioms, cliches or phrases.\nbut in Hungarian with rich morphological settings and with flexible word order these probablities would be much harder to estimate or infer.\nAlso in general one might need to also consider if there are other lexemes participating (i.e.¬†particles etc other than nouns as well as coverbs in isolated form.)\n\nThe challenge of course would be to establish that the phenomena is within confidence levels. For any given corpus size combining enough features (i.e.¬†conditioning, quickly leads to sparsity which means it would be hard to find a threshold for a confidence interval separating real constructs from noise.\nIn (Pajzs 2002) the authors only considered a few common verbs to work with\nthe main advantage for this approach might be that one would also have frequencies, possibly by year and thus be able to prioritize language learning tasks."
  },
  {
    "objectID": "posts/2024/2024-03-27-gradio/gradio_local.html",
    "href": "posts/2024/2024-03-27-gradio/gradio_local.html",
    "title": "gradio local model",
    "section": "",
    "text": "import gradio as gr\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\nfrom threading import Thread\n\ntokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\nmodel = model.to('cuda:0')\n\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -&gt; bool:\n        stop_ids = [29, 0]\n        for stop_id in stop_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False\n\ndef predict(message, history):\n    history_transformer_format = history + [[message, \"\"]]\n    stop = StopOnTokens()\n\n    messages = \"\".join([\"\".join([\"\\n&lt;human&gt;:\"+item[0], \"\\n&lt;bot&gt;:\"+item[1]])\n                for item in history_transformer_format])\n\n    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n    generate_kwargs = dict(\n        model_inputs,\n        streamer=streamer,\n        max_new_tokens=1024,\n        do_sample=True,\n        top_p=0.95,\n        top_k=1000,\n        temperature=1.0,\n        num_beams=1,\n        stopping_criteria=StoppingCriteriaList([stop])\n        )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    partial_message = \"\"\n    for new_token in streamer:\n        if new_token != '&lt;':\n            partial_message += new_token\n            yield partial_message\n\ngr.ChatInterface(predict).launch()\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 2\n      1 import gradio as gr\n----&gt; 2 import torch\n      3 from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n      4 from threading import Thread\n\nModuleNotFoundError: No module named 'torch'\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {Gradio Local Model},\n  date = {2024-03-27},\n  url = {https://orenbochman.github.io/posts/2024/2024-03-27-gradio/gradio_local.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. ‚ÄúGradio Local Model.‚Äù March 27, 2024.\nhttps://orenbochman.github.io/posts/2024/2024-03-27-gradio/gradio_local.html.",
    "crumbs": [
      "Home",
      "Blog",
      "Posts",
      "2024",
      "2024 03 27 Gradio",
      "gradio local model"
    ]
  }
]