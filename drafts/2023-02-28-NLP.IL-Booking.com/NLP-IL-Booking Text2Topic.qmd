---
title: Text2topic  Leverage reviews data for multi-label topics classification in Booking.com
subtitle: NLP.IL
date: 2023-02-28
---

# Text2topic  Leverage reviews data for multi-label topics classification in Booking.com - Moran Beladev & Elina Frayerman

## Abstract:

Having millions of customer reviews, we would like to better understand them and leverage this data for different use cases. For example, finding popular activities per destination, detecting popular facilities per property, allowing the users to filter reviews by specific topics, detecting violence in reviews and summarizing most discussed topics per property.

In this talk, we will present how we build a multilingual multi-label topic classification model that supports zero-shot, to match reviews with unseen usersâ€™ search topics.

We will show how fine-tuning BERT-like models on the tourism domain with a small dataset can outperform other pre-trained models and will share experiment results of different architectures.

Furthermore, we will present how we collected the data using an active learning approach and AWS Sagemaker ground truth tool, and we will show a short demo of the model with explainability using Streamlit.

## Moran Beladev Bio: 

Moran is a machine learning manager at booking.com, researching and developing computer vision and NLP models for the tourism domain. Moran is a Ph.D candidate in information systems engineering at Ben Gurion University, researching NLP aspects in temporal graphs. Previously worked as a Data Science Team Leader at Diagnostic Robotics, building ML solutions for the medical domain and NLP algorithms to extract clinical entities from medical visit summaries.

{{< video https://www.youtube.com/watch?v=TS99fPQbFqc&list=PL5zieHHAlvAqoGCKxj-orzBAMkP4zLsnl&t=1800s 
    title = "Review2topic: Building Topics Detection Model to Leverage Reviews Data in Booking.com"
>}}

## Slides


![slide](session1/ss001.png){}

![slide](session1/ss002.png){}

![slide](session1/ss003.png){}

![slide](session1/ss004.png){}

![What is CIP](session1/ss005.png){}

![What is CIP](session1/ss006.png){}

![Text2Topic](session1/ss007.png){}

![Overview](session1/ss008.png){}

![Data Sources](session1/ss009.png){}

![Data Sources](session1/ss010.png){}

![Data Sources](session1/ss011.png){}

![Data Sources](session1/ss012.png){}

![Data Sources](session1/ss013.png){}

![Motivation/Goals](session1/ss014.png){}

![slide](session1/ss015.png){}

![How it Works?](session1/ss016.png){}

![Cross Encoder architecture](session1/ss017.png){}

![Cross Encoder architecture](session1/ss018.png){}

![Bi-Encoder architecture](session1/ss019.png){}

![Bi-Encoder architecture](session1/ss020.png){}

![Bi-Encoder architecture](session1/ss021.png){}

![Bi-Encoder architecture](session1/ss022.png){}

![Bi-Encoder self-supervised](session1/ss023.png){}

![Main Differences](session1/ss024.png){}

![Dynamic Padding](session1/ss025.png){}

![Dynamic Padding](session1/ss026.png){}

![Dynamic Padding](session1/ss027.png){}

![Evaluation](session1/ss028.png){}

![Results](session1/ss029.png){}

![Metrics](session1/ss030.png){}

![Results](session1/ss031.png){}

- note Muse-large used as a baseline!

![slide](session1/ss032.png){}

![slide](session1/ss033.png){}

![slide](session1/ss034.png){}

Well done! They did the experiment way past the point where the effects maxed. The main takeaway here is that 100 docs suffice for getting good results on a new topic.

![slide](session1/ss035.png){}

![slide](session1/ss036.png){}

![slide](session1/ss037.png){}


Great talk - the padding tip is probably worth the price of admission :-)
