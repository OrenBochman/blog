---
draft: true
title: MCMC algorithms
revealjs: 
    html-math-method: katex
    chalkboard: 
      buttons: false
    preview-links: auto
    css: styles.css
---

so I've simplified all this using a quarto  pseudocode plugin
```bash
quarto add leovan/quarto-pseudocode
```
based on [
pseudocode.js
Public](https://github.com/SaswatPadhi/pseudocode.js)

# MCMC Algorithms
## Metropolis-Hastings
```pseudocode
#| label: alg-metropolis-hastings
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true
\begin{algorithm}
\caption{Metropolis-Hastings Algorithm}
\begin{algorithmic}[1]
\Procedure{MetropolisHastings}{$\mathbf{\Theta_0}, p(\mathbf{\Theta}), q(\mathbf{\Theta}|\mathbf{\Theta}')$}
\State Initialize $\mathbf{\Theta} \gets \mathbf{\Theta_0}$
\For{$i = 1, 2, \dots, N$}
\State Draw $\mathbf{\Theta}' \sim q(\mathbf{\Theta}'|\mathbf{\Theta})$
\State Calculate acceptance ratio $\alpha = \frac{p(\mathbf{\Theta}')q(\mathbf{\Theta}|\mathbf{\Theta}')}{p(\mathbf{\Theta})q(\mathbf{\Theta}'|\mathbf{\Theta})}$
\State Draw $u \sim \text{Uniform}(0, 1)$
\If{$u < \alpha$}
\State Accept proposed move: $\mathbf{\Theta} \gets \mathbf{\Theta}'$
\Else
\State Reject proposed move: $\mathbf{\Theta} \gets \mathbf{\Theta}$
\EndIf
\EndFor
\State \textbf{return} ${\mathbf{\Theta}_1, \mathbf{\Theta}_2, \dots, \mathbf{\Theta}_N}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
```
The procedure `MetropolisHastings` takes as input :
- the target distribution $p(x)$, 
- the proposal distribution $q(x,y)$, 
- the initial sample $x_0$, and
- the total number of samples to generate $N$. 
The procedure returns:
- the sequence of samples $(x_0, x_1, \ldots, x_N)$.

# Gibbs Sampling
```pseudocode
#| label: alg-gibbs-sampling
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true

\begin{algorithm}[h]
\caption{Gibbs Sampling algorithm}
\begin{algorithmic}[1]
\Procedure{GibbsSampling}{$p(x,y), x^{(0)}, y^{(0)}, N$}
\State Initialize $x_0 = x^{(0)}$ and $y_0 = y^{(0)}$.
\For{$t=1$ to $N$}
\State Sample $x_t \sim p(x|y_{t-1})$.
\State Sample $y_t \sim p(y|x_t)$.
\EndFor
\State \textbf{return} $(x_1, \ldots, x_N), (y_1, \ldots, y_N)$
\EndProcedure
\end{algorithmic}
\end{algorithm}

```
The procedure `GibbsSampling` takes as input :
- the joint distribution $p(x,y)$, 
- the initial values for $x$ and $y$
-  ($x^{(0)}$ and $y^{(0)}$), and 
- the total number of samples to generate $N$. 
The procedure returns:
-  the sequences of samples for $x$ and $y$, $(x_1, \ldots, x_N)$ and $(y_1, \ldots, y_N)$, respectively

```pseudocode
#| label: alg-invsamp
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true
\begin{algorithm}
\caption{Inverse Sampling algorithm}
\begin{algorithmic}
\Procedure{InverseSampling}{$F^{-1}(u), U_1, \ldots, U_N$}
    \For{$i=1$ to $N$}
        \State Generate a uniform random number $u_i \sim U(0,1)$.
        \State Compute $x_i = F^{-1}(u_i)$.
    \EndFor
    \State \textbf{return} $(x_1, \ldots, x_N)$
\EndProcedure
\end{algorithmic}
\end{algorithm}
```
The procedure `InverseSampling` takes as input :
- the inverse cumulative distribution function $F^{-1}(u)$ and 
- the number of samples to generate $N$.
 The procedure generates $N$ uniform random numbers $u_1, \ldots, u_N$ and computes the corresponding samples $x_1, \ldots, x_N$ using the inverse cumulative distribution function $F^{-1}(u)$.
 
  The procedure returns:
  - the sequence of samples $(x_1, \ldots, x_N)$.
Note: that in this algorithm, we assume that the inverse cumulative distribution function $F^{-1}(u)$ is available, and can be used to generate samples from a distribution with cumulative distribution function $F(x)$.

```pseudocode
#| label: alg-hmc
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true
\begin{algorithm}[h]
\caption{Hamiltonian Monte Carlo algorithm}
\begin{algorithmic}[1]
\Procedure{HamiltonianMC}{$\pi(x), \nabla \log \pi(x), L, \epsilon, M$}
\State Initialize $x_0$.
\For{$m=1$ to $M$}
\State Sample momentum $p_m \sim \mathcal{N}(0, I)$.
\State Set $x = x_{m-1}$ and $p = p_m$.
\State Simulate Hamiltonian dynamics for $L$ steps with step size $\epsilon$:
\For{$l=1$ to $L$}
\State Update momentum: $p \leftarrow p - \frac{\epsilon}{2} \nabla \log \pi(x)$.
\State Update position: $x \leftarrow x + \epsilon p$.
\EndFor
\State Flip the momentum: $p \leftarrow -p$.
\State Compute the Metropolis-Hastings acceptance probability:
\State $\alpha = \min \left(1, \frac{\pi(x')}{\pi(x)} \frac{p(x|x')}{p(x'|x)} \right)$, where $x' = x$ and $p' = p$ after the simulation.
\State Accept or reject the proposal:
\State With probability $\alpha$, set $x_m = x'$.
\State With probability $1-\alpha$, set $x_m = x_{m-1}$.
\EndFor
\State \textbf{return} $(x_1, \ldots, x_M)$
\EndProcedure
\end{algorithmic}
\end{algorithm}
```
In this algorithm statement, the Hamiltonian Monte Carlo algorithm is encapsulated in a procedure called \textsc{HamiltonianMC}. 
The procedure `HamiltonianMC` takes as input:
-  the target distribution $\pi(x)$, 
- its gradient $\nabla \log \pi(x)$,
-  the number of steps to simulate Hamiltonian dynamics $L$, 
- the step size $\epsilon$, and
-  the total number of samples to generate $M$. 
The procedure returns:
-  the sequence of samples $(x_1, \ldots, x_M)$.
Note: that in this algorithm, we first sample a momentum variable $p$ from a normal distribution, then simulate Hamiltonian dynamics for $L$ steps using the leapfrog method. We then compute the Metropolis-Hastings acceptance probability based on the updated proposal, and accept or reject the proposal according to this probability. We repeat this process for $M$ iterations to generate the desired samples.