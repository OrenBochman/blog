---
title: lobe formation
categories:
    - deep neural networks
    - brain fart
---

# The Signal and Noise.
Alice : "What is the reason that models overfit?"
Bob: "A simple explaination for models overfitting is that they are unable to discriminate between the signal from the noise when they learn, so they learn both."
Alice "Why is overfitting so common?"
Bob "Well there is this thing called effect size and for the model to learn it it need to see enough evidence that it can be fairly confident the essfect is real and not due to chance, there may be two effects - one might be the price and the other is that we carry some unique products and a third might be some promotion we did"
Alice: "What do you men by chance ?"
Bob: "Say we want to learn what kind of people like our store enogh to come back. We collect data for a week and build a model. When we check it out we might find out that the model's prediction mostly relating to if people who stooped to ask directions to some events happening across the steets - Not clients"
Alice "How is that chance?"
Bob "Well if we went on collecting for more days and hours when there are no events taking place we might actually start learning about our customers. And the fact that we had
so many vistiors was due to bad luck of running our survay in that specific data and  time."
Alice "Ok and these effect, why are they hard to track?" 
Bob "So the model is easily fooled by randomness say we run some realy good promotion but there was poor weather one time and the bus cancled the nearby stop the second time  then poor weather again - we then look at the data and the promotion is not so great, particularly since we dont collect data on the buss or the weather and we can't conroll for these factors, unless we do some extra work" 
Alice "So what doe one do?"
Bob "We could improve our survey but there will always be more confounders out there, what we tend to do is to increase our sampling proceedure, and make it more random. We might also sample at a secondry location, run the survey longer and at different days and times. But if the effect has a lower power the only thing that will realy work is collecting more data."
Alice "Why is that?"
Bob "Well most of our clients are adults but we occesinaly sell to kids or to soldiers. I'd say the odds we to a kid is one to thirty, and the odds we sell to a soldier is one in two hundred. Over a year both add up  but if we just look at a day or even a week there might not be any such sales."





 you model can lean the main effects but to lean the more interesting parts is often more of a challange because 
they are sparse. So the model is going to be overwhelmed by evidence that point at some effect that has arrisen mostly though chance this is what I call noise. With more data the noise tends to cancel out. But no matter how much data you get there will be ever larger more conving evidence for new pattern. Most real datasets are mostly with the signal having a power law distribution." sparse so the due to there will evidence  data th  for  and they will often  
The problem is that after the model learn the main effects the the noise overtakes the signal. 
What do I mean by noise,  is the noise  
let's try to formalise signal and noise
we have binary sequences
|--|--|----|----|
|0 |10|1100|1101|

The simplest remedy is to get more data. With more data the more of the signal should overcome more of the noise because the signal is addative while the noise being random is not. What I mean is that the signal is a consistant patten while the noise is a random pattern. If we double the data, we expect to double the signal while the noise avarages out.
Why do we need to double the model - perhaps we dont but the model has a certain capacity. If we have a long signal it will need to compete with shorter noise patterns, some of which will be more frequent. So we unless we can avoid fitting noise agressively we need to also increase the capacity of the model.
My idea is to consider a model as a topology - connecting inputs to outputs. by partitioning the model into subtopologies which I call lobes and routing related signal to those partitions I hope to make better use of the model capacity and increase the signal to noise ratio in these specific sub topologies. An added bonus is that the model should be able to learn faster.
challanges:
1. lets call a path from the input to the output is a `neural path` or `NP`
1. we need to have a model with sufficent `NP` capacity to represent the main substructures in our data.
1. we want to map inputs or input batches to neural pathways using a router.
1. the router will allow pathwwys to specilize.
1. a first order router should pick a subtopology with
    1. one core neural path from inputs to outputs
    1. additional capacity by adding neural pathways with shared nodes 
    1. randomly damage neural pathways that are further from the core neuroal pathway.
this is only enough to build neural pathways. To build lobes we want to have related neral pathways preferentialy have shared nodes and to be close together.
The Second order router will :
    1. needs to handle batches with two classess of outputs.
    1. it will will pick routes that are union of two neural pathways.
    1. If the NP do not cross it will pick at least one extra NP that it will also persitence of crossover pathwats to cross the above topologies. I
 
1. picking an intial model with a topology that can be split into 
   many different sub topologies.
1. splitting the model into topologies that are  related topologies
1. randomizing the 
1. finding which inputs have related signals.
1. finding an initial model topology that can be split into a big enough larger models.
1. making the subtopologies specilize

so we need to learn two things at once
1. the router 
2. the main model 
 