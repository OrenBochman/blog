---
title : scrach pad
draft: true
---

this is a scratch pad for intermediate stuff for working on posts or groups of posts.

1. using the python environment 

```{bash}
source env/bin/activate
```

## LLM prompts for working with video tanscripts

the following are prompt I evolved for working with transcripts to get a faster start 
on notes using video of transcript.

### Transcript summary

> Please help me to summarize the following lesson transcript for my course notes. Match the speaker's language. Pay special attention not to omit technical details


### Blog summary:

> Please help me to summarize the following blog post for my course notes while matching the original language as best as you can. Pay special attention to any technical details

### Insights

- working with transcripts gets you most of what was said in the course.
- most of what was said is not important we need to abstract it.
- I can rewrite sentences to make them shorter. But if the instructed is confused this will only help so much.
- highlighting using [content goes here]{.mark} is great for pointing out the important stuff.
- notes with one highlight per paragraph are very boring to read.
- good lectures have structure.
- good papers have structure.
- so how can we make the notes less boring and more compelling reading???
    - first we drop most of the boring stuff
    - reduce content into a list or points and convert to short paragraphs.
- think about presenting using sketch notes !?

 - mermaid charts are helpful
 - asciiflow [asciiflow.com]


## regex

remove time codes from transcripts
pure numeric line numbers from transcripts


%20 --> space
.([0-9]+).([0-9]+).png  --> -$1-$2.png

.align-center width="450px" height="300px"
-->
.column-margin


## quarto the missing utils

start by running
```{bash}
quarto render --log site.log --log-level info
```

from this log you can grep the following

|grep | content|
|---|---|
|`Cite`|missing citations|
| `????` | missing images|
| `packages are required`| missing package|
|`meta`| posts with missing metadata|



## transcipts





some code for working with 
```{python}
# get all the transcripts in a folder

import os
import glob
import re

#path = '/some/path/to/file'
path = ""
tokens = []
for filename in glob.glob(os.path.join(path, '*.vtt')):
   with open(os.path.join(os.getcwd(), filename), 'r') as f: # open in readonly mode
      # do your stuff
      tokens = []
      for line in f:
        transcript_1 = re.sub(r'^\d{2}.*\n?', '', line, flags=re.MULTILINE)
        transcript_2 = re.sub(r'^\d+\n?', '', transcript_1, flags=re.MULTILINE)
        if len(line)>0:
            for token in transcript_2.split():
                tokens.append(token)
for i in range(20):
    print(tokens[i])
```

```{python}
def chunk_token(tokens, max_tokens_per_chunk):
    """
    Splits a list of tokens into chunks, each with up to max_tokens_per_chunk tokens,
    ensuring that each chunk ends with a word that ends with a full stop.

    :param tokens: List of tokens (strings)
    :param max_tokens_per_chunk: Maximum number of tokens per chunk
    :return: List of strings, each representing a chunk of tokens
    """
    chunks = []
    current_chunk = []
    last_full_stop_index = -1  # Keep track of the last token ending with a full stop within the current chunk

    for i, token in enumerate(tokens):
        current_chunk.append(token)
        if token.endswith('.'):
            last_full_stop_index = len(current_chunk) - 1

        # If adding another token would exceed the limit, or we're at the last token
        if i == len(tokens) - 1 or (len(current_chunk) == max_tokens_per_chunk and last_full_stop_index != -1):
            # If the current token doesn't end with a full stop and we've seen one before,
            # split the chunk at the last full stop.
            if not token.endswith('.') and last_full_stop_index != -1:
                # Split at the last full stop seen
                next_chunk = current_chunk[last_full_stop_index+1:]
                current_chunk = current_chunk[:last_full_stop_index+1]
            else:
                next_chunk = []

            # Join the current chunk into a string and add it to the chunks list
            chunks.append(' '.join(current_chunk))
            # Start the next chunk with the remaining tokens if any
            current_chunk = next_chunk
            last_full_stop_index = -1  # Reset the last full stop index for the new chunk

    # Handle any remaining tokens in the current chunk
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

tokens_count = len(tokens)
print(tokens_count)
chunks = chunk_token(tokens,150)
chunks_count=len(chunks)
print(chunks_count)

for i in range(1):
    print(f"{i}: {chunks[i]}, {len(chunks[i].split())}")

```