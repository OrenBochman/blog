{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title : scrach pad\n",
        "draft: true\n",
        "---\n",
        "\n",
        "\n",
        "this is a scratch pad for intermediate stuff for working on posts or groups of posts.\n",
        "\n",
        "1. using the python environment \n",
        "\n",
        "\n",
        "```{bash}\n",
        "source env/bin/activate\n",
        "```\n",
        "\n",
        "\n",
        "## LLM prompts for working with video tanscripts\n",
        "\n",
        "the following are prompt I evolved for working with transcripts to get a faster start \n",
        "on notes using video of transcript.\n",
        "\n",
        "### Transcript summary\n",
        "\n",
        "> Please help me to summarize the following lesson transcript for my course notes. Match the speaker's language. Pay special attention not to omit technical details\n",
        "\n",
        "\n",
        "### Blog summary:\n",
        "\n",
        "> Please help me to summarize the following blog post for my course notes while matching the original language as best as you can. Pay special attention to any technical details\n",
        "\n",
        "### Insights\n",
        "\n",
        "- working with transcripts gets you most of what was said in the course.\n",
        "- most of what was said is not important we need to abstract it.\n",
        "- I can rewrite sentences to make them shorter. But if the instructed is confused this will only help so much.\n",
        "- highlighting using [content goes here]{.mark} is great for pointing out the important stuff.\n",
        "- notes with one highlight per paragraph are very boring to read.\n",
        "- good lectures have structure.\n",
        "- good papers have structure.\n",
        "- so how can we make the notes less boring and more compelling reading???\n",
        "    - first we drop most of the boring stuff\n",
        "    - reduce content into a list or points and convert to short paragraphs.\n",
        "- think about presenting using sketch notes !?\n",
        "\n",
        " - mermaid charts are helpful\n",
        " - asciiflow [asciiflow.com]\n",
        "\n",
        "\n",
        "## regex\n",
        "\n",
        "remove time codes from transcripts\n",
        "pure numeric line numbers from transcripts\n",
        "\n",
        "\n",
        "%20 --> space\n",
        ".([0-9]+).([0-9]+).png  --> -$1-$2.png\n",
        "\n",
        ".align-center width=\"450px\" height=\"300px\"\n",
        "-->\n",
        ".column-margin\n",
        "\n",
        "\n",
        "## quarto the missing utils\n",
        "\n",
        "start by running\n",
        "\n",
        "```{bash}\n",
        "quarto render --log site.log --log-level info\n",
        "```\n",
        "\n",
        "\n",
        "from this log you can grep the following\n",
        "\n",
        "|grep | content|\n",
        "|---|---|\n",
        "|`Cite`|missing citations|\n",
        "| `????` | missing images|\n",
        "| `packages are required`| missing package|\n",
        "|`meta`| posts with missing metadata|\n",
        "\n",
        "\n",
        "\n",
        "## transcipts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "some code for working with "
      ],
      "id": "6e18881e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get all the transcripts in a folder\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "\n",
        "#path = '/some/path/to/file'\n",
        "path = \"\"\n",
        "tokens = []\n",
        "for filename in glob.glob(os.path.join(path, '*.vtt')):\n",
        "   with open(os.path.join(os.getcwd(), filename), 'r') as f: # open in readonly mode\n",
        "      # do your stuff\n",
        "      tokens = []\n",
        "      for line in f:\n",
        "        transcript_1 = re.sub(r'^\\d{2}.*\\n?', '', line, flags=re.MULTILINE)\n",
        "        transcript_2 = re.sub(r'^\\d+\\n?', '', transcript_1, flags=re.MULTILINE)\n",
        "        if len(line)>0:\n",
        "            for token in transcript_2.split():\n",
        "                tokens.append(token)\n",
        "for i in range(20):\n",
        "    print(tokens[i])"
      ],
      "id": "4b73b578",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def chunk_token(tokens, max_tokens_per_chunk):\n",
        "    \"\"\"\n",
        "    Splits a list of tokens into chunks, each with up to max_tokens_per_chunk tokens,\n",
        "    ensuring that each chunk ends with a word that ends with a full stop.\n",
        "\n",
        "    :param tokens: List of tokens (strings)\n",
        "    :param max_tokens_per_chunk: Maximum number of tokens per chunk\n",
        "    :return: List of strings, each representing a chunk of tokens\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    last_full_stop_index = -1  # Keep track of the last token ending with a full stop within the current chunk\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        current_chunk.append(token)\n",
        "        if token.endswith('.'):\n",
        "            last_full_stop_index = len(current_chunk) - 1\n",
        "\n",
        "        # If adding another token would exceed the limit, or we're at the last token\n",
        "        if i == len(tokens) - 1 or (len(current_chunk) == max_tokens_per_chunk and last_full_stop_index != -1):\n",
        "            # If the current token doesn't end with a full stop and we've seen one before,\n",
        "            # split the chunk at the last full stop.\n",
        "            if not token.endswith('.') and last_full_stop_index != -1:\n",
        "                # Split at the last full stop seen\n",
        "                next_chunk = current_chunk[last_full_stop_index+1:]\n",
        "                current_chunk = current_chunk[:last_full_stop_index+1]\n",
        "            else:\n",
        "                next_chunk = []\n",
        "\n",
        "            # Join the current chunk into a string and add it to the chunks list\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            # Start the next chunk with the remaining tokens if any\n",
        "            current_chunk = next_chunk\n",
        "            last_full_stop_index = -1  # Reset the last full stop index for the new chunk\n",
        "\n",
        "    # Handle any remaining tokens in the current chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "tokens_count = len(tokens)\n",
        "print(tokens_count)\n",
        "chunks = chunk_token(tokens,150)\n",
        "chunks_count=len(chunks)\n",
        "print(chunks_count)\n",
        "\n",
        "for i in range(1):\n",
        "    print(f\"{i}: {chunks[i]}, {len(chunks[i].split())}\")"
      ],
      "id": "b66097b3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}