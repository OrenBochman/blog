<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Oren Bochman&#39;s Blog</title>
<link>https://orenbochman.github.io/</link>
<atom:link href="https://orenbochman.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Personal website, portfolio and blog</description>
<image>
<url>https://orenbochman.github.io/images/nlp-brain-wordcloud.jpg</url>
<title>Oren Bochman&#39;s Blog</title>
<link>https://orenbochman.github.io/</link>
</image>
<generator>quarto-1.8.21</generator>
<lastBuildDate>Mon, 01 Sep 2025 01:57:50 GMT</lastBuildDate>
<item>
  <title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2014/Dropout/</link>
  <description><![CDATA[ 






<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="JMLR:v15:srivastava14a">(Srivastava et al. 2014)</span> the authors, present a novel regularization technique for deep neural networks called “dropout.” The key idea behind dropout is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much and significantly reduces overfitting. The authors show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification, and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
<div class="no-row-height column-margin column-container"></div><p>The technique had been in use in some earlier works, but this paper popularized it and showed its effectiveness on a wide range of tasks. The idea behind drop out is pretty simple and people have since come up with many variations of it. It has become a standard technique in the deep learning toolbox.</p>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
<p>– <span class="citation" data-cites="JMLR:v15:srivastava14a">(Srivastava et al. 2014)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
</section>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="JMLR:v15:srivastava14a">(Srivastava et al. 2014)</span> the authors presents a regularization technique called Dropout, aimed at addressing the critical problem of overfitting in deep neural networks (DNNs). Dropout randomly drops units (neurons) during training to prevent co-adaptation of units, which can lead to overfitting. This novel technique is demonstrated to significantly improve the performance of neural networks across a wide range of tasks, including computer vision, speech recognition, and natural language processing.</p>
<div class="no-row-height column-margin column-container"><div id="ref-JMLR:v15:srivastava14a" class="csl-entry">
Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <span>“Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”</span> <em>Journal of Machine Learning Research</em> 15 (56): 1929–58. <a href="http://jmlr.org/papers/v15/srivastava14a.html">http://jmlr.org/papers/v15/srivastava14a.html</a>.
</div></div></section>
<section id="core-idea" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="core-idea">Core Idea</h2>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig1.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;1: the effect of dropout"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig1.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: the effect of dropout
</figcaption>
</figure>
</div>
</div></div><p>Dropout works by randomly removing units from the network during each training iteration. This prevents the network from becoming overly reliant on specific units, thus reducing overfitting. During testing, all units are used, but their weights are scaled to account for the dropout during training. This approximates the averaging of an exponential number of thinned networks that would otherwise be computationally infeasible.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig2.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;2: Training vs Test"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig2.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Training vs Test
</figcaption>
</figure>
</div>
</div></div></section>
<section id="methodology-and-theoretical-motivation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="methodology-and-theoretical-motivation">Methodology and Theoretical Motivation</h2>
<p>Dropout’s theoretical foundation stems from biological principles, specifically the idea of genetic robustness in sexual reproduction. In the analogy, a network’s hidden units act like genes that must learn to function independently of one another, preventing complex co-adaptations that may not generalize well to unseen data. The stochastic nature of dropout introduces noise during training, which acts as a form of model averaging and regularization.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig3.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;3: figure 3"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig3.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: figure 3
</figcaption>
</figure>
</div>
</div></div></section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<ul>
<li>Model Averaging: Dropout enables the training of many subnetworks (thinned networks) simultaneously, which leads to a more robust model.</li>
<li>Regularization Effect: Dropout reduces overfitting more effectively than other methods such as L1/L2 regularization or early stopping.</li>
<li>Efficiency: Dropout provides a computationally feasible approximation of model averaging by scaling weights at test time, as opposed to maintaining an ensemble of networks.</li>
</ul>
</section>
<section id="experimental-results" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results</h2>
<p>The authors demonstrate the efficacy of dropout across several benchmark datasets:</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig4.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;4: figure 4"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig4.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: figure 4
</figcaption>
</figure>
</div>
</div></div><ul>
<li>MNIST: Error rates are reduced from 1.60% (standard neural network) to 0.95% using dropout with additional max-norm regularization.</li>
<li>CIFAR-10 and CIFAR-100: Dropout networks outperform previous methods, with an error reduction to 12.6% on CIFAR-10 and 37.2% on CIFAR-100.</li>
<li>TIMIT (Speech Data): Dropout reduces the phone error rate from 23.4% to 21.8%, showing significant improvements over non-dropout models.</li>
<li>ImageNet: Dropout helps achieve state-of-the-art results in image classification tasks, significantly lowering the top-5 error rate.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig5.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;5: image samples"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig5.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: image samples
</figcaption>
</figure>
</div>
</div></div></section>
<section id="advantages-of-dropout" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-dropout">Advantages of Dropout</h2>
<ul>
<li>Generality: Dropout works across a variety of architectures, including fully connected, convolutional, and recurrent neural networks.</li>
<li>Ease of Use: Dropout is simple to implement, requiring only one additional hyperparameter (the dropout rate, typically 0.5 for hidden layers).</li>
<li>Compatibility with Other Methods: Dropout can be combined with techniques like unsupervised pretraining, max-norm regularization, and momentum, further improving model performance.</li>
</ul>
</section>
<section id="limitations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<ul>
<li>Increased Training Time: Dropout can significantly slow down training, typically requiring 2-3 times more iterations to converge.</li>
<li>Tuning of Hyperparameters: While simple, the dropout rate must be carefully selected, and higher learning rates and momentum are generally required for optimal performance.</li>
<li>Application-Specific Benefits: Although dropout improves performance in vision and speech recognition tasks, the improvements in certain domains like text classification (e.g., Reuters RCV1 dataset) are less pronounced. <sup>1</sup></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I think this is because if we drop a few words from a sentence a reader can often guess them from the context and redundancy in natural languages.</p></div></div></section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The paper introduces dropout as a powerful and simple regularization technique that significantly reduces overfitting in deep neural networks. Dropout provides a computationally efficient method to approximate model averaging and works across a range of architectures and tasks, achieving state-of-the-art results on several benchmarks. However, it comes at the cost of increased training time, and some tuning is required for optimal performance.</p>
<p>Dropout represents a substantial advancement in neural network training, and its adoption has since become widespread in the deep learning community.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<p><a href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">link to the paper</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="embeded paper"><embed src="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" class="col-page" style="width:8.5in;height:11in"></a></p>
<figcaption>embeded paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks}
    from {Overfitting}},
  url = {https://orenbochman.github.io/reviews/2014/Dropout/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Dropout: A Simple Way to Prevent Neural
Networks from Overfitting.”</span> <a href="https://orenbochman.github.io/reviews/2014/Dropout/">https://orenbochman.github.io/reviews/2014/Dropout/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2014/Dropout/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
</item>
<item>
  <title>FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/FontCLIP/</link>
  <description><![CDATA[ 






<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Acquiring the desired font for various design tasks can be challenging and requires professional typographic knowledge. While previous font retrieval or generation works have alleviated some of these difficulties, they often lack support for multiple languages and semantic attributes beyond the training data domains. To solve this problem, we present FontCLIP: a model that connects the semantic understanding of a large vision-language model with typographical knowledge. We integrate typography-specific knowledge into the comprehensive vision-language knowledge of a pretrained CLIP model through a novel finetuning approach. We propose to use a compound descriptive prompt that encapsulates adaptively sampled attributes from a font attribute dataset focusing on Roman alphabet characters. FontCLIP’s semantic typographic latent space demonstrates two unprecedented generalization abilities. First, FontCLIP generalizes to different languages including Chinese, Japanese, and Korean (CJK), capturing the typographical features of fonts across different languages, even though it was only finetuned using fonts of Roman characters. Second, FontCLIP can recognize the semantic attributes that are not presented in the training data. FontCLIP’s dual-modality and generalization abilities enable multilingual and cross-lingual font retrieval and letter shape optimization, reducing the burden of obtaining desired fonts.</p>
<p>— <span class="citation" data-cites="Tatsukawa2024Fontclip">(Tatsukawa et al. 2024)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-Tatsukawa2024Fontclip" class="csl-entry">
Tatsukawa, Yuki, I‐Chao Shen, Anran Qi, Yuki Koyama, Takeo Igarashi, and Ariel Shamir. 2024. <span>“FontCLIP: A Semantic Typography Visual‐language Model for Multilingual Font Applications.”</span> <em>Computer Graphics Forum</em> 43 (2). <a href="https://doi.org/10.1111/cgf.15043">https://doi.org/10.1111/cgf.15043</a>.
</div></div></div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {FontCLIP: {A} {Semantic} {Typography} {Visual-Language}
    {Model} for {Multilingual} {Font} {Applications}},
  url = {https://orenbochman.github.io/reviews/2024/FontCLIP/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“FontCLIP: A Semantic Typography
Visual-Language Model for Multilingual Font Applications.”</span> <a href="https://orenbochman.github.io/reviews/2024/FontCLIP/">https://orenbochman.github.io/reviews/2024/FontCLIP/</a>.
</div></div></section></div> ]]></description>
  <category>paper-review</category>
  <category>typography</category>
  <category>vision-language-model</category>
  <category>CLIP</category>
  <category>multilingual</category>
  <category>cross-lingual</category>
  <category>semantic attributes</category>
  <guid>https://orenbochman.github.io/reviews/2024/FontCLIP/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2024/FontCLIP/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Handwriting beautification using token means</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2013/HandwritingBeautification/</link>
  <description><![CDATA[ 






<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: beautification"><img src="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/fig_1.png" class="img-fluid figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: beautification
</figcaption>
</figure>
</div>
<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="Zitnick2013Beautification">(Zitnick 2013)</span> the author shows how we can use a model for beautifying handwriting. The problem raised is that there is lots of variation in handwriting for a single individual and come up this a method to reduce this by a clever form of avaraging.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Zitnick2013Beautification" class="csl-entry">
Zitnick, C. Lawrence. 2013. <span>“Handwriting Beautification Using Token Means.”</span> <em>ACM Trans. Graph.</em> 32 (4). <a href="https://doi.org/10.1145/2461912.2461985">https://doi.org/10.1145/2461912.2461985</a>.
</div></div><p>The data is captured from a tablet and thus had a three dimensional structure. Central to this paper is are two ideas:</p>
<ol type="1">
<li>How to effectively average similar tokens to get a suitable mean token.
<ul>
<li>they use a moving window</li>
<li>the sample using a curvature based sampling
<ul>
<li><span class="citation" data-cites="Whitney1937Regular">(Whitney 1937)</span></li>
<li><span class="citation" data-cites="mokhtarian1992theory">(Mokhtarian and Mackworth 1992)</span></li>
<li><span class="citation" data-cites="dudek1997shape">(Dudek and Tsotsos 1997)</span></li>
</ul></li>
<li>they use a fine alignment using
<ul>
<li>affinity matrix</li>
<li>dynamic programming to find the best warping between two sequences.</li>
</ul></li>
<li>they also use a visual similarity metric to make the model more <strong>robust</strong> to graphemes with similar strokes but different shapes.</li>
</ul></li>
<li>How to decide which tokens are similar.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-Whitney1937Regular" class="csl-entry">
Whitney, Hassler. 1937. <span>“On Regular Closed Curves in the Plane.”</span> <em>Compositio Mathematica</em> 4: 276–84. <a href="http://www.numdam.org/item/CM_1937__4__276_0/">http://www.numdam.org/item/CM_1937__4__276_0/</a>.
</div><div id="ref-mokhtarian1992theory" class="csl-entry">
Mokhtarian, Farzin, and Alan K Mackworth. 1992. <span>“A Theory of Multiscale, Curvature-Based Shape Representation for Planar Curves.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 14 (8): 789–805.
</div><div id="ref-dudek1997shape" class="csl-entry">
Dudek, Gregory, and John K Tsotsos. 1997. <span>“Shape Representation and Recognition from Multiscale Curvature.”</span> <em>Computer Vision and Image Understanding</em> 68 (2): 170–89.
</div></div><p>Once these are solved, it becomes a matter of clustering tokens by similarity and then averaging the tokens in each cluster to get a mean token.</p>
<p>The mean token are then used to replace the original token in the handwriting data. The authors show that this method can be used to improve the quality of handwriting data.</p>
<p>Q. As time goes by there is more data and the replacements pool towards the cluster avarage. It seems that replacement might be more uniform if the earlier replacements were updated as their cluster avarage drifts…</p>
<p>This naturally leads to a kind of time series.</p>
<p>Perhaps the key idea is how the authors convert the text to a sequence of vectors use a token mean to represent the data. This is a simple idea but</p>
</section>
<section id="the-approach" class="level2">
<h2 class="anchored" data-anchor-id="the-approach">3. The approach</h2>
<p>We represent the stylus’s samples by storing the difference vectors between the stylus positions</p>
<p>i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5CPhi%20=%20%5C%7B%5Cphi_1,%20%5Cldots%20,%20%5Cphi_a%5C%7D"> with</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cphi_i%20=%20%5C%7Bx_i,%20y_i,%20p_i%5C%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?(x_i,%20y_i)"> is the difference in the stylus’s pixel position between samples i − 1 and i.</p>
<p><img src="https://latex.codecogs.com/png.latex?p_i"> is the stylus’s pressure.</p>
</section>
<section id="stroke-resampling" class="level2">
<h2 class="anchored" data-anchor-id="stroke-resampling">3.1 Stroke resampling</h2>
<p>As I understand it data is captured uniformly from a tablet and thus had a three dimensional structure. The authors then to resample the data to more faithfully represent the the curvature that is the building block of strokes within the handwriting.</p>
<div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: stroke resampling - uniform v.s. curvature based"><img src="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/fig_2.png" class="img-fluid figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: stroke resampling - uniform v.s. curvature based
</figcaption>
</figure>
</div>
<p>They represent samples taken at regular distance intervals using <img src="https://latex.codecogs.com/png.latex?%5CPhi%5Ed%20=%20%5C%7B%20%5Cphi%5Ed%20_1%20,%20%5Cldots,%20%5Cphi%5Ed_n%20%5C%7D"> where the sample magnitude <img src="https://latex.codecogs.com/png.latex?r_i"> is constant for all samples. c.f. Figure&nbsp;2</p>
<p>Curvature based sampling:</p>
<p>We compute a stroke representation <img src="https://latex.codecogs.com/png.latex?%CE%A6%5Ec%20=%20%5C%7B%5Cphi%5Ec_1,%20%5Cldots%20,%20%5Cphi%5Ec_n%5C%7D"> with the same parameterization as <img src="https://latex.codecogs.com/png.latex?%CE%A6">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%CF%86%5Ec_i%20=%5C%7Bx_i,%20y_i,%20p_i%20%5C%7D"></p>
<p><span id="eq-stroke"><img src="https://latex.codecogs.com/png.latex?%0Az_i=z_%7Bi-1%7D+min(1,%20%5Cfrac%7B%5Calpha%20%5CDelta_%5Ctheta%20%5Cbeta_j%7D%7B2%5Cpi%7D)%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?z_i"> is the point on the curve.</li>
<li><img src="https://latex.codecogs.com/png.latex?z_%7Bi-1%7D"> is the previous point on the curve.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the sampling density parameter (minimum value = 24)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CDelta_%5Ctheta%20%5Cin%20(0,%5Cpi%5D"> is the angle between samples <img src="https://latex.codecogs.com/png.latex?%CF%86_%7Bi%E2%88%921%7D"> and <img src="https://latex.codecogs.com/png.latex?%CF%86_i"></li>
<li><img src="https://latex.codecogs.com/png.latex?r_j"> is the stroke magnitude.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%20=%20max(0,%20min(1,%20r_j%20%E2%88%92%20%5Csqrt%7B2%7D))."> is a parameter that controls for discretization of the stylus.</li>
</ul>
<!-- todo try to understand this better find/create an implementation -->
</section>
<section id="refining-strokes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="refining-strokes">3.2 Refining strokes</h2>
<p>When a user writes they generate a large set of stroke samples, denoted <img src="https://latex.codecogs.com/png.latex?%CE%A6"> (for the rest of the paper we assume a curvature-based sampling and drop the superscript c.) From <img src="https://latex.codecogs.com/png.latex?%CE%A6"> we create overlapping fixed length sequences of stroke samples called tokens c.f. Figure&nbsp;1. Each token contains n stroke samples.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>matrix profiles
</div>
</div>
<div class="callout-body-container callout-body">
<p>Using matrix profiles math behind stumpy might be usefull in making this work faster and better, not sure about real time.</p>
</div>
</div>
<section id="fine-scale-alignment" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="fine-scale-alignment">Fine-scale alignment</h3>

<div class="no-row-height column-margin column-container"><div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: fine alignment"><img src="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/fig_3.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: fine alignment
</figcaption>
</figure>
</div></div><p>The <strong>match cost</strong> <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7Bk,l%7D"> is found using a linear combination of three features,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbeta_%7Bk,l%7D%20=%20%5CDelta_%7B%5Chat%20r%7D%20+%20%5CDelta_%5Ctheta%20+%20%5Cdelta_p%20%5Cqquad%0A"></p>
<p>computed from:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?t_%7Bi,k%7D"> and <img src="https://latex.codecogs.com/png.latex?t_%7Bj,l%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Chat%20r"> is the absolute difference between <img src="https://latex.codecogs.com/png.latex?%5Chat%20r_k"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%20r_l">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CDelta_%5Ctheta"> is the absolute angular distance between <img src="https://latex.codecogs.com/png.latex?%CE%B8_k"> and <img src="https://latex.codecogs.com/png.latex?%CE%B8_l">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdelta_p"> measures if both strokes have consistent visibility. That is, <img src="https://latex.codecogs.com/png.latex?%5Cdelta_p%20=%201"> if <img src="https://latex.codecogs.com/png.latex?p_k%20=%200"> and <img src="https://latex.codecogs.com/png.latex?p_l%20=%200">, or <img src="https://latex.codecogs.com/png.latex?p_k%20%3E%200"> and <img src="https://latex.codecogs.com/png.latex?p_l%20%3E%200">, and <img src="https://latex.codecogs.com/png.latex?%5Cdelta_p%20=%200"> otherwise.</li>
</ul>
</section>
<section id="merging-stroke-sets" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="merging-stroke-sets">Merging stroke sets</h3>
<blockquote class="blockquote">
<p>Once two or more tokens are aligned, we can merge them by averaging the stroke samples.</p>
</blockquote>

<div class="no-row-height column-margin column-container"><div id="fig-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: matches"><img src="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/fig_4.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: matches
</figcaption>
</figure>
</div></div></section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also</h2>
<ul>
<li>http://larryzitnick.org/</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Handwriting Beautification Using Token Means},
  url = {https://orenbochman.github.io/reviews/2013/HandwritingBeautification/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Handwriting Beautification Using Token
Means.”</span> <a href="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/">https://orenbochman.github.io/reviews/2013/HandwritingBeautification/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2013/HandwritingBeautification/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
</item>
<item>
  <title>ImageNet Classification with Deep Convolutional Neural Networks</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2012/imagenet/</link>
  <description><![CDATA[ 






<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p><span class="citation" data-cites="krizhevsky2012imagenet">(Krizhevsky, Sutskever, and Hinton 2012)</span> is a seminal paper in the field of deep learning. It introduced the AlexNet architecture, which won the ImageNet Large Scale Visual Recognition Challenge in 2012. The paper is a great starting point for anyone interested in deep learning, as it provides a detailed explanation of the architecture and training process of the network.</p>
<div class="no-row-height column-margin column-container"></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
<p>In <span class="citation" data-cites="krizhevsky2012imagenet">(Krizhevsky, Sutskever, and Hinton 2012)</span> titled “ImageNet Classification with Deep Convolutional Neural Networks”, the authors Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton presents the development and training of a large deep convolutional neural network (CNN) for image classification using the ImageNet dataset.</p>
<div class="no-row-height column-margin column-container"><div id="ref-krizhevsky2012imagenet" class="csl-entry">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“Imagenet Classification with Deep Convolutional Neural Networks.”</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div></div><p>Thus paper marks a pivotal point in the development of deep learning, particularly in the realm of computer vision. The authors introduced a large convolutional neural network (CNN) trained on the ImageNet dataset, which significantly outperformed previous models, winning the <a href="https://www.image-net.org/challenges/LSVRC">ImageNet Large Scale Visual Recognition Challenge</a> (ILSVRC) in 2012 with a top-5 test error rate of 15.3%.</p>
<section id="key-contributions" class="level3">
<h3 class="anchored" data-anchor-id="key-contributions">Key Contributions:</h3>
<ul>
<li><p><strong>Architecture</strong>: The CNN consists of five convolutional layers and three fully connected layers, with the final layer being a softmax classifier that distinguishes between 1000 categories. This architecture involves a total of 60 million parameters and 650,000 neurons.</p></li>
<li><p><strong>GPU Utilization</strong>: Training was performed on two GTX 580 GPUs to speed up the process, allowing them to handle the large network size and dataset. This took approximately 5-6 days to complete.</p></li>
<li><p><strong>Techniques to Improve Performance</strong>: The network used a variety of novel techniques to improve both performance and training time:</p>
<ul>
<li><strong>Rectified Linear Units</strong> (ReLUs): These non-saturating neurons were employed to speed up training, which was crucial for dealing with such a large model.</li>
<li><strong>Dropout</strong>: A regularization method was used in fully connected layers to prevent overfitting by randomly dropping some neurons during training.</li>
<li><strong>Data Augmentation</strong>: The authors employed various forms of data augmentation, including random crops, horizontal flips, and color variation via principal component analysis (PCA), which greatly expanded the size of the training set and further reduced overfitting.</li>
</ul></li>
</ul>
<p>overfitting.</p>
</section>
</section>
<section id="results-and-impact" class="level2">
<h2 class="anchored" data-anchor-id="results-and-impact">Results and Impact</h2>
<p>The network trained for the ILSVRC 2010 and 2012 challenges achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, for ILSVRC 2010, far surpassing previous methods based on feature extraction and boosting. In the ILSVRC 2012 competition, the network reduced the top-5 error to 15.3%, compared to the 26.2% achieved by the second-best entry. This result not only established CNNs as the state-of-the-art model for image classification tasks but also cemented the importance of deep learning in the broader machine learning community.</p>
</section>
<section id="challenges-and-limitations" class="level2">
<h2 class="anchored" data-anchor-id="challenges-and-limitations">Challenges and Limitations</h2>
<p>The authors acknowledge that their network size was constrained by the available GPU memory and that improvements in both hardware and larger datasets could potentially improve the performance of such models in the future.</p>
<p>The CNN’s architecture and optimization techniques pioneered by this paper have set a foundation for subsequent advances in deep learning, particularly in image recognition tasks.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The paper demonstrated the feasibility and efficacy of training deep networks on large-scale datasets and provided key insights into architectural choices, regularization, and optimization. This work has since inspired a plethora of follow-up research, leading to advancements such as transfer learning, fine-tuning on smaller datasets, and the further development of GPU-based training methods. The innovations introduced in this paper laid the groundwork for the modern AI revolution in image recognition and beyond.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {ImageNet {Classification} with {Deep} {Convolutional}
    {Neural} {Networks}},
  url = {https://orenbochman.github.io/reviews/2012/imagenet/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“ImageNet Classification with Deep
Convolutional Neural Networks.”</span> <a href="https://orenbochman.github.io/reviews/2012/imagenet/">https://orenbochman.github.io/reviews/2012/imagenet/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/imagenet/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
</item>
<item>
  <title>Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2012/dropout/</link>
  <description><![CDATA[ 






<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="hinton2012improvingneuralnetworkspreventing">(Hinton et al. 2012)</span> titled “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors”, the authors, Hinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov introduces a new regularization technique called “Dropout” that helps to prevent overfitting in neural networks. Dropout is a simple and effective way to improve the performance of neural networks by preventing co-adaptation of feature detectors. The authors show that dropout can be used to improve the performance of a wide range of neural networks, including deep networks, convolutional networks, and recurrent networks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hinton2012improvingneuralnetworkspreventing" class="csl-entry">
Hinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. <span>“Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.”</span> <a href="https://arxiv.org/abs/1207.0580">https://arxiv.org/abs/1207.0580</a>.
</div></div></section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification, and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
</blockquote>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>, introduces the dropout technique as an innovative method to prevent overfitting in neural networks. Overfitting occurs when a model performs well on training data but poorly on unseen test data, particularly when dealing with a large number of parameters and limited training samples. The paper addresses this by proposing the use of dropout, a regularization technique that randomly omits units (neurons) during training.</p>
</section>
<section id="core-ideas" class="level2">
<h2 class="anchored" data-anchor-id="core-ideas">Core Ideas</h2>
<p>The central concept behind dropout is to prevent co-adaptation of feature detectors. In a traditional neural network, feature detectors can co-adapt to specific patterns in the training data, which leads to poor generalization. By randomly omitting neurons with a probability of 0.5 during training, each neuron is forced to contribute independently to the final output. This reduces the reliance on specific sets of neurons and ensures that each feature detector learns useful patterns.</p>
<p>Another significant advantage of dropout is that it acts as an efficient form of model averaging. Training with dropout can be seen as training an ensemble of neural networks that share parameters, making it computationally feasible to obtain better generalization without having to train multiple models.</p>
</section>
<section id="experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results</h2>
<p>The authors demonstrate the effectiveness of dropout on several benchmark datasets, including MNIST, CIFAR-10, ImageNet, TIMIT, and the Reuters corpus.</p>
<ul>
<li>MNIST: Dropout reduced the error rate from 160 errors to around 110 by applying 50% dropout to hidden units and 20% dropout to input units.</li>
<li>TIMIT: Dropout improved frame classification accuracy in speech recognition tasks, reducing the error rate by 3% in comparison to standard training methods.</li>
<li>CIFAR-10: The authors achieved a 16.6% error rate without dropout and 15.6% with dropout, outperforming previous state-of-the-art results.</li>
<li>ImageNet: Dropout applied to deep convolutional neural networks (CNNs) reduced the error rate from 48.6% to 42.4%.</li>
<li>Reuters Corpus: Dropout reduced classification error from 31.05% to 29.62%.</li>
</ul>
</section>
<section id="theoretical-contributions" class="level2">
<h2 class="anchored" data-anchor-id="theoretical-contributions">Theoretical Contributions</h2>
<p>The theoretical underpinning of dropout is grounded in model averaging and regularization. In standard practice, model averaging is performed by training multiple models and averaging their predictions, but this approach is computationally expensive. Dropout provides a far more efficient alternative by implicitly training an ensemble of models that share parameters, thus achieving the benefits of model averaging without the overhead of training separate models.</p>
<p>Additionally, dropout mitigates the problem of overfitting by introducing noise during training, making the model more robust. At test time, all units are used, but their outgoing weights are scaled to reflect the fact that fewer units were active during training.</p>
</section>
<section id="discussion-and-impact" class="level2">
<h2 class="anchored" data-anchor-id="discussion-and-impact">Discussion and Impact</h2>
<p>The introduction of dropout represents a major step forward in the development of deep learning models, as it allows for better generalization across a variety of tasks. Its simplicity, coupled with its effectiveness, has made dropout a standard tool in neural network training. The experiments conducted in the paper demonstrate its utility across a wide range of tasks, from image recognition to speech processing, providing compelling evidence of its broad applicability.</p>
<p>The idea of preventing co-adaptation of feature detectors to improve generalization is an elegant solution to a longstanding problem in neural network training. By ensuring that each neuron must work independently, dropout forces the model to learn more robust features that generalize well to unseen data.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This paper is a highly influential paper that introduced a novel technique for improving the generalization of deep learning models. The results speak for themselves, with dropout achieving state-of-the-art performance across multiple datasets and tasks. The technique has since become a standard part of neural network training, revolutionizing the field and contributing to the success of deep learning in real-world applications.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Improving {Neural} {Networks} by {Preventing} {Co-Adaptation}
    of {Feature} {Detectors}},
  url = {https://orenbochman.github.io/reviews/2012/dropout/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Improving Neural Networks by Preventing
Co-Adaptation of Feature Detectors.”</span> <a href="https://orenbochman.github.io/reviews/2012/dropout/">https://orenbochman.github.io/reviews/2012/dropout/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/dropout/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
</item>
<item>
  <title>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/LLM2Vec/</link>
  <description><![CDATA[ 






<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="behnamghader2024llm2veclargelanguagemodels">(BehnamGhader et al. 2024)</span> the authors consider using LLMs which are mostly decoder only transformers as text encoders. This allows them to use the LLMs for NLP tasks like chunking, NEW and POS. Recall that T5 <span class="citation" data-cites="raffel2020exploring">(Raffel et al. 2020)</span> can do this is a decoder encode model.</p>
<div class="no-row-height column-margin column-container"><div id="ref-behnamghader2024llm2veclargelanguagemodels" class="csl-entry">
BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. <span>“LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders.”</span> <a href="https://arxiv.org/abs/2404.05961">https://arxiv.org/abs/2404.05961</a>.
</div><div id="ref-raffel2020exploring" class="csl-entry">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. <span>“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.”</span> <em>Journal of Machine Learning Research</em> 21 (140): 1–67.
</div></div></section>
<section id="tricks" class="level2 callout-info">
<h2 class="anchored" data-anchor-id="tricks">Tricks</h2>
<ol type="1">
<li>enabling bidirectional attention,</li>
<li>masked next token prediction, and</li>
<li>unsupervised contrastive learning.</li>
</ol>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Large decoder-only language models (LLMs) are the state-of-the-art models on most of today’s NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 4 popular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data (as of May 24, 2024). Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="The 3 steps of LLM2Vec"><img src="https://orenbochman.github.io/reviews/2024/LLM2Vec/fig1.png" class="img-fluid figure-img" alt="The 3 steps of LLM2Vec"></a></p>
<figcaption>The 3 steps of LLM2Vec</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Evaluation on word level tasks"><img src="https://orenbochman.github.io/reviews/2024/LLM2Vec/fig2.png" class="img-fluid figure-img" alt="Evaluation on word level tasks"></a></p>
<figcaption>Evaluation on word level tasks</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Unsupervised results"><img src="https://orenbochman.github.io/reviews/2024/LLM2Vec/fig3.png" class="img-fluid figure-img" alt="Unsupervised results"></a></p>
<figcaption>Unsupervised results</figcaption>
</figure>
</div>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="paper"><embed src="paper.pdf" class="col-page" width="800"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://github.com/McGill-NLP/llm2vec">code</a></li>
</ul>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/44OukEJyRsU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {LLM2Vec: {Large} {Language} {Models} {Are} {Secretly}
    {Powerful} {Text} {Encoders}},
  url = {https://orenbochman.github.io/reviews/2024/LLM2Vec/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“LLM2Vec: Large Language Models Are Secretly
Powerful Text Encoders.”</span> <a href="https://orenbochman.github.io/reviews/2024/LLM2Vec/">https://orenbochman.github.io/reviews/2024/LLM2Vec/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/LLM2Vec/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2024/LLM2Vec/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>MambaVision A Hybrid Mamba-Transformer Vision Backbone</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/mamba-vision/</link>
  <description><![CDATA[ 






<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In <span class="citation" data-cites="hatamizadeh2024mambavision">(Hatamizadeh and Kautz 2024)</span>, the authors apply the State Space Model (SSM) inherent in recently introduced Mamba architecture, <span class="citation" data-cites="gu2023mamba">(Gu and Dao 2023)</span>, for vision tasks. They point out that prior work on using the Mamba architecture for vision was ill-suited these tasks and propose a remedy in the form of a hybrid Mamba-Transformer architecture which they call MambaVision. Thier experiment show that MambaVision outperforms other vision architectures on ImageNet-1K, MS COCO and ADE20K datasets.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gu2023mamba" class="csl-entry">
Gu, Albert, and Tri Dao. 2023. <span>“Mamba: Linear-Time Sequence Modeling with Selective State Spaces.”</span> <em>arXiv Preprint arXiv:2312.00752</em>.
</div></div><p>The paper’s main innovation is <mark>more self-attention blocks in the final layers of the transformer which improves the models ability to capture long-range spatial dependencies</mark>.</p>
</section>
<section id="the-problems-with-mamba-for-vision-tasks" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-problems-with-mamba-for-vision-tasks">The problems with Mamba for vision tasks</h2>
<p>A <em>dilettante reader</em> like myself might be interested in the author’s outline of the shortcomings of the Mamba architecture for vision tasks and earlier attempt in <span class="citation" data-cites="zhu2024vision">(Zhu et al. 2024)</span> <em>vision mamba</em> model which directed thier efforts the right direction.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhu2024vision" class="csl-entry">
Zhu, Lianghui, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. 2024. <span>“Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model.”</span> <em>arXiv Preprint arXiv:2401.09417</em>.
</div></div><div class="page-columns page-full"><blockquote class="blockquote">
<p>… the Mamba’s autoregressive formulation, while effective for tasks requiring sequential data processing, faces limitations in computer vision tasks that benefit from a <strong>full receptive field</strong><sup>1</sup>:</p>
<ol type="1">
<li><p>Unlike sequences where order matters, image pixels do not have a sequential dependency in the same way. Instead, spatial relationships are often local and need to be considered in a more parallel and integrated manner. Hence, this results in inefficiency for processing spatial data</p></li>
<li><p>an autoregressive model like Mamba processes data step-by-step, limiting its ability to capture and utilize global context in one forward pass. In contrast, vision tasks often require understanding the global context to make accurate predictions about local regions</p></li>
</ol>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;seeing the full picture or at least big parts of it</p></div></div></div>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Vision Mamba (Vim) and others have proposed modifications such as bidirectional SSMs to address lack of global context and spatial understanding. While bidirectional SSMs have the potential to capture more comprehensive context, they introduce significant latency due to the need to process the entire sequence before making predictions. Additionally, the increased complexity can lead to challenges in training, risk of overfitting, and may not always result in better accuracy. Due to these pitfalls, backbones with Vision Transformer (ViT) and Convolutional Neural Network (CNN) architectures still outperform best Mamba-based vision models on different vision tasks. — <span class="citation" data-cites="hatamizadeh2024mambavision">(Hatamizadeh and Kautz 2024, 2)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-hatamizadeh2024mambavision" class="csl-entry">
Hatamizadeh, Ali, and Jan Kautz. 2024. <span>“MambaVision: A Hybrid Mamba-Transformer Vision Backbone.”</span> <em>arXiv Preprint arXiv:2407.08083</em>.
</div></div></div>
<p>To sum all this up - Mamba’s auto regressive nature is well suited to temporal and sequential data like text and speech but is ill suited to handle spatial data like images where order manifests as a hierarchy of spatial neighborhoods which should be processed in parallel. Thus for vision, mamba suffer a loss in the efficiency of the flow of information both locally and globally. As such pre mamba vision models fare better.</p>
<p>The next section outlines the ideas espoused in prior work both pre and post mamba. This section summarizes both the earlier work on computer vision models since the introduction of Transformers and some results since the introduction of the Mamba architecture.</p>
<ul>
<li>Vision Transformer (ViT) <span class="citation" data-cites="dosovitskiy2021imageworth16x16words">(Dosovitskiy et al. 2021)</span> showed that CNNs can be replaced with self-attention, but wasn’t data efficient.</li>
<li>Data-efficient Image Transformer (DeiT) <span class="citation" data-cites="touvron2021training">(Touvron et al. 2021)</span> used distillation to train ViT more efficient.</li>
<li>LeViT model <span class="citation" data-cites="graham2021levit">(Graham et al. 2021)</span> introduced a redesign for MLP and self-attention with a Lenet like pyramid pooling structure.</li>
<li>Cross-covariance Image Transformer (XCiT) <span class="citation" data-cites="ali2021xcit">(Ali et al. 2021)</span> introduced transposed self-attention mechanism more effectively modeling interactions between feature channels.</li>
<li>The Pyramid Vision Transformer (PVT) <span class="citation" data-cites="wang2021pyramid">(Wang et al. 2021)</span> improving efficiency by adopting a hierarchical structure with patch embedding at the start of each stage and spatial dimension reduction.</li>
<li>Swin Transformer <span class="citation" data-cites="liu2021swin">(Liu et al. 2021)</span> used shifted windows to improve the efficiency of self-attention computation.</li>
<li>Twins Transformer <span class="citation" data-cites="chu2021twins">(Chu et al. 2021)</span> featured spatially separable self-attention that significantly enhanced efficiency.</li>
<li>Focal Transformer <span class="citation" data-cites="yang2021focal">(Yang et al. 2021)</span> used a focal mechanism to improve the efficiency of self-attention computation for capturing long-range interactions.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-dosovitskiy2021imageworth16x16words" class="csl-entry">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div><div id="ref-touvron2021training" class="csl-entry">
Touvron, Hugo, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. 2021. <span>“Training Data-Efficient Image Transformers &amp; Distillation Through Attention.”</span> In <em>International Conference on Machine Learning</em>, 10347–57. PMLR.
</div><div id="ref-graham2021levit" class="csl-entry">
Graham, Benjamin, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and Matthijs Douze. 2021. <span>“Levit: A Vision Transformer in Convnet’s Clothing for Faster Inference.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 12259–69.
</div><div id="ref-ali2021xcit" class="csl-entry">
Ali, Alaaeldin, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, et al. 2021. <span>“Xcit: Cross-Covariance Image Transformers.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 20014–27.
</div><div id="ref-wang2021pyramid" class="csl-entry">
Wang, Wenhai, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. 2021. <span>“Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 568–78.
</div><div id="ref-liu2021swin" class="csl-entry">
Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. <span>“Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 10012–22.
</div><div id="ref-chu2021twins" class="csl-entry">
Chu, Xiangxiang, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. 2021. <span>“Twins: Revisiting the Design of Spatial Attention in Vision Transformers.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 9355–66.
</div><div id="ref-yang2021focal" class="csl-entry">
Yang, Jianwei, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. 2021. <span>“Focal Attention for Long-Range Interactions in Vision Transformers.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 30008–22.
</div></div></section>
<section id="the-mambavision-architecture---macro" class="level2">
<h2 class="anchored" data-anchor-id="the-mambavision-architecture---macro">3.1 The MambaVision Architecture - Macro</h2>
<p>MambaVision has a hierarchical architecture consisting of 4 different stages. The first two stages consist of CNN-based layers for fast feature extraction at higher input resolutions, while stage 3 and 4 include the proposed MambaVision and Transformer blocks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Architecture of hierarchical MambaVision"><img src="https://orenbochman.github.io/reviews/2024/mamba-vision/fig2.png" class="img-fluid figure-img" alt="Architecture of hierarchical MambaVision"></a></p>
<figcaption>Architecture of hierarchical MambaVision</figcaption>
</figure>
</div>
<p>The first two blocks in stages 1 and 2</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%20z%20=%20GELU(BN(Conv_%7B3%C3%973%7D(z)))%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Az%20=%20BN(Conv_%7B3%C3%973%7D(%5Chat%20z))%20+%20z%0A"></p>
<p>Where GELU is the Gaussian Error Linear Unit activation function, a modern alternative to the rectified linear unit (ReLU) function, and BN is good old batch normalization layer which transforms the inputs to have zero mean and unit variance which speeds up training.</p>
</section>
<section id="the-mambavision-architecture---micro" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-mambavision-architecture---micro">3.2 The MambaVision Architecture - Micro</h2>

<div class="no-row-height column-margin column-container"><div id="fig-micro" class="quarto-float quarto-figure quarto-figure-left anchored" data-fig-align="left" data-group="my-gallery">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-micro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="fig3.png" class="lightbox" data-gallery="my-gallery" title="Figure&nbsp;1: Architecture of MambaVision block"><img src="https://orenbochman.github.io/reviews/2024/mamba-vision/fig3.png" class="img-fluid quarto-figure quarto-figure-left figure-img" width="300"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-micro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Architecture of MambaVision block
</figcaption>
</figure>
</div></div><p>The authors redesigned the original Mamba mixer to make it more suitable for vision tasks.</p>
<ol type="1">
<li>regular convolution replaces causal convolution</li>
<li>added a symmetric branch without SSM , consisting of an additional convolution and SiLU activation, to compensate for any content lost due to the sequential constraints of SSMs.</li>
<li>These branches are concatenated and project via a final linear layer.</li>
</ol>
<p>This combination ensures that the final feature representation incorporates both the sequential and spatial information, leveraging the strengths of both branches.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AX_1%20&amp;=%20Scan(%CF%83(Conv(Linear(C,%20%5Cfrac%7BC%7D%7B2%7D%20)(X_%7Bin%7D))))%20%5C%5C%0AX_2%20&amp;=%20%CF%83(Conv(Linear(C,%20%5Cfrac%7BC%7D%7B2%7D%20)(X_%7Bin%7D)))%20%5C%5C%0AX_%7Bout%7D%20&amp;=%20Linear(%20%5Cfrac%7BC%7D%7B2%7D%20,%20C)(Concat(X_1,%20X_2))%20%5C%5C%0A%5Cend%7Balign*%7D%0A"></p>
</section>
<section id="ablation-studies" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ablation-studies">Ablation Studies</h2>
<p>Section 4 the experiment looks at MambaVision’s performance in image classification as well as other downstream tasks like, object detection, instance segmentation and semantic segmentation tasks. The authors note that the model was equipped with the model with specialized heads for different tasks and required fine tuning the original model. I am a somewhat critical of calling this the performance on downstream tasks when we are talking about models with different layers that were fine tuned using different optimizers on task specific datasets.</p>
<p>The results section outline an <strong>ablation study</strong><sup>2</sup> used to identify the optimal way to integrate the Vision Transformer (ViT) with the Mamba architecture.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;investigating the effects of removing parts of a model</p></div></div><p>As usual, the authors provide a family of models with different sizes to gauge the performance characteristics for scaling the model.</p>
<p>The various models</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://arxiv.org/pdf/2407.08083">paper</a></li>
<li><a href="https://github.com/NVlabs/MambaVision">code</a></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {MambaVision {A} {Hybrid} {Mamba-Transformer} {Vision}
    {Backbone}},
  url = {https://orenbochman.github.io/reviews/2024/mamba-vision/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“MambaVision A Hybrid Mamba-Transformer Vision
Backbone.”</span> <a href="https://orenbochman.github.io/reviews/2024/mamba-vision/">https://orenbochman.github.io/reviews/2024/mamba-vision/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/mamba-vision/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2024/mamba-vision/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Multi-column Deep Neural Networks for Image Classification</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/</link>
  <description><![CDATA[ 






<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="cireşan2012multicolumndeepneuralnetworks">(Cireşan, Meier, and Schmidhuber 2012)</span> titled “Multi-column Deep Neural Networks for Image Classification”, the authors, Dan Cireşan, Ueli Meier, Juergen Schmidhuber introduce a biologically plausible deep artificial neural network architecture that achieves near-human performance on tasks such as the recognition of handwritten digits or traffic signs. The method uses small receptive fields of convolutional winner-take-all neurons to yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. The authors demonstrate that their approach outperforms humans on a traffic sign recognition benchmark and improves the state-of-the-art on various image classification benchmarks.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks. — <span class="citation" data-cites="cireşan2012multicolumndeepneuralnetworks">(Cireşan, Meier, and Schmidhuber 2012)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
</section>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="cireşan2012multicolumndeepneuralnetworks">(Cireşan, Meier, and Schmidhuber 2012)</span> the authors make significant strides in the field of image classification by demonstrating the effectiveness of multi-column deep neural networks (DNNs). <mark>This work is noteworthy for its pioneering approach in applying deep learning techniques to image classification tasks, which have since become the foundation of modern computer vision systems.</mark></p>
<div class="no-row-height column-margin column-container"><div id="ref-cireşan2012multicolumndeepneuralnetworks" class="csl-entry">
Cireşan, Dan, Ueli Meier, and Juergen Schmidhuber. 2012. <span>“Multi-Column Deep Neural Networks for Image Classification.”</span> <a href="https://arxiv.org/abs/1202.2745">https://arxiv.org/abs/1202.2745</a>.
</div></div></section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<p>The authors present a system that uses several deep neural networks, each operating as a “column,” which are trained independently. The outputs of these networks are then averaged to form the final prediction. This multi-column approach exploits the diversity between different networks and boosts classification accuracy, reducing the impact of overfitting and improving generalization. Notably, the method achieved state-of-the-art results on several image classification benchmarks at the time, including the MNIST digit recognition task.</p>
<p>One of the central contributions of this paper is the demonstration of how <mark>combining multiple deep networks can outperform single networks in complex image classification tasks</mark>. The authors trained their models on NVIDIA GPUs, which allowed them to scale deep networks efficiently—a relatively new practice when this paper was published, underscoring its innovative edge.</p>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Improvement on Benchmarks</strong>: The multi-column DNN approach delivered unprecedented accuracy on datasets like MNIST, achieving an error rate of just 0.23%. This represents one of the early breakthroughs that paved the way for deep learning in computer vision.</p></li>
<li><p><strong>Effective Use of Parallelism</strong>: The paper highlights the use of modern GPUs to efficiently train deep networks, illustrating how hardware advancements can accelerate research progress.</p></li>
<li><p><strong>Generalizability</strong>: While the paper focuses on MNIST and other datasets, the multi-column DNN framework offers a flexible approach to other image classification tasks. The general architecture and training methodology could be adapted to more complex datasets, making this work highly relevant across a variety of image recognition problems.</p></li>
<li><p><strong>Robustness</strong>: By averaging outputs from multiple networks, the system reduces the sensitivity to the specific architecture or initialization of a single network. This ensemble-like approach increases robustness and reduces error rates.</p></li>
</ul>
</section>
<section id="weaknesses" class="level2">
<h2 class="anchored" data-anchor-id="weaknesses">Weaknesses</h2>
<ul>
<li><p><strong>Lack of Theoretical Insight</strong>: Although the empirical results are impressive, the paper does not delve deeply into the theoretical reasons behind the success of multi-column architectures. It remains unclear how much of the performance gain is due to ensembling versus the intrinsic strength of the individual networks.</p></li>
<li><p><strong>Computational Cost</strong>: The approach requires training multiple deep neural networks independently, which could be computationally expensive for larger datasets or higher-dimensional inputs. While GPUs mitigate this to an extent, scaling the multi-column approach to larger tasks would demand significant computational resources.</p></li>
<li><p><strong>Limited Applicability to Other Modalities</strong>: The paper focuses solely on image classification. While it hints at the potential for multi-column networks in other domains (e.g., audio or text), the paper doesn’t explore these extensions or provide empirical evidence beyond the image domain.</p></li>
</ul>
</section>
<section id="impact-and-relevance" class="level2">
<h2 class="anchored" data-anchor-id="impact-and-relevance">Impact and Relevance</h2>
<p>This paper marked a turning point for deep learning in computer vision, showing the power of combining deep networks for complex tasks like image classification. Its success on benchmarks like MNIST helped popularize deep learning as a dominant method for pattern recognition and set the stage for more advanced techniques. Although it primarily focuses on image classification, the insights regarding ensemble learning through independent deep networks have since inspired various approaches in different machine learning areas, including speech recognition and natural language processing.</p>
<p>The paper is particularly significant when viewed in the context of its time (2012), as it predated the massive adoption of deep learning across industries. Its methods were fundamental to later developments in deep convolutional neural networks, which have become a cornerstone of state-of-the-art models in computer vision tasks today.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Ciresan, Meier, and Schmidhuber’s work on multi-column deep neural networks represents a crucial step forward in the development of image classification techniques. Its impact on deep learning, especially in terms of model ensembling and parallelization using GPUs, cannot be overstated. While it comes with some computational challenges and lacks deep theoretical explanation, the paper’s practical results and novel approach have solidified its place as a landmark contribution in the history of deep learning.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Multi-Column {Deep} {Neural} {Networks} for {Image}
    {Classification}},
  url = {https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Multi-Column Deep Neural Networks for Image
Classification.”</span> <a href="https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/">https://orenbochman.github.io/reviews/2012/Multi-column
deep neural networks for image classi cation/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>NIN — Network in Network</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2013/NIN/</link>
  <description><![CDATA[ 






<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="lin2014networknetwork">(Lin, Chen, and Yan 2014)</span> the authors, Lin, Min, Qiang Chen, and Shuicheng Yan, of this paper titled “Network in Network” paper came up with a way of connencting somee ideas on improving CNNs which had been mostly getting bigger <img src="https://latex.codecogs.com/png.latex?(VGG%20%3E%20AlexNet%20%3E%20LeNet)"> . They replaced traditional linear filters in convolutional neural networks (CNNs) with multilayer perceptrons (MLPs) to enhance local feature abstraction. <mark>This new architecture, called NIN, also introduces <strong>global average pooling</strong> in place of <em>fully connected layers</em> thus reducing overfitting, improving model interpretability and more significantly reducing the size of the network.</mark></p>
<div class="no-row-height column-margin column-container"></div><p>It took a while for the idea to catch on, but in computer vision, most of the parameters are in the fully connected layers, and the NIN architecture enables us to to reduce the number of parameters in the fully connected layers thereby <mark>breaking the curse of dimensionality in CNN</mark>. Once people realized this the NIN architecture became more widely adopted and influenced the development of more sophisticated deep learning architectures like the <mark>Inception architecture</mark> and further refined into the Resnet architecture</p>
<p>The NIN architecture has a significant impact on the design of CNNs by demonstrating that local feature abstraction can be enhanced with MLPs, leading to better performance with fewer parameters. Global average pooling, which replaces fully connected layers, makes the architecture more robust to overfitting and spatial translations, making it a powerful tool for image classification tasks. This combination of techniques has influenced the development of more sophisticated deep learning architectures, particularly in domains where model interpretability and reduced overfitting are critical.</p>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>We propose a novel deep network structure called “Network In Network” (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking multiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.</p>
<p>– <span class="citation" data-cites="lin2014networknetwork">(Lin, Chen, and Yan 2014)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
</section>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="lin2014networknetwork">(Lin, Chen, and Yan 2014)</span> the authors introduced a novel deep learning architecture that aims to improve the abstraction capabilities of convolutional neural networks (CNNs) by incorporating multilayer perceptrons (MLPs) into the convolution layers. This approach, termed “Network in Network,” replaces the conventional linear filters used in CNNs with small neural networks, allowing for better local feature modeling. The NIN architecture also introduces global average pooling as a substitute for traditional fully connected layers to reduce overfitting and improve the interpretability of the model.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2014networknetwork" class="csl-entry">
Lin, Min, Qiang Chen, and Shuicheng Yan. 2014. <span>“Network in Network.”</span> <a href="https://arxiv.org/abs/1312.4400">https://arxiv.org/abs/1312.4400</a>.
</div></div></section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<p>The NIN paper makes several key contributions to the deep learning landscape:</p>
<ol type="1">
<li><p><strong>Mlpconv Layer</strong>: Instead of using traditional linear filters, NIN proposes the use of multilayer perceptrons (MLPs) within the convolutional layers (termed mlpconv layers). These layers act as universal function approximators, capable of modeling more complex representations within local receptive fields. This structure allows for better abstraction of non-linear latent concepts, overcoming the limitations of traditional linear filters in CNNs.</p>
<p><img src="https://orenbochman.github.io/reviews/2013/NIN/fig1.png" class="img-fluid"> <img src="https://orenbochman.github.io/reviews/2013/NIN/fig2.png" class="img-fluid"></p></li>
<li><p><strong>Global Average Pooling</strong>: NIN introduces global average pooling as an alternative to fully connected layers in the final classification stage. This technique computes the spatial average of each feature map, feeding the result directly into a softmax layer for classification. By avoiding fully connected layers, the model becomes less prone to overfitting, thus improving generalization performance. Furthermore, this method provides more interpretable results by establishing a direct correspondence between feature maps and class labels.</p></li>
<li><p><strong>State-of-the-Art Performance</strong>: The authors demonstrate that NIN achieves state-of-the-art performance on several benchmark datasets, including CIFAR-10, CIFAR-100, and SVHN, without the need for extensive data augmentation or model ensembling. The architecture consistently outperforms other methods, such as maxout networks and CNNs with dropout regularization, especially in terms of classification accuracy.</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./results.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Cifar-10 error rates"><img src="https://orenbochman.github.io/reviews/2013/NIN/results.png" class="img-fluid figure-img" style="width:80.0%" alt="Cifar-10 error rates"></a></p>
<figcaption>Cifar-10 error rates</figcaption>
</figure>
</div>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Innovative Architecture</strong>: The introduction of MLPs into convolutional layers is a simple yet effective modification that significantly enhances the representational power of the model. This makes NIN a powerful alternative to traditional CNNs, especially for tasks that require fine-grained feature extraction and abstraction.</p></li>
<li><p><strong>Reduced Overfitting</strong>: The use of global average pooling not only replaces the computationally expensive fully connected layers but also serves as a built-in regularizer, reducing the need for additional techniques like dropout. This structural regularization helps to prevent overfitting, particularly on datasets with limited training examples, such as CIFAR-100.</p></li>
<li><p><strong>Better Interpretability</strong>: The global average pooling layer allows for easier interpretation of the learned feature maps, as each map is directly associated with a class. This increases the transparency of the network’s the decision-making process compared to conventional CNNs.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Visulization"><img src="https://orenbochman.github.io/reviews/2013/NIN/fig4.png" class="img-fluid figure-img" style="width:80.0%" alt="Visulization"></a></p>
<figcaption>Visulization</figcaption>
</figure>
</div>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<ul>
<li><p><strong>Limited Novelty in Pooling</strong>: While global average pooling is effective, the concept is not entirely new, and its novelty is limited. Previous works have proposed similar techniques for specific tasks. However NIN certainly demonstrates the concepts efficacy.</p></li>
<li><p><strong>Scalability</strong>: The paper focuses primarily on relatively small datasets like CIFAR-10, CIFAR-100, SVHN, and MNIST. While NIN excels in these scenarios, it would be interesting to see how the architecture performs on larger, more complex datasets such as ImageNet, where the size and variety of the data might pose additional challenges.</p></li>
<li><p><strong>Lack of Depth Exploration</strong>: While the architecture consists of three stacked mlpconv layers, the paper does not deeply explore the impact of adding more layers or experimenting with deeper NIN networks. Such exploration could provide insight into how well the architecture scales with increased model depth.</p></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>NIN architecture is an elegant and effective solution to improving feature abstraction and reducing overfitting in CNNs. By embedding MLPs within convolutional layers and using global average pooling for classification, NIN achieves state-of-the-art performance across a variety of tasks. NIN presented a strong case for the importance of local feature modeling and interpretable classification mechanisms in modern deep learning.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
<ul>
<li><a href="https://www.youtube.com/watch?v=QfNvhP6k6ZM">Alex Smola Course Video on NIN</a>, his <a href="https://c.d2l.ai/stanford-cs329p/">course</a> and <a href="https://www.d2l.ai/">book</a></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {NIN -\/-\/- {Network} in {Network}},
  url = {https://orenbochman.github.io/reviews/2013/NIN/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“NIN --- Network in Network.”</span> <a href="https://orenbochman.github.io/reviews/2013/NIN/">https://orenbochman.github.io/reviews/2013/NIN/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2013/NIN/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
</item>
<item>
  <title>Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/q-star/</link>
  <description><![CDATA[ 






<p>The announcement of the GPT-5 strawberry model has sparked a lot of interest in this paper which seems to be the theory behind Open.ai’s new model.</p>
<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TLDR</h2>
<p>In <span class="citation" data-cites="wang2024qimprovingmultistepreasoning">(Wang et al. 2024)</span>, titled “Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning,” the authors propose a new framework called Q* to improve the multi-step reasoning capabilities of Large Language Models (LLMs). The authors identify a key issue with LLMs: their auto-regressive nature often leads to errors, hallucinations, and inconsistencies in multi-step reasoning tasks.</p>
<div class="no-row-height column-margin column-container"></div><div class="callout-info">
<p>some questions to consider:</p>
<ol type="1">
<li>In the Q* MDP what are the states, <strong>actions</strong>, rewards, and transition probabilities?</li>
<li>The idea of a utility function is central to the Q* framework. How is the utility function defined in this context?</li>
</ol>
</div>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Large Language Models (LLMs) have demonstrated impressive capability in many natural language tasks. However, the auto-regressive generation process makes LLMs prone to produce errors, hallucinations and inconsistent statements when performing multi-step reasoning. In this paper, by casting multi-step reasoning of LLMs as a heuristic search problem, we aim to alleviate the pathology by introducing Q*, a general, versatile and agile framework for guiding LLMs decoding process with deliberative planning. By learning a plug-and-play Q-value model as heuristic function for estimating expected future rewards, our Q* can effectively guide LLMs to select the most promising next reasoning step without fine-tuning LLMs for the current task, which avoids the significant computational overhead and potential risk of performance degeneration on other tasks. Extensive experiments on GSM8K, MATH and MBPP demonstrate the superiority of our method, contributing to improving the reasoning performance of existing open-source LLMs.</p>
<p>— <span class="citation" data-cites="wang2024qimprovingmultistepreasoning">(Wang et al. 2024)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-wang2024qimprovingmultistepreasoning" class="csl-entry">
Wang, Chaojie, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. 2024. <span>“Q*: Improving Multi-Step Reasoning for LLMs with Deliberative Planning.”</span> <a href="https://arxiv.org/abs/2406.14283">https://arxiv.org/abs/2406.14283</a>.
</div></div></div>
</section>
<section id="the-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-problem">The Problem</h2>
<p>LLMs face challenges in multi-step reasoning tasks, frequently producing errors due to their auto-regressive generation process. While previous efforts have tried to enhance LLMs’ “System 1” capabilities (fast but less accurate), complex reasoning requires “System 2” processes—more deliberative and logical thinking.</p>
</section>
<section id="the-q-framework" class="level2">
<h2 class="anchored" data-anchor-id="the-q-framework">The Q* Framework</h2>
<ul>
<li>The authors cast multi-step reasoning as a Markov Decision Process (MDP) and introduce the Q* framework. The framework uses a Q-value model as a heuristic function to guide LLMs during the decoding process.</li>
<li>Q* operates without the need for fine-tuning LLMs for each task, avoiding computational overhead and the risk of performance degradation in other tasks.</li>
<li>Q* leverages A* search to select the most promising next reasoning step, using a proxy Q-value model to guide this process.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./figure1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Q* framework overview"><img src="https://orenbochman.github.io/reviews/2024/q-star/figure1.png" class="img-fluid figure-img" alt="Q* framework overview"></a></p>
<figcaption>Q* framework overview</figcaption>
</figure>
</div>
</section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<ul>
<li><mark>Formalizing multi-step reasoning of LLMs as an MDP</mark></li>
<li>Introducing general approaches to estimate the optimal Q-value of state-action pairs, including offline reinforcement learning, rollouts, and completion with stronger LLMs.</li>
<li>Casting multi-step reasoning tasks as a <strong>heuristic search problem</strong> to find the optimal reasoning trace with maximum utility.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./algorithm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Deliberative planning Algorithm for LLMs with A*"><img src="https://orenbochman.github.io/reviews/2024/q-star/algorithm.png" class="img-fluid figure-img" alt="Deliberative planning Algorithm for LLMs with A*"></a></p>
<figcaption>Deliberative planning Algorithm for LLMs with A*</figcaption>
</figure>
</div>
</section>
<section id="estimation-of-optimal-q-value" class="level2">
<h2 class="anchored" data-anchor-id="estimation-of-optimal-q-value">Estimation of Optimal Q-value</h2>
<p>The authors present several ways to estimate the optimal Q-value:</p>
<ul>
<li>Offline Reinforcement Learning: Using Fitted Q-iteration to update the Q-value model iteratively.</li>
<li>Learning from Rollout: Performing random rollouts or Monte Carlo Tree Search (MCTS) to identify the best reasoning sequence.</li>
<li>Completion with Stronger LLMs: Using a stronger LLM to complete the trajectory and estimate the optimal Q-value.</li>
</ul>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<p>The framework was tested on math reasoning (GSM8K, MATH) and code generation (MBPP) tasks. Results showed that:</p>
<ul>
<li>Q* improves the reasoning capability of existing open-source LLMs.</li>
<li>Q* outperforms traditional Best-of-N methods and existing LLMs enhanced with alignment techniques like PPO (Proximal Policy Optimization).</li>
</ul>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Q* provides an agile deliberation framework for LLMs, improving their multi-step reasoning ability without the need for extensive fine-tuning. It is generalizable across various reasoning tasks, making it a versatile tool for enhancing LLM performance.</p>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Novel Framework</strong>: Introducing Q* as a deliberative planning framework is a novel approach to improving multi-step reasoning.</p></li>
<li><p><strong>Versatility</strong>: Q* can be applied to various reasoning tasks without task-specific modifications.</p></li>
<li><p><strong>Extensive Evaluation</strong>: The authors conducted experiments across multiple datasets, demonstrating the efficacy of their approach.</p></li>
</ul>
</section>
<section id="weaknesses-and-areas-for-improvement" class="level2">
<h2 class="anchored" data-anchor-id="weaknesses-and-areas-for-improvement">Weaknesses and Areas for Improvement</h2>
<ul>
<li><p><strong>Complexity</strong>: The paper introduces a relatively complex framework, which might present challenges in understanding and implementing the approach.</p></li>
<li><p><strong>Dependency on Q-value Estimation</strong>: The performance of Q* is heavily reliant on the accuracy of the Q-value model, which might be sensitive to the chosen estimation method.</p></li>
<li><p><strong>Limited Exploration of Alternatives</strong>: While the paper focuses on Q*<em>, there could be a discussion on how this method compares with other deliberative planning methods in more depth.</em></p></li>
</ul>
</section>
<section id="overall-impression" class="level2">
<h2 class="anchored" data-anchor-id="overall-impression"><em>Overall Impression</em></h2>
<p><em>The paper presents a solid contribution to improving multi-step reasoning in LLMs. The Q*</em> framework is a promising approach, particularly in its ability to generalize across different reasoning tasks. However, its complexity and the reliance on accurate Q-value estimation are potential hurdles that might need further exploration.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://arxiv.org/abs/2406.14283#">paper</a></li>
<li><a href="https://github.com/NVlabs/MambaVision">code</a></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Q*: {Improving} {Multi-step} {Reasoning} for {LLMs} with
    {Deliberative} {Planning}},
  url = {https://orenbochman.github.io/reviews/2024/q-star/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Q*: Improving Multi-Step Reasoning for LLMs
with Deliberative Planning.”</span> <a href="https://orenbochman.github.io/reviews/2024/q-star/">https://orenbochman.github.io/reviews/2024/q-star/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/q-star/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2024/q-star/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/smaller-weaker-yet-better/</link>
  <description><![CDATA[ 






<p>in <span class="citation" data-cites="bansal2024smallerweakerbettertraining">(Bansal et al. 2024)</span> the authors consider the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. They evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. They then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Their findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bansal2024smallerweakerbettertraining" class="csl-entry">
Bansal, Hritik, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, and Mehran Kazemi. 2024. <span>“Smaller, Weaker, yet Better: Training LLM Reasoners via Compute-Optimal Sampling.”</span> <a href="https://arxiv.org/abs/2408.16737">https://arxiv.org/abs/2408.16737</a>.
</div></div><p>Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Smaller, {Weaker,} {Yet} {Better:} {Training} {LLM}
    {Reasoners} via {Compute-Optimal} {Sampling}},
  url = {https://orenbochman.github.io/reviews/2024/smaller-weaker-yet-better/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Smaller, Weaker, Yet Better: Training LLM
Reasoners via Compute-Optimal Sampling.”</span> <a href="https://orenbochman.github.io/reviews/2024/smaller-weaker-yet-better/">https://orenbochman.github.io/reviews/2024/smaller-weaker-yet-better/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/smaller-weaker-yet-better/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2024/smaller-weaker-yet-better/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Some dynamics of signaling games</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2014/Huttegger-dynamics-of-signaling-games/</link>
  <description><![CDATA[ 







<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/images/lit-review-cover.jpg" class="nolightbox img-fluid figure-img"></p>
<figcaption>cover</figcaption>
</figure>
</div><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Fe7S5wbxEPQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Some Dynamics of Signaling Games by Brian Skyrms
</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="mindmap.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="mindmap"><img src="https://orenbochman.github.io/reviews/2014/Huttegger-dynamics-of-signaling-games/mindmap.png" class="img-fluid figure-img" alt="mindmap"></a></p>
<figcaption>mindmap</figcaption>
</figure>
</div></div>

<blockquote class="blockquote">
<p>“The most important thing in communication is hearing what isn’t said.”</p>
<p>— Peter Drucker</p>
</blockquote>
<!-- LEDE personal context why I reviewed this source 
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.
-->
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>TL;DR - Too Long; Didn’t Read about dynamics of signaling games <!-- Short Catchy title -->
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="dynamics of signaling games in a nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="dynamics of signaling games in a nutshell"></a></p>
<figcaption>dynamics of signaling games in a nutshell</figcaption>
</figure>
</div>
<!-- 1. What are the research questions? -->
<p>The research questions addressed in the paper revolve around understanding the dynamics of signaling games using evolutionary and learning models, in contrast to traditional analyses based on static equilibrium concepts. Specifically, the authors explore:</p>
<ul>
<li><strong>How different dynamical models, such as evolutionary dynamics (replicator dynamics, Moran process) and learning dynamics (reinforcement learning), affect the outcomes of signaling interactions</strong> across a spectrum of games with varying degrees of alignment of interests between sender and receiver.</li>
<li><strong>The conditions under which reliable or honest signaling can emerge and be maintained</strong> in scenarios with misaligned interests, particularly in the context of costly signaling.</li>
<li><strong>The stability and attractors of these dynamical systems</strong>, including convergence to signaling systems, partial pooling equilibria, or other outcomes.</li>
<li><strong>The impact of factors such as mutation rates, population size (finite vs.&nbsp;infinite), and the probability of different states of the world</strong> on the evolutionary and learning trajectories in signaling games.</li>
<li><strong>Whether Pareto optimal Nash equilibria are always reached through natural dynamics</strong> in signaling games.</li>
<li><strong>The dynamics of signaling games with diametrically opposed interests</strong>, where no signaling equilibrium exists.</li>
<li><strong>The behavior of individual learning models like reinforcement learning</strong> in various signaling game settings, including cases with more than two states, signals, and acts, and games with conflicts of interest or costly signaling.</li>
</ul>
<!-- 2. What are the main findings? -->
<p>The main findings of the paper highlight the importance of considering dynamics when analyzing signaling games, as static equilibrium analysis provides an incomplete picture. Some key findings include:</p>
<ul>
<li>In <strong>Lewis signaling games</strong> (fully aligned interests), the <strong>replicator dynamics do not always guarantee the emergence of perfect signaling</strong>, and outcomes can depend on initial conditions and the probability of states.</li>
<li>Introducing <strong>mutations (selection mutation dynamics)</strong> can alter the results of replicator dynamics and sometimes promote the emergence of signaling systems.</li>
<li><strong>Finite populations</strong> under the <strong>Moran process</strong> can favor perfectly informative signaling strategies in Lewis games even when they are not Nash equilibria, a contrast to infinite population models.</li>
<li>In <strong>costly signaling games</strong>, the <strong>replicator dynamics</strong> can lead to <strong>pooling, separating, and dynamically stable hybrid equilibria</strong>, suggesting that partial information transfer at low costs can be an evolutionarily significant outcome.</li>
<li><strong>Mutations in costly signaling games</strong> can stabilize the rest point corresponding to <strong>hybrid equilibria</strong>.</li>
<li>In games with <strong>opposed interests</strong>, the <strong>replicator dynamics</strong> can result in complex, non-equilibrium behavior such as <strong>strange attractors</strong>, where information transfer fluctuates.</li>
<li><strong>Reinforcement learning</strong> in Lewis signaling games converges to perfect signaling with probability one only in a specific, simple case (two states, two signals, two acts with equiprobable states). In more general cases, the outcomes are more complex, and suboptimal equilibria can have a positive probability of being reached.</li>
<li><strong>The explanatory significance of signaling equilibria depends on the underlying dynamics</strong>, and Pareto optimal equilibria are not always the outcome of evolutionary or learning processes.</li>
</ul>
<!-- 3. In historical context why was this important? -->
<p>In historical context, this research is important because it represents a move beyond the traditional focus on static equilibrium analysis in game theory, particularly in the study of signaling and communication. For a long time, understanding signaling relied heavily on concepts like Pareto optimal Nash equilibria and evolutionarily stable strategies. However, the authors emphasize that <strong>analyzing the dynamics of these games through models of evolution and learning provides a more nuanced and realistic understanding of how signaling interactions unfold</strong>. This shift was motivated by the realization that many games have multiple Nash equilibria, and it was unclear which, if any, would be selected by natural processes. The paper contributes to a growing body of literature that uses evolutionary game theory and learning models to investigate the foundations of communication, cooperation, and strategic interaction in various biological and social contexts. It highlights that the stability and likelihood of different signaling outcomes are deeply intertwined with the underlying dynamic processes, questioning the sole reliance on static equilibrium concepts for explaining signaling phenomena.</p>
</div>
</div>
<p>Here is a lighthearted Deep Dive into the paper:</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
<section id="abstract" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Information transfer is a basic feature of life that includes signaling within and between organisms. Owing to its interactive nature, signaling can be investigated by using game theory. Game theoretic models of signaling have a long tradition in biology, economics, and philosophy. For a long time the analyses of these games has mostly relied on using static equilibrium concepts such as Pareto optimal Nash equilibria or evolutionarily stable strategies. More recently signaling games of various types have been investigated with the help of game dynamics, which includes dynamical models of evolution and individual learning. A dynamical analysis leads to more nuanced conclusions as to the outcomes of signaling interactions. Here we explore different kinds of signaling games that range from interactions without conflicts of interest between the players to interactions where their interests are seriously misaligned. We consider these games within the context of evolutionary dynamics (both infinite and finite population models) and learning dynamics (reinforcement learning). Some results are specific features of a particular dynamical model, whereas others turn out to be quite robust across different models. This suggests that there are certain qualitative aspects that are common to many real-world signaling interactions.</p>
<p>— <span class="citation" data-cites="huttegger2014some">(Huttegger et al. 2014)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-huttegger2014some" class="csl-entry">
Huttegger, Simon, Brian Skyrms, Pierre Tarres, and Elliott Wagner. 2014. <span>“Some Dynamics of Signaling Games.”</span> <em>Proceedings of the National Academy of Sciences</em> 111 (supplement_3): 10873–80.
</div></div></div>
</section>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This book/paper uses lots of big terms so let’s break them down so we can understand them better</p>
<dl>
<dt>Signaling Game</dt>
<dd>
A game theory model that analyzes strategic interactions involving the transmission of information from a sender to a receiver through signals, with payoffs depending on the state of the world, the signal sent, and the action taken by the receiver.
</dd>
<dt>Lewis Signaling Game</dt>
<dd>
A basic type of signaling game where the interests of the sender and the receiver are perfectly aligned; the goal is for the receiver to take an action that matches the true state of the world known by the sender.
</dd>
<dt>Costly Signaling</dt>
<dd>
A signaling mechanism where the signal itself imposes a cost on the sender. This cost can be state-dependent or fixed and can potentially ensure the honesty of the signal if higher costs are associated with misrepresentation.
</dd>
<dt>Replicator Dynamics</dt>
<dd>
A fundamental model in evolutionary game theory that describes how the frequency of strategies in a population changes over time based on their relative success (payoff). Strategies with higher-than-average payoffs increase in proportion.
</dd>
<dt>Moran Process</dt>
<dd>
A model of evolutionary dynamics in finite populations where, at each step, an individual is randomly chosen to die and is replaced by a copy of another individual chosen with probability proportional to their fitness.
</dd>
<dt>Nash Equilibrium</dt>
<dd>
A state in a game where no player can improve their payoff by unilaterally changing their strategy, given the strategies of the other players.
</dd>
<dt>Evolutionarily Stable Strategy (ESS)</dt>
<dd>
A strategy that, if adopted by a population, cannot be invaded by any rare mutant strategy.
</dd>
<dt>Pareto Optimal Nash Equilibrium</dt>
<dd>
A Nash equilibrium where there is no other outcome that makes at least one player better off without making any player worse off.
</dd>
<dt>Partial Pooling Equilibrium</dt>
<dd>
An equilibrium in a signaling game where some signals are sent in multiple states of the world, and some actions are taken in response to multiple signals, resulting in imperfect information transfer.
</dd>
<dt>Selection Mutation Dynamics</dt>
<dd>
A variation of the replicator dynamics that incorporates a small probability of mutation, where individuals randomly switch to other available strategies.
</dd>
<dt>Hybrid Equilibrium</dt>
<dd>
In costly signaling games, an equilibrium where senders mix between sending signals truthfully and untruthfully, and receivers sometimes respond to signals and sometimes ignore them.
</dd>
<dt>Reinforcement Learning</dt>
<dd>
A type of learning where individuals adjust their behavior based on past rewards or payoffs, increasing the likelihood of choosing actions that have been successful in the past.
</dd>
<dt>Lyapunov Function</dt>
<dd>
A function whose value decreases along the trajectories of a dynamical system, often used to prove the stability of equilibrium points.
</dd>
<dt>Strange Attractor</dt>
<dd>
A bounded region in the state space of a dynamical system that attracts nearby trajectories but exhibits chaotic behavior, meaning trajectories within it diverge exponentially over time.
</dd>
<dt>Fixation Probability</dt>
<dd>
In finite population models, the probability that a single mutant individual carrying a particular strategy will eventually spread through the entire population, replacing all other strategies.
</dd>
<dt>Pooling Equilibrium</dt>
<dd>
An equilibrium in a signaling game where the sender sends the same signal regardless of the state of the world, providing no information to the receiver.
</dd>
<dt>Separating Equilibrium</dt>
<dd>
An equilibrium in a signaling game where the sender sends a different signal for each different state of the world, allowing the receiver to perfectly infer the state.
</dd>
<dt>Synonyms (in signaling)</dt>
<dd>
Different signals that are used to represent the same state.
</dd>
<dt>Information Bottleneck (in signaling)</dt>
<dd>
Multiple different states that are represented by the same signal.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<!-- USING AN UNNUMBERED MARKDOWN LIST THE OUTLINE GOES HERE -->
<ul>
<li>Replicator Dynamics
<ul>
<li>Describes the replicator dynamics as the fundamental model of evolutionary game theory.</li>
<li>Presents both one-population and two-population replicator dynamics.</li>
<li>Notes that the replicator dynamics in signaling games is often not structurally stable, making it important to study the effects of perturbations such as mutation.</li>
<li>Discusses the selection mutation dynamics as a plausible perturbation to the replicator dynamics.</li>
</ul></li>
<li>Lewis Signaling Games
<ul>
<li>Presents the replicator dynamics of Lewis signaling games, noting that signaling systems are locally asymptotically stable but other stable rest points may exist.</li>
<li>Discusses the existence and stability of partial pooling equilibria in Lewis signaling games with more than two signals, states, and acts.</li>
<li>Highlights the instability of interior rest points (“tower of Babel” situations) in all Lewis signaling games.</li>
<li>Describes the selection mutation dynamics of Lewis signaling games, noting the dependence of outcomes on mutation parameters.</li>
</ul></li>
<li>Costly Signaling Games
<ul>
<li>Presents the replicator dynamics of costly signaling games such as the Spence game and the Sir Philip Sidney game.</li>
<li>Discusses the existence and stability of pooling, separating, and hybrid equilibria in these games.</li>
<li>Highlights the significance of hybrid equilibria as evolutionarily stable outcomes with partial information transfer at low costs.</li>
<li>Notes that hybrid equilibria remain stable under perturbations of the replicator dynamics, suggesting their theoretical and empirical relevance.</li>
</ul></li>
<li>Opposed Interests
<ul>
<li>Presents the two-population replicator dynamics of signaling games with completely opposed interests.</li>
<li>Notes that no signaling equilibrium exists in these games.</li>
<li>Highlights the emergence of information transfer off equilibrium due to the existence of a strange attractor in the interior of the state space.</li>
</ul></li>
<li>Finite Population Dynamics
<ul>
<li>Describes the frequency-dependent Moran process as a model of evolution in finite populations.</li>
<li>Presents the fixation probability of a mutant strategy in a monomorphic population.</li>
<li>Discusses Lewis signaling games under the Moran process, where perfectly informative signaling strategies are evolutionarily favored under weak selection.</li>
</ul></li>
<li>Costly Signaling Games
<ul>
<li>Presents the Moran process with mutation, where new individuals can adopt a random strategy with a small probability.</li>
<li>Discusses a two-state, two-signal, two-act signaling game with conflicting interests, noting that the population spends a significant amount of time signaling informatively despite the absence of a Nash equilibrium supporting such behavior.</li>
<li>Highlights the importance of non-Nash play in finite populations with rare mutations.</li>
<li>Describes the role of cost-free preplay signaling in promoting cooperation in Prisoner’s Dilemma and Stag Hunt games.</li>
</ul></li>
<li>Reinforcement Learning
<ul>
<li>Describes reinforcement learning as a model of individual learning where players adjust their strategies based on past payoffs.</li>
<li>Presents the Herrnstein–Roth–Erev reinforcement learning model.</li>
<li>Discusses the convergence properties of reinforcement learning in Lewis signaling games, noting that convergence to a signaling system is guaranteed only in the simplest case.</li>
<li>Highlights the emergence of suboptimal equilibria containing synonyms and information bottlenecks in more general Lewis signaling games.</li>
</ul></li>
<li>Discussion
<ul>
<li>Summarizes the findings on the replicator dynamics, highlighting the dependence of signaling outcomes on population size, mutation rates, and the alignment of interests.</li>
<li>Emphasizes the possibility of information transfer off equilibrium in games with conflicting interests.</li>
<li>Notes the importance of non-Nash play in finite populations and the potential for reinforcement learning to lead to both optimal and suboptimal signaling equilibria.</li>
<li>Concludes that the explanatory significance of signaling equilibria depends on the underlying dynamics, and that natural dynamics may not always select a Pareto optimal outcome.</li>
</ul></li>
</ul>
</section>
<section id="reflections" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="reflections">Reflections</h2>
<!-- Beyond the outline -->
<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.</p>
<p>Nunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.</p>

<div class="no-row-height column-margin column-container"><div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_1.jpg" class="lightbox" data-gallery="figures" title="Figure&nbsp;2: figure 1"><img src="https://orenbochman.github.io/reviews/2014/Huttegger-dynamics-of-signaling-games/fig_1.jpg" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: figure 1
</figcaption>
</figure>
</div></div><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.</p>
<p>Nunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Some Dynamics of Signaling Games},
  url = {https://orenbochman.github.io/reviews/2014/Huttegger-dynamics-of-signaling-games/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Some Dynamics of Signaling Games.”</span> <a href="https://orenbochman.github.io/reviews/2014/Huttegger-dynamics-of-signaling-games/">https://orenbochman.github.io/reviews/2014/Huttegger-dynamics-of-signaling-games/</a>.
</div></div></section></div> ]]></description>
  <category>draft</category>
  <category>review</category>
  <guid>https://orenbochman.github.io/reviews/2014/Huttegger-dynamics-of-signaling-games/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/images/lit-review-cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Temporal Abstraction in Reinforcement Learning with the Successor Representation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2023/temporal-abstraction/</link>
  <description><![CDATA[ 






<p>This paper review is an extended introduction to temporal abstraction using options. It covers lots of advanced concepts in reinforcement learning that were introduced in Doina’s Precup’s talk in the Coursera Specialization on Reinforcement Learning by Martha White and Adam White. The paper is a deep dive into the topic of options and the successor representation. It is a long paper with lots of advanced concepts and algorithms. The paper is a great resource for anyone interested in reinforcement learning and temporal abstraction.</p>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Ever since I saw <span class="citation" data-cites="Martha2022SubTasks">(White 2022)</span> the video lecture on subtasks by Martha White about learning tasks in parallel. However the video does not address the elephant in the room - how to discover the options.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Martha2022SubTasks" class="csl-entry">
White, Martha. 2022. <span>“Developing Reinforcement Learning Agents That Learn Many Subtasks.”</span> <em>YouTube</em>. <a href="https://www.youtube.com/watch?v=GmGL9cVfJG4&amp;t=6s">https://www.youtube.com/watch?v=GmGL9cVfJG4&amp;t=6s</a>.
</div><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/GmGL9cVfJG4" title="Martha White - Developing Reinforcement Learning Agents that Learn Many Subtasks?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
</figcaption>
</figure>
</div><div id="fig-option-discovery" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-option-discovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/m9gYmYEYuIs" title="Marlos C. Machado - Representation-driven Option Discovery in Reinforcement Learning?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-option-discovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Talk at Cohere.AI by Marlos C. Machado on Representation-driven Option Discovery in Reinforcement Learning. He discusses the Representation-driven Option Discovery (ROD) cycle and how it can be used to discover options in reinforcement learning. The talk covers much of the material in the paper as well as some more recent follow up work.
</figcaption>
</figure>
</div><div id="ref-Machado2024Cohere" class="csl-entry">
———. 2024. <span>“Representation-Driven Option Discovery in Reinforcement Learning.”</span> <em>YouTube</em>. <a href="https://www.youtube.com/watch?v=m9gYmYEYuIs">https://www.youtube.com/watch?v=m9gYmYEYuIs</a>.
</div></div>

<p>This is a hefty paper 70 pages with 8 algorithms many figures and citations from research spanning thirty years. It is filled to the brim with fascinating concepts that are developed by the authors but builds on lots of work by earlier researchers. It may seem to cover a niche topic but <span class="citation" data-cites="Machado2024Cohere">(c.f. Machado 2024, time 773)</span> makes an eloquent argument that this paper deals with a fundamental question of where options come and if we put aside the jargon for a second we are trying to capture a form of intelingence that includes elements of generalization, planning, problem solving, learning at a level much closer what we are familiar with. And these familiar forms of mental abstractions much harder to consider in the context of Supervised or Unsupervised learning which lack the ineraction with the environment that is the hallmark of reinforcement learning.</p>
<p>I came about this paper by accident. I a quick summary before I realized how long it was and I put out my first pass, and I hope to flesh it including perhaps a bit of code.</p>
<p>I’ve been developing my own ideas regarding the creation and aggregation of options in reinforcement learning. My thinking to date has been different. I am exploring a Bayesian based tasks. I’ve considered creating shared semantics via emergent symbolic semantics and looking at a number of composability mechanisms for state, language and of options including using hierarchial bayesian models. While working on coding environments for this subjects a search led to this amazing paper!</p>
<p>In <span class="citation" data-cites="reinforcement_2024">(<strong>reinforcement_2024?</strong>)</span> Marlos C. Machado, has given a talk that explains many of the complex ideas within this paper. This talk is available on YouTube.</p>
<p>Marlos C. Machado is a good speaker and going over that paper and the video certainly helps to understand the challenges of temporal abstractions as well as the solutions that the paper proposes.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<p>This paper posits that <strong>successor representations</strong>, which encode states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstraction like options if these are not known.</p>
<p>Options are a powerful form of temporal abstraction that allows agents to make predictions and to operate at different levels of abstraction within an environment in ways idiosyncratic of human approach to tackle many problems. One of the key questions has been how to discover good options. The paper presents a rather simple yet powerful answer to this.</p>
</div>
</div>
<p>Here is a lighthearted Deep Dive into the paper</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Abstract
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation, which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the successor representation can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent’s representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we also discuss how the successor representation allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the successor representation to combine them. Our results shed light on important design decisions involved in the definition of options and demonstrate the synergy of different methods based on the successor representation, such as eigenoptions and the option keyboard.</p>
<p>— <span class="citation" data-cites="machado2023temporal">(Machado et al. 2023)</span></p>
</blockquote>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"></div></section>
<section id="the-review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-review">The Review</h2>
<section id="introduction-1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="introduction-1">Introduction</h3>

<div class="no-row-height column-margin column-container"><div id="fig-option-framework" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-option-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/GntIVgNKkCI" title="DeepHack.RL: Doina Precup - Temporal abstraction in reinforcement learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-option-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Doina Precup’s Talk at DeepHack.RL on Temporal abstraction in reinforcement learning covers both the intro and the background material on options.
</figcaption>
</figure>
</div></div><p>In this section, the authors introduce the reinforcement learning problem and the options framework. Next they discuss the benefits of using options and highlight the option discovery problem. Next they present the successor representation (SR) as a representation learning method that is conducive to option discovery, summarizing its use cases and connecting it to neuroscience They go on to describe the paper’s focus on temporally-extended exploration and the use of eigenoptions and covering options. The finnish the introduction by highlight the paper’s evaluation methodology and the use of toy domains and navigation tasks for clarity and intuition.</p>
<p>In <span class="citation" data-cites="Doina2017DeepHack">(Precup 2017)</span> Doina precup gives a talk on temporal abstraction in reinforcement learning. This talk covers both the introduction and the background material on options and is on YouTube.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Doina2017DeepHack" class="csl-entry">
Precup, Doina. 2017. <span>“DeepHack.RL: Temporal Abstraction in Reinforcement Learning.”</span> <em>YouTube</em>. <a href="https://www.youtube.com/watch?v=GntIVgNKkCI">https://www.youtube.com/watch?v=GntIVgNKkCI</a>.
</div></div></section>
</section>
<section id="background" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<ul>
<li><p>Defines the reinforcement learning problem, covering Markov Decision Processes, policies, value functions, and common algorithms such as Q-learning.</p></li>
<li><p>Introduces the options framework <span class="citation" data-cites="Sutton1999BetweenMA">(Richard S. Sutton, Precup, and Singh 1999)</span>, <span class="citation" data-cites="precup2000temporal">(Precup and Sutton 2000)</span>, defining its components (initiation set, policy, termination condition), execution models, and potential benefits.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-Sutton1999BetweenMA" class="csl-entry">
Sutton, Richard S., Doina Precup, and Satinder Singh. 1999. <span>“Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning.”</span> <em>Artificial Intelligence</em> 112 (1): 181–211. https://doi.org/<a href="https://doi.org/10.1016/S0004-3702(99)00052-1">https://doi.org/10.1016/S0004-3702(99)00052-1</a>.
</div><div id="ref-precup2000temporal" class="csl-entry">
Precup, Doina, and Richard S. Sutton. 2000. <span>“Temporal Abstraction in Reinforcement Learning.”</span> PhD thesis, University of Massachusetts Amherst.
</div></div><p>An option <img src="https://latex.codecogs.com/png.latex?%5Comega%20%5Cin%20%5COmega"> is a 3-tuple</p>
<p><span id="eq-6"><img src="https://latex.codecogs.com/png.latex?%0A%5Comega%20=%20%3CI_%5Comega%20,%20%5Cpi_%5Comega%20,%20%5Cbeta_%5Comega%20%3E%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<ul>
<li>where
<ul>
<li><img src="https://latex.codecogs.com/png.latex?I_%5Comega%20%E2%8A%86%20S"> the options’s initiation set,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Comega%20:%20%5Cmathcal%7BS%7D%20%5Ctimes%20%5Cmathcal%7BA%7D%20%5Crightarrow%20%5B0,%201%5D"> the option’s policy, such that <img src="https://latex.codecogs.com/png.latex?%5Csum_a%20%5Cpi_%5Comega%20(%C2%B7,%20a)%20=%201">, and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta_%5Comega%20:%20%5Cmathcal%7BS%7D%20%5Crightarrow%20%5B0,%201%5D"> the option’s termination condition <sup>1</sup></li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;the probability that option <img src="https://latex.codecogs.com/png.latex?%CF%89"> will terminate at a given state.</p></div></div></section>
<section id="a-framework-for-option-discovery-from-representation-learning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-framework-for-option-discovery-from-representation-learning">A Framework for Option Discovery from Representation Learning</h2>

<div class="no-row-height column-margin column-container"><div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" width="250px" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;4: Representation-driven Option Discovery (ROD) cycle [@Machado2019EfficientEI]. The option discovery algorithms discussed in this paper can be seen as instantiating this cycle. The incoming arrow to Collect samples depicts the start of the process. The arrow from Define option to Option set highlights the output generated by the ROD cycle. Note that other generated artifacts can also be used by the agent outside the ROD cycle, such as the learned representation."><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/fig_1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Representation-driven Option Discovery (ROD) cycle <span class="citation" data-cites="Machado2019EfficientEI">(Machado 2019)</span>. The option discovery algorithms discussed in this paper can be seen as instantiating this cycle. The incoming arrow to Collect samples depicts the start of the process. The arrow from Define option to Option set highlights the output generated by the ROD cycle. Note that other generated artifacts can also be used by the agent outside the ROD cycle, such as the learned representation.
</figcaption>
<div id="ref-Machado2019EfficientEI" class="csl-entry">
Machado, Marlos C. 2019. <span>“Efficient Exploration in Reinforcement Learning Through Time-Based Representations.”</span> <em>Revue De Geographie Alpine-Journal of Alpine Research</em>. PhD thesis.
</div></figure>
</div></div><ul>
<li><p>Introduces a general framework for option discovery driven by representation learning, named the <strong>Representation-driven Option Discovery</strong> (ROD) cycle.</p>
<ul>
<li>Collect samples</li>
<li>Learn a representation</li>
<li>Derive an intrinsic reward function from the representation</li>
<li>Learn to maximize intrinsic reward</li>
<li>Define option</li>
</ul></li>
<li><p>Presents a step-by-step explanation of the ROD cycle, outlining its iterative and constructivist nature, as depicted in the figure.</p></li>
</ul>
</section>
<section id="the-successor-representation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-successor-representation">The Successor Representation</h2>

<div class="no-row-height column-margin column-container"><div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" width="250px" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;5: Example similar to Dayans 1993 of the SR, w.r.t. the uniform random policy, of state A (left). Consider a navigation task where the agent has access to its (x, y) coordinates. It is tempting to use some distance metric such as the Euclidean distance to define distance between states. However, if one considers the gray tiles to be walls, an agent in point A can reach point B much quicker than point C. The SR captures this distinction, ensuring that, in this representation, point A is closer to point B than it is to point C. The plots of the SR were generated using a discretization of the grid, where each tile is a state. Red represents larger values while blue represents smaller values (states that are temporally further away). Recall the SR of a state, in the tabular case, is an |S|-dimensional representation, thus allowing us to depict it as a heatmap over the state space."><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/fig_3.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Example similar to Dayans 1993 of the SR, w.r.t. the uniform random policy, of state A (left). Consider a navigation task where the agent has access to its (x, y) coordinates. It is tempting to use some distance metric such as the Euclidean distance to define distance between states. However, if one considers the gray tiles to be walls, an agent in point A can reach point B much quicker than point C. The SR captures this distinction, ensuring that, in this representation, point A is closer to point B than it is to point C. The plots of the SR were generated using a discretization of the grid, where each tile is a state. Red represents larger values while blue represents smaller values (states that are temporally further away). Recall the SR of a state, in the tabular case, is an |S|-dimensional representation, thus allowing us to depict it as a heatmap over the state space.
</figcaption>
</figure>
</div></div><ul>
<li><p>Presents the successor representation (SR) as a method to extract representations from observations.</p></li>
<li><p>In <span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 4.1)</span> Defines the SR in the tabular setting, explaining its ability to capture environment dynamics by encoding expected future state visitation, as shown in Equation 7 and the figure.</p></li>
</ul>
<div class="no-row-height column-margin column-container"></div><p><span id="eq-7"><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi_%7B%5Cpi%7D(s,s')%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi,p%7D%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%20%CE%B3%5Et%20%5Cmathbb%7B1%7D_%7B%5C%7BS_t=s'%7D%5C%7D%20%7C%20%7BS_0%20=%20s%20%7D%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>

<div class="no-row-height column-margin column-container"><div id="fig-4" class="quarto-float quarto-figure quarto-figure-center anchored" width="250px" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;6: First three PVFs in the (a) four-room domain. Gray squares represent walls and white squares represent accessible states. Four actions are available: up, down, right, and left. The transitions are deterministic and the agent is not allowed to move into a wall. (b-d) These plots depict the first, second, and third eigenvectors associated with each state. The axes are rotated for clarity. The bottom left corner of the four-room domain is the state closer to the reade"><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/fig_4.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: First three PVFs in the (a) four-room domain. Gray squares represent walls and white squares represent accessible states. Four actions are available: up, down, right, and left. The transitions are deterministic and the agent is not allowed to move into a wall. (b-d) These plots depict the first, second, and third eigenvectors associated with each state. The axes are rotated for clarity. The bottom left corner of the four-room domain is the state closer to the reade
</figcaption>
</figure>
</div></div><ul>
<li>Discusses the estimation of the SR with temporal-difference learning, its connection to general value functions <sup>2</sup>, and its relationship to the transition probability matrix Equation 9.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;“the SR can be estimated from samples with temporal-difference learning methods <span class="citation" data-cites="sutton1988learning">(Richard S. Sutton 1988)</span>, where the reward function is replaced by the state occupancy”</p><div id="ref-sutton1988learning" class="csl-entry">
Sutton, Richard S. 1988. <span>“Learning to Predict by the Methods of Temporal Differences.”</span> <em>Machine Learning</em> 3: 9–44.
</div></div></div><p><span id="eq-8"><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi(S_t,j)%20%5Cleftarrow%20%5Chat%7B%5CPsi%7D(S_t,%20j)%20+%20%5Ceta%20%5B%5Cmathbb%7B1%7D_%7B%5C%7BS_t=j%5C%7D%7D%20+%20%5Cgamma%20%5Chat%7B%5CPsi%7D(S_%7Bt+1%7D,%20j)%20%E2%88%92%20%5Chat%7B%5CPsi%7D(S_t,%20j)%5D%0A%5Ctag%7B3%7D"></span></p>
<p><span id="eq-9"><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi_%5Cpi%20=%20%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20(%5Cgamma%20P_%5Cpi)%5Et%20=%20(I-%5Cgamma%20P_%5Cpi)%5E%7B-1%7D%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<ul>
<li><p>Introduces successor features (SFs) as a generalization of the SR to the function approximation setting, extending the definition of the SR to arbitrary features, as shown in Equation 11.</p></li>
<li><p>Highlights the relationship between the SR and PVFs.</p></li>
</ul>
</section>
<section id="temporally-extended-exploration" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="temporally-extended-exploration">Temporally-Extended Exploration</h2>
<ul>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 5)</span> discusses temporally-extended exploration with options and its potential to enhance exploration in RL.</p></li>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 5.1)</span> introduces eigenoptions, which are options defined by the eigenvectors of the SR. &gt; “Eigenoptions are options defined by the eigenvectors of the SR.2 Each eigenvector assigns an intrinsic reward to every state in the environment.”</p>
<ul>
<li><p>Explains the concept of eigenoptions using the four-room domain as an example (Figure 5).</p></li>
<li><p>Describes how to learn eigenoptions’ policies using an intrinsic reward function derived from the eigenvectors of the SR.</p></li>
<li><p>Defines the initiation set and termination condition of eigenoptions, as shown in Equation 16.</p></li>
<li><p>Presents Theorem 1, which guarantees the existence of at least one terminal state for each eigenoption.</p></li>
</ul></li>
<li><p>Introduces covering options, which are point options defined by the bottom eigenvector of the graph Laplacian and aim to minimize the environment’s cover time.</p>
<ul>
<li>Explains the concept of covering options using the four-room domain (Figure 7).</li>
<li>Describes how to learn covering options’ policies using a simplified intrinsic reward function.</li>
<li>Defines the initiation set and termination condition of covering options.</li>
<li>Highlights the iterative nature of covering option discovery, where options are added one by one at each iteration.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"></div></section>
<section id="evaluation-of-temporally-extended-exploration-with-options" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evaluation-of-temporally-extended-exploration-with-options">Evaluation of Temporally-Extended Exploration with Options</h2>
<ul>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 6)</span>Evaluates eigenoptions and covering options in the context of temporally-extended exploration.</p></li>
<li><p>Uses the diffusion time, a task-agnostic metric, to quantify exploration effectiveness by measuring the expected number of decisions required to navigate between states.</p></li>
<li><p>Presents results comparing eigenoptions and covering options:</p>
<ul>
<li>Shows that both approaches can reduce diffusion time in the four-room domain when computed in closed form (Figure 8).</li>
<li>Discusses the impact of different initiation set sizes, highlighting the trade-off between avoiding sink states and ensuring option availability.</li>
</ul></li>
<li><p>Investigates the effectiveness of eigenoptions and covering options in an online setting:</p>
<ul>
<li>Demonstrates the robustness of eigenoptions to online SR estimation (Figure 11).</li>
<li>Reveals the challenges of using covering options online, particularly due to their restrictive initiation set and reliance on a single eigenvector (Figure 12).</li>
</ul></li>
<li><p>Explores the impact of using options on reward maximization in a fixed task:</p>
<ul>
<li>Shows that eigenoptions can accelerate reward accumulation when used for temporally-extended exploration in Q-learning (Figure 9).</li>
<li>Observes that covering options do not consistently improve reward maximization in this setting, likely due to their sparse initiation set.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fig-9" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_9.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;7: figure 9"><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/fig_9.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: figure 9
</figcaption>
</figure>
</div></div>
</section>
<section id="iterative-option-discovery-with-the-rod-cycle" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="iterative-option-discovery-with-the-rod-cycle">Iterative Option Discovery with the ROD Cycle</h2>
<ul>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 7)</span> introduces Covering Eigenoptions (CEO), a new algorithm that performs multiple iterations of the ROD cycle for option discovery.</p></li>
<li><p>Describes the steps of CEO, emphasizing its use of eigenoptions and online SR estimation, as outlined in Algorithm 2.</p></li>
<li><p>Demonstrates the benefits of multiple ROD cycle iterations with CEO, showing a significant reduction in the number of steps needed to visit all states in the four-room domain (Figure 14).</p></li>
<li><p>Illustrates the behavior of CEO over multiple iterations, highlighting its ability to progressively discover more complex options (Figure 14).</p></li>
<li><p>Combining Options with the Option Keyboard</p></li>
<li><p>Discusses the option keyboard as a way to combine existing options to create new options without additional learning, potentially expanding the agent’s behavioral repertoire.</p></li>
<li><p>Introduces Generalized Policy Evaluation (GPE) and Generalized Policy Improvement (GPI), generalizations of standard policy evaluation and improvement.</p></li>
<li><p>Explains how to use GPE and GPI to synthesize options from linear combinations of rewards induced by eigenvectors of the SR, as outlined in Algorithm 3.</p></li>
<li><p>Combining Eigenoptions with the Option Keyboard</p></li>
<li><p>Demonstrates the synergy of eigenoptions and the option keyboard.</p></li>
<li><p>Presents a qualitative analysis of options generated by combining eigenoptions with the option keyboard (Figures 16 and 17).</p></li>
<li><p>Shows that the option keyboard leads to a combinatorial explosion of new options, as evidenced by the number of unique options generated (Figure 18).</p></li>
<li><p>Demonstrates the diversity of options generated by the option keyboard through heatmaps showing the frequency of termination in different states (Figures 19 and 20).</p></li>
<li><p>Presents a quantitative analysis of the diffusion time induced by eigenoptions combined with the option keyboard, highlighting the improvement in exploration effectiveness (Figures 21 and 22).</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-machado2023temporal" class="csl-entry">
Machado, Marlos C., Andre Barreto, Doina Precup, and Michael Bowling. 2023. <span>“Temporal Abstraction in Reinforcement Learning with the Successor Representation.”</span> <em>Journal of Machine Learning Research</em> 24 (80): 1–69. <a href="http://jmlr.org/papers/v24/21-1213.html">http://jmlr.org/papers/v24/21-1213.html</a>.
</div></div></section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">Related Work</h2>
<ul>
<li>Discusses option discovery methods for planning and bottleneck options, including those based on spectral clustering and normalized cuts.</li>
<li>Mentions other option discovery methods for temporally-extended exploration, such as diffusion options.</li>
<li>Outlines extensions of the SR and option discovery methods to function approximation, including linear and non-linear function approximation techniques.</li>
<li>Discusses the connection of the SR to other reinforcement learning concepts, such as proto-value functions, slow-feature analysis, and dual representations.</li>
<li>Highlights the relationship of the SR to neuroscience, including its potential to model hippocampal place fields and grid cell activations.</li>
<li>Mentions the SR’s application to explaining human behavior and decision-making.</li>
</ul>
</section>
<section id="conclusion" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li>Highlights the potential of using the SR as the main substrate for temporal abstraction, pointing out promising directions for future work.</li>
<li>Emphasizes the importance of iterative option discovery and its role in building intelligent agents capable of continual learning and complex skill acquisition.</li>
</ul>
<p>Here are the successor representations algorithms from the paper:</p>

<div class="no-row-height column-margin column-container"><div id="fig-11" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./alg_1.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;8: successor representations algorithms"><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/alg_1.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: successor representations algorithms
</figcaption>
</figure>
</div></div><p>Next is the covering eigenoptions algorithm:</p>

<div class="no-row-height column-margin column-container"><div id="fig-12" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./alg_2.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;9: Covering Eigenoptions algorithm"><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/alg_2.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Covering Eigenoptions algorithm
</figcaption>
</figure>
</div></div></section>
<section id="study-guide-for-the-paper" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="study-guide-for-the-paper">Study guide for the paper</h2>
<ol type="1">
<li>What is an option in reinforcement learning?</li>
</ol>
<p>We actually took the definition from the paper. But here is another from the video. This is perhaps a more elegant definition. It comes from []</p>
<p>In reinforcement learning, an <strong>option</strong> is a temporally extended course of actions that allows an agent to operate at different levels of abstraction within an environment. Options are a form of temporal abstraction that enables agents to make predictions and execute actions over extended time horizons, providing a way to structure and organize the agent’s behavior. <img src="https://latex.codecogs.com/png.latex?%0Av_%7B%5Cpi,%5Cbeta%7D%5E%7Bc,z%7D(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi,%5Cbeta%7D%20%5Cleft%5B%20%5Csum_%7Bj=1%7D%5EK%20c(S_j)%20+%20%5Cgamma%5E%7BK-1%7D%20z(S_k)%20%7C%20S_0%20=%20s%20%5Cright%5D%20%5Cqquad%20%5Ctext%7Bfor%20all%20%7D%20s%20%5Cin%20S%0A"></p>
<ul>
<li>where
<ul>
<li><img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi,%5Cbeta%7D%5E%7Bc,z%7D(s)"> is the value function of the option,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi"> is the policy,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is the termination condition,</li>
<li><img src="https://latex.codecogs.com/png.latex?c"> is the extrinsic reward function, <img src="https://latex.codecogs.com/png.latex?z"> is the intrinsic reward function,</li>
<li><img src="https://latex.codecogs.com/png.latex?S_j"> is the state at time <img src="https://latex.codecogs.com/png.latex?j">,</li>
<li><img src="https://latex.codecogs.com/png.latex?K"> is the duration of the option, and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor.</li>
</ul></li>
</ul>
<ol type="1">
<li>How can options be used ?</li>
</ol>
<ul>
<li>For planning: you can use eigenvectors of the SR to identify bottleneck, states that are difficult to reach under a random walk, and then use options to guide the agent to those states. c.f. <span class="citation" data-cites="Solway2014OptimalBH">(Solway et al. 2014)</span></li>
<li>For exploration: you can use eigenoptions to encourage exploration by driving the agent toward states that are difficult to reach under a random walk.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-Solway2014OptimalBH" class="csl-entry">
Solway, Alec, Carlos Diuk, N. Córdova, Debbie M. Yee, A. Barto, Y. Niv, and M. Botvinick. 2014. <span>“Optimal Behavioral Hierarchy.”</span> <em>PLoS Computational Biology</em> 10.
</div></div><ol type="1">
<li>Explain the successor representation</li>
</ol>
<p>The <strong>successor representation (SR)</strong> is a method in reinforcement learning that represents states based on their expected future visits under a given policy. It captures the environment’s dynamics by encoding how likely an agent is to visit each state in the future, starting from a particular state.</p>
<ul>
<li>The SR is denoted as <img src="https://latex.codecogs.com/png.latex?%CE%A8_%CF%80">, where <img src="https://latex.codecogs.com/png.latex?%CF%80"> represents the agent’s policy.</li>
<li>It can be estimated online using <strong>temporal difference learning</strong> and generalized to function approximation using <strong>successor features</strong>.</li>
</ul>
<p>The SR allows for <strong>Generalized Policy Evaluation (GPE)</strong>: once the SR is learned, an agent can immediately evaluate its performance under any reward function that can be expressed as a linear combination of the features used to define the SR.</p>
<p>The SR offers a powerful tool for discovering and using temporal abstractions in reinforcement learning, enabling the development of more intelligent and efficient agents. It is used in option discovery methods like eigenoptions and covering options, providing a natural framework for identifying and leveraging temporally extended courses of actions.</p>
<p>Here is a breakdown of the mathematical definition of the SR:</p>
<p><span id="eq-SR"><img src="https://latex.codecogs.com/png.latex?%0A%CE%A8_%5Cpi%20(s,%20s')%20=%20%20%5Cmathbb%7BE%7D_%7B%CF%80,p%7D%20%5B%5Csum%5E%5Cinfty_%7Bt=0%7D%20%CE%B3%5Et%5Cmathbb%7B1%7D_%7BS_t%20=%20s'%7D%20%7C%20S_0%20=%20s%20%5D%20%5Cqquad%0A%5Ctag%7B5%7D"></span></p>
<ul>
<li>Where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?s,%20s'"> are states in the environment.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor, determining the weight of future rewards.</li>
<li>The <strong>expectation (E)</strong> is taken over the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and the transition probability kernel <img src="https://latex.codecogs.com/png.latex?p">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7B1%7D_%7BS_t%20=%20s'%7D"> is an indicator function that equals 1 if the agent is in state s’ at time <img src="https://latex.codecogs.com/png.latex?t">, and 0 otherwise.</li>
</ul></li>
</ul>
<p>This equation calculates the expected discounted number of times the agent will visit state s’ in the future, given that it starts in state s and follows policy π. The SR matrix stores these expected visitations for all state pairs.</p>
<ol start="2" type="1">
<li>Explain what is an eigenoption a covering option and the difference</li>
</ol>
<p><strong>Eigenoptions</strong> and <strong>covering options</strong> are two methods for option discovery in RL that use the successor representation (SR). Options represent temporally extended courses of actions.</p>
<p><strong>Eigenoptions</strong> are options defined by the eigenvectors of the SR.</p>
<ul>
<li>Each eigenvector of the SR assigns an intrinsic reward to every state in the environment.</li>
<li>An eigenoption aims to reach the state with the highest (or lowest) value in the corresponding eigenvector.</li>
<li>They encourage exploration by driving the agent toward states that are difficult to reach under a random walk.</li>
<li>Eigenoptions have a broad initiation set, meaning they can be initiated from many states.</li>
<li>They terminate when the agent reaches a state with a (locally) maximum value in the eigenvector, meaning the agent can’t accumulate more positive intrinsic reward.</li>
<li>Eigenoptions tend to have different durations based on the eigenvalue they are derived from, allowing the agent to operate at different timescales.</li>
</ul>
<p><strong>Covering options</strong> are defined by the bottom eigenvector of the graph Laplacian, which is equivalent to the top eigenvector of the SR under certain conditions.</p>
<ul>
<li>They aim to minimize the environment’s expected cover time, which is the number of steps needed for a random walk to visit every state.</li>
<li>Each covering option connects two specific states: one with the lowest value and one with the highest value in the corresponding eigenvector.</li>
<li>They are discovered iteratively. After each option is discovered, the environment’s graph is updated, and the process repeats.</li>
<li>Covering options have a restrictive initiation set, containing only the single state with the lowest value in the eigenvector.</li>
<li>They terminate when they reach the state with the highest value in the eigenvector.</li>
</ul>
<p>Here’s a table summarizing the <strong>key differences</strong> between eigenoptions and covering options:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 37%">
<col style="width: 53%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Eigenoption</th>
<th style="text-align: left;">Covering Option</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Definition</strong></td>
<td style="text-align: left;">Based on any eigenvector of the SR</td>
<td style="text-align: left;">Based on the bottom eigenvector of the graph Laplacian (equivalent to the top eigenvector of the SR under certain conditions)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Goal</strong></td>
<td style="text-align: left;">Reach states with high/low values in the corresponding eigenvector</td>
<td style="text-align: left;">Minimize environment’s cover time</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Initiation Set</strong></td>
<td style="text-align: left;">Broad (many states)</td>
<td style="text-align: left;">Restrictive (single state)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Termination Condition</strong></td>
<td style="text-align: left;">Reaching a (local) maximum in the eigenvector</td>
<td style="text-align: left;">Reaching the state with the highest value in the eigenvector</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Discovery Process</strong></td>
<td style="text-align: left;">Can be discovered in parallel, in a single iteration</td>
<td style="text-align: left;">Discovered iteratively, one option at a time</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Timescale</strong></td>
<td style="text-align: left;">Different eigenoptions can have different durations</td>
<td style="text-align: left;">Generally have similar durations</td>
</tr>
</tbody>
</table>
<p>Both eigenoptions and covering options can be effective for exploration, but they have different strengths and weaknesses. Eigenoptions can learn more diverse behaviors and capture different timescales, while covering options may be simpler to implement and can guarantee improvement in the environment’s cover time.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>There are a few blog posts that dive deeper into some of the concepts in the paper.</p>
<ul>
<li><a href="https://medium.com/@marlos.cholodovskis/the-representation-driven-option-discovery-cycle-e3f5877696c2">The Representation-driven Option Discovery</a></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Temporal {Abstraction} in {Reinforcement} {Learning} with the
    {Successor} {Representation}},
  url = {https://orenbochman.github.io/reviews/2023/temporal-abstraction/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Temporal Abstraction in Reinforcement
Learning with the Successor Representation.”</span> <a href="https://orenbochman.github.io/reviews/2023/temporal-abstraction/">https://orenbochman.github.io/reviews/2023/temporal-abstraction/</a>.
</div></div></section></div> ]]></description>
  <category>review</category>
  <category>Reinforcement learning</category>
  <category>podcast</category>
  <guid>https://orenbochman.github.io/reviews/2023/temporal-abstraction/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2023/temporal-abstraction/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>TheoremLlama An End-To-End Framework to Train a General-Purpose Large Language Model to Become a Lean4 Expert</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/theorem-llama/</link>
  <description><![CDATA[ 






<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. Similar methods have shown promising results in code generation. However, most modern LLMs exhibit suboptimal performance due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data. This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address the challenges, this paper proposes TheoremLlama, an end-to-end framework to train a general-purpose LLM to become a Lean4 expert. This framework encompasses NL-FL aligned dataset generation methods, training approaches for the LLM formal theorem prover, and techniques for LLM Lean4 proof writing. Using the dataset generation method, we provide <a href="https://huggingface.co/datasets/RickyDeSkywalker/OpenBootstrappedTheorem">Open Bootstrapped Theorems</a> (OBT), an NL-FL aligned and bootstrapped dataset. A key innovation in this framework is the NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leveraging the NL reasoning ability of LLMs for formal reasoning. The TheoremLlama framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. We have also open-sourced our model checkpoints and generated dataset1, and will soon make all the code publicly available2.</p>
</blockquote>
</section>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p><span class="citation" data-cites="wang2024theoremllamatransforminggeneralpurposellms">(Wang et al. 2024)</span> the authors present an end-to-end framework to train a Large Language Model (LLM) to become a <a href="https://github.com/leanprover/lean4">Lean4</a> expert. <sup>1</sup> <mark>The framework includes NL-FL aligned dataset generation methods, training approaches for the LLM formal theorem prover, and techniques for LLM Lean4 proof writing.</mark> The authors demonstrate the effectiveness of TheoremLlama by achieving cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%.<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="ref-wang2024theoremllamatransforminggeneralpurposellms" class="csl-entry">
Wang, Ruida, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, and Tong Zhang. 2024. <span>“TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts.”</span> <a href="https://arxiv.org/abs/2407.03203">https://arxiv.org/abs/2407.03203</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;Note: lean has a serious learning curve and is used in many current mathematics research projects.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;But is this sufficient to make a difference in the real world? GPT-4 isn’t expected to do well on lean…. </p></div></div><p>TheoremLlama is a significant step towards using LLMs’ natural language abilities to formalize theorem proving in Lean4, improving mathematical reasoning, and tackling major issues with data alignment and training approaches.</p>
<p>However, <mark>the lack of aligned NL and Formal Language (FL) theorem-proving data frequently makes it difficult for contemporary LLMs to operate efficiently.</mark> The lack of available resources impedes the advancement of efficient training approaches and strategies to fully utilize LLMs’ potential in creating formal mathematical proofs. In order to overcome these limitations, a team of researchers from The Hong Kong University of Science and Technology and the University of Illinois Urban-Champagin has introduced TheoremLlama, an end-to-end framework created to specialize a general-purpose LLM in Lean4 theorem proving.</p>
<p>TheoremLlama is made up of various important parts, which are as follows:</p>
<ul>
<li><p>NL-FL Aligned Dataset Generation: TheoremLlama presents techniques for creating an NL-FL-aligned dataset in order to get over data shortage. This dataset, called Open Bootstrapped Theorems (OBT), uses a bootstrapping technique to include NL proofs into Lean4 code. By integrating NL reasoning into Lean4 scenarios, the framework improves LLMs’ comprehension and execution of formal reasoning.</p></li>
<li><p>Formal Training for LLM Theorem Provers: The system applies new training strategies to help LLMs become successful Lean4 theorem provers. Methods like block training and curriculum data sorting have been utilized to enhance the LLM’s in-context learning and guarantee reliable training on the OBT dataset. LLM Lean4 Proof Writing: This part is about improving the LLM’s capacity to write formal proofs in Lean4 on its own. The LLM refines its formal reasoning abilities iteratively by using correctly generated proofs as examples.</p></li>
<li><p>TheoremLlama’s NL-FL bootstrapping approach is a significant invention that enables efficient training by coordinating natural language reasoning with formal mathematical language constraints. The framework’s efficiency has been demonstrated by experimental findings, which on the MiniF2F-Valid and Test datasets, respectively, yielded cumulative accuracies of 36.48% and 33.61%. These outcomes outperformed GPT-4’s baseline findings, which on the same datasets yielded accuracies of 22.95% and 25.41%.</p></li>
</ul>
<p>In conclusion, TheoremLlama is an important step towards using LLMs’ natural language abilities to formalize theorem proving in Lean4, improving mathematical reasoning, and tackling major issues with data alignment and training approaches</p>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also:</h2>
<ul>
<li><a href="https://arxiv.org/abs/2407.03203">paper</a></li>
<li>https://www.marktechpost.com/2024/07/10/theoremllama-an-end-to-end-framework-to-train-a-general-purpose-large-language-model-to-become-a-lean4-expert/</li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {TheoremLlama {An} {End-To-End} {Framework} to {Train} a
    {General-Purpose} {Large} {Language} {Model} to {Become} a {Lean4}
    {Expert}},
  url = {https://orenbochman.github.io/reviews/2024/theorem-llama/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“TheoremLlama An End-To-End Framework to Train
a General-Purpose Large Language Model to Become a Lean4 Expert.”</span>
<a href="https://orenbochman.github.io/reviews/2024/theorem-llama/">https://orenbochman.github.io/reviews/2024/theorem-llama/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/theorem-llama/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2024/theorem-llama/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/tree-attention/</link>
  <description><![CDATA[ 






<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>in <span class="citation" data-cites="shyam2024treeattn">(Shyam et al. 2024)</span> the authors propose a new algorithm for parallelizing attention computation across multiple GPUs. This enables cross-device decoding to be performed asymptotically faster (up to 8 x faster in our experiments) than alternative approaches such as Ring Attention, while also requiring significantly less communication volume and incurring 2 x less peak memory.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p><mark>Self-attention is the core mathematical operation of modern transformer architectures and is also a significant computational bottleneck due to its quadratic complexity in the sequence length</mark>. In this work, we derive the scalar energy function whose gradient computes the self-attention block, thus elucidating the theoretical underpinnings of self-attention, providing a Bayesian interpretation of the operation and linking it closely with energy-based models such as Hopfield Networks. Our formulation reveals that the reduction across the sequence axis can be efficiently computed in parallel through a tree reduction. Our algorithm, for parallelizing attention computation across multiple GPUs enables cross-device decoding to be performed asymptotically faster (up to 8× faster in our experiments) than alternative approaches such as Ring Attention, while also requiring significantly less communication volume and incurring 2 x less peak memory.</p>
<p>— <span class="citation" data-cites="shyam2024treeattn">(Shyam et al. 2024)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-shyam2024treeattn" class="csl-entry">
Shyam, Vasudev, Jonathan Pilault, Emily Shepperd, Quentin Anthony, and Beren Millidge. 2024. <span>“Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters.”</span> <a href="https://arxiv.org/abs/2408.04093">https://arxiv.org/abs/2408.04093</a>.
</div></div></div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p><a href="https://github.com/Zyphra/tree_attention">code</a></p>
</section>
<section id="softmax" class="level2 callout-info">
<h2 class="anchored" data-anchor-id="softmax">Softmax</h2>
<p>The softmax operation can be derived as the gradient of the following scalar function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdelta%20z_i%20%5Csum_%7Ba=1%7D%5En%20exp(z_a)%20=%20%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum%5En_%7Ba=1%7D%20e%5E%7Bz_j%7D%7D%20=%20%5Ctext%7Bsoftmax%7D(z_j)%0A"></p>
</section>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Tree {Attention:} {Topology-Aware} {Decoding} for
    {Long-Context} {Attention} on {GPU} {Clusters}},
  url = {https://orenbochman.github.io/reviews/2024/tree-attention/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Tree Attention: Topology-Aware Decoding for
Long-Context Attention on GPU Clusters.”</span> <a href="https://orenbochman.github.io/reviews/2024/tree-attention/">https://orenbochman.github.io/reviews/2024/tree-attention/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/tree-attention/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
</item>
<item>
  <title>ViT — An Image is worth 16x16 words: Transformers for Image Recognition at scale</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2020/ViT/</link>
  <description><![CDATA[ 






<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</p>
<p>— <span class="citation" data-cites="DBLP:journals/corr/abs-2010-11929">(Dosovitskiy et al. 2020)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-DBLP:journals/corr/abs-2010-11929" class="csl-entry">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>CoRR</em> abs/2010.11929. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div></div></div>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also</h2>
<ul>
<li><a href="https://arxiv.org/abs/2010.11929">Paper</a></li>
<li><a href="https://github.com/google-research/vision_transformer">Code - Vision Transformer and MLP-Mixer Architectures</a></li>
<li><a href="https://iclr.cc/virtual/2021/oral/3458">ICLR - Video &amp; Slides</a></li>
<li><a href="https://research.google/pubs/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/">Blog post</a></li>
<li>Third-party reviews:
<ul>
<li><a href="https://www.youtube.com/@YannicKilcher">Review by Yannic Kilcher</a></li>
<li><a href="https://www.youtube.com/watch?v=aD-D8-D-ZyY">Sahil Khose</a></li>
<li><a href="https://www.youtube.com/watch?v=DVoHvmww2lQ">AI Coffee Break with Letitia</a></li>
<li><a href="https://medium.com/@ManishChablani/vision-transformer-vit-an-image-is-worth-16x16-words-transformers-for-image-recognition-at-a4bd5c6f17a7">Manish Chablani — Review</a></li>
</ul></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {ViT -\/-\/- {An} {Image} Is Worth 16x16 Words: {Transformers}
    for {Image} {Recognition} at Scale},
  url = {https://orenbochman.github.io/reviews/2020/ViT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“ViT --- An Image Is Worth 16x16 Words:
Transformers for Image Recognition at Scale.”</span> <a href="https://orenbochman.github.io/reviews/2020/ViT/">https://orenbochman.github.io/reviews/2020/ViT/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2020/ViT/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2020/ViT/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>title</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/</link>
  <description><![CDATA[ 







<!-- VIDEOS GO HERE 

::: {.column-margin #fig-subtasks}


Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
:::

-->
<!-- A QUOTE by someone more famous than the author of the paper for context, add highlighting for emphasis, verse is a nice touch! 

> "The ideal market completely disregards those spikes—but a realistic model cannot." [Mandelbrot highlights the inadequacy of models ignoring extreme price movements, emphasizing the need for a framework that can accommodate them.]{.mark}

-->
<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/cover.webp" class="nolightbox img-fluid figure-img" width="250"></p>
<figcaption>cover</figcaption>
</figure>
</div></div><p>This is the follow up book to All of Statistics by Wasserman. This one covers non-parametric methods. Though the first book also covered a number of non-parametric methods like wavelets, this book goes into more detail on the subject. It too is a bit dated as it was written before the deep learning revolution which falls under the non-parametric methods. However, it is still a good reference for the basics of non-parametric statistics.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>TL;DR - Too Long; Didn’t Read about XXX <!-- Short Catchy title -->
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="XXX in a nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="XXX in a nutshell"></a></p>
<figcaption>XXX in a nutshell</figcaption>
</figure>
</div>
<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. <!-- SHORT & OPINIONATED--></p>
<!-- 1. What are the research questions? -->
<!-- 2. What are the main findings? -->
<!-- 3. In historical context why was this important? -->
</div>
</div>
<p>Here is a light hearted Deep Dive into the book:</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This book uses lots of big terms so let’s break them down so we can understand them better</p>
<dl>
<dt><img src="https://latex.codecogs.com/png.latex?%5Csigma">-field</dt>
<dd>
A class of events <img src="https://latex.codecogs.com/png.latex?A"> such that (i) <img src="https://latex.codecogs.com/png.latex?%5Cemptyset%20%5Cin%20A">, (ii) <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20A"> implies that <img src="https://latex.codecogs.com/png.latex?A%5Ec%20%5Cin%20A"> and (iii) <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20%5Cldots%20%5Cin%20A"> implies that <img src="https://latex.codecogs.com/png.latex?%5Cbigcup_%7Bi=1%7D%5E%5Cinfty%20A_i%20%5Cin%20A">.
</dd>
<dt>Probability measure</dt>
<dd>
A function <img src="https://latex.codecogs.com/png.latex?P"> defined on a <img src="https://latex.codecogs.com/png.latex?%5Csigma">-field <img src="https://latex.codecogs.com/png.latex?A"> such that <img src="https://latex.codecogs.com/png.latex?P(A)%20%5Cgeq%200"> for all <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20A">, <img src="https://latex.codecogs.com/png.latex?P(%5COmega)%20=%201"> and if <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20%5Cldots%20%5Cin%20A"> are disjoint then <img src="https://latex.codecogs.com/png.latex?P%5Cleft(%5Cbigcup_%7Bi=1%7D%5E%5Cinfty%20A_i%5Cright)%20=%20%5Csum_%7Bi=1%7D%5E%5Cinfty%20P(A_i)">.
</dd>
<dt>Probability space</dt>
<dd>
The triple <img src="https://latex.codecogs.com/png.latex?(%5COmega,%20A,%20P)"> where <img src="https://latex.codecogs.com/png.latex?%5COmega"> is the sample space, <img src="https://latex.codecogs.com/png.latex?A"> is a <img src="https://latex.codecogs.com/png.latex?%5Csigma">-field of events, and <img src="https://latex.codecogs.com/png.latex?P"> is a probability measure.
</dd>
<dt>Random variable</dt>
<dd>
A map <img src="https://latex.codecogs.com/png.latex?X%20:%20%5COmega%20%5Cto%20%5Cmathbb%7BR%7D"> such that, for every real <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Comega%20%5Cin%20%5COmega%20:%20X(%5Comega)%20%5Cleq%20x%5C%7D%20%5Cin%20A">.
</dd>
<dt>Mean (of a function <img src="https://latex.codecogs.com/png.latex?a">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?E(a(X))%20=%20%5Cint%20a(x)%20dF(x)%20%5Cequiv%20%5Cleft%5C%7B%20%5Cint%20a(x)%20f(x)%20dx%20%5Ctext%7B%20in%20the%20continuous%20case,%20%7D%20%5Csum_j%20a(x_j)%20f(x_j)%20%5Ctext%7B%20in%20the%20discrete%20case%7D%20%5Cright%5C%7D">.
</dd>
<dt>Variance (of a random variable <img src="https://latex.codecogs.com/png.latex?X">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?V(X)%20=%20E((X%20-%20E(X))%5E2)">.
</dd>
<dt>Mean squared error (mse)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bmse%7D%20=%20%5Ctext%7Bbias%7D%5E2(%5Chat%7B%5Ctheta%7D_n)%20+%20V(%5Chat%7B%5Ctheta%7D_n)">. Also defined as <img src="https://latex.codecogs.com/png.latex?R(f(x),%20%5Chat%7Bf%7D_n(x))%20=%20E%20(%20L(f(x),%20%5Chat%7Bf%7D_n(x))%20)">.
</dd>
<dt>Confidence set</dt>
<dd>
A set <img src="https://latex.codecogs.com/png.latex?C_n"> of possible values of a quantity of interest <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, which depends on the data <img src="https://latex.codecogs.com/png.latex?X_1,%20%5Cldots,%20X_n">.
</dd>
<dt>Plug-in estimator (of <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20T(F)">)</dt>
<dd>
Defined by <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_n%20=%20T(%5Chat%7BF%7D_n)">.
</dd>
<dt>Linear functional (of <img src="https://latex.codecogs.com/png.latex?F">)</dt>
<dd>
A functional of the form <img src="https://latex.codecogs.com/png.latex?%5Cint%20a(x)%20dF(x)">.
</dd>
<dt>Influence function (<img src="https://latex.codecogs.com/png.latex?L_F(x)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?L_F(x)%20=%20%5Clim_%7B%5Cepsilon%20%5Cto%200%7D%20%5Cfrac%7BT((1%20-%20%5Cepsilon)F%20+%20%5Cepsilon%20%5Cdelta_x)%20-%20T(F)%7D%7B%5Cepsilon%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Cdelta_x"> is a point mass at <img src="https://latex.codecogs.com/png.latex?x">.
</dd>
<dt>Empirical influence function (<img src="https://latex.codecogs.com/png.latex?%5Chat%7BL%7D(x)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BL%7D(x)%20=%20L_%7B%5Chat%7BF%7D_n%7D(x)">.
</dd>
<dt>Nonparametric delta method</dt>
<dd>
The approximation <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BT(%5Chat%7BF%7D_n)%20-%20T(F)%7D%7B%5Chat%7Bs%7D_e%7D%20%5Capprox%20N(0,%201)"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bs%7D_e%20=%20%5Chat%7B%5Ctau%7D/%5Csqrt%7Bn%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D%5E2%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20%5Chat%7BL%7D%5E2(X_i)">.
</dd>
<dt>Empirical probability distribution (<img src="https://latex.codecogs.com/png.latex?%5Chat%7BP%7D_n(A)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BP%7D_n(A)%20=%20%5Cfrac%7B%5Ctext%7Bnumber%20of%20%7D%20X_i%20%5Cin%20A%7D%7Bn%7D">.
</dd>
<dt>Gâteaux derivative (of <img src="https://latex.codecogs.com/png.latex?T"> at <img src="https://latex.codecogs.com/png.latex?F"> in the direction <img src="https://latex.codecogs.com/png.latex?G">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?L_F(G)%20=%20%5Clim_%7B%5Cepsilon%20%5Cto%200%7D%20%5Cfrac%7BT((1%20-%20%5Cepsilon)F%20+%20%5Cepsilon%20G)%20-%20T(F)%7D%7B%5Cepsilon%7D">.
</dd>
<dt>Hadamard differentiable (at <img src="https://latex.codecogs.com/png.latex?F">)</dt>
<dd>
There exists a linear functional <img src="https://latex.codecogs.com/png.latex?L_F"> on <img src="https://latex.codecogs.com/png.latex?D"> such that for any <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_n%20%5Cto%200"> and <img src="https://latex.codecogs.com/png.latex?%5C%7BD,%20D_1,%20D_2,%20%5Cldots%5C%7D%20%5Csubset%20D"> such that <img src="https://latex.codecogs.com/png.latex?d(D_n,%20D)%20%5Cto%200"> and <img src="https://latex.codecogs.com/png.latex?F%20+%20%5Cepsilon_n%20D_n%20%5Cin%20%5Cmathcal%7BF%7D">, <img src="https://latex.codecogs.com/png.latex?%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20%5Cleft(%5Cfrac%7BT(F%20+%20%5Cepsilon_n%20D_n)%20-%20T(F)%7D%7B%5Cepsilon_n%7D%20-%20L_F(D_n)%5Cright)%20=%200">.
</dd>
<dt>Linear smoother (estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n"> of <img src="https://latex.codecogs.com/png.latex?r">)</dt>
<dd>
For each <img src="https://latex.codecogs.com/png.latex?x">, there exists a vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bl%7D(x)%20=%20(l_1(x),%20%5Cldots,%20l_n(x))%5ET"> such that <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)%20=%20%5Csum_%7Bi=1%7D%5En%20l_i(x)%20Y_i">.
</dd>
<dt>Leave-one-out cross-validation score (<img src="https://latex.codecogs.com/png.latex?cv%20=%20%5Chat%7BR%7D(h)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(h)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(Y_i%20-%20%5Chat%7Br%7D%5E%7B(-i)%7D(x_i))%5E2"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D%5E%7B(-i)%7D"> is the estimator obtained by omitting the <img src="https://latex.codecogs.com/png.latex?i">th pair <img src="https://latex.codecogs.com/png.latex?(x_i,%20Y_i)">.
</dd>
<dt>Histogram estimator (<img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)%20=%20m%20%5Csum_%7Bj=1%7D%20%5Cfrac%7B%5Chat%7Bp%7D_j%7D%7Bh%7D%20I(x%20%5Cin%20B_j)"> where <img src="https://latex.codecogs.com/png.latex?h=1/m"> is the binwidth, <img src="https://latex.codecogs.com/png.latex?B_j"> are the bins, and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D_j"> is the proportion of observations in bin <img src="https://latex.codecogs.com/png.latex?B_j">.
</dd>
<dt>Cross-validation estimator of risk (<img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Cint%20%5Chat%7Bf%7D_n%5E2(x)%20dx%20-%20%5Cfrac%7B2%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20%5Chat%7Bf%7D_n%5E%7B(-i)%7D(X_i)">.
</dd>
<dt>Sobolev space of order <img src="https://latex.codecogs.com/png.latex?m"> (<img src="https://latex.codecogs.com/png.latex?W%5E%7B(m)%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5C%7B%20f%20%5Cin%20L_2(0,%201)%20:%20D%5Em%20f%20%5Cin%20L_2(0,%201)%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?D%5Em%20f"> is the <img src="https://latex.codecogs.com/png.latex?m">th weak derivative of <img src="https://latex.codecogs.com/png.latex?f">.
</dd>
<dt>Sobolev space of order <img src="https://latex.codecogs.com/png.latex?m"> and radius <img src="https://latex.codecogs.com/png.latex?c"> (<img src="https://latex.codecogs.com/png.latex?W%5E%7B(m,%20c)%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5C%7B%20f%20:%20f%20%5Cin%20W%5E%7B(m)%7D,%20%5C%7CD%5Em%20f%20%5C%7C_2%20%5Cleq%20c%5E2%20%5C%7D">.
</dd>
<dt>Periodic Sobolev class (<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BW%7D%5E%7B(m,%20c)%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5C%7B%20f%20%5Cin%20W%5E%7B(m,%20c)%7D%20:%20D%5Ej%20f(0)%20=%20D%5Ej%20f(1),%20j%20=%200,%20%5Cldots,%20m%20-%201%20%5C%7D">.
</dd>
<dt>Ellipsoid (<img src="https://latex.codecogs.com/png.latex?%5CTheta">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5C%7B%20%5Ctheta%20:%20%5Csum_%7Bj=1%7D%5E%5Cinfty%20a_j%5E2%20%5Ctheta_j%5E2%20%5Cleq%20c%5E2%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?a_j"> is a sequence of numbers such that <img src="https://latex.codecogs.com/png.latex?a_j%20%5Cto%20%5Cinfty"> as <img src="https://latex.codecogs.com/png.latex?j%20%5Cto%20%5Cinfty">.
</dd>
<dt>Minimax risk</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Cinf_%7B%5Chat%7B%5Ctheta%7D_n%7D%20%5Csup_%7B%5Ctheta%20%5Cin%20%5CTheta%7D%20E_%5Ctheta%20%5BL(%5Chat%7B%5Ctheta%7D_n,%20%5Ctheta)%5D"> where the infimum is over all estimators <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_n"> and the supremum is over a class of parameters <img src="https://latex.codecogs.com/png.latex?%5CTheta">.
</dd>
<dt>Linear shrinkage estimator (in Normal means problem)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D%20=%20bZ%20=%20(bZ_1,%20%5Cldots,%20b_n%20Z_n)">.
</dd>
<dt>Soft threshold estimator</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_i%20=%20%5Ctext%7Bsign%7D(Z_i)(%7CZ_i%7C%20-%20%5Clambda)_+">.
</dd>
<dt>Hard threshold estimator</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_i%20=%20Z_i%20I(%7CZ_i%7C%20%3E%20%5Clambda)">.
</dd>
<dt>Multiresolution analysis (MRA) of <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> (generated by <img src="https://latex.codecogs.com/png.latex?%5Cphi">)</dt>
<dd>
A sequence of closed subspaces <img src="https://latex.codecogs.com/png.latex?V_j%20%5Csubset%20L_2(%5Cmathbb%7BR%7D)">, <img src="https://latex.codecogs.com/png.latex?j%20%5Cin%20%5Cmathbb%7BZ%7D">, such that <img src="https://latex.codecogs.com/png.latex?V_j%20%5Csubset%20V_%7Bj+1%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cbigcup_%7Bj%20%5Cin%20%5Cmathbb%7BZ%7D%7D%20V_j"> is dense in <img src="https://latex.codecogs.com/png.latex?L_2(%5Cmathbb%7BR%7D)">, <img src="https://latex.codecogs.com/png.latex?%5Cbigcap_%7Bj%20%5Cin%20%5Cmathbb%7BZ%7D%7D%20V_j%20=%20%5C%7B0%5C%7D">, <img src="https://latex.codecogs.com/png.latex?f(x)%20%5Cin%20V_j%20%5CLeftrightarrow%20f(2x)%20%5Cin%20V_%7Bj+1%7D">, and there exists a scaling function (father wavelet) <img src="https://latex.codecogs.com/png.latex?%5Cphi%20%5Cin%20V_0"> such that <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Cphi(x%20-%20k)%20:%20k%20%5Cin%20%5Cmathbb%7BZ%7D%5C%7D"> forms a Riesz basis for <img src="https://latex.codecogs.com/png.latex?V_0">.
</dd>
<dt>Father wavelet (scaling function) (<img src="https://latex.codecogs.com/png.latex?%5Cphi">)</dt>
<dd>
A function in <img src="https://latex.codecogs.com/png.latex?V_0"> that generates the basis for <img src="https://latex.codecogs.com/png.latex?V_0"> in a multiresolution analysis.
</dd>
<dt>Mother wavelet (<img src="https://latex.codecogs.com/png.latex?%5Cpsi">)</dt>
<dd>
A function such that <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Cpsi_%7Bjk%7D(x)%20=%202%5E%7Bj/2%7D%5Cpsi(2%5Ej%20x%20-%20k)%20:%20j,%20k%20%5Cin%20%5Cmathbb%7BZ%7D%5C%7D"> forms an orthonormal basis for <img src="https://latex.codecogs.com/png.latex?L_2(%5Cmathbb%7BR%7D)">.
</dd>
<dt>Wavelet regression</dt>
<dd>
A method to estimate a regression function by projecting the data onto a wavelet basis and then shrinking the wavelet coefficients.
</dd>
<dt>Thresholding</dt>
<dd>
A nonlinear shrinkage method used in wavelet regression where small wavelet coefficients are set to zero.
</dd>
<dt>Hard thresholding (estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7Bjk%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7Bjk%7D%20=%20%5Cleft%5C%7B%20%5Cbegin%7Barray%7D%7Bll%7D%200%20&amp;%20%5Ctext%7Bif%20%7D%20%7CD_%7Bjk%7D%7C%20%3C%20%5Clambda%20%5C%5C%20D_%7Bjk%7D%20&amp;%20%5Ctext%7Bif%20%7D%20%7CD_%7Bjk%7D%7C%20%5Cgeq%20%5Clambda%20%5Cend%7Barray%7D%20%5Cright.">.
</dd>
<dt>Soft thresholding (estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7Bjk%7D">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7Bjk%7D%20=%20%5Ctext%7Bsign%7D(D_%7Bjk%7D)(%7CD_%7Bjk%7D%7C%20-%20%5Clambda)_+">.
</dd>
<dt>Universal threshold (<img src="https://latex.codecogs.com/png.latex?%5Clambda">)</dt>
<dd>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D%20%5Csqrt%7B%5Cfrac%7B2%20%5Clog%20n%7D%7Bn%7D%7D">.
</dd>
<dt>Besov space (<img src="https://latex.codecogs.com/png.latex?B_%7Bp,q%7D%5E%5Cxi">)</dt>
<dd>
A function space that generalizes Sobolev spaces and characterizes smoothness in terms of differences.
</dd>
<dt>Overcomplete dictionary</dt>
<dd>
A set of basis functions where the number of basis functions <img src="https://latex.codecogs.com/png.latex?m"> is greater than the number of observations <img src="https://latex.codecogs.com/png.latex?n">.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ol type="1">
<li><strong>Introduction</strong>
<ul>
<li>1.1 <strong>What Is Nonparametric Inference?</strong>
<ul>
<li>Basic idea: Use data to infer an unknown quantity with <strong>minimal assumptions</strong>.</li>
<li>Often involves <strong>infinite-dimensional statistical models</strong>.</li>
<li>The term ‘nonparametric inference’ in this book refers to modern statistical methods that aim to keep the number of underlying assumptions as weak as possible.</li>
<li>Problems considered in the book include:
<ul>
<li>Estimating the <strong>distribution function (cdf)</strong>: Given an iid sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n%20%5Csim%20F">, estimate <img src="https://latex.codecogs.com/png.latex?F(x)%20=%20P(X%20%E2%89%A4%20x)">.</li>
<li>Estimating <strong>functionals</strong>: Given an iid sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n%20%5Csim%20F">, estimate a functional <img src="https://latex.codecogs.com/png.latex?T(F)"> such as the mean <img src="https://latex.codecogs.com/png.latex?T(F%20)%20=%20%E2%88%AB%20xdF%20(x)">.</li>
<li><strong>Density estimation</strong>: Given an iid sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n%20%5Csim%20F">, estimate the density <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20F%20%E2%80%B2(x)">.</li>
<li><strong>Nonparametric regression or curve estimation</strong>: Given <img src="https://latex.codecogs.com/png.latex?(X_1,%20Y_1),%20.%20.%20.%20,%20(X_n,%20Yn)"> estimate the regression function <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20E(Y%20%7CX%20=%20x)">.</li>
<li><strong>Normal means</strong>: Given <img src="https://latex.codecogs.com/png.latex?Y_i%20%5Csim%20N(%CE%B8_i,%20%CF%83%5E2)">, <img src="https://latex.codecogs.com/png.latex?i%20=%201,%20.%20.%20.%20,%20n">, estimate <img src="https://latex.codecogs.com/png.latex?%CE%B8%20=%20(%CE%B8_1,%20.%20.%20.%20,%20%CE%B8_n)">.</li>
</ul></li>
<li>Typically assumes the unknown quantity (distribution <img src="https://latex.codecogs.com/png.latex?F">, density <img src="https://latex.codecogs.com/png.latex?f">, or regression function <img src="https://latex.codecogs.com/png.latex?r">) lies in some <strong>large set <img src="https://latex.codecogs.com/png.latex?F"> called a statistical model</strong>. For example, when estimating a density <img src="https://latex.codecogs.com/png.latex?f">, we might assume <img src="https://latex.codecogs.com/png.latex?f%20%E2%88%88%20F%20=%20%5C%7B%20g%20:%20%E2%88%AB%20(g%E2%80%B2%E2%80%B2(x))%5E2dx%20%E2%89%A4%20c%5E2%20%5C%7D">.</li>
</ul></li>
<li>1.2 <strong>Notation and Background</strong>
<ul>
<li>Summary of some useful notation (see also Table 1.1).</li>
<li>Definition of <strong>mean of <img src="https://latex.codecogs.com/png.latex?a"></strong>: <img src="https://latex.codecogs.com/png.latex?E(a(X))%20=%20%E2%88%AB%20a(x)dF%20(x)%20%E2%89%A1%20%5C%7B%20%E2%88%AB%20a(x)f(x)dx"> continuous case, <img src="https://latex.codecogs.com/png.latex?%E2%88%91_j%20a(x_j)f(x_j)"> discrete case.</li>
<li>Definition of <strong>variance of <img src="https://latex.codecogs.com/png.latex?X"></strong>: <img src="https://latex.codecogs.com/png.latex?V(X)%20=%20E(X%20%E2%88%92%20E(X))%5E2">.</li>
<li>Brief review of probability.
<ul>
<li><strong>Sample space <img src="https://latex.codecogs.com/png.latex?%CE%A9"></strong>: The set of possible outcomes of an experiment.</li>
<li><strong>Events and <img src="https://latex.codecogs.com/png.latex?%CF%83">-field <img src="https://latex.codecogs.com/png.latex?A"></strong>: A class of events <img src="https://latex.codecogs.com/png.latex?A"> is a <img src="https://latex.codecogs.com/png.latex?%CF%83">-field if (i) <img src="https://latex.codecogs.com/png.latex?%E2%88%85%20%E2%88%88%20A">, (ii) <img src="https://latex.codecogs.com/png.latex?A%20%E2%88%88%20A"> implies that <img src="https://latex.codecogs.com/png.latex?A%5Ec%20%E2%88%88%20A"> and (iii) <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20.%20.%20.%20%E2%88%88%20A"> implies that <img src="https://latex.codecogs.com/png.latex?%E2%8B%83_%7Bi=1%7D%5E%E2%88%9E%20A_i%20%E2%88%88%20A">.</li>
<li><strong>Probability measure <img src="https://latex.codecogs.com/png.latex?P"></strong>: A function <img src="https://latex.codecogs.com/png.latex?P"> defined on a <img src="https://latex.codecogs.com/png.latex?%CF%83">-field <img src="https://latex.codecogs.com/png.latex?A"> such that <img src="https://latex.codecogs.com/png.latex?P(A)%20%E2%89%A5%200"> for all <img src="https://latex.codecogs.com/png.latex?A%20%E2%88%88%20A">, <img src="https://latex.codecogs.com/png.latex?P(%CE%A9)%20=%201"> and if <img src="https://latex.codecogs.com/png.latex?A_1,%20A_2,%20.%20.%20.%20%E2%88%88%20A"> are disjoint then <img src="https://latex.codecogs.com/png.latex?P(%E2%8B%83_%7Bi=1%7D%5E%E2%88%9E%20A_i)%20=%20%E2%88%91_%7Bi=1%7D%5E%E2%88%9E%20P(A_i)">.</li>
<li><strong>Probability space <img src="https://latex.codecogs.com/png.latex?(%CE%A9,%20A,%20P)"></strong>: The triple consisting of the sample space, the <img src="https://latex.codecogs.com/png.latex?%CF%83">-field, and the probability measure.</li>
<li><strong>Random variable <img src="https://latex.codecogs.com/png.latex?X%20:%20%CE%A9%20%E2%86%92%20R"></strong>: A map <img src="https://latex.codecogs.com/png.latex?X%20:%20%CE%A9%20%E2%86%92%20R"> such that, for every real <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?%5C%7B%CF%89%20%E2%88%88%20%CE%A9%20:%20X(%CF%89)%20%E2%89%A4%20x%5C%7D%20%E2%88%88%20A">.</li>
<li><strong>Mean squared error (mse)</strong>: <img src="https://latex.codecogs.com/png.latex?mse%20=%20bias%5E2(%CE%B8%CC%82_n)%20+%20V(%CE%B8%CC%82_n)"> (1.10).</li>
</ul></li>
</ul></li>
<li>1.3 <strong>Confidence Sets</strong>
<ul>
<li>Much of nonparametric inference is devoted to finding an estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%CE%B8%7D_n"> of some quantity of interest <img src="https://latex.codecogs.com/png.latex?%CE%B8"> and providing <strong>confidence sets</strong> for these quantities.</li>
<li>Let <img src="https://latex.codecogs.com/png.latex?F"> be a class of distribution functions and let <img src="https://latex.codecogs.com/png.latex?%CE%B8"> be some quantity of interest. Let <img src="https://latex.codecogs.com/png.latex?C_n"> be a set of possible values of <img src="https://latex.codecogs.com/png.latex?%CE%B8"> which depends on the data <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n">. The coverage of <img src="https://latex.codecogs.com/png.latex?C_n"> is <img src="https://latex.codecogs.com/png.latex?P_F(%CE%B8%20%E2%88%88%20C_n)">. <img src="https://latex.codecogs.com/png.latex?C_n"> is a <img src="https://latex.codecogs.com/png.latex?1%20%E2%88%92%20%CE%B1"> confidence set if <img src="https://latex.codecogs.com/png.latex?%5Cinf_%7BF%7D%20P_F(%CE%B8%20%E2%88%88%20C_n)%20%E2%89%A5%201%20%E2%88%92%20%CE%B1">.</li>
</ul></li>
<li>1.4 <strong>Useful Inequalities</strong>
<ul>
<li><strong>Jensen’s inequality</strong>: If <img src="https://latex.codecogs.com/png.latex?g"> is convex then <img src="https://latex.codecogs.com/png.latex?Eg(X)%20%E2%89%A5%20g(EX)"> (1.32). If <img src="https://latex.codecogs.com/png.latex?g"> is concave then <img src="https://latex.codecogs.com/png.latex?Eg(X)%20%E2%89%A4%20g(EX)"> (1.33).</li>
</ul></li>
<li>1.5 <strong>Bibliographic Remarks</strong>
<ul>
<li>References on probability inequalities and their use in statistics and pattern recognition include Devroye et al.&nbsp;(1996) and van der Vaart and Wellner (1996). To review basic probability and mathematical statistics, I recommend Casella and Berger (2002), van der Vaart (1998) and Wasserman (2004).</li>
</ul></li>
</ul></li>
<li><strong>Estimating the cdf and Statistical Functionals</strong>
<ul>
<li>2.1 <strong>The cdf</strong>
<ul>
<li>Definition of the empirical cumulative distribution function (cdf) <img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_n(x)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20I(X_i%20%5Cle%20x)">.</li>
<li>Glivenko-Cantelli Theorem: <img src="https://latex.codecogs.com/png.latex?%5CVert%20%5Chat%7BF%7D_n%20-%20F%20%5CVert_%5Cinfty%20%5Cstackrel%7Ba.s.%7D%7B%5Clongrightarrow%7D%200">.</li>
<li>Dvoretzky-Kiefer-Wolfowitz (DKW) inequality: <img src="https://latex.codecogs.com/png.latex?P(%5CVert%20%5Chat%7BF%7D_n%20-%20F%20%5CVert_%5Cinfty%20%3E%20%5Cepsilon)%20%5Cle%202e%5E%7B-2n%5Cepsilon%5E2%7D"> for all <img src="https://latex.codecogs.com/png.latex?F"> and all <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%3E%200">.</li>
<li>Construction of a nonparametric <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> confidence band for <img src="https://latex.codecogs.com/png.latex?F(x)"> using the DKW inequality: <img src="https://latex.codecogs.com/png.latex?(L(x),%20U(x))"> where <img src="https://latex.codecogs.com/png.latex?L(x)%20=%20%5Cmax%5C%7B%5Chat%7BF%7D_n(x)%20-%20%5Cepsilon_n,%200%5C%7D"> and <img src="https://latex.codecogs.com/png.latex?U(x)%20=%20%5Cmin%5C%7B%5Chat%7BF%7D_n(x)%20+%20%5Cepsilon_n,%201%5C%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_n%20=%20%5Csqrt%7B%5Cfrac%7B1%7D%7B2n%7D%20%5Clog(%5Cfrac%7B2%7D%7B%5Calpha%7D)%7D">.</li>
<li>Theorem 2.6 summarizes the construction of the confidence band.</li>
<li>Example 2.7 illustrates a 95 percent confidence band.</li>
</ul></li>
<li>2.2 <strong>Estimating Statistical Functionals</strong>
<ul>
<li>Introduction to statistical functionals <img src="https://latex.codecogs.com/png.latex?T(F)"> which map a distribution function <img src="https://latex.codecogs.com/png.latex?F"> to a number.</li>
<li>Plug-in estimator: Estimate <img src="https://latex.codecogs.com/png.latex?T(F)"> by <img src="https://latex.codecogs.com/png.latex?T(%5Chat%7BF%7D_n)">.</li>
<li>Example 2.32 (The mean): Plug-in estimator for the mean <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20%5Cint%20xdF(x)"> is <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D%20=%20%5Cint%20xd%5Chat%7BF%7D_n(x)%20=%20%5Cbar%7BX%7D_n">. Asymptotic nonparametric 95 percent confidence interval for <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D_n%20%5Cpm%202%20%5Chat%7Bse%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bse%7D%5E2%20=%20%5Chat%7B%5Csigma%7D%5E2/n">.</li>
<li>Statistical functionals in the form <img src="https://latex.codecogs.com/png.latex?T(F)%20=%20a(T_1(F),%20.%20.%20.%20,%20T_m(F))">.</li>
</ul></li>
<li>2.3 <strong>Influence Functions</strong>
<ul>
<li>Definition of the influence function <img src="https://latex.codecogs.com/png.latex?L(x)"> of a functional <img src="https://latex.codecogs.com/png.latex?T"> at <img src="https://latex.codecogs.com/png.latex?F">.</li>
<li><img src="https://latex.codecogs.com/png.latex?T((1%20-%20%5Cepsilon)F%20+%20%5Cepsilon%20%5Cdelta_x)%20%5Capprox%20T(F)%20+%20%5Cepsilon%20L(x)"> for small <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">, where <img src="https://latex.codecogs.com/png.latex?%5Cdelta_x"> is a point mass at <img src="https://latex.codecogs.com/png.latex?x">.</li>
<li>Asymptotic variance of the plug-in estimator <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bn%7D(T(%5Chat%7BF%7D_n)%20-%20T(F))%20%5Cstackrel%7Bd%7D%7B%5Clongrightarrow%7D%20N(0,%20%5Csigma%5E2)"> where <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20=%20V_F(L(X_1))">.</li>
<li>Estimated standard error <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bse%7D%20=%20(%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20%5Chat%7BL%7D(X_i)%5E2)%5E%7B1/2%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Chat%7BL%7D"> is the estimated influence function.</li>
<li>Pointwise asymptotic <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> confidence interval <img src="https://latex.codecogs.com/png.latex?T(%5Chat%7BF%7D_n)%20%5Cpm%20z_%7B1-%5Calpha/2%7D%20%5Chat%7Bse%7D">.</li>
</ul></li>
<li>2.4 <strong>Empirical Probability Distributions</strong>
<ul>
<li>The empirical probability distribution <img src="https://latex.codecogs.com/png.latex?P_n"> assigns mass <img src="https://latex.codecogs.com/png.latex?1/n"> to each observation <img src="https://latex.codecogs.com/png.latex?X_i">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_n(x)%20=%20P_n((-%5Cinfty,%20x%5D)">.</li>
</ul></li>
<li>2.5 <strong>Bibliographic Remarks</strong>
<ul>
<li>Provides references for further reading on empirical processes, statistical functionals, and influence functions.</li>
</ul></li>
<li>2.6 <strong>Appendix</strong>
<ul>
<li>(Content of the appendix is not detailed in the provided excerpts).</li>
</ul></li>
</ul></li>
<li><strong>The Bootstrap and the Jackknife</strong>
<ul>
<li>3.1 <strong>The Jackknife</strong>
<ul>
<li>Introduction to the jackknife as a method for estimating bias and variance.</li>
<li>Definition of the jackknife estimator <img src="https://latex.codecogs.com/png.latex?T_%7B(i)%7D">: the estimator computed by omitting the <img src="https://latex.codecogs.com/png.latex?i">-th observation.</li>
<li>Jackknife estimate of bias: <img src="https://latex.codecogs.com/png.latex?bias_%7Bjack%7D%20=%20(n-1)(%5Cbar%7BT%7D_%7B(.)%7D%20-%20T_n)">, where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BT%7D_%7B(.)%7D%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20T_%7B(i)%7D"> and <img src="https://latex.codecogs.com/png.latex?T_n%20=%20T(X_1,%20.%20.%20.%20,%20X_n)">.</li>
<li>Jackknife estimate of variance: <img src="https://latex.codecogs.com/png.latex?V_%7Bjack%7D%20=%20%5Cfrac%7Bn-1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(T_%7B(i)%7D%20-%20%5Cbar%7BT%7D_%7B(.)%7D)%5E2">.</li>
<li>Approximate <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> confidence interval using the jackknife: <img src="https://latex.codecogs.com/png.latex?T_n%20%5Cpm%20t_%7Bn-1,%201-%5Calpha/2%7D%20%5Csqrt%7BV_%7Bjack%7D%7D">.</li>
<li>Example 3.3 illustrating jackknife estimation of bias and standard error for the mean.</li>
<li>Example 3.5 showing the inconsistency of the jackknife variance estimator for the median.</li>
</ul></li>
<li>3.2 <strong>The Bootstrap</strong>
<ul>
<li>Introduction to the bootstrap as a general method for statistical inference based on resampling.</li>
<li>Nonparametric bootstrap procedure:
<ol type="1">
<li>Draw a bootstrap sample <img src="https://latex.codecogs.com/png.latex?X%5E*_1,%20.%20.%20.%20,%20X%5E*_n"> with replacement from the original data <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n">.</li>
<li>Compute the statistic of interest <img src="https://latex.codecogs.com/png.latex?T%5E*_n%20=%20T(X%5E*_1,%20.%20.%20.%20,%20X%5E*_n)">.</li>
<li>Repeat steps 1 and 2 <img src="https://latex.codecogs.com/png.latex?B"> times to get <img src="https://latex.codecogs.com/png.latex?T%5E*_%7Bn,1%7D,%20.%20.%20.%20,%20T%5E*_%7Bn,B%7D">.</li>
</ol></li>
<li>Bootstrap estimate of standard error: <img src="https://latex.codecogs.com/png.latex?se_%7Bboot%7D%20=%20%5Csqrt%7B%5Cfrac%7B1%7D%7BB%7D%20%5Csum_%7Bb=1%7D%5EB%20(T%5E*_%7Bn,b%7D%20-%20%5Cbar%7BT%7D%5E*_n)%5E2%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BT%7D%5E*_n%20=%20%5Cfrac%7B1%7D%7BB%7D%20%5Csum_%7Bb=1%7D%5EB%20T%5E*_%7Bn,b%7D">.</li>
<li>Example 3.12 provides pseudo-code for bootstrapping the median.</li>
</ul></li>
<li>3.3 <strong>Parametric Bootstrap</strong>
<ul>
<li>Procedure for parametric bootstrap:
<ol type="1">
<li>Assume a parametric model <img src="https://latex.codecogs.com/png.latex?F_%5Ctheta"> for the data.</li>
<li>Estimate the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> from the data to get <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D">.</li>
<li>Draw bootstrap samples from <img src="https://latex.codecogs.com/png.latex?F_%7B%5Chat%7B%5Ctheta%7D%7D">.</li>
<li>Compute the statistic of interest for each bootstrap sample.</li>
</ol></li>
<li>Useful when a good parametric model is known or can be assumed.</li>
</ul></li>
<li>3.4 <strong>Bootstrap Confidence Intervals</strong>
<ul>
<li>Introduction to different types of bootstrap confidence intervals.</li>
<li><strong>Normal approximation interval</strong>: <img src="https://latex.codecogs.com/png.latex?T_n%20%5Cpm%20z_%7B1-%5Calpha/2%7D%20se_%7Bboot%7D">.</li>
<li><strong>Percentile interval</strong>: <img src="https://latex.codecogs.com/png.latex?C_n%20=%20(T%5E*_%7B(B%5Calpha/2)%7D,%20T%5E*_%7B(B(1-%5Calpha/2))%7D)">, using the <img src="https://latex.codecogs.com/png.latex?%5Calpha/2"> and <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha/2"> quantiles of the bootstrap sample.</li>
<li><strong>Pivotal interval</strong>: Based on a pivotal quantity <img src="https://latex.codecogs.com/png.latex?R_n(X,%20%5Ctheta)"> whose distribution does not depend on <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Approximate the distribution of <img src="https://latex.codecogs.com/png.latex?R_n(X,%20%5Ctheta)"> by the distribution of <img src="https://latex.codecogs.com/png.latex?R%5E*_n(X%5E*,%20T_n)">. The <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> bootstrap pivotal interval is <img src="https://latex.codecogs.com/png.latex?(T_n%20-%20R%5E*_%7B1-%5Calpha/2%7D,%20T_n%20-%20R%5E*_%7B%5Calpha/2%7D)">, where <img src="https://latex.codecogs.com/png.latex?R%5E*_%5Cbeta"> is the <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> quantile of <img src="https://latex.codecogs.com/png.latex?R%5E*_n">.</li>
<li><strong>Bootstrap studentized pivotal interval</strong>: <img src="https://latex.codecogs.com/png.latex?(%5Chat%7B%5Ctheta%7D_n%20-%20z%5E*_%7B1-%5Calpha/2%7D%20%5Chat%7Bse%7D_%7Bboot%7D,%20%5Chat%7B%5Ctheta%7D_n%20-%20z%5E*_%7B%5Calpha/2%7D%20%5Chat%7Bse%7D_%7Bboot%7D)">, where <img src="https://latex.codecogs.com/png.latex?z%5E*_%5Cbeta"> is the <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> quantile of <img src="https://latex.codecogs.com/png.latex?Z%5E*_n,b%20=%20%5Cfrac%7BT%5E*_%7Bn,b%7D%20-%20T_n%7D%7B%5Chat%7Bse%7D%5E*_b%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bse%7D%5E*_b"> is an estimate of the standard error of <img src="https://latex.codecogs.com/png.latex?T%5E*_%7Bn,b%7D">.</li>
<li>Example 3.17 shows various bootstrap confidence intervals for skewness.</li>
</ul></li>
<li>3.5 <strong>Some Theory</strong>
<ul>
<li>Brief overview of theoretical aspects of the bootstrap.</li>
<li>Consistency of the bootstrap distribution.</li>
<li>Discussion of when the bootstrap works and when it might fail.</li>
</ul></li>
<li>3.6 <strong>Bibliographic Remarks</strong>
<ul>
<li>Provides references for further reading on the jackknife and bootstrap methods.</li>
</ul></li>
<li>3.7 <strong>Appendix</strong>
<ul>
<li>(Content of the appendix is not detailed in the provided excerpts).</li>
</ul></li>
</ul></li>
<li><strong>Smoothing: General Concepts</strong>
<ul>
<li>Introduction to the necessity of smoothing data to estimate curves like probability density functions (<img src="https://latex.codecogs.com/png.latex?f">) or regression functions (<img src="https://latex.codecogs.com/png.latex?r">).</li>
<li>Discussion of two main types of problems studied:
<ul>
<li><strong>Density estimation</strong>: Estimating the probability density function <img src="https://latex.codecogs.com/png.latex?f"> given a sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n%20%5Csim%20f">.</li>
<li><strong>Regression</strong>: Estimating the regression function <img src="https://latex.codecogs.com/png.latex?r"> given pairs <img src="https://latex.codecogs.com/png.latex?(x_1,%20Y_1),%20.%20.%20.%20,%20(x_n,%20Yn)"> where <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20r(x_i)%20+%20%5Cepsilon_i"> and <img src="https://latex.codecogs.com/png.latex?E(%5Cepsilon_i)%20=%200">.</li>
</ul></li>
<li>Example 4.3 (Density estimation) showing histograms of astronomy data with different amounts of smoothing.</li>
<li>Example 4.4 (Nonparametric regression) discussing the Cosmic Microwave Background (CMB) data.</li>
<li>Example 4.5 (Nonparametric regression) illustrating the LIDAR experiment data and regressograms.</li>
<li>Example 4.6 (Nonparametric regression) showing BPD (Bronchopulmonary Dysplasia) data and fits from different regression methods.</li>
<li>Example 4.7 (Nonparametric regression) presenting rock data and additive model fits.</li>
<li>4.1 <strong>The Bias–Variance Tradeoff</strong>
<ul>
<li>Introduction to the concepts of bias and variance in the context of curve estimation.</li>
<li>Definition of <strong>risk or mean squared error (mse)</strong>: <img src="https://latex.codecogs.com/png.latex?mse%20=%20R(f(x),%20%5Chat%7Bf%7D_n(x))%20=%20E(L(f(x),%20%5Chat%7Bf%7D_n(x)))"> (4.9).</li>
<li>Relationship between risk, bias, and variance: <img src="https://latex.codecogs.com/png.latex?R(f(x),%20%5Chat%7Bf%7D_n(x))%20=%20bias_x%5E2%20+%20V_x"> (4.10) and <img src="https://latex.codecogs.com/png.latex?risk%20=%20mse%20=%20bias%5E2%20+%20variance"> (4.11).</li>
<li>Illustration of the bias-variance tradeoff where bias increases and variance decreases with more smoothing.</li>
<li>Example illustrating the mse of a histogram estimator: <img src="https://latex.codecogs.com/png.latex?mse%20%5Capprox%20Ah%5E4%20+%20B/(nh)"> (4.18).</li>
</ul></li>
<li>4.2 <strong>Kernels</strong>
<ul>
<li>Introduction to kernels as a smoothing tool.</li>
<li>Examples of common kernels: boxcar, Gaussian, Epanechnikov, and tricube.</li>
</ul></li>
<li>4.3 <strong>Which Loss Function?</strong>
<ul>
<li>Discussion of loss functions beyond squared error, such as <img src="https://latex.codecogs.com/png.latex?L_p"> loss and Kullback–Leibler loss.</li>
<li>Reason for the continued popularity of <img src="https://latex.codecogs.com/png.latex?L_2"> (squared error) loss.</li>
</ul></li>
<li>4.4 <strong>Confidence Sets</strong>
<ul>
<li>Brief mention of the desire to provide confidence sets for estimated curves.</li>
</ul></li>
<li>4.5 <strong>The Curse of Dimensionality</strong>
<ul>
<li>Explanation of how the difficulty of nonparametric estimation increases with the dimensionality of the data.</li>
<li>Even with computational advancements, the statistical challenge of high-dimensional data remains, leading to large confidence intervals.</li>
</ul></li>
<li>4.6 <strong>Bibliographic Remarks</strong>
<ul>
<li>Listing of several key texts on smoothing methods.</li>
</ul></li>
</ul></li>
<li><strong>Nonparametric Regression</strong>
<ul>
<li>Introduction to nonparametric regression, also known as “learning a function”.</li>
<li>Given <img src="https://latex.codecogs.com/png.latex?n"> pairs of observations <img src="https://latex.codecogs.com/png.latex?(x_1,%20Y_1),%20.%20.%20.%20,%20(x_n,%20Yn)"> where <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20r(x_i)%20+%20%5Cepsilon_i"> and <img src="https://latex.codecogs.com/png.latex?E(%5Cepsilon_i)%20=%200">.</li>
<li>The goal is to estimate the regression function <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20E(Y%20%7CX%20=%20x)">.</li>
<li>Methods considered include local regression methods (kernel regression, local polynomial regression) and penalization methods (splines).</li>
<li>Chapter 8 and 9 will cover an approach based on orthogonal functions.</li>
<li>All estimators in this chapter are linear smoothers.</li>
<li>5.1 <strong>Review of Linear and Logistic Regression</strong>
<ul>
<li>Brief review of standard parametric regression techniques as a contrast to nonparametric methods.</li>
</ul></li>
<li>5.2 <strong>Linear Smoothers</strong>
<ul>
<li>Definition of a linear smoother: An estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n"> of <img src="https://latex.codecogs.com/png.latex?r"> is a linear smoother if <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)%20=%20%5Csum_%7Bi=1%7D%5En%20%5Cbeta_i(x)Y_i">.</li>
<li>Vector of fitted values <img src="https://latex.codecogs.com/png.latex?r%20=%20(%5Chat%7Br%7D_n(x_1),%20.%20.%20.%20,%20%5Chat%7Br%7D_n(x_n))%5ET">.</li>
<li>Relationship <img src="https://latex.codecogs.com/png.latex?r%20=%20LY"> where <img src="https://latex.codecogs.com/png.latex?L"> is the smoothing matrix.</li>
</ul></li>
<li>5.3 <strong>Choosing the Smoothing Parameter</strong>
<ul>
<li>The importance of selecting an appropriate smoothing parameter (e.g., bandwidth <img src="https://latex.codecogs.com/png.latex?h">).</li>
<li>Cross-validation as a common data-driven method for choosing the smoothing parameter.</li>
<li>The cross-validation score <img src="https://latex.codecogs.com/png.latex?cv%20=%20%5Chat%7BR%7D(h)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(Y_i%20-%20%5Chat%7Br%7D%5E%7B(-i)%7D(x_i))%5E2"> (5.30).</li>
<li>Definition of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D%5E%7B(-i)%7D(x)%20=%20%5Csum_%7Bj=1%7D%5En%20Y_j%20%5Cbeta_%7Bj,(-i)%7D(x)"> (5.31) with specific conditions on <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7Bj,(-i)%7D(x)"> (5.32).</li>
<li>Generalized cross-validation (GCV) as an alternative.</li>
</ul></li>
<li>5.4 <strong>Local Regression</strong>
<ul>
<li>Introduction to local regression techniques.</li>
<li><strong>Kernel regression (Nadaraya-Watson estimator)</strong>: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5En%20K(%5Cfrac%7Bx-x_i%7D%7Bh%7D)Y_i%7D%7B%5Csum_%7Bi=1%7D%5En%20K(%5Cfrac%7Bx-x_i%7D%7Bh%7D)%7D"> (5.38).</li>
<li><strong>Local polynomial regression</strong>: Fitting a local polynomial of degree <img src="https://latex.codecogs.com/png.latex?p"> in a neighborhood of <img src="https://latex.codecogs.com/png.latex?x">.</li>
<li>Example 5.54 illustrating local linear regression for the LIDAR data.</li>
<li>Theorem 5.60 stating that the local linear estimator has weights <img src="https://latex.codecogs.com/png.latex?%5Cbeta_i(x)%20=%20%5Cfrac%7BK_h(x-x_i)(S_2(x)%20-%20S_1(x)(x_i%20-%20x))%7D%7BS_0(x)S_2(x)%20-%20S_1(x)%5E2%7D"> where <img src="https://latex.codecogs.com/png.latex?S_j(x)%20=%20%5Csum_%7Bi=1%7D%5En%20K_h(x-x_i)(x_i%20-%20x)%5Ej">.</li>
</ul></li>
<li>5.5 <strong>Penalized Regression, Regularization and Splines</strong>
<ul>
<li>Introduction to penalized regression and regularization methods.</li>
<li><strong>Smoothing splines</strong>: Minimizing a penalized residual sum of squares <img src="https://latex.codecogs.com/png.latex?M(%5Clambda)%20=%20%5Csum_%7Bi=1%7D%5En%20(Y_i%20-%20r(x_i))%5E2%20+%20%5Clambda%20%5Cint%20(r''(x))%5E2%20dx"> (5.70, 5.71).</li>
<li>Theorem 5.73 stating that the minimizer is a natural cubic spline with knots at the data points.</li>
<li><strong>Basis functions for splines</strong>: Truncated power basis (Theorem 5.74) and B-spline basis (Theorem 5.76).</li>
</ul></li>
<li>5.6 <strong>Variance Estimation</strong>
<ul>
<li>Estimating the variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2(x)%20=%20V(%5Cepsilon%7CX=x)">.</li>
<li>Method based on squared residuals <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D%5E2(x)%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5En%20K_h(x-x_i)(Y_i%20-%20%5Chat%7Br%7D_n(x_i))%5E2%7D%7B%5Csum_%7Bi=1%7D%5En%20K_h(x-x_i)%7D"> (5.89).</li>
<li>Iterative procedure for estimating <img src="https://latex.codecogs.com/png.latex?r(x)"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2(x)">.</li>
<li>Example 5.96 showing variance estimation for the CMB data.</li>
</ul></li>
<li>5.7 <strong>Confidence Bands</strong>
<ul>
<li>Constructing confidence bands for the regression function <img src="https://latex.codecogs.com/png.latex?r(x)">.</li>
<li>Typically of the form <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)%20%5Cpm%20c%20%5C%20se(x)"> (5.97).</li>
<li>The bias problem: confidence bands are often for <img src="https://latex.codecogs.com/png.latex?r_n(x)%20=%20E(%5Chat%7Br%7D_n(x))"> not <img src="https://latex.codecogs.com/png.latex?r(x)">.</li>
<li>Constructing approximate pointwise confidence bands using the asymptotic normality of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)">.</li>
<li>Simultaneous confidence bands using <img src="https://latex.codecogs.com/png.latex?%5Ckappa_0"> and <img src="https://latex.codecogs.com/png.latex?c"> derived from the <img src="https://latex.codecogs.com/png.latex?L_%5Cinfty"> norm of a Gaussian process.</li>
<li>Approximate <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha"> simultaneous confidence band <img src="https://latex.codecogs.com/png.latex?I(x)%20=%20%5Chat%7Br%7D_n(x)%20%5Cpm%20c%20%5Chat%7B%5Csigma%7D(x)%20%7C%7C%5Cbeta(x)%7C%7C"> (5.104).</li>
<li>Example 5.105 showing simultaneous confidence bands for the CMB data.</li>
<li>Remark on adjusting for the uncertainty in the smoothing parameter choice using the Bonferroni inequality.</li>
<li>Remark on using bootstrap methods for confidence bands.</li>
</ul></li>
<li>5.8 <strong>Average Coverage</strong>
<ul>
<li>Discussion of confidence bands with average coverage instead of pointwise or simultaneous coverage.</li>
</ul></li>
<li>5.9 <strong>Summary of Linear Smoothing</strong>
<ul>
<li>Steps to construct the estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n"> and a confidence band.</li>
<li>Example 5.110 applying the summary to the LIDAR data.</li>
</ul></li>
<li>5.10 <strong>Local Likelihood and Exponential Families</strong>
<ul>
<li>Extending local methods to likelihood-based estimation for exponential families.</li>
<li>Local likelihood estimation for binary regression (logistic regression).</li>
<li>Example 5.119 showing local linear logistic regression.</li>
<li>Example 5.120 illustrating local likelihood for BPD data.</li>
</ul></li>
<li>5.11 <strong>Scale-Space Smoothing</strong>
<ul>
<li>An approach that examines <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_h(x)"> over a range of bandwidths <img src="https://latex.codecogs.com/png.latex?h">.</li>
<li>The scale-space surface <img src="https://latex.codecogs.com/png.latex?S%20=%20%5C%7B%20r_h(x),%20x%20%5Cin%20X%20,%20h%20%5Cin%20H%20%5C%7D">.</li>
<li>Method SiZer (significant zero crossings of derivatives).</li>
</ul></li>
<li>5.12 <strong>Multiple Regression</strong>
<ul>
<li>Nonparametric regression with multiple covariates <img src="https://latex.codecogs.com/png.latex?r(x_1,%20.%20.%20.%20,%20x_p)">.</li>
<li>Challenges due to the curse of dimensionality.</li>
<li><strong>Additive models</strong>: <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20%5Cmu%20+%20%5Csum_%7Bj=1%7D%5Ep%20r_j(x_j)">.</li>
<li>Backfitting algorithm for fitting additive models.</li>
<li>Example 5.126 applying additive models to rock data.</li>
<li><strong>Projection pursuit regression</strong>: <img src="https://latex.codecogs.com/png.latex?r(x_1,%20.%20.%20.%20,%20x_p)%20=%20%5Cmu%20+%20%5Csum_%7Bm=1%7D%5EM%20r_m(%5Calpha_m%5ET%20x)">.</li>
<li>Algorithm for projection pursuit regression.</li>
<li>Example 5.130 applying projection pursuit regression to rock data.</li>
<li><strong>Regression trees</strong>: Partitioning the covariate space into rectangles and fitting a constant in each.</li>
<li>Complexity parameter <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and cost-complexity pruning (5.132).</li>
<li>Example 5.133 showing a regression tree for rock data.</li>
<li><strong>Multivariate Adaptive Regression Splines (MARS)</strong>: Building a model from piecewise linear basis functions.</li>
<li><strong>Tensor Product Models</strong>: <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20%5Csum_%7Bm=1%7D%5EM%20%5Cbeta_m%20h_m(x)"> where <img src="https://latex.codecogs.com/png.latex?h_m"> are basis functions in a tensor product space (5.135).</li>
</ul></li>
<li>5.13 <strong>Other Issues</strong>
<ul>
<li><strong>Plug-In Bandwidths</strong>: Using formulas for asymptotically optimal bandwidths.</li>
<li><strong>Choice of Kernel</strong>: Impact of kernel shape.</li>
<li><strong>Boundary Effects</strong>: Problems near the boundaries of the data range.</li>
<li><strong>Varying Coefficient Models</strong>: <img src="https://latex.codecogs.com/png.latex?r(x)%20=%20%5Csum_%7Bj=1%7D%5Ep%20%5Cbeta_j(x)%20x_j">.</li>
<li><strong>Quantile Regression</strong>: Modeling conditional quantiles of <img src="https://latex.codecogs.com/png.latex?Y"> given <img src="https://latex.codecogs.com/png.latex?X">.</li>
<li><strong>Derivative Estimation</strong>: Estimating derivatives of the regression function (5.143).</li>
</ul></li>
<li>5.14 <strong>Bibliographic Remarks</strong>
<ul>
<li>References for further reading on nonparametric regression.</li>
</ul></li>
<li>5.15 <strong>Appendix</strong>
<ul>
<li>(Content of the appendix is not detailed in the provided excerpts).</li>
</ul></li>
</ul></li>
<li><strong>Density Estimation</strong>
<ul>
<li>Introduction to the problem of estimating the probability density function <img src="https://latex.codecogs.com/png.latex?f(x)"> from a sample <img src="https://latex.codecogs.com/png.latex?X_1,%20.%20.%20.%20,%20X_n">.</li>
<li>6.1 <strong>Cross-Validation</strong>
<ul>
<li>Using cross-validation to evaluate the quality of a density estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n"> with the risk <img src="https://latex.codecogs.com/png.latex?R%20=%20E(L)"> (integrated mean squared error).</li>
</ul></li>
<li>6.2 <strong>Histograms</strong>
<ul>
<li>Definition of a histogram estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)%20=%20m%20%5Csum_%7Bj=1%7D%5Em%20%5Chat%7Bp%7D_j%20I(x%20%5Cin%20B_j)"> (6.7), where <img src="https://latex.codecogs.com/png.latex?h%20=%201/m"> is the binwidth, <img src="https://latex.codecogs.com/png.latex?Y_j"> is the count in bin <img src="https://latex.codecogs.com/png.latex?B_j">, and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D_j%20=%20Y_j/n">.</li>
<li>Example 6.8 showing histograms of astronomy data with different numbers of bins (oversmoothing, undersmoothing, and cross-validation chosen).</li>
<li>Theorem 6.9 relating the expected value of the histogram estimator to the true density and its derivatives.</li>
<li>The risk (integrated squared error) of a histogram estimator <img src="https://latex.codecogs.com/png.latex?J(h)%20=%20E%20%5Cint%20(f%CC%82_n(x)%20%E2%88%92%20f(x))%5E2%20dx"> (6.12).</li>
<li>The leave-one-out cross-validation score for histograms <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Csum_%7Bi=1%7D%5En%20(%5Chat%7Bf%7D_n%5E%7B(-i)%7D(X_i))%5E2"> (6.14).</li>
<li>Theorem 6.15 giving a formula for the cross-validation score: <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Cfrac%7B2%7D%7Bh(n%20-%201)%7D%20-%20%5Cfrac%7Bn%20+%201%7D%7Bh(n%20-%201)%7D%20%5Csum_%7Bj=1%7D%5Em%20%5Chat%7Bp%7D_j%5E2"> (6.16).</li>
<li>Example 6.17 showing the application of cross-validation to choose the number of bins for the astronomy data.</li>
<li>Constructing confidence sets for <img src="https://latex.codecogs.com/png.latex?f"> at the resolution of the histogram.</li>
</ul></li>
<li>6.3 <strong>Kernel Density Estimation</strong>
<ul>
<li>The kernel density estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_n(x)%20=%20%5Cfrac%7B1%7D%7Bnh%7D%20%5Csum_%7Bi=1%7D%5En%20K(%5Cfrac%7Bx%20-%20X_i%7D%7Bh%7D)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20K_h(x%20-%20X_i)"> (6.23).</li>
<li>The mean integrated squared error (MISE) as a measure of performance (6.27).</li>
<li>Asymptotic MISE (AMISE) and optimal bandwidth (6.29, 6.31).</li>
<li>Rule-of-thumb bandwidth selection based on assuming a Normal distribution (6.33).</li>
<li>Leave-one-out cross-validation for kernel density estimation <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5En%20(%5Chat%7Bf%7D_n%5E%7B(-i)%7D(X_i))%5E2"> (6.34).</li>
<li>Theorem 6.35 giving a formula for the cross-validation score: <img src="https://latex.codecogs.com/png.latex?%5Chat%7BJ%7D(h)%20=%20%5Cfrac%7B1%7D%7Bn(n-1)h%7D%20%5Csum_%7Bi=1%7D%5En%20%5Csum_%7Bj=1%7D%5En%20K(%5Cfrac%7BX_i%20-%20X_j%7D%7Bh%7D)%20-%20%5Cfrac%7B2%7D%7Bn(n-1)h%7D%20%5Csum_%7Bi%20%5Cneq%20j%7D%20K(%5Cfrac%7BX_i%20-%20X_j%7D%7Bh%7D)">.</li>
<li>Example 6.36 applying kernel density estimation to the astronomy data and using cross-validation for bandwidth selection.</li>
<li>Confidence bands for kernel density estimators using the relationship with regression.</li>
</ul></li>
<li>6.4 <strong>Local Polynomials</strong>
<ul>
<li>Extending the idea of local fitting to density estimation.</li>
<li>Local polynomial density estimation.</li>
<li>Relation to kernel density estimation when the polynomial degree <img src="https://latex.codecogs.com/png.latex?p=0">.</li>
</ul></li>
<li>6.5 <strong>Multivariate Problems</strong>
<ul>
<li>Challenges of density estimation in higher dimensions (curse of dimensionality).</li>
</ul></li>
<li>6.6 <strong>Converting Density Estimation Into Regression</strong>
<ul>
<li>Transforming the density estimation problem into a regression problem by binning the data and using regression techniques on the counts.</li>
<li>Applying kernel regression to the square root of the counts.</li>
<li>Constructing confidence bands using regression techniques.</li>
<li>Example 6.51 applying this method to the Bart Simpson distribution.</li>
</ul></li>
<li>6.7 <strong>Bibliographic Remarks</strong>
<ul>
<li>Listing key references for density estimation.</li>
</ul></li>
<li>6.8 <strong>Appendix</strong>
<ul>
<li>Proof of Theorem 6.11 regarding the bias of the histogram estimator.</li>
</ul></li>
</ul></li>
<li><strong>Normal Means and Minimax Theory</strong>
<ul>
<li>Introduction to the Normal means problem and its role in unifying some nonparametric problems, serving as a basis for Chapters 8 and 9.</li>
<li>The chapter is noted as being more theoretical.</li>
<li>Recommendation for readers not interested in theoretical details to read sections 7.1, 7.2, and 7.3 and then skip to the next chapter.</li>
<li>7.1 <strong>The Normal Means Model</strong>
<ul>
<li>Defining the basic Normal means model: <img src="https://latex.codecogs.com/png.latex?Zi%20%5Csim%20N(%5Ctheta_i,%20%5Csigma%5E2_n)">, for <img src="https://latex.codecogs.com/png.latex?i%20=%201,%202,%20.%20.%20."> (7.2), where <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20(%5Ctheta_1,%20%5Ctheta_2,%20.%20.%20.)"> is the unknown parameter and <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2_n"> is assumed known.</li>
<li>Practical note: In reality, <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2_n"> would often need to be estimated (as discussed in Chapter 5), which might affect the exact theoretical results.</li>
<li>Example 7.3 illustrating the model with a one-way analysis of variance setup: <img src="https://latex.codecogs.com/png.latex?X_%7Bij%7D%20=%20%5Ctheta_i%20+%20%5Csigma%20%5Cdelta_%7Bij%7D"> where <img src="https://latex.codecogs.com/png.latex?%5Cdelta_%7Bij%7D"> are independent <img src="https://latex.codecogs.com/png.latex?N(0,1)"> random variables.</li>
</ul></li>
<li>7.2 <strong>Function Spaces</strong>
<ul>
<li>Introduction to function spaces relevant to the theoretical development.</li>
</ul></li>
<li>7.3 <strong>Connection to Regression and Density Estimation</strong>
<ul>
<li>Discussing how the Normal means problem relates to nonparametric regression and density estimation problems.</li>
</ul></li>
<li>7.4 <strong>Stein’s Unbiased Risk Estimator (sure)</strong>
<ul>
<li>Introducing Stein’s Unbiased Risk Estimator as a method for estimating the risk of an estimator.</li>
<li>Example showing the application of sure to the soft threshold estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_i%20=%20sign(Zi)(%7CZi%7C%20-%20%5Clambda)_+"> and providing the risk estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(Z)%20=%20%5Csum_%7Bi=1%7D%5En%20(%5Csigma%5E2%20-%202%5Csigma%5E2I(%7CZi%7C%20%5Cleq%20%5Clambda)%20+%20min(Z%5E2_i%20,%20%5Clambda%5E2)%20)"> (7.22).</li>
<li>Mentioning that sure is not appropriate for the hard threshold estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D_i%20=%20Zi%20I(%7CZi%7C%20%3E%20%5Clambda)">.</li>
</ul></li>
<li>7.5 <strong>Minimax Risk and Pinsker’s Theorem</strong>
<ul>
<li>Discussing the concept of minimax risk.</li>
<li>Introducing Pinsker’s Theorem (Theorem 7.28) which gives an exact expression for the asymptotic minimax risk over an ellipsoid <img src="https://latex.codecogs.com/png.latex?%5CTheta(c%5E2)%20=%20%5C%7B%5Ctheta%20:%20%5Csum%20a_i%5E2%20%5Ctheta_i%5E2%20%5Cleq%20c%5E2%20%5C%7D">.</li>
</ul></li>
<li>7.6 <strong>Linear Shrinkage and the James–Stein Estimator</strong>
<ul>
<li>Returning to model (7.1) and exploring improvements over the MLE using linear estimators of the form <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D%20=%20bZ%20=%20(bZ_1,%20.%20.%20.%20,%20bZ_n)"> where <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20b%20%5Cleq%201">.</li>
<li>Defining the set of linear shrinkage estimators <img src="https://latex.codecogs.com/png.latex?L%20=%20%5C%7BbZ%20:%20b%20%5Cin%5C%7D">.</li>
<li>Presenting the James-Stein estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D%5E%7BJS%7D%20=%20(1%20-%20%5Cfrac%7B(n-2)%5Csigma%5E2_n%7D%7B%5C%7CZ%5C%7C%5E2%7D)_+%20Z"> (7.41) and noting it minimizes the estimated risk over <img src="https://latex.codecogs.com/png.latex?L">.</li>
<li>Mentioning a block estimation scheme by Cai et al.&nbsp;(2000) using the James-Stein estimator within blocks.</li>
<li>Theorem 7.50 (Cai, Low and Zhao, 2000) providing asymptotic optimality of their block estimator over Sobolev ellipsoids <img src="https://latex.codecogs.com/png.latex?%5CTheta(m,%20c)%20=%20%5C%7B%5Ctheta%20:%20%5Csum_%7Bi=1%7D%5E%5Cinfty%20a_i%5E2%20%5Ctheta%5E2_i%20%5Cleq%20c%5E2%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?a_%7B2i%7D%20=%20a_%7B2i+1%7D%20=%201%20+%20(2i%5Cpi)%5E%7B2m%7D">.</li>
</ul></li>
<li>7.7 <strong>Adaptive Estimation Over Sobolev Spaces</strong>
<ul>
<li>Discussing adaptive estimation methods for Sobolev spaces.</li>
</ul></li>
<li>7.8 <strong>Confidence Sets</strong>
<ul>
<li>Focusing on the construction of confidence sets <img src="https://latex.codecogs.com/png.latex?B_n%20%5Csubset%20R%5En"> for <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20(%5Ctheta_1,%20...,%20%5Ctheta_n)"> such that <img src="https://latex.codecogs.com/png.latex?%5Cinf_%7B%5Ctheta%20%5Cin%20R%5En%7D%20P_%5Ctheta(%5Ctheta%20%5Cin%20B_n)%20%5Cgeq%201%20-%20%5Calpha"> (7.51).</li>
<li>Describing different methods for constructing confidence sets.</li>
</ul></li>
<li>7.9 <strong>Optimality of Confidence Sets</strong>
<ul>
<li>Discussing the concept of optimal confidence sets.</li>
</ul></li>
<li>7.10 <strong>Random Radius Bands?</strong>
<ul>
<li>Questioning the use of random radius bands for confidence sets.</li>
</ul></li>
<li>7.11 <strong>Penalization, Oracles and Sparsity</strong>
<ul>
<li>Connecting penalization methods to oracle properties and the concept of sparsity in the parameter vector <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
<li>Mentioning the LASSO (Tibshirani (1996)) and basis pursuit (Chen et al.&nbsp;(1998)) in the context of criterion (7.89).</li>
<li>Relating soft thresholding to wavelet methods (Chapter 9).</li>
</ul></li>
<li>7.12 <strong>Bibliographic Remarks</strong>
<ul>
<li>Providing references for further reading on Normal means and minimax theory.</li>
</ul></li>
<li>7.13 <strong>Appendix</strong>
<ul>
<li>Containing technical proofs, including the proof of Theorem 7.28 (Pinsker’s Theorem).</li>
</ul></li>
</ul></li>
<li>Nonparametric Inference Using Orthogonal Functions</li>
</ol>
<ul>
<li>This chapter introduces the use of orthogonal functions (e.g., Fourier series, wavelets) for nonparametric inference in both regression and density estimation.</li>
<li>For nonparametric regression, the function is expanded in an orthonormal basis: - “r(x) = ∑∞ j=1 θjφj(x) (8.3)”.</li>
<li>The concept of a modulator is introduced for shrinking the coefficients:
<ul>
<li>“A modulator is a vector b = (b1, . . . , bn) such that 0 ≤ bj ≤ 1, j = 1, . . . , n.&nbsp;A modulation estimator is an estimator of the form θ̂ = bZ = (b1Z1, b2Z2, . . . , bnZn). (8.8)”.</li>
</ul></li>
<li>Risk estimation and minimization over classes of modulators are discussed, connecting to the Pinsker bound.</li>
<li>The chapter also covers irregular designs and density estimation using orthogonal functions.</li>
</ul>
<ol start="9" type="1">
<li>Wavelets and Other Adaptive Methods</li>
</ol>
<ul>
<li>This chapter focuses on wavelets, a powerful tool for adaptive nonparametric estimation, particularly for functions with varying degrees of smoothness.</li>
<li>Haar wavelets are introduced as a simple example.</li>
<li>The construction of more general wavelets through multiresolution analysis (MRA) is outlined:</li>
<li>“9.11 Definition. Given a function φ, define V0, V1, . . . , as in (9.10). We say that φ generates a multiresolution analysis (MRA) of R if Vj ⊂ Vj+1, j ≥ 0, (9.12) and ⋃ j≥0 Vj is dense in L2(R). (9.13)”.</li>
<li>Wavelet regression techniques, including wavelet thresholding (VisuShrink and SureShrink), are discussed for denoising and estimating functions from noisy data. The oracle risk and the performance of thresholding estimators are mentioned.</li>
<li>Besov spaces are introduced as function spaces that are well-suited for characterizing the properties of functions that can be effectively estimated using wavelets.</li>
<li>The construction of confidence sets using wavelet methods is covered.</li>
<li>Other adaptive methods, such as the Intersection of Confidence Intervals (ICI) method by Goldenshluger and Nemirovski, are briefly introduced as alternatives to wavelet-based approaches for adapting to unknown smoothness.</li>
</ul>
<ol start="10" type="1">
<li>Other Topics</li>
</ol>
<ul>
<li>This final chapter covers a range of more advanced and specialized topics in nonparametric inference.</li>
<li>Measurement error in covariates and responses and its impact on estimation are discussed, along with potential correction methods. Inverse problems, where the goal is to infer an underlying function or parameter from indirect observations, are introduced.</li>
<li>Nonparametric Bayes, semiparametric inference, and issues with correlated errors are briefly mentioned.</li>
<li>The chapter touches upon classification, sieves, shape-restricted inference, and testing in a nonparametric framework.</li>
<li>Finally, computational issues relevant to implementing nonparametric methods are highlighted.</li>
</ul>
</section>
<section id="faq" class="level2">
<h2 class="anchored" data-anchor-id="faq">FAQ</h2>
<ol type="1">
<li>What is nonparametric inference and how does it differ from parametric inference?</li>
</ol>
<p>Nonparametric inference refers to statistical methods where the structure of the underlying model is not fully specified by a finite number of parameters. Instead of assuming a particular distribution (like a normal distribution) with fixed parameters, nonparametric methods make fewer assumptions about the data-generating process. They aim to estimate functions or distributions directly from the data. This contrasts with parametric inference, which relies on models defined by a fixed set of parameters. Nonparametric methods are more flexible and can capture complex relationships in the data, but they often require larger sample sizes and can be less efficient if a simple parametric model is indeed appropriate.</p>
<ol start="2" type="1">
<li>What are confidence sets and why are they important in nonparametric inference?</li>
</ol>
<p>Confidence sets are generalizations of confidence intervals to higher dimensions or to function spaces. Instead of providing a range of plausible values for a single parameter, a confidence set provides a set of plausible functions, distributions, or other statistical objects. They are crucial in nonparametric inference because the goal is often to estimate an entire function or distribution, and it’s important to quantify the uncertainty associated with this estimate. A confidence set provides a way to assess the range of plausible estimates consistent with the observed data at a certain confidence level.</p>
<ol start="3" type="1">
<li>What are the bootstrap and the jackknife, and how are they used in nonparametric inference?</li>
</ol>
<p>The bootstrap and the jackknife are resampling techniques used to estimate the properties of an estimator (like its variance or bias) or to construct confidence intervals without relying on strong parametric assumptions.</p>
<p>Bootstrap: This method involves repeatedly resampling with replacement from the original data to create many “bootstrap samples.” The statistic of interest is calculated for each bootstrap sample, and the distribution of these statistics is used to approximate the sampling distribution of the original estimator. This can be used to estimate standard errors and construct bootstrap confidence intervals.</p>
<p>Jackknife: This technique involves systematically leaving out one observation at a time from the original data and calculating the statistic of interest for each of these “leave-one-out” samples. These values are then used to estimate bias and variance of the estimator. Both methods are valuable in nonparametric settings where analytical derivations of standard errors or confidence intervals might be difficult or unreliable due to the complexity of the estimators or the lack of distributional assumptions.</p>
<ol start="4" type="1">
<li>What is the bias-variance tradeoff in smoothing, and how does it relate to the choice of smoothing parameters like bandwidth?</li>
</ol>
<p>Smoothing techniques, such as kernel density estimation and nonparametric regression, aim to uncover underlying patterns in noisy data by averaging or weighting nearby observations. The bias-variance tradeoff is a fundamental concept in these methods.</p>
<p>Bias: This refers to the error introduced by approximating a complex real-world problem (which may be nonlinear) by a simpler model (the smoothed estimate). If the smoothing is too strong (e.g., a large bandwidth), the estimate might be overly smooth and miss important details in the true underlying function, leading to high bias.</p>
<p>Variance: This refers to the variability of the estimator. If the smoothing is weak (e.g., a small bandwidth), the estimator might be too sensitive to the noise in the data, resulting in high variance. The choice of the smoothing parameter (like the bandwidth <img src="https://latex.codecogs.com/png.latex?h"> in kernel methods) controls this tradeoff. A larger bandwidth typically leads to a smoother estimate with lower variance but potentially higher bias. A smaller bandwidth leads to a more wiggly estimate with higher variance but potentially lower bias. The goal is to choose a bandwidth that balances bias and variance to minimize the overall risk (e.g., mean squared error).</p>
<ol start="5" type="1">
<li>What are linear smoothers in nonparametric regression, and what are some examples?</li>
</ol>
<p>A nonparametric regression estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)"> is called a linear smoother if it can be expressed as a linear combination of the response variables <img src="https://latex.codecogs.com/png.latex?Y_i">, i.e., <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7Dn(x)%20=%20%5Csum%7Bi=1%7D%5E%7Bn%7D%20l_i(x)%20Y_i">, where <img src="https://latex.codecogs.com/png.latex?l_i(x)"> are weights that depend on the predictor variables <img src="https://latex.codecogs.com/png.latex?x_i"> and the point of estimation <img src="https://latex.codecogs.com/png.latex?x">, but not on the <img src="https://latex.codecogs.com/png.latex?Y_i">’s. These weights essentially determine how much each observation <img src="https://latex.codecogs.com/png.latex?Y_i"> contributes to the estimate at <img src="https://latex.codecogs.com/png.latex?x">. The collection of these weights for all <img src="https://latex.codecogs.com/png.latex?x_i"> forms the smoothing matrix <img src="https://latex.codecogs.com/png.latex?L">, such that the vector of fitted values <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Chat%7Br%7D%7D%20=%20LY">.</p>
<p>Examples of linear smoothers discussed include:</p>
<p>Regressogram: This method divides the range of the predictor variable into bins and estimates the regression function within each bin by the average of the <img src="https://latex.codecogs.com/png.latex?Y_i">’s in that bin. The weights <img src="https://latex.codecogs.com/png.latex?l_i(x)"> are constant within each bin and zero outside.</p>
<p>Local Averages: This estimator averages the <img src="https://latex.codecogs.com/png.latex?Y_i">’s for <img src="https://latex.codecogs.com/png.latex?x_i"> values within a certain distance (defined by a bandwidth <img src="https://latex.codecogs.com/png.latex?h">) of the target point <img src="https://latex.codecogs.com/png.latex?x">. The weights <img src="https://latex.codecogs.com/png.latex?l_i(x)"> are equal for points within the window and zero outside.</p>
<p>Kernel Estimators: These methods use a kernel function <img src="https://latex.codecogs.com/png.latex?K"> and a bandwidth <img src="https://latex.codecogs.com/png.latex?h"> to weight the <img src="https://latex.codecogs.com/png.latex?Y_i">’s based on the distance of their corresponding <img src="https://latex.codecogs.com/png.latex?x_i">’s from the target <img src="https://latex.codecogs.com/png.latex?x">. The weights <img src="https://latex.codecogs.com/png.latex?l_i(x)"> are proportional to <img src="https://latex.codecogs.com/png.latex?K((x-x_i)/h)">.</p>
<p>Local Polynomials: These estimators fit a low-degree polynomial to the data within a local neighborhood of <img src="https://latex.codecogs.com/png.latex?x"> and use the value of the fitted polynomial at <img src="https://latex.codecogs.com/png.latex?x"> as the estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Br%7D_n(x)">. The weights are determined by the polynomial fit.</p>
<p>Smoothing Splines: These methods find a smooth function that minimizes the residual sum of squares plus a penalty term that penalizes the roughness of the function. The resulting estimator is also a linear smoother.</p>
<ol start="6" type="1">
<li>How is the concept of minimaxity used in nonparametric inference, particularly in the context of normal means problems?</li>
</ol>
<p>Minimaxity in statistics refers to finding an estimator that performs optimally in the worst-case scenario over a given class of parameters or functions. In nonparametric inference, where the underlying function or parameter might belong to a large, infinite-dimensional space, minimax theory helps in understanding the fundamental limitations of estimation. It aims to find an estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctheta%7D"> that minimizes the maximum risk <img src="https://latex.codecogs.com/png.latex?R(%5Ctheta,%20%5Chat%7B%5Ctheta%7D)"> over all <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> in a class <img src="https://latex.codecogs.com/png.latex?%5CTheta">, where the risk <img src="https://latex.codecogs.com/png.latex?R"> quantifies the error of the estimator (e.g., mean squared error).</p>
<p>In the context of normal means problems, which serve as a simplified yet insightful model for many nonparametric problems (like regression or density estimation in an orthonormal basis), minimax theory helps determine the best possible rate of convergence for the risk. For instance, Pinsker’s theorem provides the minimax mean squared error for estimating a function in a Sobolev ellipsoid, showing that the optimal rate depends on the smoothness of the function class.</p>
<p>Minimax theory also guides the development of estimators that achieve these optimal rates. Examples like shrinkage estimators (e.g., James-Stein estimator in the finite normal means problem, and wavelet thresholding in the sequence space setting related to function estimation) are often motivated by minimax considerations, aiming to reduce risk compared to simpler estimators like the MLE, especially when many parameters are involved and some might be zero or small.</p>
<ol start="7" type="1">
<li>What role do orthogonal functions and wavelets play in nonparametric inference? Orthogonal functions (like Fourier series, cosine basis) and wavelets provide flexible bases for representing functions and distributions in nonparametric inference.</li>
</ol>
<p>Orthogonal Functions: By expanding an unknown function <img src="https://latex.codecogs.com/png.latex?r(x)"> in an orthonormal basis <img src="https://latex.codecogs.com/png.latex?%7B%5Cphi_j(x)%7D">, we can represent it by its coefficients <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j%20=%20%5Cint%20r(x)%20%5Cphi_j(x)%20dx">. The data can then be projected onto these basis functions, yielding noisy estimates of these coefficients. Nonparametric inference then proceeds by appropriately shrinking or selecting these coefficients. This approach transforms the problem of estimating a function into a problem of estimating a sequence of coefficients, which can be analyzed using techniques similar to the normal means problem. Modulator estimators, which apply weights <img src="https://latex.codecogs.com/png.latex?b_j"> to the estimated coefficients <img src="https://latex.codecogs.com/png.latex?Z_j">, are a common approach. The choice of basis can be tailored to the expected properties of the function (e.g., smoothness, periodicity).</p>
<p>Wavelets: Wavelets are basis functions that are localized in both time (or space) and frequency. This localization property makes them particularly well-suited for analyzing functions with local features like sharp changes or discontinuities. Wavelet methods involve decomposing the data into wavelet coefficients, applying a thresholding rule to these coefficients (to remove noise and capture important features), and then reconstructing the function from the thresholded coefficients. Wavelet shrinkage estimators like VisuShrink and SureShrink are popular nonparametric regression and density estimation techniques known for their adaptivity to varying smoothness of the underlying function. They can achieve near-optimal performance over a wide range of function spaces.</p>
<p>Both orthogonal functions and wavelets allow for a sparse representation of many signals and functions, meaning that many of their coefficients are close to zero. This sparsity is exploited in nonparametric methods to achieve efficient estimation and adaptivity.</p>
<ol start="8" type="1">
<li>What are some of the challenges and extensions in nonparametric inference discussed in the text? The text touches upon several challenges and extensions in nonparametric inference:</li>
</ol>
<p>Curse of Dimensionality: In multivariate settings, many nonparametric methods suffer from the curse of dimensionality, where the amount of data needed to achieve a certain level of accuracy increases exponentially with the number of predictor variables.</p>
<p>Measurement Error: When the predictor variables are measured with error, it can significantly affect the performance of nonparametric estimators. Deconvolution techniques are needed to correct for this bias.</p>
<p>Inverse Problems: These problems involve inferring an unknown function or parameter from indirect measurements. They are often ill-posed, requiring regularization techniques, many of which have nonparametric flavors.</p>
<p>Nonparametric Bayes: This area combines the flexibility of nonparametric models with the framework of Bayesian inference, using priors over infinite-dimensional spaces of functions or distributions (e.g., Dirichlet process mixtures).</p>
<p>Semiparametric Inference: These methods combine parametric and nonparametric components in a model. For example, one might model the effect of some covariates parametrically while leaving the effect of others unspecified.</p>
<p>Correlated Errors: The standard assumptions of independence often do not hold in practice. Nonparametric methods for data with correlated errors are more complex and less developed than those for independent data.</p>
<p>Classification: While the main focus is on regression and density estimation, nonparametric methods are also used for classification problems, such as <img src="https://latex.codecogs.com/png.latex?k">-nearest neighbors or kernel-based methods.</p>
<p>Shape-Restricted Inference: In some applications, prior knowledge about the shape of the function (e.g., monotonicity, convexity) is available. Nonparametric methods that incorporate these shape constraints can lead to improved estimates.</p>
<p>Testing: Nonparametric hypothesis testing aims to compare distributions or test for relationships without assuming specific parametric forms.</p>
<p>Computational Issues: Implementing nonparametric methods can be computationally intensive, especially for large datasets or complex techniques like bootstrap or wavelet analysis.</p>
<p>These topics highlight the ongoing research and the breadth of problems that nonparametric inference seeks to address, often requiring specialized techniques beyond the basic methodologies of regression and density estimation.</p>
</section>
<section id="some-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="some-thoughts">Some Thoughts</h2>
<p>The book provide a comprehensive overview of nonparametric inference, starting from fundamental concepts like cdf and functional estimation and progressing to advanced topics such as wavelet analysis, minimax theory, and specialized problems. The book emphasizes both the theoretical underpinnings and the practical application of various nonparametric methods for regression, density estimation, and other inference tasks. The inclusion of exercises at the end of each chapter suggests that the book is intended as a textbook for students and researchers in statistics and related fields.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Title},
  url = {https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Title.”</span> <a href="https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/">https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/</a>.
</div></div></section></div> ]]></description>
  <category>draft</category>
  <category>review</category>
  <guid>https://orenbochman.github.io/reviews/2006/all-of-nonparametric-statistics/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/images/lit-review-cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>🧠 Theory-Based Bayesian Models of Inductive Learning and Reasoning</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2006/griffiths-Bayesian-models-of-inductive-learning-and-reasoning/</link>
  <description><![CDATA[ 







<!-- VIDEOS GO HERE 

::: {.column-margin #fig-subtasks}


Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
:::

-->
<!-- A QUOTE by someone more famous than the author of the paper for context, add highlighting for emphasis, verse is a nice touch!  -->
<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/images/lit-review-cover.jpg" class="nolightbox img-fluid figure-img"></p>
<figcaption>cover</figcaption>
</figure>
</div></div><blockquote class="blockquote">
<p>“The ideal market completely disregards those spikes—but a realistic model cannot.” <mark>Mandelbrot highlights the inadequacy of models ignoring extreme price movements, emphasizing the need for a framework that can accommodate them.</mark></p>
</blockquote>
<!-- LEDE personal context why I reviewed this source 

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.

-->
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>TL;DR - Too Long; Didn’t Read about Inductive Learning
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Inductive Learning in a nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="Inductive Learning in a nutshell"></a></p>
<figcaption>Inductive Learning in a nutshell</figcaption>
</figure>
</div>
<p>Inductive learning can be used a framework for learning to reason about the world</p>
<!-- 1. What are the research questions? -->
<!-- 2. What are the main findings? -->
<!-- 3. In historical context why was this important? -->
</div>
</div>
<!-- 
Here is a lighthearted Deep Dive into the paper:

<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">
</source>
</audio>
-->
<section id="abstract" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. We argue that both components are necessary to explain the nature, use and acquisition of human knowledge, and we introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.</p>
<p>— <span class="citation" data-cites="tenenbaum2006theory">(Tenenbaum, Griffiths, and Kemp 2006)</span> <!--CITATION HERE--></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-tenenbaum2006theory" class="csl-entry">
Tenenbaum, Joshua B, Thomas L Griffiths, and Charles Kemp. 2006. <span>“Theory-Based Bayesian Models of Inductive Learning and Reasoning.”</span> <em>Trends in Cognitive Sciences</em> 10 (7): 309–18.
</div></div></div>
</section>
<section id="glossary" class="level2">
<h2 class="anchored" data-anchor-id="glossary">Glossary</h2>
<p>This paper uses lots of big terms so let’s break them down so we can understand them better</p>
<dl>
<dt>Inductive Inference</dt>
<dd>
The process of reasoning from specific observations or examples to more general conclusions or theories.
</dd>
<dt>Bayesian Inference</dt>
<dd>
A method of statistical inference in which Bayes’ theorem is used to update the probability for a hypothesis as more evidence or information becomes available.
</dd>
<dt>Prior Probability (Prior)</dt>
<dd>
The initial probability assigned to a hypothesis before any data is observed, reflecting prior knowledge or beliefs.
</dd>
<dt>Likelihood</dt>
<dd>
The probability of observing the given data under a specific hypothesis and background theory.
</dd>
<dt>Posterior Probability (Posterior)</dt>
<dd>
The updated probability of a hypothesis after considering the observed data, calculated using Bayes’ theorem (proportional to the product of the prior and the likelihood).
</dd>
<dt>Intuitive Theory</dt>
<dd>
A system of related concepts, causal laws, structural constraints, or explanatory principles that guide inductive inference in a particular domain of knowledge.
</dd>
<dt>Structured Probabilistic Model</dt>
<dd>
A formal representation of an intuitive theory that defines a probability distribution over possible observables, often based on a graph structure capturing relationships.
</dd>
<dt>Taxonomy</dt>
<dd>
A hierarchical system of classification, often represented as a tree structure, used to organize categories and concepts.
</dd>
<dt>Size Principle</dt>
<dd>
The tendency in Bayesian learning for smaller, more specific hypotheses to be increasingly preferred over larger, more general hypotheses as more randomly sampled examples are observed.
</dd>
<dt>Causal Graphical Model</dt>
<dd>
A probabilistic model that represents causal relationships between variables using a graph where nodes represent variables and edges represent direct causal influences.
</dd>
<dt>Causal Grammar</dt>
<dd>
A formal system, inspired by linguistic grammars, that generates causal graphical models based on an ontology and a set of causal laws.
</dd>
<dt>Hierarchical Bayesian Model</dt>
<dd>
A Bayesian model with multiple levels of parameters, where the priors for lower-level parameters are themselves governed by higher-level parameters, allowing for learning at different levels of abstraction.
</dd>
</dl>
</section>
<section id="outline" class="level2">
<h2 class="anchored" data-anchor-id="outline">Outline</h2>
<ul>
<li>Introduction
<ul>
<li>Describes the importance of inductive inference in human cognition, highlighting the challenge of generalizing from sparse data.</li>
<li>Mentions that traditional accounts of induction emphasize either statistical learning or domain-specific knowledge.</li>
<li>Argues for a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.</li>
</ul></li>
<li>Theory-based Bayesian Models
<ul>
<li>Introduces the core idea of theory-based Bayesian models, where prior knowledge shapes the hypothesis space and probability distributions.</li>
<li>Notes the distinction between domain-general statistical mechanisms and domain-specific knowledge representations.</li>
<li>Presents Bayes’ rule as a framework for combining prior knowledge (P(h|T)) and likelihood (P(x|h,T)) to calculate posterior probabilities (P(h|x,T)).</li>
<li>Emphasizes the role of domain theory in generating hypothesis spaces, prior probabilities, and likelihoods, forming a probabilistic version of intuitive theories.</li>
</ul></li>
<li>Learning Names for Things
<ul>
<li>Describes the classic category learning task as an abstraction of word learning for object kinds.</li>
<li>Presents limitations of traditional statistical models for word learning that assume simple notions of categories and label-category mappings.</li>
<li>Introduces a Bayesian model for word learning that utilizes a tree-structured taxonomy of objects as the hypothesis space of word meanings.</li>
<li>Discusses how the model explains children’s generalization patterns, showing that generalization follows a gradient according to taxonomic distance, which sharpens with more examples.</li>
<li>Mentions the “size principle” as a general principle of Bayesian learning, leading to the preference for smaller, more specific hypotheses with increasing evidence.</li>
</ul></li>
<li>Reasoning about Hidden Properties
<ul>
<li>Introduces the task of property induction, where learners must generalize a novel property observed in one or more categories to other categories.</li>
<li>Presents a theory-based Bayesian model for property induction in the domain of biological species, assuming a tree-structured taxonomy and a mutation process for property generation.</li>
<li>Highlights the model’s ability to account for generalizations of “blank” biological properties, surpassing models based on generic knowledge representations.</li>
<li>Discusses the need for different priors to account for generalizations of different kinds of predicates, such as anatomical, behavioral, and disease properties.</li>
<li>Presents examples of Bayesian models using different structured graphs and stochastic processes to capture the specific inductive biases for different property types.</li>
</ul></li>
<li>Learning Theories to Support Property Induction
<ul>
<li>Addresses the problem of learning the taxonomic tree structure from raw data (species-property pairs) using Bayesian inference.</li>
<li>Explains how the best tree structure maximizes the likelihood of the observed properties, reflecting the smooth variation of features over the tree.</li>
<li>Discusses the role of a “taxonomic principle” as an abstract domain principle that guides the learning process.</li>
<li>Notes that the Bayesian framework allows for learning abstract domain principles themselves, choosing the best structural form (e.g., tree, linear order, clusters) based on the trade-off between complexity and fit to the data.</li>
<li>Emphasizes the ability to incorporate explicit instructions at any level of abstraction within the hierarchical Bayesian framework, which can lead to dramatic changes in inferences.</li>
</ul></li>
<li>Causal Learning and Reasoning
<ul>
<li>Highlights the central role of causal cognition in intuitive theories, suggesting that causality is often seen as defining a theory.</li>
<li>Presents the idea of causal graphical models as a particular kind of structured probabilistic model used to make inferences about observable events.</li>
<li>Discusses the learning of causal models, contrasting bottom-up statistical cues with top-down approaches relying on abstract domain knowledge about causal mechanisms.</li>
<li>Introduces the concept of “causal grammars” as probabilistic models that generate causal graphical models based on an ontology and a set of causal laws.</li>
<li>Provides examples of theory-based Bayesian causal induction, showing how abstract knowledge about risk factors, diseases, and symptoms can guide the learning of causal networks in a medical domain, and how domain principles such as the “activation law” can explain causal inferences in the “blicket detector” paradigm.</li>
</ul></li>
<li>Learning Abstract Causal Theories
<ul>
<li>Acknowledges the open question of how abstract causal principles or “framework theories” are learned.</li>
<li>Mentions the infinite relational model (IRM) as an example of a Bayesian model that can infer the number of classes, class membership, and relationships between classes from data.</li>
<li>Notes that while IRM can learn some simple framework knowledge, more powerful methods for learning in probabilistic logical systems are needed to account for richer causal theories.</li>
</ul></li>
<li>Conclusion
<ul>
<li>Reiterates the potential of the theory-based Bayesian framework for understanding human cognition, emphasizing the integration of both sophisticated inference processes and knowledge representations.</li>
<li>Acknowledges the limitations of current models and the need to address algorithmic and psychological plausibility issues.</li>
<li>Concludes that probabilistic inference over hierarchies of increasingly abstract and flexibly structured representations is a crucial idea for explaining inductive learning and reasoning.</li>
</ul></li>
</ul>
</section>
<section id="reflection-on-the-paper-and-beyond." class="level2">
<h2 class="anchored" data-anchor-id="reflection-on-the-paper-and-beyond.">Reflection on the paper and beyond.</h2>
<p>This seems to be one of the way to capture the issues of aggregation rules for signals in the transitions from simple to complex signaling systems.</p>
<p>In reality these are two different things</p>
<ol type="1">
<li>learning a model from some state space.</li>
<li>aggregating symbols using a rule like a grammar.</li>
</ol>
<p>The two problems are related but only tangentially. This is one reason that transitioning from the simple Lewis signaling model to the more complex ones is so difficult.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {🧠 {Theory-Based} {Bayesian} {Models} of {Inductive}
    {Learning} and {Reasoning}},
  url = {https://orenbochman.github.io/reviews/2006/griffiths-Bayesian-models-of-inductive-learning-and-reasoning/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“🧠 Theory-Based Bayesian Models of Inductive
Learning and Reasoning.”</span> <a href="https://orenbochman.github.io/reviews/2006/griffiths-Bayesian-models-of-inductive-learning-and-reasoning/">https://orenbochman.github.io/reviews/2006/griffiths-Bayesian-models-of-inductive-learning-and-reasoning/</a>.
</div></div></section></div> ]]></description>
  <category>review</category>
  <category>induction</category>
  <guid>https://orenbochman.github.io/reviews/2006/griffiths-Bayesian-models-of-inductive-learning-and-reasoning/</guid>
  <pubDate>Mon, 01 Sep 2025 01:57:50 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/images/lit-review-cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>2BP: 2-Stage Backpropagation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/two-stage-backpropagation/</link>
  <description><![CDATA[ 






<p>in <span class="citation" data-cites="shyam2024treeattentiontopologyawaredecoding">(Shyam et al. 2024)</span> the authors …</p>
<div class="no-row-height column-margin column-container"><div id="ref-shyam2024treeattentiontopologyawaredecoding" class="csl-entry">
Shyam, Vasudev, Jonathan Pilault, Emily Shepperd, Quentin Anthony, and Beren Millidge. 2024. <span>“Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters.”</span> <a href="https://arxiv.org/abs/2408.04093">https://arxiv.org/abs/2408.04093</a>.
</div></div><p>As Deep Neural Networks (DNNs) grow in size and complexity, they often exceed the memory capacity of a single accelerator, necessitating the sharding of model parameters across multiple accelerators. Pipeline parallelism is a commonly used sharding strategy for training large DNNs. However, current implementations of pipeline parallelism are being unintentionally bottlenecked by the automatic differentiation tools provided by ML frameworks. This paper introduces 2-stage backpropagation (2BP). By splitting the backward propagation step into two separate stages, we can reduce idle compute time. We tested 2BP on various model architectures and pipelining schedules, achieving increases in throughput in all cases. Using 2BP, we were able to achieve a 1.70x increase in throughput compared to traditional methods when training a LLaMa-like transformer with 7 billion parameters across 4 GPUs.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {2BP: {2-Stage} {Backpropagation}},
  url = {https://orenbochman.github.io/reviews/2024/two-stage-backpropagation/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“2BP: 2-Stage Backpropagation.”</span> <a href="https://orenbochman.github.io/reviews/2024/two-stage-backpropagation/">https://orenbochman.github.io/reviews/2024/two-stage-backpropagation/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/two-stage-backpropagation/</guid>
  <pubDate>Invalid Date</pubDate>
</item>
<item>
  <title>Vibe coding GPT5 Edition</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/posts/2025/2025-09-26-vibe-coding/</link>
  <description><![CDATA[ 






<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/banner_wallnuts.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="will the the brains of GPT-5 will drive you nuts?"><img src="https://orenbochman.github.io/images/banner_wallnuts.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="600" alt="will the the brains of GPT-5 will drive you nuts?"></a></p>
</figure>
</div>
<figcaption>will the the brains of GPT-5 will drive you nuts?</figcaption>
</figure>
</div>
<p>A few days back, a friend discussed on FB his attempt at a mathematical puzzle and the outcomes of using a commercial LLM and a self-hosted LLM to solve it, and he then raised a couple of questions as to why the LLM was able to get the result so much faster and better than he could.</p>
<p>My response was too long and is best published here.</p>
<section id="why-are-the-llms-so-good-at-coding-and-reasoning-about-code" class="level2">
<h2 class="anchored" data-anchor-id="why-are-the-llms-so-good-at-coding-and-reasoning-about-code">Why are the LLMs so good at coding and reasoning about code?</h2>
<ol type="1">
<li>LLMs are trained on lots of code and problems. More so to boost their overall reasoning benchmarks. Some LLMs may have seen a substantial percentage of all code written in history.</li>
<li>The deeper reasons for this are that <mark>code is much easier to do next token prediction than natural language</mark>. Intuitively, the number of <strong>legit next token</strong> by class is minimal. Additionally, the number of variables that need to be kept in context is usually tiny for most algorithms, even if complete programs have many variables. Even so, the full number of free variables in a regular program is many orders of magnitude smaller than the words in a natural language text of similar length.</li>
<li>Also, there are some areas where LLMs are not so good at coding. When a library or API undergoes breaking changes, it becomes asymptotically more likely that the LLM will make mistakes.</li>
<li>LLMs are generalists, and there is substantial knowledge on making LLMs that are much better specialists. If you use LLMs through APIs, you may have seen how many coding specialists are out there already.</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>overthinking bugs
</div>
</div>
<div class="callout-body-container callout-body">
<p>Reinforcement Learning (RL) is one area where a language model can utilize a tool to learn in a closed-loop environment, ensuring its code compiles, passes tests, is well-documented, and adheres to a set of conventions.</p>
<p>So, I won’t be surprised to see this area continue to improve over time. However, for now, if relevant, requesting a single page of code with no APIs can yield fantastic results.</p>
</div>
</div>
</section>
<section id="vibe-coding-and-tough-problems" class="level2">
<h2 class="anchored" data-anchor-id="vibe-coding-and-tough-problems">Vibe coding and Tough problems</h2>
<p>Around the time <img src="https://latex.codecogs.com/png.latex?GPT_5"> was launched. More recently, I’ve made an effort to solve a large number of graduate-level questions from graduate or PhD-level textbooks in statistics, price theory, and RL. These were recommended in the five specializations I took (NLP, RL, Bayesian statistics, Kalman filters, and Microeconomics), as well as in some real-world data science problems from my work that require these skills. Anyhow, <img src="https://latex.codecogs.com/png.latex?GPT_5"> can <del>solve</del> <em>supply answers that are not obviously wrong</em> for many of these problems where its predecessors failed dismally.</p>
<p>In fact, my early strategy for vibe coding, combined with improvements in memory across sessions, suggests that repeated failures on complex problems, followed by simpler cases, often elicit an MVP solution for the full problem.</p>
<p>For example, <img src="https://latex.codecogs.com/png.latex?GPT_5"> failed to provide LaTeX derivations of advanced Bayesian statistics with Rationals summarized in under five words on the right-hand side. However, after a week of failures, it began to offer derivations on its own. Unexpectedly, each step has an explanation. Thorny quantities that the professor didn’t name in the code or the course are being given their correct technical terms. I don’t know how this happens, but I think it can recall the few sessions where I pasted the derivations I had stitched together and cleaned up, plus the code sessions I bug-fixed,</p>
<p>To get back on point, both <img src="https://latex.codecogs.com/png.latex?GPT_%7B3:4%7D"> were able to reason much better in code, YAML, and now in Markdown than in natural languages. <img src="https://latex.codecogs.com/png.latex?GPT_5"> still has tokenizer issues, but if I implement some workarounds in the prompt, the time savings can be dramatic.</p>
</section>
<section id="new-old-prompting-strategies" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="new-old-prompting-strategies">New &amp; Old prompting strategies</h2>
<section id="peeking-at-the-sources" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="peeking-at-the-sources">Peeking at the Sources</h3>
<p>Growing ire with <img src="https://latex.codecogs.com/png.latex?GPT_3"> pridilications for BS and my background with Wikipedia led me to add to my preferences a request for reputable sources for any significant claim. I also requested that it provide the consensus view to mitigate its tendency, as seen in <img src="https://latex.codecogs.com/png.latex?GPT_3:4">, to pander to its users at the expense of factual correctness.</p>
<p>This was largely ignored at the time, but later on, <img src="https://latex.codecogs.com/png.latex?GPT_3:4"> would undermine my credibility by furnishing its replies with fake sources that included time-wasting or potentially malicious URLs<sup>1</sup>.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I do believe it picked this habit from Wikipedia-based spammers</p></div></div><p>At some point, I started to hear about the consensus view from sources X, Y, and Z; then <img src="https://latex.codecogs.com/png.latex?GPT_5"> began to provide excellent sources in a sidebar. Looking at these, though, many are totally irrelevant. Asking suggests that <img src="https://latex.codecogs.com/png.latex?GPT_5"> didn’t consult the sources except perhaps the top three sources.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Overthinking reputable sources
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>What is happening here? What it is ain’t exactly clear…</p>
</blockquote>
<p>I have thought about and even designed systems that store and utilize sources. There are many possible solutions, but one idea I had is citation embeddings. Create special tokens that embed the citation in the text (within each context window).</p>
<p>Learning about citations is an emergent ability for language models, but this is usually superficial. Particularly if we consider that Wikidata has several million citations on record as linked data, and the web likely has many more.</p>
<p>What makes search engines work well is an inverse index. AFAIK, LLMs are not trying to mimic this feature, but they are definitely trying to improve information retrieval across the deep network</p>
<p>It appears that <img src="https://latex.codecogs.com/png.latex?GPT_5"> can respond to the request in my prompt regarding consensus views, as it keeps referencing an older textbook I never read. Is this for real? Like everything else with LLMs, it is hard to say. But the sources may contain URLs that have been accessed via training or search. Either way, <span class="marked">we should verify</span><sup>2</sup>, that it actually knows where the full text of the books we are discussing is available online. Or that there is an earlier, more accessible copy on one of the author’s web pages.</p>
<p>One last point, I gave notebook LM my website URL as a source, and it became a top source. This quelled my blind trust in the sources once again.</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;evey time you verify you delay LLM related brain rot</p></div></div></section>
<section id="reasoning-summary" class="level3">
<h3 class="anchored" data-anchor-id="reasoning-summary">Reasoning summary</h3>
<ol type="1">
<li>Almost all of my prompts lead to long reasoning chains. I started peeking at these messages. I was skeptical of these, as in many cases, these were made-up outputs with mostly coincidental relation to the actual chat session.</li>
<li>I don’t really know at this point if there is any better relevance to these notes or not. In the back of my mind, I suspect that this is primarily a trick to slow down queries or to make users wait for a much slower GPU allocation.<br>
</li>
<li>What is clear from reading just a few plans:
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?GPT_5"> consistently claims it can’t access the pages I give it (but only in the plan in the chat session, this failure is not mentioned, leading to a <em>zero support hallucination</em>. The reasoning summary can bridge the gap before wasting time on iterating on a hallucination.</li>
<li><img src="https://latex.codecogs.com/png.latex?GPT_5"> states its assumptions about my prompt. These are often not what I had in mind in two ways. Correcting a misconception or adding a missing fact is good practice - one of the advantages of a <img src="https://latex.codecogs.com/png.latex?GPT_?"> is to get you to train to think more precisely.</li>
<li>Frequently, the plan expands my initial prompt with ambitious sub-goals to do more than requested (e.g., <em>provide extra metrics I had to look up</em>), but in practice, the actual response doesn’t enact any of these planned extras or even much of the request. (This is evidence that the plans are fake or that they don’t really correlate with how the LLM is actually reasoning.)</li>
</ol></li>
</ol>
</section>
<section id="becoming-a-master-of-your-own-domain-and-reading-between-the-lines" class="level3">
<h3 class="anchored" data-anchor-id="becoming-a-master-of-your-own-domain-and-reading-between-the-lines">Becoming a master of your own domain and reading between the lines</h3>
<blockquote class="blockquote">
<p>‘Teutch who directed the intricacies of a private infinity’– Rhialto the Marvelous by Jack Vance</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?GPT_%7B%5B3:5%5D%7D"> often hints that you know you are making mistakes. But it also has few, if any, qualms in humoring you if you insist you are correct. Ignore these hints, and you will quickly become the master of your own infinity. I.E., you will interact with the <img src="https://latex.codecogs.com/png.latex?GPT_?"> in a domain of knowledge that is essentially your own creation, and after you do a reality check, you will have to go back to the point where <img src="https://latex.codecogs.com/png.latex?GPT_?"> suggested it knew better and start over from there.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Since you made it this far, here are some conclusions.</p>
<p>We need to be even more skeptical of GPT outputs than before. Research is conclusive that as you feel smarter by asking GPT about black holes and Crypto, <mark>you are actually getting dumber</mark>. Try to engage more in conversations with actual intelligent people as much as you do with the GPT.</p>
<p><mark>Language models were invented to correct spelling mistakes. We scaled them up and found they could do many things, but anything else they do that isn’t correcting typos is suspect.</mark></p>
<p>Ultimately, GPT can either waste your time or boost your productivity, and the choice is yours. I’ve provided you with some tips here, but you’d be best served by considering them as such, rather than wasting time with <img src="https://latex.codecogs.com/png.latex?GPT_5"> .</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {Vibe Coding {GPT5} {Edition}},
  date = {2025-09-25},
  url = {https://orenbochman.github.io/posts/2025/2025-09-26-vibe-coding/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“Vibe Coding GPT5 Edition.”</span> September
25, 2025. <a href="https://orenbochman.github.io/posts/2025/2025-09-26-vibe-coding/">https://orenbochman.github.io/posts/2025/2025-09-26-vibe-coding/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/posts/2025/2025-09-26-vibe-coding/</guid>
  <pubDate>Wed, 24 Sep 2025 21:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/images/calabi_yau_manifold.png" medium="image" type="image/png" height="144" width="144"/>
</item>
</channel>
</rss>
