---
date: 2022-05-01
lastmod: 2024-05-06
title: Course Into 
subtitle: RL Fundamentals
description: In week 1 we define some key concepts like rewards, states, action, value functions, action values functions. We consider the the multi-armed bandit problem, leading to exploration explotation dillema, and the epsilon greedy algorithm.
author: Oren Bochman
categories:
  - Coursera
  - notes
  - rl
  - reinforcement learning
  - the k-armed bandit problem
  - bandit algorithms
  - exploration 
  - explotation
  - epsilon greedy algorithm
  - sample avarage method
jupyter: python3
image: /images/nlp-brain-wordcloud.jpg
---

![RL logo](img/logo.png){.column-margin} ![RL algorithms](img/alg_selector.jpeg){.column-margin}

# Intro

The teachers are world class, researches from the best university for this subject. However, this high level comes with the following caveat: their delivery is always terse and precise. They frequently reference old material which you may not have *fully digested* yet. In fact I discovered to my üò± horror how easy it is go through the material, quizzes and programming assignments scoring 100% but not connect the dots.

::: callout-caution
## Deceptively simple :bulb: {.unnumbered}

[This course is **deceptively simple**]{.mark} - the chart in the margin provides a great summary of the material for the whole specialization. Only a handful of concepts are needed to master RL.

-   This specialization is all about connecting the dots.
-   We revisit the same ideas over and over improving them in small but significant ways by relaxing the assumptions. e.g. from bandits with one state we move to MDP with many states and get the ability to formulate plans. From Dynamic programming with a fully specified model we move to model free settings where we might not be able to efficiently learn a model. From tabular methods where we treat each state as a separate entity we we move to function approximation and deep learning where we can generalize from one state to many others.
-   In this course and the more connections you make the better you will understand and remember material.
-   And the greater you facility to apply RL to new problems.
:::

The following are my tips for getting the most from this specialization

::: callout-tip
## Connecting The Dot to see the Forest For the Trees üéÑ 

To connect the dots I :heart: recommend:

1.  **Annotate** üñäÔ∏è you e-copy of the book üìñ
2.  **Flash cards** üóÇÔ∏è are your üßë‚Äçü§ù‚Äçüßë friends. We don't need too many but they can help you keep the essentials (algorithms, definitions, some formulas, a few diagrams) fresh in your mind.
3.  **Review** üëÅÔ∏è the videos/quizzes until nothing seems surprising/confusing [^1].
4.  **Review** üëÅÔ∏è your notes every time you complete a part of the specialization. Also a great idea if have an RL interview üíº
5.  **Coding**: If you have time do extra RL coding
    1.  Start with developing more environments, simple and complex ‚õ©Ô∏è
    2.  Implement more algorithms - from the course, the books, papers.‚õ©Ô∏è
    3.  The notebooks also try to teach you experiments and analysis comparing algorithms performance. If you assimilate this part you are really going to shine. ‚õ©Ô∏è
:::

[^1]: The annotated book and flashcards will help here. This material is really logical - if you are surprised/confused you never assimilated some part of the material. Once you do it should become almost intuitive to reason about from scratch.

::: callout-tip
## Mathematical Mnemonics üòç

As a Mathematics major I can attest that Mathematics becomes 10x easier so long as you can recall üß† the basic definitions and their notation.

I have extracted the essentials from the text book below. Best to memorize these or at least keep a copy handy and you are well on your way to grokking this course

-   $G_t$ **return** at time t, for a $(s_t,a_t,r_t...)$ sequence discounted by $\gamma\in(0,1)$.
-   $r(s,a)$ - **expected immediate rewards** for action $a$ in state $s$
-   $\pi$ **policy** - a decision making rule for every state.
-   $\pi_*$ **optimal policy** - which returns the maximum rewards.
-   $p(s',r \vert s,a)$ - **transition probability** to state $s'$ with reward $r$ from state $s$ via action $a$ AKA **four valued dynamics** function.
-   $p(s' \vert s,a)$ - **transition probability** to state $s'$ from state $s$ via action $a$ AKA **Markov process transition matrix**
-   $v_\pi(s)$ - state's **value** under policy $\pi$ which is its expected return.
-   $q_\pi(s,a)$ - the **action value** in state $s$ under policy $\pi$.
:::
