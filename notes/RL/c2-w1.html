<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="description" content="In this module we learn about Sample based MC methods that allow learning from sampled episodes. We revise our initial algorithm to better handle exploration. In off policy RL we see methods to learn a policy using samples from another policy, corrected using importance sampleing.">

<title>Oren Bochman's Blog ‚Äì Sample-based Learning Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<style>

      .quarto-title-block .quarto-title-banner {
        background: /images/banner_black_3.jpg;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="twitter:title" content="Oren Bochman‚Äôs Blog - Sample-based Learning Methods">
<meta name="twitter:description" content="In this module we learn about Sample based MC methods that allow learning from sampled episodes. We revise our initial algorithm to better handle exploration. In off policy RL we see methods to learn a policy using samples from another policy, corrected using importance sampleing.">
<meta name="twitter:image" content="https://orenbochman.github.io//images/nlp-brain-wordcloud.jpg">
<meta name="twitter:creator" content="@orenbochman">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Oren Bochman‚Äôs Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-about" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">about</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-about">    
        <li>
    <a class="dropdown-item" href="../../about.html">
 <span class="dropdown-text">About</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-notes" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">notes</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-notes">    
        <li>
    <a class="dropdown-item" href="../../nlp.html">
 <span class="dropdown-text">NLP Specilization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../dnn.html">
 <span class="dropdown-text">Neural Networks for Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../model-thinking.html">
 <span class="dropdown-text">Model Thinking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../xai.html">
 <span class="dropdown-text">XAI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../rl.html">
 <span class="dropdown-text">Reinforcement Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../rhetoric.html">
 <span class="dropdown-text">Rhetoric</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../tfp.html">
 <span class="dropdown-text">TFP</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../ab-testing.html">
 <span class="dropdown-text">AB testing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../cognitiveai.html">
 <span class="dropdown-text">Cognitive AI</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/orenbochman"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/OrenBochman/blog">
 <span class="dropdown-text">Source Code</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/OrenBochman/blog/issues">
 <span class="dropdown-text">Report a Bug</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../archive.html"> <i class="bi bi-archive" role="img">
</i> 
<span class="menu-text">Archive</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Sample-based Learning Methods</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Sample-based Learning Methods</h1>
            <p class="subtitle lead">Monte-Carlo Methods for Prediction &amp; Control</p>
                  <div>
        <div class="description">
          In this module we learn about Sample based MC methods that allow learning from sampled episodes. We revise our initial algorithm to better handle exploration. In off policy RL we see methods to learn a policy using samples from another policy, corrected using importance sampleing.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Coursera</div>
                <div class="quarto-category">notes</div>
                <div class="quarto-category">rl</div>
                <div class="quarto-category">reinforcement learning</div>
                <div class="quarto-category">Monte-Carlo methods</div>
                <div class="quarto-category">Any visit Monte-Carlo prediction</div>
                <div class="quarto-category">First visit Monte-Carlo prediction</div>
                <div class="quarto-category">Monte-Carlo with Exploring Starts GPI</div>
                <div class="quarto-category">Exploring Starts</div>
                <div class="quarto-category">Monte-Carlo with ∆ê-soft GPI</div>
                <div class="quarto-category">∆ê-soft policies</div>
                <div class="quarto-category">Off-policy learning</div>
                <div class="quarto-category">Importance sampling</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Friday, March 1, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#lesson-1-introduction-to-monte-carlo-methods" id="toc-lesson-1-introduction-to-monte-carlo-methods" class="nav-link active" data-scroll-target="#lesson-1-introduction-to-monte-carlo-methods">Lesson 1: Introduction to Monte-Carlo Methods</a></li>
  <li><a href="#sec-mc-control" id="toc-sec-mc-control" class="nav-link" data-scroll-target="#sec-mc-control">Lesson 2: Monte Carlo for Control</a>
  <ul class="collapse">
  <li><a href="#sec-l2g1" id="toc-sec-l2g1" class="nav-link" data-scroll-target="#sec-l2g1">MC Action-Value Functions</a></li>
  </ul></li>
  <li><a href="#lesson-3-exploration-methods-for-monte-carlo" id="toc-lesson-3-exploration-methods-for-monte-carlo" class="nav-link" data-scroll-target="#lesson-3-exploration-methods-for-monte-carlo">Lesson 3: Exploration Methods for Monte Carlo</a>
  <ul class="collapse">
  <li><a href="#lesson-4-off-policy-learning-for-prediction" id="toc-lesson-4-off-policy-learning-for-prediction" class="nav-link" data-scroll-target="#lesson-4-off-policy-learning-for-prediction">Lesson 4: Off-policy learning for prediction</a>
  <ul class="collapse">
  <li><a href="#sec-l4g1" id="toc-sec-l4g1" class="nav-link" data-scroll-target="#sec-l4g1">Off-policy learning</a></li>
  <li><a href="#sec-l4g2" id="toc-sec-l4g2" class="nav-link" data-scroll-target="#sec-l4g2">Target and behavior policies</a></li>
  <li><a href="#sec-l4g3" id="toc-sec-l4g3" class="nav-link" data-scroll-target="#sec-l4g3">Importance sampling</a></li>
  <li><a href="#sec-l4g4" id="toc-sec-l4g4" class="nav-link" data-scroll-target="#sec-l4g4">Importance sampling ratio</a></li>
  <li><a href="#emma-brunskill-batch-reinforcement-learning" id="toc-emma-brunskill-batch-reinforcement-learning" class="nav-link" data-scroll-target="#emma-brunskill-batch-reinforcement-learning">Emma Brunskill: Batch Reinforcement Learning</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<style>

.alg  {
  border-left-color: pink;
}

.alg ul {
    list-style-type: none; /* Remove bullets */
}

@counter-style repeating-emoji {
  system: cyclic;
  symbols: "üå∞" "ü•ú" "ü•î" "ü••"; // unicode code point
  suffix: " ";
}

.tldr ul {
  list-style-type: repeating-emoji;

}


</style>
<div class="tldr callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TLDR ü•úü•úü•ú
</div>
</div>
<div class="callout-body-container callout-body">

<ul>
<li>In this module we will embrace the paradigm of ‚Äúlearning from experience‚Äù.</li>
<li>This is called Sample based Reinforcement Learning and it we will let us relax some strong of the requirements of dynamic programming, namely knowing the table of MDP dynamics.</li>
<li>We will first use efficient Monte-Carlo ‚öÖüÉÅ methods for üîÆ prediction problem of estimating <span class="math inline">v_\pi(S)</span> value functions and action‚Äìvalue functions <span class="math inline">q_\pi(a)</span> from sampled episodes.</li>
<li>We will revise our algorithm to better handle exploration using exploring starts and <span class="math inline">\epsilon</span>‚Äìsoft policies.</li>
<li>We will adapt GPI algorithms for use with Mote-Carlo to solve the üéÆ control problem of policy improvement.</li>
<li>With off policy learning learn a policy using samples from another policy, by corrected using importance sampling.</li>
</ul>
</div>
</div>
<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center callout-margin-content">
<figure class="figure">
<p><a href="img/alg_selector.jpeg" class="lightbox" data-gallery="slides" title="RL algorithms"><img src="img/alg_selector.jpeg" class="img-fluid figure-img" alt="RL algorithms"></a></p>
<figcaption>RL algorithms</figcaption>
</figure>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reading
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked=""><a href="http://incompleteideas.net/book/RLbook2020.pdf#page=91">RL Book¬ß5.0-5.5 (pp.91-104)</a></label></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definitions
</div>
</div>
<div class="callout-body-container callout-body">
<div id="dfn-action-value">
<dl>
<dt>Action Value Function</dt>
<dd>
<p><span class="math inline">q_\pi(a) \doteq \mathbb{E}[G_t \vert A_t=a] \space \forall a \in \{a_1 ... a_k\}</span></p>
</dd>
</dl>
</div>
<div id="dfn-bootstrap">
<dl>
<dt>Bootstrapping</dt>
<dd>
<p>‚Äúlearning by guessing from a guess‚Äù or more formally</p>
<p>the process of updating an estimate of the value or action-value function based on other estimated values. It involves using the current estimate of the value function to update and improve the estimate itself.</p>
</dd>
</dl>
</div>
<div id="dfn-control">
<dl>
<dt>Control</dt>
<dd>
<p>to approximate optimal policies using the DP approach of GPI</p>
</dd>
</dl>
</div>
<div id="dfn-epsilon-soft">
<dl>
<dt>Epsilon Soft Policy</dt>
<dd>
<p>A policy in which each possible action is assigned at least <span class="math inline">\epsilon / |A|</span> probability.</p>
</dd>
</dl>
</div>
<div id="dfn-exploring-starts">
<dl>
<dt>Exploring Starts</dt>
<dd>
<p>Learning the value or action values of a policy by starting in each action value state at least once.</p>
</dd>
</dl>
</div>
<div id="dfn-mc">
<dl>
<dt>Monte-Carlo Methods</dt>
<dd>
<p>Estimation methods which relies on repeated random sampling. Also see <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte-Carlo methods <i class="bi bi-wikipedia"></i></a></p>
</dd>
</dl>
</div>
<div id="dfn-on-policy-learning">
<dl>
<dt>On-policy learning</dt>
<dd>
<p>learning a policy <span class="math inline">\pi</span> by sampling from <span class="math inline">\pi</span></p>
</dd>
</dl>
</div>
<div id="dfn-off-policy-learning">
<dl>
<dt>Off-policy learning</dt>
<dd>
<p>learning a policy <span class="math inline">\pi</span> by sampling from some other policy <span class="math inline">\pi'</span></p>
</dd>
</dl>
</div>
<div id="dfn-prediction">
<dl>
<dt>Prediction</dt>
<dd>
<p>Estimating <span class="math inline">v_\pi(s)</span> is called policy evaluation in the DP literature.</p>
<p>We also refer to it as the Prediction problem <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</dd>
</dl>
</div>
<div id="dfn-return">
<dl>
<dt>Return (<span class="math inline">G_t</span>)</dt>
<dd>
<p><span class="math inline">G_0 \doteq R_1+ \gamma^1 R_2 + \cdots+ \gamma^n R_n</span></p>
<p>i.e.&nbsp;the discounted sum of future rewards</p>
</dd>
</dl>
</div>
<div id="dfn-tqabular">
<dl>
<dt>Tabular methods</dt>
<dd>
<p>RL methods for which the action-values can be represented by a table</p>
</dd>
</dl>
</div>
<div id="dfn-value">
<dl>
<dt>Value Function <span class="math inline">v_\pi(s)</span></dt>
<dd>
<p><span class="math inline">v_\pi(s) \doteq \mathbb{E}[G_t|S_t=s]</span></p>
<p>i.e.&nbsp;a state‚Äôs value is its expected return</p>
</dd>
</dl>
</div>
</div>
</div>
<ul>
<li>Sample based methods learning from experience, without having prior knowledge of the underlying MDP model.</li>
<li>We will cover tabular methods in which the action-values can be represented by a table.</li>
</ul>
<section id="lesson-1-introduction-to-monte-carlo-methods" class="level1 page-columns page-full">
<h1>Lesson 1: Introduction to Monte-Carlo Methods</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand how <a href="#dfn-mc">Monte-Carlo</a> can be used to estimate <span class="math inline">v(s)</span> value functions from sampled interaction</label></li>
<li><label><input type="checkbox" checked="">Identify problems that can be solved using <a href="#dfn-mc">Monte-Carlo</a> methods</label></li>
<li><label><input type="checkbox" checked="">Use <a href="#dfn-mc">Monte-Carlo</a> prediction to estimate the value function for a given policy.</label></li>
</ul>
</div>
</div>
<ul>
<li>After completing the introduction we all think that MDPs and DP are the best?</li>
<li>Alas, Martha burst this bubble, introducing some shortcomings of DP, namely they require us to know a model of the dynamics <span class="math inline">p(s,a|s',r)</span> and rewards <span class="math inline">r</span> of the MDP to estimate <span class="math inline">v(s)</span> or <span class="math inline">q(a)</span>.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-mc-methods.png" class="lightbox" data-gallery="slides" title="MC methods for Policy evaluation"><img src="img/rl-mc-methods.png" class="img-fluid figure-img" alt="MC methods for Policy evaluation"></a></p>
<figcaption>MC methods for Policy evaluation</figcaption>
</figure>
</div></div><p>let us now try to understand how <a href="#dfn-mc">Monte-Carlo</a> can be used to estimate <span class="math inline">v(s)</span> value functions from sampled interaction.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-mc-12-dice.png" class="lightbox" data-gallery="slides" title="12 dice"><img src="img/rl-mc-12-dice.png" class="img-fluid figure-img" alt="12 dice"></a></p>
<figcaption>12 dice</figcaption>
</figure>
</div></div><div id="exm-dp-dice" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Rolling 12 Dice)</strong></span> &nbsp;</p>
<ul>
<li>Say our MDP requires rolling 12 dice.
<ul>
<li>this is probably intractable to estimate theoretically using DP.</li>
<li>this is likely to be error prone (particularly and constitutionally).</li>
<li>this will be easy to estimate using MC methods</li>
</ul></li>
</ul>
</div>
<ul>
<li>For most MDPs knowing the dynamics and rewards is an unreasonably strong requirement.</li>
<li>If we can treat this like a bandit problem we can try to use the long term averages rewards to estimate value of a state</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-mc-bandit.png" class="lightbox" data-gallery="slides" title="MC bandits"><img src="img/rl-mc-bandit.png" class="img-fluid figure-img" alt="MC bandits"></a></p>
<figcaption>MC bandits</figcaption>
</figure>
</div></div><p>more formally we can use the MC value prediction algorithm.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-mc-alg.png" class="lightbox" data-gallery="slides" title="MC value prediction algorithm any visit"><img src="img/rl-mc-alg.png" class="img-fluid figure-img" alt="MC value prediction algorithm any visit"></a></p>
<figcaption>MC value prediction algorithm any visit</figcaption>
</figure>
</div></div><div id="nte-mc-value-prediction-any-visit" class="alg callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;1: MC prediction any visit for estimating <span class="math inline">V \approx v_\pi</span>
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Input: a policy <span class="math inline">\pi</span> to be evaluated</li>
<li>Initialize:
<ul>
<li><span class="math inline">V(s) \in \mathbb{R}</span>, arbitrarily, <span class="math inline">\forall s \in S</span></li>
<li>Returns(s) an empty list, for all <span class="math inline">s \in S</span></li>
</ul></li>
<li>Loop forever (for each episode):
<ul>
<li>Generate an episode following <span class="math inline">\pi: S_0, A_0, R_1, S_1, A_1, R_2,\ldots, S_{T-1}, A_{T-1}, R_T</span></li>
<li><span class="math inline">G \leftarrow 0</span></li>
<li>Loop for each step of episode, <span class="math inline">t = T-1, T-2,..., 0</span>:
<ul>
<li><span class="math inline">G \leftarrow \gamma G + R_{t+1}</span></li>
<li>Append <span class="math inline">G</span> to <span class="math inline">Returns(S_t)</span></li>
<li><span class="math inline">V(S_t) \leftarrow average(Returns(S_t))</span></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-mc-first-visit-prediction.png" class="lightbox" data-gallery="slides" title="MC value prediction algorithm first visits"><img src="img/rl-mc-first-visit-prediction.png" class="img-fluid figure-img" alt="MC value prediction algorithm first visits"></a></p>
<figcaption>MC value prediction algorithm first visits</figcaption>
</figure>
</div></div><div id="nte-mc-value-prediction-first-visit" class="alg callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note&nbsp;2: MC prediction fist visit for estimating <span class="math inline">V \approx v_\pi</span>
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Input: a policy <span class="math inline">\pi</span> to be evaluated</li>
<li>Initialize:
<ul>
<li><span class="math inline">V(s) \in \mathbb{R}</span>, arbitrarily, <span class="math inline">\forall s \in S</span></li>
<li>Returns(s) an empty list, for all <span class="math inline">s \in S</span></li>
</ul></li>
<li>Loop forever (for each episode):
<ul>
<li>Generate an episode following <span class="math inline">\pi: S_0, A_0, R_1, S_1, A_1, R_2,\ldots, S_{T-1}, A_{T-1}, R_T</span></li>
<li><span class="math inline">G \leftarrow 0</span></li>
<li>Loop for each step of episode, <span class="math inline">t = T-1, T-2,..., 0</span>:
<ul>
<li><span class="math inline">G \leftarrow \gamma G + R_{t+1}</span></li>
<li>Unless <span class="math inline">S_t</span> appears in <span class="math inline">S_0, S_1,\ldots,S_{t-1}</span>:
<ul>
<li>Append <span class="math inline">G</span> to <span class="math inline">Returns(S_t)</span></li>
<li><span class="math inline">V(S_t) \leftarrow average(Returns(S_t))</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Any visit / First-visit
</div>
</div>
<div class="callout-body-container callout-body">
<p>The book uses presents a small variation called the <em>first visit MC method</em>, We considered the any-visit case. This estimates <span class="math inline">v_\pi(s)</span> using the average of the returns following an episode‚Äôs first visit to <span class="math inline">s</span>, whereas this the every-visit MC alg averages the returns following all visits to <span class="math inline">s</span></p>
<p>Although it not so clear I added the line excluding the later visits in struckout</p>
</div>
</div>
<p>What lies behind the algorithm is the following technique for efficently computing returns.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-mc-calc.png" class="lightbox" data-gallery="slides" title="Efficient returns calculations"><img src="img/rl-mc-calc.png" class="img-fluid figure-img" alt="Efficient returns calculations"></a></p>
<figcaption>Efficient returns calculations</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-mc-incremental-updates.png" class="lightbox" data-gallery="slides" title="incremental-update rule"><img src="img/rl-mc-incremental-updates.png" class="img-fluid figure-img" alt="incremental-update rule"></a></p>
<figcaption>incremental-update rule</figcaption>
</figure>
</div></div>
<p>this bring us to our second example:</p>
<div id="exm-black-jack-mdp" class="theorem example page-columns page-full">
<p><span class="theorem-title"><strong>Example 2 (Blackjack MDP)</strong></span> &nbsp;</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-bj-example.png" class="lightbox" data-gallery="slides" title="Blackjack example"><img src="img/rl-bj-example.png" class="img-fluid figure-img" alt="Blackjack example"></a></p>
<figcaption>Blackjack example</figcaption>
</figure>
</div></div><ul>
<li><strong>Undiscounted</strong> MDP where each game of blackjack corresponds to an episode with
<ul>
<li>Rewards:
<ul>
<li>r= -1 for a loss</li>
<li>r= 0 for a draw</li>
<li>r= 1 for a win</li>
</ul></li>
<li>Actions : <span class="math inline">a\in \{\text{Hit}, \text{Stick}\}</span></li>
<li>States S:
<ul>
<li>player has a usable ace (Yes/No) <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
<li>sum of cards (12-21)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></li>
<li>The card the dealer‚Äôs card shows (Ace-10)</li>
</ul></li>
<li>Cards are dealt with replacement<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></li>
<li>Policy <span class="math inline">\pi</span>:
<ul>
<li>if sum &lt; 20, stick</li>
<li>otherwise, hit</li>
</ul></li>
</ul></li>
</ul>
</div>
<p>In the programming assignment we will produce the following graphs</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-bj-outcomes.png" class="lightbox" data-gallery="slides" title="Blackjack outcomes"><img src="img/rl-bj-outcomes.png" class="img-fluid figure-img" alt="Blackjack outcomes"></a></p>
<figcaption>Blackjack outcomes</figcaption>
</figure>
</div></div><ul>
<li>In real world settings we typical don‚Äôt know theoretical functions like values, action values or rewards. Out best option is to sample reality in trial and error experiment of testing different interventions.</li>
<li>However under certain conditions such samples may be enough to perform the <a href="#dfn-prediction">prediction task</a> learn a <a href="#dfn-value">value function</a> or the <a href="#dfn-action-value">action value function</a> .</li>
<li>We can these function to learn better policies from this experience.</li>
<li>A second scenario involves historical samples collected from past interactions. We can use probabilistic methods like MCMC to estimate <span class="math inline">q(a)</span>.</li>
</ul>
<p>we can use the MC prediction alg to estimate the expected returns for a state given a policy <span class="math inline">\pi</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The key limitations of <em>MC value estimation algorithm</em> is its requirement for episodic tasks and for completing such an episode before it starts. In some games an episode can be very long.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="emoji" data-emoji="bulb">üí°</span> Is this really so? <span class="emoji" data-emoji="thinking">ü§î</span>
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>If we work in the Bayesian paradigm with some prior and use Bayesian updating.</li>
<li>At every step we should have well defined means.</li>
<li>So it seems one can perhaps do sample based on non-episodic tasks</li>
<li>One more idea is to treat n_steps as an episode.</li>
<li>Without episodic we most likely lose the efficient updating. <span class="emoji" data-emoji="thinking">ü§î</span></li>
<li>Perhaps we can use the online update rule for the mean.</li>
</ul>
</div>
</div>
<ul>
<li><p><label><input type="checkbox">TODO - try to implement this as an algorithm.</label></p></li>
<li><p>To ensure well-defined average sample returns, we define Monte Carlo methods only on episodic tasks that all eventually terminate - only on termination are value estimates and policies updated.</p></li>
</ul>
<p>Implications of MC Learning</p>
<ul>
<li><p>We don‚Äôt need to keep a large mode of the environment.</p></li>
<li><p>We estimate the values of each state independently of other states</p></li>
<li><p>Computation for updating values or each state is independent of the size of the MDP^[in DP we had to solve <span class="math inline">n\times n</span> - simultaneous equations]</p></li>
</ul>
</section>
<section id="sec-mc-control" class="level1 page-columns page-full">
<h1>Lesson 2: Monte Carlo for Control</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox">Estimate action-value functions using <a href="#dfn-mc">Monte Carlo</a> <a href="#sec-l2g1">#</a></label></li>
<li><label><input type="checkbox">Understand the importance of maintaining exploration in Monte Carlo algorithms <a href="#sec-l2g2">#</a></label></li>
<li><label><input type="checkbox" checked="">Understand how to use <a href="#dfn-mc">Monte Carlo</a> methods to implement a GPI algorithm. <a href="#sec-l2g3">#</a></label></li>
<li><label><input type="checkbox" checked="">Apply <a href="#dfn-mc">Monte Carlo</a> with exploring starts to solve an MDP <a href="#sec-l2g4">#</a></label></li>
</ul>
</div>
</div>
<section id="sec-l2g1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-l2g1">MC Action-Value Functions</h2>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-mc-action-values.png" class="lightbox" data-gallery="slides" title="action values"><img src="img/rl-mc-action-values.png" class="img-fluid figure-img" alt="action values"></a></p>
<figcaption>action values</figcaption>
</figure>
</div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-mc-backoff.png" class="lightbox" data-gallery="slides" title="back off"><img src="img/rl-mc-backoff.png" class="img-fluid figure-img" alt="back off"></a></p>
<figcaption>back off</figcaption>
</figure>
</div></div>
<p>This back off diagram indicates that the value of a state S depends on the values of its actions.</p>
<ul>
<li>Recall that <a href="#def-contol">control</a> is simply improving a policy using our action values estimate.</li>
<li>Policy improvement is done by <strong>Greedyfying</strong> a policy <span class="math inline">\pi</span> at a state <span class="math inline">s</span> by selecting the action <span class="math inline">a</span> with the highest action value.</li>
<li>If we are missing some action values we can make the policy worse!</li>
<li>We need to ensure that our RL algorithm engages the different actions of a state. There are two strategies:
<ul>
<li>Exploring starts</li>
<li><span class="math inline">\epsilon</span>-Soft strategies</li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-exploring-starts.png" class="lightbox" data-gallery="slides" title="exploring starts"><img src="img/rl-exploring-starts.png" class="img-fluid figure-img" alt="exploring starts"></a></p>
<figcaption>exploring starts</figcaption>
</figure>
</div></div><p>The following is the MC alg with exploring start for estimation.</p>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-exploring-starts-pseudocode.png" class="lightbox" data-gallery="slides" title="exploring starts pseudocode"><img src="img/rl-exploring-starts-pseudocode.png" class="img-fluid figure-img" alt="exploring starts pseudocode"></a></p>
<figcaption>exploring starts pseudocode</figcaption>
</figure>
</div></div><p>Let‚Äôs recap how GPI looks:</p>
<ul>
<li>Keeping <span class="math inline">\pi_0</span> fixed we do evaluation of <span class="math inline">q_\pi</span> using MC‚ÄìES</li>
<li>We improve <span class="math inline">\pi_0</span> by picking the actions with the highest values</li>
<li>We stop when we don‚Äôt improve <span class="math inline">\pi</span></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/rl-monte-carlo-GPI-01.png" class="lightbox" data-gallery="slides" title="exploring starts pseudocode"><img src="img/rl-monte-carlo-GPI-01.png" class="img-fluid figure-img" alt="exploring starts pseudocode"></a></p>
<figcaption>exploring starts pseudocode</figcaption>
</figure>
</div></div><p>Here, in the evaluation step, we estimate the action-values using MC prediction, with exploration driven by exploring Starts or an <span class="math inline">\epsilon</span>-soft policy</p>
</section>
</section>
<section id="lesson-3-exploration-methods-for-monte-carlo" class="level1">
<h1>Lesson 3: Exploration Methods for Monte Carlo</h1>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox" checked="">Understand why Exploring Starts can be problematic in real problems <a href="#sec-l3g1">#</a></label></li>
<li><label><input type="checkbox" checked="">Describe an alternative exploration method for Monte Carlo control <a href="#sec-l3g2">#</a></label></li>
</ul>
</div>
</div>
<p>G1. Exploring start can be problematic as we may not able to say try all actions on all states.</p>
<ul>
<li><p>many actions or states it could be risky/unethical</p></li>
<li><p>it could cost too much - we need too many experiments.</p></li>
</ul>
<p>Note: The Blackjack MDP can be improved using Exploring Starts since each initial state can be sampled. Recall there were 200 states.</p>
<p>G2. A second approach to policy improvement is an generalization of both the <span class="math inline">\epsilon</span>- greedy policy and the random uniform, which we learned for multi-armed bandits in the fundamentals. This is called <span class="math inline">\epsilon</span>-soft.</p>
<p>An <span class="math inline">\epsilon</span>-soft policy is one for which state the each action has a probability of at least <span class="math inline">\frac{\epsilon}{|A|}</span></p>
<p>The pro is we have never ending exploration</p>
<p>The con is we can never reach a deterministic optimal policy - but we can get to a stochastic policy where the best choice is <span class="math inline">1-\epsilon+\frac{\epsilon}{|A|}</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="img/wk2-epsilon-soft-pseudocode.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="MC control with epsilon-soft policies"><img src="img/wk2-epsilon-soft-pseudocode.png" class="img-fluid figure-img" alt="MC control with epsilon-soft policies"></a></p>
<figcaption>MC control with epsilon-soft policies</figcaption>
</figure>
</div>
<p>The Highlights indicate modification of the Exploring Starts alg</p>
<ol type="1">
<li><p>We can start with Uniform-random as its epsilon-soft.</p></li>
<li><p>Episode generation uses the current <span class="math inline">\pi</span> (<span class="math inline">\epsilon</span>-soft policy) <em>before</em> it is improved.</p></li>
<li><p>We drop the first-visit check - this is an every-visit MC algorithm.</p></li>
<li><p>The new policy generated in each iteration is <span class="math inline">\epsilon</span>-greedy w.r.t. the current action-value estimate, which is improved prior.</p></li>
<li><p>The optimal <span class="math inline">\epsilon</span>-soft policy is an <span class="math inline">\epsilon</span>-soft policy.</p></li>
</ol>
<section id="lesson-4-off-policy-learning-for-prediction" class="level2">
<h2 class="anchored" data-anchor-id="lesson-4-off-policy-learning-for-prediction">Lesson 4: Off-policy learning for prediction</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lesson Learning Goals
</div>
</div>
<div class="callout-body-container callout-body">
<ul class="task-list">
<li><label><input type="checkbox">Understand how off-policy learning can help deal with the exploration problem <a href="#sec-l4g1">#</a></label></li>
<li><label><input type="checkbox">Produce examples of target policies and examples of behavior policies. <a href="#sec-l4g2">#</a></label></li>
<li><label><input type="checkbox">Understand importance sampling <a href="#sec-l4g3">#</a></label></li>
<li><label><input type="checkbox">Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution. <a href="#sec-l4g4">#</a></label></li>
<li><label><input type="checkbox">Understand how to use importance sampling to correct returns <a href="#sec-l4g5">#</a></label></li>
<li><label><input type="checkbox">Understand how to modify the Monte Carlo prediction algorithm for off-policy learning. <a href="#sec-l4g6">#</a></label></li>
</ul>
</div>
</div>
<section id="sec-l4g1" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g1">Off-policy learning</h3>
<ul>
<li>Off-policy learning is a way to learn a policy <span class="math inline">\pi</span> using samples from another policy <span class="math inline">\pi'</span>.</li>
<li>This is useful when we have a policy that is easier to sample from than the policy we want to learn.</li>
<li>A key idea is to correct the returns using importance sampling.</li>
</ul>
<p>For example suppose we can use a rule based model to generate samples of agent state, action and rewards - but we don‚Äôt realy have an MDP, value function or policy. We could start with a uniform random policy and then use the samples to learn a better policy. However this would require us to interact with the environment and our agents may not be able to do this. In the case of Sugarscape model the agents are not realy making descisions, they are following rules.</p>
<p>If we wished to develop agent that learn using RL with differnt rules on or off and other settings and use those to learn a policy using many samples. One advantage of the Sugarscape model is that it is highly hetrogenous so we get a rich set of samples to work with. A second advantage is that the rule based model can be fast to sample from and we can generate many samples by running it using hyperparameters optimized testbed.</p>
<p>So if we have lots of samples we may not need to explore as much initially, but rather learn to exploit the samples we have. Once we learn a near optimal policy for the samples we can use our agent to explore new vistas in our environment.</p>
</section>
<section id="sec-l4g2" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g2">Target and behavior policies</h3>
<ul>
<li>The target policy is the policy we want to learn.</li>
<li>The behavior policy is the policy we sample from.</li>
</ul>
</section>
<section id="sec-l4g3" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g3">Importance sampling</h3>
<ul>
<li><p>Importance sampling is a technique to estimate the expected value of a target distribution using samples from a different distribution.</p></li>
<li><p>Why cant we just use the samples from the behavior policy to estimate the target policy?</p></li>
<li><p>The answer is that the samples from the behavior policy are biased towards the behavior policy.</p></li>
<li><p>In the target policy we may have states that are never visited by the behavior policy.</p></li>
<li><p>For example we might want to learn a policy that focuses on trade rather than combat or Vica versa. This extreme idea of introducing/eliminating some action would significantly change behavioral trajectories. Sample based methods could be able to handle these changes - if we can restrit them to each subset of actions but clearly the expected return of states will be diverge in the long run.</p></li>
<li><p>So what we want is someway to correct the returns from the behavior policy to the target policy.</p></li>
<li><p>It is used to correct returns from the behavior policy to the target policy.</p></li>
</ul>
<p><span id="eq-trajectory-probability"><span class="math display">
\begin{align*}
  P(A_t, S_{t+1}, &amp; A_{t+1}, ... ,S_T | S_t, A_{t:T-1} \sim \pi) \\
  &amp; = \pi(A_t|S_t)p(S_{t+1}|S_t, A_t)\pi(A_{t+1}, S_{t+1}) \cdot\cdot\cdot p(S_T|S_{T-1}, A_{T-1}) \\
  &amp; = \prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)
\end{align*}
\tag{1}</span></span></p>
</section>
<section id="sec-l4g4" class="level3">
<h3 class="anchored" data-anchor-id="sec-l4g4">Importance sampling ratio</h3>
<p><strong>Definition:</strong> The importance sampling ratio (rho, <span class="math inline">\rho</span>) is the relative probability of the trajectory under the target vs behavior policy:</p>
<p><span id="eq-importance-sampling"><span class="math display">
\begin{align}
\rho_{t:T-1} &amp; \doteq \frac{\prod_{k=t}^{T-1} \pi(A_k \mid S_k)p(S_{k+1} \mid S_k, A_k)}{\prod_{k=t}^{T-1} b(A_k \mid S_k)p(S_{k+1} \mid S_k, A_k)} \\
             &amp; = \prod_{k=t}^{T-1} \frac{\pi(A_k \mid S_k)}{b(A_k \mid S_k)}
\end{align}
\tag{2}</span></span></p>
<p><span id="eq-value1"><span class="math display">
v_\pi(s) = \mathbb{E}_b[\rho_{t:T-1} \cdot G_t \mid S_t = s] \qquad
\tag{3}</span></span></p>
<p><span id="eq-value2"><span class="math display">
V(s) \doteq \frac{\displaystyle \sum_{t\in \mathscr T(s)}\rho_{t:T(t) - 1} \cdot G_t}{|\mathscr T (s)|} \qquad
\tag{4}</span></span></p>
<p><span id="eq-value3"><span class="math display">
V(s) \doteq \frac{\displaystyle \sum_{t\in \mathscr T(s)} \Big(\rho_{t:T(t) - 1} \cdot G_t\Big)}{\displaystyle \sum_{t\in \mathscr T(s)}\rho_{t:T(t) - 1}} \qquad
\tag{5}</span></span></p>
</section>
<section id="emma-brunskill-batch-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="emma-brunskill-batch-reinforcement-learning">Emma Brunskill: Batch Reinforcement Learning</h3>
<ul>
<li>Emma Brunskill is a professor at Stanford University.</li>
<li>In batch RL we have a fixed dataset of samples and we want to learn a policy from this data.</li>
<li>This is useful when we have a fixed dataset of samples and we want to learn a policy from this data.</li>
<li>The key idea is to use importance sampling to correct the returns from the behavior policy to the target policy.</li>
</ul>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><strong>Prediction</strong> in the sense that we want to predict for <span class="math inline">\pi</span> how well it will preforms i.e.&nbsp;its expected returns for a state<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>worth either 1 or 11<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>face card are worth 10<a href="#fnref3" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p>this is a big simplifying assumption<a href="#fnref4" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>CC SA BY-NC-ND</div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Sample-Based {Learning} {Methods}},
  date = {2024-03-01},
  url = {https://orenbochman.github.io//notes/RL/c2-w1.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Bochman, Oren. 2024. <span>‚ÄúSample-Based Learning Methods.‚Äù</span> March
1, 2024. <a href="https://orenbochman.github.io//notes/RL/c2-w1.html">https://orenbochman.github.io//notes/RL/c2-w1.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/www\.quarto\.org\/custom");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="OrenBochman/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2024, Oren Bochman
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../license.html">
<p>License</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../trademark.html">
<p>Trademark</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.algTitle || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        titlePrefix = el.dataset.algTitle;
        titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
        titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
      });
    })(document);
    </script>
  
<script>var lightboxQuarto = GLightbox({"openEffect":"zoom","closeEffect":"zoom","selector":".lightbox","loop":false,"descPosition":"bottom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>