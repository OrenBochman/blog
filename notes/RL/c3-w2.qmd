---
date: 2024-04-02
lastmod: 2024-04-02
title: Constructing Features for Prediction
subtitle: Prediction and Control with Function Approximation
author: Oren Bochman
draft: false
categories:
  - Coursera
  - notes
  - rl
  - reinforcement learning
  - the k-armed bandit problem
  - bandit algorithms
  - exploration 
  - explotation
  - epsilon greedy algorithm
  - sample avarage method
image: /images/nlp-brain-wordcloud.jpg
title-block-banner: /images/banner_black_3.jpg
---

![RL algorithms](img/alg_selector.jpeg){.column-margin}

# Introduction

::: {.callout-tip collapse="true"}
### Readings {.unnumbered}

-   [x] [@sutton2018reinforcement§9.4-9.5.0, pp. 204-210] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)
-   [x] [@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)
-   [x] [@sutton2018reinforcement§9.7, pp. 223-228] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)

:::

# Lesson 1: Feature Construction for Linear Methods 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] *Define* the difference between **coarse coding** and tabular representations [\#](#sec-l1g1)
-   [x] *Explain* the trade-off when designing representations between discrimination and generalization [\#](#sec-l1g2)
-   [x] *Understand* how different coarse coding schemes affect the functions that can be represented [\#](#sec-l1g3)
-   [x] *Explain* how tile coding is a (computationally?) convenient case of coarse coding [\#](#sec-l1g4)
-   [x] *Describe* how designing the tilings affects the resultant representation [\#](#sec-l1g5)
-   [x] *Understand* that tile coding is a computationally efficient implementation of coarse coding [\#](#sec-l1g6)

:::

# Lesson 2: Neural Networks 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] *Define* a neural network [\#](#sec-l2g1)
-   [x] *Define* activation functions [\#](#sec-l2g2)
-   [x] *Define* a feed-forward architecture [\#](#sec-l2g3)
-   [x] *Understand* how neural networks are doing feature construction [\#](#sec-l2g4)
-   [x] *Understand* how neural networks are a non-linear function of state [\#](#sec-l2g5)
-   [x] *Understand* how deep networks are a composition of layers [\#](#sec-l2g6)
-   [x] *Understand* the tradeoff between learning capacity and challenges presented by deeper networks [\#](#sec-l2g7)

:::

# Lesson 3: Training Neural Networks 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] *Compute* the gradient for a single hidden layer neural network [\#](#sec-l3g1)
-   [x] *Understand* how to compute the gradient for arbitrarily deep networks [\#](#sec-l3g2)
-   [x] *Understand* the importance of initialization for neural networks [\#](#sec-l3g3)
-   [x] *Describe* strategies for initializing neural networks [\#](#sec-l3g4)
-   [x] *Describe* optimization techniques for training neural networks [\#](#sec-l3g5)

:::


::: callout-note

### Discussion prompt {.unnumbered}

> What properties of the representation are important for our online setting? This contrasts the offline, batch setting. 

:::