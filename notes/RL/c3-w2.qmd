---
date: 2024-04-02
lastmod: 2024-04-02
title: Constructing Features for Prediction
subtitle: Prediction and Control with Function Approximation
author: Oren Bochman
draft: false
categories:
  - Coursera
  - notes
  - rl
  - reinforcement learning
keywords:
  - reinforcement learning
  - neural networks
  - feature construction
  - tile coding
  - coarse coding
  - feed-forward architecture
  - activation functions
  - deep networks
  - gradient
  - online setting
  - offline setting
  - representation
image: /images/nlp-brain-wordcloud.jpg
title-block-banner: /images/banner_black_3.jpg
---

![RL algorithms](img/alg_selector.jpeg){.column-margin}

# Introduction

::: {.callout-tip collapse="true"}
### Readings {.unnumbered}

-   [x] [@sutton2018reinforcement§9.4-9.5.0, pp. 204-210] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)
-   [x] [@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)
-   [x] [@sutton2018reinforcement§9.7, pp. 223-228] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)

:::

# Lesson 1: Feature Construction for Linear Methods 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] *Define* the difference between **coarse coding** and tabular representations [\#](#sec-l1g1)
-   [x] *Explain* the trade-off when designing representations between discrimination and generalization [\#](#sec-l1g2)
-   [x] *Understand* how different coarse coding schemes affect the functions that can be represented [\#](#sec-l1g3)
-   [x] *Explain* how tile coding is a (computationally?) convenient case of coarse coding [\#](#sec-l1g4)
-   [x] *Describe* how designing the tilings affects the resultant representation [\#](#sec-l1g5)
-   [x] *Understand* that tile coding is a computationally efficient implementation of coarse coding [\#](#sec-l1g6)

:::


## Coarse Coding (Video)

In this video, Adam White introduces the concept of **coarse coding**, covering the first learning objective of this lesson.

Coarse coding are a way to represent states in a more general way than tabular representations. This allows for generalization across states. The trade-off is that the representation is less discriminative.


## Define the difference between **coarse coding** and tabular representations {#sec-l1g1}


![approximation](img/rl-coding_states.png){.column-margin}

Recall that linear function approximation are paramertized by a weight vector $\mathbf{w}$ and a feature vector $\mathbf{x}(s)$. 

As we saw in the previous unit tabular representations associates one feature per state, this is called a one-hot encoding of the state space.

![one hot coding](img/rl-tabular_coding.png){.column-margin}

 We associate one hot encoding with an indicator function $\delta_{ij}(s)$. This is a very discriminative representation but it does generalize.

![state aggregation](img/rl-state-aggregation.png){.column-margin}


We also discussed using **state aggregation** for the 1000 state random walk example.
In state aggregation we break the continuous state space into discrete regions and associate a feature with each region. This is a more general representation than tabular representations but less discriminative. 

![coarse coding](img/rl-coarse-coding.png){.column-margin}

**Coarse coding** uses multiple overlapping shapes to represent states. This is a more General representation than state aggregation but less discriminative. Features are the circles they are in. If the circles overlap, we can have items that are in multiple circles. I.e. they are characterized by multiple features. In the example shown there can be from one to three active features. 

So the difference is that tabular representations are one hot encodings while coarse coding uses membership in multiple overlapping shapes to represent states.

How does coarse coding relates to state aggregation?

Coarse coding is also a generalization of state aggregation. In state aggregation we break the state space into discrete regions and associate a feature with each region. But we don't let these regions overlap. In coarse coding we allow the regions to overlap which can give greater generalization as regions can share features.

In this video the term Reception Field is used to describe the region of the state space that a feature is associated with. This is an idea that comes from CNNs.


## Generalization Properties of Coarse Coding (Video)

In this video Martha White discusses the generalization properties of coarse coding.

She looks at using small overlapping 1-d intervals to represent a 1-d function.

We see that changing shape size and number of effects the generalization properties of the representation.

![scale](rl-scale-generalization.png)

![shape](rl-shape-generalization.png)

![discrimination](rl-shape-discrimination.png)


Next we looked at using short interval vs longer intervals to approximate a 1-d function. We see that the longer intervals give a smoother approximation.


## The trade-off between discrimination and generalization {#sec-l1g2}





## Tile Coding  (Video)

In this video, Martha White introduces the concept of **tile coding**, covering the last learning objective of this lesson

This is a computationally efficient implementation of coarse coding. We use multiple overlapping tilings to represent states. Each tiling is a grid of tiles. Each tile is a feature.

If we use one tiling we get state aggregation. If we use multiple tilings we get coarse coding. One tiling means we don't discriminate between states that are in the same tile. Multiple tilings means we can discriminate between states that are in the same tile in one tiling but not in another.

The textbook goes into some more details about how we can generelize using tile coding - using regular tilings generates in a diagonal pattern. Using random tilings generates more spherical regions.

One caveat is that in high dimensional spaces we end up an exponential number of features. This is called the curse of dimensionality.


## Using Tile Coding in TD (Video)

# Lesson 2: Neural Networks 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] *Define* a neural network [\#](#sec-l2g1)
-   [x] *Define* activation functions [\#](#sec-l2g2)
-   [x] *Define* a feed-forward architecture [\#](#sec-l2g3)
-   [x] *Understand* how neural networks are doing feature construction [\#](#sec-l2g4)
-   [x] *Understand* how neural networks are a non-linear function of state [\#](#sec-l2g5)
-   [x] *Understand* how deep networks are a composition of layers [\#](#sec-l2g6)
-   [x] *Understand* the tradeoff between learning capacity and challenges presented by deeper networks [\#](#sec-l2g7)

:::

# Lesson 3: Training Neural Networks 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] *Compute* the gradient for a single hidden layer neural network [\#](#sec-l3g1)
-   [x] *Understand* how to compute the gradient for arbitrarily deep networks [\#](#sec-l3g2)
-   [x] *Understand* the importance of initialization for neural networks [\#](#sec-l3g3)
-   [x] *Describe* strategies for initializing neural networks [\#](#sec-l3g4)
-   [x] *Describe* optimization techniques for training neural networks [\#](#sec-l3g5)

:::


::: callout-note

### Discussion prompt {.unnumbered}

> What properties of the representation are important for our online setting? This contrasts the offline, batch setting. 

:::