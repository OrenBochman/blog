{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "date: 2024-04-02\n",
        "lastmod: 2024-04-02\n",
        "title: Constructing Features for Prediction\n",
        "subtitle: Prediction and Control with Function Approximation\n",
        "author: Oren Bochman\n",
        "draft: false\n",
        "categories:\n",
        "  - Coursera\n",
        "  - notes\n",
        "  - rl\n",
        "  - reinforcement learning\n",
        "keywords:\n",
        "  - reinforcement learning\n",
        "  - neural networks\n",
        "  - feature construction\n",
        "  - tile coding\n",
        "  - coarse coding\n",
        "  - feed-forward architecture\n",
        "  - activation functions\n",
        "  - deep networks\n",
        "  - gradient\n",
        "  - online setting\n",
        "  - offline setting\n",
        "  - representation\n",
        "image: /images/nlp-brain-wordcloud.jpg\n",
        "title-block-banner: /images/banner_black_3.jpg\n",
        "---\n",
        "\n",
        "\n",
        "![RL algorithms](img/alg_selector.jpeg){.column-margin}\n",
        "\n",
        "# Introduction\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "### Readings {.unnumbered}\n",
        "\n",
        "-   [x] [@sutton2018reinforcement§9.4-9.5.0, pp. 204-210] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n",
        "-   [x] [@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n",
        "-   [x] [@sutton2018reinforcement§9.7, pp. 223-228] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n",
        "\n",
        ":::\n",
        "\n",
        "# Lesson 1: Feature Construction for Linear Methods \n",
        "\n",
        "::: callout-note\n",
        "### Learning Objectives {.unnumbered}\n",
        "\n",
        "-   [x] *Define* the difference between **coarse coding** and tabular representations [\\#](#sec-l1g1)\n",
        "-   [x] *Explain* the trade-off when designing representations between discrimination and generalization [\\#](#sec-l1g2)\n",
        "-   [x] *Understand* how different coarse coding schemes affect the functions that can be represented [\\#](#sec-l1g3)\n",
        "-   [x] *Explain* how tile coding is a (computationally?) convenient case of coarse coding [\\#](#sec-l1g4)\n",
        "-   [x] *Describe* how designing the tilings affects the resultant representation [\\#](#sec-l1g5)\n",
        "-   [x] *Understand* that tile coding is a computationally efficient implementation of coarse coding [\\#](#sec-l1g6)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Coarse Coding (Video)\n",
        "\n",
        "In this video, Adam White introduces the concept of **coarse coding**, covering the first learning objective of this lesson.\n",
        "\n",
        "Coarse coding are a way to represent states in a more general way than tabular representations. This allows for generalization across states. The trade-off is that the representation is less discriminative.\n",
        "\n",
        "\n",
        "## The difference between **coarse coding** and tabular representations {#sec-l1g1}\n",
        "\n",
        "\n",
        "![approximation](img/rl-coding_states.png){.column-margin}\n",
        "\n",
        "Recall that linear function approximation are paramertized by a weight vector $\\mathbf{w}$ and a feature vector $\\mathbf{x}(s)$. \n",
        "\n",
        "As we saw in the previous unit tabular representations associates one feature per state, this is called a one-hot encoding of the state space.\n",
        "\n",
        "![one hot coding](img/rl-tabular_coding.png){.column-margin}\n",
        "\n",
        " We associate one hot encoding with an indicator function $\\delta_{ij}(s)$. This is a very discriminative representation but it does generalize.\n",
        "\n",
        "![state aggregation](img/rl-state-aggregation.png){.column-margin}\n",
        "\n",
        "\n",
        "We also discussed using **state aggregation** for the 1000 state random walk example.\n",
        "In state aggregation we break the continuous state space into discrete regions and associate a feature with each region. This is a more general representation than tabular representations but less discriminative. \n",
        "\n",
        "![coarse coding](img/rl-coarse-coding.png){.column-margin}\n",
        "\n",
        "**Coarse coding** uses multiple overlapping shapes to represent states. This is a more General representation than state aggregation but less discriminative. Features are the circles they are in. If the circles overlap, we can have items that are in multiple circles. I.e. they are characterized by multiple features. In the example shown there can be from one to three active features. \n",
        "\n",
        "So the difference is that tabular representations are one hot encodings while coarse coding uses membership in multiple overlapping shapes to represent states.\n",
        "\n",
        "How does coarse coding relates to state aggregation?\n",
        "\n",
        "Coarse coding is also a generalization of state aggregation. In state aggregation we break the state space into discrete regions and associate a feature with each region. But we don't let these regions overlap. In coarse coding we allow the regions to overlap which can give greater generalization as regions can share features.\n",
        "\n",
        "In this video the term Reception Field is used to describe the region of the state space that a feature is associated with. This is an idea that comes from CNNs.\n",
        "\n",
        "\n",
        "## Generalization Properties of Coarse Coding (Video)\n",
        "\n",
        "In this video Martha White discusses the generalization properties of coarse coding.\n",
        "\n",
        "She looks at using small overlapping 1-d intervals to represent a 1-d function.\n",
        "\n",
        "We see that changing shape size and number of effects the generalization properties of the representation.\n",
        "\n",
        "![scale](rl-scale-generalization.png){.column-margin}\n",
        "\n",
        "![shape](rl-shape-generalization.png){.column-margin}\n",
        "\n",
        "![discrimination](rl-shape-discrimination.png){.column-margin}\n",
        "\n",
        "\n",
        "Next we looked at using short interval vs longer intervals to approximate a 1-d function. We see that the longer intervals give a smoother approximation.\n",
        "\n",
        "\n",
        "## The trade-off between discrimination and generalization {#sec-l1g2}\n",
        "\n",
        "## Tile Coding  (Video)\n",
        "\n",
        "In this video, Martha White introduces the concept of **tile coding**. This is simply a implementation of coarse coding using multiple overlapping grids.\n",
        "\n",
        "## Explain how tile coding is a (computationally?) convenient case of coarse coding  {#sec-l1g4}\n",
        "\n",
        "Tile coding is a computationally efficient implementation of coarse coding. We use multiple overlapping tilings to represent states. Each tiling is a grid of tiles. Each tile is a feature.\n",
        "\n",
        "If we use one tiling we get state aggregation. If we use multiple tilings we get coarse coding. One tiling means we don't discriminate between states that are in the same tile. Multiple tilings means we can discriminate between states that are in the same tile in one tiling but not in another.\n",
        "\n",
        "## Describe how designing the tilings affects the resultant representation {#sec-l1g5}\n",
        "\n",
        "The textbook goes into some more details about how we can generelize using tile coding - using regular tilings generates in a diagonal pattern. Using random tilings generates more spherical regions.\n",
        "\n",
        "However we also saw that the number size and shape of the tiles affects the generalization properties of the representation. And that increasing the overlap between the tiles an increase the discrimination properties of the representation.\n",
        "\n",
        "## *Understand* that tile coding is a computationally efficient implementation of coarse coding {#sec-l1g6}\n",
        "\n",
        "\n",
        "Tile coding is a computationally efficient implementation of coarse coding. Since grids are uniform it is easy to compute which cells a state is in. A second  reason is that end up with a sparse representations thus the dot product is just the sum of the weights of the active features for each state.\n",
        "\n",
        "One caveat is that in high dimensional spaces we end up an exponential number of features. This is called the curse of dimensionality.\n",
        "\n",
        "\n",
        "## Using Tile Coding in TD (Video)\n",
        "\n",
        "In this video, Adam White shows how to use tile coding in TD learning.\n",
        "He goes back to the 1000 state random walk example and shows how to use tile coding to approximate the value function. We end up needing  six tiles. \n",
        "\n",
        "![tile coding v.s. state aggregation](img/rl-tile-coding-performance.png){.column-margin}\n",
        "\n",
        "## Feature Construction for Linear Methods \n",
        "\n",
        "In the textbook we see two forms of features for linear methods that are not covered in the videos.\n",
        "\n",
        "The first are polynomials. We might use polynomials features for the state to represent the state space. This seems to be a good for problems where RL is dealing to a greater extent with  interpolation or regression.\n",
        "\n",
        "The following is given as an example of a polynomial feature representation of the state space. It took a bit of time to understand what was going on here.\n",
        "\n",
        "They explain about the different combination of two features $s_1$ and $s_2$ doesn't cover some edge cases but using four  $(1,s_1,s_2,s_1s_2)$ covers all the possible combinations of the two features. We might also want to include higher powers of the atoms and that is what the polynomial representation is doing.\n",
        "\n",
        "$$\n",
        "x_i(s) = \\prod_{j=1}^k s_j^{c_{ij}}\n",
        "$$\n",
        "\n",
        "It important to point out that we  are not using the polynomials as a function approximation basis function. What we are talking about is a formulation of multinomial from a set of fixed numbers $s_1 \\lsots s_k$ I.e. we are talking about all the possible products product from powers of these atoms.\n",
        "\n",
        "\n",
        "The second are Fourier bases.\n",
        "\n",
        "$$\n",
        "x_i(s) = \\cos\\left(\\frac{2\\pi s^T a_i}{b}\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "The book mentions that the Fourier basis is particularly useful for periodic functions.\n",
        "\n",
        "\n",
        "There are many other orthogonal bases used as functnio expansions that could be used, as features for linear function approximation. \n",
        "\n",
        "- Walsh functions and Haar wavelets have discrete support and are used in signal processing.\n",
        "- Legendre polynomials are used in physics.\n",
        "- Chebyshev polynomials are used in numerical analysis.  \n",
        "\n",
        "\n",
        "## Other Forms of Coarse Coding\n",
        "\n",
        "\n",
        "In the textbook we see that there are other forms of coarse coding. \n",
        "\n",
        "\n",
        "For example in section 9.5.5 we see using radial basis functions. \n",
        "\n",
        "An RBF\n",
        ": is a real-valued function whose value depends only on the distance between the input and a fixed point (called the center).   \n",
        "\n",
        "Visualizing - Imagine a hill or bump centered at a specific point. The height of the hill at any other point depends solely on its distance from the center. The hill gets flatter as you move away from the cente\n",
        "\n",
        "![one dimensional radial basis functions](rl-radial-basis-functions.png){.column-margin}\n",
        "$$\n",
        "x_i(s) = \\exp\\left(-\\frac{\\|s-c_i\\|^2}{2\\sigma_i^2}\\right)\n",
        "$$\n",
        "\n",
        "- Where \n",
        "- $c_i$ is the center of the radial basis function and \n",
        "- $\\sigma_i$ is the width.\n",
        "\n",
        "This is a form of coarse coding where the features are the distance from a set of centers. This is a more general representation than tile coding but less discriminative. The advantage of RBFs over tiles is that they are approximate functions that vary smoothly and are differentiable. However it appears there is both a computational cost and no real advantage in having continuous/differential features according to the book.\n"
      ],
      "id": "94d48ae0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig.width": 3,
        "fig.height": 3
      },
      "source": [
        "#| label: fig-radial-basis-functions\n",
        "#| fig-cap: One-dimensional radial basis functions with centers at -2, 0, and 2.\n",
        "#| code-fold: true\n",
        "#| column: margin\n",
        "#| echo: false\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def gaussian_rbf(x, center, sigma):\n",
        "    return np.exp(-(x - center)**2 / (2 * sigma**2))\n",
        "\n",
        "# Parameters\n",
        "centers = [-2, 0, 2]\n",
        "sigma = 0.8\n",
        "x_range = np.linspace(-4, 4, 200)\n",
        "\n",
        "# Plot the RBFs\n",
        "fig, ax = plt.subplots()\n",
        "for center in centers:\n",
        "    rbf = gaussian_rbf(x_range, center, sigma)\n",
        "    ax.plot(x_range, rbf, label=f'Center = {center}')\n",
        "\n",
        "# Add labels and formatting\n",
        "_ = ax.set_xlabel('x')\n",
        "_ = ax.set_ylabel('RBF Value')\n",
        "_ = ax.set_title('One-Dimensional Radial Basis Functions')\n",
        "ax.legend()\n",
        "ax.grid(True)\n",
        "\n",
        "# Adjust layout to avoid overlapping text\n",
        "plt.tight_layout()"
      ],
      "id": "fig-radial-basis-functions",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I find this a bit disappointing as it seems like a nice intermediate step between linear function approximation with its convergence guarantees and neural networks which have no such guarantees.\n",
        "\n",
        "# Lesson 2: Neural Networks \n",
        "\n",
        "::: callout-note\n",
        "### Learning Objectives {.unnumbered}\n",
        "\n",
        "-   [x] *Define* a neural network [\\#](#sec-l2g1)\n",
        "-   [x] *Define* activation functions [\\#](#sec-l2g2)\n",
        "-   [x] *Define* a feed-forward architecture [\\#](#sec-l2g3)\n",
        "-   [x] *Understand* how neural networks are doing feature construction [\\#](#sec-l2g4)\n",
        "-   [x] *Understand* how neural networks are a non-linear function of state [\\#](#sec-l2g5)\n",
        "-   [x] *Understand* how deep networks are a composition of layers [\\#](#sec-l2g6)\n",
        "-   [x] *Understand* the tradeoff between learning capacity and challenges presented by deeper networks [\\#](#sec-l2g7)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## What is a Neural Network? (Video)\n",
        "\n",
        "In this video, Martha White introduces the concept of a neural network. We look at a simple one layer feed forward  neural network. Where the $output=f(sW)$ is a non-linear function of the input. \n",
        "\n",
        "\n",
        "## Define a neural network {#sec-l2g1}\n",
        "\n",
        "![](img/rl-feedforward-nn.png){.column-margin}\n",
        "\n",
        "A Neural network consists of a network of nodes which process and pass on information. \n",
        "\n",
        "- The circles are the noes\n",
        "- The lines are the connections\n",
        "- The nodes are organized in layers\n",
        "\n",
        "Data starts at the input layer. It is passed through the connections to the hidden layer. The hidden layer is preforms some computation on the data and passes it to the output layer. This process repeats until the last layer produces the output of the network.\n",
        "\n",
        "\n",
        "## Neural Networks Mechanics\n",
        "\n",
        "A node in the network is a function\n",
        "\n",
        "$$\n",
        "output = f[(w_1 \\times input_1) + (w_2 \\times input_2) + \\ldots + (w_n \\times input_n) + b]\n",
        "$$\n",
        "\n",
        "- where: \n",
        "  - $w_i$ are the weights, \n",
        "  - $input_i$ are the inputs, and \n",
        "  - $b$ is the bias.\n",
        "  - $f$ is the activation function.\n",
        "\n",
        "The sum of the product of the weights and inputs is a linear operation. The activation function $f$ is where a non-linearity is introduced into the network.\n",
        "\n",
        "\n",
        "## Define activation functions {#sec-l2g2}\n",
        "\n",
        "Activation functions are non-linear functions that are applied to the output of a node. They introduce non-linearity into the network.\n",
        "\n",
        "![tanh activation](img/rl-activation-functions-tanh.png){.column-margin}\n",
        "\n",
        "![rectified linear activation function](img/rl-activation-functions-relu.png){.column-margin}\n",
        "\n",
        "Martha White also mentions threshold activation functions. However these are not used in practice as they are not differentiable. There is some work since this course came out on compressing neural networks to use threshold activation functions which are easy to compute on a CPU as matrix multiplication becomes a series of comparisons. However these are trained with a differentiable approximation of the threshold function and then quantized to the threshold function.\n",
        "\n",
        "## The Neural Network Implementation\n",
        "\n",
        "![Neural Network Implementation](img/rl-nn-implementation.png){.column-margin}\n",
        "\n",
        "A neural network is a parameterized function that is a composition of linear and non-linear functions. It is a function of the state. The linear functions are the weights and the non-linear functions are the activation functions. The weights are learned from data.\n",
        "\n",
        "## Define a feed-forward architecture {#sec-l2g3}\n",
        "\n",
        "A feed forward architecture is a neural network where the connections between nodes do not form a cycle. The data flows from the input layer to the output layer.\n",
        "\n",
        "An example of a non-feed forward architecture is a recurrent neural network where the connections between nodes form cycles.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## How neural networks are doing feature construction {#sec-l2g4}\n",
        "\n",
        "## Non-linear Approximation with Neural Networks (Video)\n",
        "\n",
        "## How neural networks are a non-linear function of state {#sec-l2g5}\n",
        "\n",
        "## Deep Neural Networks (Video)\n",
        "\n",
        "## How deep networks are a composition of layers {#sec-l2g6}\n",
        "\n",
        "## The tradeoff between learning capacity and challenges presented by deeper networks {#sec-l2g7}\n",
        "\n",
        "\n",
        "# Lesson 3: Training Neural Networks \n",
        "\n",
        "::: callout-note\n",
        "### Learning Objectives {.unnumbered}\n",
        "\n",
        "-   [x] *Compute* the gradient for a single hidden layer neural network [\\#](#sec-l3g1)\n",
        "-   [x] *Understand* how to compute the gradient for arbitrarily deep networks [\\#](#sec-l3g2)\n",
        "-   [x] *Understand* the importance of initialization for neural networks [\\#](#sec-l3g3)\n",
        "-   [x] *Describe* strategies for initializing neural networks [\\#](#sec-l3g4)\n",
        "-   [x] *Describe* optimization techniques for training neural networks [\\#](#sec-l3g5)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "::: callout-note\n",
        "\n",
        "### Discussion prompt {.unnumbered}\n",
        "\n",
        "> What properties of the representation are important for our online setting? This contrasts the offline, batch setting. \n",
        "\n",
        ":::"
      ],
      "id": "7115b15e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}