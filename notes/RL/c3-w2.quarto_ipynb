{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "date: 2024-04-02\n",
        "lastmod: 2024-04-02\n",
        "title: Constructing Features for Prediction\n",
        "subtitle: Prediction and Control with Function Approximation\n",
        "author: Oren Bochman\n",
        "draft: false\n",
        "categories:\n",
        "  - Coursera\n",
        "  - notes\n",
        "  - rl\n",
        "  - reinforcement learning\n",
        "keywords:\n",
        "  - reinforcement learning\n",
        "  - neural networks\n",
        "  - feature construction\n",
        "  - tile coding\n",
        "  - coarse coding\n",
        "  - feed-forward architecture\n",
        "  - activation functions\n",
        "  - deep networks\n",
        "  - gradient\n",
        "  - online setting\n",
        "  - offline setting\n",
        "  - representation\n",
        "image: /images/nlp-brain-wordcloud.jpg\n",
        "title-block-banner: /images/banner_black_3.jpg\n",
        "---\n",
        "\n",
        "\n",
        "![RL logo](img/logo.png){.column-margin} \n",
        "\n",
        "![RL algorithms](img/alg_selector.png){.column-margin group=\"slides\"}\n",
        "\n",
        "# Introduction\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "### Readings {.unnumbered}\n",
        "\n",
        "-   [x] [@sutton2018reinforcement§9.4-9.5.0, pp. 204-210] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n",
        "-   [x] [@sutton2018reinforcement§9.5.3-9.5.4, pp. 215-222] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n",
        "-   [x] [@sutton2018reinforcement§9.7, pp. 223-228] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=194)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "We discussed methods for representing large, an possibly continuous state spaces. Ways to construct features. A representation is an agent's internal encoding of the state, the agent constructs features to summarize the current input. Whenever we are talking about features and representation learning, we are in the land of function approximation. \n",
        "\n",
        "\n",
        "# Lesson 1: Feature Construction for Linear Methods \n",
        "\n",
        "::: callout-note\n",
        "### Learning Objectives {.unnumbered}\n",
        "\n",
        "-   [x] *Define* the difference between **coarse coding** and tabular representations [\\#](#sec-l1g1)\n",
        "-   [x] *Explain* the trade-off when designing representations between discrimination and generalization [\\#](#sec-l1g2)\n",
        "-   [x] *Understand* how different coarse coding schemes affect the functions that can be represented [\\#](#sec-l1g3)\n",
        "-   [x] *Explain* how tile coding is a (computationally?) convenient case of coarse coding [\\#](#sec-l1g4)\n",
        "-   [x] *Describe* how designing the tilings affects the resultant representation [\\#](#sec-l1g5)\n",
        "-   [x] *Understand* that tile coding is a computationally efficient implementation of coarse coding [\\#](#sec-l1g6)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Coarse Coding (Video)\n",
        "\n",
        "In this video, Adam White introduces the concept of **coarse coding**, covering the first learning objective of this lesson.\n",
        "\n",
        "Coarse coding are a way to represent states in a more general way than tabular representations. This allows for generalization across states. The trade-off is that the representation is less discriminative.\n",
        "\n",
        "\n",
        "## The difference between **coarse coding** and tabular representations {#sec-l1g1}\n",
        "\n",
        "\n",
        "![approximation](img/rl-coding_states.png){.column-margin}\n",
        "\n",
        "Recall that linear function approximation are paramertized by a weight vector $\\mathbf{w}$ and a feature vector $\\mathbf{x}(s)$. \n",
        "\n",
        "As we saw in the previous unit tabular representations associates one feature per state, this is called a one-hot encoding of the state space.\n",
        "\n",
        "![one hot coding](img/rl-tabular_coding.png){.column-margin}\n",
        "\n",
        " We associate one hot encoding with an indicator function $\\delta_{ij}(s)$. This is a very discriminative representation but it does generalize.\n",
        "\n",
        "![state aggregation](img/rl-state-aggregation.png){.column-margin}\n",
        "\n",
        "\n",
        "We also discussed using **state aggregation** for the 1000 state random walk example.\n",
        "In state aggregation we break the continuous state space into discrete regions and associate a feature with each region. This is a more general representation than tabular representations but less discriminative. \n",
        "\n",
        "![coarse coding](img/rl-coarse-coding.png){.column-margin}\n",
        "\n",
        "**Coarse coding** uses multiple overlapping shapes to represent states. This is a more General representation than state aggregation but less discriminative. Features are the circles they are in. If the circles overlap, we can have items that are in multiple circles. I.e. they are characterized by multiple features. In the example shown there can be from one to three active features. \n",
        "\n",
        "So the difference is that tabular representations are one hot encodings while coarse coding uses membership in multiple overlapping shapes to represent states.\n",
        "\n",
        "How does coarse coding relates to state aggregation?\n",
        "\n",
        "Coarse coding is also a generalization of state aggregation. In state aggregation we break the state space into discrete regions and associate a feature with each region. But we don't let these regions overlap. In coarse coding we allow the regions to overlap which can give greater generalization as regions can share features.\n",
        "\n",
        "In this video the term Reception Field is used to describe the region of the state space that a feature is associated with. This is an idea that comes from CNNs.\n",
        "\n",
        "\n",
        "## Generalization Properties of Coarse Coding (Video)\n",
        "\n",
        "In this video Martha White discusses the generalization properties of coarse coding.\n",
        "\n",
        "She looks at using small overlapping 1-d intervals to represent a 1-d function.\n",
        "\n",
        "We see that changing shape size and number of effects the generalization properties of the representation.\n",
        "\n",
        "![scale](rl-scale-generalization.png){.column-margin}\n",
        "\n",
        "![shape](rl-shape-generalization.png){.column-margin}\n",
        "\n",
        "![discrimination](rl-shape-discrimination.png){.column-margin}\n",
        "\n",
        "\n",
        "Next we looked at using short interval vs longer intervals to approximate a 1-d function. We see that the longer intervals give a smoother approximation.\n",
        "\n",
        "\n",
        "## The trade-off between discrimination and generalization {#sec-l1g2}\n",
        "\n",
        "## Tile Coding  (Video)\n",
        "\n",
        "In this video, Martha White introduces the concept of **tile coding**. This is simply a implementation of coarse coding using multiple overlapping grids.\n",
        "\n",
        "## Explain how tile coding is a (computationally?) convenient case of coarse coding  {#sec-l1g4}\n",
        "\n",
        "Tile coding is a computationally efficient implementation of coarse coding. We use multiple overlapping tilings to represent states. Each tiling is a grid of tiles. Each tile is a feature.\n",
        "\n",
        "If we use one tiling we get state aggregation. If we use multiple tilings we get coarse coding. One tiling means we don't discriminate between states that are in the same tile. Multiple tilings means we can discriminate between states that are in the same tile in one tiling but not in another.\n",
        "\n",
        "## Describe how designing the tilings affects the resultant representation {#sec-l1g5}\n",
        "\n",
        "The textbook goes into some more details about how we can generalize using tile coding - using regular tilings generates in a diagonal pattern. Using random tilings generates more spherical regions.\n",
        "\n",
        "However we also saw that the number size and shape of the tiles affects the generalization properties of the representation. And that increasing the overlap between the tiles an increase the discrimination properties of the representation.\n",
        "\n",
        "## *Understand* that tile coding is a computationally efficient implementation of coarse coding {#sec-l1g6}\n",
        "\n",
        "\n",
        "Tile coding is a computationally efficient implementation of coarse coding. Since grids are uniform it is easy to compute which cells a state is in. A second  reason is that end up with a sparse representations thus the dot product is just the sum of the weights of the active features for each state.\n",
        "\n",
        "One caveat is that in high dimensional spaces we end up an exponential number of features. This is called the curse of dimensionality.\n",
        "\n",
        "\n",
        "## Using Tile Coding in TD (Video)\n",
        "\n",
        "In this video, Adam White shows how to use tile coding in TD learning.\n",
        "He goes back to the 1000 state random walk example and shows how to use tile coding to approximate the value function. We end up needing  six tiles. \n",
        "\n",
        "![tile coding v.s. state aggregation](img/rl-tile-coding-performance.png){.column-margin}\n",
        "\n",
        "## Feature Construction for Linear Methods \n",
        "\n",
        "In the textbook we see two forms of features for linear methods that are not covered in the videos.\n",
        "\n",
        "The first are polynomials. We might use polynomials features for the state to represent the state space. This seems to be a good for problems where RL is dealing to a greater extent with  interpolation or regression.\n",
        "\n",
        "The following is given as an example of a polynomial feature representation of the state space. It took a bit of time to understand what was going on here.\n",
        "\n",
        "They explain about the different combination of two features $s_1$ and $s_2$ doesn't cover some edge cases but using four  $(1,s_1,s_2,s_1s_2)$ covers all the possible combinations of the two features. We might also want to include higher powers of the atoms and that is what the polynomial representation is doing.\n",
        "\n",
        "$$\n",
        "x_i(s) = \\prod_{j=1}^k s_j^{c_{ij}}\n",
        "$$\n",
        "\n",
        "It important to point out that we  are not using the polynomials as a function approximation basis function. What we are talking about is a formulation of multinomial from a set of fixed numbers $s_1 \\lsots s_k$ I.e. we are talking about all the possible products product from powers of these atoms.\n",
        "\n",
        "\n",
        "The second are Fourier bases.\n",
        "\n",
        "$$\n",
        "x_i(s) = \\cos\\left(\\frac{2\\pi s^T a_i}{b}\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "The book mentions that the Fourier basis is particularly useful for periodic functions.\n",
        "\n",
        "\n",
        "There are many other orthogonal bases used as functnio expansions that could be used, as features for linear function approximation. \n",
        "\n",
        "- Walsh functions and Haar wavelets have discrete support and are used in signal processing.\n",
        "- Legendre polynomials are used in physics.\n",
        "- Chebyshev polynomials are used in numerical analysis.  \n",
        "\n",
        "\n",
        "## Other Forms of Coarse Coding\n",
        "\n",
        "\n",
        "In the textbook we see that there are other forms of coarse coding. \n",
        "\n",
        "\n",
        "For example in section 9.5.5 we see using radial basis functions. \n",
        "\n",
        "An RBF\n",
        ": is a real-valued function whose value depends only on the distance between the input and a fixed point (called the center).   \n",
        "\n",
        "Visualizing - Imagine a hill or bump centered at a specific point. The height of the hill at any other point depends solely on its distance from the center. The hill gets flatter as you move away from the cente\n",
        "\n",
        "![one dimensional radial basis functions](rl-radial-basis-functions.png){.column-margin}\n",
        "$$\n",
        "x_i(s) = \\exp\\left(-\\frac{\\|s-c_i\\|^2}{2\\sigma_i^2}\\right)\n",
        "$$\n",
        "\n",
        "- Where \n",
        "- $c_i$ is the center of the radial basis function and \n",
        "- $\\sigma_i$ is the width.\n",
        "\n",
        "This is a form of coarse coding where the features are the distance from a set of centers. This is a more general representation than tile coding but less discriminative. The advantage of RBFs over tiles is that they are approximate functions that vary smoothly and are differentiable. However it appears there is both a computational cost and no real advantage in having continuous/differential features according to the book.\n"
      ],
      "id": "8220b83f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig.width": 3,
        "fig.height": 3
      },
      "source": [
        "#| label: fig-radial-basis-functions\n",
        "#| fig-cap: One-dimensional radial basis functions with centers at -2, 0, and 2.\n",
        "#| code-fold: true\n",
        "#| column: margin\n",
        "#| echo: false\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def gaussian_rbf(x, center, sigma):\n",
        "    return np.exp(-(x - center)**2 / (2 * sigma**2))\n",
        "\n",
        "# Parameters\n",
        "centers = [-2, 0, 2]\n",
        "sigma = 0.8\n",
        "x_range = np.linspace(-4, 4, 200)\n",
        "\n",
        "# Plot the RBFs\n",
        "fig, ax = plt.subplots()\n",
        "for center in centers:\n",
        "    rbf = gaussian_rbf(x_range, center, sigma)\n",
        "    ax.plot(x_range, rbf, label=f'Center = {center}')\n",
        "\n",
        "# Add labels and formatting\n",
        "_ = ax.set_xlabel('x')\n",
        "_ = ax.set_ylabel('RBF Value')\n",
        "_ = ax.set_title('One-Dimensional Radial Basis Functions')\n",
        "ax.legend()\n",
        "ax.grid(True)\n",
        "\n",
        "# Adjust layout to avoid overlapping text\n",
        "plt.tight_layout()"
      ],
      "id": "fig-radial-basis-functions",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I find this a bit disappointing as it seems like a nice intermediate step between linear function approximation with its convergence guarantees and neural networks which have no such guarantees.\n",
        "\n",
        "# Lesson 2: Neural Networks \n",
        "\n",
        "::: callout-note\n",
        "### Learning Objectives {.unnumbered}\n",
        "\n",
        "-   [x] *Define* a neural network [\\#](#sec-l2g1)\n",
        "-   [x] *Define* activation functions [\\#](#sec-l2g2)\n",
        "-   [x] *Define* a feed-forward architecture [\\#](#sec-l2g3)\n",
        "-   [x] *Understand* how neural networks are doing feature construction [\\#](#sec-l2g4)\n",
        "-   [x] *Understand* how neural networks are a non-linear function of state [\\#](#sec-l2g5)\n",
        "-   [x] *Understand* how deep networks are a composition of layers [\\#](#sec-l2g6)\n",
        "-   [x] *Understand* the tradeoff between learning capacity and challenges presented by deeper networks [\\#](#sec-l2g7)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## What is a Neural Network? (Video)\n",
        "\n",
        "In this video, Martha White introduces the concept of a neural network. We look at a simple one layer feed forward  neural network. Where the $output=f(sW)$ is a non-linear function of the input. \n",
        "\n",
        "\n",
        "## Define a neural network {#sec-l2g1}\n",
        "\n",
        "![](img/rl-feedforward-nn.png){.column-margin}\n",
        "\n",
        "A Neural network consists of a network of nodes which process and pass on information. \n",
        "\n",
        "- The circles are the noes\n",
        "- The lines are the connections\n",
        "- The nodes are organized in layers\n",
        "\n",
        "Data starts at the input layer. It is passed through the connections to the hidden layer. The hidden layer is preforms some computation on the data and passes it to the output layer. This process repeats until the last layer produces the output of the network.\n",
        "\n",
        "\n",
        "## Deep Neural Networks (Video)\n",
        "\n",
        "In  this video, Martha White introduces the concept of neural networks with multiple hidden layers and activation functions.\n",
        "\n",
        "## Neural Networks Mechanics\n",
        "\n",
        "A node in the network is a function\n",
        "\n",
        "$$\n",
        "output = f[(w_1 \\times input_1) + (w_2 \\times input_2) + \\ldots + (w_n \\times input_n) + b]\n",
        "$$\n",
        "\n",
        "- where: \n",
        "  - $w_i$ are the weights, \n",
        "  - $input_i$ are the inputs, and \n",
        "  - $b$ is the bias.\n",
        "  - $f$ is the activation function.\n",
        "\n",
        "The sum of the product of the weights and inputs is a linear operation. The activation function $f$ is where a non-linearity is introduced into the network.\n",
        "\n",
        "\n",
        "## Define activation functions {#sec-l2g2}\n",
        "\n",
        "Activation functions are non-linear functions that are applied to the output of a node. They introduce non-linearity into the network.\n",
        "\n",
        "![tanh activation](img/rl-activation-functions-tanh.png){.column-margin}\n",
        "\n",
        "![rectified linear activation function](img/rl-activation-functions-relu.png){.column-margin}\n",
        "\n",
        "Martha White also mentions threshold activation functions. However these are not used in practice as they are not differentiable. There is some work since this course came out on compressing neural networks to use threshold activation functions which are easy to compute on a CPU as matrix multiplication becomes a series of comparisons. However these are trained with a differentiable approximation of the threshold function and then quantized to the threshold function.\n",
        "\n",
        "## The Neural Network Implementation\n",
        "\n",
        "![Neural Network Implementation](img/rl-nn-implementation.png){.column-margin}\n",
        "\n",
        "A neural network is a parameterized function that is a composition of linear and non-linear functions. It is a function of the state. The linear functions are the weights and the non-linear functions are the activation functions. The weights are learned from data.\n",
        "\n",
        "## Define a feed-forward architecture {#sec-l2g3}\n",
        "\n",
        "A feed forward architecture is a neural network where the connections between nodes do not form a cycle. The data flows from the input layer to the output layer.\n",
        "\n",
        "An example of a non-feed forward architecture is a recurrent neural network where the connections between nodes form cycles.\n",
        "\n",
        "## Non-linear Approximation with Neural Networks (video)\n",
        "\n",
        "## How Neural Networks are doing feature construction {#sec-l2g4}\n",
        "\n",
        ":::{.column-margin}\n",
        "![neural feature 1](rl-activation-1.png)\n",
        "\n",
        "darker means greater activation for the feature\n",
        ":::\n",
        "\n",
        ":::{.column-margin}\n",
        "![neural feature 2](rl-activation-2.png)\n",
        "\n",
        "the one generalize differently\n",
        ":::\n",
        "\n",
        "We construct a non-linear function of the state using a neural network.\n",
        "\n",
        "recall A node takes the form \n",
        "\n",
        "$$\n",
        "output = f[(w_1 \\times input_1) + \\ldots + (w_n \\times input_n) + b]\n",
        "$$\n",
        "\n",
        "\n",
        "We call this output of the node a feature! We can see that these features are a non-linear function of the inputs. We repeat this process until we evaluate all the nodes of the final layer. And the output of this final layer is called the representation.\n",
        "\n",
        "Note: This is not very different from tile coding where we pass input to a tile coder and get back a new representation of the state. \n",
        "\n",
        "In both cases we are constructing a non-linear mapping of the input of the features. And we take a nonlinear function of the representation to form the output - a nonlinear approximation of the state. \n",
        "\n",
        "Recall that in tile coding we had to set some hyper-parameters: size shape of tiles + number of tiling. These are fixed before training.\n",
        "In a neural network we also have hyperparameters for the size of the layers, the number of layers, the activation functions.  These too are fixed before training.\n",
        "\n",
        "The difference is that Neural networks have weights that get updated during training. But tile coding does not change during training.\n",
        "\n",
        "## How neural networks are a non-linear function of state {#sec-l2g5}\n",
        "\n",
        ":::{.column-margin}\n",
        "![neural feature 3](rl-activation-3.png)\n",
        "\n",
        "there are no hard boundaries\n",
        ":::\n",
        "\n",
        ":::{.column-margin}\n",
        "![neural feature 4](rl-activation-4.png)\n",
        "this shows how it generelises\n",
        ":::\n",
        "\n",
        "\n",
        "Neural networks are non linear functions of the because of the non-linear nature of the activation functions. These are applied recursively as we move to the final layer.\n",
        "\n",
        "\n",
        "## Deep Neural Networks (Video)\n",
        "\n",
        "## How deep networks are a composition of layers {#sec-l2g6}\n",
        "\n",
        "Neural networks are modular. We can add or remove layers. Each layer is a function of the previous layer. The output of the previous layer is the input to the next layer.\n",
        "\n",
        "Depth allows composition of features. Each layer can learn a different representation of the input. The final layer can learn a representation of the input that is a composition of the representations learned by the previous layers\n",
        "\n",
        "We can design the network to remove undesirable features. For example we can design a network with a bottleneck that has less features than the input. This forces the network to learn a compressed representation of the input.\n",
        "\n",
        "## The tradeoff between learning capacity and challenges presented by deeper networks {#sec-l2g7}\n",
        "\n",
        "Depth can increase the learning capacity of the network by allowing the network to learn complex compositions and abstractions. However, deeper networks are harder to train. \n",
        "\n",
        "# Lesson 3: Training Neural Networks \n",
        "\n",
        "::: callout-note\n",
        "### Learning Objectives {.unnumbered}\n",
        "\n",
        "-   [x] *Compute* the gradient for a single hidden layer neural network [\\#](#sec-l3g1)\n",
        "-   [x] *Understand* how to compute the gradient for arbitrarily deep networks [\\#](#sec-l3g2)\n",
        "-   [x] *Understand* the importance of initialization for neural networks [\\#](#sec-l3g3)\n",
        "-   [x] *Describe* strategies for initializing neural networks [\\#](#sec-l3g4)\n",
        "-   [x] *Describe* optimization techniques for training neural networks [\\#](#sec-l3g5)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "::: callout-note\n",
        "\n",
        "### Discussion prompt {.unnumbered}\n",
        "\n",
        "> What properties of the representation are important for our online setting? This contrasts the offline, batch setting. \n",
        "\n",
        ":::\n",
        "\n",
        "## Gradient Descent for Training Neural Networks (Video)\n",
        "\n",
        ":::{.column-margin}\n",
        "![](nn-notation.png){.column-margin}\n",
        ":::\n",
        "\n",
        "If we use the square error loss then \n",
        "\n",
        "$$\n",
        "L(\\hat y_k,y_k) = (\\hat y_k-y_k)^2 \\qquad\n",
        "$$ {#eq-loss}\n",
        "\n",
        "\n",
        "$$\n",
        "A = A −αδ^As\n",
        "$$\n",
        "\n",
        "$$\n",
        "B = B −αδ^Bx\n",
        "$$\n",
        "\n",
        "Let’s start at the output of the network and work backwards. Recall:\n",
        "$$\n",
        "x = f_A(sA)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y} = f_B(xB)\n",
        "$$\n",
        "\n",
        "We start by taking the partial derivative of the loss function with respect to the first set of weights B. \n",
        "\n",
        "We use the chain rule given the derivative of L with respect to $\\hat{Y} \\times \\frac{∂\\hat{y}}{∂B}$. The next step is again to use the chain rule for this derivative.\n",
        "\n",
        "$$\n",
        "\\frac{∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂\\hat{y}_k}{∂B_{jk}}\n",
        "$$\n",
        "\n",
        "let’s introduce a new variable, θ where θ is the output of the hidden layer times the last set of weights.\n",
        "\n",
        "$$\n",
        "θ \\dot = xB\n",
        "$$\n",
        "\n",
        "Thus\n",
        "\n",
        "$$\n",
        "\\hat y \\dot = f_B(θ)\n",
        "$$\n",
        "\n",
        "Rewriting we have:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂f_B(\\theta_k)}{∂\\theta} \\frac{∂\\theta_k}{∂B_{jk}}\n",
        "$$\n",
        "\n",
        "and since \n",
        "\n",
        "$$\n",
        "\\frac{∂\\theta_k}{∂B_{jk}} = x_j\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂f_B(\\theta_k)}{∂\\theta} x_j\n",
        "$$\n",
        "\n",
        "\n",
        "now that we calculated the gradient for the last layer we can move to the previous layer.\n",
        "\n",
        "we use \n",
        "\n",
        "$$\n",
        "\\Psi \\dot =  sA\n",
        "$$\n",
        "\n",
        "and \n",
        "\n",
        "$$\n",
        "x \\dot = f_A(\\Psi)\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}} &= \\delta_k^B \\frac {∂\\theta_k}{∂A_{ij}} \\newline\n",
        " & = \\delta_k^B B_{jk} \\frac {∂x_j}{∂A_{ij}} \\newline\n",
        "  & = \\delta_k^B B_{jk} \\frac {∂f_A(\\Psi_j)}{∂\\Psi_j} \\frac {∂\\Psi_j}{∂A_{ij}} \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "since \n",
        "\n",
        "$$\n",
        "\\frac {∂\\Psi_j}{∂A_{ij}} = s_{ij}\n",
        "$$\n",
        "\n",
        "we have\n",
        "\n",
        "$$\n",
        "\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}} = \\delta_k^B B_{jk} \\frac {∂f_A(\\Psi_j)}{∂\\Psi_j} s_{ij}\n",
        "$$\n",
        "\n",
        "We can clean up this derivative by again, defining a term $δ_A$.\n",
        "\n",
        "$$\n",
        "δ^A_j = (B_{jk}δ^B_k ) \\frac{∂f_A(ψ_j)}{∂ψ_j}\n",
        "$$\n",
        "\n",
        "The final result will be:\n",
        "\n",
        "$$\n",
        "\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}}= δ^A_j s_i\n",
        "$$\n",
        "\n",
        "Obtaining as a final result for both gradients the next expressions\n",
        "\n",
        "$$\n",
        "\\frac {∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = δ^B_k x_j\n",
        "$$\n",
        "\n",
        "## Computing the Gradient for a Single Hidden Layer Neural Network {#sec-l3g1}\n",
        "\n",
        "Let's summerize the results:\n",
        "\n",
        "$$\n",
        "\\frac {∂L(\\hat{y}_k,y_k)}{∂B_{jk}} = δ^B_k x_j \\qquad \n",
        "\\frac{∂L(\\hat{y}_k,y_k)}{∂A_{ij}}= δ^A_j s_i\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "δ^B_k = \\frac {∂L(\\hat{y}_k,y_k)}{∂\\hat{y}_k} \\frac{∂f_B(\\theta_k)}{∂\\theta} \\qquad\n",
        "δ^A_j = (B_{jk}δ^B_k ) \\frac{∂f_A(ψ_j)}{∂ψ_j}\n",
        "$$\n",
        "\n",
        "\n",
        "## Computing the Gradient for Arbitrarily Deep Networks {#sec-l3g2}\n",
        "\n",
        "![Gradient Descent Pseudo-code](img/nn-backpop.png){#fig-gradient-descent-pseudo-code  .column-margin }\n",
        "\n",
        "![Gradient Descent Pseudo-code for RELU](img/nn-backpop-relu.png){#fig-gradient-descent-RELU  .column-margin }\n",
        "\n",
        "Now that we have estimated the gradient for a hidden layer neural network.\n",
        "We can use it to learn to optimize the weights of the network by updating the weights to minimize the error in the loss function in the direction of the negative gradient.\n",
        "\n",
        "The [pseudocode in the figure](#fig-gradient-descent-pseudo-code) outlines how to  implementing the backprop algorithm with Stochastic gradient descent.\n",
        "\n",
        "For each data point s, y in our dataset, we first get our prediction $\\hat{y}$ from the network. This is the forward pass. Then we can estimate the loss using the actual value $y$\n",
        "\n",
        "Next we compute the gradients starting from the output. We first compute $δ^B$ and the gradient for $B$, then we use this gradient to update the parameters $B$, with the step size $α_B$ for the last layer.\n",
        "\n",
        "Next, we update the parameters $A$. We compute $δ^A$ which reuses $δ^B$. \n",
        "\n",
        "Notice, that by computing the gradients of the end of the network first, we avoid recomputing the same terms for A, that were already computed for $δB$. We then compute the gradient for A and update A with this gradient using step size $α_A$.\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "Next we look at how we adapt the [pseudocode](#fig-fig-gradient-descent-RELU)  to work with the ReLU activation on the hidden layer and a linear unit for the output. \n",
        "\n",
        "First, we compute the error for the output layer, then we compute the derivative of the ReLU units with respect to $\\Psi$, and finally, we use the aerial signal from the output layer along with you to compute the air signal for the hidden layer, the rest remains the same\n",
        "\n",
        "\n",
        "\n",
        "## Optimization Strategies for NNs (Video)\n",
        "\n",
        "\n",
        "## The Importance of Initialization for Neural Networks {#sec-l3g3}\n",
        "\n",
        "One simple yet effective initialization strategy for the weights, is to randomly sample the initial weights from a normal distribution with small variance Fig. 42. This way, each neuron has a different output from other neurons within its layer. This provides a more diverse set of potential features. \n",
        "\n",
        "![Weights initialization](img/rl-feedforward-nn.png){.column-margin}\n",
        "\n",
        "By keeping the variants small, we ensure that the output of each neuron is within the same range as its neighbors. One downside to this strategy is that, as we add more inputs to a neuron, the variance of the output grows.\n",
        "\n",
        "\n",
        "## Strategies for Initializing Neural Networks {#sec-l3g4}\n",
        "\n",
        "$$\n",
        "W_{init} ~ N(0,1)\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "W_{init} ~ \\frac{N(0,1)}{\\sqrt{n_{in}}}\n",
        "$$\n",
        "\n",
        "## Optimization Techniques for Training Neural Networks {#sec-l3g5}\n",
        "\n",
        "- momentum update AKA heavy ball method\n",
        "$$\n",
        "W_{t+1} ← W_t −α∇_wL(W_t) + λM_t\n",
        "$$\n",
        "\n",
        "$$\n",
        "M_{t+1} = λM_t −α∇_wL\n",
        "$$\n",
        "\n",
        "vector step size adaptation\n",
        "\n",
        "- separate step size for each weight\n",
        "\n",
        "![](nn-vector-step-size.png){.column-margin}\n"
      ],
      "id": "bc0d15a7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}