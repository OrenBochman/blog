---
date: 2024-04-03
lastmod: 2024-04-03
title: Control with Approximation
subtitle: Prediction and Control with Function Approximation
author: Oren Bochman
draft: false
categories:
  - Coursera
  - notes
  - rl
  - reinforcement learning
keywords:
  - reinforcement learning
  - neural networks
  - feature construction
  - Episodic Semi-Gradient Sarsa Algorithm
  - Differential Semi-Gradient Sarsa Algorithm
image: /images/nlp-brain-wordcloud.jpg
title-block-banner: /images/banner_black_3.jpg
---

![RL logo](img/logo.png){.column-margin} 

![RL algorithms](img/alg_selector.png){.column-margin group="slides"}


::: {.callout-tip collapse="true"}
### Readings {.unnumbered}

-   [x] [@sutton2018reinforcement§10 pp. 243-246] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=243)
-   [x] [@sutton2018reinforcement§10.3 pp. 249-252] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=249)
:::

# Lesson 1: Episodic Sarsa with Function Approximation

::: callout-note
### Learning Objectives {.unnumbered}

- [x] *Explain* the update for Episodic Sarsa with function approximation [\#](#sec-l1g1)
- [x] *Introduce* the feature choices, including passing actions to features or stacking state features [\#](#sec-l1g2)
- [x] *Visualize* value function and learning curves [\#](#sec-l1g3)
- [x] *Discuss* how this extends to Q-learning easily, since it is a subset of Expected Sarsa [\#](#sec-l1g4)
:::

## Episodic SARSA with Function Approximation (Video)

In this video Adam White discusses the Episodic SARSA algorithm with function approximation. He explains how this algorithm can be used to solve reinforcement learning problems with large state spaces. He also discusses the importance of feature choices in this algorithm and how they can impact the performance of the system.

## How to Construct action Dependent Features {#sec-l1g1}

![Stacking](nn-feature-stacking.png){.column-margin}

We see two techniques for constructing action dependent features. 

The first is called "Stacking" and involves concatenating the state features with the action features. This is a simple and effective way to construct action dependent features.

Adam White explains that this technique is used both for linear function approximation and for neural networks in much the same way. However the main issue with stacking is that it keeps the same state features used for different actions separate. This seems to be both an overparameterization (i.e. overfitting) and a an impediment to learning a good representation of the state.

The next technique called "Passing Actions to Features" attempts to remedy this issue. In this technique, the state features are passed through a neural network that also takes the action as input. This allows the network to learn a better representation of the state that is dependent on the action. This technique is more complex but can lead to better performance.



## Explain how to use SARSA in episodic tasks with function approximation

## The the update for Episodic SARSA with function approximation {#sec-l1g1}

So far we have been using function approximation to parametize state value function,

$$
V_\pi(s) ≈ \hat{v}(s,w) \ \dot = \ \mathbf{w}^T \cdot \mathbf{x}(S) \qquad
$$ {#eq-parametize-state-value-function}

But for SARSA we need to parametize the action value function,

$$
q_\pi(s) ≈ \hat{q}(s,a,\mathbf{w}) \ \dot = \ \mathbf{w}^T \cdot \mathbf{x}(s,a) \qquad
$$ {#eq-parametize-action-value-function}

## Feature Choices in Episodic SARSA with Function Approximation {#sec-l1g2}


## Visualizing Value Function and Learning Curves {#sec-l1g3}


## Episodic SARSA in Mountain Car (Video)


## Expected SARSA with Function Approximation (Video)

## How this extends to Q-learning easily, since it is a subset of Expected SARSA {#sec-l1g4}

# Lesson 2: Exploration under Function Approximation 

::: callout-note
### Learning Objectives {.unnumbered}

- [x] *Understanding* optimistically initializing your value function as a form of exploration [\#](#sec-l2g1)
:::

## Exploration under Function Approximation (Video)

## Optimistic Initialization as a Form of Exploration {#sec-l2g1}

# Lesson 3: Average Reward 

::: callout-note
### Learning Objectives {.unnumbered}

- [x] *Describe* the average reward setting [\#](#sec-l3g1)
- [x] *Explain* when average reward optimal policies are different from discounted solutions [\#](#sec-l3g2)
- [x] *Understand* how differential value functions are different from discounted value functions [\#](#sec-l3g3)
:::

## Average Reward: A New Way of Formulating Control Problems  (Video)

## The Average Reward Setting {#sec-l3g1}

## When Average Reward Optimal Policies are Different from Discounted Solutions {#sec-l3g2}

## Differential Value Functions v.s. Discounted Value Functions {#sec-l3g3}

## Satinder Singh on Intrinsic Rewards (Video)

Satinder Singh is a professor at the University of Michigan. He is a leading researcher in reinforcement learning and has made significant contributions to the field. In this video, he discusses intrinsic rewards and how they can be used to improve learning in reinforcement learning systems. It's worth noting that he is one of the reaserchers who has worked on options with Doina Precup.

::: callout-note

### Discussion prompt {.unnumbered}

> What are the issues with extending some of the exploration methods we learned about bandits and Dyna to the full RL problem? 
  How can we do visitation counts or UCB with function approximation?
  
> A control agent with function approximation has to explore to find the best policy, learn a good state representation, and try to get a lot of reward, all at the same time. 
  How might an agent balance these potentially conflicting goals?  

:::


::: {.column-margin #fig-subtasks}
{{< video https://youtu.be/L-9Pbhpk7pQ
    title='From Reinforcement Learning to Artificial Intelligence ?' >}}

This is a high level talk by Satinder Singh on AI and RL
:::

::: {.column-margin #fig-subtasks}
{{< video https://youtu.be/jn1NE8uIxgw
    title='Steps Towards Continual Learning' >}}

This talk  titled 'Steps Towards Continual Learning' by Satinder Singh on Reinforcement Learning at DLSS & RLSS 2017 - Montreal
:::



::: {.column-margin #fig-subtasks}
{{< video https://youtu.be/Z7JhZx3urEY
    title='Discovery in Reinforcement Learning' >}}

Talk titled 'Discovery in Reinforcement Learning' at Beijing Academy of Artificial Intelligence by Satinder Singh on Two Pieces of Research on Exploration in Reinforcement Learning.
:::
