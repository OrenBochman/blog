---
date: 2024-04-03
lastmod: 2024-04-03
title: Control with Approximation
subtitle: Prediction and Control with Function Approximation
author: Oren Bochman
draft: false
categories:
  - Coursera
  - notes
  - rl
  - reinforcement learning
keywords:
  - reinforcement learning
  - neural networks
  - feature construction
  - Episodic Semi-Gradient Sarsa Algorithm
  - Differential Semi-Gradient Sarsa Algorithm
image: /images/nlp-brain-wordcloud.jpg
title-block-banner: /images/banner_black_3.jpg
---

![RL algorithms](img/alg_selector.png){.column-margin}

::: {.callout-tip collapse="true"}
### Readings {.unnumbered}

-   [x] [@sutton2018reinforcement§10 pp. 243-246] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=243)
-   [x] [@sutton2018reinforcement§10.3 pp. 249-252] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=249)
:::

# Lesson 1: Episodic Sarsa with Function Approximation

::: callout-note
### Learning Objectives {.unnumbered}

- [x] *Explain* the update for Episodic Sarsa with function approximation [\#](#sec-l1g1)
- [x] *Introduce* the feature choices, including passing actions to features or stacking state features [\#](#sec-l1g2)
- [x] *Visualize* value function and learning curves [\#](#sec-l1g3)
- [x] *Discuss* how this extends to Q-learning easily, since it is a subset of Expected Sarsa [\#](#sec-l1g4)
:::

## Episodic Sarsa with Function Approximation (Video)

## Episodic Sarsa in Mountain Car (Video)

## Expected Sarsa with Function Approximation (Video)






# Lesson 2: Exploration under Function Approximation 

::: callout-note
### Learning Objectives {.unnumbered}

- [x] *Understanding* optimistically initializing your value function as a form of exploration [\#](#sec-l2g1)
:::

## Exploration under Function Approximation (Video)

# Lesson 3: Average Reward 

::: callout-note
### Learning Objectives {.unnumbered}

- [x] *Describe* the average reward setting [\#](#sec-l3g1)
- [x] *Explain* when average reward optimal policies are different from discounted solutions [\#](#sec-l3g2)
- [x] *Understand* how differential value functions are different from discounted value functions [\#](#sec-l3g3)
:::

## Average Reward: A New Way of Formulating Control Problems  (Video)

## Satinder Singh on Intrinsic Rewards (Video)

Satinder Singh is a professor at the University of Michigan. He is a leading researcher in reinforcement learning and has made significant contributions to the field. In this video, he discusses intrinsic rewards and how they can be used to improve learning in reinforcement learning systems. It's worth noting that he is one of the reaserchers who has worked on options with Doina Precup.





::: callout-note

### Discussion prompt {.unnumbered}

> What are the issues with extending some of the exploration methods we learned about bandits and Dyna to the full RL problem? 
  How can we do visitation counts or UCB with function approximation?
  
> A control agent with function approximation has to explore to find the best policy, learn a good state representation, and try to get a lot of reward, all at the same time. 
  How might an agent balance these potentially conflicting goals?  

:::
