---
date: 2024-04-03
lastmod: 2024-04-03
title: Control with Approximation
subtitle: Prediction and Control with Function Approximation
author: Oren Bochman
draft: false
categories:
  - Coursera
  - notes
  - rl
  - reinforcement learning
  - the k-armed bandit problem
  - bandit algorithms
  - exploration 
  - explotation
  - epsilon greedy algorithm
  - sample avarage method
image: /images/nlp-brain-wordcloud.jpg
title-block-banner: /images/banner_black_3.jpg
---

![RL algorithms](img/alg_selector.jpeg){.column-margin}

::: {.callout-tip collapse="true"}
### Readings {.unnumbered}

-   [x] [@sutton2018reinforcement§10 pp. 243-246] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=243)
-   [x] [@sutton2018reinforcement§10.3 pp. 249-252] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=249)
:::

# Lesson 1: Episodic Sarsa with Function Approximation

::: callout-note
### Learning Objectives {.unnumbered}

- [x] *Explain* the update for Episodic Sarsa with function approximation [\#](#sec-l1g1)
- [x] *Introduce* the feature choices, including passing actions to features or stacking state features [\#](#sec-l1g2)
- [x] *Visualize* value function and learning curves [\#](#sec-l1g3)
- [x] *Discuss* how this extends to Q-learning easily, since it is a subset of Expected Sarsa [\#](#sec-l1g4)
:::

# Lesson 2: Exploration under Function Approximation 

::: callout-note
### Learning Objectives {.unnumbered}

- [x] *Understanding* optimistically initializing your value function as a form of exploration [\#](#sec-l2g1)
:::

# Lesson 3: Average Reward 

::: callout-note
### Learning Objectives {.unnumbered}

- [x] *Describe* the average reward setting [\#](#sec-l3g1)
- [x] *Explain* when average reward optimal policies are different from discounted solutions [\#](#sec-l3g2)
- [x] *Understand* how differential value functions are different from discounted value functions [\#](#sec-l3g3)
:::


::: callout-note

### Discussion prompt {.unnumbered}

> What are the issues with extending some of the exploration methods we learned about bandits and Dyna to the full RL problem? 
  How can we do visitation counts or UCB with function approximation?
  
> A control agent with function approximation has to explore to find the best policy, learn a good state representation, and try to get a lot of reward, all at the same time. 
  How might an agent balance these potentially conflicting goals?  

:::
