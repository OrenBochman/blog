---
date: 2024-04-04
lastmod: 2024-04-04
title: Policy Gradient 
subtitle: Prediction and Control with Function Approximation
author: Oren Bochman
draft: false
categories:
  - Coursera
  - notes
  - rl
  - reinforcement learning
  - the k-armed bandit problem
  - bandit algorithms
  - exploration 
  - explotation
  - epsilon greedy algorithm
  - sample avarage method
image: /images/nlp-brain-wordcloud.jpg
title-block-banner: /images/banner_black_3.jpg
---

![RL algorithms](img/alg_selector.jpeg){.column-margin}

::: {.callout-tip collapse="true"}
### Readings {.unnumbered}

-   [x] [@sutton2018reinforcement§13 pp. 321-336] [book](http://incompleteideas.net/book/RLbook2020.pdf#page=321)

:::


# Lesson 1: Learning Parameterized Policies 


::: callout-note
### Learning Objectives {.unnumbered}

-   [x] Understand how to define policies as parameterized functions [\#](#l1g1)
-   [x] Define one class of parameterized policies based on the softmax function [\#](#l1g2)
-   [x] Understand the advantages of using parameterized policies over action-value based methods [\#](#l1g3)
:::

# Lesson 2: Policy Gradient for Continuing Tasks 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] Describe the objective for policy gradient algorithms [\#](#l2g1)
-   [x] Describe the results of the policy gradient theorem [\#](#l2g2)
-   [x] Understand the importance of the policy gradient theorem [\#](#l2g3)
:::

# Lesson 3: Actor-Critic for Continuing Tasks 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] Derive a sample-based estimate for the gradient of the average reward objective [\#](#l3g1)
-   [x] Describe the actor-critic algorithm for control with function approximation, for continuing tasks [\#](#l3g2)

:::

# Lesson 4: Policy Parameterizations 

::: callout-note
### Learning Objectives {.unnumbered}

- [x] Derive the actor-critic update for a softmax policy with linear action preferences [\#](#l4g1)
- [x] Implement this algorithm [\#](#l4g2)
- [x] Design concrete function approximators for an average reward actor-critic algorithm [\#](#l4g3)
- [x] Analyze the performance of an average reward agent [\#](#l4g4)
- [x] Derive the actor-critic update for a gaussian policy [\#](#l4g5)
- [x] Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions [\#](#l4g6)

:::


::: callout-note

### Discussion prompt {.unnumbered}

> Are tasks really ever continuing? Everything eventually breaks or dies. 
  It’s clear that individual people do not learn from death, but we don’t live forever. 
  Why might the continuing problem formulation be a reasonable model for long-lived agents?  
:::
