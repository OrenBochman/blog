---
date: 2024-04-04
lastmod: 2024-04-04
title: Policy Gradient 
subtitle: Prediction and Control with Function Approximation
author: Oren Bochman
draft: false
categories:
  - Coursera
  - notes
  - rl
  - reinforcement learning
keywords:
  - reinforcement learning
  - neural networks
  - feature construction
  - deep networks
  - The Policy Gradient Theorem
  - Policy Gradient
  - Reinforce Algorithm
  - Actor-Critic Algorithm
  - Gaussian Policies
image: /images/nlp-brain-wordcloud.jpg
title-block-banner: /images/banner_black_3.jpg
---

![RL logo](img/logo.png){.column-margin} 

::: {#fig-episodic-semi-gradient-sarsa .column-margin}

![RL algorithm decision tree](img/alg_selector.png){group="slides"}

The algorithms we will be discussing in this lesson are all part of the policy gradient family.
:::

::: {.callout-tip collapse="true"}
### Readings {.unnumbered}

-   [x] [@sutton2018reinforcement§13 pp. 321-336] [@fig-chapter-13-policy-gradient]

Extra recourses - may be useful to review though not required:

- [Intro to Policy Optimization @ Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)
- [Understanding Policy Gradients](https://johnwlambert.github.io/policy-gradients/)
- [Policy Gradient Explained](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146)
:::


::: {.column-margin #fig-subtasks}
{{< video https://www.youtube.com/watch?v=PtAIh9KSnjo
    title='Deep Reinforcement Learning ( OpenAI)' >}}

This talk titled 'SDeep Reinforcement Learning Policy Gradients and Q-Learning' by John Schulman (who developed PPO and TRPO and Chat-GPT) on Reinforcement Learning at at the Deep Learning School on September 24/25, 2016
:::


::: {.column-margin #fig-subtasks}
{{< video https://www.youtube.com/watch?v=AKbX1Zvo7r8
    title='Policy Gradients and Advantage Estimation' >}}

Talk titled 'Policy Gradients and Advantage Estimation' by Pieter Abbeel. in his "Foundations of Deep RL Series"
:::

::: {.column-margin #fig-subtasks}
{{< video https://www.youtube.com/watch?v=y3oqOjHilio
    title='Policy-Gradient and Actor-Critic methods' >}}

Research Scientist Hado van Hasselt covers policy algorithms that can learn policies directly and actor critic algorithms that combine value predictions for more efficient learning. From DeepMind x UCL | Deep Learning Lecture Series 2021
:::

# Lesson 1: Learning Parameterized Policies 


::: callout-note
### Learning Objectives {.unnumbered}

-   [x] Understand how to define policies as parameterized functions [\#](#sec-l1g1)
-   [x] Define one class of parameterized policies based on the softmax function [\#](#sec-l1g2)
-   [x] Understand the advantages of using parameterized policies over action-value based methods [\#](#sec-l1g3)
:::

## Learning Policies Directly (Video)


## How to Define Policies as Parameterized Functions {#sec-l1g1}

::: {#fig-policy-gradient .column-margin}
![policy gradient](pg-01.png){group="slides"}

:::

::: {#fig-policy-gradient-param .column-margin}
![policy parametrization](pg-02.png){group="slides"}

:::


::: {#fig-policy-gradient-constraint .column-margin}
![policy parametrization constraints](pg-03.png){group="slides"}

:::


::: {#fig-policy-softmax .column-margin}
![softmax properties](pg-04-softmax-policy.png){group="slides"}

:::

- negative values of h lead to positive action probabilities.
- equal values of h lead to equal action probabilities.


::: {#fig-policy-action-preferences .column-margin}
![softmax policy v.s. epsilon-greedy](pg-05-action-preferences.png){group="slides"}

:::

- the softmax policy is a better option over than the $\epsilon$-greedy policy over the action-value based methods.

## Define one class of parameterized policies based on the softmax function {#sec-l1g2}


$$
\pi(a|s, \theta) \doteq \frac{e^{h(s, a, \theta)}}{\sum_{b\in \mathcal{A}} e^{h(s, b, \theta)}} \qquad
$$ {#eq-softmax-policy}

- the numerator is the exponential of the action preference
- the denominator is the sum of the exponentials of all action preferences

## Advantages of Policy Parameterization (Video)

In this video we consider the advantages of using parameterized policies over action-value based methods. We will see that parameterized policies are more flexible than action-value based methods and can start off stochastic and then become deterministic.

## Advantages of Using Parameterized Policies over Action-Value Based Methods {#sec-l1g3}



# Lesson 2: Policy Gradient for Continuing Tasks 

- parameterized policies are more flexible than action-value based methods
- can start off stochastic and then become deterministic

In function approximation, the optimal policy is not necessarily deterministic.
Thus it is best to be able to learn stochastic policies.

Example where the optimal policy is stochastic:

::: {#fig-stochastic-policy .column-margin}

![When a deterministic policy fails](pg-06-stochastic-policy.png){group="slides"}

A minimal environment where a determinsitic policy would fail the optimal policy $\pi_*$ is stochastic.
:::

- Sometimes it is just easier to learn a stochastic policy.
- E.g. in mountain car, the parameterized value function is complex, but the parameterized policy is simple.


::: {#fig-mountain-car-policy .column-margin}
![mountain car environment values and policy](pg-07-stochastic-simpler.png){group="slides"}

In mountain car, the parameterized value function is complex, but the parameterized policy is simple.
:::

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] Describe the objective for policy gradient algorithms [\#](#sec-l2g1)
-   [x] Describe the results of the policy gradient theorem [\#](#sec-l2g2)
-   [x] Understand the importance of the policy gradient theorem [\#](#sec-l2g3)
:::

## The Objective for Learning Policies (Video)

In this video Martha White dive deep into the objective for policy gradient algorithms. She then contrasts this with the gradient of the value fn objectives and lists some challanges for estimating the gradient of the average reward objective. We will learn that we can use the policy gradient theorem and to overcome these challenges and derive a gradient with an neat update rule .


## The objective for policy gradient algorithms {#sec-l2g1}

Formalizing the goal as an Objective

$$
G_t = \sum_{t=0}^{T}  R_{t} \quad \text{episodic}
$$

$$
G_t = \sum_{t=0}^{\infty} \gamma^t R_{t} \quad \text{continuing - discounted reward}
$$

$$
G_t = \sum_{t=0}^{\infty} R_{t} - r(\pi) \quad \text{episodic - average reward}
$$

The average reward Objective for a policy is as follows:
$$
r(\pi) = \sum_{t=0}^{T} \mu(s) \sum_{a} \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) r \quad \text{avg. reward objective} 
$$ {#eq-average-reward-objective}

What does this mean?

- the last sum is the expected reward for a state-action pair. $\mathbb{E}[R_t \mid S_t = s , A_t=a]$
- the last two sums together are the expected reward for a state under weighted by the policy $\pi$. $\mathbb{E}_\pi[R_t \mid S_t = s]$
- full sum ads the time we spend in state $s$ under $\pi$ therefore the expected reward for a state under the policy $\pi$ and the environment dynamics $p$. $\mathbb{E}_\pi[R_t]$

to optimize the average reward, we need to estimate the gradient of [the avg. objective](#eq-average-reward-objective)


$$
\nabla_\theta r(\pi) = \nabla_\theta \sum_{t=0}^{T} \textcolor{red}{\underbrace{\mu(s)}_{\text{Depends on }\theta}} \sum_{a} \pi(a \mid s, \theta) \sum_{s',r} p(s',r \mid s,a) r \qquad
$$ {#eq-average-reward-objective-gradient}

1. Methods based on this are called policy gradient methods.
2. We are trying to maximize the average reward.

There are a few challenges with using the gradient in [#eq-average-reward-objective-gradient]:

According to the lesson $\mu(s)$ depends on $\theta$. Martha White point out that this state importance though parameterized only by s actually depends on the the policy $\pi$ which will evolve during its training based on the values of $\theta$. Which means out notation here is a bit misleading.  She then contrasts it with the value function gradient is being evaluated using a fixed policy.

$$
\begin{align*}
\nabla_w \bar{VE} &= \nabla_w \sum_{s}\textcolor{red}{\underbrace{\mu(s)}_{\text{Independent of }\mathbf{w}}}  [V_{\pi}(s)-\bar{v}(s,w)]^2 \newline
&=\sum_{s} \textcolor{red}{\mu(s)}  \nabla_w [V_{\pi}(s)-\bar{v}(s,w)]^2 
\end{align*}
$$


We can avg reward as an objective for policy optimization and the its for the stochastic gradient ascent. Next we will consider how the policy gradient theorem can help us estimate the gradient of the average reward objective despite these setbaks.

## The Policy Gradient Theorem (Video)

## The results of the policy gradient theorem {#sec-l2g2}

## The importance of the policy gradient theorem {#sec-l2g3}


# Lesson 3: Actor-Critic for Continuing Tasks 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] Derive a sample-based estimate for the gradient of the average reward objective [\#](#sec-l3g1)
-   [x] Describe the actor-critic algorithm for control with function approximation, for continuing tasks [\#](#sec-l3g2)

:::

# Lesson 4: Policy Parameterizations 

::: callout-note
### Learning Objectives {.unnumbered}

- [x] Derive the actor-critic update for a softmax policy with linear action preferences [\#](#sec-l4g1)
- [x] Implement this algorithm [\#](#sec-l4g2)
- [x] Design concrete function approximators for an average reward actor-critic algorithm [\#](#sec-l4g3)
- [x] Analyze the performance of an average reward agent [\#](#sec-l4g4)
- [x] Derive the actor-critic update for a gaussian policy [\#](#sec-l4g5)
- [x] Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions [\#](#sec-l4g6)

:::


::: callout-note

### Discussion prompt {.unnumbered}

> Are tasks really ever continuing? Everything eventually breaks or dies. 
  It’s clear that individual people do not learn from death, but we don’t live forever. 
  Why might the continuing problem formulation be a reasonable model for long-lived agents?  
:::


::: {#fig-chapter-13-policy-gradient }
![Chapter 13](ch13-policy-gradient.pdf){.col-page width=700px height=1000px}

Chapter 13 of [@sutton2018reinforcement] covering policy gradient methods.
:::