---
date: 2024-04-04
lastmod: 2024-04-04
title: Policy Gradient 
subtitle: Prediction and Control with Function Approximation
author: Oren Bochman
draft: false
categories:
  - Coursera
  - notes
  - rl
  - reinforcement learning
keywords:
  - reinforcement learning
  - neural networks
  - feature construction
  - deep networks
  - The Policy Gradient Theorem
  - Policy Gradient
  - Reinforce Algorithm
  - Actor-Critic Algorithm
  - Gaussian Policies
image: /images/nlp-brain-wordcloud.jpg
title-block-banner: /images/banner_black_3.jpg
---

![RL logo](img/logo.png){.column-margin} 

::: {#fig-episodic-semi-gradient-sarsa .column-margin}

![RL algorithm decision tree](img/alg_selector.png){group="slides"}

The algorithms we will be discussing in this lesson are all part of the policy gradient family. Sometimes the behavior codified in the policy is much simpler then the action value function. So learning a policy directly can be more efficient. In reality, learning policy is an end to end solution and to solve many real-world RL problem we will want to put to bear as much of the methods as we have learned so far. This can be done under the umbrella of these policy gradient methods. Once we cover the policy gradient theorem, we will see how we still get to use action value approximations for estimating the gradient of the average reward objective. Another way that we will make use of value functions approximations is in the actor-critic algorithm.

:::

::: {.callout-tip collapse="true"}
### Course Readings {.unnumbered}

-   [x] [@sutton2018reinforcement§13 pp. 321-336] [@fig-chapter-13-policy-gradient]

:::

::: {.callout-tip collapse="true"}
### Extra Readings {.unnumbered}

Extra resources I found useful to review though not required, nor part of the course material 

- [Intro to Policy Optimization @ Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)
- [Understanding Policy Gradients](https://johnwlambert.github.io/policy-gradients/)
- [Policy Gradient Explained](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146)
:::


:::: {.callout-tip collapse="true"}
### Videos {.unnumbered}

::: {.column-margin #fig-deep-reinforcement-learning}
{{< video https://www.youtube.com/watch?v=PtAIh9KSnjo
    title='Deep Reinforcement Learning' >}}

Talk titled 'Deep Reinforcement Learning Policy Gradients and Q-Learning' by John Schulman on Reinforcement Learning at at the Deep Learning School on September 24/25, 2016
:::

::: {.column-margin #fig-policy-gradients-and-advantage-estimation}
{{< video https://www.youtube.com/watch?v=AKbX1Zvo7r8
    title='Policy Gradients and Advantage Estimation' >}}

Talk titled 'Policy Gradients and Advantage Estimation' by Pieter Abbeel. in his 'Foundations of Deep RL Series'
:::

::: {.column-margin #fig-policy-grad-limitations}
{{< video https://www.youtube.com/watch?v=y3oqOjHilio
    title='Policy-Gradient and Actor-Critic methods' >}}

Research Scientist Hado van Hasselt from Deep Mind covers policy algorithms that can learn policies directly and actor critic algorithms that combine value predictions for more efficient learning. From DeepMind x UCL | Deep Learning Lecture Series 2021
:::

I found policy gradient methods to be complicated at first blush. Perhaps a bit less so then the value function since they introduced function approximation. Also these methods are foundational for most of the algorithms in Deep RL. The course material is very concise and laser focused on very specific goals. Also I was disappointed that this course does not cover more modern algorithms like TRPO, PPO or other Deep learning algorithm. I cannot stress this point enough. In the last video in the previous lecture's notes by Satinder Singh, all of the research on using Meta gradients to learn intrinsic rewards is also built on top of policy gradient methods - where he and his students looked at proprgating these gradients through multiple the planing algorithms and later through the learning algorithm to learn a reward function and tackle the issues of exploration.

Here are three extra videos by experts in the field that delve deeper into this topic. Each of these instructors have published papers with some of the most groundbreaking algorithms in the field and have a lot of insights to share. 

1. In [this lecture](#fig-deep-reinforcement-learning) [John Schulman](http://joschu.net/) covers Deep Reinforcement Learning Policy Gradients and Q-Learning. John Schulman is a research scientist at OpenAI and has published many papers on RL and Robotics. John Schulman who developed PPO and TRPO and Chat-GPT)


2. In [this lecture](#fig-policy-gradients-and-advantage-estimation) [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/) covers policy gradients and advantage estimation. Pieter Abbeel is a professor at UC Berkeley and has published many papers on RL and Robotics.

3. In [this lesson from a Deep Mind Course](#fig-policy-grad-limitations) [Hado van Hasselt](https://hadovanhasselt.com/about/) covers some advantages as well as challenges of policy gradient methods.

::::

# Lesson 1: Learning Parameterized Policies 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] Understand how to define policies as parameterized functions [\#](#sec-l1g1)
-   [x] Define one class of parameterized policies based on the softmax function [\#](#sec-l1g2)
-   [x] Understand the advantages of using parameterized policies over action-value based methods [\#](#sec-l1g3)
:::

## Learning policies directly (Video)

::: {#fig-policy-gradient .column-margin}
![Energy pumping policy](pg-01.png){group="slides"}

In the mountain car environment, the parameterized value function is complex, but the parameterized policy is simple. 
:::


In this lesson course instructor Adam White introduces the idea of learning policies directly. He contrasts this with learning value functions and explains why learning policies directly can be more flexible and powerful. 

## How to Parametrize a Policies? {#sec-l1g1}

::: {#fig-policy-gradient-param .column-margin}
![policy parametrization](pg-02.png){group="slides"}

When we parametrize a policy we will use the greek letter $\theta$ to denote the parameters of the policy.
:::

::: {#fig-policy-gradient-constraint .column-margin}
![policy parametrization constraints](pg-03.png){group="slides"}

Constraints on the policy parameters can be used to ensure that the policy is valid.
:::

So far we have been mostly looking at learning value functions. But when it comes to function approximation, it is often simpler to learn a policy directly.

In [the mountain car environment](#fig-policy-gradient) we see the power pumping policy which accelerates the car in the direction it is moving. This is a near optimal policy for this environment. The policy is simple and can be learned directly and it makes no use of value functions. This may not always be the case.

A visual summary of the policy parametrization is shown in [the figure](#fig-policy-gradient-param). Recall that the policy is a function that takes in a state and outputs a probability distribution over actions. We will use the greek letter $\theta$ to denote the parameters of the policy. This way we can reference the parameters of $\hat{Q}(s,a,w)$ the action value function are denoted by $\mathbf{w}$.


Since we are dealing with probabilities, the policy parameters must satisfy certain constraints. For example, the probabilities must sum to one. This is shown in [the figure](#fig-policy-gradient-constraint). These policy parameters constraints will ensure that the policy is valid. 

## Define one class of parameterized policies based on the softmax function {#sec-l1g2}

::: {#fig-policy-softmax .column-margin}
![softmax properties](pg-04-softmax-policy.png){group="slides"}

:::

The Softmax policy also called the Boltzmann distribution is a probability distribution over actions given a state. It is parameterized by a vector of action preferences $h(s, a, \theta)$. 

$$
\pi(a \mid s, \theta) \doteq \frac{e^{h(s, a, \theta)}}{\sum_{b\in \mathcal{A}} e^{h(s, b, \theta)}} \text{(softmax policy)} \qquad 
$$ {#eq-softmax-policy}

- the numerator is the exponential of the action preference
- the denominator is the sum of the exponentials of all action preferences

Some properties of the softmax policy are that it can take in a vector of weights for different actions and output a probability distribution over actions.
A second property is that the softmax policy generalizes the max function.
A third property is that unlike the max function which is discontinuous the softmax policy is differentiable, making it amenable to gradient-based optimization.

- negative values of h lead to positive action probabilities.
- equal values of h lead to equal action probabilities.
- the softmax policy is a better option over than the $\epsilon$-greedy policy over the action-value based methods.

## Advantages of Policy Parameterization (Video)

In this video we consider the advantages of using parameterized policies over action-value based methods. We will see that parameterized policies are more flexible than action-value based methods and can start off stochastic and then become deterministic.

## Advantages of using parameterized policies over action-value based methods {#sec-l1g3}

::: {#fig-policy-action-preferences .column-margin}
![softmax policy v.s. epsilon-greedy](pg-05-action-preferences.png){group="slides"}

Softmax policy v.s. $\epsilon$-greedy
:::

::: {#fig-short-corridor-env .column-margin}

![Short corridor with switched action](pg-06-stochastic-policy.png){group="slides"}

In the Short corridor with switched action environment a deterministic policy fails to reach the goal. The only optimal policy is stochastic.
:::

> One advantage of parameterizing policies according to the soft-max in action preferences is **that the approximate policy can approach a deterministic policy**, whereas with $\epsilon$-greedy action selection over action values there is always an $\epsilon$ probability of selecting a random action.

> A second advantage of parameterizing policies according to the soft-max in action preferences is that **it enables the selection of actions with arbitrary probabilities**. In problems with significant function approximation, the best approximate policy may be stochastic.

For example, in card games with imperfect information the optimal play is
often a mixed strategy which means you should take two different actions each with a specific probability, such as when bluffing in Poker.

Action-value methods have no natural way of finding stochastic optimal policies, whereas policy approximating methods can, as shown in [The short Corridor environment](#fig-short-corridor-env) 


# Lesson 2: Policy Gradient for Continuing Tasks 

- parameterized policies are more flexible than action-value based methods
- can start off stochastic and then become deterministic

In function approximation, the optimal policy is not necessarily deterministic.
Thus it is best to be able to learn stochastic policies.

Example where the optimal policy is stochastic:

- Sometimes it is just easier to learn a stochastic policy.
- E.g. in mountain car, the parameterized value function is complex, but the parameterized policy is simple.


::: {#fig-mountain-car-policy .column-margin}
![mountain car environment values and policy](pg-07-stochastic-simpler.png){group="slides"}

In mountain car, the parameterized value function is complex, but the parameterized policy is simple.
:::

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] Describe the objective for policy gradient algorithms [\#](#sec-l2g1)
-   [x] Describe the results of the policy gradient theorem [\#](#sec-l2g2)
-   [x] Understand the importance of the policy gradient theorem [\#](#sec-l2g3)
:::

## The Objective for Learning Policies (Video)

In this video Martha White dive deep into the objective for policy gradient algorithms. She then contrasts this with the gradient of the value fn objectives and lists some challanges for estimating the gradient of the average reward objective. We will learn that we can use the policy gradient theorem and to overcome these challenges and derive a gradient with an neat update rule .


## The objective for policy gradient algorithms {#sec-l2g1}

Formalizing the goal as an Objective

$$
G_t = \sum_{t=0}^{T}  R_{t} \quad \text{episodic}
$$

$$
G_t = \sum_{t=0}^{\infty} \gamma^t R_{t} \quad \text{continuing - discounted reward}
$$

$$
G_t = \sum_{t=0}^{\infty} R_{t} - r(\pi) \quad \text{episodic - average reward}
$$

The average reward Objective for a policy is as follows:
$$
r(\pi) = \sum_{t=0}^{T} \mu(s) \sum_{a} \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a) r \quad \text{avg. reward objective} 
$$ {#eq-average-reward-objective}

What does this mean?

- the last sum is the expected reward for a state-action pair. $\mathbb{E}[R_t \mid S_t = s , A_t=a]$
- the last two sums together are the expected reward for a state under weighted by the policy $\pi$. $\mathbb{E}_\pi[R_t \mid S_t = s]$
- full sum ads the time we spend in state $s$ under $\pi$ therefore the expected reward for a state under the policy $\pi$ and the environment dynamics $p$. $\mathbb{E}_\pi[R_t]$

to optimize the average reward, we need to estimate the gradient of [the avg. objective](#eq-average-reward-objective)


$$
\nabla_\theta r(\pi) = \nabla_\theta \sum_{t=0}^{T} \textcolor{red}{\underbrace{\mu(s)}_{\text{Depends on }\theta}} \sum_{a} \pi(a \mid s, \theta) \sum_{s',r} p(s',r \mid s,a) r \qquad
$$ {#eq-average-reward-objective-gradient}

1. Methods based on this are called policy gradient methods.
2. We are trying to maximize the average reward.

There are a few challenges with using the gradient in [#eq-average-reward-objective-gradient]:

According to the lesson $\mu(s)$ depends on $\theta$. Martha White point out that this state importance though parameterized only by s actually depends on the the policy $\pi$ which will evolve during its training based on the values of $\theta$. Which means out notation here is a bit misleading.  She then contrasts it with the value function gradient is being evaluated using a fixed policy.

$$
\begin{align*}
\nabla_w \bar{VE} &= \nabla_w \sum_{s}\textcolor{red}{\underbrace{\mu(s)}_{\text{Independent of }\mathbf{w}}}  [V_{\pi}(s)-\bar{v}(s,w)]^2 \newline
&=\sum_{s} \textcolor{red}{\mu(s)}  \nabla_w [V_{\pi}(s)-\bar{v}(s,w)]^2 
\end{align*}
$$


We can avg reward as an objective for policy optimization and the its for the stochastic gradient ascent. Next we will consider how the policy gradient theorem can help us estimate the gradient of the average reward objective despite these setbaks.

## The Policy Gradient Theorem (Video)

## The results of the policy gradient theorem {#sec-l2g2}

The product rule
$$
\nabla(f(x)g(x)) = \nabla f(x)g(x) + f(x)\nabla g(x)
$$ {#eq-product-rule}

therefore:

$$
\begin{align*}
\nabla_\theta r(\pi) &= \sum_{t=0}^{T} \nabla \mu(s) \sum_{a} \pi(a \mid s,\theta) \sum_{s',r} p(s',r \mid s,a) r +  \sum_{t=0}^{T} \mu(s) \nabla \sum_{a} \pi(a \mid s, \theta) \sum_{s',r} p(s',r \mid s,a) r \\
 &= \sum_{s\in \mathcal{S}}  \mu(s) \sum_{a} \pi(a \mid s,\theta)  q(s,a) 
\end{align*}
$$ {#eq-policy-gradient-theorem}

## The importance of the policy gradient theorem {#sec-l2g3}


# Lesson 3: Actor-Critic for Continuing Tasks 

::: callout-note
### Learning Objectives {.unnumbered}

-   [x] Derive a sample-based estimate for the gradient of the average reward objective [\#](#sec-l3g1)
-   [x] Describe the actor-critic algorithm for control with function approximation, for continuing tasks [\#](#sec-l3g2)

:::

# Lesson 4: Policy Parameterizations 

::: callout-note
### Learning Objectives {.unnumbered}

- [x] Derive the actor-critic update for a softmax policy with linear action preferences [\#](#sec-l4g1)
- [x] Implement this algorithm [\#](#sec-l4g2)
- [x] Design concrete function approximators for an average reward actor-critic algorithm [\#](#sec-l4g3)
- [x] Analyze the performance of an average reward agent [\#](#sec-l4g4)
- [x] Derive the actor-critic update for a gaussian policy [\#](#sec-l4g5)
- [x] Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions [\#](#sec-l4g6)

:::


::: callout-note

### Discussion prompt {.unnumbered}

> Are tasks really ever continuing? Everything eventually breaks or dies. 
  It’s clear that individual people do not learn from death, but we don’t live forever. 
  Why might the continuing problem formulation be a reasonable model for long-lived agents?  
:::


::: {#fig-chapter-13-policy-gradient }
![Chapter 13](ch13-policy-gradient.pdf){.col-page width=700px height=1000px}

Chapter 13 of [@sutton2018reinforcement] covering policy gradient methods.
:::