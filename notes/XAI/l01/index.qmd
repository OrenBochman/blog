---
date: 2023-03-05
title: Introduction to XAI
subtitle: XAI Course Notes
description: |
    In this introduction lecture on explainability in AI, we will delve into the key topics that surround this emerging field. 
    We will first provide an overview of the motivation for explainability, exploring how it helps us to achieve more transparent and trustworthy AI systems, particularly from a managerial perspective. 
    We will then define some of the key terminology in the field and differentiate between black box explanation and interpretable ML. 
    We will discuss the differences between global and local explanations, and include many examples from different fields and use cases throughout the lecture.
    Next, we will examine the "built-in" feature importance methods that are commonly used for regression and trees, and discuss the strengths and limitations of these methods.
    Overall, this lecture will provide a comprehensive introduction to explainability in AI, covering the key topics and terminology that are essential for understanding this field.
categories:
    - explainable AI
    - XAI
    - machine learning
    - ML
    - data science
    - contrafactuals
    - casual inference
    - CI
authors:
    - id: "Rada"
      name: "Rada Menczel"
      role: "Senior Manager, Data Science"
      affiliations:
        - id: "1"
          name: "Senior Director, Data Science Anaplan"
          department: Data Science
          url: https://www.linkedin.com/in/rada-menczel/
    - id: "Maya"
      name: "Maya Bercovitch"
      role: "Senior Director, Data Science"
      affiliations:
        - id: "2"
          name: "Senior Director, Data Science Anaplan"
          department: Data Science
          url: https://www.linkedin.com/in/maya-bercovitch/
image: XAI_Poster.jpg
nocite: | 
  @molnar2022
#format: 
#  revealjs:
#    toc: false
#    theme: dark
#    progress: true
#    hash: true
#    fragments: true
#    fragment-in-url: true
#    css: [commentry.css]
#    from: "markdown+emoji"
#lightbox: False
---

# Course

![course poster](XAI_Poster.jpg){.column-margin}

## Course Goals

-   The XAI course provides a comprehensive overview of explainable AI,
    -   covering both theory and practice, and
    -   exploring various use cases for explainability.
-   Participants will learn how
    -   to generate explanations,
    -   to evaluate explanations, and
    -   effectively communicate these to diverse stakeholders.

[overview link](https://learn.microsoft.com/en-us/events/learn-events/reactor-explainableaicourse/)

## Session Description

-   In this introduction lecture on explainability in AI, we will delve into the key topics that surround this emerging field.
-   Overall, this lecture will provide a comprehensive introduction to explainability in AI, covering the key topics and terminology that are essential for understanding this field.

## Session Description

-   Motivate explainability.
    -   Explore how it achieve greater transparency and trustworthiness in AI systems,
-   Provide the the key terminology
-   Discuss the differences between global and local explanations
-   Examine the "built-in" feature importance methods commonly used for regression and trees.

## Session Video

{{< video https://youtu.be/OUc4Z8HIUyk width="1227" height="690" >}}

## Speakers

![Speakers](sl_001.png){.column-margin}

# Introduction to XAI

## What is Explainability?

![Explainability Definition](sl_002.png){.column-margin}

## What do we mean by **Explainability**?

-   We define explainability by:

> "The ability of an AI system or algorithm to explain its decision making process in a way that humans can understand" [^1]

>   An explanation is the answer to a **why** question -- [@Miller17Explanation]

[^1]: Unfortunately, this is a circular definition.

## What do we mean by **Explainability**?

> The capacity of an model to back predictions with a human understandable interpretation of the impact of inputs on predictions.

-   What humans find understandable differs widely.
-   Learning in ML can differ greatly:
    -   Parametric models learn a handful of parameters,
    -   Non-parametric model may learn billions.
-   Explanations are subjective
    -   Artifacts of the model, not the data
    -   Reflect any inductive bias in the model [^2]

[^2]: Trees are highly sensitive to small changes in the data

# Agenda

## Talk Agenda

-   Motivation
-   What is XAI
-   Introduction to trees
-   XAI in the forest


# Motivation 

-   AI market size is rapidly expanding and projected to reach 1.6 Billion by 2030 [@PR2022AI]
-   More ML projects are reaching deployment

# Motivation 

![AI market size in 2022 from [@PR2022AI]](sl_006.png){.column-margin}

# Motivation 

[![Gartner on AI Project deployments](sl_007.png)](https://www.gartner.com/en/newsroom/press-releases/2019-07-15-gartner-survey-reveals-leading-organizations-expect-t#:~:text=Today%2C%20the%20average%20number%20of,or%20ML%20projects%20in%20place.)

# Motivation 

[![AI adoption in the enterprise 2021](sl_008.png)](https://www.oreilly.com/radar/ai-adoption-in-the-enterprise-2021/)

# How can XAI be useful?

## XAI to Avoid Biases in ML Models

![Avoiding ML bias in [@Dastin2018Amazon]](sl_010.png)

::: commentary
-   Source of the bias is that they trained on 10 years of worker's CVs. surprise their workforce had a bias and the model perpetuated it.
:::

## XAI to Avoid Biases in ML Models

-   XAI can **reveal bias** before models reach production.
-   Example:
    -   A US based client started doing business abroad.
    -   New non US prospects were misclassified.
    -   :exploding_head: XAI showed the **country** biased against non US prospects.
    -   $\implies$ dropped the country feature from the model.

## XAI to Avoid Biases in ML Models - Comments 1 {visibility="uncounted"}


::: commentary
-   Devils Advocate::smiling_imp:
    -   Q. Why add a features like country if all activity is in one country?
    -   Q. Why drop it? Won't country be an informative feature going forward?
    -   Q. Won't this be an issue for each new country added?
    -   $\implies$ Partial Pooling can learn to strike a balance :thinking:
:::

------------------------------------------------------------------------

## XAI to Avoid Biases in ML Models - Comments 2 {visibility="uncounted"}

::: commentary
![](Diogenes_looking_for_an_unbiased_estimator.jpg)

-   What is a unbiased estimator?
    -   estimators are unbiased w.r.t. some specific criteria.
    -   there is a bias variance trade-off.
    -   which is worse depends on the cost of type I errors vs type II errors

:::

## XAI to Avoid Biases in ML Models - Comments 3 {visibility="uncounted"}

![](eupati-types1-2-errors.png)

::: commentary

-   adding more criteria will reduce its performance on the main metric (i.e. variance).
-   people tend to like a biased estimator with small variance to unbiased one with high variance.
-   it looks like a class imbalance problem for which there are well know solutions like re-sampling and weighting.
-   the datasets in upstream models may be the issue
    -   how can we detect and correct in these models.
    -   ignoring for the moment the costs of sourcing better data what do we do when the bias comes from the real world (gender gap in payment).
-   and how can we avoid making the bias bigger?
:::

## XAI to Avoid Biases in ML Models

::: r-stack
![Chat GPT political bias](bias_1.png){width="55%"}

![Response](bias_2.png){.fragment .zoom-in}
:::

## XAI to Avoid Biases in ML Models

-   Predicting which prospective customers will convert
    -   current market is is in the US
    -   Model accuracy on test is high
    -   Predictions distribution over time is off?
-   *What to do next?*

## Feature selection

![XAI for feature selection.](xai-feature-selection.png)

-   One learns in linear regression 101, that the $\text{adjusted } R^2$ let's you gauge the performance of models built with different features. This means we already should have a principled approach to feature selection.
-   the most obvious method – stepwise regression is prone to overfitting if there are many features and the Bonferroni point [^3] which governs the admissibly of non-spurious features is $\approx \sqrt{2\log p}$ for the t-test (where p is the number if predictors). However this is will reject good features.
-   the [**Benjamini–Hochberg procedure**](https://en.wikipedia.org/wiki/False_discovery_rate#BH_procedure) procedure is less conservative and avoid the use of p-values which are amenable to p-hacking.
-   In black box model like a Deep Neural Networks the model learns its own features so again I don't see how XAI is going to be able to help out.
-   @GelmanHill2007Regression pointers out that adding features to a regression can lead to a regression formula that does not make sense. They suggest a procedure that lead to an interpretable model. However the culture in ML is rather different than in statistical learning.

-   If we work with a Causal DAG we may well de have even more to say on the
-   Q. So what more can XAI informs us as to features selection?

[^3]: The Bonferroni point, or *adjusted p value* is the point at which you need to adjust the p-value threshold due to multiple comparisons when performing feature selection . In simpler terms, it's about accounting for the increased chance of falsely identifying significant features when you test many features simultaneously


## XAI to Investigating Bugs 1

|                                   |                                     |
|------------------------------------|------------------------------------|
| ![Investigating Bugs](bugs1.png)                                           | ![Investigating Bugs](bugs2.png){ width="450"} |
| ![Investigating Bugs](bugs3.png){width="450" style="vertical-align: top;"} | ![Investigating Bugs](bugs4.png){ width="450"  style="vertical-align: top;"} |

## XAI to Investigating Bugs 2

|                                  |                                                         |
|------------------------------------|------------------------------------|
| ![Investigating Bugs](bugs5.png) | ![Investigating Bugs](bugs6.png){.fragment width="450"} |

## XAI to support business decisions

-   External data consumption to improve prediction
-   Explainability to create a personalized well suited sales pitch

## Who Needs Explanations ?

![Who needs explanations](sl_020.png){.column-margin}

## Explaining the Data vs. Explaining the Model

::: columns
::: {.column width="50%"}
### Feature Description

-   Characteristics of the input data
-   E.g.:
    -   Feature correlation
    -   Anomalies & Extreme values
    -   Feature values distribution
:::

::: {.column width="50%"}
### Feature Contribution

-   Feature's impact on predictions
-   Not aligned with feat. correlation to target variable
-   E.g.:
    -   Feature importance in trees
    -   SHAP values
:::
:::

![slide](sl_023.png){.column-margin}

## Properties of Explanations

::: columns
::: {.column width="50%"}
### White Box

-   An interpretable model.
-   Humans can understand how the model makes predictions.
-   Examples:
    -   linear and logistic regression
    -   decision tree
:::

::: {.column width="50%"}
### Black Box

-   Do not reveal their internal mechanisms
-   Cannot be understood by looking at their parameters
-   Examples:
    -   Deep Neural Nets
    -   XGBoost, Random Forest
:::
:::

# Properties of Explanation Methods

-   Predictive mode interpretation level
-   Explanation creation time
-   Model Agnostic vs. model specific
-   Global and Local explanations
-   Explanation structure
-   Explanation reproducibilty

## Performance & Interpretability Trade-off

![spectrum of interpretability](sl_026.png){.column-margin}


## Performance & Interpretability Trade-off

![The trade-off between predcitive power and interpretability](sl_027.png){.column-margin}

## Intrinsic & Extrinsic Methods

::: columns
::: {.column width="50%"}
### Intrinsic

-   ML model that are considered interpretable due to their **simple structure**.
-   Explanation methods that rely on looking into ML models, like its parameters
-   No additional complexity or resources requires
:::

::: {.column width="50%"}
### Extrinsic

-   Applying methods that **analyze the model after training**

-   Post hoc methods can also be applied to intrinsically interperetable models

-   **Additional complexity** - XAI algorithms and computation resources requried
:::
:::

## Post Hoc XAI using Surrogate Models

![Post Hoc methods create and use a surrogate model to explain predictions](sl_030.png)

## Model Specific & Model Agnostic Methods

::: columns
::: {.column width="50%"}
### Model Specific

-   Limited to specific model type.
-   Examples:
    -   Regression weights in a linear model
    -   GINI importance score in a decision tree
:::

::: {.column width="50%"}
### Model Agnostic

-   XAI tools for any ML Model
-   Pos hoc methods that
-   Map input output pairs
-   Examples:
    -   SHAP
    -   LIME
:::
:::

## Local and Global Methods

![Contasting **Global** with **Local** views of the data](sl_034.png){.column-margin}

## Explain the Predictions of a Segment

![A segment of the data](sl_035.png){.column-margin}

## Explanation Structure

![Explanation Structure - Numerical, Graphs and Text](sl_042.png){.column-margin}

## Graphs Representation for SHAP

::: r-stack

![SHAP global feat importance](SHAP_global_feat_importance.webp)
![SHAP bee-swarm plot shows the global importance of each feature and the distribution of effect sizes](SHAP_bee_swarm.webp)
![SHAP dependence plot](SHAP_dependence_plot.webp)


:::

![SHAP Visulizations](sl_041.png){.column-margin}

------------------------------------------------------------------------

## Explanation Repoducibility

-   Most post hoc techniques use random samples of the data and premutation vlues

-   This results in inconsistant results - for the same model we can get different explanations.

-   As data scientists we should be aware of this and consider consistanc if applicable/required.

![slide](sl_044.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_045.png){.column-margin}

# Part 1 Summary

-   The demand for XAI is high

-   XAI can be achieved in many ways

-   Think about the set of considerations discussed before choosing a method[^4]

-   Choose wisely

[^4]: The last lecture provides some insights and charts to assist this step!

------------------------------------------------------------------------

# Disicion Trees

## Why Decision Trees?

-   Easy to explain.

-   Clear structure - order and hierarchy.

-   Simple interpretability.

-   Can be converted into rules.

-   Often used as a [surrogate model](https://en.wikipedia.org/wiki/Surrogate_mode)

------------------------------------------------------------------------

## How do we build Decision Trees?

> Entropy - the measurement of the impurity or randomness in the data points

## Information Theory: Entropy

![slide](sl_050.png){.column-margin}

------------------------------------------------------------------------

![Entropy](sl_051.png){.column-margin}

------------------------------------------------------------------------

![Entropy](sl_052.png){.column-margin}

## Information Theory: Conditional Entropy

![Conditional Entropy](sl_053.png){.column-margin}

## Information Theory: Mutual Information

------------------------------------------------------------------------

![Mutual Information](sl_054.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_055.png){.column-margin}

------------------------------------------------------------------------

## Decision Tree

![slide](sl_056.png){.column-margin}

------------------------------------------------------------------------

## Example

![slide](sl_057.png){.column-margin}

------------------------------------------------------------------------

## Decision Tree - Entropy Calculation

![slide](sl_058.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_059.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_060.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_061.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_062.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_063.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_064.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_065.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_066.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_067.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_068.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_069.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_070.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_071.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_072.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_073.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_074.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_075.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_076.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_077.png){.column-margin}

$$
\begin{aligned}
Entropy(Play)&= -p_{No}\log(P_{No})-p_{Yes}\log(P_{Yes})\\
&= -\frac{5}{14}\log_2{\frac{5}{14}}-\frac{9}{14}\log_2{\frac{9}{14}}\\
&=0.94
\end{aligned} \qquad
$$ {#eq-entropy-calculation}

------------------------------------------------------------------------

## Information Theory: Discretization

![slide](sl_078.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_079.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_080.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_081.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_082.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_083.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_084.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_085.png){.column-margin}

------------------------------------------------------------------------

## Information Theory: Gini Index

![GINI Index](sl_086.png){.column-margin}

------------------------------------------------------------------------

## Entropy and Gini

![slide](sl_087.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_088.png){.column-margin}

------------------------------------------------------------------------

## Decision Tree - Iris Dataset

![slide](sl_089.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_090.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_091.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_092.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_093.png){.column-margin}

## Decision Tree - Titanic Dataset

![slide](sl_094.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_095.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_096.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_097.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_098.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_099.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_100.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_101.png){.column-margin}

------------------------------------------------------------------------

## Feature Importance - Mean Decrease in Impurity (MDI)

First introduced in [@breiman2001]

![Mean Decrease in Impurity Feature Importance](sl_102.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_103.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_104.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_105.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_106.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_107.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_108.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_109.png){.column-margin}

------------------------------------------------------------------------

![MDI Feature Importance Calculation](sl_110.png){.column-margin}

------------------------------------------------------------------------

\![[MDI Feature Importance Results](sl_112.png){.column-margin}

------------------------------------------------------------------------

## Feature Importance - Permutation Feature Importance

This is defined by [sk-learn](https://scikit-learn.org/stable/modules/permutation_importance.html) as follows:

-   Inputs: fitted predictive model $m$, tabular dataset (training or validation) $D$.
-   Compute the reference score $s$ of the model $m$ on data $D$ (for instance the accuracy for a classifier or the $R^2$ for a regressor).
-   For each feature j (column of D):
    -   For each repetition $k$ in $1,\ldots,K$:
        -   Randomly shuffle column $j$ of dataset $D$ to generate a corrupted version of the data named $\bar D_{k,j}$.
        -   Compute the score $s_{k,j$ of model $m$ on corrupted data $\bar D_{k,j}$.
    -   Compute importance$i_j$ for feature $f+j$ defined as:

$$
i_j=s-\frac{1}{K}\sum_{k=1}^Ks_{k_j} \qquad
$$ {#eq-permutation_feature_importance}

------------------------------------------------------------------------

![slide](sl_115.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_116.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_117.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_118.png){.column-margin}

------------------------------------------------------------------------

## Random Forest

introduced in [@ho1995random] and extended in [@breiman2001a].

-   Ensemble of decision trees.

    -   **N** – number of training samples

    -   **M** – number of features

    -   **n_estimators** – The number of trees in the forest

    -   Create **n_estimators decision trees using**

        -   N samples with replacement

        -   $m<M$ features for each step typically $m-\sqrt{M}$

------------------------------------------------------------------------

![wisdom of crowds](sl_121.png){.column-margin}

## Decision Tree - Iris Dataset

![slide](sl_122.png){.column-margin}

## MDI Feat Importance Iris Dataset

------------------------------------------------------------------------

![slide](sl_123.png){.column-margin}

## How to calculate Feature Importance in Random Forest?

![MDI Feature importance for Random Forest](sl_125.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_126.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_127.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_128.png){.column-margin}

------------------------------------------------------------------------

![slide](sl_129.png){.column-margin}

## Feature Importance Methods

![slide](sl_130.png){.column-margin}

## Feature Importance Score

![slide](sl_131.png){.column-margin}

------------------------------------------------------------------------

# Summary

-   Motivation
-   Explain XAI
-   Introduction to decision Trees
-   XAI in the Forest

# Thank You

![contact](sl_133.png)

------------------------------------------------------------------------

![credits](sl_134.png){.column-margin}

### References

-   https://www.youtube.com/watch?v=6qisPX7o-bg

::: {#refs}
:::
