---
date: 2023-03-23
title: 4 Counterfactual Explanations - Explaining and Debugging
subtitle: XAI Course Notes
description: |
  How to explain a machine learning model such that the explanation is truthful to the model and yet interpretable to people? 
  This question is key to ML explanations research because explanation techniques face an inherent tradeoff between fidelity and interpretability --- a high-fidelity explanation for an ML model tends to be complex and hard to interpret, while an interpretable explanation is often inconsistent with the ML model. 
  In this talk, I will present counterfactual (CF) explanations that bridge this tradeoff. 
  Rather than approximate an ML model or rank features by their predictive importance, a CF explanation “interrogates” a model to find required changes that would flip the model’s decision and presents those examples to a user.
  Such examples offer a true reflection of how the model would change its prediction, thus helping decision-subject decide what they should do next to obtain a desired outcome and helping model designers debug their model. 
  Using benchmark datasets on loan approval, I will compare counterfactual explanations to popular alternatives like LIME and SHAP. 
  I will also present a case study on generating CF examples for image classifiers that can be used for evaluating fairness and even improving the generalizability of a model.
categories:
    - explainable AI
    - XAI
    - machine learning
    - ML
    - data science
    - contrafactuals
    - casual inference
    - CI
image: XAI_Poster.jpg
---

## Session Video

{{< video https://youtu.be/P-neqnCjnZI?t=5 >}}

## Course Leaders:

-   Bitya Neuhof   - DataNights
-   Yasmin Bokobza - Microsoft

## Speaker:

-   Amit Sharma - Microsoft

Amit Sharma is a Principal Researcher at Microsoft Research India. His work bridges causal inference techniques with machine learning, to make ML models generalize better, be explainable and avoid hidden biases. To this end, Amit has co-led the development of the open-source DoWhy library for causal inference and [DiCE](https://github.com/interpretml/DiCE){alt="Generate Diverse Counterfactual Explanations for any machine learning model."} library for counterfactual explanations. The broader theme in his work is how machine learning can be used for better decision-making, especially in sensitive domains. In this direction, Amit collaborates with NIMHANS on mental health technology, including a recent app, MindNotes, that encourages people to break the stigma and reach out to professionals.

His work has received many awards including:

-    a Best Paper Award at ACM CHI 2021 conference,
-   Best Paper Honorable Mention at ACM CSCW 2016 conference,
-   the 2012 Yahoo! Key Scientific Challenges Award and
-   the 2009 Honda Young Engineer and Scientist Award.

Amit received his:

-    Ph.D. in computer science from Cornell University and
-    B.Tech. in Computer Science and Engineering from the Indian Institute of Technology (IIT) Kharagpur.
-   [Profile](https://www.microsoft.com/en-us/research/people/amshar/)

# What is this session about?

How to explain a machine learning model such that the explanation is truthful to the model and yet interpretable to people? This question is key to ML explanations research because explanation techniques face an inherent tradeoff between fidelity and interpretability: a high-fidelity explanation for an ML model tends to be complex and hard to interpret, while an interpretable explanation is often inconsistent with the ML model.

In this talk, I will present counterfactual (CF) explanations that bridge this tradeoff. Rather than approximate an ML model or rank features by their predictive importance, a CF explanation "interrogates" a model to find required changes that would flip the model's decision and presents those examples to a user. Such examples offer a true reflection of how the model would change its prediction, thus helping decision-subject decide what they should do next to obtain a desired outcome and helping model designers debug their model. Using benchmark datasets on loan approval, I will compare counterfactual explanations to popular alternatives like LIME and SHAP. I will also present a case study on generating CF examples for image classifiers that can be used for evaluating the fairness of models as well as improving the generalizability of a model.

Amit pointed out that he is primarily interested in CI and that he later got interested in XAI at the cusp of CI and XAI. Also out that his initially his XAI work focused on deterministic, differential models. Later when people asked about using them with old school models like sk-learn random forest they went back to the drawing board and discovered that by sampling counterfactual locally they got the got even better results.

He pointed out that LIME and SHAP may give us feature importance but that as such their explanation are not actionable in the sense that they do not spell out to the user/data-scientist/developer what changes/interventions/effects would be needed to cross the decision boundary to arrive at the desired.

![contractual explanation properties](Screenshot%20from%202023-03-23%2018-23-50.png)

-   actionability - all things being equal, CFX should be actionable
-   diversity - we want to understand different casual choices
-   proximity - the CFX should be similar to the "query" in the sense of a local explanation.
-   user constraints - it should suggest actions that can be performed by the user.
    -   the user cannot easily
    -   become younger
    -   change sex,
    -   get a degree
-   sparsity
    -   a good CFX should only require change a minimal set of features. i.e. a few small steps in two or three dimensions to cross the decision boundary.
-   casual constraints

references are to:

-   [@wachter2018counterfactual]
-   [@russell2019efficient]

![contractual loss function](Screenshot%20from%202023-03-23%2018-24-29.png)

![slide](Screenshot%20from%202023-03-23%2018-28-49.png)

![slide](Screenshot%20from%202023-03-23%2018-29-00.png)

![slide](Screenshot%20from%202023-03-23%2018-34-00.png)

![slide](Screenshot%20from%202023-03-23%2018-34-18.png)

![slide](Screenshot%20from%202023-03-23%2018-34-44.png)

![slide](Screenshot%20from%202023-03-23%2018-37-09.png)

![slide](Screenshot%20from%202023-03-23%2018-41-51.png)

![slide](Screenshot%20from%202023-03-23%2018-42-40.png)

![slide](Screenshot%20from%202023-03-23%2018-49-07.png)

![slide](Screenshot%20from%202023-03-23%2018-55-02.png)

![slide](Screenshot%20from%202023-03-23%2019-00-26.png)

# Resources:

- [@mothilal2020dice]
- [@mothilal2020explaining]
- [DoWhy](https://github.com/py-why/dowhy) is a Python library for causal inference that supports explicit modeling and testing of causal assumptions. DoWhy is based on a unified language for causal inference, combining causal graphical models and potential outcomes frameworks. [Microsoft Research Blog]() | Video Tutorial | [@sharma2020dowhy] | [@dowhy_gcm] | [Slides](https://www2.slideshare.net/AmitSharma315/dowhy-an-endtoend-library-for-causal-inference)

Action Items:

1.  Once again I want to put some JSON-LD data as a Knowledge Graph into this article but I don't have the tools to do it with.
    1.  collect the people's info using a headless CMS like [sanity](sanity.io) or [blazegraph](https://blazegraph.com/)
    2.  store the data on the papers using bibtex
    3.  use the YAML metadata with categories
    4.  some ontology for concepts and conferences
    5.  write a sequence of queries
    6.  visualize and interact with the output of the queries
2.  Try out DiCE [notbook](https://github.com/interpretml/DiCE/blob/main/docs/source/notebooks/DiCE_getting_started.ipynb)
3.  Try out DoWhy [notebook](https://github.com/py-why/dowhy/blob/main/docs/source/example_notebooks/tutorial-causalinference-machinelearning-using-dowhy-econml.ipynb)
4.  Review the papers
5.  Consider:
    1.  how can we use MCMC + XCF to generate useful examples for debugging our model.