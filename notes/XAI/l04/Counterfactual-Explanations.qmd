---
date: 2023-03-23
title: 4 Counterfactual Explanations - Explaining and Debugging
subtitle: XAI Course Notes
description: |
  How to explain a machine learning model such that the explanation is truthful to the model and yet interpretable to people? 
  This question is key to ML explanations research because explanation techniques face an inherent tradeoff between fidelity and interpretability --- a high-fidelity explanation for an ML model tends to be complex and hard to interpret, while an interpretable explanation is often inconsistent with the ML model. 
  In this talk, I will present counterfactual (CF) explanations that bridge this tradeoff. 
  Rather than approximate an ML model or rank features by their predictive importance, a CF explanation ‚Äúinterrogates‚Äù a model to find required changes that would flip the model‚Äôs decision and presents those examples to a user.
  Such examples offer a true reflection of how the model would change its prediction, thus helping decision-subject decide what they should do next to obtain a desired outcome and helping model designers debug their model. 
  Using benchmark datasets on loan approval, I will compare counterfactual explanations to popular alternatives like LIME and SHAP. 
  I will also present a case study on generating CF examples for image classifiers that can be used for evaluating fairness and even improving the generalizability of a model.
categories:
    - explainable AI
    - XAI
    - machine learning
    - ML
    - data science
    - contrafactuals
    - casual inference
    - CI
image: XAI_Poster.jpg
---

## Session Video

{{< video https://youtu.be/P-neqnCjnZI?t=5 >}}

## Course Leaders:

-   Bitya Neuhof - DataNights
-   Yasmin Bokobza - Microsoft

## Speaker:

-   Amit Sharma - Microsoft

Sharma is a Principal Researcher at Microsoft Research India. His work bridges CI (causal inference) techniques with machine learning, to make ML models generalize better, be explainable and avoid hidden biases. To this end, Sharma has co-led the development of the open-source `DoWhy` library for causal inference and [DiCE](https://github.com/interpretml/DiCE){alt="Generate Diverse Counterfactual Explanations for any machine learning model."} library for counterfactual explanations. The broader theme in his work is how ML can be used for better decision-making, especially in sensitive domains. In this direction, Sharma collaborates with NIMHANS on mental health technology, including a recent app, MindNotes, that encourages people to break the stigma and reach out to professionals.

His work has received many awards including:

-   a Best Paper Award at ACM CHI 2021 conference,
-   Best Paper Honorable Mention at ACM CSCW 2016 conference,
-   the 2012 Yahoo! Key Scientific Challenges Award and
-   the 2009 Honda Young Engineer and Scientist Award.

Amit received his:

-   Ph.D. in computer science from Cornell University and
-   B.Tech. in Computer Science and Engineering from the Indian Institute of Technology (IIT) Kharagpur.
-   [Profile](https://www.microsoft.com/en-us/research/people/amshar/)

# What is this session about?

How to explain a machine learning model such that the explanation is truthful to the model and yet interpretable to people? This question is key to ML explanations research because explanation techniques face an inherent trade-off between fidelity and interpretability: a high-fidelity explanation for an ML model tends to be complex and hard to interpret, while an interpretable explanation is often inconsistent with the ML model.

In this talk, the speaker presented counterfactual explanations (CFX) that bridge this trade-off. Rather than approximate an ML model or rank features by their predictive importance, a CF explanation "interrogates" a model to find required changes that would flip the model's decision and presents those examples to a user. Such examples offer a true reflection of how the model would change its prediction, thus helping decision-subject decide what they should do next to obtain a desired outcome and helping model designers debug their model. Using benchmark datasets on loan approval, I will compare counterfactual explanations to popular alternatives like LIME and SHAP. I will also present a case study on generating CF examples for image classifiers that can be used for evaluating the fairness of models as well as improving the generalizability of a model.

The speaker pointed out that he is primarily interested lay in CI and that when he later got interested in XAI his focused was on the cusp of CI and XAI.

Sharma shared that initially his work on XAI focused on deterministic, differential models. Only later when people asked about using them with traditional ML models like sk-learn and random forest that he went back to the drawing board and discovered how sampling counterfactual locally it is possible to got even better results.

Sharma also pointed out a shortcomings of algorithms like LIME and SHAP. While these present feature importance, their explanation are not actionable. This is in the sense that they fail to spell out to decision maker which interventions would allow them to cross decision boundaries, with least resistance, into their zone of desired outcomes.

## Outline

![outline](talk-outline.png){.column-margin}

## Background

![Assessing human decision-making](asscessing-human-decision-making.png){.column-margin}

A great starting point for ML tasks if often best motivated by considering pros and cons of human capabilities in these tasks. Sharma points out that in [@weichselbaumer2019] researchers used counterfactual thinking to study if employers discriminate against women wearing a head scarf. The idea was to they sent resumes sent to German Companies and modified names and images of applicants. German companies usually require images in the C.V. The study found there was discrimination.

## Counterfactual Definition

![What is a counterfactual](what-is-a-counterfactuals.png){.column-margin}

Sharma presents the definition for a counterfactual provided by Judea Pearl

> Given a system output $y$, a counterfactual $y_{X_i=x'}$ is the output of a system had some input $X_i$ changed but everything else unaffected by $X_i$ remained the same. ‚Äî [@pearl2009].

Under the [holistic](https://en.wikipedia.org/wiki/Holism) paradigm introduced in @smuts1926holism complex real world systems are inherently interconnected with the implication that that a change to just one thing will end up changing everything. ML Models of reality are reductionist, make simplifying assumptions Linear model and many traditional ML model will allow us to test a CF type intervention.

And this can be be very useful.

# The many uses of model CF Models

Estimating $f(X_i=x')-f(x)$ can provide:

1.  **Individual Effect** of Feature a feature $X_i$

$$X_i = E[Y_{X_i=x'}\mid X=x,Y=y]-E[Y \mid X=x] \qquad$$ {#eq-individual-effect}

2.  Explanation of how important is feature $X_i$

3.  Bias in model M if $X_i$ is a sensitive feature[^1]

4.  More generally, CF provide a natural way to debug ML models via [fuzz testing](https://en.wikipedia.org/wiki/Fuzzing){.column-margin}

[^1]: what is a sensitive feature?

## Why do we need CF Explanations?

![Feature Importance](explaining-ml-predictions.png){.column-margin}

## Feature importance is not enough?

![A problem with SHAP & LIME](the-problem.png){.column-margin}

Suppose an ML model recommends that an individual should be denied a loan

-   üßë *Loan Officer* : would like to understand why this individual was denied?

-   üë≥ *Individual*: would also like to know what she could do get the loan approved?

Sharma points out two shortcomings of traditional XAI methods

-   Feature importance is inadequate to fully inform the stakeholders if it does not suggest a useful action.
-   Feature importance can have **low fidelity** üò°
    -   The top feature may mandate unrealistic changes.

        e.g. "*increase your income by 5x*" üôÄ

    -   While the third Credit years and may not be on the path of least resistance to getting the loan.

        e.g. "*just wait three more years and you will be approved.*" üëç

## Desiderata for counterfactuals

![Desiderata of contractual explanation](CFX-Desiderata.png){.column-margin}

1.  **Actionalbility -** *Ceteris paribus* a CFX should be actionable for the decision subject.
2.  **Diversity** - we want to understand different casual choices
3.  **Proximity** - the CFX should be similar to the "query" in the sense of a local explanation.
4.  **User constraints** - it should only suggest actions that can be performed by the user. A loan applicant cannot easily, become younger, change sex or get a degree.[^2]
5.  **Sparsity -** a CFX should only require change a minimal set of features. i.e. a few small steps in two or three dimensions to cross the decision boundary.
6.  **Casual constraints**

[^2]: Such factors can create a bias leading to discrimination.

-   Going further it is suggested that we should view CFX as aggregating *feasibility* with *diversity* components

Before introducing his ideas Sharma references two prior works.

-   In the lengthy [@wachter2018counterfactual], the authors suggest that to comply with GDPR regulations CFX should take the form:

    > Score $p$ was returned because variables $V$ had values $(v_1,v_2,...)$ associated with them. **If** $V$ instead had values $(v_1',v_2',...)$, and all other variables had remained constant, **then** score $p'$ would have been returned.

    And and an approach to come up with suitable CFXs. Sharma references a formula:

    $$ 
    C= \arg \min_c loss_y(f(c),y)+|x-c| \qquad
    $$ {#eq-wachter-constratint}

    üè¥ But this formula is not in the üìÉ paper ‚Äî perhaps it is a simplification of the idea.\
    ü§î I believe it suggests their recipe to generate desirable CFX by picking a change $c$ in feature $x$ with a minimal impact on y as measured by some loss function on outcome $y$.

-   This approach is summarized in [\@molnar2022¬ß9.3.1.1](https://christophm.github.io/interpretable-ml-book/counterfactual.html#method-by-wachter-et-al.)

-   In [@russell2019efficient] the author introduced a CFX algorithm based on mixed integer programming that supports diversity. However it is limited to linear ML models.

## General Optimization framework

![CF loss function](CFX-loss-function.png){.column-margin}

This is the simple framework used in DICE to generate diverse counterfactual explanations.

-   what is the easiest way to get CFX ?

-   if the model is differentialable and

-   if we have deep model we know gradient descent.

What have we have here ?

-   we start with a mean of a Wachter type constraint
    -   this is being minimized.
-   we add a **proximity constraint** weighted by hyper parameter $\lambda_1$
    -   this is being minimized.
-   we add a **diversity constraint** weighted by hyper parameter $\lambda_2$
    -   this is being maximized.
    -   based on K some kind of metric for the distance for CFX distances.

Sharma considers this approach dated in lieu of more recent publications.

He references to other methods.

I think though he is talking about **MOC** which is based on multi-objective optimization problem, introduced in [@Dandl_2020] which the authors compare to DiCE [@mothilal2020dice] **Recourse** from [@ustun2019] and **Tweaking** from [@Tolomei2017]

## Diverse CFX

![diverse CFX](diverse-CFX.png){.column-margin} these can be used to inspect the black box model and understand what is going in there

## Generating debugging edge-cases

![CFX as a way to generate debugging edge cases](CFX-for-generating-debugging-edge-cases.png){.column-margin}

## Quantitative Evaluation for CFX

![Quantitative evaluation for CFX](quantitative-evaluation-for-CFX.png){.column-margin}

This is how we translate the desiderata into a formal model using metrics.

![results comparing CF based methods](results-comparing-CF-based-methods.png){.column-margin}

## How does DiCE compare with LIME and SHAP

![Comparing DiCE with LIME and CHAP](Comparing-DiCE-with-LIME-and-CHAP.png){.column-margin}

The results section from the DiCE paper!

## Practical considerations

![Practical Considerations](practical-considerations.png){.column-margin}

## Returning to the optimization problem

![Returning to the optimization problem](returning-to-the-optimization-problem.png){.column-margin}

## How to Generate a CF for a ML model

![How to generate a counterfactual for a ML model](how-to-genenrate-CFX-for-a-ML-model.png){.column-margin}

## Conclusion

![Methods](CFX-methods.png){.column-margin}

+-------------+-------------+-------------------------------+----------------------------------------------------+-------------+
| Data        | Name        | Citation                      | Python                                             | R           |
+=============+=============+===============================+====================================================+=============+
| Tabular     | DoWhy       | [@dowhy]\                     | [pywhy](https://www.pywhy.org/)                    |             |
|             |             | [@dowhy_gcm]                  |                                                    |             |
+-------------+-------------+-------------------------------+----------------------------------------------------+-------------+
| Tabular     | DiCE        | [@mothilal2020dice]           | [github](https://github.com/interpretml/DiCE)      |             |
+-------------+-------------+-------------------------------+----------------------------------------------------+-------------+
| Tabular     | MOC         | [@Dandl_2020]                 | [github](https://github.com/dandls/moc)            |             |
+-------------+-------------+-------------------------------+----------------------------------------------------+-------------+
| Tabular     | Recourse    | [@ustun2019]                  |                                                    |             |
+-------------+-------------+-------------------------------+----------------------------------------------------+-------------+
| Tabular     | Tweaking    | [@Tolomei2017]                |                                                    |             |
+-------------+-------------+-------------------------------+----------------------------------------------------+-------------+
| Text        | Checklist   | [@ribeiro-etal-2020-beyond]   | [checklist](https://github.com/marcotcr/checklist) |             |
+-------------+-------------+-------------------------------+----------------------------------------------------+-------------+
| Text        | Litmus      |                               | [litmus](https://github.com/microsoft/Litmus)      |             |
+-------------+-------------+-------------------------------+----------------------------------------------------+-------------+
| Image       | CF-CLIP     | [@yu2022cfx]                  | [CF-CLIP](https://github.com/yingchen001/CF-CLIP)  |             |
+-------------+-------------+-------------------------------+----------------------------------------------------+-------------+

## Conclusion

![Conclusion](conclusion.png){.column-margin}

# Resources:

-   [DoWhy](https://github.com/py-why/dowhy) is a Python library for causal inference that supports explicit modeling and testing of causal assumptions. DoWhy is based on a unified language for causal inference, combining causal graphical models and potential outcomes frameworks. [Microsoft Research Blog]() \| Video Tutorial \| [@sharma2020dowhy] \| [@dowhy_gcm] \| [Slides](https://www2.slideshare.net/AmitSharma315/dowhy-an-endtoend-library-for-causal-inference)

Action Items:

1.  Once again I want to put some JSON-LD data as a Knowledge Graph into this article but I don't have the tools to do it with.
    1.  collect the people's info using a headless CMS like [sanity](sanity.io) or [blazegraph](https://blazegraph.com/)
    2.  store the data on the papers using bibtex
    3.  use the YAML metadata with categories
    4.  some ontology for concepts and conferences
    5.  write a sequence of queries
    6.  visualize and interact with the output of the queries
2.  Try out DiCE [notbook](https://github.com/interpretml/DiCE/blob/main/docs/source/notebooks/DiCE_getting_started.ipynb)
3.  Try out DoWhy [notebook](https://github.com/py-why/dowhy/blob/main/docs/source/example_notebooks/tutorial-causalinference-machinelearning-using-dowhy-econml.ipynb)
4.  Review the papers
5.  Consider:
    1.  how can we use MCMC + XCF to generate useful examples for debugging our model.
