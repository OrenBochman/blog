---
date: 2023-03-28
title: Lecture 5 --- Explainable AI in practice
subtitle: XAI Course Notes
description: |
  How to properly incorporate explanations in machine learning projects and what aspects should you keep in mind?
  Over the past few years the need to explain the output of machine learning models has received growing attention. 
  Explanations not only reveal the reasons behind models predictions and increase users' trust in the model, but they can be used for different purposes.
  To fully utilize explanations and incorporate them into machine learning projects the following aspects of explanations should taken into consideration --- explanation goals, the explanation method, and explanations’ quality. 
  In this talk, we will discuss how to select the appropriate explanation method based on the intended purpose of the explanation. 
  Then, we will present two approaches for evaluating explanations, including practical examples of evaluation metrics, while highlighting the importance of assessing explanation quality.
  Next, we will examine the various purposes explanation can serve, along with the stage of the machine learning pipeline the explanation should be incorporated in. 
  Finally we will present a real use case of script classification as malware-related in Microsoft and how we can benefit from high-dimensional explanations in this context.
categories:
    - explainable AI
    - XAI
    - machine learning
    - ML
    - data science
    - contrafactuals
    - casual inference
    - CI
image: XAI_Poster.jpg
---

The XAI course provides a comprehensive overview of explainable AI, covering both theory and practice, and exploring various use cases for explainability. 

Participants will learn not only how to generate explanations, but also how to evaluate and effectively communicate these explanations to diverse stakeholders.

[overview link](https://learn.microsoft.com/en-us/events/learn-events/reactor-explainableaicourse/)

## Series Poster

![series poster](XAI_Poster.jpg){.column-margin}

## Session Descriprion

  How to properly incorporate explanations in machine learning projects and what aspects should you keep in mind?
  Over the past few years the need to explain the output of machine learning models has received growing attention. 
  Explanations not only reveal the reasons behind models predictions and increase users' trust in the model, but they can be used for different purposes.
  To fully utilize explanations and incorporate them into machine learning projects the following aspects of explanations should taken into consideration --- explanation goals, the explanation method, and explanations’ quality. 
  In this talk, we will discuss how to select the appropriate explanation method based on the intended purpose of the explanation. 
  Then, we will present two approaches for evaluating explanations, including practical examples of evaluation metrics, while highlighting the importance of assessing explanation quality.
  Next, we will examine the various purposes explanation can serve, along with the stage of the machine learning pipeline the explanation should be incorporated in. 
  Finally we will present a real use case of script classification as malware-related in Microsoft and how we can benefit from high-dimensional explanations in this context.

## Session Video

{{< video https://youtu.be/pHE9TePw9qs class="column-margin" >}}
