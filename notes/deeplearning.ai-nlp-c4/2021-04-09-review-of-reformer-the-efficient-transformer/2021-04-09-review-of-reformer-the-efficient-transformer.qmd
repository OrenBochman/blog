---
layout: post
title: "review of Reformer: The Efficient Transformer"
description: review of the 2020 paper "Reformer The Efficient Transformer" on improving the
  transformer architecture for the deeplearning.ai NLP specialization.
date: 2021-04-10T10:50:09.040Z
categories: nlp coursera notes paper
tags:
  - attention
  - deep learning
  - literature review
  - paper
img: literature-review-open-book.jpg
lastmod: 2022-05-02T08:54:22.953Z
draft: true
---
<style>
hr { 	clear:both;  }
img[src*='#sl'] { 
  float: right; 
  width:35%; 
  margin:10px 10px 10px 0px; 
  border: 2px solid gold;
  display: block;
}
img[src*='#hi'] { 
  width:85%; 
  display: block;
  margin: 10px auto 10px auto;
  border: 2px solid gold;
}
img[src*='#logo'] {
      width: 20%;
      float: right
}
.logo {
      width: 20%;
      float: right
}
</style>
:::{.logo}
![deeplearning.ai](logo_deeplearning_ai.png)
:::

# Reformer: The Efficient Transformer
Reformer presents some innovations which allow a more more efficent transformer.
Location sensativ hashing allows attending back to distances of 1,000,000 positions back in the sequence.

# Inroduction
![page 1](page0.png)
<hr>
Hashing attention
Locality sensative hashing
LSH attention
![page 2](page2.png)
<hr>
![page 3](page3.png)
<hr>
![page 4](page4.png)
<hr>
![page 5](page5.png)
<hr>
![page 6](page6.png)
<hr>
![page 7](page7.png)
<hr>
![page 8](page8.png)
<hr>
![page 9](page9.png)
<hr>
![page 10](page12.png)
<hr>
![page 11](page11.png)
