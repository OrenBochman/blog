---
date: 2017-07-02
last-modified: 2023-01-19
title: Deep Neural Networks - Notes for lecture 1a 
subtitle: For the course by Geoffrey Hinton on Coursera
description: Why do we need machine learning?
categories: [deep learning, neural networks, notes, coursera] 
title-block-banner: banner_deep.jpg
---

{{< pdf lec1.pdf class="column-margin" >}}

These is the first installment of notes to the course "Deep Neural Networks" by [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) I took on Coursera

This was one of the first course online on the subject.

Hinton was one of the leading researchers on deep learning, his students are some of the most important reaserchers today. He introduced some algorithms and methods that were not published.

This course is dated, the SOTA results have improved, it does not cover transformers and probably all the results have been beaten as this is a fast moving field.

Still this is an interesting, if mathematically sophisticated introduction to deep learning.

# Lecture 1a: Why do we need ML?

::: column-margin
{{< video https://youtu.be/4w0_mJ_6QoI title="Lecture 1 : Why do we need machine learning?" >}}
:::

## What is ML?

-   It is very hard ðŸ˜± to write programs that solve problems like [recognizing a 3d object]{.mark} from a novel viewpoint in new lighting conditions in a cluttered scene.
    -   We donâ€™t know what program to write because we donâ€™t know ðŸ˜• how its done in our brain. ðŸ§ 
    -   Even if we had an clue ðŸ’¡ how to do it â€” the program would be ðŸ˜± horrendously complicated.
-   It is hard to write a program to compute the probability that a [credit card transaction is fraudulent]{.mark}.
    -   There may not be any rules that are both simple and reliable. We need to combine a very large number of weak rules.
    -   [Fraud is a moving target]{.mark}. The program needs to keep changing.

## The ML approach

-   Instead of writing a program by hand for each specific task, we [collect lots of examples]{.mark} that specify the correct output for a given input.
-   A [ML algorithm ðŸ¤– then takes these examples and produces a program that does the job]{.mark}.
    -   ]The program produced by the ML algorithm ðŸ¤–] may look very different from a typical hand-written program]{.mark}. It may contain millions of numbers ðŸ™€ ...
    -   If we do it right, the program works for new cases as well as the ones we trained it on.
    -   If the data changes the program can change too by training on the new data.
-   Massive amounts of computation are now cheaper than paying someone to write a task-specific program.

## Some examples of tasks best solved by ML

-   Recognizing patterns:
    -   Objects in real scenes
    -   Facial identities or facial expressions
    -   Spoken words
-   Recognizing anomalies:
    -   Unusual sequences of credit card transactions
    -   Unusual patterns of sensor readings in a nuclear power plant[^1]
-   Prediction:
    -   Future stock prices or currency exchange rates[^2].
    -   Which movies will a person like[^3]?

[^1]:  use a program no one can interprest to control a nuclear reactor ðŸ˜’

[^2]:  :confused: *if there be predicted at all*

[^3]: a recommendation system is probably best for this ðŸ˜Ž

## The standard example of ML

::: column-margin
![Drosophila melanogaster Proboscis](Drosophila_melanogaster_Proboscis.jpg){credit='Sanjay Acharya'}
:::

-   A lot of genetics is done on fruit flies.
    -   They are convenient because they breed fast.
    -   We already know a lot about them.
-   The MNIST database of hand-written digits is the the ML equivalent of [fruit flies]{.mark}. ðŸ¦‹
    -   They are publicly available and we can learn them quite fast in a moderate-sized neural net.
    -   We know a huge amount about how well various ML methods do on MNIST.
-   We will use MNIST as our standard task.

## Beyond MNIST --- The ImageNet task

-   1000 different object classes in 1.3 million high-resolution training images from the web.
    -   Best system in 2010 competition got 47% error for its first choice and 25% error for its top 5 choices.
-   [Jitendra Malik](https://en.wikipedia.org/wiki/Jitendra_Malik), an eminent neural net sceptic ðŸ’€, said that this competition is a good test of whether DNNs ðŸ§  work well for object recognition.
-   A very deep neural net [@krizhevsky2012imagenet] gets less that 40% error for its first choice and less than 20% for its top 5 choices.

### The Speech recognition task

-   A speech recognition system has several stages:
    -   Pre-processing: Convert the sound wave into a vector of acoustic coefficients. Extract a new vector about every 10 mille seconds.
    -   The acoustic model: Use a few adjacent vectors of acoustic coefficients to place bets on which part of which phoneme is being spoken.
    -   Decoding: Find the sequence of bets that does the best job of fitting the acoustic data and also fitting a model of the kinds of things people say.
-   Deep neural networks pioneered by [George Dahl](https://scholar.google.com/citations?user=ghbWy-0AAAAJ) and [Abdel-rahman Mohamed](https://scholar.google.com/citations?user=tJ_PrzgAAAAJ&hl=en) are now replacing the previous ML method for the acoustic model.

## Phone recognition on the TIMIT benchmark

![Phone recognition](phone_recognition.png){.column-margin} He discusses work from from @mohamed2012acoustic

-   After standard post-processing using a bi-phone model, a deep net with 8 layers gets 20.7% error rate.
-   The best previous speaker independent result on TIMIT [^4] was 24.4% and this required averaging several models.[^5]
-   NLP researcher [Li Deng](https://scholar.google.com/citations?user=GQWTo4MAAAAJ&hl=en) at Microsoft Research realized that this result could change the way speech recognition was done.

[^4]: is a corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. Each transcribed element has been delineated in time. TIMIT was designed to further acoustic-phonetic knowledge and automatic speech recognition systems

[^5]: this is a massive jump in SOTA performance
