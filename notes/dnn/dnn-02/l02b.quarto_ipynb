{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "date: 2017-07-18\n",
        "last-modified: 2013-01-20\n",
        "title: Deep Neural Networks - Notes for lecture 2b \n",
        "subtitle: For the course by Geoffrey Hinton on Coursera\n",
        "description: Notes for Deep learning focusing on Perceptrons, the first generation of neural networks.\n",
        "categories: [deep learning, neural networks, notes, coursera] \n",
        "title-block-banner: banner_deep.jpg\n",
        "editor: \n",
        "  markdown: \n",
        "    wrap: 72\n",
        "---\n",
        "\n",
        "{{< pdf lec2.pdf class=\"column-margin\" >}}\n",
        "\n",
        "\n",
        "The lecture starts with the history of [Perceptrons](https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=Perceptron&id=1195848318&wpFormIdentifier=titleform) [@enwiki:1195848318]\n",
        "\n",
        "Then covers with The perceptron convergence procedure. Next is a deeper dive into the computational geometry of Perceptrons\n",
        "\n",
        "I also added a python implementation from scratch.\n",
        "\n",
        "# Lecture 2b: Perceptrons: The first generation of neural networks\n",
        "\n",
        "::: column-margin\n",
        "\n",
        "{{< video https://youtu.be/TVJBOQzIKLY title=\"2b: Perceptrons: The first generation of neural networks\" >}}\n",
        "\n",
        ":::\n",
        "\n",
        "## The standard paradigm for statistical pattern recognition \n",
        "\n",
        "![The standard Perceptron architecture](standard_Perceptron_architecture.png){.column-margin}\n",
        "\n",
        "\n",
        "1. Convert the raw input vector into a vector of feature activations. Use hand-written programs based on common-sense to define the features.\n",
        "2. Learn how to weight each of the feature activations to get a single scalar quantity.\n",
        "3. If this quantity is above some threshold, decide that the input vector is a positive example of the target class. \n",
        "\n",
        "\n",
        "## The history of Perceptrons\n",
        "\n",
        "- Perceptrons were introduced in [@rosenblatt1962principles] by [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) who popularized them\n",
        "  - They appeared to have a very powerful learning algorithm.\n",
        "  - Lots of grand claims were made for what they could learn to do.\n",
        "- In [@minsky69perceptrons] the authors, analysed what Perceptrons could do and showed their limitations. ^[The main results are available [here](https://en.wikipedia.org/wiki/Perceptrons_(book))]\n",
        "  - Many people thought these limitations applied to all neural network models.\n",
        "- The perceptron learning procedure is still widely used today for tasks with enormous feature vectors that contain many millions of features. \n",
        "\n",
        "::: column-margin\n",
        "![A perceptron](perceptron.png)\n",
        ":::\n",
        "\n",
        "-   why the **bias** can be implemented as a special input unit?\n",
        "-   biases can be treated using weights using an input that is always  one.\n",
        "-   a **threshold** is equivalent to having a negative bias.\n",
        "-   we can avoid having to figure out a separate learning rule for the\n",
        "    bias by using a trick:\n",
        "  -   A bias is exactly equivalent to a weight on an extra input line that\n",
        "    always has an activation of 1.\n",
        "\n",
        "### Binary threshold neurons (decision units)\n",
        "\n",
        "![Binary Theshold Unit](binary_theshold_unit.png){.column-margin}\n",
        "\n",
        "- Introduced in [@mcculloch43a]\n",
        "  - First compute a weighted sum of the inputs from other neurons\n",
        "(plus a bias).\n",
        "  - Then output a 1 if the weighted sum exceeds zero. \n",
        "\n",
        "\n",
        "$$\n",
        "z = b+ \\sum_i{ x_i w_i}\n",
        "$$\n",
        "$$\n",
        " y = \\left\\{ \n",
        "   \\begin{array}{ll}\n",
        "       1 & \\text{if} \\space z \\ge 0 \\\\\n",
        "       0 & \\text{otherwise}\n",
        "   \\end{array}\n",
        "    \\right.\n",
        "$$\n",
        "\n",
        "\n",
        "## How to learn biases using the same rule as we use for learning weights\n",
        "\n",
        "-  A threshold is equivalent to having a negative bias.\n",
        "- We can avoid having to figure out a separate learning rule for the bias by using a trick:\n",
        "  - A bias is exactly equivalent to a weight on an extra input line that always has an activity of 1.\n",
        "  - We can now learn a bias as if it were a weight.\n",
        "\n",
        "## The Perceptron convergence procedure: Training binary output neurons as classifiers\n",
        "\n",
        "-  Add an extra component with value 1 to each input vector. The **bias** weight on this component is minus the threshold. Now we can forget the threshold.\n",
        "-  Pick training cases using any policy that ensures that every training case will keep getting picked.\n",
        "    -  If the output unit is correct, leave its weights alone.\n",
        "    -  If the output unit incorrectly outputs a zero, add the input vector to the weight vector.\n",
        "    -  If the output unit incorrectly outputs a 1, subtract the input vector from the weight vector. \n",
        "- This is guaranteed to find a set of weights that gets the right answer for all the training cases **if any such set exists**.\n",
        "\n",
        "## A full implementation of a perceptrons:\n",
        "\n",
        "code and image from: [Implementing the Perceptron Algorithm in\n",
        "Python](https://towardsdatascience.com/perceptron-algorithm-in-python-f3ac89d2e537)\n",
        "\n",
        "![Perceptron](2022-09-23-08-25-30.png){.column-margin}\n"
      ],
      "id": "f914689c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = datasets.make_blobs(n_samples=150,n_features=2,\n",
        "                           centers=2,cluster_std=1.05,\n",
        "                           random_state=2)\n",
        "#Plotting\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], 'r^')\n",
        "plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'bs')\n",
        "plt.xlabel(\"feature 1\")\n",
        "plt.ylabel(\"feature 2\")\n",
        "plt.title('Random Classification Data with 2 classes')\n",
        "\n",
        "def step_func(z):\n",
        "        return 1.0 if (z > 0) else 0.0\n",
        "      \n",
        "def perceptron(X, y, lr, epochs):\n",
        "    '''\n",
        "    X: inputs\n",
        "    y: labels\n",
        "    lr: learning rate\n",
        "    epochs: Number of iterations\n",
        "    m: number of training examples\n",
        "    n: number of features \n",
        "    '''\n",
        "    m, n = X.shape    \n",
        "    # Initializing parapeters(theta) to zeros.\n",
        "    # +1 in n+1 for the bias term.\n",
        "    theta = np.zeros((n+1,1))\n",
        "    \n",
        "    # list with misclassification count per iteration.\n",
        "    n_miss_list = []\n",
        "    \n",
        "    # Training.\n",
        "    for epoch in range(epochs):\n",
        "        # variable to store misclassified.\n",
        "        n_miss = 0\n",
        "        # looping for every example.\n",
        "        for idx, x_i in enumerate(X):\n",
        "            # Inserting 1 for bias, X0 = 1.\n",
        "            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          \n",
        "            # Calculating prediction/hypothesis.\n",
        "            y_hat = step_func(np.dot(x_i.T, theta))\n",
        "            # Updating if the example is misclassified.\n",
        "            if (np.squeeze(y_hat) - y[idx]) != 0:\n",
        "                theta += lr*((y[idx] - y_hat)*x_i)\n",
        "                # Incrementing by 1.\n",
        "                n_miss += 1\n",
        "        # Appending number of misclassified examples\n",
        "        # at every iteration.\n",
        "        n_miss_list.append(n_miss)\n",
        "    return theta, n_miss_list"
      ],
      "id": "92c2380f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_decision_boundary(X, theta):\n",
        "    \n",
        "    # X --> Inputs\n",
        "    # theta --> parameters\n",
        "    \n",
        "    # The Line is y=mx+c\n",
        "    # So, Equate mx+c = theta0.X0 + theta1.X1 + theta2.X2\n",
        "    # Solving we find m and c\n",
        "    x1 = [min(X[:,0]), max(X[:,0])]\n",
        "    m = -theta[1]/theta[2]\n",
        "    c = -theta[0]/theta[2]\n",
        "    x2 = m*x1 + c\n",
        "    \n",
        "    # Plotting\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"r^\")\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n",
        "    plt.xlabel(\"feature 1\")\n",
        "    plt.ylabel(\"feature 2\")\n",
        "    plt.title('Perceptron Algorithm')\n",
        "    plt.plot(x1, x2, 'y-')"
      ],
      "id": "85c864ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "theta, miss_l = perceptron(X, y, 0.5, 100)\n",
        "plot_decision_boundary(X, theta)"
      ],
      "id": "841746b9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}