{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Text Mining With Python\n",
        "subtitle: a number of NLP tasks in Python\n",
        "date: 2011-11-29\n",
        "categories:\n",
        "    - python\n",
        "    - NLP\n",
        "    - text mining\n",
        "    - code\n",
        "#jupyter: \n",
        "#  kernelspec:\n",
        "#    name: \"ipykernel\"\n",
        "#    language: \"python\"\n",
        "#    display_name: \"Python 3 (ipykernel)\"\n",
        "lastmod: 2022-04-30\n",
        "---\n",
        "\n",
        "\n",
        "![](pexels-brett-jordan-8573113.jpg)\n"
      ],
      "id": "c39b68be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np                           # library for scientific computing and matrix \n",
        "import matplotlib.pyplot as plt              # visualization library\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk                                  # Python library for NLP\n",
        "from nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer    \n",
        "\n",
        "nltk.download('twitter_samples')"
      ],
      "id": "87c35628",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nltk.download('stopwords') # <1>\n",
        "\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english') \n",
        "    \n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet) # <2>\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet) # <3>\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)  # <4>\n",
        "    tweet = re.sub(r'#', '', tweet)             # <5>\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, # <6>\n",
        "                               reduce_len=True) # <6>\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)    # <6>\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # <7>\n",
        "                word not in string.punctuation):  # <8>\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # <9>\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ],
      "id": "940d540a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  download the stopwords\n",
        "2.  remove stock market tickers like \\$GE\n",
        "3.  remove old style retweet text \"RT\"\n",
        "4.  remove hyperlinks\n",
        "5.  remove hashtags\n",
        "6.  tokenize tweets\n",
        "7.  remove stopwords\n",
        "8.  remove punctuation\n",
        "9.  stemming word\n"
      ],
      "id": "b04dbb94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = defaultdict(int)\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs\n",
        "\n",
        "def build_vocab(freqs):\n",
        "    vocab = [k for k, v in freq.items() if (v > 1 and k != '\\n')]\n",
        "    vocab.sort()\n",
        "    return vocab"
      ],
      "id": "2c3b2e40",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "processing unknown tokens\n"
      ],
      "id": "272dacc4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def assign_unk(word):\n",
        "    \"\"\"\n",
        "    Assign tokens to unknown words\n",
        "    \"\"\"\n",
        "    \n",
        "    # Punctuation characters\n",
        "    # Try printing them out in a new cell!\n",
        "    punct = set(string.punctuation)\n",
        "    \n",
        "    # Suffixes\n",
        "    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
        "\n",
        "    # Loop the characters in the word, check if any is a digit\n",
        "    if any(char.isdigit() for char in word):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Loop the characters in the word, check if any is a punctuation character\n",
        "    elif any(char in punct for char in word):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Loop the characters in the word, check if any is an upper case character\n",
        "    elif any(char.isupper() for char in word):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Check if word ends with any noun suffix\n",
        "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Check if word ends with any verb suffix\n",
        "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Check if word ends with any adjective suffix\n",
        "    elif any(word.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Check if word ends with any adverb suffix\n",
        "    elif any(word.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"--unk_adv--\"\n",
        "    \n",
        "    # If none of the previous criteria is met, return plain unknown\n",
        "    return \"--unk--\""
      ],
      "id": "52da877c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# select the lists of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
        "tweets = all_positive_tweets + all_negative_tweets\n",
        "\n",
        "# let's see how many tweets we have\n",
        "print(\"Number of tweets: \", len(tweets))"
      ],
      "id": "10db0b5e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}