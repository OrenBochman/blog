---
title: Tidy Text Mining With R
subtitle: an update on NLP with R
date: 2011-11-29
categories: [R, NLP, Text Mining, tidyverse, code]
---

Computational Linguistics tasks:

-   [x] create a corpus
-   [x] tokenize it
-   [x] clean it up
-   [ ] create a vocabulary
-   [ ] create a frequency list
-   [x] create a term document matrix TDF
-   [ ] list n-grams
-   [ ] generate word clouds
-   [ ] mine TDF it for collocations\
-   similarity
    -   cosine similarity
    -   TDIDF
    -   Nearest neighbor word clustering
-   embedding
    -   [ ] word embeddings
    -   [ ] sentence embeddings
-   [ ] concordance
    -   [ ] KWIC, keywords in context
    -   [ ] KWOC, keywords out of context

## Setup

So this little gem - if it actually works should facilitate installing dependencies as needed
and avoiding the time penalty if they are not.


```{r}
#| lst-label: setup
#| label: setup
#| 
require_install <- function(libs) {

    for (i in libs){
        if( !is.element(i, .packages(all.available = TRUE)) ) {
            install.packages(i)
        }
        library(i,character.only = TRUE)
        }
}

require_install(libs=c('SnowballC','tidytext','dplyr','wordcloud','janeaustenr','gutenbergr','quanteda'))
```

## Corpus

here we build a small corpus by inserting some document into a `tibble`

```{r }
#| lst-label: tm-load
#| label: tm-load
 
doc1 <- "drugs, hospitals, doctors"
doc2 <- "smog, pollution, micro-plastics, environment."
doc3 <- "doctors, hospitals, healthcare"
doc4 <- "pollution, environment, water."
doc5 <- "I love NLP with deep learning."
doc6 <- "I love machine learning."
doc7 <- "He said he was keeping the wolf from the door."
doc8 <- "Time flies like an arrow, fruit flies like a banana."
doc9 <- "pollution, greenhouse gasses, GHG, hydrofluorocarbons, ozone hole, global warming. Montreal Protocol."
doc10 <- "greenhouse gasses, hydrofluorocarbons, perfluorocarbons, sulfur hexafluoride, carbon dioxide, carbon monoxide, CO2, hydrofluorocarbons, methane, nitrous oxide."

text <- c(doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8, doc9, doc10)

tidy_corpus <- tibble(doc_id = 1:10, text=text)   
tidy_corpus
```

`unnest_tokens` - Split a column into tokens, flattening the table into one-token-per-row.

```{r}
#| label: normalize
#| lst-label: normalize

tidy_corpus %>% 
    unnest_tokens(word, text)

head(tidy_corpus)
```

note `unnest_tokens` removes punctuation and lower cases

3.  inspect the corpus

## Text preprocessing

```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) %>%
  ungroup()

original_books

library(tidytext)
tidy_books <- original_books %>%
  unnest_tokens(word, text)

tidy_books
```

```{r}
data(stop_words)

tidy_books <- tidy_books %>%
  anti_join(stop_words)
tidy_books
```

6.  this removes stop words

```{r}
tidy_books %>%
  count(word, sort = TRUE) 
```

```{r}
library(ggplot2)

tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

```{r}
#| lst-label: grab_books_by_hgwells
#| label: grab_books_by_hgwells
#devtools::install_github("ropensci/gutenbergr")
library(gutenbergr)

cache_load <- function(ids) {
  # Ensure the gutenbergr library is available
  if (!requireNamespace("gutenbergr", quietly = TRUE)) {
    stop("The 'gutenbergr' package is required but not installed. Please install it using install.packages('gutenbergr').")
  }
  
  library(gutenbergr)
  
  # Initialize an empty data frame to hold all books' data
  all_books_data <- data.frame()
  
  # Loop through each ID in the provided vector
  for (id in ids) {
    # Define the filename for local storage
    filename <- paste0(id, ".csv")
    
    # Check if the file exists locally
    if (file.exists(filename)) {
      # Load the local copy
      book_data <- read.csv(filename, stringsAsFactors = FALSE)
      #cat("Loaded local copy of ID:", id, "\n")
    } else {
      # Download the book data using gutenbergr
      book_data <- gutenberg_download(id, mirror = "http://mirrors.xmission.com/gutenberg/")
      
      # Save a local copy
      write.csv(book_data, filename, row.names = FALSE)
      #cat("Downloaded and saved local copy of ID:", id, "\n")
    }
    
    # Combine the current book data with the accumulated data frame
    all_books_data <- rbind(all_books_data, book_data)
  }
  
  # Return the combined data frame
  return(all_books_data)
}



cache_load <- function(ids) {
  # Ensure the gutenbergr library is available
  if (!requireNamespace("gutenbergr", quietly = TRUE)) {
    stop("The 'gutenbergr' package is required but not installed. Please install it using install.packages('gutenbergr').")
  }
  
  library(gutenbergr)
  
  # Initialize an empty data frame to hold all books' data
  all_books_data <- data.frame()
  
  # Loop through each ID in the provided vector
  for (id in ids) {
    # Define the filename for local storage
    filename <- paste0(id, ".csv")
    
    # Check if the file exists locally
    if (file.exists(filename)) {
      # Load the local copy
      book_data <- read.csv(filename, stringsAsFactors = FALSE)
      cat("Loaded local copy of ID:", id, "\n")
    } else {
      # Attempt to download the book data using gutenbergr
      book_data <- tryCatch({
        gutenberg_download(id, mirror = "http://mirrors.xmission.com/gutenberg/")
      }, error = function(e) {
        # Return an empty data frame in case of error (book not found)
        data.frame()
      })
      
      # Check if the book data is not empty (book was successfully downloaded)
      if (nrow(book_data) > 0) {
        # Save a local copy
        write.csv(book_data, filename, row.names = FALSE)
        cat("Downloaded and saved local copy of ID:", id, "\n")
      } else {
        # Inform the user that the book is missing and will not be saved
        cat("Book with ID:", id, "is missing and will not be saved locally.\n")
      }
    }
    
    # Only combine the current book data if it's not empty
    if (nrow(book_data) > 0) {
      all_books_data <- rbind(all_books_data, book_data)
    }
  }
  
  # Return the combined data frame
  return(all_books_data)
}

```

```{r}
hgwell_ids = c(35, 36, 159, 456, 1047, 3691, 5230, 11870, 12163, 23218, 28218, 35461,39585)
hgwells <- cache_load(hgwell_ids)


```

```{r}
#| warning: false
#| message: false
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tidy_hgwells %>%
  count(word, sort = TRUE)
```


```{r}
#| warning: false
#| message: false
bronte_ids = c(1260, 768, 969, 9182, 767)
bronte <- cache_load(bronte_ids)


tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tidy_bronte %>%
  count(word, sort = TRUE)
```

```{r}
library(tidyr)

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = author, values_from = proportion) %>%
  pivot_longer(`Brontë Sisters`:`H.G. Wells`,
               names_to = "author", values_to = "proportion")

frequency
```

```{r}
#| warning: false
#| message: false
library(scales)
library(ggplot2)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, 
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```

```{r}
  cor.test(data = frequency[frequency$author == "Brontë Sisters",], ~ proportion + `Jane Austen`)
```

```{r}
cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         ~ proportion + `Jane Austen`)
```

kwik and kwok

```{r}
library(quanteda)
library(gutenbergr)

austen_works = gutenberg_works(author == "Austen, Jane")
austen <- cache_load(austen_works$gutenberg_id)

head(hgwells)
# tidy_hgwells <- hgwells %>%
#   unnest_tokens(word, text) %>%
#   anti_join(stop_words)

#head(tidy_hgwells)

the_corpus <- corpus(austen)
the_tokens <- tokens(the_corpus,case_insensitive = TRUE)
kwic_table <- kwic(the_tokens,pattern = "lady",index = 1:100)
#kwic_table <- kwic(tokens(tidy_hgwells$word),pattern = "time")

#kwic_table <- kwic(tokens(tidy_hgwells$word),pattern = "machine",index = 1:400, case_insensitive = TRUE)
nrow(kwic_table)
head(kwic_table,10)
```
