---
date: 2017-08-06
title: Notes for Lesson 2 of Deep Neural Networks 
subtitle: course by Geffory Hinton on Coursa
description: Notes on  Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera
categories: [deep learning, neural networks, notes, coursera] 
---

# Lecture 2

{{< video https://youtu.be/oID20dIrV94?list=PLiPvV5TNogxKKwvKb1RKwkq2hm7ZvpHz0
    title="Lecture 2 : The Perceptron learning procedure" 
    width="1024" 
    height="720" >}}

## Lecture 2a: Types of neural network architectures 

Feed forward networks are the subject of the first half of the course. 

|   |   |
|---|---|
|![Recurrent nets](2022-09-20-12-20-04.png) | `Feed forward nets` - regression and classication for images and tabular data.|
|![Recurrent nets](2022-09-20-12-21-47.png)|  `Recurrent nets` - sequence to sequence |
|![Hopfield net](2022-09-20-12-18-26.png)| `Hopfield nets` - associative memory using symmetric nets with no hidden units 
|![](2022-09-20-12-19-16.png)| `Boltzmann machines` - symmetric nets with hidden units 

credit: images from [The Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/)

## Lecture 2b: Perceptrons: The first generation of neural networks 

![perceptrons](2022-09-20-12-25-42.png)

- why the `bias` can be implemented as a special input unit?
- biases can be treated using weights using an input that is always one.
- a `threshold` is equivalent to having a negative bias. 
- we can avoid having to figure out a separate learning rule for the bias by using a trick: 
- A bias is exactly equivalent to a weight on an extra input line that always has an activation of 1. 

### The Perceptron convergence procedure: Training binary output neurons as classifiers

code and image from: [Implementing the Perceptron Algorithm in Python](https://towardsdatascience.com/perceptron-algorithm-in-python-f3ac89d2e537)
![](2022-09-23-08-25-30.png)
In english:

1. Add an extra component with value 1 to each input vector. The “bias” weight on this component is minus the threshold. Now we can forget the threshold. 
2. Pick training cases using any policy that ensures that every training case will keep getting picked. 
   1. If the output unit is correct, leave its weights alone. 
   2. If the output unit incorrectly outputs a zero, add the input vector to the weight vector. 
   3. If the output unit incorrectly outputs a 1, subtract the input vector from the weight vector. 
 This is guaranteed to find a set of weights that gets the right answer for all the training cases `if any such set exists`. 
a full implementation of a perceptrons:

```python
def perceptron(X, y, lr, epochs):
    '''
    X: inputs
    y: labels
    lr: learning rate
    epochs: Number of iterations
    m: number of training examples
    n: number of features 
    '''
    m, n = X.shape    
    # Initializing parapeters(theta) to zeros.
    # +1 in n+1 for the bias term.
    theta = np.zeros((n+1,1))
    
    # list with misclassification count per iteration.
    n_miss_list = []
    
    # Training.
    for epoch in range(epochs):
        # variable to store misclassified.
        n_miss = 0
        # looping for every example.
        for idx, x_i in enumerate(X):
            # Inserting 1 for bias, X0 = 1.
            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          
            # Calculating prediction/hypothesis.
            y_hat = step_func(np.dot(x_i.T, theta))
            # Updating if the example is misclassified.
            if (np.squeeze(y_hat) - y[idx]) != 0:
                theta += lr*((y[idx] - y_hat)*x_i)
                # Incrementing by 1.
                n_miss += 1
        # Appending number of misclassified examples
        # at every iteration.
        n_miss_list.append(n_miss)
    return theta, n_miss_list
```

## Lecture 2c: A geometrical view of perceptrons 

### Geometry review

- A `point` (a.k.a. `location`) and an arrow from the origin to that point, are often used interchangeably. 
- A `hyperplane` is the high-dimensional equivalent of a plane in 3-D. 
- The `scalar product` or `inner product` between two vectors  
  - sum of element-wise products. 
  - The scalar product between two vectors that have an angle of less than 90 degrees between them is positive.  
  - For more than 90 degrees it's negative. 

### Weight-space

![](2022-09-23-10-25-43.png)


- Has one dimension per weight.
- A `point` in the space represents a particular setting of all the weights.
- Assuming that we have eliminated the threshold, each `training case` can be represented as a `hyperplane` through the origin.
  - The `weights` must lie on one side of this `hyperplane` to get the answer correct.
- Each training case defines a plane (shown as a black line)
  - The plane goes through the origin and is perpendicular to the input vector.
  - On one side of the plane the output is wrong because the scalar product of the weight vector with the input vector has the wrong sign.
 
## Lecture 2d: Why the learning works ?

We look at the geometrical interpretation which is the proof for the convergence of the Perceptron learning algorithm works. 
We are trying to find a `decision surface` by solving a `convex optimization` problem. The surface is a hyperplane represented by a line where on side is the correct set and the other is incorrect. 
The weight vectors form a `cone`:
  - This means that wights are closed under addition and positive scaler product.
  - At zero it is zero. 
  
### The cone of feasible solutions 

![The cone of feasible solutions](2022-09-25-11-03-48.png)
To get all training cases right we need to find a point on the right side of all the planes. But there may not be any such point! 
If there are any weight vectors that get the right answer for all cases, they lie in a hyper-cone with its apex at the origin.

- The average of two good weight vectors is a good weight vector. 
- The problem is convex. 

Two training case form two hyper planes (shown in black). 
the good weights lie between them  average of good weights is good.  
Geometry of learning using weight space: 

 - it has one dimension per weight (vertex in the graph).  
 - a point in the space represents a particular setting of all the weights. 
 - each training case (once we eliminate the threshold/bias) is a hyper plane through the origin 
 - once the threshold is eliminated each training case can be represented as a hyper place through the origin 
 - the weight must lie on one side of this place to get the answer correct .

Caveats: 

  - convergence depends on the picking the right features 
  - deep nets don't use this procedure - as it only converges for single layer perceptrons - but for more than one layer sum of a solution is not necessarily also a solution. 

## Lecture 2e: What perceptrons can't do 

- This story motivates the need for more powerful networks. 
- These ideas will be important in future lectures, when we're working on moving beyond these limitations. 

# References