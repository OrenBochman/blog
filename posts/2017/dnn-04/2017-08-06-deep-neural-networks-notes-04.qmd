---
date: 2017-08-06
title: Notes for Lesson 4 of Deep Neural Networks 
subtitle: course by Geffory Hinton on Coursa
description: Notes on  Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera
categories: [deep learning, neural networks, notes, coursera] 
---

{{< pdf lec4.pdf width="1024" height="720">}}


{{< video https://youtu.be/Rtk_juucCHc
    title="Lecture 4 : Learning feature vectors for words" 
    width="1024" 
    height="720" >}}

## Lecture 4a: Learning to predict the next word 

We have the basic method for creating hidden layers (backprop), we're going to see what can be achieved with them. We start to ask how the network learns to use its hidden units, with a toy application to family trees and a real application to language modeling. 

A simple example of relational information  

Another way to express the same information 

Christopher = Penelope 

Andrew = Christine 

Margaret = Arthur 

Colin 

Roberto = Maria 

Gina = Emilio 

Alfonso 

Victoria = James 

Charlotte 

Jennifer = Charles 

Lucia = Marco 

Pierro = Francesca 

Angela = Tomaso 

Sophia 

Make a set of propositions using the 12 relationships:

- son, daughter, nephew, niece, father, mother, uncle, aunt 
- brother, sister, husband, wife 
(colin has-father james) 
(colin has-mother victoria) 
(james has-wife victoria) this follows from the two above 
(charlotte has-brother colin) 
(victoria has-brother arthur) 
(charlotte has-uncle arthur) this follows from the above 
A relational learning task  
The structure of the neural net  
Given a large set of triples that come from some family trees, figure out the regularities. 

The obvious way to express the regularities is as symbolic rules: 

HasMother(x,y) & HasHusband(y,z)⇒HasFather(x, z)
  
Finding the symbolic rules involves a difficult search through a very large discrete space of possibilities.  
Can a neural network capture the same knowledge by searching through a continuous space of weights?  
local encoding of person 2 
output 
distributed encoding of person 2 
units that learn to predict features of the output from features of the inputs 
distributed encoding of person 1 
distributed encoding of relationship 
inputs local encoding of relationship 
local encoding of person 1 
 
Architecture: 
the net has bottle necks to force it to learn relations using distributed encoding. 
bottle necks are too small 
What the network learns  
What the network learns  
o 
3 
o 
o 
3 
Christopher 
Andrew 
Arthur 
James 
Charles 
Colin 
Penelope 
Christine 
Victoria 
Jennifer 
Margaret 
Charlotte 
Christopher 
Andrew 
Arthur 
James 
Charles 
Colin 
Christine 
Victoria 
Jennifer 
Margaret 
Charlotte 
 
The six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer. 
- Nationality, generation, branch of the family tree. 
These features are only useful if the other bottlenecks use similar 
representations and the central layer learns how features predict 
other features. For example: 
Input person is of generation 3 and 
relationship requires answer to be one generation up 
implies 
Output person is of generation 2 
This video introduces distributed representations. It's not actually about predicting words, but it's building up to that. 
It does a great job of looking inside the brain of a neural network. That's important, but not always easy to do. 

## Lecture 4b: A brief diversion into cognitive science 

This video is part of the course, i.e. it's not optional, despite what Geoff says in the beginning of the video. 
This video gives a high-level interpretation of what's going on in the family tree network. 
This video contrasts two types of inference: 

- Conscious inference, based on relational knowledge. 
- Unconscious inference, based on distributed representations. 

## Lecture 4c: Another diversion: The Softmax output function 

A Softmax cost function is a general-purpose ML component/technique for combining binary discriminators into a probability distribution to construct a classifier 
We've seen binary threshold output neurons and logistic output neurons. This video presents a third type. 
This one only makes sense if we have multiple output neurons. 
Problems with squared error 
The first "problem with squared error" is a problem that shows up when we're combining the squared error loss function with logistic output units. The logistic has small gradients, if the input is very positive or very negative. 
When assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but multiple binaries with square error do not encode this constraint. 

Softmax 

Cross-entropy
:  the right cost function to use with SoftMax 

The output units in a softmax group 
use a non-local non-linearity: 
softmax 
group 
this is called the "logit" 
zj 
jegroup 
öYi 
 
The right cost function is the 
negative log probability of the right 
answer. 
C has a very big gradient when the 
target value is 1 and the output is 
almost zero. 
— A value of 0.000001 is much better 
than 0.000000001 
— The steepness of dC/dy exactly 
balances the flatness of dy/dz 
öZi 
t j log 
target value 
öyj öZi 
 
the cross entropy cost function - is the correct cost function to use with SoftMax   
Architectural Note: 
SoftMax unit +Cross-Entropy loss function => for classification 
                                                                                                                                                                                                                               
## Lecture 4d: Neuro-probabilistic language models 

This is the first of several applications of neural networks that we'll studying in some detail, in this course. 
Synonyms: word embedding; word feature vector; word encoding. 
All of these describe the learned collection of numbers that is used to represent a word. 
"embedding" emphasizes that it's a location in a high-dimensional space: it's where the words are embedded in that space. When we check to see which words are close to each other, we're thinking about that embedding. 
"feature vector" emphasizes that it's a vector instead of a scalar, and that it's componential, i.e. composed of multiple feature values. 
"encoding" is very generic and doesn't emphasize anything specific. 
looks at the trigram model 

## Lecture 4e: Ways to deal with the large number of possible outputs 

When softmax is very big it becomes hard to train and store. 

- Way 1: a serial architecture, based on trying candidate next words, using feature vectors (like in the family example). This means fewer parameters, but still a lot of work. 
- Way 2: using a binary tree. 
- Way 3: Collobert & Weston's search for good feature vectors for words, without trying to predict the next word in a sentence. 

Displaying the learned feature vectors in a 2-D map  
t-sne output 
We can get an idea of the quality of the learned feature vectors by displaying them in a 2-D map. 
— Display very similar vectors very close to each other. 
Use a multi-scale method called "t-sne" that also displays similar clusters near each other. 
The learned feature vectors capture lots of subtle semantic 
distinctions, just by looking at strings of words. 
— No extra supervision is required. 
— The information is all in the contexts that the word is used in. 
— Consider "She scrommed him with the frying pan." 
 
ather 
increasingly 
greatly 
briefly 
c tay 
already 
only just 
both 
either 
even 
yet 
then 
once 
y _o icia.lly 
*bortny 
A.xyely cu51Nt1y 
never 
immediately 
not 
even tuallyaga.in 
»rmery 
tably 
probably 
likely possibly 
perh 
ce 
ther 
but 
whe 
because 
whil 
before 
e 
here 
tod.atr 
 
Displaying learned feature vectors. Pretty picture! 
