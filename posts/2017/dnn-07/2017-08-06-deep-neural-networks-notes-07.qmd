---
layout: post
title: Deep Neural Networks - Notes From Hinton's Course 
date: 2017-08-06 4:44
categories: [] 
author: Oren Bochman
summary: 
---
Notes from Hinton's Coursera course 

# Neural Networks for Machine Learning 
## Lecture 7a: Modeling sequences: A brief overview 
This video talks about some advanced material that will make a lot more sense after you complete the course: it introduces some generative models for unsupervised learning (see video 1e), namely Linear Dynamical Systems and Hidden Markov Models. These are neural networks, but they've very different in nature from the deterministic feedforward networks that we've been studying so far. For now, don't worry if those two models feel rather mysterious. 
However, Recurrent Neural Networks are the next topic of the course, so make sure that you understand them. 
## Lecture 7b: Training RNNs with back propagation 
Most important prerequisites to perhaps review: videos 3d and 5c (about backprop with weight sharing). 
After watching the video, think about how such a system can be used to implement the brain of a robot as it's producing a sentence of text, one letter at a time. What would be input; what would be output; what would be the training signal; which units at which time slices would represent the input & output? 
## Lecture 7c: A toy example of training an RNN 
Clarification at 3:33: there are two input units. Do you understand what each of those two is used for? 
The hidden units, in this example, as in most neural networks, are logistic. That's why it's somewhat reasonable to talk about binary states: those are the extreme states. 
## Lecture 7d: Why it is difficult to train an RNN 
This is all about backpropagation with logistic hidden units. If necessary, review video 3d and the example that we studied in class. 
Remember that Geoffrey explained in class how the backward pass is like an extra long linear network? That's the first slide of this video. 
Echo State Networks: At 6:36, "oscillator" describes the behavior of a hidden unit (i.e. the activity of the hidden unit oscillates), just like we often use the word "feature" to functionally describe a hidden unit. 
Echo State Networks: like when we were studying perceptrons, the crucial question here is what's learned and what's not learned. ESNs are like perceptrons with randomly created inputs. 
At 7:42: the idea is good initialization with subsequent learning (using backprop's gradients and stochastic gradient descent with momentum as the optimizer). 
## Lecture 7e: Long-term Short-term-memory 
This video is about a solution to the vanishing or exploding gradient problem. Make sure that you understand that problem first, because otherwise this video won't make much sense. 
The material in this video is quite advanced. 
In the diagram of the memory cell, there's a somewhat new type of connection: a multiplicative connection. 
It's shown as a triangle. 
It can be thought of as a connection of which the strength is not a learned parameter, but is instead determined by the rest of the neural network, and is therefore probably different for different training cases. 
This is the interpretation that Mr Hinton uses when he explains backpropagation through time through such a memory cell. 
That triangle can, alternatively, be thought of as a multiplicative unit: it receives input from two different places, it multiplies those two numbers, and it sends the product somewhere else as its output. 
Which two of the three lines indicate input and which one indicates output is not shown in the diagram, but is explained. 
In Geoffrey's explanation of row 4 of the video, "the most active character" means the character that the net, at this time, consider most likely to be the next character in the character string, based on what the pen is doing. 
## Lecture 9a: Overview of ways to improve generalization 
In the discussion of overfitting, we assume that the bottleneck of our ability to do machine learning is the  
amount of data that we have; not the amount of training time or computer power that we have. 
Preventing overfitting 
Approach 1: Get more data! 
— Almost always the best bet if you 
have enough compute power to 
train on more data. 
Approach 2: Use a model that has 
the right capacity: 
— enough to fit the true regularities. • 
Approach 3: Avel 
models. 
— Use models v 
— Or train the rx 
subsets of tlu 
is called "bag 
Approach 4: (Ba) 
Four approaches to reduce overfitting due to too many parameters to training rows: 
Getting more data (increases signal to noise) 
consider normalization 
Sample-wise  
Feature wise pixel standardization 
PCA whitening - reduces dimension + whiting 
ZCA - http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf - the idea is to reducing effect of correlation in adjacent pixels by normalizing feature variance and reducing correlation at features. (does not reduce dimensions of the data) 
consider augmentation. 
random crop/rotation/shear/mirroring/flip 
scaling 
blocking out rectangles 
elastic deformation mesh (used in Unet) 
contrast stretching + adaptive histogram equalization = Contrast limited adaptive histogram equalization (CLAHE) 
ZCA whitening transform  
 
 
 
10 
15 
20 
25 
0 5 10 15 20 25 
10 
15 
20 
25 
0 5 10 15 2025 
0 5 10 1520 25 
0 5 10 1520 25 
10 
15 
20 
25 
10 
15 
20 
25 
 
10 
15 
20 
25 
0 
10 
15 
20 
25 
0 
5 
5 
10 15 20 25 
10 15 20 25 
15 
20 
25 
0 
10 
15 
20 
25 
0 
5 
5 
10 15 20 25 
10 15 20 25 
10 
15 
20 
25 
10 
15 
20 
25 
I폐言 
하디결 
 
 
Standardized Feature MNIST Images 
ZCA Whitening MNIST Images 
random affine transforms 


 
Contrast Stretching 
Histogram Equalization 
Adaptive Histogram Equalization  
 
code: 
image-augmentation-deep-learning-keras (on nminst) 
data augmentation with elastic deformations  
 
(Reduce) model capacity 
early stopping. 
regularization schemes. 
Average different models (architectures/algs/ partitions of the data aka bagging) - see lecture 10 
Bayesian train same model many times and then use multiple weights to predict.  
 
A new issue - is that we now need to fit hyperparameters. 
This introduces a new idea - a three way split of the data 
training - for learning model parameters 
validation - for fitting hyper parameters 
test  - for a final unbiased  estimate of the network 
A further refinement is n-way cross validation: 
 
 
 
 
 
 
 
## Lecture 9b: Limiting the size of the weights 
Limiting the size of the weigh 
The standard L2 weight penalty 
involves adding an extra term to 
the cost function that penalizes 
the squared weights. 
— This keeps the weights small 
unless they have big error 
derivatives. 
öWi 
 
There is some math in this video. It's not complicated math. You should make sure to understand it. 
## Lecture 9c: Using noise as a regularizer 
adding noise to the input can have a regularizing effect. Think lets add noise to a picture - it drowns out most of the small features say blurring them. But for large items - we are trying to learn - they look mostly the same. The advantage is that adding noise is easy. Anyhow - this is more of a buildup of an abstract idea that will later be interpreted using full Bayesian learning. 
L2 weight-decay via noisy inp 
Suppose we add Gaussian noise to the inputs. 
— The variance of the noise is amplified by 
the squared weight before going into the 
next layer. 
In a simple net with a linear output unit directly 
connected to the inputs, the amplified noise 
gets added to the output. 
This makes an additive contribution to the 
This slide serves to show that noise is not a crazy idea. 
The penalty strength can be thought of as being  
σ2i
, or twice that (to compensate for the 1/2 in the weight decay cost function), but that detail is not important here. 
Second slide (the math slide) 
 
 
 
 
The reason why the middle term is zero is that all of the epsilons have mean zero. 
You may notice that the result is not exactly like the L2 penalty of the previous video: the factor 1/2 is missing. Or equivalently, the strength of the penalty is not sigma i squared, but twice that. The main point, however, is that this noise is equivalent to an L2 penalty. 
Jargon: overfitting, underfitting, generalization, and regularization 
Overfitting can be thought of as the model being too confident about what the data is like: more confident than would be justified, given the limited amount of training data that it was trained on. 
If an alien from outer space would take one look at a street full of cars (each car being a training case), and it so happens that there were only two Volkswagens there, one dark red and one dark blue, then the alien might conclude "all Volkswagens on Earth are of dark colours." That would be overfitting. 
If, on the other hand, the alien would be so reluctant to draw conclusions that he even fails to conclude that cars typically have four wheels, then that would be underfitting. 
We seek the middle way, where we don't draw more than a few unjustified conclusions, but we do draw most of the conclusions that really are justified. 
Regularization means forcing the model to draw fewer conclusions, thus limiting overfitting. If we overdo it, we end up underfitting. 
Jargon: "generalization" typically means the successful avoidance of both overfitting and underfitting. Since overfitting is harder to avoid, "generalization" often simply means the absence of (severe) overfitting. 
The "accidental regularities" that training data contains are often complicated patterns. However, NNs can learn complicated patterns quite well. 
Jargon: "capacity" is learning capacity. It's the amount of potential (artificial) brain power in a model, and it mostly depends on the number of learned parameters (weights & biases). 
## Lecture 9d: Introduction to the full Bayesian approach 
The full Bayesian approach could provide an alternative to using SGD. However with the exception of very simple models it is usually computationally intractable as it requires finding the prior distribution for all the parameters.  
We can start with an prior P(params) - and adjust it given each training item.  
Given some data we would have to calculate its likelihood i.e. p(data)  
But to do this we would need to see how it effects all parameter settings - this is the real issue as for 10 settings for 100 nodes we would need to test 10^100 weight combinations... 
 
this is an outline of the Bayesian approach.  
there is a prior distribution over parameters, there is data, say the training data and we can calculate its likelihood and combine it  with the prior to get a posterior.  
With sufficient Bayesian updating will in the limit beat an uninformative prior. 
 
but he does not go into how much data. 
The Bayesian frameworl 
The Bayesian framework assumes that we 
distribution for everything. 
— The prior may be very vague. 
— When we see some data, we combine our 
with a likelihood term to get a posterior distr 
— The likelihood term takes into account how 
observed data is given the parameters of th 
 
a 100 coin tosses motivates the frequentist approach which uses the (ML) maximal likelihood estimate of the probability.  
Next calculates the ml is 0.53 by differentiating and setting the derivative equal to zero. 
Next asks what if we have only one coin toss. 
which is a kin to asking "what if the experiment is too small and there are unobserved outcomes?" in which case we cannot account for their likelihood in a ML estimate. 
A coin tossing example 
Suppose we know nothing about coins excep 
tossing event produces a head with some unl 
probability p and a tail with probability I-p. 
— Our model of a coin has one parameter, p 
Suppose we observe 100 tosses and there al 
What is p? 
here D is the data and W is a set of weights. 
 
 
Bayes Theorem 
joint probability 
prior probability of 
weight vector W 
probabilit 
data give 
p(W) p(DlW) 
ID) 
 
However, it may be possible to approximate a prior. 
The terms "prior", "likelihood term", and "posterior" are explained in a more mathematical way at the end of the video, so if you're confused, just keep in mind that a mathematical explanation follows. 
For the coin example, try not to get confused about the difference between "p" (the probability of seeing heads) and "P" (the abbreviation for "probability"). 
Jargon: "maximum likelihood" means maximizing the likelihood term, without regard to any prior that there may be. 
At 8:22 there's a slightly incorrect statement in the explanation, though not in the slide. The mean is not at .53 (although it is very close to that). What's really at .53 is the mode, a.k.a. the peak, a.k.a. the most likely value. 
The Bayesian approach is to average the network's predictions, at test time, where "average" means that we use network parameters according to the posterior distribution over parameter settings given the training data. Essentially, we're averaging the predictions from many predictors: each possible parameter setting is a predictor, and the weight for that weighted average is the posterior probability of that parameter setting. 
"It's helpful to know that whenever you see a squared error being minimized, you can make a probabilistic interpretation of what's going on, and in that probabilistic interpretation, you'll be maximizing the  
log probability under a Gausian. " 
So the proper Bayesian approach, is to find the full posterior distribution over all possible weight vectors. 
If there's more than a handful of weights, that's hopelessly difficult when you have  
a non-linear net. Bayesians have a lot of ways of  
approximating this distribution, often using Monte Carlo methods.  
But for the time being, let's try and do something simpler.  
Let's just try to find the most probable weight vector.  
So the single setting of the weights that's most probable given the prior 
knowledge we have and given the data. So what we're going to try and do is find  
an optimal value of W by starting with some random weight vector, and then  
adjusting it in the direction that improves the probability of that weight  
factor given the data. It will only be a local optimum. 
The Bayesian interpretation of weight 
-log I D) 
= —logp(D I W) 
1 
(yc -tc)2 
1)   
assuming that the model 
makes a Gaussian prediction 
— log p(W) 
20w 
t 
assuming a 
for the weig 
 
## Lecture 9e: The Bayesian interpretation of weight decay 
In this video, we use Bayesian thinking (which is widely accepted as very reasonable) to justify weight decay (which may sound like an arbitrary hack). 
Maximum A Posteriori (MAP) learning means looking for that setting of the network parameters that has greatest posterior probability given the data. 
As such it's somewhat different from the simpler "Maximum Likelihood" learning, where we look for the setting of the parameters that has the greatest likelihood term: there, we don't have a prior over parameter settings, so it's not very Bayesian at all. Slide 1 introduces Maximum Likelihood learning. Try to understand well what that has to do with the Bayesian "likelihood term", before going on to the next slide. 
The reason why we use Gaussians for our likelihood and prior is that that makes the math simple, and fortunately it's not an insane choice to make. However, it is somewhat arbitrary. 
10:15: Don't worry about the absence of the factor 1/2 in the weight decay strength. It doesn't change the story in any essential way. 
 
## Lecture 10a: Why it helps to combine models 
The papers: 
Improving neural networks by preventing co-adaptation of feature detectors 2012 
G. E. Hinton , N. Srivastava, A. Krizhevsky, I. Sutskever and R. R. Salakhutdinov 
Abstract 
When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This “overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorically large variety of internal contexts in which it must operate. Random “dropout” gives big improvements on many benchmark tasks and sets new records for speech and object recognition. 
Adaptive Mixtures of Local Experts 1998 
Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan and Geoffrey E. Hinton 
Abstract 
"We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network." 
My Notes: 
The main prior idea is to utilize or learn to partition the training data so that one can train specialized models that are local experts on the problem space and then  use some linear combination of the expert's predictions to  make predictions. 
The paper points out that prior work for used an error functions that does not encourage cooperation rather than specialization, which required using many experts in each prediction. Later work added penalty terms in the objective function to gate a single active exert in the prediction.(Jacobs, Jordan, and Barton, 1990). The paper offers an alternative error function that encourages specialization.  
The difference difference between the error functions. 
The cooperative error function: 
does not encourage 
ion. They assume that the 
output 01 
linear combination of the outputs of the local experts, with the gatinl 
the proportion of each local output in the linear combination. So the 
where öiC is the output vector of expert i on case c, p: is the propo 
expert i to the combined output vector, and is the desired output 
This error measure compares the desired output with a blend of t] 
experts, so, to minimize the error, each local expert must make its out 
 
The competitive error function 

The error defined in (3) is simply the negative log probability of generating the desired output vector under the mixture of gaussians model described at the end of the next section.  
To see why this error function works better, it is helpful to compare the derivatives of the two error functions with respect to the output of an expert. From (2) we get 

In equation 4 the term 
pci
 is used to weight the derivative for expert i. 
while from equation 3 we get 
ell*' 
In equation 5 the weighting term takes into account how well expert i does relative to other experts, which is a more useful measure of the relevance of expert i to training case c, especially early in the training. Suppose, for example, that the gating network initially gives equal weights to all experts and k ~d c − ~o c i k > 1 for all the experts. Equation 4 will adapt the best-fitting expert the slowest, whereas equation 5 will adapt it the fastest. 
My wrap up 
Game theoretic framework have to formalize cooperative and competitive aspects of learning and how these might influence network architectures. c.f.  Semantics, Representations and Grammars for Deep Learning (2015) David Balduzzi. 
There has been lots of progress in training single models for multiple tasks. c.f. One Model To Learn Them All (2017) Lukasz Kaiser et all. - covered in this video: One Neural network learns EVERYTHING ?! which uses mixture of expert layer which come from later work:  
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017) 
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean in which mixture of experts is used within large neural networks 
This lecture is about using a mixture of experts to reduce overfitting. The notion is to train lower capacity models specializing on subsets of the data and learn to predict which one would be the best predictor. Then use the best model for prediction. Alternatively we might average the results of the simpler models. 
The lecture is challenging as it skims the prior work failing to sufficiently motivate why the different error function arise (they depend on the way the learning scheme are set up) as the paper tries to bridge between competitive learning and a modular neural network.   
There's, again, a lot of math, although it's less difficult than in videos 9d and 9e. Be sure to understand the formulas before moving on. 
We're going to combine many models, by using the average of their predictions, at test time. 
5:38: There's a mistake in the explanation of why that term disappears. 
The mistake is that -2(t-ybar) is not a random variable, so it makes no sense to talk about its variance, mean, correlations, etc. 
The real reason why the term disappears is simply that the right half of the term, i.e. i, is zero, because ybar is the mean of the yi values. 
## Lecture 10b: Mixtures of Experts 
This is a different way of combining multiple models. 
"Nearest neighbor" is a very simple regression method that's not a neural network. 
7:22: The formula is confusing. 
The idea is a weighted average of squared errors (weighted by those probabilities p_i). 
That can be written as an weighted expectation, with weights p_i, of (t-y_i)^2; or as a sum of p_i * (t-y_i)^2. The formula on the slide mixes those two notations. 
On the next slide it's written correctly. 
10:03: This formula is not trivial to find, but if you differentiate and simplify, you will find it. 
## Lecture 10c: The idea of full Bayesian learning 
In this video you learn what exactly we want to do with that difficult-to-compute posterior distribution. 
We learn about doing which is so time-consuming that we can never do it for normal-size neural networks. This is a theory video. 
We average the predictions from many weight vectors on test data, with averaging weights coming from the posterior over weight vectors given the training data. 
That sounds simple and is indeed, in a sense, what happens. 
However, there's more to be said about what this "averaging" entails. 
The Bayesian approach is all about probabilities, so the idea of producing a single number as output has no place in the Bayesian approach. 
Instead, the output is a distribution, indicating how likely the net considers every possible output value to be. 
In video 9e we introduced the idea that the scalar output from a network really is the mean of such a predictive distribution. We need that idea again here. 
That is what Geoffrey means at 6:37. "Adding noise to the output" is a way of saying that the output is simply the centre of a predictive distribution. 
What's averaged is those distributions: the predictive distribution of the Bayesian approach is the weighted mean of all those Gaussian predictive distributions of the various weight vectors. 
By the way, the result of this averaging of many such Gaussian distributions is not a Gaussian distribution. 
However, if we're only interested in the mean of the predictive distribution (which would not be very Bayesian in spirit), then we can simply average the outputs of the networks to get that mean. You can mathematically verify this for yourself. 
## Lecture 10d: Making full Bayesian learning practical 
Maximum Likelihood is the least Bayesian. 
Maximum A Posteriori (i.e. using weight decay) is slightly more Bayesian. 
This video introduces a feasible method that's even closer to the Bayesian ideal. However, it's necessarily still an approximation. 
4:22: "save the weights" means recording the current weight vector as a sampled weight vector. 
## Lecture 10e: Dropout 
This is not Bayesian. This is a specific way of adding noise (that idea was introduced in general in video 9c). It's a recent discovery and it works very, very well. 
Dropout can be viewed in different ways: 
One way to view this method is that we add noise. 
Another more complicated way, which is introduced first in the video, is about weight sharing and different models. 
That second way to view it serves as the explanation of why adding noise works so well. 
The first slide in other words: a mixture of models involves taking the arithmetic mean (a.k.a. "the mean") of the outputs, while a product of models involves taking the geometric mean of the outputs, which is a different kind of mean. 
## Lecture 11a: Hopfield Nets 
Neural networks and physical systems with emergent collective computational abilities   J. J. HOPFIELD 1982 
Now, we leave behind the feedforward deterministic networks that are trained with backpropagation gradients. We're going to see quite a variety of different neural networks now. 
These networks do not have output units. 
These networks have units that can only be in states 0 and 1. 
These networks do not have units of which the state is simply a function of the state of other units. 
These networks are, instead, governed by an "energy function". 
Best way to really understand Hopfield networks: Go through the example of the Hopfield network finding a low energy state, by yourself. Better yet, think of different weights, and do the exercise with those. 
Typically, we'll use Hopfield networks where the units have state 0 or 1; not -1 or 1. 
## Lecture 11b: Dealing with spurious minima 
The last in-video question is not easy. Try to understand how the perceptron learning procedure is used in a Hopfield net; it's not very thoroughly explained. 
## Lecture 11c: Hopfield nets with hidden units 
This video introduces some sophisticated concepts, and is not entirely easy. 
An "excitatory connection" is a connection of which the weight is positive. "inhibitory", likewise, means a negative weight. 
We look for an energy minimum, "given the state of the visible units". That means that we look for a low energy configuration, and we'll consider only configurations in which the visible units are in the state that's specified by the data. So we're only going to consider flipping the states of the hidden units. 
Be sure to really understand the last two sentences that Geoffrey speaks in this video. 
## Lecture 11d: Using stochastic units to improve search 
We're still working with a mountain landscape analogy. 
This time, however, it's not an analogy for parameter space, but for state space. 
A particle is, therefore, not a weight vector, but a configuration. 
What's the same is that we're, in a way, looking for low points in the landscape. 
We're also using the physics analogy of systems that can be in different states, each with their own energy, and subject to a temperature. 
This analogy is introduced in slide 2. 
This is the analogy that originally inspired Hopfield networks. 
The idea is that at a high temperature, the system is more inclined to transition into configurations with high energy, even though it still prefers low energy. 
3:25: "the amount of noise" means the extent to which the decisions are random. 
4:20: If T really were 0, we'd have division by zero, which is not good. What we really mean here is "as T gets really, really small (but still positive)". 
For mathematicians: it's the limit as T goes to zero from above. 
Thermal equilibrium, and this whole random process of exploring states, is much like the exploration of weight vectors that we can use in Bayesian methods. It's called a Markov Chain, in both cases. 
## Lecture 11e: How a Boltzmann machine models data 
Now, we're making a generative model of binary vectors. In contrast, mixtures of Gaussians are a generative model of real-valued vectors. 
4:38: Try to understand how a mixture of Gaussians is also a causal generative model. 
4:58: A Boltzmann Machine is an energy-based generative model. 
5:50: Notice how this is the same as the earlier definition of energy. What's new is that it's mentioning visible and hidden units separately, instead of treating all units the same way. 
## Lecture 12a: Boltzmann machine learning 
6:50: Clarification: The energy is linear in the weights, but quadratic in the states. What matters for this argument is just that it's linear in the weights. 
## Lecture 12c: Restricted Boltmann Machines 
3:02. Here, a "particle" is a configuration. These particles are moving around the configuration space, which, when considered with the energy function, is our mountain landscape. 
4:58. It's called a reconstruction because it's based on the visible vector at t=0 (via the hidden vector at t=0). It will, typically, be quite similar to the visible vector at t=0. 
A "fantasy" configuration is one drawn from the model distribution by running a Markov Chain for a long time. 
The word "fantasy" is chosen as part of the analogy of a Boltzmann Machine vs. a brain that learned several memories. 
## Lecture 12d: An example of RBM learning 
This is not an easy video. Prerequisite is a rather extensive understanding of what an RBM does. Be sure to understand video 12c quite well before proceeding with 12d. 
Prerequisite for this video is that you understand the "reconstruction" concept of the previous video. 
The first slide is about an RBM, but uses much of the same phrases that we previously used to talk about deterministic feedforward networks. 
The hidden units are described as feature detectors, or "features" for short. 
The weights are shown as arrows, even though a Boltzmann Machine has undirected connections. 
That's because calculating the probability of the hidden units turning on, given the state of the visible units, is exactly like calculating the real-valued state of a logistic hidden unit, in a deterministic feedforward network. 
However, in a Boltzmann Machine, that number is then treated as a probability of turning on, and an actual state of 1 or 0 is chosen, randomly, based on that probability. 
We'll make further use of that similarity next week. 
2:30. That procedure for changing energies, that was just explained, is a repeat (in different words) of the Contrastive Divergence story of the previous video. If you didn't fully realize that, then review. 
## Lecture 13a: The ups and downs of back propagation 
6:15: Support Vector Machines are a popular method for regression: for learning a mapping from input to output, as we have been doing with neural networks during the first half of the course. 
## Lecture 13b: Belief Nets 
7:43. For this slide, keep in mind Boltzmann Machines. There, too, we have hidden units and visible units, and it's all probabilistic. BMs and SBNs have more in common than they have differences. 
9:16. Nowadays, "Graphical Models" are sometimes considered as a special category of neural networks, but in the history that's described here, they were considered to be very different types of systems. 
## Lecture 13c: Learning sigmoid belief nets 
It would be good to read the first part of "The math of Sigmoid Belief Nets" before watching this video. 
4:39. The second part of "The math of Sigmoid Belief Nets" mathematically derives this formula. Read it after finishing this video. 
7:04. Actually, those numbers aren't quite correct, although they're not very far off. The take-home message, however, is correct: p(0,1) and p(1,0) are large, while the other two are small. 
7:33. Here's "explaining away" rephrased in a few more ways: 
If the house jumps, everybody starts wondering what might have caused that. Was there an earthquake? Did a truck hit the house? We're not at all sure. 
When the wind then carries, through the open window, the voice of an upset truck driver bemoaning his bad luck, we know that a truck hit the house. That finding "explains away" the possibility that there might have been an earthquake: all of a sudden, we no longer suspect that there might have been an earthquake, even though we haven't consulted the seismological office. 
In other words: as soon as we learn something about one possible cause (truck hits house), we can make an inference about other possible causes (earthquake). 
## Lecture 13d: The wake-sleep algorithm 
4:38. Another way to say this is that the multiple units behave independently: the probability of unit 2 turning on has nothing to do with whether or not unit 1 turned on. 
5:30. The green weights are the weights of the Sigmoid Belief Net. 
An "unbiased sample" from some distribution is a sample that's really drawn from that distribution. A "biased sample" is a sample that's not quite from the intended distribution. 
We don't really do maximum likelihood learning. We just use the maximum likelihood learning rule, while substituting "a sample from the posterior" by "a sample from the approximate posterior". The only "maximum likelihood" part of it is that the formula for going from that sample to delta w is the same. 
## Lecture 15a: From PCA to autoencoders 
Remember how, in assignment 4, we're use unsupervised learning to obtain a different representation of each data case? PCA is another example of that, but for PCA, there's even greater emphasis on obtaining that different representation. 
Chapter 15 is about unsupervised learning using deterministic feedforward networks. 
By contrast, the first part of the course was about supervised learning using deterministic feedforward networks, and the second part was about unsupervised learning using very different types of networks. 
0:26. A linear manifold is a hyperplane. 
1:25. A curved manifold is no longer a hyperplane. One might say it's a bent hyperplane, but really, "hyperplane" means that it's not bent. 
1:37. "N-dimensional data" means that the data has N components and is therefore handled in a neural network by N input units. 
1:58. Here, that "lower-dimensional subspace" is yet another synonym for "linear manifold" and "hyperplane". 
2:46 and 3:53. Geoffrey means the squared reconstruction error. 
4:43. Here, for the first time, we have a deterministic feedforward network with lots of output units that are not a softmax group. 
An "autoencoder" is a neural network that learns to encode data in such a way that the original can be approximately reconstructed. 
## Lecture 15b: Deep autoencoders 
2:51. "Gentle backprop" means training with a small learning rate for not too long, i.e. not changing the weights a lot. 
## Lecture 15c: Deep autoencoders for document retrieval 
"Latent semantic analysis" and "Deep Learning" sound pretty good as phrases... there's definitely a marketing component in choosing such names :) 
1:14. The application for the method in this video is this: "given one document (called the query document), find other documents similar to it in this giant col## Lection of documents." 
2:04. Some of the text on this slide is still hidden, hence for example the count of 1 for "reduce". 
3:09. This slide is a bit of a technicality, not very central to the story. If you feel confused, postpone focusing on this one until you've understood the others well. 
6:49. Remember t-SNE? 
## Lecture 15d: Semantic Hashing 
We're continuing our attempts to find documents (or images), in some huge given pile, that are similar to a single given document (or image). 
Last time, we focused on making the search produce truly similar documents. This time, we focus on simply making the search fast (while still good). 
This video is one of the few times when machine learning goes hand in hand very well with intrinsically discrete computations (the use of bits, in this case). 
We'll still use a deep autoencoder. 
This video is an example of using noise as a regularizer (see video 9c). 
Crucial in this story is the notion that units of the middle layer, the "bottleneck", are trying to convey as much information as possible in their states to base the reconstruction on. 
Clearly, the more information their states contain, the better the reconstruction can potentially be. 
## Lecture 15e: Learning binary codes for image retrieval 
It is essential that you understand video 15d before you try 15e. 
7:13. Don't worry if you don't understand that last comment. 
## Lecture 15f: Shallow autoencoders for pre-training 
This video is quite separate from the others of chapter 15. 
 
CNN Architecture & hyper parameters 
 
Convolutional Neural Network  example 
INPUT [F,F,3]  
CONV [F,F,K] - basis sensor 
RELU [F,F,K ] - elementwise activation 
POOL [F/2,F/2,S] - down sampling  
FC  - convers volume to class probability 
Hyper parameters: 
K – depth is the number of filters/kernels to use say 12 
F - the RECEPTIVE FIELD or spatial extent of the filters – pixels width and height a neuron sees say 32x32 
S – the STRIDE = step size for the offset used for sliding the filters so that there is an overlap neurons – say 1 
P the amount of PADDING= padding round input with zeros, used because output and input might otherwise have different sizes 
 
As of 2015 per STRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET the recommendation is to Removing  
Pooling 
Removing normalization also recommended  
 
INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC 
 
Seems FC and CONV are functionally equivalent and can be interchanged. 
Some other techniques/layers types: 
1x1 convolution 
Dilated convolutions (acting on spaced out pixels) 
Replacing Max Pooling with ROI region of interrest pooling 
Loss layer – represent the overall error 
Dropout layer - Regularization by droping a unit with probabpility p 
DropConnect - Regularization by dropping  connections instead of units  
Stochastic pooling  
Weight decay \gamma = 0.001 
Image whitening and contrast normalization in preprocessing 

# Some questions I have possed on DNN 
Q1. Is there a way to assess the impact of a trainng case or a batch on the model's, specific layers and specific units?
A1. Over the years since I posed this question I have noticed that it is something researchers seem to have looked at. 
- At first glance it seems like it is im[pssible to assess the impact.  SGD works on mini batches or the full data.
- But when we analy`se  MNIST errors we take the worst misclassifications and we can look at the activation they generate at different level. We can see the activation that leads to a misclassification. So it turns out that it is possible. 
- Hinton also desribed full using MCMC for full baysian learning . Mackay also put DNN on more or less solid baysian footing. I have not implementated it so I cannot speak to the details but intuitively with a posterior it should be possible to condition on a point. 

Lets imagine we could be advised by a "demon" regarding the can assess the over all contribution to signal or noise of different aspects of our model according to the following typology: 
First kind – overall model 
Second kind – each hyper parameter  
Third kind – at each layer 
Fourth kind – at each unit (neuron) 
Fifth kind – at the weights level 
Sixth Kind - part of an training item  that activates neurons (pixels/sounds/words) 
I'm considering an analytics platform that would be based on collecting data from Jason Yosinski's data visualization toolbox  
 
One way to do this is to have a procedure that can quickly unlearn/forget training sets then do a diff. 
(might not be very useful if there are millions of weights) 
We may need some measure of uncertainty from non parametric methods that describes how if we are adding more learning points in places that are fitting our manifold at new point which are like new (learning new atoms or their relations) or we are just moving the surface back and forth at a single location or its neighborhood.  
 
e.g. learn the feature that differentiates birds from bees (generalizing) rather than modelling different points of view for each instance of  bird and bee (modeling noise). 
 
For each row in the data set what do we learn from it ? 
 
more atomic concepts 
Relations on atomic concepts 
better smoothing – fitting missing data 
Short term relationships a>b 
long distance relation a>b>...>c>d 
 
NN loves more data - more features, more layers more observation 
but the model can be grow very big and if we use lots of data we will need to train for a very long time 
 
I would like to explore the following ideas 
 
running some parametric algorithm on the data to bootstrap the neural net's prior distributions closer the final values 
 
similar to the above I'd like to training nn dynamically and possibly non parametrically (you can have more CPU, memory, storage, data etc. but you get penalized for it) 
The TF graph should be expanded/contracted layers membership increased or decreased layers increased, hyper params adjusted during training. 
 
 
Bayesian methods allow choices to be made about where in input space new data should be collected in order that it be the most informative (MacKay, 1992c). Such use of the model itself to guide the collection of data during training is known as active learning. 
 
 
MacKay, D. J. C. (1992c). Information-based objective functions for active data selection. Neural Computation 4 (4), 590-604.  
 
The relative importance of different inputs can be determined using the Bayesian technique of automatic relevance determination (MacKay, 1994a, 1995b; Neal, 1994), based on the use of a separate regularization coefficient for each input. If a particular coefficient acquires a large value, this indicates that the corresponding input is irrelevant and can be eliminated.  
 
Neal, R. M. (1994). Bayesian Learning for Neural Networks. Ph.D. thesis, University of Toronto, Canada. 
  
MacKay, D. J. C. (1994a). Bayesian methods for backpropagation networks. In E. Domany, J. L. van Hemmen, and K. Schulten (Eds.), Models of Neural Networks III, Chapter 6. New York: Springer-Verlag.  
 
MacKay, D. J. C. (1995b). Bayesian non-linear modelling for the 1993 energy prediction competition. In G. Heidbreder (Ed.), Maximum Entropy and Bayesian Methods 
  
 
Questions: In your own words describe a neural network 
 
A Neural Network consists of a graph with the inputs in one side and outputs on the other and between them are hidden units. All these nodes are connected with the connection strength between of the vertex connecting the units called its weight. Generally the graph is bipartite and can thus be organized using layers.  
 
The graph can be trained so that the  
 
 
Weights are the vertices  
Actions – the nodes ? what are these 
Model selection - 
Chaos –  
What is the importance of relative weights – within the same layer, between layers 
Given answers for the above should we use that for bootstrapping the wights instead of using random weights. 
 
Geometry of second order methods. 
Won't using Mini Batched steps help where there is a complex structure. 
 
What is there are many local minima in our surface – how can we learn it all if we are always growing downhill. 
What happens if we have a chaotic surface – I think we can get this with a logistic function - 
What about an oscillation. 
 
Difference between first and second order learning methods  
 
In reinforcement models the game being played is a markov decision process 
 
Do GAN take this concept one step further ? 
 
 
For DNN what filters/kernels are initially selected. Are some different basis functions going to work better than others.  
Also how about making some basis functions size independent by adding a 3by three five by five seven by seven etc. version.  
For video filters that are time dependent. 
Also what about using non orthogonal basis. 
 
Also what about forcing the system to drop basis which is redundant 
 
For DNN we see that usually we have square on square configurations to reduce and mix the data. What about triangular or hexagonal architecture. 
Howa bout looking at RGB&Grey 
 
Postscript: 
 
Batch normalization: Accelerating ... 
Input: Values of overa mini-batch: B = 
Parameters to be leamed: -y, '3 
Output: {Yi 
Xi — 11B 
2 
// mini-b; 
 
Pix2Pix 
 
Attention - all you need is attention 
 
Group Equivariant Convolutional Networks 
 
Steerable CNNs 
 
logarithmic spiral 
 
fractal affine embeddings 
 
simulate stereo vision modes 
 
Visualization 
 
distil journal 
 
Activation-atlas 
 
https://aiyproject.withgoogle.com/open_speech_recording  
https://github.com/petewarden/open-speech-recording 
https://distill.pub/2016/augmented-rnns/ 
Attention and Augmented Recurrent Neural Networks 
- http://colah.github.io/ 
- https://github.com/sunnyyeti/Solutions-to-Neural-Network-for-machine-learning-by-Geoffrey-Hinton-on-Coursera 
- https://github.com/BradNeuberg/hinton-coursera/blob/master/assignment3/a3.m 
- https://github.com/Chouffe/hinton-coursera/tree/master/hw3 
- https://github.com/tensorflow/compression/blob/master/examples/bls2017.py 
- https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/ 
nlp 
- https://arxiv.org/abs/1803.06643 
- https://arxiv.org/abs/1811.00937 
 
