---
date: 2021-09-14
title: Customer Lifetime Value - Pareto/NBD (BTYD) Model
subtitle: CLV and Buy Till You Drop Models
description: "Customer Lifetime Value (CLV) models and Buy Till You Drop (BTYD) models are used to estimate the total value a customer brings to a business over their entire relationship. These models help businesses make informed decisions about customer acquisition, retention, and marketing strategies."
fig-caption: # Marketing Research models
categories: [PPC, data science, digital marketing, quantitative marketing, intelligence] 
keywords: [CLV, BTYD, Pareto/NBD, BG/NBD, customer lifetime value, churn prediction, retention strategies, customer segmentation, predictive analytics, Bayesian modeling, Bayesian data analysis, hierarchical models]
bibliography: references.bib
resources: 
    - fader2005counting.pdf
    - Schmittlein1987Counting.pdf
    - Schmittlein1987Counting.m4a
    - fader2010customer.pdf
    - BTYD-walkthrough.pdf
---

![customer churn time series](chaotic-time-series.jpg){#fig-btyd .column-margin}


::: {.callout-note}
## TL;DR - Too Long; Didn't Read on Buying till you are dying 

![Buy till you die in a nutshell](/images/in_the_nut_shell_coach_retouched.jpg)

The Buy Till You Die (BTYD) model is an early customer lifetime value (CLV) model that estimates the total value a customer brings to a business over their entire relationship.  The pain point it addresses is the invisible churn event -- the fact that your when you customers leave they don't tell you they are leaving.

The model is based on the idea that customers will continue to make purchases until they "die" or simply churn. This model helps businesses understand customer behavior, predict future sales, and make informed decisions about marketing and retention strategies.

In this post I look at the first such model, the Pareto/NBD model in which the authors mapped out the behavioral story and developed a hierarchical Bayesian framework for "counting your customers". As such the model has some challenging mathematics as well as some baked in assumptions. 

In the next two posts we will cover two models that made the mathematics much simpler and those are the ones usually implemented in practice. So in this post I'll go over the assumptions, the behavioral story, then try to present a simple hierarchical model with a plate diagram that can help understand what is going on at a glance.

Besides this I will also try to explain the terminology, unpack the business questions. Finally I would also like to outline why the model got the technical name Pareto / NBD, and the authors approach to estimating its parameters. (There are two parameters per customer)
:::

<audio controls>
  <source src="Schmittlein1987Counting.mp3" type="audio/mp3">
  Your browser does not support the audio element.
</audio>

A while back I worked on some projects that involved lead generation for clients of a digital marketing company. At the time I was encouraged to read articles and I read a number of articles on CLV. I was impressed by the idea that given a lead we could put it through such a model and decide if we should use it or sell it thus recouping some of the acquisition costs. This idea stayed with me for a long time despite being quite unrealistic in just about any situation I ever worked in. Sure someone might pay you for a lead. But how much can a CLV tell you about a prospect who never bought anything from you?

So in this and the next two posts I will cover some of the original CLV models. My experience is that going to the source can be clear up lots of misconceptions like the one described above. 

Many firms interested in estimating customer lifetime value (CLV) 
This can let understand if a brand is growing, it can help forecast retail sales. It can suggest when and to whom they might intervene with  a retention interventions to deter churn. And to some extent it can even help set bounds on acquisition costs for advertising.

In this post I will cover the basics of CLV and BTYD models, their applications, and some key considerations for implementation. 

The natural starting point for CLV is estimating the likelihood that clients are retained and continue to purchase. For CLV we also need to estimate how long they will stay and the revenue they generate during their lifetime.

In terms of practical implementation the BTYD models in this note are covered by the BTYD R package and explained in [Buy 'Til You Die - A Walkthrough](https://cran.r-project.org/web/packages/BTYD/vignettes/BTYD-walkthrough.pdf)  

Also there is a PYMC marketing sample [here](https://www.pymc-marketing.io/en/latest/notebooks/clv/pareto_nbd.html)



## CLV and Buy Till You Die (BTYD) Models

In this section I will try to explain the BTYD model. I will start with some motivation, the behavioral story and then my attempt at a short hierarchical model together with a plate diagram. Also the model is called Pareto/NBD referencing the Pareto type II (to model churn) and Negative binomial distribution (to model repeat purchases). But neither of these distributions appear directly in the model. So for a deeper I also try to explain thees in the callouts bellow.

### Motivation

Pareto/NBD BTYD model considers repeat-buying behavior in settings where customer churn is not directly observed and assumes customers buy at a randomly at a steady rate until they churn.

-  Time to churn is modelled using the Pareto (exponential-gamma mixture) timing model.
- Repeat-buying behavior while active is modelled using the NBD (Poisson-gamma mixture) counting model. 

The paper and model are associated with confusing acronyms.
Both the Pareto and negative binomial distributions do not appear directly in the model. Thus the naming conventions needs some explanation.

The authors posit the following business questions:
 
1. How many customers does the firm currently have? 
2. How has this customer base grown over the past year? 
3. Which individuals on this list most likely represent active customers? Inactive customers? 
4. What level of transactions should be expected next year by those on the list, both individually and collectively


## Pareto/NBD (BTYD) Model

This model is from [@Schmittlein1987Counting] in which the authors propose a framework for understanding customer behavior over time, though [primarily they are concerned with counting active customers]{.mark}, which is important in at least three settings: 

- Monitoring the size and growth rate of a firm's ongoing customer base, 
- Evaluating a new product's success based on the pattern of trial and repeat purchases, and 
- Targeting a subgroup of customers for advertising and promotions. 

They develop a model based on the number and timing of the customers' previous transactions. This approach allows computation of the probability that any particular customer is still active.

### The Behavioral Story and The Key Assumption

Each customer buys according to a Poisson process until they churn both of these are modeled  heterogeneously across customers in terms of both buying rate and lifetime. Thus the model is built on two latent processes for each customer:

1. **Purchasing process:**

   * Each customer $i$ makes repeat transactions according to a **Poisson process** with rate parameter $\lambda_i$.
   * The $\lambda_i$ are drawn independently from a  **Gamma distribution** for each customers.
   * The authors show that aggregated across customers, this leads to a **negative binomial distribution** of transactions (hence "NBD").
   * While this captures heterogeneity in purchasing behavior, it makes an assumption on the timing of purchases: the time between purchases is **exponentially distributed** which has a memoryless property^[i.e. when the future is independent of the past]. This is a simplifying assumption but is not a good fit for bursty purchasing behavior or for periodic purchases (e.g. monthly subscriptions).

2. **Churn process:**

   * Each customer has a Churn probability modeled as an **exponential lifetime** with Churn rate $\mu_i$.
   * Again $\mu_i$ parameter values are drawn independently from a  **Gamma distribution** for each customers.
   * Therefore Survival times are heterogeneous across the population. The authors demonstrate that when aggregated across customers, the distribution of survival times follows a **Pareto II distribution**. Hence the Pareto in the name.
   * This captures heterogeneity in customer lifetimes, but again the **exponential lifetime** assumption implies a memoryless property: the probability of Churn is constant over time and the time to churn is exponentially distributed. Again this is a reasonable simplifying assumption but needs to be validated against real-world data when applying the model.


3. **Observed data:**

   * For each customer, we see the history of transactions up to a calibration window.
   * What we don't see is whether the customer is alive or has already churned — the model infers this.
   * We also assume that the two processes (purchasing and Churn) are independent. 

So:

* While alive, purchases follow a Poisson process.
* The lifetime of activity follows an exponential process.
* Across customers, heterogeneity in both processes is captured via Gamma distributions.
* Hence, the **Pareto/NBD**.

---

### Hierarchical Model Specification

The authors, D.G. Morrison, is noted in the references for previous work titled "Analysis of Consumer Purchase Data: A Bayesian Approach," further indicating a connection to Bayesian methodologies. The paradigm the authors leans to the Bayesian. The model is Hierarchical and key components of the derivation is uses bayes theorem. For inference, the approach is less clear cut, as the are challenges calibrating the model to the data, picking parameters and setting priors.


For customer $i$:

1. **Transaction process (Poisson-Gamma → Negative Binomial):**

$$
y_{i}(t) \mid \lambda_i, \, \text{alive} \sim \mathcal{Poi}(\lambda_i t) \quad \text{with } \lambda_i \sim \mathcal{Gamma}(r, \alpha)
$$ {#eq-transaction-process}

2. **Churn process (Exponential-Gamma → Pareto):**

$$
T_i \mid \mu_i \sim \mathcal{Exp}(\mu_i) \quad \text{with } 
\mu_i \sim \mathcal{Gamma}(s, \beta)
$$ {#eq-churn-process}

3. **Joint prior (independence across processes):**

$$
p(\lambda_i, \mu_i) = p(\lambda_i) \, p(\mu_i)
$$

4. **Observed data:**
   For each customer, we observe:

   * $x_i$: number of repeat transactions in calibration period,
   * $t_{x,i}$: time of last transaction,
   * $T_i$: length of calibration period.

5. **Posterior inference:**

   * From $(x_i, t_{x,i}, T_i)$, we update beliefs about whether $i$ is still active.

   * We can compute:

     * Probability that customer is still alive.
     * Expected future transactions.
     * CLV (Customer Lifetime Value).



* **Hierarchical model:** Poisson with Gamma heterogeneity for buying, Exponential with Gamma heterogeneity for Churn, yielding the Pareto/NBD mixture.

In a more compact form we can write:

$$\begin{aligned}
\lambda_i &\sim \mathrm{Gamma}(r,\alpha) && \text{(purchase‐rate heterogeneity)}\\
\mu_i &\sim \mathrm{Gamma}(s,\beta) && \text{(churn‐rate heterogeneity)}\\
T_i^{\ast}\mid \mu_i &\sim \mathrm{Exp}(\mu_i) && \text{(latent lifetime)}\\[2pt]
N_i \mid \lambda_i, T_i^{\ast}, T_i &\sim \text{PoissonProcess}\!\left(\lambda_i\ \text{on}\ [0,\tau_i]\right),\quad \tau_i=\min(T_i,T_i^{\ast})\\
x_i &= N_i(\tau_i),\qquad
t_{x,i}=\max\{t\le \tau_i:\text{event at }t\}\ \ (\text{or }0\text{ if }x_i=0)
\end{aligned}
$$

- In the classic BTYD setup, $(r,\alpha,s,\beta)$ are fixed unknowns estimated by MLE; fully Bayesian just adds priors on them.
- Inputs $\mathbf{x}_i=(x_i,t_{x,i},T_i)$. 
- Outputs typically $P(\text{alive}\mid \mathbf{x}_i)$, $E[N_i(T_i,T_i+\tau)\mid \mathbf{x}_i]$, and CLV.
- Integrating out $\mu_i$ gives $T_i^{\ast}\sim$ Pareto-II $(s,\beta)$, which is the "Pareto" in Pareto/NBD.



```{python}
#| fig-cap: "Plate diagram for the Pareto/NBD"
#| echo: false
#| warning: false
# label: fig-pareto-nbd-pgm

# Pareto/NBD (BTYD) plate diagram with daft
#   λ_i ~ Gamma(r, α)          (purchase rate heterogeneity)
#   μ_i ~ Gamma(s, β)          (Churn-rate heterogeneity)
#   T_i* | μ_i ~ Exponential(μ_i)  (latent lifetime)
#   {x_i, t_{x,i}} | λ_i, T_i*  (Poisson transactions observed as summaries over [0, T_i])
# Pareto/NBD plate (classic BTYD, MLE mixture): Gamma heterogeneity -> Pareto lifetime marginal
import matplotlib.pyplot as plt

import daft

pgm = daft.PGM(shape=[8, 6], origin=[0, 0], grid_unit=1.2)

# Fixed population parameters (estimated by MLE; NOT random here)
#pgm.add_node(daft.Node("r",     r"$r$",        1.0, 5.2))     # fixed
#pgm.add_node(daft.Node("alpha", r"$\alpha$",   2.2, 5.2))     # fixed
#pgm.add_node(daft.Node("alpha", r"$\alpha$",   2.2, 5.2))     # fixed
pgm.add_node(daft.Node("lambda_prior", r"$\mathcal{Gamma}(r, \alpha)$", 1.6, 5.2,fixed=True, offset=(0, 5)))
pgm.add_node(daft.Node("mu_prior", r"$\mathcal{Gamma}(s, \beta)$", 6.4, 5.2,fixed=True, offset=(0, 5)))
#pgm.add_node(daft.Node("s",     r"$s$",        5.8, 5.2))     # fixed
#pgm.add_node(daft.Node("beta",  r"$\beta$",    7.0, 5.2))     # fixed

# Customer-level latent rates
pgm.add_node(daft.Node("lam",   r"$\lambda_i$", 1.6, 3.2))
pgm.add_node(daft.Node("mu",    r"$\mu_i$",     6.4, 3.2))
pgm.add_node(daft.Node("Tstar", r"$T_i^\ast$",  6.4, 2.1))  # latent lifetime (not observed)

# Observed summaries over calibration window [0, T_i]
pgm.add_node(daft.Node("x",     r"$x_i$",       3.0, 1.2, observed=True))
pgm.add_node(daft.Node("tx",    r"$t_{x,i}$",   4.6, 1.2, observed=True))
pgm.add_node(daft.Node("T",     r"$T_i$",       6.2, 1.2, observed=True))

# Edges: fixed params parametrize heterogeneity
pgm.add_edge("lambda_prior", "lam")    
pgm.add_edge("mu_prior", "mu")    

#pgm.add_edge("r", "lam")    
#pgm.add_edge("alpha", "lam")

#pgm.add_edge("s", "mu")      
#pgm.add_edge("beta", "mu")
pgm.add_edge("mu", "Tstar")    # Exponential lifetime given μ
pgm.add_edge("lam", "x")      
pgm.add_edge("lam", "tx")
pgm.add_edge("Tstar","x")    
pgm.add_edge("Tstar","tx")
pgm.add_edge("T","x")      
pgm.add_edge("T","tx")

pgm.add_plate(daft.Plate([0.9, 0.2, 6.2, 3.8], "$i=1:N$" ))
pgm.render()
plt.tight_layout()
plt.show()

# Optional: save a file for your Quarto doc
plt.savefig("pareto_nbd_pgm.png", dpi=200, bbox_inches="tight")
```

where

* $r,\alpha$: Gamma **shape** and **rate** governing heterogeneity in purchase rates $\lambda_i$.
* $s,\beta$: Gamma **shape** and **rate** governing heterogeneity in dropout rates $\mu_i$.
* $\lambda_i$: individual’s Poisson purchase rate **while alive**.
* $\mu_i$: individual’s exponential **dropout (churn) rate**.
* $T_i^\ast$: individual’s **(latent) churn time**; not directly observed.
* $T_i$: length of the **calibration window** used to summarize past behavior.
* $x_i$: **repeat transactions** observed in $[0,T_i]$.
* $t_{x,i}$: **recency** — time of the last purchase within the calibration window.
* $\mathbf{x}_i = (x_i,t_{x,i},T_i)$: input features used by the model.
* $N_i(T_i,T_i+\tau)$: **future transactions** in a holdout horizon $\tau$ (predicted).
* $P(\text{alive}\mid \mathbf{x}_i)$: **survival probability** at the end of calibration (predicted).




::: {.callout-tip}

## Where the "Pareto" comes from in the name

The model assumes:

$$
\mu_i \sim \text{Gamma}(s,\beta),
$$

so at the **hierarchical level**, it is Gamma.

However, when you integrate this out to get the **distribution of customer lifetimes** (the time until Churn), you get something that is **Pareto-like**.

---

1. Conditional lifetime distribution

Given Churn rate $\mu_i$, the lifetime $T_i$ is exponential:

$$
T_i \mid \mu_i \sim \mathcal{Exp}(\mu_i).
$$

So

$$
P(T_i > t \mid \mu_i) = e^{-\mu_i t}.
$$

2. Mix over $\mu_i \sim \mathcal{Gamma}(s, \beta)$

Now take expectation over the heterogeneity distribution:

$$
P(T_i > t) = \int_0^\infty e^{-\mu t} \cdot \frac{\beta^s}{\Gamma(s)} \mu^{s-1} e^{-\beta \mu} \, d\mu.
$$

This integral evaluates to:

$$
P(T_i > t) = \left(\frac{\beta}{\beta + t}\right)^s.
$$

---

3. Recognize the distribution

That survival function is exactly that of a **Pareto Type II distribution** (also called Lomax):

$$
T_i \sim \text{ParetoII}(s, \beta).
$$

So:

* At the **parameter level**, Churn rates $\mu$ are Gamma.
* At the **lifetime level**, the implied distribution of Churn times $T$ is Pareto.

That is why the model is called **Pareto/NBD**:

* **NBD** comes from the Poisson–Gamma mixture for transactions.
* **Pareto** comes from the Exponential–Gamma mixture for lifetimes.

:::


## Pareto/NBD

In [@fader2005counting] the authors developed a new model, the beta-geometric/NBD (BG/NBD), which represents a slight variation in the "behavioral story" associated with the Pareto/NBD, which is much easier to implement, and this has replaced the Pareto/NBD in many applications.


### Pareto/NBD BTYD model Assumptions

1. While active, the number of transactions made by a customer in a time period of length $t$ is distributed Poisson with transaction rate $\lambda$. This is equivalent to assuming that the time between transactions is distributed exponential with transaction rate $\lambda$.
2. **Heterogeneity** in transaction rates across customers follows a $\Gamma(r,\alpha)$ where $r$ is the shape parameter $\alpha$ is the scale parameter.
3. Each customer has an unobserved "lifetime" of length $\tau$. This point at which the customer becomes inactive is distributed exponential with Churn rate $\mu$.
4. Heterogeneity in Churn rates across customers follows a $\Gamma(r,\beta)$ distribution with shape parameter $s$ and scale parameter $\beta$.
5. The transaction rate $\lambda$ and the Churn rate $\mu$ vary independently across customers.

In practice parameter estimation is difficult and so BG/NBD was proposed as a "drop in" replacement.
