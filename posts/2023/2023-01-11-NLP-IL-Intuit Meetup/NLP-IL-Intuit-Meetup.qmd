---
Title: NLP IL F2F Meetup at Intuit
date: 23-01-11
draft: true
---

# Contents


# Session Video
{{< video https://www.youtube.com/watch?v=o6SFgeU69tg&list=PLN5QgHTG8IMJNhBg3cCa3MaZXPtJKr7Ze&index=4 title = "NLP IL Meetup at Intuit" start = "116" >}}

# Introductions
![intuit](session0/intuit.png){}
![shani gershtein](session0/shani-gershtein.png){}
![nlp.il](session0/nlp_il_highlights.png){}
![nlp.il at intuit](session0/nlp_il_at_intuit.png){}
![nlp.il at intuit](session0/Ido_Farhi.png){}
![who we are](session0/who_we_are.png){}
![who we serve](session0/who_we_serve.png){}

![quickbooks](session0/quickbooks.png){}
![credit karma](session0/credit_karma.png){}
![mail chimp](session0/mail_chimp.png){}
![intuit israel](session0/intuit_israel.png){}
![intuit nlp team](session0/intuit_nlp_team.png){}
![DS IL](session0/ds_il.png){}

# SCROLLS: Standardized CompaRison Over Long Language Sequences

## Paper

[Standardized CompaRison Over Long Language Sequences SCROLLS](https://arxiv.org/abs/2201.03533)

## Abstract

NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and handpick ones where the text is naturally long, while prioritizing tasks that involve synthesizing information across the input. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. Initial baselines, including Longformer Encoder-Decoder, indicate that there is ample room for improvement on SCROLLS. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.

## Speaker

- [Uri Shaham](https://www.linkedin.com/in/uri-shaham/) [Uri_Shaham](https://twitter.com/Uri_Shaham) [Page](https://urisha.github.io/) - PhD candidate in Tel Aviv university,

- Uri is a Ph.D. student at the Tel Aviv University NLP lab, working with Omer Levy. His research focuses on conditional language generation, involving model architectures, inference algorithms, and evaluation benchmarks.

## Slides

![SCROLLS](session1/Screenshot from 2023-03-08 17-24-23.png){}

![SOTA in NLU](session1/Screenshot from 2023-03-08 17-25-15.png){}

![Problem - Transformers](session1/Screenshot from 2023-03-08 17-25-54.png){}

![Problem - Solutions](session1/Screenshot from 2023-03-08 17-27-03.png){}

![Evaluation on long texts](session1/Screenshot from 2023-03-08 17-28-04.png){}
![Can we do better?](session1/Screenshot from 2023-03-08 17-29-24.png){}

- Preplexity of next token prediction 
- Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 284–294, Melbourne, Australia. Association for Computational Linguistics.
- Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through Memorization: Nearest Neighbor Language Models. In International Conference on Learning Representations (ICLR), 2020b
- Simeng Sun, Kalpesh Krishna, Andrew MattarellaMicke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 807–822, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
- Ofir Press, Noah A. Smith, and Mike Lewis. 2021a. Shortformer: Better language modeling using shorter inputs. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5493–5505, Online. Association for Computational Linguistics.

![SCROLLS](session1/Screenshot from 2023-03-08 17-29-36.png){}
![Building SCROLS](session1/Screenshot from 2023-03-08 17-30-54.png){}
![Building SCROLS](session1/Screenshot from 2023-03-08 17-31-24.png){}
![Desiderata](session1/Screenshot from 2023-03-08 17-31-32.png){}
![Tasks](session1/Screenshot from 2023-03-08 17-31-55.png){}
![Example Q&A](session1/Screenshot from 2023-03-08 17-32-52.png){}
![delete](session1/Screenshot from 2023-03-08 17-33-56.png){}
![Examples require long-range reasoning](session1/Screenshot from 2023-03-08 17-34-36.png){}
![Does Context imporove peformance](session1/Screenshot from 2023-03-08 17-34-38.png){}
![Processing the entire input helps](session1/Screenshot from 2023-03-08 17-35-07.png){}
![Analysis](session1/Screenshot from 2023-03-08 17-35-34.png){}
![Does More context improve performance](session1/Screenshot from 2023-03-08 17-37-36.png){}
![Language understading is crucial](session1/Screenshot from 2023-03-08 17-46-58.png){}
![Is more context all you need?](session1/Screenshot from 2023-03-08 17-49-54.png){}
![Is more context all you need?](session1/Screenshot from 2023-03-08 17-50-52.png){}
![How far is SCROLLS from being solved](session1/Screenshot from 2023-03-08 17-50-59.png){}
![Big room for improvement?](session1/Screenshot from 2023-03-08 17-51-23.png){}
![Leaderboard](session1/Screenshot from 2023-03-08 17-51-44.png){}
![Conclusions](session1/Screenshot from 2023-03-08 17-52-37.png){}
![Leaderboard](session1/Screenshot from 2023-03-08 17-55-47.png){}
## Notes
- Few comments about this talk.
- Met with a company that worked on patents and had lots of issues with long range. 
- Most of the points rasied were 'straw men' so there is not much surprise.

# Efficient Long-Text Understanding with Short-Text Models
## Paper
[Efficient Long-Text Understanding with Short-Text Models](https://arxiv.org/abs/2208.00748)
## Abstract: 
Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles and long documents, due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.
## Speaker
- Maor Ivgi - PhD candidate in Tel Aviv university,
- Maor is an NLP researcher and entrepreneur. He has vast experience in implementing state-of-the-art deep learning models for real-world use cases. He received his masters in Computer Science at Tel-Aviv University advised by Prof. Jonathan Berant, focusing on NLP models’ Robustness. As a Ph.D. candidate at Prof. Berant’s lab, his research is focused on long-range reasoning in large language models.
## Slides

![Efficient Long-Text Understanding with Short-Text Models](session2/Screenshot from 2023-03-08 18-03-48.png){}
![NLP Papers](session2/Screenshot from 2023-03-08 18-04-11.png){}
- NLP seems to have reached new level of maturity for use in Industry
    - c.f. Attetion is all you need
    - c.f. BERT pre-training of deep bidirectional transformers for language understanding
![Model Timeline](session2/Screenshot from 2023-03-08 18-04-33.png){}
![Q&A challanges](session2/Screenshot from 2023-03-08 18-08-53.png){}
![Transformers - Good on short text NLU](session2/Screenshot from 2023-03-08 18-09-02.png){}
![Long Text NLU Fail](session2/Screenshot from 2023-03-08 18-09-05.png){}

![Transformers Quadratic dependency limits](session2/Screenshot from 2023-03-08 18-09-16.png){}
![Transformers Attention complexity](session2/Screenshot from 2023-03-08 18-12-44.png){}
- Transformers have issues with long texts: 
    - self attention is $O(n^2)$ 
    - cross attention is $O(nk)$
![Novel Transformer Architecture Papers](session2/Screenshot from 2023-03-08 18-13-05.png){}
- Efficent LLM papers are:
- Hard to understand,
- Hard to generelize (due to platform specific engeneering tricks)
- Expensive to reproduce
- Inference run into Memory is an issue
- Training is often on begining of document so does not see the end
- Self Attention is has a limited window size.

![SLED - Locality](session2/Screenshot from 2023-03-08 18-16-04.png){}
![SLED - Properties](session2/Screenshot from 2023-03-08 18-17-52.png){}
- SLED's Approch
    - Assume locality of information: "In an encoder-deocder architecture, the encoder can effectively contextualiza input tokens with local context only, leaving long range dependency to be handled by the decoder."
    - Split text into short fixed length overlapping chunks of text (short contexts).
    - Prepend the `prefix/prompt` to each chunk 
    - The decoder will need to put it all together.
![SLED Properties](session2/Screenshot from 2023-03-08 18-18-43.png){}
![Model Size effect](session2/Screenshot from 2023-03-08 18-20-47.png){}
![SLED Performance Boost](session2/Screenshot from 2023-03-08 18-23-55.png){}
![SLED is Competitive with short text models](session2/Screenshot from 2023-03-08 18-24-34.png){}
![Analysis](session2/Screenshot from 2023-03-08 18-25-03.png){}
- this is a great slide!
- it summerises lots of info
- SLED's Analysis
    - Contextual encoding is crucial
    - Cheating is not enough
    - The is real benefit in fusion 
![Finding a Needle in a Haystack](session2/Screenshot from 2023-03-08 18-25-30.png){}
![Finding a Needle perfectly](session2/Screenshot from 2023-03-08 18-27-07.png){}
![Fusing Information Pieces](session2/Screenshot from 2023-03-08 18-29-19.png){}
- what is cheeting?

![Cheating is not enough](session2/Screenshot from 2023-03-08 18-31-20.png){}

- Quantifing SLED's benefits using relative improvement.

$$
\text{Relative Improvement} = \frac{Score(SLED)-Score(Bart)}{Score(Bart)}
$$
![Gains Formula from longer inputs Gains](session2/Screenshot from 2023-03-08 18-32-48.png){}
![Chart of longer inputs Gains](session2/Screenshot from 2023-03-08 18-33-45.png){}
![Limitations & Future Work](session2/Screenshot from 2023-03-08 18-43-21.png){}

- Limits & Future Work
    - Long outputs are still a constraint
    - No explicit global contextualization
    - No explicit globabl positional information
    - No applicable for deocder-ony architecture
    - (Corrective) pre-training is expected to help

![Takeways](session2/Screenshot from 2023-03-08 18-43-51.png){}
- Takeaways
    - Individual pieces of information are localized
    - Fusioin in decoder works
    - SLED does well on long range tasks.
![Questions](session2/Screenshot from 2023-03-08 18-44-07.png){}

- Main points
They point out that the encoder can usually do a adequate job of understanding the input by looking at local context. Mostly a window with a few surrounding sentences. It uses this to create encode the input into a compact representation we call the state. The decoder will then be leverage the compression with "adaquate" encodings to efficently retrieve results from much longer contexts during inference on different tasks.

# An Overview of Modern Speech Recognition
## Abstract
Automatic speech recognition has been impacted by advances in related fields like image processing and natural language processing in recent years. One notable achievement in these areas has been the use of self-supervised learning to improve performance in computer vision and NLP tasks. This led to the development of the first self-supervised language model for speech representations, which has demonstrated impressive results in various NLP tasks. In this talk, we will review the key principles of automatic speech recognition and discuss the current progress, research, and challenges in the field
## Speaker 
- Gal Hever - Algorithm Developer, Vision Map 
- MSc in Data Science, with over a decade of accumulated expertise in Machine Learning & Data Analytics from 8200, academy, and industry. Deploying algorithms to production by applying data-driven Machine Learning & AI solutions end to end, starting from research to development and testing.
## Slides
![Overview](session3/Screenshot from 2023-03-09 12-06-33.png){}
![Conversational AI](session3/Screenshot from 2023-03-09 12-26-06.png){}
![ASR](session3/Screenshot from 2023-03-09 12-26-15.png){}
![ASR input challanges](session3/Screenshot from 2023-03-09 12-26-34.png){}
![Signal & Noise](session3/Screenshot from 2023-03-09 12-26-36.png){}
![Ideal System](session3/Screenshot from 2023-03-09 12-26-40.png){}
![ASR Task](session3/Screenshot from 2023-03-09 12-27-02.png){}
![slide009](session3/Screenshot from 2023-03-09 12-28-01.png){}
![slide010](session3/Screenshot from 2023-03-09 12-38-13.png){}
![slide011](session3/Screenshot from 2023-03-09 12-38-26.png){}
![WER Metric](session3/Screenshot from 2023-03-09 12-38-54.png){}
![ASR History](session3/Screenshot from 2023-03-09 12-39-17.png){}
![ASR Time Line](session3/Screenshot from 2023-03-09 12-42-19.png){}
![Augumentations](session3/Screenshot from 2023-03-09 12-43-37.png){}
![WER we are 21](session3/Screenshot from 2023-03-09 12-44-16.png){}
![WER we are 2](session3/Screenshot from 2023-03-09 12-44-37.png){}
![ASR challanges](session3/Screenshot from 2023-03-09 12-45-14.png){}
![diversity challange](session3/Screenshot from 2023-03-09 12-46-00.png){}
![language is dynamic](session3/Screenshot from 2023-03-09 12-47-26.png){}
![whar's next](session3/Screenshot from 2023-03-09 12-47-57.png){}
![covid understanding challenges](session3/Screenshot from 2023-03-09 12-49-03.png){}
![Non verbal communication 1](session3/Screenshot from 2023-03-09 12-49-21.png){}
![Non verbal communication 2](session3/Screenshot from 2023-03-09 12-49-57.png){}
![DataNights Cohort](session3/Screenshot from 2023-03-09 12-51-10.png){}
![QR for ASR Course](session3/Screenshot from 2023-03-09 12-51-53.png){}
![Questions](session3/Screenshot from 2023-03-09 12-52-15.png){}

So not much to say:
- I've read a couple of books on the subject, but this shows more up to date results.
- Show me the papers?
- The Data Nights course should be be worth taking
