---
title: OLS regression From Scratch
date: 2023-02-01
categories:
    - data science
    - ml
    - algorithms
---

# OLS regression

OLS regression is a method for estimating the parameters of a linear regression model. The goal is to find the line that best fits a set of data points. The line is represented by an equation of the form
$$
y = mx + b
$$

where :

- $y$ is the **dependent variable**, 
- $x$ is the **independent variable**, 
- $m$ is the **slope** of the line, and 
- $b$ is the **y-intercept**.

## Generate random data


```{python}
# Import the necessary libraries
import random
import xarray as xr

# Generate random data
n = 100
x1 = [random.uniform(0, 1) for _ in range(n)]
x2 = [random.uniform(0, 1) for _ in range(n)]
x3 = [random.uniform(0, 1) for _ in range(n)]
y = [2 * x1[i] + 3 * x2[i] + 4 * x3[i] + random.normalvariate(0, 1) for i in range(n)]

# Create an xarray
data = xr.Dataset({
    'x1': ('sample', x1),
    'x2': ('sample', x2),
    'x3': ('sample', x3),
    'y': ('sample', y),
})

# Add attributes to the variables
data.x1.attrs['description'] = 'Independent variable 1'
data.x2.attrs['description'] = 'Independent variable 2'
data.x3.attrs['description'] = 'Independent variable 3'
data.y.attrs['description'] = 'Dependent variable'

# Save the xarray to a CSV file
data.to_dataframe().to_csv('your_dataset.csv')
data.to_netcdf("your_dataset.nc")

# Print the xarray
print(data)
```

##  
```{python}
import numpy as np
import xarray as xr

# Load the data from the CSV file into an xarray dataset
data = xr.open_dataset('your_dataset.nc')

# Get the X and Y variables from the xarray dataset
X = data[['x1', 'x2', 'x3']]
Y = data['y']

# Add a column of ones to X for the intercept term
ones = xr.ones_like(Y)
ones = ones.rename({'y': 'dim_1'})
ones.coords['dim_1'] = Y.coords['sample']
X = xr.merge([X, ones.to_dataset()])

# Estimate the regression parameters using OLS
XTX = X.transpose('sample', 'dim_1').dot(X)
XTY = X.transpose('sample', 'dim_1').dot(Y)
beta = np.linalg.inv(XTX).dot(XTY)

# Print the regression parameters
print(beta)

```

```{python}

y_pred = X.dot(beta)

print(beta)

```

```{python}
def mean_squared_error(y_true:ndarray, y_pred:ndarray):
    n = len(y_true)
    mse = sum([(y_true[i] - y_pred[i])**2 for i in range(n)]) / n
    return mse

def r2_score(y_true:ndarray, y_pred:ndarray):
    ssr = sum([(y_true[i] - y_pred[i])**2 for i in range(len(y_true))])
    sst = sum([(y_true[i] - np.mean(y_true))**2 for i in range(len(y_true))])
    r2 = 1 - (ssr / sst)
    return r2

rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print("RMSE: ", rmse)
print("R-squared: ", r2)

```

```{python}
plt.scatter(X[:, 1], y, color='blue')
plt.plot(X[:, 1], y_pred, color='red')
plt.title('OLS Regression')
plt.xlabel('Independent variable')
plt.ylabel('Dependent variable')
plt.show()
```

why we need to add ones?

                                |   |   |   |   |  |   
                                |   |   |   |   | 1|   <-- Intercept term (constant)
    | Y |   |   |   |   |       | X | X | X | X | 1|   <-- Predictor variables and intercept
    |   |   |   |   |   |       |   |   |   |   |  |
    |   |   |   |   |   |   *   |   |   |   |   |  |   <-- Linear regression equation
    |   |   |   |   |   |       |   |   |   |   |  |


In the diagram above, we have m samples (represented by the rows) and n predictor variables (represented by the columns). The intercept term is added as a constant column of ones to the predictor variables, creating an m x (n+1) matrix of features.

When we perform linear regression, we want to estimate the coefficients for each of the predictor variables (and the intercept term) that best fit the data. We can write the linear regression equation as:

makefile

Y = X * beta

where Y is an m x 1 matrix of response values, X is the m x (n+1) matrix of predictor variables (including the intercept term), and beta is a (n+1) x 1 matrix of coefficients.

By adding a column of ones to X, we ensure that the first element of beta corresponds to the intercept term, and the remaining elements correspond to the coefficients for the predictor variables. Without the intercept term, the regression line would be forced to pass through the origin, which may not be appropriate for all datasets.