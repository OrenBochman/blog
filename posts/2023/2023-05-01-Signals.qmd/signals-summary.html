<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Oren Bochman">
<meta name="dcterms.date" content="2024-05-01">
<meta name="keywords" content="game theory, signaling games, partial pooling, evolution, reinforcement learning, signaling systems, evolution of language">

<title>Oren Bochman’s Blog - Skryms Signals Summary and Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="twitter:title" content="Oren Bochman’s Blog - Skryms Signals Summary and Models">
<meta name="twitter:description" content="learing language games">
<meta name="twitter:image" content="https://orenbochman.github.io/posts/2023/2023-05-01-Signals.qmd/thumbnail_blog.png">
<meta name="twitter:creator" content="@orenbochman">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../images/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Oren Bochman’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-book" role="button" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-book" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-bi-book">    
        <li>
    <a class="dropdown-item" href="../../../notes.html">
 <span class="dropdown-text">All Notes</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../nlp.html">
 <span class="dropdown-text">NLP Specilization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../dnn.html">
 <span class="dropdown-text">Neural Networks for Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../model-thinking.html">
 <span class="dropdown-text">Model Thinking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../xai.html">
 <span class="dropdown-text">XAI</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rl.html">
 <span class="dropdown-text">rl</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../rhetoric.html">
 <span class="dropdown-text">rhetoric</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../tfp.html">
 <span class="dropdown-text">TFP</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../ab-testing.html">
 <span class="dropdown-text">AB testing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../cognitiveai.html">
 <span class="dropdown-text">cognitive AI</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/orenbochman"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-github" role="button" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/OrenBochman/blog">
 <span class="dropdown-text">Source Code</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/OrenBochman/blog/issues">
 <span class="dropdown-text">Report a Bug</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="../../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../archive.html"> <i class="bi bi-archive" role="img">
</i> 
<span class="menu-text">Archive</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Skryms Signals Summary and Models</h1>
            <p class="subtitle lead">learing language games</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Oren Bochman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 1, 2024</p>
      </div>
    </div>
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>game theory, signaling games, partial pooling, evolution, reinforcement learning, signaling systems, evolution of language</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#signals" id="toc-signals" class="nav-link active" data-scroll-target="#signals">1. Signals</a>
  <ul class="collapse">
  <li><a href="#big-research-questions" id="toc-big-research-questions" class="nav-link" data-scroll-target="#big-research-questions">Big Research Questions</a></li>
  </ul></li>
  <li><a href="#sender-receiver" id="toc-sender-receiver" class="nav-link" data-scroll-target="#sender-receiver">Sender-Receiver</a>
  <ul class="collapse">
  <li><a href="#evolution" id="toc-evolution" class="nav-link" data-scroll-target="#evolution">Evolution</a></li>
  </ul></li>
  <li><a href="#evolution-1" id="toc-evolution-1" class="nav-link" data-scroll-target="#evolution-1">Evolution</a></li>
  <li><a href="#desiderata-for-learning-algorithms-of-signaling-systems" id="toc-desiderata-for-learning-algorithms-of-signaling-systems" class="nav-link" data-scroll-target="#desiderata-for-learning-algorithms-of-signaling-systems">Desiderata for learning algorithms of signaling systems</a></li>
  <li><a href="#evolution-2" id="toc-evolution-2" class="nav-link" data-scroll-target="#evolution-2">4 Evolution</a>
  <ul class="collapse">
  <li><a href="#ess" id="toc-ess" class="nav-link" data-scroll-target="#ess">ESS</a></li>
  <li><a href="#ess-motivating-example-hawk-dove-game" id="toc-ess-motivating-example-hawk-dove-game" class="nav-link" data-scroll-target="#ess-motivating-example-hawk-dove-game">ESS Motivating Example Hawk Dove Game</a></li>
  <li><a href="#ess-criteria" id="toc-ess-criteria" class="nav-link" data-scroll-target="#ess-criteria">ESS Criteria</a></li>
  <li><a href="#differential-reproduction---replicator-dynamics" id="toc-differential-reproduction---replicator-dynamics" class="nav-link" data-scroll-target="#differential-reproduction---replicator-dynamics">Differential Reproduction - Replicator dynamics</a></li>
  <li><a href="#langauge-intergration-problem" id="toc-langauge-intergration-problem" class="nav-link" data-scroll-target="#langauge-intergration-problem">Langauge intergration problem:</a>
  <ul class="collapse">
  <li><a href="#problem-definition" id="toc-problem-definition" class="nav-link" data-scroll-target="#problem-definition"><strong>Problem Definition</strong></a></li>
  <li><a href="#solution-approach" id="toc-solution-approach" class="nav-link" data-scroll-target="#solution-approach"><strong>Solution Approach</strong></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#learning" id="toc-learning" class="nav-link" data-scroll-target="#learning">Learning</a>
  <ul class="collapse">
  <li><a href="#rotherev-rl-alg" id="toc-rotherev-rl-alg" class="nav-link" data-scroll-target="#rotherev-rl-alg">Roth–Erev RL alg:</a></li>
  <li><a href="#bushmosteller-rl" id="toc-bushmosteller-rl" class="nav-link" data-scroll-target="#bushmosteller-rl">Bush–Mosteller RL</a></li>
  <li><a href="#goldilocks-rl" id="toc-goldilocks-rl" class="nav-link" data-scroll-target="#goldilocks-rl">Goldilocks RL</a></li>
  <li><a href="#rl-variants" id="toc-rl-variants" class="nav-link" data-scroll-target="#rl-variants">RL variants:</a></li>
  <li><a href="#beyond-the-book" id="toc-beyond-the-book" class="nav-link" data-scroll-target="#beyond-the-book">Beyond the book:</a></li>
  </ul></li>
  <li><a href="#networks-i-logic-and-information-processing" id="toc-networks-i-logic-and-information-processing" class="nav-link" data-scroll-target="#networks-i-logic-and-information-processing">11. Networks I: Logic and Information Processing</a>
  <ul class="collapse">
  <li><a href="#logic" id="toc-logic" class="nav-link" data-scroll-target="#logic">Logic</a></li>
  <li><a href="#information-processing" id="toc-information-processing" class="nav-link" data-scroll-target="#information-processing">Information processing</a>
  <ul class="collapse">
  <li><a href="#inventing-the-code-game" id="toc-inventing-the-code-game" class="nav-link" data-scroll-target="#inventing-the-code-game">Inventing the code Game</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#complex-signals-and-compositionality" id="toc-complex-signals-and-compositionality" class="nav-link" data-scroll-target="#complex-signals-and-compositionality">12. Complex Signals and Compositionality</a>
  <ul class="collapse">
  <li><a href="#some-thoughts" id="toc-some-thoughts" class="nav-link" data-scroll-target="#some-thoughts">Some thoughts</a></li>
  </ul></li>
  <li><a href="#signals-bibliography" id="toc-signals-bibliography" class="nav-link" data-scroll-target="#signals-bibliography">Signals Bibliography</a>
  <ul class="collapse">
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"></a></li>
  <li><a href="#discarded-bibliographical-items" id="toc-discarded-bibliographical-items" class="nav-link" data-scroll-target="#discarded-bibliographical-items">Discarded Bibliographical Items</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>In <span class="citation" data-cites="Skyrms2010signals">(<a href="#ref-Skyrms2010signals" role="doc-biblioref">Skyrms 2010</a>)</span> philosopher and mathematician Brian Skyrms discusses how one can extend the concept of a signaling games into a full fledged signaling systems and to some extent a rudimentary language.</p>
<div class="no-row-height column-margin column-container"></div><p>I like many other found Signals to be a fascinating little book worth reading at least a couple of times. While Skyrms starts with a basic exposition motivated by Greek philosophers he eventually makes a deep dive into areas like reinforcement learning, replicator dynamics, mean field games and some other deep mathematical fields without much of introduction. In places the monographs seems incomplete and may require hunting the papers in the bibliography and possibly more recent work by the same authors.</p>
<p>I slowly noticed it being cited in more and more papers which I read. This sort of indicated that intellectually more people we on the same path of thinking how to equip their problem solving with a signaling system or better yet to evolve a more sophisticated language.<br>
</p>
<p>I went back several times to review the chapter on Complex signals, which I feel is the most interesting for real-world application. I began to think that the Lewis games are too rudimentary since signaling systems that evolve/learned from them are basically n-k maps of signals to meaning.</p>
<p>What I wanted was a recipe for quickly agent that need to evolve and teach/learn a language for efficient communication.</p>
<p>I wanted to go the relevant papers he covers on this area and then to see of there were newer results he did not cover. This turned out to be a bit of a challenge. In the mean time I also learned some courses on RL and even tried a couple of ideas from this book at work. I think I should summarize at least some of the more interesting results from the book.</p>
<p>Besides a summary I also want to try to implement some of the keystone models in the book to see if I can derive the reductionist simple language learning game.</p>
<section id="signals" class="level2">
<h2 class="anchored" data-anchor-id="signals">1. Signals</h2>
<section id="big-research-questions" class="level3">
<h3 class="anchored" data-anchor-id="big-research-questions">Big Research Questions</h3>
<p><strong>Q1. How can interacting individuals spontaneously learn to signal?</strong></p>
<p><strong>Q2. How can species spontaneously evolve signaling systems?</strong></p>
</section>
</section>
<section id="sender-receiver" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sender-receiver">Sender-Receiver</h2>
<blockquote class="blockquote">
<p>There are two players, the sender and the receiver.<br>
Nature chooses a state at random and the sender observes the state chosen.<br>
The sender then sends a signal to the receiver, who cannot observe the state directly but does observe the signal.<br>
The receiver then chooses an act, the outcome of which affects them both, with the payoff depending on the state.<br>
Both have pure common interest—they get the same payoff—and there is exactly one “correct” act for each state.<br>
In the correct act-state combination they both get positive payoff; otherwise payoff is zero.<br>
The simplest case is one where there are the same number of states, acts, and signals.</p>
</blockquote>
<p>A separating equilibrium is called a signaling system</p>
<blockquote class="blockquote">
<p>If we start with a pair of sender and receiver strategies, and switch the messages around the same way in both, we get the same payoffs. In particular, permutation of messages takes one signaling-system equilibrium into another.</p>
</blockquote>
<p>We can understand a signaling system as a encoding look-up table by the sender and a decoding lookup table for the reciever which is the inverse of the first. The product of two permutations is the identity matrix. Each permutation of the identity matrix gives a valid signaling system</p>
<p><strong>Q3. Is there a most salient</strong><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <strong>signaling system?</strong></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Salience is a concept from Schelling’s Game theory that suggest that one solution to a coordination problem might be naturally better then others. (e.g.&nbsp;meeting a relative at the airport). This can be due to an externality to the pure coordination problem. Salience can also arise from non uniformity of the state distribution - by providing less frequent messages longer messages based on binary coding. The salience hierarcy might be grounded in risk - more urgent messages might be shorter and learned before the longer ones.</p></div></div><p><strong>Q4. How can two agents with different signaling find a SS that is midway between them (including systems with both shared and unique states)?</strong></p>
<ul>
<li><p>Its fairly clear that under the rules of the Lewis game all valid signaling systems are isomorphic and none are more salient.</p></li>
<li><p>In nature salience might arise and a systems leading to greatest fitness in its users would be the most salient.</p></li>
</ul>
<p><strong>Information in signals</strong></p>
<p><strong>Q5. How can we minimally extend this framework to handle Errors and Deception</strong></p>
<blockquote class="blockquote">
<p>Signals carry information. The natural way to measure the information in a signal is to measure the extent that the use of that particular signal changes probabilities. Accordingly, there are two kinds of information in the signals in Lewis sender-receiver games: information about what state the sender has observed and information about what act the receiver will take. The ﬁrst kind of infor- mation measures effectiveness of the sender’s use of signals to discriminate states; the second kind measures the effectiveness of the signal in changing the receiver’s probabilities of action.</p>
</blockquote>
<p>[ ] TODO: estimate information content of each signal for sender and receiver for separating and partial pooling cases</p>
<p>[ ] TODO: use entropy for message level estimates of sender and receiver under separating signal, a synonym, a homonym.</p>
<p>[ ] TODO: use entropy KL divergence to estimate a the distance of the signaling distribution from a separating distribution.</p>
<p>Actually there are a number of extentions one would like to consider for the lewis framework:</p>
<ol type="1">
<li>bottlenecks
<ol type="1">
<li>more state than signals</li>
<li>more signals thant states</li>
</ol></li>
<li>basic logical reasoning,</li>
<li>multiple senders and or receevers
<ol type="1">
<li>rewarding coordination (each state requires different actions from the agents - they are learning different receiver maps )</li>
<li>rewarding correlated equilibrium (sender lets the receivers pick from correlated states at random allowing the receivers avoid penalty of miscoordination.)</li>
</ol></li>
</ol>
<p>complex signals</p>
<ol type="1">
<li>conjunction of signals,</li>
<li>ordered signals,</li>
<li>recursive signals, group</li>
</ol>
<section id="evolution" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="evolution">Evolution</h3>
<p>In Banes 1982</p>
<p>We first see two competing Signaling systems being tested in a population</p>
<p><span class="citation" data-cites="hofbauer1998evolutionary">(<a href="#ref-hofbauer1998evolutionary" role="doc-biblioref">Hofbauer and Sigmund 1998</a>)</span> Population dynamics - can be used to identify which dynamic equlibria are stable or unstable given an intial population of strategies</p>
<div class="no-row-height column-margin column-container"></div><p>There is a figure showing the field dynamics with basins of attractions arrising from the population dynamics equations</p>
<p>We also see symmetry breaking selecting a signaling system to a system</p>
<p><span class="math display">
\frac{dp(A)}{dt}=p(A)[U(A)-U]
</span></p>
<p>where</p>
<ul>
<li>U(A) is the average payoff to strategy A and</li>
<li>U is the average payoff in the population.</li>
</ul>
<div id="e0ae8ceb" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pylab <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>alpha, beta <span class="op">=</span> <span class="dv">1</span>, <span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>xvalues, yvalues <span class="op">=</span> meshgrid(arange(<span class="dv">0</span>, <span class="fl">2.1</span>, <span class="fl">0.1</span>), arange(<span class="dv">0</span>, <span class="fl">2.1</span>, <span class="fl">0.1</span>))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>xdot <span class="op">=</span> xvalues <span class="op">*</span> alpha <span class="op">-</span> beta</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>ydot <span class="op">=</span> yvalues <span class="op">*</span> alpha <span class="op">-</span> beta</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>streamplot(xvalues, yvalues, xdot, ydot)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="signals-summary_files/figure-html/cell-2-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="signals-summary_files/figure-html/cell-2-output-1.png" width="594" height="416" class="figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>we have a discussion of how signals might arise.</p>
</section>
</section>
<section id="evolution-1" class="level2">
<h2 class="anchored" data-anchor-id="evolution-1">Evolution</h2>
<div id="02513653" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mesa <span class="im">import</span> Agent, Model</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mesa.time <span class="im">import</span> StagedActivation</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LewisAgent(Agent):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.message <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action<span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> send(<span class="va">self</span>):</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> recive(<span class="va">self</span>):</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> calc_reward(<span class="va">self</span>):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_reward(<span class="va">self</span>):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reward <span class="op">=</span> model.reward</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Placeholder for learning logic</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Agent </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> received reward: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>reward<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Sender(LewisAgent):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model):</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> send(<span class="va">self</span>):</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> <span class="va">self</span>.model.get_state()</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Learning to map states to signals</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.message <span class="op">=</span> <span class="va">self</span>.model.states_signals[state]</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Sender </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> sends signal for state </span><span class="sc">{</span>state<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>message<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Receiver(LewisAgent):</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model):</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> recive(<span class="va">self</span>):</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.received_signals<span class="op">=</span>[]</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> sender <span class="kw">in</span> <span class="va">self</span>.model.senders:</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.received_signals.append(sender.message)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Learning to map signals to actions</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.received_signals)<span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action <span class="op">=</span> <span class="va">self</span>.model.signals_actions[<span class="va">self</span>.received_signals[<span class="dv">0</span>]]</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span>:</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action <span class="op">=</span> <span class="va">self</span>.model.signals_actions[<span class="va">self</span>.received_signals[<span class="dv">0</span>]]</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> calc_reward(<span class="va">self</span>):</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>      action <span class="op">=</span> <span class="va">self</span>.model.signals_actions[<span class="va">self</span>.received_signals[<span class="dv">0</span>]]</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>      correct_action <span class="op">=</span> <span class="va">self</span>.model.states_actions[<span class="va">self</span>.model.current_state]</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>      reward <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> action <span class="op">==</span> correct_action <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>      model.reward <span class="op">=</span> reward</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SignalingGame(Model):</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, senders_count<span class="op">=</span><span class="dv">1</span>, recievers_count<span class="op">=</span><span class="dv">1</span>, k<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.senders_count<span class="op">=</span>senders_count</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.recievers_count<span class="op">=</span>recievers_count</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_agents <span class="op">=</span> <span class="va">self</span>.recievers_count<span class="op">+</span><span class="va">self</span>.senders_count</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g., 0 -&gt; A, 1 -&gt; B, ...</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.states_signals <span class="op">=</span> {i: <span class="bu">chr</span>(<span class="dv">65</span> <span class="op">+</span> i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)} </span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g., A -&gt; 0, B -&gt; 1, ...</span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.signals_actions <span class="op">=</span> {<span class="bu">chr</span>(<span class="dv">65</span> <span class="op">+</span> i): i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)}</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># state 0 needs action 0, state 1 needs action 1, ...</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.states_actions <span class="op">=</span> {i: i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)}  </span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create agents</span></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.senders <span class="op">=</span> []</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.receivers<span class="op">=</span>[]</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.my_agents<span class="op">=</span>[]</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.uid<span class="op">=</span><span class="dv">0</span></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.senders_count):</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>            sender <span class="op">=</span> Sender(<span class="va">self</span>.uid, <span class="va">self</span>)</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.senders.append(sender)</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.my_agents.append(sender)</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.uid <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span> (<span class="va">self</span>.recievers_count):</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>            reciever <span class="op">=</span> Receiver(<span class="va">self</span>.uid, <span class="va">self</span>)</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.receivers.append(reciever)</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.my_agents.append(reciever)</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.uid <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.schedule <span class="op">=</span> StagedActivation(</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>          model<span class="op">=</span><span class="va">self</span>,</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>          agents<span class="op">=</span><span class="va">self</span>.my_agents, </span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>          stage_list <span class="op">=</span> [<span class="st">'send'</span>,<span class="st">'recive'</span>,<span class="st">'calc_reward'</span>,<span class="st">'set_reward'</span>]</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_state(<span class="va">self</span>):</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.current_state</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_state <span class="op">=</span> random.choice(<span class="bu">list</span>(<span class="va">self</span>.states_signals.keys()))</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"New state of the world: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>current_state<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.schedule.step()</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a><span class="co"># Running the model</span></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Number of states, signals, and actions</span></span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SignalingGame(senders_count<span class="op">=</span><span class="dv">1</span>,recievers_count<span class="op">=</span><span class="dv">1</span>,k<span class="op">=</span>k)</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"--- Step </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>    model.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--- Step 1 ---
New state of the world: 1
Sender 0 sends signal for state 1: B
Agent 0 received reward: 1
Agent 1 received reward: 1
--- Step 2 ---
New state of the world: 0
Sender 0 sends signal for state 0: A
Agent 0 received reward: 1
Agent 1 received reward: 1
--- Step 3 ---
New state of the world: 2
Sender 0 sends signal for state 2: C
Agent 0 received reward: 1
Agent 1 received reward: 1
--- Step 4 ---
New state of the world: 2
Sender 0 sends signal for state 2: C
Agent 0 received reward: 1
Agent 1 received reward: 1
--- Step 5 ---
New state of the world: 1
Sender 0 sends signal for state 1: B
Agent 0 received reward: 1
Agent 1 received reward: 1
--- Step 6 ---
New state of the world: 2
Sender 0 sends signal for state 2: C
Agent 0 received reward: 1
Agent 1 received reward: 1
--- Step 7 ---
New state of the world: 1
Sender 0 sends signal for state 1: B
Agent 0 received reward: 1
Agent 1 received reward: 1
--- Step 8 ---
New state of the world: 2
Sender 0 sends signal for state 2: C
Agent 0 received reward: 1
Agent 1 received reward: 1
--- Step 9 ---
New state of the world: 2
Sender 0 sends signal for state 2: C
Agent 0 received reward: 1
Agent 1 received reward: 1
--- Step 10 ---
New state of the world: 0
Sender 0 sends signal for state 0: A
Agent 0 received reward: 1
Agent 1 received reward: 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/oren/work/blog/env/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:

The AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.
We would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919
</code></pre>
</div>
</div>
<p>In this simulation the agents are not learning - they are accessing the predefined signals and actions in the model hence rewards are always 1.</p>
<p>Player in Lewis signaling games can reach three type of equilibria</p>
<ol type="1">
<li>Separating equilibrium in which receiver fully recovers the state from the signal and can take the appropriate action</li>
<li>Partial pooling equilibrium in which <em>synonyms</em> or <em>homophones</em> frustrate the receiver for always recovering the state.</li>
<li>Full pooling equilibrium in which all signals are the same and the agents are unable to communicate.</li>
</ol>
<p>A one word synonym for “desired qualities” derived from desire that used in academic literature is “desiderata”.</p>
<p>Skryms next considers bottle necks - which are cases where there are more signals than actions and vica versa.</p>
<ul>
<li>In the case of more signals than actions successful learning will result a partial polling equilibrium with some synonyms.</li>
<li>In the case of more actions than signals the best an agent can learn is a partial pooling equilibrium with homophones.</li>
</ul>
<p>Both synonyms and homophones have drawbacks however:</p>
<p>While synonyms increase the cognitive load and the number of signals that need to be learned they do not prevent the recovery of the state being communicated. Homophones require the receiver to select an interpretation at random leading to lower payoffs since the receiver unable to recover the state cannot select the correct action. If the number of signal is the same as the number of actions, the pigeon hole principle guarantees that for every synonym there must be a homophone.</p>
<p>If we consider that for recoverability we need action and signals to be fully correlated it is easy to see that each failure to correlate</p>
<p>action to signals results in a (partial) pooling solution. Thus there are far more partial pooling equilibria than separating equilibria. and it is thus no surprise that natural language is rife with homophones and synonyms.</p>
<p>In lieu of the fact that partial pooling equilibrium far out number the separating ones with and with out bottlenecks, setting up and later learning a separating signaling system with minimal homophones/synonyms is not trivial task. (If we also factor in cost/risk of miscommunication some homophones are clearly worse than others)</p>
<ul>
<li><p>Evolution for example may not be the best way for this.</p></li>
<li><p>While researchers have very basic algorithms to do so, in terms of convergence rate and sample efficiency.</p></li>
</ul>
<p>Although not considered it is easy to see that there are far more partial pooling</p>
<p>We can conclude proceed to discuss the desiderata for learning algorithms.</p>
<p>Note: Dropout Algorithm Introducing bottlenecks into neural networks tend to improve their ability to generalize by forcing them to avoid memorizing inputs and come up with more resiliant representations. This suggest that partial pooling equilibria may play a more significant role in structured/complex signaling systems.</p>
</section>
<section id="desiderata-for-learning-algorithms-of-signaling-systems" class="level2">
<h2 class="anchored" data-anchor-id="desiderata-for-learning-algorithms-of-signaling-systems">Desiderata for learning algorithms of signaling systems</h2>
<ol type="1">
<li>State recovery - we prefer the algorithm to learn a separating equilibrium and if avoid pooling equilibrium with homophones.</li>
<li>Convergence - we want the algorithm to quickly converge to the equilibrium.</li>
<li>Sample efficiency - we want the algorithm to learn after minimal exposure to stimuli.</li>
</ol>
<p>Some questions</p>
<ul>
<li><p>How different are the task of creating the signaling system from learning it?</p>
<ul>
<li><p>the main difference perhaps is that one party has a mapping and it is up to the second to learn it. they can’t find unused symbols and mach them to a new state.</p></li>
<li><p>there may be many speakers so making changes will be costly.</p></li>
</ul></li>
<li><p>Can switching roles of sender and receiver give better outcomes in learning ?</p>
<ul>
<li>this may change for different extensions</li>
</ul></li>
<li><p>If there are multiple agent learning can create or learn the signaling system better or faster</p>
<ul>
<li><p>what if they have groups with established signal systems</p></li>
<li><p>how can they find a new set of mapping with minimal permutation from their original</p></li>
</ul></li>
<li><p>If states used for reward are not random are there better schedules for learning are not random</p></li>
</ul>
<p>What if each has knowledge of a working signaling system already help adding more players seem to</p>
</section>
<section id="evolution-2" class="level1 page-columns page-full">
<h1>4 Evolution</h1>
<p>The three essential factors in Darwin’s account are</p>
<ol type="1">
<li>natural variation - mutation, gene flow via migration, genetic drift and recombination in sexual reproduction.</li>
<li>differential reproduction - <span class="citation" data-cites="Taylor1978ESS">(<a href="#ref-Taylor1978ESS" role="doc-biblioref">Taylor and Jonker 1978</a>)</span> replicator dynamics</li>
<li>inheritance</li>
</ol>
<div class="no-row-height column-margin column-container"></div><section id="ess" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="ess">ESS</h3>
<p>In <span class="citation" data-cites="Smith1973LogicAnimalConflict">(<a href="#ref-Smith1973LogicAnimalConflict" role="doc-biblioref">Smith and Price 1973</a>)</span> the authors introduced a novel solution concept - the ESS or Evolutionary stable strategy, improving on the notion of the Nash equilibrium by replacing agent level play dominance with statistical dominance of strategies.</p>
<div class="no-row-height column-margin column-container"></div></section>
<div id="ex-ess-hak-dove">
<section id="ess-motivating-example-hawk-dove-game" class="level2">
<h2 class="anchored" data-anchor-id="ess-motivating-example-hawk-dove-game">ESS Motivating Example Hawk Dove Game</h2>
<table class="table">
<caption>Hawk Dove Game</caption>
<thead>
<tr class="header">
<th></th>
<th>Hawk</th>
<th>Dove</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Hawk</strong></td>
<td>0</td>
<td>3</td>
</tr>
<tr class="even">
<td><strong>Dove</strong></td>
<td>1</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>This explains why hyper-aggressive Hawks type who can defeat more peaceful Doves type do not wipe them out. Hawks have an advantage if there are mostly doves. Once they are in a majority Hawk-Hawk interaction lead to serious injury and death. ESS is a frequency dependent equilibrium.</p>
</section>
</div>
<section id="ess-criteria" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ess-criteria">ESS Criteria</h2>
<p>In <span class="citation" data-cites="Smith1973LogicAnimalConflict">(<a href="#ref-Smith1973LogicAnimalConflict" role="doc-biblioref">Smith and Price 1973</a>)</span> the authors introduce the following criteria in terms of payoffs for a strategy to be an ESS.</p>
<div class="no-row-height column-margin column-container"></div><p>A strategy, S, is evolutionary stable if for any other strategy, M, either:</p>
<ol type="1">
<li>Fitness (S played against S) &gt; Fitness (M played against S) or:</li>
<li>Fitnesses are equal against S, but Fitness(S against M) &gt; Fitness(M against M)</li>
</ol>
<p>Where under the first mutants are expelled quickly and under 2 less so.</p>
</section>
<section id="differential-reproduction---replicator-dynamics" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="differential-reproduction---replicator-dynamics">Differential Reproduction - Replicator dynamics</h2>
<p>Replicator dynamics is driven by Darwinian ﬁtness—expected number of progeny.</p>
<p>so <span class="math inline">fitness \sim \mathbb E(|progeny|)</span> where on average you get what you expect. For strategy <span class="math inline">S</span> the population</p>
<p><span class="math display">
x_{t}(S) = \frac{x_{t-1}(S) \times fitness(S)}{mean\_fitness}
</span></p>
<p>and for continuous time<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;I think that we should consider a lewis hirarcy of games based on lewis games with<br>
a. logic<br>
b. conjuctive signals</p></div></div><p><span class="math display">
\frac{dx}{dt} = x (fitness(S) - {mean\_fitness})
</span></p>
<p>The main outcomes of this chapter are that for a two state/signal/action Lewis game</p>
<ol type="1">
<li>Multiple isomorphic signaling systems we could call languages will arise leading to a population of agents split equaly</li>
<li>In a population of agents whose fitness depends on use of the language the stable state is one in which just one of the language is used by the entire population. Other equilibria are unstable which leads to spontaneous breaking of the symmetry and a gradual drift of the population towards one of the stable states.</li>
</ol>
<p>Notes:</p>
<ol type="1">
<li>The analysis fails to consider spatial dynamics. It seems that a in a local pockets of language 1, agents with language 2 might have lower fitness.</li>
<li>There is a cost of switching and agents typicaly are not born with a fully formed language ability they need to learn a language and that has costs and requires access to signalers with the said language.</li>
<li>In reality <em>Pidgeons</em> and <em>Creoles</em> are often formed. This is a language that is a mix of two or more languages. This is a partial pooling equilibrium. The existence of creoles suggest that the population dynamics of language formation is more complex than the simple Lewis game.</li>
</ol>
</section>
<section id="langauge-intergration-problem" class="level2">
<h2 class="anchored" data-anchor-id="langauge-intergration-problem">Langauge intergration problem:</h2>
<section id="problem-definition" class="level3">
<h3 class="anchored" data-anchor-id="problem-definition"><strong>Problem Definition</strong></h3>
<p>Given a set of signaling systems <span class="math inline">{\pi_1,\pi_2,\ldots,\pi_𝑛}</span>, find a permutation <span class="math inline">\pi_m</span> such that:</p>
<p><span class="math display">
\pi_m =\arg \min_\pi \sum_{𝑖=1}^𝑛 d(\pi,\pi_i)
</span> where d is the Cayley distance between permutations, i.e.&nbsp;the minimum number of transpositions required to transform one permutation into another.</p>
</section>
<section id="solution-approach" class="level3">
<h3 class="anchored" data-anchor-id="solution-approach"><strong>Solution Approach</strong></h3>
<p>Finding the exact median permutation is a computationally challenging task because the problem is NP-hard. However, there are heuristic and approximation methods to approach this problem. One common approach is to use a greedy algorithm that iteratively improves a candidate solution based on the distances to all permutations in the set.</p>
<p>Here is a simple heuristic approach to estimate a solution:</p>
<ol type="1">
<li><p><strong>Start with an Initial Guess</strong>: You can start with any permutation, such as 𝜋1π1​ or any permutation randomly chosen from the set.</p></li>
<li><p><strong>Iterative Improvement</strong>:</p>
<ul>
<li>For each element in the permutation, consider swapping it with every other element.</li>
<li>Calculate the new total distance after each possible swap.</li>
<li>If a swap results in a lower total distance, make the swap permanent.</li>
<li>Repeat this process until no improving swaps are found.</li>
</ul></li>
</ol>
<p>This approach doesn’t guarantee an optimal solution but can often produce a good approximation in a reasonable time frame.</p>
<p>Here’s a Python function that demonstrates this basic heuristic:</p>
<div id="b77ff9b4" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cayley_distance(pi, sigma):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Calculate the Cayley distance between two permutations."""</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    temp <span class="op">=</span> <span class="bu">list</span>(pi)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(pi)):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> temp[i] <span class="op">!=</span> sigma[i]:</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            swap_index <span class="op">=</span> temp.index(sigma[i])</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            temp[i], temp[swap_index] <span class="op">=</span> temp[swap_index], temp[i]</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> count</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> median_permutation(permutations):</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(permutations[<span class="dv">0</span>])  <span class="co"># Assuming all permutations are of the same length</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> <span class="bu">list</span>(permutations[<span class="dv">0</span>])  <span class="co"># Start with the first permutation as an initial guess</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    improving <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> improving:</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        improving <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        best_distance <span class="op">=</span> <span class="bu">sum</span>(cayley_distance(current, p) <span class="cf">for</span> p <span class="kw">in</span> permutations)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, j <span class="kw">in</span> itertools.combinations(<span class="bu">range</span>(n), <span class="dv">2</span>):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            current[i], current[j] <span class="op">=</span> current[j], current[i]  <span class="co"># Swap elements</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            new_distance <span class="op">=</span> <span class="bu">sum</span>(cayley_distance(current, p) <span class="cf">for</span> p <span class="kw">in</span> permutations)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> new_distance <span class="op">&lt;</span> best_distance:</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>                best_distance <span class="op">=</span> new_distance</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>                improving <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>                current[i], current[j] <span class="op">=</span> current[j], current[i]  <span class="co"># Swap back if no improvement</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> current</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>permutations <span class="op">=</span> [</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>],</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">3</span>],</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>],</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>]</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Median permutation:"</span>, median_permutation(permutations))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Median permutation: [1, 2, 3, 4]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_199748/3341496467.py:21: DeprecationWarning:

Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.

/tmp/ipykernel_199748/3341496467.py:24: DeprecationWarning:

Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.
</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="learning" class="level1 page-columns page-full">
<h1>Learning</h1>
<p>Two type of learning are considered.</p>
<ol type="1">
<li><p>Evolution learning using knowledge hard-coded into the genome of the agents. Learning happens though replicator dynamics incorporating randomization followed by natural selection. Also other biologically inspired ideas like mutation and use of a fitness function can come into play.</p>
<p>The down side of Evolution is that is takes many generation for many structures to emerge. (Richard Dawkings states that the evolution of different morphology of the eye are quick taking only 80 generation to evolve in a simulation from the most rudimentary light sensitive cell and elsewhere suggest that 8 generations are needed to see changes in this type of framework.</p></li>
<li><p>RL refers to the type of learning from experience by an organism during its lifetime.</p></li>
<li><p>Noam Chomsky and others Linguistics hypothesize that Language learning faculties are to a large extent passed through evolution and for this reason individuals can learn languages based on a rather minimal amount of stimulus. This has also be a reason why many in their field abandoned their work on solving linguistics and went on to research the mysteries of the human brain. I feel that to a large extent this book demonstrates that scientifically the notion of the brain requiring a specialized mechanism to evolve/learn complex language is an unnecessary assumption. (Of course it is possible that the brain has co-evolved together with language and that such mechanism do exist.)</p>
<ol type="1">
<li><p>in one sense the book starts with very simple systems of communication with just a lexicon.</p></li>
<li><p>The formation of more complex systems with syntax are treated in chapter 12 but these results here seem to satisfy a mathematician or a philosopher etc, without delving into different linguistic niceties that might satisfy a linguist.</p></li>
<li><p>However the Lewis game needs only a small tweak (the receiver getting multiple partial signals) to allow a signaling system with a grammar to emmerge via Roth-Erev RL. We can also make a categorical statement that this type of RL is a general purpose learning mechanism not a language specific one.</p></li>
</ol></li>
</ol>
<p>In agents we have learning that is based on evolution and requires subsequent generations of agents becoming fitter.</p>
<p>Here are two conceptual ideas to base RL on</p>
<dl>
<dt>Law of effect</dt>
<dd>
<p>Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more ﬁrmly connected with the situation, so that, when it recurs, they will be more likely to recur. — Edward Thorndike, Animal Intelligence, 1911</p>
</dd>
<dt>Law of practice</dt>
<dd>
<p>Learning slows down as reinforcements accrue</p>
</dd>
</dl>
<section id="rotherev-rl-alg" class="level2">
<h2 class="anchored" data-anchor-id="rotherev-rl-alg">Roth–Erev RL alg:</h2>
<ol type="1">
<li>set starting weight for each option</li>
<li>weights evolve by addition of rewards gotten</li>
<li>probability of choosing an alternative is proportional to its weight.</li>
</ol>
<div id="9884ab6f" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mesa <span class="im">import</span> Agent, Model</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mesa.time <span class="im">import</span> StagedActivation</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LearningRule:</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, options, learning_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> {option: <span class="fl">1.0</span> <span class="cf">for</span> option <span class="kw">in</span> options}  <span class="co"># Start with equal weights for all options</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_weights(<span class="va">self</span>, option, reward):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the weight of the chosen option by adding the reward scaled by the learning rate</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        old_weight <span class="op">=</span> <span class="va">self</span>.weights[option]</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights[option] <span class="op">+=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> reward</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Updated weight for option </span><span class="sc">{</span>option<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>old_weight<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>weights[option]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_option(<span class="va">self</span>):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Select an option based on the weighted probabilities</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> <span class="bu">sum</span>(<span class="va">self</span>.weights.values())</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        probabilities <span class="op">=</span> [<span class="va">self</span>.weights[opt] <span class="op">/</span> total <span class="cf">for</span> opt <span class="kw">in</span> <span class="va">self</span>.weights]</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.choice(<span class="bu">list</span>(<span class="va">self</span>.weights.keys()), p<span class="op">=</span>probabilities)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LewisAgent(Agent):</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model, learning_options):</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.message <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rule <span class="op">=</span> LearningRule(learning_options, learning_rate<span class="op">=</span><span class="fl">0.1</span>)  <span class="co"># Initialize learning with given options</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_reward(<span class="va">self</span>):</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Agent </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> received reward: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>reward<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Sender(LewisAgent):</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> send(<span class="va">self</span>):</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> <span class="va">self</span>.model.get_state()</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.message <span class="op">=</span> <span class="va">self</span>.learning_rule.choose_option()  <span class="co"># Send a signal based on the learned weights</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Sender </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> sends signal for state </span><span class="sc">{</span>state<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>message<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_learning(<span class="va">self</span>):</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rule.update_weights(<span class="va">self</span>.model.current_state, <span class="va">self</span>.reward)  <span class="co"># Update weights based on the state and received reward</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Receiver(LewisAgent):</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> receive(<span class="va">self</span>):</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.received_signals <span class="op">=</span> [sender.message <span class="cf">for</span> sender <span class="kw">in</span> <span class="va">self</span>.model.senders]</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.received_signals:</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.action <span class="op">=</span> <span class="va">self</span>.learning_rule.choose_option()  <span class="co"># Choose an action based on received signals and learned weights</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> calc_reward(<span class="va">self</span>):</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        correct_action <span class="op">=</span> <span class="va">self</span>.model.states_actions[<span class="va">self</span>.model.current_state]</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reward <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> <span class="va">self</span>.action <span class="op">==</span> correct_action <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Receiver </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> calculated reward: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>reward<span class="sc">}</span><span class="ss"> for action </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>action<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_learning(<span class="va">self</span>):</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> signal <span class="kw">in</span> <span class="va">self</span>.received_signals:</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.learning_rule.update_weights(signal, <span class="va">self</span>.reward)  <span class="co"># Update weights based on signals and rewards</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SignalingGame(Model):</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, senders_count<span class="op">=</span><span class="dv">1</span>, receivers_count<span class="op">=</span><span class="dv">1</span>, k<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> k</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the states, signals, and actions mapping</span></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.states_signals <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(k))  <span class="co"># States are simply numbers</span></span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.signals_actions <span class="op">=</span> <span class="bu">list</span>(<span class="bu">chr</span>(<span class="dv">65</span> <span class="op">+</span> i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k))  <span class="co"># Signals are characters</span></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.states_actions <span class="op">=</span> {i: i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)}  <span class="co"># Mapping states to correct actions</span></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.senders <span class="op">=</span> [Sender(i, <span class="va">self</span>, <span class="va">self</span>.signals_actions) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(senders_count)]</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.receivers <span class="op">=</span> [Receiver(i <span class="op">+</span> senders_count, <span class="va">self</span>, <span class="va">self</span>.signals_actions) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(receivers_count)]</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.schedule <span class="op">=</span> StagedActivation(<span class="va">self</span>, stage_list<span class="op">=</span>[<span class="st">'send'</span>, <span class="st">'receive'</span>, <span class="st">'calc_reward'</span>, <span class="st">'set_reward'</span>, <span class="st">'update_learning'</span>])</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_state(<span class="va">self</span>):</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random.choice(<span class="va">self</span>.states_signals)</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_state <span class="op">=</span> <span class="va">self</span>.get_state()</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"New state of the world: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>current_state<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.schedule.step()</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Running the model</span></span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SignalingGame(senders_count<span class="op">=</span><span class="dv">1</span>, receivers_count<span class="op">=</span><span class="dv">1</span>, k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"--- Step </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>    model.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--- Step 1 ---
New state of the world: 1
--- Step 2 ---
New state of the world: 0
--- Step 3 ---
New state of the world: 1
--- Step 4 ---
New state of the world: 2
--- Step 5 ---
New state of the world: 2
--- Step 6 ---
New state of the world: 1
--- Step 7 ---
New state of the world: 0
--- Step 8 ---
New state of the world: 0
--- Step 9 ---
New state of the world: 1
--- Step 10 ---
New state of the world: 2</code></pre>
</div>
</div>
</section>
<section id="bushmosteller-rl" class="level2">
<h2 class="anchored" data-anchor-id="bushmosteller-rl">Bush–Mosteller RL</h2>
<ol type="1">
<li><p>If an act is chosen and a reward is gotten the probability is incremented by adding some fraction of the distance between the original probability and probability one</p>
<p><span class="math display">
pr_{new}(A)=(1-\alpha)pr_{old}(A) + a(1)
</span></p></li>
<li><p>Alternative action probabilities are decremented so that everything adds to one</p></li>
</ol>
</section>
<section id="goldilocks-rl" class="level2">
<h2 class="anchored" data-anchor-id="goldilocks-rl">Goldilocks RL</h2>
<p>We consider if there is a Goldilocks point in the RL exploration exploitation dilemma which has a good balance of the two.</p>
<ul>
<li><p>If we stop learning too fast we are <strong>too cold</strong></p></li>
<li><p>If we exploring too much we are <strong>too hot</strong></p></li>
<li><p>At the limit is the Goldilocks RL point</p></li>
</ul>
<p><strong>Q: is there Goldilocks RL Alg?</strong></p>
<ul>
<li><p>Roth—Erev, Thompson sampling &amp; UCB don’t get stuck</p></li>
<li><p>Epsilon greedy is too hot</p></li>
<li><p>Bush–Mosteller is too cold</p></li>
</ul>
</section>
<section id="rl-variants" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="rl-variants">RL variants:</h2>
<ul>
<li><p>BM variants like dynamically adjusting aspiration levels</p></li>
<li><p>exponential response rule. The basic idea is to make probabilities proportional to the exponential of past reinforcements. <span class="citation" data-cites="Blume2002">(<a href="#ref-Blume2002" role="doc-biblioref">Blume et al. 2002</a>)</span></p></li>
<li><p>best response dynamics, aka Cournot dynamics</p></li>
</ul>
<div class="no-row-height column-margin column-container"></div></section>
<section id="beyond-the-book" class="level2">
<h2 class="anchored" data-anchor-id="beyond-the-book">Beyond the book:</h2>
<ul>
<li>^[citation needed ]^ investigating RL for this task also suggest that Roth-Erev with forgetting leads to more efficient learning.</li>
<li>^[citation needed]^ Another paper suggest that a learning with a certain prior can be better than Roth-Erev learning.</li>
</ul>
<p>Adding Learning</p>
<div id="36690905" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mesa <span class="im">import</span> Agent, Model</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mesa.time <span class="im">import</span> StagedActivation</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LewisAgent(Agent):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model):</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.message <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action<span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> send(<span class="va">self</span>):</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> recive(<span class="va">self</span>):</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> calc_reward(<span class="va">self</span>):</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_reward(<span class="va">self</span>):</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reward <span class="op">=</span> model.reward</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Placeholder for learning logic</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Agent </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> received reward: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>reward<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Sender(LewisAgent):</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model):</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> send(<span class="va">self</span>):</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> <span class="va">self</span>.model.get_state()</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Learning to map states to signals</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.message <span class="op">=</span> <span class="va">self</span>.model.states_signals[state]</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Sender </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> sends signal for state </span><span class="sc">{</span>state<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>message<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Receiver(LewisAgent):</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model):</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> recive(<span class="va">self</span>):</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.received_signals<span class="op">=</span>[]</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> sender <span class="kw">in</span> <span class="va">self</span>.model.senders:</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.received_signals.append(sender.message)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Learning to map signals to actions</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.received_signals)<span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action <span class="op">=</span> <span class="va">self</span>.model.signals_actions[<span class="va">self</span>.received_signals[<span class="dv">0</span>]]</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span>:</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action <span class="op">=</span> <span class="va">self</span>.model.signals_actions[<span class="va">self</span>.received_signals[<span class="dv">0</span>]]</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> calc_reward(<span class="va">self</span>):</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>      action <span class="op">=</span> <span class="va">self</span>.model.signals_actions[<span class="va">self</span>.received_signals[<span class="dv">0</span>]]</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>      correct_action <span class="op">=</span> <span class="va">self</span>.model.states_actions[<span class="va">self</span>.model.current_state]</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>      reward <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> action <span class="op">==</span> correct_action <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>      model.reward <span class="op">=</span> reward</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SignalingGame(Model):</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, senders_count<span class="op">=</span><span class="dv">1</span>, recievers_count<span class="op">=</span><span class="dv">1</span>, k<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.senders_count<span class="op">=</span>senders_count</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.recievers_count<span class="op">=</span>recievers_count</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_agents <span class="op">=</span> <span class="va">self</span>.recievers_count<span class="op">+</span><span class="va">self</span>.senders_count</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g., 0 -&gt; A, 1 -&gt; B, ...</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.states_signals <span class="op">=</span> {i: <span class="bu">chr</span>(<span class="dv">65</span> <span class="op">+</span> i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)} </span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g., A -&gt; 0, B -&gt; 1, ...</span></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.signals_actions <span class="op">=</span> {<span class="bu">chr</span>(<span class="dv">65</span> <span class="op">+</span> i): i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)}</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>        <span class="co"># state 0 needs action 0, state 1 needs action 1, ...</span></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.states_actions <span class="op">=</span> {i: i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)}  </span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create agents</span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.senders <span class="op">=</span> []</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.receivers<span class="op">=</span>[]</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.my_agents<span class="op">=</span>[]</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.uid<span class="op">=</span><span class="dv">0</span></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.senders_count):</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>            sender <span class="op">=</span> Sender(<span class="va">self</span>.uid, <span class="va">self</span>)</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.senders.append(sender)</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.my_agents.append(sender)</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.uid <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span> (<span class="va">self</span>.recievers_count):</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>            reciever <span class="op">=</span> Receiver(<span class="va">self</span>.uid, <span class="va">self</span>)</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.receivers.append(reciever)</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.my_agents.append(reciever)</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.uid <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.schedule <span class="op">=</span> StagedActivation(</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>          model<span class="op">=</span><span class="va">self</span>,</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>          agents<span class="op">=</span><span class="va">self</span>.my_agents, </span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>          stage_list <span class="op">=</span> [<span class="st">'send'</span>,<span class="st">'recive'</span>,<span class="st">'calc_reward'</span>,<span class="st">'set_reward'</span>]</span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_state(<span class="va">self</span>):</span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.current_state</span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_state <span class="op">=</span> random.choice(<span class="bu">list</span>(<span class="va">self</span>.states_signals.keys()))</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"New state of the world: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>current_state<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.schedule.step()</span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a><span class="co"># Running the model</span></span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Number of states, signals, and actions</span></span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SignalingGame(senders_count<span class="op">=</span><span class="dv">2</span>,recievers_count<span class="op">=</span><span class="dv">1</span>,k<span class="op">=</span>k)</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"--- Step </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a>    model.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--- Step 1 ---
New state of the world: 2
Sender 0 sends signal for state 2: C
Sender 1 sends signal for state 2: C
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1
--- Step 2 ---
New state of the world: 1
Sender 0 sends signal for state 1: B
Sender 1 sends signal for state 1: B
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1
--- Step 3 ---
New state of the world: 1
Sender 0 sends signal for state 1: B
Sender 1 sends signal for state 1: B
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1
--- Step 4 ---
New state of the world: 0
Sender 0 sends signal for state 0: A
Sender 1 sends signal for state 0: A
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1
--- Step 5 ---
New state of the world: 0
Sender 0 sends signal for state 0: A
Sender 1 sends signal for state 0: A
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1
--- Step 6 ---
New state of the world: 2
Sender 0 sends signal for state 2: C
Sender 1 sends signal for state 2: C
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1
--- Step 7 ---
New state of the world: 1
Sender 0 sends signal for state 1: B
Sender 1 sends signal for state 1: B
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1
--- Step 8 ---
New state of the world: 1
Sender 0 sends signal for state 1: B
Sender 1 sends signal for state 1: B
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1
--- Step 9 ---
New state of the world: 2
Sender 0 sends signal for state 2: C
Sender 1 sends signal for state 2: C
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1
--- Step 10 ---
New state of the world: 2
Sender 0 sends signal for state 2: C
Sender 1 sends signal for state 2: C
Agent 0 received reward: 1
Agent 1 received reward: 1
Agent 2 received reward: 1</code></pre>
</div>
</div>
</section>
</section>
<section id="networks-i-logic-and-information-processing" class="level1 page-columns page-full">
<h1>11. Networks I: Logic and Information Processing</h1>
<section id="logic" class="level2">
<h2 class="anchored" data-anchor-id="logic">Logic</h2>
</section>
<section id="information-processing" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="information-processing">Information processing</h2>
<section id="inventing-the-code-game" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="inventing-the-code-game">Inventing the code Game</h3>
<p>The world has say four states {S1…S4}. In this extended Lewis game where an agent is a receiver of two messages, each with a partial specification the first is {s1||s2} or {s3||s4} and the second {s1||s3} or {s2||s4}. The agent needs to process the two messages it to get the full state specification and take the appropriate action in response for getting a reward !</p>
<p>The added problem here is that the messages one of two flags, and one of two other flags do not have an established system for the message so learning the content of the signals needs to evolve together with the inference.</p>
<p>The sender can be two agents or one agent with a complex message.</p>
<p>Jeffrey Barrett in Barrett 2007a, 2007b. showed that this can be learned with Roth Erev RL</p>
<p>this is more interesting if there are errors:</p>
<ul>
<li><p>is a 10% chance of senders making mistakes with only 3% errors by the receiver?! Skyrms explains this due to the inference being like a taking a vote in a Condorcet signaling system.</p></li>
<li><p>receiver errors are considered in <span class="citation" data-cites="Nowak1999">(<a href="#ref-Nowak1999" role="doc-biblioref">Nowak and Krakauer 1999</a>)</span> where the authors claim they lead to syntax formation.</p></li>
</ul>
<div class="no-row-height column-margin column-container"></div></section>
</section>
</section>
<section id="complex-signals-and-compositionality" class="level1 page-columns page-full">
<h1>12. Complex Signals and Compositionality</h1>
<dl>
<dt>CCSS</dt>
<dd>
<p>complex composeable signaling systems</p>
</dd>
<dd>

</dd>
</dl>
<ul>
<li><p>The use of complex signals is not unique to humans.</p></li>
<li><p>In <span class="citation" data-cites="Nowak1999">(<a href="#ref-Nowak1999" role="doc-biblioref">Nowak and Krakauer 1999</a>)</span> the authors make a case that complex signals can increase the ﬁdelity of information transmission, by preventing simple signals getting crowded together as the space of potential signals gets ﬁlled up. Also some complex signalsing systems should be simpler to learn. (<em>can we specify a maximaly learnable family?</em>) and process inforamtion</p></li>
<li><p>considered CCSS as conffering greater Darwinian fitness in contexts where <em>rich information processing is important.</em></p>
<ul>
<li>Q: <strong>Is there a metric for measuring the advantage and or the importance of such information processing needs?</strong></li>
</ul></li>
<li><p>In <span class="citation" data-cites="batali1998">(<a href="#ref-batali1998" role="doc-biblioref">Batali 1998</a>)</span> the author investigates the emergence of complex signals in populations of neural nets.</p></li>
<li><p>in <span class="citation" data-cites="Kirby2000">(<a href="#ref-Kirby2000" role="doc-biblioref">Kirby 2002</a>)</span> the author, extends the model in a small population of interacting artiﬁcial agents.</p></li>
<li><p>These two papers assume Structured meanings like &lt;John, loves, Mary&gt;. But I am more interested in the ability of evolving arbitrary structures like a sketch map of resources, a distribution of prices, a small bitmap etc.</p></li>
<li><p>Skryms takes a similar reductionist POV: finding how to evolve a complex signaling system with minimal departure from the Lewis signaling game and other models already covered….</p></li>
<li><p>It is suggested that the “Inventing the code Game” is a sufficient framework creating basic composeable messages. If the receiver considers a sequence of two partial signals as conjunction the and can integrated into one full message!</p>
<ul>
<li><p>Red &gt; Top</p></li>
<li><p>Green&gt; Bottom</p></li>
<li><p>Yellow&gt; Left</p></li>
<li><p>Blue &gt; Right</p></li>
</ul>
<p>to signal the state of &lt;bottom, left&gt; a sender can send &lt;green,yellow&gt; or &lt;yellow,green&gt; and the receiver can compose them.</p></li>
<li><p>But if it is also possible to evolve and learn order for signals a richer form of composeability become possible. Subject–predicate or operator–sentence.</p></li>
<li><p>Sensitivity to temporal order is something many organisms have already developed in responding to perceptual signals.</p></li>
<li><p>More generally, we can say that temporal pattern recognition is a fundamental mechanism for anticipating the future.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-Nowak1999" class="csl-entry" role="listitem">
Nowak, Martin A., and David C. Krakauer. 1999. <span>“The Evolution of Language.”</span> <em>Proceedings of the National Academy of Sciences</em> 96 (14): 8028–33. <a href="https://doi.org/10.1073/pnas.96.14.8028">https://doi.org/10.1073/pnas.96.14.8028</a>.
</div><div id="ref-Kirby2000" class="csl-entry" role="listitem">
Kirby, Simon. 2002. <span>“<span>Natural Language From Artificial Life</span>.”</span> <em>Artificial Life</em> 8 (2): 185–215. <a href="https://doi.org/10.1162/106454602320184248">https://doi.org/10.1162/106454602320184248</a>.
</div></div><p>Skryms points out that temporal order is another mechanism that evolves and that they come together.</p>
<p>Unfortunately Skryms seems to get sidetracked once he point out about order and does not explain how order sensitivity eveloves in “Making the code game”.</p>
<div id="08558159" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mesa <span class="im">import</span> Agent, Model</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mesa.time <span class="im">import</span> StagedActivation</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LewisAgent(Agent):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model):</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.message <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action<span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> send(<span class="va">self</span>):</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> recive(<span class="va">self</span>):</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> calc_reward(<span class="va">self</span>):</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> set_reward(<span class="va">self</span>):</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reward <span class="op">=</span> model.reward</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Placeholder for learning logic</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Agent </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> received reward: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>reward<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Sender(LewisAgent):</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model):</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> send(<span class="va">self</span>):</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> <span class="va">self</span>.model.get_state()</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Learning to map states to signals</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">type</span>(state) <span class="kw">is</span> <span class="bu">str</span>:</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.message <span class="op">=</span> <span class="va">self</span>.model.states_signals[state]</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.message <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>          <span class="cf">while</span> <span class="bu">len</span>(<span class="va">self</span>.message)<span class="op">&gt;</span><span class="dv">0</span>:</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>            message <span class="op">=</span> {model.states_signals[<span class="va">self</span>.message.pop()]}</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.message <span class="op">=</span> <span class="va">self</span>.message.union(message)</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Sender </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> sends signal for state </span><span class="sc">{</span>state<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>message<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Receiver(LewisAgent):</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unique_id, model):</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(unique_id, model)</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> recive(<span class="va">self</span>):</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.received_signals<span class="op">=</span>[]</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.action <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> sender <span class="kw">in</span> <span class="va">self</span>.model.senders:</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.received_signals.append(sender.message)</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Learning to map signals to actions</span></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>received_signals<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span><span class="bu">type</span>(<span class="va">self</span>.received_signals)<span class="op">=</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> signal_set <span class="kw">in</span> <span class="va">self</span>.received_signals:</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>          actions <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>          <span class="cf">while</span> <span class="bu">len</span>(signal_set)<span class="op">&gt;</span><span class="dv">0</span>:</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>            action  <span class="op">=</span> {model.signals_actions[<span class="va">self</span>.message.pop()]}</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>            actions  <span class="op">=</span> actions.union(action)</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>          <span class="va">self</span>.action <span class="op">=</span>  <span class="va">self</span>.action.intersection(actions)</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="ss">f"Reciever </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unique_id<span class="sc">}</span><span class="ss"> action : </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>action<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> calc_reward(<span class="va">self</span>):</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>      action <span class="op">=</span> <span class="va">self</span>.action</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>      correct_action <span class="op">=</span> <span class="va">self</span>.model.states_actions[<span class="va">self</span>.model.current_state]</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>      reward <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> action <span class="op">==</span> correct_action <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>      model.reward <span class="op">=</span> reward</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SignalingGame(Model):</span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, senders_count<span class="op">=</span><span class="dv">1</span>, recievers_count<span class="op">=</span><span class="dv">1</span>, k<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.senders_count<span class="op">=</span>senders_count</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.recievers_count<span class="op">=</span>recievers_count</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_agents <span class="op">=</span> <span class="va">self</span>.recievers_count<span class="op">+</span><span class="va">self</span>.senders_count</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g., 0 -&gt; A, 1 -&gt; B, ...</span></span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.states_signals <span class="op">=</span> {i: <span class="bu">chr</span>(<span class="dv">65</span> <span class="op">+</span> i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)} </span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g., A -&gt; 0, B -&gt; 1, ...</span></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.signals_actions <span class="op">=</span> {<span class="bu">chr</span>(<span class="dv">65</span> <span class="op">+</span> i): i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)}</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>        <span class="co"># state 0 needs action 0, state 1 needs action 1, ...</span></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.states_actions <span class="op">=</span> {i: i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)}  </span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create agents</span></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.senders <span class="op">=</span> []</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.receivers<span class="op">=</span>[]</span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.my_agents<span class="op">=</span>[]</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.uid<span class="op">=</span><span class="dv">0</span></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.senders_count):</span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a>            sender <span class="op">=</span> Sender(<span class="va">self</span>.uid, <span class="va">self</span>)</span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.senders.append(sender)</span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.my_agents.append(sender)</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.uid <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span> (<span class="va">self</span>.recievers_count):</span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a>            reciever <span class="op">=</span> Receiver(<span class="va">self</span>.uid, <span class="va">self</span>)</span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.receivers.append(reciever)</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.my_agents.append(reciever)</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.uid <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.schedule <span class="op">=</span> StagedActivation(</span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>          model<span class="op">=</span><span class="va">self</span>,</span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>          agents<span class="op">=</span><span class="va">self</span>.my_agents, </span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>          stage_list <span class="op">=</span> [<span class="st">'send'</span>,<span class="st">'recive'</span>,<span class="st">'calc_reward'</span>,<span class="st">'set_reward'</span>]</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_state <span class="op">=</span> random.choice(<span class="bu">list</span>(<span class="va">self</span>.states_signals.keys()))</span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.current_state_set <span class="op">=</span> {random.choice(<span class="bu">list</span>(<span class="va">self</span>.states_signals.keys()))}</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"New state of the world: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>current_state<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.schedule.step()</span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_state(<span class="va">self</span>):</span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.senders_count <span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="va">self</span>.current_state</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: </span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> {<span class="va">self</span>.current_state, random.choice(<span class="bu">list</span>(<span class="va">self</span>.states_signals.keys()))}</span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a><span class="co"># Running the model</span></span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Number of states, signals, and actions</span></span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a>steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SignalingGame(senders_count<span class="op">=</span><span class="dv">2</span>,recievers_count<span class="op">=</span><span class="dv">1</span>,k<span class="op">=</span>k)</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"--- Step </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a>    model.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--- Step 1 ---
New state of the world: 0
Sender 0 sends signal for state {0}: set()
Sender 1 sends signal for state {0, 2}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0
--- Step 2 ---
New state of the world: 2
Sender 0 sends signal for state {1, 2}: set()
Sender 1 sends signal for state {1, 2}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0
--- Step 3 ---
New state of the world: 0
Sender 0 sends signal for state {0}: set()
Sender 1 sends signal for state {0, 2}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0
--- Step 4 ---
New state of the world: 2
Sender 0 sends signal for state {0, 2}: set()
Sender 1 sends signal for state {0, 2}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0
--- Step 5 ---
New state of the world: 0
Sender 0 sends signal for state {0, 1}: set()
Sender 1 sends signal for state {0, 1}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0
--- Step 6 ---
New state of the world: 2
Sender 0 sends signal for state {1, 2}: set()
Sender 1 sends signal for state {0, 2}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0
--- Step 7 ---
New state of the world: 1
Sender 0 sends signal for state {1, 2}: set()
Sender 1 sends signal for state {1}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0
--- Step 8 ---
New state of the world: 1
Sender 0 sends signal for state {1, 2}: set()
Sender 1 sends signal for state {1, 2}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0
--- Step 9 ---
New state of the world: 2
Sender 0 sends signal for state {0, 2}: set()
Sender 1 sends signal for state {1, 2}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0
--- Step 10 ---
New state of the world: 1
Sender 0 sends signal for state {1, 2}: set()
Sender 1 sends signal for state {1}: set()
self.received_signals=[set()]
type(self.received_signals)=&lt;class 'list'&gt;
self.received_signals=[set(), set()]
type(self.received_signals)=&lt;class 'list'&gt;
Reciever 2 action : set()
Agent 0 received reward: 0
Agent 1 received reward: 0
Agent 2 received reward: 0</code></pre>
</div>
</div>
<section id="some-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="some-thoughts">Some thoughts</h2>
<ol type="1">
<li><p>learning in the original Lewis language games is exponential in the size of the lexicon. It would seem that some complex signals systems should have orders of magnitude advantage in learning rates compared to the original variants. Lets consider a Lewis signaling system with 27 signals.<br>
The learning is <span class="math inline">O(e^{27})\propto5\times10^{12}</span></p></li>
<li><p>Under a conjunctive structure with three messages a lexicon of 9 messages would be required.<br>
The learning is <span class="math inline">O(e^{9})\propto 8.1\times10^{3}</span></p></li>
<li><p>Under Say we have a VSO complex signal with 3 signals per a positional POS category. This leads to 27 signal lexicon under the original lexicon. Using the complex system only 3 three signals need to be learned.<br>
So that learning is <span class="math inline">O(e^{3}) \propto 20</span></p>
<p>If we factor learning time as part of the costs of signaling we should expect complex signaling systems to emerge quickly. Also if we consider learning as part of In this case partial pooling states are acceptable and even desirable each signal now has three meaning depending on its position.</p></li>
<li><p>In NLP we never see such a perfect utilization of a SS where all synthetically messages are semantically meaningful. On the other hand NLP allow nesting so that sequence like V(VSO)(VSO) corresponding to 3<sup>8</sup> messages and adding a sub-category modifier prefix (MVMSMO) leads to (3^6) signals 729 signals without</p></li>
<li><p>For a simulation - some predators can be introduced into the environment nearby agents will signal it presence. Receivers who take that appropriate action will survive. Those that do not may die. Agent have longevity and must learn the language. When agents die they are replaced by infants without a uniform signaling weights.</p></li>
<li><p>Another point is that seems obvious is that if we learn/evolve the lexicon with just one one new word at a time the task becomes trivial. We just need to learn one new state to signal and one new signal to action mapping. But learning just one is a one to one matching. If we have some sense of the salience of the signals we can just order them in that order and we keep increasing fitness…. till we reach some marginal rate of fitness where new signals do almost nothing for our survival.</p></li>
<li><p>If we can evolve a complex signaling system we can move to next steps like optimizing our lexicon and grammar for:</p>
<ol type="1">
<li><p>minimizing communications errors, (error detection and correction)</p></li>
<li><p>maximizing information transmission. (compression)</p></li>
<li><p>minimizing cost of acquisition. (acquisition)</p></li>
<li><p>the trade off between grammatic generalization and easily learnability v.s. making the system harder to learn but more efficient for communication.</p></li>
<li><p>how do we handle inference (for logic)</p></li>
<li><p>how do we take advantage of predictability for partial messages</p></li>
<li><p>what about a convention for grammar - useful for agents that need to exchange data in different formats efficently.</p></li>
<li><p>Costs of morphotactics - can we do all this in practive with human sound systems. Can we figure our metrics for human languages.</p></li>
<li><p>Given a (human) language tree can we posit a most pasimonius path for its evolution.</p></li>
</ol></li>
<li></li>
</ol>
</section>
</section>
<section id="signals-bibliography" class="level1 page-columns page-full">
<h1>Signals Bibliography</h1>
<p>The following bibliographical entries are on General and Evolutionary Game Theory:</p>
<ul>
<li><p>von Neumann, J. and Morgenstern, O. (1944) Theory of Games and Economic Behavior. Princeton: Princeton University Press.</p></li>
<li><p>Weibull, J. (1995) Evolutionary Game Theory. Cambridge, MA: MIT Press.</p></li>
</ul>
<section id="section" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="section"></h2>
<p>In <span class="citation" data-cites="Skyrms2010signals">(<a href="#ref-Skyrms2010signals" role="doc-biblioref">Skyrms 2010</a>)</span> the author discusses how a Lewis signaling games can be viewed as a mechanism in which a rudimentary signaling system can give rise to a simple language.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Skyrms2010signals" class="csl-entry" role="listitem">
Skyrms, Brian. 2010. <span>“<span class="nocase">14512 Complex Signals and Compositionality</span>.”</span> In <em><span class="nocase">Signals: Evolution, Learning, and Information</span></em>. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780199580828.003.0013">https://doi.org/10.1093/acprof:oso/9780199580828.003.0013</a>.
</div></div><p>The languages arising from Lewis signaling games are limited.</p>
<p>The chapter on complex signals in terms tend to give rise to a</p>
<p>the bibliography</p>
<ul>
<li><p>Aldous, D. (1985) “Exchangeability and Related Topics.” In L’E´cole d’e´te´ de probabilite´s de Saint-Flour, XIII–1983 1–198. Berlin: Springer.</p></li>
<li><p>Alon, U. (2006) An Introduction to Systems Biology: Design Principles and Biological Circuits. Chapman and Hall.</p></li>
<li><p>Alon, U. (2007) “<strong>Network Motifs</strong>: Theory and Experimental Approaches.” Nature 8: 450–461. Anderson, C. and N. R. Franks (2001) “Teams in Animal Societies.” Behavioral Ecology 12: 534–540.</p></li>
<li><p>Argiento, R., R. Pemantle, B. Skyrms, and S. Volkov (2009) “Learning to Signal: Analysis of a Micro-Level <strong>Reinforcement Model</strong>.” Stochastic Processes and their Applications 119: 373–390.</p></li>
<li><p><del>Aristotle Historia Animalium Book IX. Aristotle Physics Book II.</del></p></li>
<li><p>Asher, N., I. Sher and M. Williams (2001) “Game Theoretical Foundations for Gricean Constraints.” In Proceedings of the Thirteenth Amsterdam Colloquium. Amsterdam: IILC.</p></li>
<li><p><strong>Aumann, R. (1987) “Subjectivity and Correlation in Randomized Strategies.” Journal of Mathematical Economics 1: 67–96.</strong></p></li>
<li><p><strong>Aumann, R. (1987) “Correlated Equilibrium as an Expression of Bayesian Rationality.” Econometrica 55: 1–18.</strong></p></li>
<li><p>Bala, V. and S. Goya (2000) “A Noncooperative Model of Network Formation.” Econometrica 68: 1181–1231.</p></li>
<li><p><del>Barnes, J. (2001) Early Greek Philosophy. 2nd edn. London: Penguin.</del></p></li>
<li><p><del>Barnes, J. (1982) The Presocratic Philosophers. London: Routledge.</del></p></li>
<li><p><span class="citation" data-cites="Barrett2006Numerical">(<a href="#ref-Barrett2006Numerical" role="doc-biblioref">Barrett 2006</a>)</span> <del>Barrett, J. A. (2006) “Numerical Simulations of the Lewis Signaling Game: Learning Strategies, Pooling Equilibria, and the Evolution of Grammar.” Working Paper MBS06–09. University of California, Irvine.</del></p></li>
<li><p><span class="citation" data-cites="Barrett2009Evolution">(<a href="#ref-Barrett2009Evolution" role="doc-biblioref">Barrett 2009</a>)</span> <del>Barrett, J. A. (2007a) “The Evolution of Coding in Signaling Games.” Theory and Decision. DOI: 10.1007/s11238–007–9064–0.</del></p></li>
<li><p><span class="citation" data-cites="Barrett2007Dynamic">(<a href="#ref-Barrett2007Dynamic" role="doc-biblioref">Barrett 2007</a>)</span> <del>Barrett, J. A. (2007b) “Dynamic Partitioning and the Conventionality of Kinds.” Philosophy of Science 74: 527–546.</del></p></li>
<li><p>Barrett, J. A. and K. Zollman (2007) “The Role of Forgetting in the Evolution and Learning of Language.” preprint.</p></li>
<li><p><span class="citation" data-cites="batali1998">Batali (<a href="#ref-batali1998" role="doc-biblioref">1998</a>)</span> <del>Batali, J. (1998) “Computational Simulations of the Evolution of Grammar.” In Approaches to the Evolution of Language: Social and Cognitive Bases, ed.&nbsp;J. R. Hurford et al.&nbsp;Cambridge: Cambridge University Press.</del></p></li>
<li><p><del>Bauer, W. D. and U. Mathesius (2004) “Plant Responses to Bacterial Quorum-Sensing Signals.” Current Opinion in Plant Biology 7: 429–433.</del></p></li>
<li><p>Beggs, A. (2005) “On the Convergence of <strong>Reinforcement Learning</strong>.” Journal of Economic Theory 122: 1–36.</p></li>
<li><p>Benaim, M. (1999) “Dynamics of Stochastic Approximation Algorithms.” In Seminaire de Probabilites 33. Berlin: Springer Verlag.</p></li>
<li><p>Benaim, M., S. J. Shreiber, and P. Tarres (2004) “Generalized Urn Models of Evolutionary Processes.” Annals of Applied Probability 14: 1455–1478.</p></li>
<li><p>Bereby-Meyer, Y. and I. Erev (1998) “On Learning How to be a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain.” Journal of Mathematical Psychology 42: 266–286.</p></li>
<li><p>Berg, R. M. van den (2008) Proclus’ Commentary on the Cratylus in Context. Leiden: Brill.</p></li>
<li><p>Bergstrom, T. (2002) “Evolution of Social Behavior: Individual and Group Selection Models.” Journal of Economic Perspectives 16: 231–238.</p></li>
<li><p>Bergstrom, C. T. and M. Lachmann (1998) “Signaling Among Relatives III. Talk is Cheap.” Proceedings of the National Academy of Sciences USA 95: 5200–5105.</p></li>
<li><p>Berleman, J. E., J. Scott, T. Chumley, and J. R. Kirby (2008) “Predataxis Behavior in Myxococcus Xanthus.” Proceedings of the National Academy of Sciences USA 105: 17127–17132.</p></li>
<li><p>Berninghaus, S., K.-M. Ehrhart, M. Ott, and B. Vogt (2007) “Evolution of Networks–an Experimental Analysis.” Journal of Evolutionary Economics 17: 317–347.</p></li>
<li><p>Bickerton, D. (1990) Language and Species. Chicago: University of Chicago Press.</p></li>
<li><p>Bjornerstedt, J. and J. Weibull (1995) “Nash Equilibrium and Evolution by Imitation.” In K. Arrow et al.&nbsp;(eds.), 155–71, The Rational Foundations of Economic Behavior. New York: Macmillan.</p></li>
<li><p>Bloch, F. and M. Jackson (2007) “The Formation of Networks with Transfers among Players.” Journal of Economic Theory 133: 83–110.</p></li>
<li><p>Bloch, F. and B. Dutta (2009) “Communication Networks with Endogenous Link Strength.” Games and Economic Behavior 66: 39–56.</p></li>
<li><p>Blume, A. (2000) “Coordination and Learning with a Partial Language.” Journal of Economic Theory 95: 1–36.</p></li>
<li><p>Blume, A., D. DeJong, Y.-G. Kim, and G. B. Sprinkle (1998) “Experimental Evidence on the Evolution of the Meaning of Messages in Sender-Receiver Games.” American Economic Review 88: 1323–1340.</p></li>
<li><p>Blume, A., D. DeJong, Y.-G. Kim, and G. B. Sprinkle (2001) “Evolution of Communication with Partial Common Interest.” Games and Economic Behavior 37: 79–120.</p></li>
<li><p><span class="citation" data-cites="Blume2002">(<a href="#ref-Blume2002" role="doc-biblioref">Blume et al. 2002</a>)</span> <del>Blume, A., D. DeJong, G. Neumann, N. E. Savin (2002) “Learning and Communication in Sender-Receiver Games: An Econometric Investigation.” Journal of Applied Econometrics 17: 225–247.</del></p></li>
<li><p>Borgers, T. and R. Sarin (1997) “<strong>Learning through Reinforcement and the Replicator Dynamics</strong>.” Journal of Economic Theory 74: 235–265.</p></li>
<li><p>Borgers, T. and R. Sarin (2000) “Naive Reinforcement Learning with Endogenous Aspirations.” International Economic Review 41: 921–950.</p></li>
<li><p>Brandman, O. and T. Meyer (2008) “Feedback Loops Shape Cellular Signals in Space and Time.” Science 322: 390–395. Brentano, F. (1874) Psychology from an Empirical Standpoint. London: Routledge.</p></li>
<li><p>Brown, G. W. (1951) “Iterative Solutions of Games by Fictitious Play.” In Activity Analysis of Production and Allocation, ed.&nbsp;T. C. Koopmans. New York: Wiley.</p></li>
<li><p>Bshary, R., A. Hohner, K. Ait-el-Djoudi, and H. Fricke (2006) “Interspecific Communicative and Coordinated Hunting between Groupers and Giant Moray Eels in the Red Sea.” PLoS Biology 4:2393–2398 4:e431, DOI:10:1371/journal.pbio.0040431.</p></li>
<li><p>Bush, R. and F. Mosteller (1955) Stochastic Models of Learning. John Wiley &amp; Sons: New York.</p></li>
<li><p>Callander, S. and C. R. Plott (2005) “Principles of Network Development and Evolution: An Experimental Study.” Journal of Public Economics 89: 1469–1495.</p></li>
<li><p>Camerer, C. and T-H. Ho (1999) “Experience Weighted Attraction Learning in Normal Form Games.” Econometrica 67: 827–874.</p></li>
<li><p>Campbell, G. (2003) Lucretius on Creation and Evolution. Oxford: Oxford University Press.</p></li>
<li><p>Charrier, I. and C. B. Sturdy (2005) “Call-Based Species Recognition in the Black-Capped Chickadees.” Behavioural Processes 70: 271–281.</p></li>
<li><p>Cheney, D. and R. Seyfarth (1990) How Monkeys See the World: Inside the Mind of Another Species. Chicago: University of Chicago Press.</p></li>
<li><p><del>Chomsky, N. (1957) Syntactic Structures. The Hague: Mouton.</del></p>
<ul>
<li>general linguistics</li>
</ul></li>
<li><p>Cross, J. G. (1973) “A Stochastic Learning Model of Economic Behavior.” Quarterly Journal of Economics 87: 239–266.</p></li>
<li><p>Crawford, V. and J. Sobel (1982) “Strategic Information Transmission.” Econometrica 50: 1431–1451.</p></li>
<li><p>Cubitt, R. and R. Sugden (2003) “Common Knowledge, Salience and Convention: A Philosophical Reconstruction of David Lewis’ Game Theory.” Economics and Philosophy 19:175–210.</p></li>
<li><p><del>Dante The Divine Comedy of Dante Alighieri. Trans. H. F. Cary, ed.&nbsp;Jim Manis. PSU Electronic Classics, .</del></p>
<ul>
<li>classics</li>
</ul></li>
<li><p><del>De Morgan, A. (1838) An Essay on Probabilities and their Application to Life Contingencies and Insurance Offices. London: Longman.</del></p>
<ul>
<li>general probability</li>
</ul></li>
<li><p>Donaldson, M. C., M. Lachmann, and C. T. Bergstrom (2007) “The Evolution of Functionally Referential Meaning in a Structured World.” Journal of Theoretical Biology 246: 225–233.</p></li>
<li><p>Dretske, F. (1981) Knowledge and the Flow of Information. Cambridge: MIT Press.</p></li>
<li><p>Dugatkin, L. A. (1997) Cooperation Among Animals: An Evolutionary Perspective. Oxford: Oxford University Press.</p></li>
<li><p>Dyer, F. C. and T. D. Seeley (1991) “Dance Dialects and Foraging Range in three Asian Honey Bee Species.” Behavioral Ecology and Sociobiology 28: 227–233.</p></li>
<li><p>Edwards, W. (1961) “Probability Learning in 1000 Trials.” Journal of Experimental Psychology 62: 385–394.</p></li>
<li><p>Erev, I. and E. Haruvy (2005) “On the Potential Uses and Current Limitations of Data-Driven Learning Models.” Journal of Mathematical Psychology 49: 357–371.</p></li>
<li><p>Erev, I. and A. Roth (1998) “Predicting How People Play Games: Reinforcement Learning in Experimental Games with Unique Mixed-Strategy Equilibria.” American Economic Review 88: 848–881.</p></li>
<li><p>Estes, W. K. (1950) “Toward a Statistical Theory of Learning.” Psychological Review 57: 94–107.</p></li>
<li><p>Evans, C. S., C. L. Evans and P. Marler (1994) “On the Meaning of Alarm Calls: Functional Reference in an Avian Vocal System.” Animal Behavior 73: 23–38.</p></li>
<li><p>Falk, A. and M. Kosfeld (2003) “It’s All About Connections: Evidence on Network Formation.” IEW Working Paper 146. University of Zurich.</p></li>
<li><p>Feltovich, N. (2000) “Reinforcement-Based vs.&nbsp;Belief-Based Learning Models in Experimental Asymmetric-Information Games.” Econometrica 68: 605–641.</p></li>
<li><p>Flache, A. and M. Macy (2002) “Stochastic Collusion and the Power Law of Learning.” Journal of Conflict Resolution 46: 629–653.</p></li>
<li><p>Floridi, L. (1997) “Skepticism, Animal Rationality, and the Fortune of Chrysippus’ Dog.” Archiv fu¨r Geschichte der Philosophie 79: 27–57.</p></li>
<li><p>Frede, D. and B. Inwood (2005) Language and Learning: Philosophy of Language in the Hellenistic Age. Cambridge: Cambridge University Press. Fudenberg,</p></li>
<li><p>D. and D. Levine (1998) A Theory of Learning in Games. Cambridge, MA: MIT Press.</p></li>
<li><p>Galeotti, A. and S. Goyal (2008) “The Law of the Few.” Working paper, University of Essex.</p></li>
<li><p>Galeotti, A., S. Goyal, and J. Kamphorst (2006) “Network Formation with Heterogeneous Players.” Games and Economic Behavior 54: 353–372.</p></li>
<li><p>Gazda, S., R. C. Connor, R. K. Edgar, and F. Cox (2005) “A Division of Labour with Role Specialization in Group-hunting Bottlenose Dolphins (Tursiops truncatus) off Cedar Key, Florida.” Proceedings of the Royal Society B 272: 135–140.</p></li>
<li><p>Gentner, T. Q., K. M. Fenn, D. Margoliash, and H. C. Nusbaum (2006) “Recursive Syntactic Pattern Learning by Songbirds.” Nature 440: 1204–1207.</p></li>
<li><p>Gettier, E. (1963) “Is Justified True Belief Knowledge?” Analysis 23:121–123.</p></li>
<li><p>Godfrey-Smith, P. (1989) “Misinformation.” Canadian Journal of Philosophy 19: 522–550.</p></li>
<li><p>Godfrey-Smith, P. (2000a) “On the Theoretical Role of Genetic Coding.” Philosophy of Science 67: 26–44.</p></li>
<li><p>Godfrey-Smith, P. (2000b) “Information, Arbitrariness and Selection: Comments on Maynard-Smith.” Philosophy of Science 67: 202–207.</p></li>
<li><p>Good, I. J. (1950) Probability and the Weighing of Evidence. London: Charles Griffin.</p></li>
<li><p>Good, I. J. (1983) Good Thinking: The Foundations of Probability and its Applications. Minneapolis: University of Minnesota Press.</p></li>
<li><p>Gould, J. L. (1975) “Honey Bee Recruitment: the Dance Language Controversy.” Science 189: 685–693.</p></li>
<li><p>Gould S. J., and N. Eldredge (1977) “Punctuated Equilibria: The Tempo and Mode of Evolution reconsidered.” Paleobiology 3: 115–151.</p></li>
<li><p>Goryachev, A. B., D. J. Toh, and T. Lee (2006) “Systems Analysis of a Quorum Sensing Network: Design Constraints Imposed by the Functional Requirements, Network Topology and Kinetic Constants.” BioSystems 83: 178–187.</p></li>
<li><p>Goyal, S. (2007) Connections: An Introduction to the Economics of Networks. Princeton: Princeton University Press.</p></li>
<li><p>Grice, H. P. (1957) “Meaning.” Philosophical Review 66: 377–388.</p></li>
<li><p>Grice, H. P. (1975) “Logic and Conversation.” In Syntax and Semantics, vol.&nbsp;3, ed.&nbsp;P. Cole and J. L. Morgan, 41–58. New York: Academic Press.</p></li>
<li><p>Grice, H. P. (1989) Studies in the Way of Words. Cambridge, MA: Harvard University Press.</p></li>
<li><p>Griffiths, P. E. (2001) “Genetic Information: A Metaphor in Search of a Theory.” Philosophy of Science 68: 394–412.</p></li>
<li><p>Grim, P., P. St.&nbsp;Denis, and T. Kokalis (2002) “Learning to Communicate: The Emergence of Signaling in Spatialized Arrays of Neural Nets.” Adaptive Behavior 10: 45–70.</p></li>
<li><p>Grim, P., T. Kokalis, A. Alai-Tafti, A., and N. Kilb (2000) “Evolution of Communication in Perfect and Imperfect Worlds.” World Futures: The Journal of General Evolution 56: 179–197.</p></li>
<li><p>Grim, P., T. Kokalis, A. Alai-Tafti, N. Kilb, and P. St.&nbsp;Denis (2004) “Making Meaning Happen.” Journal of Experimental and Theoretical Artificial Intelligence 16: 209–243.</p></li>
<li><p>Gyger, M., P. Marler, and R. Pickert (1987) “Semantics of an Avian Alarm Call System: The Male Domestic Fowl,</p></li>
<li><p>Gallus Domesticus.” Behavior 102: 15–20. 184</p></li>
<li><p>Hadeler, K. P. (1981) “Stable Polymorphisms in a Selection Model with Mutation.” SIAM Journal of Applied Mathematics 41: 1–7.</p></li>
<li><p>Hailman, J., M. Ficken, and R. Ficken (1985) “The ‘Chick-a-dee’ calls of Parus atricapillus.” Semiotica 56: 191–224.</p></li>
<li><p>Hamilton, W. D. (1963) “The Evolution of Altruistic Behavior.” American Naturalist 97: 354–356.</p></li>
<li><p>Hamilton, W. D. (1964) “The Genetical Evolution of Social Behavior I and II.” Journal of Theoretical Biology 7: 1–52.</p></li>
<li><p>Hamilton, W. D. (1967) “Extraordinary Sex Ratios.” Science 156: 477–488.</p></li>
<li><p>Hamilton, W. D. (1971) “Selection of Selfish and Altruistic Behavior in Some Extreme Models.” In Man and Beast, ed.&nbsp;J. F. Eisenberg and W. S. Dillon, 59–91. Washington, D.C.: Smithsonian Institution Press.</p></li>
<li><p>Hamilton, W. D. (1995) Narrow Roads of Gene Land. vol.&nbsp;1: Evolution of Social Behavior. New York: W. H. Freeman.</p></li>
<li><p>Harley, C. B. (1981) “Learning the <strong>Evolutionarily Stable Strategy</strong>.” Journal of Theoretical Biology 89: 611–633.</p></li>
<li><p>Harms, W. F. (2004) Information and Meaning in Evolutionary Processes. Cambridge: Cambridge University Press.</p></li>
<li><p>Hauert, C., S. De Monte, J. Hofbauer, and K. Sigmund (2002) “Volunteering as Red Queen Mechanism for Cooperation in Public Goods Games.” Science 296, 1129–1132.</p></li>
<li><p>Hauser, M. D. (1988) “How Infant Vervet Monkeys Learn to Recognize Starling Alarm Calls: The Role of Experience.” Behavior 105: 187–201.</p></li>
<li><p>Hauser, M. D. (1997) The Evolution of Communication. Cambridge, MA: MIT Press.</p></li>
<li><p>Hauser, M. D., N. Chomsky, and W. T. Fitch (2002) “The Faculty of Language: What is it, Who has it, and How did it Evolve.” Science 298: 1569–1579.</p></li>
<li><p>Hebb, D. (1949) The Organization of Behavior. New York: Wiley.</p></li>
<li><p>Herman, L. M., D. G. Richards, and J. P. Wolz (1984) “Comprehension of Sentences by Bottle-Nosed Dolphins.” Cognition 16: 129–219.</p></li>
<li><p>Herrnstein, R. J. (1961) “Relative and Absolute Strength of Response as a Function of Frequency of Reinforcement.” Journal of Experimental Analysis of Behavior 4: 267–272.</p></li>
<li><p>Herrnstein, R. J. (1970) “On the Law of Effect.” Journal of the Experimental Analysis of Behavior 13: 243–266.</p></li>
<li><p>Ho, T. H., X. Wang, and C. Camerer (2008) “Individual differences in EWA Learning with Partial Payoff Information.” The Economic Journal 118: 37–59.</p></li>
<li><p>Hofbauer, J. (1985) “The Selection-Mutation Equation.” Journal of Mathematical Biology. 23: 41–53.</p></li>
<li><p>Hofbauer, J. and S. Huttegger (2008) “Feasibility of Communication in Binary Signaling Games.” Journal of Theoretical Biology 254: 843–849.</p></li>
<li><p><span class="citation" data-cites="hofbauer1998evolutionary">Hofbauer and Sigmund (<a href="#ref-hofbauer1998evolutionary" role="doc-biblioref">1998</a>)</span> <del>Hofbauer, J. and K. Sigmund (1998) Evolutionary Games and Population Dynamics. Cambridge: Cambridge University Press.</del></p></li>
<li><p>Hojman, D. A. and A. Szeidl (2008) “Core and Periphery in Networks.” Journal of Economic Theory. 139: 295–309.</p></li>
<li><p>Holland, J. (1975) Natural and Artificial Systems. Ann Arbor, Michigan: University of Michigan Press.</p></li>
<li><p>Holldobler, B. and E. O. Wilson (1990) The Ants. Cambridge, MA:</p></li>
<li><p>Belknap. Hoppe, F. M. (1984) “Polya-like Urns and the Ewens Sampling Formula.” Journal of Mathematical Biology 20: 91–94.</p></li>
<li><p>Hopkins, E. (2002) “Two Competing Models about How People Learn in Games.” Econometrica 70, 2141–2166.</p></li>
<li><p>Hopkins, E. and M. Posch (2005) “Attainability of Boundary Points under Reinforcement Learning.” Games and Economic Behavior 53: 110–125. Hume, D. (1739) A Treatise of Human Nature. London: John Noon.</p></li>
<li><p>Hurford, J. (1989) “Biological Evolution of the Saussurean Sign as a Component of the Language Acquisition Device.” Lingua 77: 187–222.</p></li>
<li><p>Huttegger, S. (2007a) “Evolution and the Explanation of Meaning.” Philosophy of Science 74: 1–27.</p></li>
<li><p>Huttegger, S. (2007b) “Evolutionary Explanations of Indicatives and Imperatives.” Erkenntnis 66: 409–436.</p></li>
<li><p>Huttegger, S. (2007c) “Robustness in Signaling Games.” Philosophy of Science 74: 839–847.</p></li>
<li><p>Huttegger, S. and B. Skyrms (2008) “Emergence of Information Transfer by Inductive Learning.” Studia Logica 89: 237–256.</p></li>
<li><p>Huttegger, S., B. Skyrms, R. Smead, and K. Zollman (2009) “Evolutionary Dynamics of Lewis Signaling Games: Signaling Systems vs.&nbsp;Partial Pooling.” Synthese. DOI: 10.1007/s11229–009–9477–0</p></li>
<li><p>Izquierdo, L., D. Izquierdo, N. Gotts, and J. G. Polhill (2007) “Transient and Asymptotic Dynamics of Reinforcement Learning in Games.” Games and Economic Behavior 61: 259–276. 186 SIGNALS: EVOLUTION, LEARNING, AND INFORMATION</p></li>
<li><p>Jackendoff, R. (2002) Foundations of Language. Oxford: Oxford University Press.</p></li>
<li><p>Jackson, M. (2008) Social and Economic Networks. Princeton: Princeton University Press.</p></li>
<li><p>Jackson, M. and A. Watts (2002) “On the Formation of Interaction Networks in Social Coordination Games.” Games and Economic Behavior 41: 265–291.</p></li>
<li><p><del>Kaiser, D. (2004) “Signaling in Myxobacteria.”Annual Review of Microbiology 58: 75–98.</del></p></li>
<li><p><del>Kant, I. (1965) [1785] Fundamental Principles of the Metaphysics of Morals. Trans. Thomas Kingsmill Abbott Project Gutenberg. 10 edn. London: Longmans Green.</del></p></li>
<li><p>Kavanaugh, M. (1980) “Invasion of the Forest by an African Savannah Monkey: Behavioral Adaptations.” Behavior 73: 239–60. Kirby, S. (2000) “Syntax without Natural Selection: How Compositionality Emerges from Vocabulary in a Population of Learners.” In The Evolutionary Emergence of Language, ed.&nbsp;C. Knight, 303–323. Cambridge: Cambridge University Press.</p></li>
<li><p>Kirby, S. (2007) “The Evolution of Meaning-Space Structure through Iterated Learning.” In Emergence of Communication and Language, ed.&nbsp;C. Lyon et al., 253–268. Berlin: Springer Verlag.</p></li>
<li><p>Kirchhof, J. and K. Hammerschmidt (2006) “Functionally referential Alarm Calls in Tamarins (Saguinus fuscicollis and Saguinus mystax)– Evidence from Playback Experiments.” Ethology 112: 346–354.</p></li>
<li><p>Kirkup, B. C. and M. A. Riley (2004) “Antibiotic-Mediated Antagonism Leads to a Bacterial Game of Rock-Paper-Scissors in vivo.” Nature 428: 412–414.</p></li>
<li><p>Komarova, N. and P. Niyogi (2004) “Optimizing the Mutual Intelligibility of Linguistic Agents in a Shared World.” Artificial Intelligence 154: 1–42.</p></li>
<li><p><strong>Komarova, N., P. Niyogi, and M. Nowak (2001) “The Evolutionary Dynamics of Grammar Acquisition.” Journal of Theoretical Biology 209: 43–59.</strong></p></li>
<li><p>Kosfeld, M. (2004) “Economic Networks in the Laboratory.” Review of Network Economics 3: 20–41.</p></li>
<li><p>Kirchhof, J. and K. Hammerschmidt (2007) “Functionally Referential Alarm Calls in Tamarins (Saguinis fusicollis and Saguinis mystax.)—Evidence from Playback Experiments.” Ethology 112: 346–354.</p></li>
<li><p>Kullback, S. and R. A. Leibler (1951) “On Information and Sufficiency.” Annals of Mathematical Statistics 22: 79–86.</p></li>
<li><p>Kullback, S. (1959) Information Theory and Statistics. New York: John Wiley.</p></li>
<li><p>Levi-Strauss, C. (1969) The Elementary Structures of Kinship. Boston: Beacon Press.</p></li>
<li><p>Lewis, D. K. (1969) Convention. Cambridge, MA: Harvard University Press.</p></li>
<li><p>Liggett, T. M. and S. Rolles (2004) “An Infinite Stochastic Model of Social Network Formation.” Stochastic Processes and their Applications 113: 65–80.</p></li>
<li><p>Lindley, D. (1956) “On a Measure of the Information Provided by an Experiment.” The Annals of Mathematical Statistics 27: 986–1005.</p></li>
<li><p><del>Lloyd, J. E. (1965) “Aggressive Mimicry in Photuris: Firefly Femmes Fatales.” Science 149: 653–654.</del></p></li>
<li><p><del>Lloyd, J. E. (1975) “Aggressive Mimicry in Fireflies: Signal Repertoires of Femmes Fatales.” Science 187: 452–453.</del></p></li>
<li><p>Luce, R. D. (1959) Individual Choice Behavior. John Wiley &amp; Sons: New York.</p></li>
<li><p>Macedonia, J. M. (1990) “What is Communicated in the Antipredator Calls of Lemurs: Evidence from Antipredator Call Playbacks to Ringtailed and Ruffed Lemurs.” Ethology 86: 177–190.</p></li>
<li><p>McKinnon, S. (1991) From a Shattered Sun. Madison: University of Wisconsin Press.</p></li>
<li><p>Macy, M. (1991) “Learning to Cooperate: Stochastic and Tacit Collusion in Financial Exchange.” American Journal of Sociology 97: 808–843.</p></li>
<li><p>Macy, M. and A. Flache (2002) “Learning Dynamics in Social Dilemmas.” Proceedings of the National Academy of Sciences of the USA 99: 7229– 7236.</p></li>
<li><p>Malinowski, B. (1920) “Kula: The Circulating Exchange of Valuables in the Archipelagoes of Eastern New Guinea.” MAN 20: 97–105.</p></li>
<li><p>Malinowski, B. (1922) Argonauts of the Western Pacific. New York: Dutton.</p></li>
<li><p>Manser, M., R. M. Seyfarth, and D. Cheney (2002) “Suricate Alarm Calls Signal Predator Class and Urgency.” Trends in Cognitive Science 6: 55–57.</p></li>
<li><p>Marden, J. P., H. P. Young, G. Arslan, and J. S. Shamma (2009) “Payoffbased dynamics for Multiplayer Weakly Acyclic Games.” SIAM Journal on Control and Optimization 48: 373–396. 188</p></li>
<li><p>Marler, P. (1999) “On Innateness: Are Sparrow Songs ‘Learned’ or ‘Innate.’” In The Design of Animal Communication, ed.&nbsp;Marc Hauser and Mark Konishi. Cambridge, MA: MIT Press.</p></li>
<li><p><span class="citation" data-cites="Smith1973LogicAnimalConflict">(<a href="#ref-Smith1973LogicAnimalConflict" role="doc-biblioref">Smith and Price 1973</a>)</span> <del>Maynard Smith, J. and G. R. Price (1973) “The Logic of Animal Conflict.” Nature 246: 15–18.</del></p></li>
<li><p>Maynard Smith, J. and G. A. Parker (1976) “The Logic of Asymmetric Contests.” Animal Behaviour 24: 159–175.</p></li>
<li><p>Maynard Smith, J. (1982) Evolution and the Theory of Games. Cambridge: Cambridge University Press.</p></li>
<li><p>Maynard Smith, J. (2000) “The Concept of Information in Biology.” Philosophy of Science 67: 177–194.</p></li>
<li><p>Maynard Smith, J. and D. Harper (2003) Animal Signals. Oxford: Oxford University Press.</p></li>
<li><p>Mayor, J. (1898) “King James I On the Reasoning Faculty in Dogs.” The Classical Review 12: 93–96.</p></li>
<li><p>McGregor, P. (2005) Animal Communication Networks. Cambridge University Press: Cambridge.</p></li>
<li><p>Merin, A. (1999) “Information, Relevance, and Social Decisionmaking: Some Principles and Results of Decision-Theoretic Semantics.” In L. Moss, J. Ginzburg, M. de Rijke (eds.), 179–221, Logic, Language, and Computation, vol.&nbsp;2. Stanford: CSLI.</p></li>
<li><p>Miller, M. B. and B. Bassler (2001) “Quorum Sensing In Bacteria.” Annual Review of Microbiology 55: 165–199.</p></li>
<li><p>Millikan, R. G. (1984) Language, Thought and Other Biological Categories. Cambridge, MA: MIT Press.</p></li>
<li><p>Milo, R., S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon (2002) “Network Motifs: Simple Building Blocks of Complex Networks.” Science 298: 824–827.</p></li>
<li><p><span class="citation" data-cites="doi:10.1073/pnas.96.14.8028">(<a href="#ref-doi:10.1073/pnas.96.14.8028" role="doc-biblioref"><strong>doi:10.1073/pnas.96.14.8028?</strong></a>)</span> <del><strong>Nowak, M. A. and D. Krakauer (1999) “The Evolution of Language.” Proceedings of the National Academy of Sciences of the USA 96: 8028–8033.</strong></del></p></li>
<li><p>Nowak, M., J. Plotkin, and D. Krakauer (1999) “The Evolutionary Language Game.” Journal of Theoretical Biology 200: 147–162.</p></li>
<li><p>Nowak, M. and K. Sigmund (1993) “A Strategy of Win-stay, Lose-shift that Outperforms Tit-for-tat in the Prisoner’s Dilemma Game.” Nature 364: 56–58.</p></li>
<li><p>Oliphant, M. (1994) “The Dilemma of Saussurean Communication.” Biosystems 37: 31–38.</p></li>
<li><p>Othmer, H. G. and A. Stevens (1997) “Aggregation, Blow Up and Collapse: The ABC’s of Taxis in Reinforced Random Walks.” SIAM Journal on Applied Mathematics 57: 1044–1081.</p></li>
<li><p>Papineau, D. (1984) “Representation and Explanation.” Philosophy of Science 51: 550–72.</p></li>
<li><p>Papineau, D. (1987) Reality and Representation. Oxford: Blackwell.</p></li>
<li><p>Parikh, P. (2001) The Use of Language. Stanford: CSLI.</p></li>
<li><p>Pawlowitsch, C. (2008) “Why Evolution Does Not Always Lead to an Optimal Signaling System.” Games and Economic Behavior 63: 203–226.</p></li>
<li><p>Pemantle, R. (1990) “Nonconvergence to Unstable Points in Urn Models and Stochastic Approximations.” Annals of Probability 18: 698–712.</p></li>
<li><p>Pemantle, R. (2007) “A Survey of Random Processes with Reinforcement.” Probability Surveys 4: 1–79.</p></li>
<li><p>Pemantle, R. and B. Skyrms (2004a) “Network Formation by Reinforcement Learning: The Long and the Medium Run.” Mathematical Social Sciences 48: 315–327.</p></li>
<li><p>Pemantle, R. and B. Skyrms (2004b) “Time to Absorption in Discounted Reinforcement Models” Stochastic Processes and Their Applications 109: 1–12.</p></li>
<li><p>Pinker, S., and R. Jackendoff (2005) “The Faculty of Language: What’s Special About It?” Cognition 95: 201–236.</p></li>
<li><p>Pitman, J. (1995) “Exchangeable and Partially Exchangeable Random Partitions.” Probability Theory and Related Fields 102: 145–158. Proclus. (2007) On Plato Cratylus. Trans. Brian Duvick. London: Duckworth.</p></li>
<li><p>Quine, W. V. O. (1936) “Truth by Convention.” In Philosophical Essays for A. N. Whitehead, ed.&nbsp;O. H. Lee. 90–124.</p></li>
<li><p>Quine, W. V. O. (1969) “Epistemology Naturalized.” In Ontological Relativity and Other Essays. New York: Columbia University Press.</p></li>
<li><p>Rainey, H. J., K. Zuberbu¨hler, and P. J. B. Slater (2004) “Hornbills Can Distinguish between Primate Alarm Calls.” Proceedings of the Royal Society of London B 271: 755–759.</p></li>
<li><p>J. Riley, R. U. Greggers, A. D. Smith, D. R. Reynolds, and R. Menzel (2005) “The Flight Paths of Honeybees Recruited by the Waggle Dance.” Nature 435: 205–207.</p></li>
<li><p>Robbins, H. (1952) “Some Aspects of the Sequential Design of Experiments.” Bulletin of the American Mathematical Society 58: 527–535. 190</p></li>
<li><p>van Rooy, Robert. (2003) “Questioning to Resolve Decision Problems.” Linguistics and Philosophy 26:727–763.</p></li>
<li><p>Roth, A. and I. Erev (1995) “Learning in Extensive Form Games: Experimental Data and Simple Dynamical Models in the Intermediate Term.” Games and Economic Behavior 8: 164–212.</p></li>
<li><p>Russell, B. (1921) The Analysis of Mind. (Lecture X) London: George Allen and Unwin.</p></li>
<li><p>Russell, B. (1948) Human Knowledge, Its Scope and Limits. New York: Simon and Schuster.</p></li>
<li><p>Salmon, T. C. (2001) “An Evaluation of Econometric Models of Adaptive Learning.” Econometrica 1597–1628.</p></li>
<li><p>Schlag, K. (1998) “Why Imitate and If So, How? A Bounded Rational Approach to Many Armed Bandits.” Journal of Economic Theory 78, 130–156.</p></li>
<li><p>Schreiber, Sebastian J. (2001) “Urn Models, Replicator Processes, and Random Genetic Drift”, SIAM Journal on Applied Mathematics, 61.6: 2148–2167.</p></li>
<li><p>Schultz, W. (2004) “Neural Coding of Basic Reward Terms of Animal Learning Theory, Game Theory, Microeconomics and Behavioural Ecology.” Current Opinion in Neurobiology 14:139–147.</p></li>
<li><p>Searcy, W. A. and S. Nowicki (2005) The Evolution of Animal Communication: Reliability and Deception in Signaling Systems. Princeton: Princeton University Press.</p></li>
<li><p><strong>Selten, R. and W. Massimo (2007) “The Emergence of Simple Languages in an Experimental Coordination Game.” Proceedings of the National Academy of Sciences of the USA 104: 7361–7366.</strong></p></li>
<li><p>Seyfarth, R. M. and D. L. Cheney (1990) “The Assessment by Vervet Monkeys of Their Own and Other Species’ Alarm Calls.” Animal Behavior 40: 754–764.</p></li>
<li><p><strong>Shannon, C. (1948) “A Mathematical Theory of Communication.” The Bell System Mathematical Journal 27: 379–423, 623–656.</strong></p></li>
<li><p><strong>Shannon, C. and W. Weaver (1949) The Mathematical Theory of Communication. Urbana: University of Illinois Press.</strong></p></li>
<li><p>Shreiber, S. (2001) “Urn Models, Replicator Processes and Random Genetic Drift.” SIAM Journal on Applied Mathematics 61: 2148–2167.</p></li>
<li><p>Sinervo, B. and C. M. Lively (1996) “The Rock-Paper-Scissors Game and the Evolution of Alternative Male Strategies.” Nature 380: 240–243.</p></li>
<li><p>Skyrms, B. (1996) Evolution of the Social Contract. Cambridge: Cambridge University Press. Skyrms, B. (1998) “Salience and Symmetry-Breaking in the Evolution of Convention.” Law and Philosophy 17: 411–418.</p></li>
<li><p>Skyrms, B. (1999) “Stability and Explanatory Significance of Some Simple Evolutionary Models.” Philosophy of Science 67: 94–113.</p></li>
<li><p>Skyrms, B. (2000) “Evolution of Inference.” In Dynamics of Human and Primate Societies, ed.&nbsp;Tim Kohler and George Gumerman, 77–88. New York: Oxford University Press.</p></li>
<li><p>Skyrms, B. (2004) The Stag Hunt and the Evolution of Social Structure. Cambridge: Cambridge University Press.</p></li>
<li><p>Skyrms, B. (2005) “Dynamics of Conformist Bias.” Monist 88: 260–269.</p></li>
<li><p>Skyrms, B. (2007) “Dynamic Networks and the Stag Hunt: Some Robustness Considerations.” Biological Theory 2: 7–9.</p></li>
<li><p>Skyrms, B. (2009) “Evolution of Signaling Systems with Multiple Senders and Receivers.” Philosophical Transactions of the Royal Society B doi:10.1098/rstb.2008.0258, 364: 771–779.</p></li>
<li><p>Skyrms, B. (2009) “Presidential Address: Signals.” Philosophy of Science 75:489–500.</p></li>
<li><p>Skyrms, B. and R. Pemantle (2000) “A Dynamic Model of Social Network Formation.” Proceedings of the National Academy of Sciences of the USA. 97: 9340–9346 192</p></li>
<li><p>Skyrms, B. and S. L. Zabell (forthcoming) “Inventing New Signals.”</p></li>
<li><p>Slobodchikoff, C. N., J. Kiriazis, C. Fischer, and E. Creef (1991) “Semantic Information Distinguishing Individual Predators in the Alarm Calls of Gunnison’s Prairie Dogs.” Animal Behaviour 42: 713–719.</p></li>
<li><p>Smith, A. (1983) [1761] Considerations Concerning the First Formation of Languages. Reprinted in Lectures on Rhetoric and Belles Lettres, ed.&nbsp;J. C. Bryce. Oxford: Oxford University Press.</p></li>
<li><p>Snowdon, C. T. (1990) “Language Capacities of Nonhuman Animals.” Yearbook of Physical Anthropology 33: 215–243.</p></li>
<li><p>Sorabji, R. (1993) Animal Minds and Human Morals: The Origins of the Western Debate. Ithaca: Cornell University Press.</p></li>
<li><p>Stander, P. E. (1990s) “Cooperative Hunting in Lions: The Role of the Individual.” Behavioral Ecology and Sociobiology 29: 445–454.</p></li>
<li><p>Stanford, P. K. (2007) Exceeding Our Grasp. Oxford: Oxford University Press.</p></li>
<li><p><span class="citation" data-cites="steels1997synthetic">Steels (<a href="#ref-steels1997synthetic" role="doc-biblioref">1997</a>)</span> <del>Steels, L. (1997) “The Synthetic Modeling of Language Origins.” Evolution of Communication 1: 1–35.</del></p></li>
<li><p><span class="citation" data-cites="steels1998origins">Steels (<a href="#ref-steels1998origins" role="doc-biblioref">1998</a>)</span> <del>Steels, L. (1998) “The Origins of Syntax in Visually Grounded Robotic Agents.” Artificial Intelligence 103: 133–156.</del></p></li>
<li><p><span class="citation" data-cites="steels2001LanguageGames">Steels (<a href="#ref-steels2001LanguageGames" role="doc-biblioref">2001</a>)</span> <del>Steels, L (2001) “Language games for autonomous robots”. IEEE Intelligent Systems, September-October 2001:17–22</del></p></li>
<li><p>Sterelny, K. (2000) “The ‘Genetic Program’ Program: A Commentary on Maynard-Smith on Information in Biology.” Philosophy of Science 67: 195–201.</p></li>
<li><p>Sterelny, K. (2003) Thought in a Hostile World: The Evolution of Human Cognition. Oxford: Blackwell.</p></li>
<li><p>Struhsaker, T. T. (1967) “Auditory Communication among Vervet Monkeys Cercopithecus aethiops.” In Social Communication among Primates, ed.&nbsp;S.A. Altmann, 281–324. Chicago: University of Chicago Press.</p></li>
<li><p>Sugden, R. (2005) The Economics of Rights, Co-operation and Welfare (Basingstoke: Macmillan).</p></li>
<li><p>Suppes, P. and R. Atkinson (1960) Markov Learning Models for Multiperson Interactions. Palo Alto, CA: Stanford University Press.</p></li>
<li><p>Taga, M. E. and B. L. Bassler (2003) “Chemical Communication Among Bacteria.” Proceedings of the National Academy of Sciences of the USA 100 Suppl. 2, 14549–14554.</p></li>
<li><p><span class="citation" data-cites="Taylor1978ESS">(<a href="#ref-Taylor1978ESS" role="doc-biblioref">Taylor and Jonker 1978</a>)</span></p></li>
<li><p>Tempelton, C., E. Greene and K. Davis (2005) “Allometry of Alarm Calls: Black-Capped Chickadees Encode Information about Predator Size.” Science 308: 1934–1937.</p></li>
<li><p>Thorndike, E. L. (1911) Animal Intelligence. New York: Macmillan.</p></li>
<li><p>Thorndike, E. L. (1927) “The Law of Effect.” American Journal of Psychology 39: 212–222.</p></li>
<li><p><strong>Trapa, P. and M. Nowak (2000) “Nash Equilibria for an Evolutionary Language Game.” Journal of Mathematical Biology 41: 172–188.</strong></p></li>
<li><p>Vanderschraaf, P. (1998) “Knowledge, Equilibrium and Convention.” Erkenntnis 49: 337–369.</p></li>
<li><p><span class="citation" data-cites="Wagner2013Costly">(<a href="#ref-Wagner2013Costly" role="doc-biblioref">Wagner 2013</a>)</span></p></li>
<li><p><strong>Wagner, E. (2009) “Communication and Structured Correlation.” Erkenntnis doi 10.1007/s10670–009–9157–y.</strong></p></li>
<li><p>Wa¨rneryd, K. (1993) “Cheap Talk, Coordination, and Evolutionary Stability.” Games and Economic Behavior 5: 532–546.</p></li>
<li><p>Watts, A. (2001) “A Dynamic Model of Network Formation.” Games and Economic Behavior 34: 331–341.</p></li>
<li><p>Weber, R. and C. Camerer (2003) “Cultural Conflict and Merger Failure: An Experimental Approach.” Management Science 49: 400–415.</p></li>
<li><p>Wei, L. and S. Durham (1978) “The Randomized Play-the-winner Rule in Medical Trials.” Journal of the American Statistical Association 73: 840–843.</p></li>
<li><p>Young, H. P. (2009) “Learning by Trial and Error.” Games and Economic Behavior 65: 626–643. 194</p></li>
<li><p>Zabell, S. L. (1992) “Predicting the Unpredictable.” Synthese 90: 205–232.</p></li>
<li><p>Zabell, S. L. (2005) Symmetry and Its Discontents: Essays in the History of Inductive Probability. Cambridge: Cambridge University Press.</p></li>
<li><p>Zeeman, E. C. (1980) “Population Dynamics from Game Theory.” In Global Theory of Dynamical Systems, Springer Lecture Notes on Mathematics 819.</p></li>
<li><p>Zollman, K. (2005) “Talking to Neighbors: The Evolution of Regional Meaning.” Philosophy of Science 72: 69–85</p></li>
<li><p>Zuidema, W. (2003) “Optimal Communication in a Noisy and Heterogeneous Environment.” In Proceedings Lecture Notes on Artificial Intelligence v. 2801 Berlin: Springer 553–563.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-Barrett2006Numerical" class="csl-entry" role="listitem">
Barrett, Jeffrey A. 2006. <span>“Numerical Simulations of the Lewis Signaling Game: Learning Strategies, Pooling Equilibria, and the Evolution of Grammar.”</span> <em>Working Paper MBS06–09</em>.
</div><div id="ref-Barrett2009Evolution" class="csl-entry" role="listitem">
———. 2009. <span>“The Evolution of Coding in Signaling Games.”</span> <em>Theory and Decision</em> 67 (2): 223–37. <a href="https://doi.org/10.1007/s11238-007-9064-0">https://doi.org/10.1007/s11238-007-9064-0</a>.
</div><div id="ref-Barrett2007Dynamic" class="csl-entry" role="listitem">
———. 2007. <span>“Dynamic Partitioning and the Conventionality of Kinds.”</span> <em>Philosophy of Science</em> 74 (4): 527–46. <a href="https://doi.org/10.1086/524714">https://doi.org/10.1086/524714</a>.
</div><div id="ref-batali1998" class="csl-entry" role="listitem">
Batali, J. 1998. <span>“Computational Simulations of the Emergence of Grammar.”</span> In <em>Approaches to the Evolution of Language: Social and Cognitive Bases</em>, edited by J. Hurford, C. Knight, and M. Studdert-Kennedy, 405–26. Cambridge: Cambridge University Press.
</div><div id="ref-Blume2002" class="csl-entry" role="listitem">
Blume, Andreas, Douglas V. Dejong, George R. Neumann, and N. E. Savin. 2002. <span>“Learning and Communication in Sender-Receiver Games: An Econometric Investigation.”</span> <em>Journal of Applied Econometrics</em> 17 (3): 225–47. <a href="http://www.jstor.org/stable/4129228">http://www.jstor.org/stable/4129228</a>.
</div><div id="ref-hofbauer1998evolutionary" class="csl-entry" role="listitem">
Hofbauer, J., and K. Sigmund. 1998. <em>Evolutionary Games and Population Dynamics</em>. Cambridge University Press. <a href="https://books.google.co.il/books?id=Xu-H0ClCHN8C">https://books.google.co.il/books?id=Xu-H0ClCHN8C</a>.
</div><div id="ref-Smith1973LogicAnimalConflict" class="csl-entry" role="listitem">
Smith, J. Maynard, and G. Randall Price. 1973. <span>“The Logic of Animal Conflict.”</span> <em>Nature</em> 246: 15–18. <a href="https://api.semanticscholar.org/CorpusID:4224989">https://api.semanticscholar.org/CorpusID:4224989</a>.
</div><div id="ref-steels1997synthetic" class="csl-entry" role="listitem">
Steels, Luc. 1997. <span>“The Synthetic Modeling of Language Origins.”</span> <em>Evolution of Communication Journal</em> 1 (October). <a href="https://doi.org/10.1075/eoc.1.1.02ste">https://doi.org/10.1075/eoc.1.1.02ste</a>.
</div><div id="ref-steels1998origins" class="csl-entry" role="listitem">
———. 1998. <span>“The Origins of Syntax in Visually Grounded Robotic Agents.”</span> <em>Artif. Intell.</em> 103 (1–2): 133–56. <a href="https://doi.org/10.1016/S0004-3702(98)00066-6">https://doi.org/10.1016/S0004-3702(98)00066-6</a>.
</div><div id="ref-steels2001LanguageGames" class="csl-entry" role="listitem">
———. 2001. <span>“Language Games for Autonomous Robots.”</span> <em>IEEE Intelligent Systems</em> 16 (5): 16–22. <a href="https://doi.org/10.1109/MIS.2001.956077">https://doi.org/10.1109/MIS.2001.956077</a>.
</div><div id="ref-Taylor1978ESS" class="csl-entry" role="listitem">
Taylor, Peter D., and Leo B. Jonker. 1978. <span>“Evolutionarily Stable Strategies and Game Dynamics.”</span> <em>Bellman Prize in Mathematical Biosciences</em> 40: 145–56. <a href="https://api.semanticscholar.org/CorpusID:15554796">https://api.semanticscholar.org/CorpusID:15554796</a>.
</div><div id="ref-Wagner2013Costly" class="csl-entry" role="listitem">
Wagner, Elliott O. 2013. <span>“The Dynamics of Costly Signaling.”</span> <em>Games</em> 4 (2): 163–81. <a href="https://doi.org/10.3390/g4020163">https://doi.org/10.3390/g4020163</a>.
</div></div></section>
<section id="discarded-bibliographical-items" class="level2">
<h2 class="anchored" data-anchor-id="discarded-bibliographical-items">Discarded Bibliographical Items</h2>
<p>The following are not interesting to me</p>
<ul>
<li><p><del>Green, E. and T. Maegner (1998) “Red Squirrels, Tamiasciurus hudsonicus, Produce Predator-Class Specific Alarm Calls.” Animal Behavior 55: 511–518.</del></p>
<ul>
<li>animal</li>
</ul></li>
<li><p><del>Savage-Rumbaugh, S., K. McDonald, R. A. Sevkic, W. D., Hopkins, and E. Rupert (1986) “Spontaneous Symbol Acquisition and Communicative Use by Pygmy-Chimpanzees (Pan Paniscus)” Journal of Experimental Psychology: General 114: 211–235.</del></p>
<ul>
<li>animal</li>
</ul></li>
<li><p><del>Savage-Rumbaugh, S. and R. Lewin (1994) Kanzi: An Ape at the Brink of the Human Mind. New York: Wiley.</del></p>
<ul>
<li>animal</li>
</ul></li>
<li><p>Schauder, S. and B. Bassler (2001) “The Languages of Bacteria.” Genes and Development 15: 1468–1480.</p>
<ul>
<li>animal</li>
</ul></li>
<li><p>Sedley, D. (1998) Lucretius and the Transformation of Greek Wisdom. Cambridge: Cambridge University Press.</p>
<ul>
<li>classics</li>
</ul></li>
<li><p>Sedley, D. (2003a) Plato’s Cratylus. Cambridge: Cambridge University Press.</p>
<ul>
<li>classics</li>
</ul></li>
<li><p>Sedley, D. (2003b) “Lucretius and the New Empedocles.” Leeds International Classical Studies 2.4: 1–12.</p>
<ul>
<li>classics</li>
</ul></li>
<li><p>Vencl, F., B. J. Blasko, and A. D. Carlson (1994) “Flash Behavior of Female Photuris Versicolor Fireflies in Simulated Courtship and Predatory Dialogues.” Journal of Insect Behavior 7: 843–858.</p>
<ul>
<li>animal</li>
</ul></li>
<li><p>Verlinsky, A. (2005) “Epicurus and his Predecessors on the Origin of Language.” In Frede and Inwood 56–100.</p>
<ul>
<li>classics</li>
</ul></li>
<li><p>von Frisch, K. (1967) The Dance Language and Orientation of the Bees. Cambridge, MA: Harvard University Press.</p>
<ul>
<li>animal</li>
</ul></li>
<li><p><del>Vitruvius (1960) The Ten Books of Architecture Bk. 2 Ch. 1. Tr. Morris Hicky Morgan New York: Dover.</del></p>
<ul>
<li>classical motivation</li>
</ul></li>
<li><p>Wolf, D. M. and A. P. Arkin (2003) “Motifs, Modules and Games in Bacteria.” Current Opinion in Microbiology 6: 125–134.</p>
<ul>
<li>animal</li>
</ul></li>
<li><p>Ziegler, R. (2007) The Kula Ring of Bronislaw Malinowski: A Simulation Model of the Co-Evolution of an Economic and Ceremonial Exchange System. Munich: C. H. Beck Verlag.</p>
<ul>
<li>anthropological</li>
</ul></li>
<li><p>Zeigler, R. (2008) “What Makes the Kula Go Round?” Social Networks 30: 107–126.</p>
<ul>
<li>anthropological</li>
</ul></li>
<li><p>Zuberbu¨hler, K. (2000) “Referential Labeling in Diana Monkeys.” Animal Behavior 59: 917–927.</p>
<ul>
<li>animal</li>
</ul></li>
<li><p>Zuberbu¨hler, K. (2001) “Predator-Specific Alarm Calls in Campbell’s Monkeys, Cercopithecus Campbelli.” Behavioral Ecology and Sociobiology 50: 414–422.</p>
<ul>
<li>animal</li>
</ul></li>
<li><p>Zuberbu¨hler, K. (2002) “A Syntactic Rule in Forest Monkey Communication.” Animal Behavior 63: 293–299.</p>
<ul>
<li>animal</li>
</ul></li>
</ul>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Skryms {Signals} {Summary} and {Models}},
  date = {2024-05-01},
  url = {https://orenbochman.github.io//posts/2023/2023-05-01-Signals.qmd/signals-summary.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Bochman, Oren. 2024. <span>“Skryms Signals Summary and Models.”</span>
May 1, 2024. <a href="https://orenbochman.github.io//posts/2023/2023-05-01-Signals.qmd/signals-summary.html">https://orenbochman.github.io//posts/2023/2023-05-01-Signals.qmd/signals-summary.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/www\.quarto\.org\/custom");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="OrenBochman/blogComments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright 2024, Oren Bochman
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../license.html">
<p>License</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../../trademark.html">
<p>Trademark</p>
</a>
  </li>  
</ul>
    <div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","selector":".lightbox","loop":false,"openEffect":"zoom"});
window.onload = () => {
  lightboxQuarto.on('slide_before_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    const href = trigger.getAttribute('href');
    if (href !== null) {
      const imgEl = window.document.querySelector(`a[href="${href}"] img`);
      if (imgEl !== null) {
        const srcAttr = imgEl.getAttribute("src");
        if (srcAttr && srcAttr.startsWith("data:")) {
          slideConfig.href = srcAttr;
        }
      }
    } 
  });

  lightboxQuarto.on('slide_after_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    if (window.Quarto?.typesetMath) {
      window.Quarto.typesetMath(slideNode);
    }
  });

};
          </script>




<script src="../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>