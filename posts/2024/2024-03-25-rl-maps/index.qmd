---
title: "RL MindMap"
---

::: column-screen
```{mermaid}
mindmap
  root((RL))
    Main Concepts
        [MDP]
          Continuing Tasks
          Episodic Tasks
          Markov Property
        [Reward]
        [Action Values]
        [Policy]
            {{maps states to likelihood of actions}}
            Deterministic
              {{one action per state}}
            Stochastic 
              {{multiple actions per state}}
        Exploration 
        Exploitation
        [Policy Ealuation - Predication]
          {}
        Control
          {}
        Dynamic programming
            Synchronous
            Asynchronous
    Learning
        On Policy learning  
          {{Agents learn from their policy}}
        Off Policy learning
          {{Agents lean from another policy or Data}}
        Online
        Offline
        Optimistic initial values
    Math
        Bellman Equations    
            {{State-Value Function}}
            {{Action-Value Function}}
            {{State-Value Optimality Function}}
            {{Action-Value Optimality Function}}
        Policy Improvement Theorem
    Algorithms
      [Bandits]
        Epsilon greedy
        Thompson sampling
        Upper confidence bound
        Contextual
        Regret
          Follow the normalized leader
          Contractual regret
      Greedyfication
      [Policy Iteration]
      [Generalized Policy Iteration]
      [Value Iteration]
      Brute force search
      Monte Carlo
      Bootstrapping
      [Sample Based Methods]
          [Temporal Difference Learning]
          [SARSA]
          [Q-Learning]

      [Function Approximation Methods]
    Others
        Dyna 
```
:::
