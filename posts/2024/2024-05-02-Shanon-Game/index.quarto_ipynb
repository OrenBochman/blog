{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "date: 2024-05-02\n",
        "title: \"Shannon Game\"\n",
        "subtitle: \"emergent complex communications protocols\"\n",
        "categories: [signaling games, compositionality, communication protocols, emergent languages, complex signaling system]\n",
        "image: /images/cover.png\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this note I want to start to explore one of my notions of compositionality of Lewis games with another game.\n",
        "In my thinking currently consider at least three senses for compositionality\n",
        "\n",
        "1. Learning a MARL `skill` in a way that agents can repurpose it across new tasks and setting.\n",
        "\n",
        "1. Learning a domain specific signaling system that serves the needs of a game framing a lewis signaling game. Serves here means:\n",
        "    - marl  $\\arg max(Returns \\mid \\text{given expected actions of other agents})$\n",
        "    - population dynamics  $\\arg max(Progeny \\mid \\text{given expected actions of other agents})$\n",
        "    \n",
        "1. Learning a structured complex signaling of systems with features more like NLP and less like a bijection.    \n",
        "    1. Learn **semantics** by break down the state into some structure and express the full state using some aggregation of the parts.\n",
        "        - e.g. do it using a structured signal, e.g. a template with slots for signals\n",
        "            - NOT X; AND X,Y; OR X,Y \n",
        "            - X-big, X-big-er, X-big-est, X-small, X-small-er, X-small-est\n",
        "            - X-1, X-2, X-3, X-4, X-5, X-6, X-7, X-8, X-9, X-10 for inflections of action-X\n",
        "            - X-1, X-2, X-3, X-4, X-5, X-6, X-7, X-8, X-9, X-10 for inflections of noun-Y\n",
        "    1. Capability to **generalize** learned semantics to new states      \n",
        "        - map a state to a prefix category and a suffix disambiguation\n",
        "        - the prefix can be a signal in partial equilibrium solution of the lewis signaling game.\n",
        "        - the might be reuse of an existing signal or a new signal.\n",
        "    1. ability to use categories\n",
        "    1. ability to learn \n",
        "1. using a hieracial model also seems to be an option.\n",
        "\n",
        "\n",
        "The stating point:\n",
        "\n",
        "> One can represent a communication protocol as a decision tree.  \n",
        "\n",
        "So in the abstraction one way that a emergent communications might arise is using three modules:\n",
        "\n",
        "\n",
        "1. Lewis signaling games to model different coordination tasks like\n",
        "    1. the alphabet and or basic signals\n",
        "    1. a lexicon built on the alphabet\n",
        "    1. a grammar to support composition of lexemes into sentences that can express complex states.\n",
        "    1. coordinating of decision rules \n",
        "        - the sender get a decision trees that map states to a string in the alphabet this is called an encoder.\n",
        "        - receiver get a decision tree that maps the string to a state space, this is called a decoder\n",
        "    The encoder-decoder might just serialize the state or it might do arbitrary complex transformations that include a compression, error detection and correction and even analysis of the agents environment required to pick the best action. However at this point we want a minimal that is restricted to serialization and deserialization, compression using a prefix code and perhaps error detection and correction.\n",
        "    1. coordinating grammar rules. We may also need a grammar to support parsing of complex signals. In a simple case we might be processing mathematical expression and need to ensure that the formulas are well formed. In more complex scenarios we may need enforce selectional restrictions and subcategorization frames. In the MVP I imagine a simple rule that allows nested clauses. \n",
        "    1. one idea for using lewis games to coordinate the decision rules if it can enumerate all possible trees and let agents pick one. Other more complex scenarios are possible too.\n",
        "\n",
        "2. a Shannon game to model the communication of information between agents in which they learn a shared communication protocol with using error detection and correction capabilities.\n",
        "\n",
        "3. a Chomsky game to model development of a shared grammar for complex signals. \n",
        "    - A trivial version is to use concatenation of signals to form new signals.\n",
        "    - A more powerful version is to use an ordered vector of signals. Using a simple prefix code this allows creation of a powerful morphology.  \n",
        "    - Probably the minimalist option though is to use a rule that allows nested clauses. If the tree allows recursion we get what Humboldt called the infinite use of finite means.\n",
        "    - Just having a recursive rule though might over-generate and we might require additional means to restrict the grammar by way of selectional restrictions^[restrict sematic roles]  and subcategorization frames ^[might restrict phrase elements to morphological categories in the lexicon with suitable features.].\n",
        "\n",
        "\n",
        "\n",
        "## Shannon Game\n",
        "\n",
        "Sharon games are about emergence of randomized communication protocols.\n",
        "\n",
        "A randomized communication protocol is a probability distribution over the set of possible\n",
        "deterministic communication protocols.\n",
        "\n",
        "We can model any deterministic communication protocol as a pair of decision rees, one for the sender and one for the receiver. The sender's decision tree maps each possible message to a signal, and the receiver's decision tree maps each possible signal to a message. \n",
        "\n",
        "\n",
        "Messages that the sender can send. \n",
        "\n",
        "The sender samples a message from this distribution and sends it to the receiver. The receiver then uses a decoding function to map the received message back to the original signal. The goal of the game is for the sender and receiver to coordinate on a communication protocol that maximizes their payoff, which is typically based on the accuracy of message transmission and reception.\n",
        "\n",
        "It is a protocol that uses randomness to encode and decode messages. \n",
        "\n",
        "This randomness can be used to introduce redundancy in the message, which can help in error detection and correction.\n"
      ],
      "id": "db7e009c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "class CommunicationAgent:\n",
        "    def __init__(self, num_strategies):\n",
        "        self.num_strategies = num_strategies\n",
        "        self.q_table = np.zeros((num_strategies, num_strategies))\n",
        "        self.learning_rate = 0.1\n",
        "        self.discount_factor = 0.9\n",
        "        self.epsilon = 0.1\n",
        "    \n",
        "    def choose_strategy(self):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.num_strategies)\n",
        "        else:\n",
        "            return np.argmax(self.q_table.sum(axis=1))\n",
        "    \n",
        "    def update_q_values(self, sender_strategy, receiver_strategy, reward):\n",
        "        max_future_q = np.max(self.q_table[receiver_strategy])\n",
        "        current_q = self.q_table[sender_strategy, receiver_strategy]\n",
        "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_future_q - current_q)\n",
        "        self.q_table[sender_strategy, receiver_strategy] = new_q\n",
        "\n",
        "# Simulation parameters\n",
        "num_strategies = 8\n",
        "num_iterations = 10000\n",
        "\n",
        "# Initialize agents\n",
        "alice = CommunicationAgent(num_strategies)\n",
        "bob = CommunicationAgent(num_strategies)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    sender_strategy = alice.choose_strategy()\n",
        "    receiver_strategy = bob.choose_strategy()\n",
        "    \n",
        "    # Simulate message transmission and reception with noise\n",
        "    # This is a placeholder for actual encoding/decoding logic\n",
        "    success = np.random.rand() < 0.8  # Assume 80% chance of success\n",
        "    \n",
        "    reward = 1 if success else -1\n",
        "    alice.update_q_values(sender_strategy, receiver_strategy, reward)\n",
        "    bob.update_q_values(receiver_strategy, sender_strategy, reward)\n",
        "\n",
        "print(\"Alice's Q-Table:\\n\", alice.q_table)\n",
        "print(\"Bob's Q-Table:\\n\", bob.q_table)"
      ],
      "id": "fb2a4046",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example illustrates a basic game-theoretic approach where the sender and receiver iteratively learn better strategies for encoding and decoding messages over a noisy channel. The reinforcement learning framework allows both parties to adapt and improve their protocols, enhancing the reliability of communication over time. This model can be extended and refined to include more sophisticated encoding/decoding techniques and more complex noise models.\n"
      ],
      "id": "2950d30b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from mesa import Agent, Model\n",
        "from mesa.time import RandomActivation\n",
        "from mesa.datacollection import DataCollector\n",
        "import numpy as np\n",
        "\n",
        "def hamming_distance(a, b):\n",
        "    return np.sum(a != b) / len(a)\n",
        "\n",
        "class Sender(Agent):\n",
        "    def __init__(self, unique_id, model):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.protocol = self.random_protocol()\n",
        "    \n",
        "    def random_protocol(self):\n",
        "        # Define a random protocol for encoding (Identity function for now)\n",
        "        return lambda msg: msg  \n",
        "    \n",
        "    def step(self):\n",
        "        self.model.original_message = np.random.randint(0, 2, self.model.message_length)  # Generate a binary message\n",
        "        encoded_message = self.protocol(self.model.original_message)\n",
        "        self.model.sent_message = encoded_message\n",
        "\n",
        "class Receiver(Agent):\n",
        "    def __init__(self, unique_id, model):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.protocol = self.random_protocol()\n",
        "    \n",
        "    def random_protocol(self):\n",
        "        # Define a random protocol for decoding (Identity function for now)\n",
        "        return lambda msg: msg  \n",
        "    \n",
        "    def step(self):\n",
        "        if self.model.sent_message is None:\n",
        "            return  # **Avoid processing before sender has sent a message**\n",
        "        \n",
        "        # Convert to NumPy array to ensure bitwise operations work\n",
        "        noisy_message = np.array(self.model.sent_message) ^ np.random.binomial(1, self.model.error_rate, self.model.message_length)\n",
        "        recovered_message = self.protocol(noisy_message)\n",
        "        self.model.recovered_message = recovered_message\n",
        "        self.evaluate_performance()\n",
        "    \n",
        "    def evaluate_performance(self):\n",
        "        original_message = self.model.original_message\n",
        "        recovered_message = self.model.recovered_message\n",
        "        distance = hamming_distance(original_message, recovered_message)\n",
        "        self.model.payoff += self.model.recovery_payoff(distance)\n",
        "        self.model.payoff += self.model.length_payoff(len(recovered_message))\n",
        "        self.model.payoff += self.model.early_recovery_payoff(self.model.current_step)\n",
        "\n",
        "class NoisyChannelModel(Model):\n",
        "    def __init__(self, message_length=10, error_rate=0.1, max_steps=100):\n",
        "        self.message_length = message_length\n",
        "        self.error_rate = error_rate\n",
        "        self.current_step = 0\n",
        "        self.max_steps = max_steps\n",
        "        self.payoff = 0\n",
        "        self.running = True  # Fix: Initialize running status\n",
        "        \n",
        "        self.schedule = RandomActivation(self)\n",
        "        \n",
        "        self.original_message = np.random.randint(0, 2, self.message_length)  # Initialize first message\n",
        "        self.sent_message = None\n",
        "        self.recovered_message = None\n",
        "        \n",
        "        sender = Sender(1, self)\n",
        "        receiver = Receiver(2, self)\n",
        "        self.schedule.add(sender)\n",
        "        self.schedule.add(receiver)\n",
        "        \n",
        "        self.datacollector = DataCollector(\n",
        "            model_reporters={\"Payoff\": \"payoff\"}\n",
        "        )\n",
        "    \n",
        "    def recovery_payoff(self, distance):\n",
        "        return 1 - distance\n",
        "    \n",
        "    def length_payoff(self, length):\n",
        "        return 1 / length if length > 0 else 0  # Avoid division by zero\n",
        "    \n",
        "    def early_recovery_payoff(self, step):\n",
        "        return (self.max_steps - step) / self.max_steps\n",
        "    \n",
        "    def step(self):\n",
        "        self.current_step += 1\n",
        "        self.schedule.step()\n",
        "        self.datacollector.collect(self)\n",
        "        if self.current_step >= self.max_steps:\n",
        "            self.running = False  # Stop the simulation\n",
        "\n",
        "# Run the model\n",
        "model = NoisyChannelModel()\n",
        "while model.running:\n",
        "    model.step()\n",
        "\n",
        "# Retrieve results\n",
        "results = model.datacollector.get_model_vars_dataframe()\n",
        "print(results)"
      ],
      "id": "5bd0253e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "so this is a variant that uses a noisy channel model to simulate the transmission of messages between a sender and receiver. The agents have protocols for encoding and decoding messages, and the model tracks the performance of the communication system based on the accuracy of message recovery, message length, and early recovery. This example demonstrates how to model and analyze the performance of communication systems in the presence of noise and other challenges.\n",
        "\n",
        "What we don't have is a way to pick different protocols or to improve them over time. \n",
        "\n",
        "I would break this down into a few steps:\n",
        "1. identify the environmental factors that would encourage the agents to evolve\n",
        "   diverse and efficient transmission protocols.\n",
        "   a. noisy channels\n",
        "   b. limited bandwidth\n",
        "   c. limited computational resources\n",
        "   d. time constraints\n",
        "   e. risks of predation.\n",
        "   \n",
        "2. allow agents randomly generate candidate protocols and evaluate their performance.\n"
      ],
      "id": "2c6ca4fb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def random_protocol():\n",
        "    # Define a random protocol for encoding/decoding\n",
        "    return lambda msg: np.random.randint(0, 2, len(msg))\n",
        "\n",
        "# which  would be used as follows\n",
        "\n",
        "class Sender(Agent):\n",
        "    def __init__(self, unique_id, model):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.protocol = random_protocol()\n",
        "    \n",
        "    def step(self):\n",
        "        message = np.random.randint(0, 2, self.model.message_length)\n",
        "        encoded_message = self.protocol(message)\n",
        "        self.model.sent_message = encoded_message"
      ],
      "id": "1f7ac628",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This could be done by introducing reinforcement learning techniques to allow the agents to adapt and learn better encoding/decoding strategies based on feedback from the environment. This would enable the agents to optimize their protocols for improved communication performance in noisy channels.\n"
      ],
      "id": "0f280ba0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}