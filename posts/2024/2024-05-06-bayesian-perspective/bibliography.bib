
@article{Barrett2006Numerical,
	author = {Jeffrey A. Barrett},
	journal = {Working Paper MBS06–09},
	title = {Numerical Simulations of the Lewis Signaling Game: Learning Strategies, Pooling Equilibria, and the Evolution of Grammar},
	year = {2006},
	abstract = {David Lewis (1969) introduced sender-receiver games as a way of investigating how meaningful language might evolve from initially random signals. In this report I investigate the conditions under which Lewis signaling games evolve to perfect signaling systems under various learning dynamics. While the 2-state/2- term Lewis signaling game with basic urn learning always approaches a signaling system, I will show that with more than two states suboptimal pooling equilibria can evolve. Inhomogeneous state distributions increase the likelihood of pooling equilibria, but learning strategies with negative reinforcement or certain sorts of mutation can decrease the likelihood of, and even eliminate, pooling equilibria. Both Moran and APR learning strategies (Bereby-Meyer and Erev 1998) are shown to promote successful convergence to signaling systems. A model is presented that illustrates how a language that codes state-act pairs in an order-based grammar might evolve in the context of a Lewis signaling game. The terms, grammar, and the corresponding partitions of the state space co-evolve to generate a language whose structure appears to reflect canonical natural kinds. The evolution of these apparent natural kinds, however, is entirely in service of the rewards that accompany successful distinctions between the sender and receiver. Any metaphysics grounded on the structure of a natural language that evolved in this way would track arbitrary, but pragmatically useful distinctions.}

}

@article{Barrett2007Dynamic,
	author = {Jeffrey A. Barrett},
	doi = {10.1086/524714},
	journal = {Philosophy of Science},
	number = {4},
	pages = {527--546},
	publisher = {University of Chicago Press},
	title = {Dynamic Partitioning and the Conventionality of Kinds},
	volume = {74},
	year = {2007}
}

@article{barrett2009evolution,
	author = {Jeffrey A. Barrett},
	doi = {10.1007/s11238-007-9064-0},
	journal = {Theory and Decision},
	number = {2},
	pages = {223--237},
	publisher = {Springer},
	title = {The Evolution of Coding in Signaling Games},
	volume = {67},
	year = {2009}
}

% - MIE alg for XAI
@incollection{skyrms2010signals,
    author = {Skyrms, Brian},
    isbn = {9780199580828},
    title = "{14512 Complex Signals and Compositionality}",
    booktitle = "{Signals: Evolution, Learning, and Information}",
    publisher = {Oxford University Press},
    year = {2010},
    month = {04},
    abstract = "{This chapter focuses on an earlier point in the evolution of signaling. It considers how one might come to have — in the most primitive way — a complex signal composed of simple signals. This is done with the smallest departure possible from signaling models that have been previously examined in this book.}",
    doi = {10.1093/acprof:oso/9780199580828.003.0013},
    url = {https://doi.org/10.1093/acprof:oso/9780199580828.003.0013},
}

@incollection{skyrms2010signalsCh12,
    author = {Skyrms, Brian},
    isbn = {9780199580828},
    title = "{14512 Complex Signals and Compositionality}",
    booktitle = "{Signals: Evolution, Learning, and Information}",
    publisher = {Oxford University Press},
    year = {2010},
    month = {04},
    abstract = "{This chapter focuses on an earlier point in the evolution of signaling. It considers how one might come to have — in the most primitive way — a complex signal composed of simple signals. This is done with the smallest departure possible from signaling models that have been previously examined in this book.}",
    doi = {10.1093/acprof:oso/9780199580828.003.0013},
    url = {https://doi.org/10.1093/acprof:oso/9780199580828.003.0013},
    eprint = {https://academic.oup.com/book/0/chapter/143895157/chapter-ag-pdf/45018345/book\_3092\_section\_143895157.ag.pdf},
}


@book{sutton2018reinforcement,
  title = {Reinforcement Learning: An Introduction},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second},
  keywords = {rl, Rinforcement Learning},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  added-at = {2019-07-13T10:11:53.000+0200},
  url = {http://incompleteideas.net/book/the-book-2nd.html}
}

@misc{rita2022emergent,
      title={Emergent Communication: Generalization and Overfitting in Lewis Games}, 
      author={Mathieu Rita and Corentin Tallec and Paul Michel and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux and Florian Strub},
      year={2022},
      eprint={2209.15342},
      archivePrefix={arXiv},
      primaryClass={cs.MA}
}

@misc{rita2022role,
      title={On the role of population heterogeneity in emergent communication}, 
      author={Mathieu Rita and Florian Strub and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux},
      year={2022},
      eprint={2204.12982},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      keywords={Emergent communication, population heterogeneity, generalization, overfitting, Lewis games},
}

@book{lewis1969convention,
	address = {Cambridge, MA, USA},
	author = {David Kellogg Lewis},
	editor = {},
	publisher = {Wiley-Blackwell},
	title = {Convention: A Philosophical Study},
	year = {1969}
}


@article{Bereby1998Loser,
title = {On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain},
journal = {Journal of Mathematical Psychology},
volume = {42},
number = {2},
pages = {266-286},
year = {1998},
issn = {0022-2496},
doi = {https://doi.org/10.1006/jmps.1998.1214},
url = {https://www.sciencedirect.com/science/article/pii/S0022249698912147},
author = {Yoella Bereby-Meyer and Ido Erev},
abstract = {One of the main difficulties in the development of descriptive models of learning in repeated choice tasks involves the abstraction of the effect of losses. The present paper explains this difficulty, summarizes its common solutions, and presents an experiment that was designed to compare the descriptive power of the specific quantifications of these solutions proposed in recent research. The experiment utilized a probability learning task. In each of the experiment's 500 trials participants were asked to predict the appearance of one of two colors. The probabilities of appearance of the colors were different but fixed during the entire experiment. The experimental manipulation involved an addition of a constant to the payoffs. The results demonstrate that learning in the loss domain can be faster than learning in the gain domain; adding a constant to the payoff matrix can affect the learning process. These results are consistent with Erev & Roth's (1996) adjustable reference point abstraction of the effect of losses, and violate all other models.}
}