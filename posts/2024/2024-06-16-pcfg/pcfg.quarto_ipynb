{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"grammar induction\"\n",
        "date: 2024-06-16\n",
        "draft: true\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "First, let's define the text input and initialize some necessary libraries.\n"
      ],
      "id": "dc559308"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sample text input (list of sentences)\n",
        "text_input = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"the lazy dog lies in the sun\",\n",
        "    \"the quick brown fox is very quick\"\n",
        "]\n",
        "\n",
        "class PCFG:\n",
        "    def __init__(self):\n",
        "        self.rules = defaultdict(list)\n",
        "        self.probabilities = defaultdict(float)\n",
        "\n",
        "    def add_rule(self, lhs, rhs, probability):\n",
        "        self.rules[lhs].append(tuple(rhs))  # Convert list to tuple\n",
        "        self.probabilities[(lhs, tuple(rhs))] = probability  # Convert list to tuple\n",
        "\n",
        "    def get_rules(self):\n",
        "        return self.rules\n",
        "\n",
        "    def get_probabilities(self):\n",
        "        return self.probabilities\n",
        "\n",
        "pcfg = PCFG()\n",
        "\n",
        "class PCFGEnvironment:\n",
        "    def __init__(self, texts):\n",
        "        self.texts = texts\n",
        "        self.current_text = 0\n",
        "        self.state = self.initialize_state()\n",
        "        self.actions = self.define_actions()\n",
        "        self.reward = 0\n",
        "\n",
        "    def initialize_state(self):\n",
        "        # Initialize the state based on the current text\n",
        "        pos_tags = nltk.pos_tag(self.texts[self.current_text].split())\n",
        "        return pos_tags\n",
        "\n",
        "    def define_actions(self):\n",
        "        # Define possible actions: create rules from POS tags\n",
        "        actions = []\n",
        "        for i in range(len(self.state)):\n",
        "            for j in range(i + 1, len(self.state) + 1):\n",
        "                actions.append(self.state[i:j])\n",
        "        return actions\n",
        "\n",
        "    def step(self, action):\n",
        "        # Apply action and calculate reward\n",
        "        lhs = action[0][1]\n",
        "        rhs = [word for word, tag in action]\n",
        "        pcfg.add_rule(lhs, rhs, random.uniform(0.1, 1.0))\n",
        "        self.reward = self.calculate_reward()\n",
        "        return self.state, self.reward\n",
        "\n",
        "    def calculate_reward(self):\n",
        "        # Define a reward function for creating a valid rule\n",
        "        return 1.0  # Simplified reward for demonstration purposes\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset environment for the next sentence\n",
        "        self.current_text = (self.current_text + 1) % len(self.texts)\n",
        "        self.state = self.initialize_state()\n",
        "        self.actions = self.define_actions()\n",
        "        self.reward = 0\n",
        "\n",
        "env = PCFGEnvironment(text_input)\n",
        "\n",
        "class RLAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "        self.alpha = 0.1  # Learning rate\n",
        "        self.gamma = 0.9  # Discount factor\n",
        "        self.epsilon = 0.1  # Exploration rate\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state_key = tuple(state)\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(self.env.actions)\n",
        "        else:\n",
        "            return max(self.env.actions, key=lambda action: self.q_table[state_key][tuple(map(tuple, action))])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        state_key = tuple(state)\n",
        "        next_state_key = tuple(next_state)\n",
        "        action_key = tuple(map(tuple, action))\n",
        "        best_next_action = max(self.q_table[next_state_key], key=self.q_table[next_state_key].get)\n",
        "        td_target = reward + self.gamma * self.q_table[next_state_key][best_next_action]\n",
        "        self.q_table[state_key][action_key] += self.alpha * (td_target - self.q_table[state_key][action_key])\n",
        "\n",
        "    def train(self, episodes):\n",
        "        for episode in range(episodes):\n",
        "            for _ in range(len(self.env.texts)):\n",
        "                state = self.env.initialize_state()\n",
        "                while True:\n",
        "                    action = self.choose_action(state)\n",
        "                    next_state, reward = self.env.step(action)\n",
        "                    self.update_q_table(state, action, reward, next_state)\n",
        "                    state = next_state\n",
        "                    if not self.env.actions:  # End of episode condition\n",
        "                        break\n",
        "                self.env.reset()\n",
        "\n",
        "agent = RLAgent(env)\n",
        "agent.train(100)\n",
        "\n",
        "# Display learned grammar rules and probabilities\n",
        "rules = pcfg.get_rules()\n",
        "probabilities = pcfg.get_probabilities()\n",
        "print(\"Learned PCFG Rules:\")\n",
        "for lhs, rhs_list in rules.items():\n",
        "    for rhs in rhs_list:\n",
        "        print(f\"{lhs} -> {' '.join(rhs)} [{probabilities[(lhs, rhs)]:.2f}]\")"
      ],
      "id": "28842a41",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}