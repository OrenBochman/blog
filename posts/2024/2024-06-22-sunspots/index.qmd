---
title: "Simplifing NN by soft weight sharing"
date: 2022-06-22
---

just a few ideas from this paper: [@Nowlan1991Simplifing](https://www.cs.utoronto.ca/~hinton/absps/sunspots.pdf) 

pruning excess weights from a neural network by
1. keep track of the importance of each unit and drop the least important ones - could work well in RL where we keep track of the importance of each state/action/features
   we might also care more about proretizing certain states and discarding others. c.f. (Mozer and Smolensky 1989)
2. Use second order gradient information to estimate network sensitivity to weight changes and purne based on that.

$$
cost = data-misfit + \lambda complexity
$$


penalties:

$$
\sum_{i} w_i^2 \qquad \text{(L2 penalty)}
$$
the authors give two ways to think about this penalty:

1. weight decay - the penalty is a function of the weights themselves based on (Plaut et al. 1986)
2. the complexity from a Bayesian perspective - is a negative log density of the weights under a Gaussian prior. 
This is equivalent to the L2 penalty. (MacKay 1991) showed that we can do better with
$$
p(w) =\sum_i \lambda_i \sum_{j} w_j^2 \qquad \text{(L2 penalty)}
$$
where i is the layer and j is the weight in that layer i.e. different layers have different penalties. This is equivalent to a Gaussian prior with different variances for each layer.

un


$$
\sum_{i} \delta(w{_i\ne 0}) \qquad \text{(L0 penalty)}
$$
$$
p(w) = \sum_{i} \delta(w_i \ne 0) + \lambda \sum_{i} w_i^2
$$
- https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/