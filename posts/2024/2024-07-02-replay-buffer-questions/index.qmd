---
title: "replay buffer questions"
date: "2024-07-02"
image: /images/cover.png

---

## Replay Buffer

1. for continuous environment we should think about **coverage**.
  - given a paramertrization of the value function, for a level of generalization/discrimination we 
    get an induced set of features. Is some set of experiences sufficent to do prediction or control.
  - if we have an estimate of the coverage can we use it to place a bound on the error of the value function.
  - can we do better if we also have an estimate $\mu(s)$ of the importance/long term probability of the states ?
2. Traces present a highly correlated view of the state space. 
  - How much do we need to wory about this.
  

1. does replay buffer violate markov state.?
  - according to [Shirli Di-Castro Shashua](https://www.linkedin.com/in/shirli-di-castro/) 
    - [Analysis of Stochastic Processes through Replay Buffers](https://proceedings.mlr.press/v162/di-castro22a/di-castro22a.pdf)
    - [Sim and Real: Better Together](https://arxiv.org/abs/2110.00445)
    - the storage operation preserves the markov property
    - the sampling operation preserves the markov property
    - the mean operation om the replay buffer violates the markov property... 
2. can reduce correlation between samples ?
3. can we be more stategic about what we keep in the RB
  - say we have a key using a $hash[\delta(state), action]$ neighbourhood
    - we can use the key to decide if to insert/replace the current buffer
    - we can use it to decide what to discard
  - we can use the buffer to estimate mu(s)
    - might also have more info like states we did not insert or deleted.
    - if we also have mu(mu) - the state importance to decide what to keep
  - do we prefer complete recent traces or many partial traces.

4. Can we use options/skills to orgenize the buffer more effectively ?
  - we should aim to keep full options traces in the buffer
  - keep traces in & out or options.
  
  - before and after the options.

Think of the four room environment - there are different options to get from one room to another.
they are composable. Once we have good coverage entry into the op


## Ergodicity 

1. in an environment is a maze and I have a one way door dividing the left side from the right parts of the maze.
   is this environment ergodic ?
2. If not how come we can still learn the optimal policy ?

   
interchip dotan castro - sim to real

## Replay buffers - 
  - storing sequence of states 
  - State action state 




PMDPs
