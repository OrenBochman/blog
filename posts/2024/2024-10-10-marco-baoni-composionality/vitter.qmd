---
title: "Vitter Algorithm"
subtitle: "Paper Review"
date: 2025-01-01
categories: [review,compositionality,neural networks,signaling systems,language evolution]
keywords: 
    compositionality
    naive compositionality
    language emergence
    deep learning
    neural networks
    signaling systems 
    emergent languages
    topographic similarity
    positional disentanglement
    bag-of-symbols disentanglement
    information gap disentanglement    
bibliography: ./bibliography.bib
---



```{python}
import heapq

class Node:
    def __init__(self, char, freq):
        self.char = char
        self.freq = freq
        self.left = None
        self.right = None

    def __lt__(self, other):
        return self.freq < other.freq

def build_huffman_tree(chars_freq):
    """
    Builds the Huffman tree for given character frequencies.

    Args:
        chars_freq: A dictionary of characters and their frequencies.

    Returns:
        The root of the Huffman tree.
    """
    nodes = []
    for char, freq in chars_freq.items():
        heapq.heappush(nodes, Node(char, freq))

    while len(nodes) > 1:
        left = heapq.heappop(nodes)
        right = heapq.heappop(nodes)
        parent = Node(None, left.freq + right.freq)
        parent.left = left
        parent.right = right
        heapq.heappush(nodes, parent)

    return nodes[0]

def encode_char(root, char, code=''):
    """
    Encodes a character using Huffman codes.

    Args:
        root: The root of the Huffman tree.
        char: The character to encode.
        code: The current code (initially empty).

    Returns:
        The Huffman code for the character.
    """
    if root is None:
        return ''

    if root.char == char:
        return code

    left_code = encode_char(root.left, char, code + '0')
    if left_code != '':
        return left_code

    right_code = encode_char(root.right, char, code + '1')
    return right_code

def decode_char(root, code):
    """
    Decodes a Huffman code to get the character.

    Args:
        root: The root of the Huffman tree.
        code: The Huffman code to decode.

    Returns:
        The decoded character.
    """
    current = root
    for bit in code:
        if bit == '0':
            current = current.left
        else:
            current = current.right

    if current.char is not None:
        return current.char

def encode_message(root, message):
    """
    Encodes a message using Huffman codes.

    Args:
        root: The root of the Huffman tree.
        message: The message to encode.

    Returns:
        The encoded message.
    """
    encoded_message = ''
    for char in message:
        encoded_message += encode_char(root, char)
    return encoded_message

def decode_message(root, encoded_message):
    """
    Decodes a Huffman-encoded message.

    Args:
        root: The root of the Huffman tree.
        encoded_message: The encoded message.

    Returns:
        The decoded message.
    """
    decoded_message = ''
    current = root
    for bit in encoded_message:
        if bit == '0':
            current = current.left
        else:
            current = current.right

        if current.char is not None:
            decoded_message += current.char
            current = root

    return decoded_message

# Example usage
chars_freq = {'a': 45, 'b': 13, 'c': 12, 'd': 16, 'e': 9, 'f': 5}
root = build_huffman_tree(chars_freq)

message = "abcdef"
encoded_message = encode_message(root, message)
print("Encoded message:", encoded_message)

decoded_message = decode_message(root, encoded_message)
print("Decoded message:", decoded_message)
```

```{python}
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pettingzoo
from pettingzoo.utils.env import AECEnv
from pettingzoo.utils import wrappers
from pettingzoo.utils.agent_selector import agent_selector


class LewisSignalingEnv(AECEnv):
    metadata = {"render_modes": ["human"], "name": "lewis_signaling_v0"}

    def __init__(self, num_signals=3, num_states=3, max_cycles=100):
        super().__init__()
        self.possible_agents = ["sender", "receiver"]
        self.agent_name_mapping = dict(zip(self.possible_agents, list(range(len(self.possible_agents)))))
        self._agent_ids = list(range(len(self.possible_agents)))
        self.num_signals = num_signals
        self.num_states = num_states
        self.max_cycles = max_cycles
        self.observation_spaces = {
            "sender": spaces.Discrete(self.num_states),  # Sender observes the state
            "receiver": spaces.Discrete(self.num_signals),  # Receiver observes the signal
        }
        self.action_spaces = {
            "sender": spaces.Discrete(self.num_signals),  # Sender sends a signal
            "receiver": spaces.Discrete(self.num_states),  # Receiver guesses the state
        }
        self.state = None
        self.signal = None
        self.cycles = 0

    def observation_space(self, agent):
        return self.observation_spaces[agent]

    def action_space(self, agent):
        return self.action_spaces[agent]

    def reset(self, seed=None, options=None):
        self.agents = self.possible_agents[:]
        self.rewards = {agent: 0 for agent in self.agents}
        self._cumulative_rewards = {agent: 0 for agent in self.agents}
        self.terminations = {agent: False for agent in self.agents}
        self.truncations = {agent: False for agent in self.agents}
        self.infos = {agent: {} for agent in self.agents}
        self.state = np.random.randint(self.num_states)
        self.signal = None
        self.cycles = 0
        self._agent_selector = agent_selector(self.agents)
        self.agent_selection = self._agent_selector.next()

    def step(self, action):
        if self.terminations["sender"] or self.terminations["receiver"]:
            return

        current_agent = self.agent_selection

        if current_agent == "sender":
            self.signal = action
            self.agent_selection = self._agent_selector.next()
        elif current_agent == "receiver":
            guess = action
            if guess == self.state:
                reward = 1
            else:
                reward = 0
            self.rewards["sender"] = reward
            self.rewards["receiver"] = reward

            if self._agent_selector.is_last(): # Check if it's the last agent
                self.cycles += 1
                if self.cycles >= self.max_cycles:
                    for agent in self.agents:
                        self.truncations[agent] = True
                self.state = np.random.randint(self.num_states)
                self._agent_selector.reinit(self.agents)
            else:
                self.agent_selection = self._agent_selector.next()

        self._clear_rewards()


    def close(self):
        if hasattr(self, "_agent_selector"):  # Check if _agent_selector exists
            del self._agent_selector   #This is the most important addition to this code.


    def env(**kwargs):
        env = LewisSignalingEnv(**kwargs)

        print("PettingZoo version:", pettingzoo.__version__)

        # Correct order: OrderEnforcingWrapper OUTSIDE AssertOutOfBoundsWrapper
        if pettingzoo.__version__ >= "1.18.1":
            env = wrappers.OrderEnforcingWrapper(env)
        else:
            env = wrappers.order_enforcing(env)
        
        env = wrappers.AssertOutOfBoundsWrapper(env) # AssertOutOfBounds INSIDE OrderEnforcing

        return env

```

```{python}
# --- Main execution in the notebook ---
num_episodes = 1000
total_rewards = {"sender": 0, "receiver": 0}

env_instance = LewisSignalingEnv.env(num_signals=3, num_states=3) # Create the environment instance

for episode in range(num_episodes):
    env_instance.reset()
    for agent in env_instance.agent_iter():
        observation, reward, termination, truncation, info = env_instance.last()
        action = env_instance.action_space(agent).sample()
        env_instance.step(action)
        total_rewards[agent] += reward
        if termination or truncation:
            break

mean_rewards = {agent: total_rewards[agent] / num_episodes for agent in total_rewards}
print(f"Mean rewards over {num_episodes} episodes:")
print(f"Sender: {mean_rewards['sender']}")
print(f"Receiver: {mean_rewards['receiver']}")

```