{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "date: 2024-01-04\n",
        "title: \"Understanding Emergent Languages\"\n",
        "keywords: \n",
        "    compositionality\n",
        "    naive compositionality\n",
        "    language emergence\n",
        "    deep learning\n",
        "    neural networks\n",
        "    signaling systems \n",
        "    emergent languages\n",
        "    topographic similarity\n",
        "    positional disentanglement\n",
        "    bag-of-symbols disentanglement\n",
        "    information gap disentanglement    \n",
        "bibliography: ./bibliography.bib\n",
        "---\n",
        "\n",
        "\n",
        "Okay, today I've been rethinking Complex signaling systems and thier Lewis Signaling Game. As I make slow progress on this I can say that I am considering a number of research questions that seem trivial for a simple two-state two-signal Lewis signaling game but become important as we delve deeper. \n",
        "Within the complex games we are dealing with sequences of signals based on some alphabet. Here are a few of the research questions that raise their heads as we consider the complexity of the signaling systems:\n",
        "\n",
        "\n",
        "## Research questions {#sec-research-questions}\n",
        "\n",
        "1. How do semantics arise for sequences?^[this is a natural question for anyone who has stated learning a new language that is from a different language family, and in this case we may be better equipped to answer it]\n",
        "    - Does **Simple lexicon^[semantics for atomic symbols, it kind of does in propositional calculus]** suffice ? \n",
        "    - Is an aggregation rule sufficient to create semantics? Does a simple lexicon help?\n",
        "    - What aggregation types lead to compositionality, entanglement and disentanglement of meaning?\n",
        "    - Is having a grammar^[rules imposing structural correctness on a sequence] sufficient to get a useful syntax sufficient?\n",
        "    - can we rely on syntax for semantics?\n",
        "    - if not how do we get a general framework for semantics?\n",
        "    - To what extents do pre-linguistic structure determine the ability of agents \n",
        "    - Will agents become great if they have greatness thrust upon them? i.e. If they get a nice signaling system early on will the be able to extend it or will it wither away^[c.f. erosion of the verb in [romance languages](https://en.wikipedia.org/wiki/Romance_verbs)]? ^[This seems more of an algorithmic question about how we reinforce sub-state coding in the algorithms that is can do generalization. c.f [section on planning](@sec-planning)]    \n",
        "1. Are there subset of sequences that are:\n",
        "    - Easier acquired by one or multiple generations of learners? ;\n",
        "    - Better suited for communication between agents ? Perhaps exhibiting greater resilience to errors, reducing risk for certain signal, handle saliency, able to compress sequence to make best use of the comms channel?\n",
        "    - More able to generalize to new or unseen states (moran process for the alphabet)\n",
        "    - a better match to represent the states.^[can we leverage representation theory to handle symmetries within the states?] \n",
        "    - easier to interpret/translate/transfer    \n",
        "1. Emergent languages may be subject to selection pressure when the Lewis game is composed with some external framing game. What choices are more conducive for agents to lean quickly and communicate effectively using **robust** learning . I.e. algorithms that lead to more stable language whose lecion, grammar and semantic persist over time?\n",
        "    - Do we need to tinker with rewards structure to realign the incentives of the agents?^[can agents that are at odds evolve a signaling system or will deception lead to the collapse of the signaling system to prefectly pooling equilibria?]\n",
        "    - Can these be grounded within the external framing game? ^[will we get the full benefits of signaling systems in our framing game to re-shape the over-all equilibria]\n",
        "    - resilience to distributional shifts in the states distribution. (e.g. changes in the framing game)\n",
        "    - resilience to co-adaptation between agents. (persistence of the lexicon, grammar, and semantics).\n",
        "1. More crucially, what interventions can we make as designers can to encourage quick emergence of a signaling system that is conducive to perfect communication, fast learning and generalization?\n",
        "1. Can we impose additional structure on the sequences (e.g. a formal grammar that decides a sequence is well formed) Does this imbue the signaling system with additional desiderata?\n",
        "1.  Natural Languages develop between many agent and evolve over long time frames. (Hebrew students in primary school read the bible written in Hebrew thousands of years ago using just thier knowledge of modern hebrew.) What about having many senders and receivers speeds things up. I.e. what choices can we make as designers to leverage this.\n",
        "1. Can the the sender plan all this machinery in advance and hide it in the the sequences allowing the receiver to learn in the same way as the simple game. Can the receiver infer the machinery from the data? Are there other paths to learning. What if they need to make revisions can they handle those too?\n",
        "\n",
        "So I ended up with more questions then I bargained for. These are questions within question. These questions suggest new and intriguing desiderata for (complex) signaling systems as some novel  path for signaling and possibly new settings for learning them.\n",
        "\n",
        "## Are Complex siganling games hard to visulaize\n",
        "\n",
        "In reality I have no idea about almost all of these questions yet here they are. Somewhat similar to how I felt at the end of my BA in mathematics. In reality I've made lots of progress with the lewis game and so I do believe I can make more progress. The reality is that to make progress you need to ask the right questions. **I have been lucky to be able to look at the work of many  others, criticize but later ask different questions than they did and so I was able to crack the issues related to the simple game**. I also developed good intuitions on the simple lewis game and over and over I realize that the complex game is in many aspects just the simple game in most respects.\n",
        "\n",
        "![ultimate tic tac toe](ultimate-tic-tac-toe.webp){.column-margin}\n",
        "\n",
        "I was looking over books this week and I came across two or three by math teacher, author and blogger [Ben Orlin](https://mathwithbaddrawings.com/ultimate-tic-tac-toe-original-post/). I felt they are too basic for me to buy but I did get the idea that with the kind of sketches he used I may be able to make more progress with these research questions. I confess am not very good at visualizing thing in my mind. So unless I can put them down on paper so it can be a bit of a struggle to get the ideas out of my head. \n",
        "\n",
        "This ultimate tic-tac-toe game seems to be similar to the coordination task in the lewis game with three states and three signals. In tic-tac-toe the game is [zero sum](https://en.wikipedia.org/wiki/Zero-sum_game) and in the Lewis game the agents are trying to [cooperate](https://en.wikipedia.org/wiki/Cooperative_game_theory). But nature (chance) can have the effect that they don't end up with a good solution or at least take a long time to lean a good system. \n",
        "\n",
        "One way to visualize this could be viewed as a pattern that is like a three rook problem in chess in red with the blue corresponding to all the misses they made along the way. Of course lewis games can start on a two by two grid but tend to be much bigger.\n",
        "\n",
        "A second aspect of visualizing is the learning in the game. I use urns or matrices with heatmap to visualize the learning in the simple game. \n",
        "\n",
        "Now this ultimate tic tac toe suggest something extra. In complex lewis games we are working with sequences.\n",
        "The simplest way to think about sequences is to put each at the top of the matrix and all the states at the side. Agents then look for an n-rook solution in the matrix. And that is a signaling system.\n",
        "However  there may be other ways to view sequences, perhaps using nesting or recursion which is a way to establish hierarchy and organize a language\n",
        "\n",
        "Say we had a sequence of three sysmbols we\n",
        "\n",
        "\n",
        "::: {.column-margin #fig-deep-reinforcement-learning}\n",
        "\n",
        "{{< video https://youtu.be/YOh9iIQ5Qco title='Math with Bad Drawings | Ben Orlin | Talks at Google' >}}\n",
        "\n",
        "\n",
        "Talk titled 'Math with Bad Drawings' by Ben Orlin at Google from Jan 29, 2019\n",
        ":::\n",
        "\n",
        "I have been doing some similar work myself trying to explain basic statistics visually. And I used my own urn models to workout the different lewis algorithms. So it's not a big surprise how looking at his books I got the idea that I need to make this more visual and concrete. \n",
        "\n",
        "What I hope is that by posing these many questions I might be able to find some organizing idea to answer them. I seems that one way to move forward is to extend the petting zoo environment and build agents that can participate in experiments that will help me answer these questions. A second challenge is to add metrics that lead to easy evaluation of communication and robustness of the equilibria in the game. A third challenge is to test different algorithms for learning the signaling system and see how they perform in some different framing scenarios. Another challenge seems to be  even harder -- that of interpreting the many emerging languages.\n",
        "\n",
        "\n",
        "## Looking fo the Organizing Ideas\n",
        "\n",
        "Earlier this week I tried to explain the main issues around compositionality to a friend. This helped to cement my understanding of the problems that researchers in this field are facing. I noticed that in many talks by RL researchers they like to presents their work from first principles. This means most of the RL talks wastes a big chunk of their time on introductory stuff. But it does allow them to talk about advanced concepts with all the more confidence their readers understand what they mean. \n",
        "\n",
        "Let's start with these concepts that come up quite a bit in the literature and might be less intuitive to the uninitiated. I realize that big words can be intimidating so I will make an effort to explain the main concepts in a simple way and if I do use more big words in these definitions, I'll mark them in italics to indicate that they are not essential to understanding the concept. Some readers might know the terms in italics and this might be helpful to them.\n",
        "\n",
        "1. **Signaling systems** - these are steady states corresponding to *separating equilibria* in to the [Lewis signaling game](https://en.wikipedia.org/wiki/Lewis_signaling_game) that allow agents to communicate about the world with each other using symbols and without making mistakes. Human languages are rife with ambiguity and do not fit into this category.\n",
        "1. Besides signaling systems the Lewis signaling game has many equilibria  that are less then conducive for perfect communication. These correspond to **partial pooling equilibria** and we can rank them by their likelihood of the agents to correctly interpret each other messages. We can generally pinpoint the issues as homonyms in the lexicon. \n",
        "1. Is the agents ignore the states or the messages then the game collapses to a **perfect pooling equilibrium**. This is a state where agents can't do better then random guessing. \n",
        "1. Even when agents are **deceptive** and messages are cheap talk, it is possible to agent to coordinate by looking at thier actions and infer from that some kind of signal. So even under a zero sum game it may be possible that they might end up with an system of communication that is better then random guessing. c.f. [@jaques2019socialinfluenceintrinsicmotivation] However this is an aside on deception and will not be pursued further. \n",
        "1. Simple signaling systems - these are signaling systems that comprise of just a lexicon of symbols and their corresponding states.\n",
        "1. Complex signaling systems - these are signaling systems that comprise of a lexicon of sequences of symbols drawn from an alphabet and each sequence represents some corresponding states. \n",
        "\n",
        "It not simple to explain how these differ differences from simple signaling systems except to say that while simple signaling systems are just a lexicon complex ones may be able to capture some or possibly all of the nonces of a natural language.\n",
        "\n",
        "One way is that perhaps some the symbols comprising the alphabet might be included in the lexicon as length one sequences, and when they are used in a sequence their semantics may be assumed to be the same as when they are used alone, unless there is an explicit entry in the lexicon that says otherwise. This would mean that the lexicon now has potential to express\n",
        "What is different from simple signaling systems is that should the states have a sub-states with some structure. In such a case if agents are able to coordinate a signaling system that respects this structure they may be able to learn to communicate much faster. Also if the system gain evloves and new states are intorduced with this structure the agents may be able to learn to communicate about these states much faster then if the structure is ignored.\n",
        "\n",
        "It worthwhile to recall that these three definitions\n",
        "\n",
        "However in this case the lexicon might have additional structure that matched the structure of the states. Idealy this structure is codified using rules so that using these rules the semantics of the atoms can be combined to give the semantics of the whole.\n",
        "\n",
        "5. **Sub states** - Pre-linguistic objects are sometimes called states and we would like to look into these and discern if they have structure like key-values pairs, some kind of hierarchy, some symmetries, a temporal structure, etc. If such a structure exists and follows some steady distribution we the lewis signaling game may have signaling systems that are conducive to perfect communication, faster learning and generalization.\n",
        "for we are primarily interested in complex signaling systems.\n",
        "\n",
        "6. **Framing Game** in movies we often have a framing device - some kind of story that is used to introduce the main story as a flashback. In MA-RL the lewis signaling game may be a means to achieve coordination between agents in a more complex game. This bigger game is called the framing game. Though in in the life long learning setting there may be many games taking place simultaneously.\n",
        "\n",
        "6. **Entanglement** - when a sequence of symbols are combined in a way that their meaning is different from the sum of their meaning we say that they are entangled. For example idioms like \"kick the bucket\" or \"keep the wolf from the door\" are non-compositional and highly entangled. We cannot assign a specific word (atomic symbol) that captures part of thier meaning. This is a non-compositional way of encoding information. Although entanglement is explained using idioms, it can happen at different levels and may be an artefact of some selection pressure in the environment to compress information about certain states. If some bound colocation is used very frequently. i.e. where both words are used together exclusively then the speakers may have a benefit in fitness or communication for encoding them as a single symbol. Another form of weak entanglement might be exemplified by the compound verbs in English. In the phrases Look out for, Look up, etc the meaning of the verb look changes in a way that is not a simple sum of the usual meaning of preposition that follows it. This is  a clich√©. But the bottom line is that entanglement representation require the agents to learn the meaning of the entangled symbols together. \n",
        "7. **Disentanglement** - when the meaning symbols can be interpreted regardless of thier neighbors. There are two problems with disentanglement. The frist is that it is rather vague definition and secondly it is not a property of languages. Sure many words in a language can be assigned a definition that is independent of the other words in the language. But the lexicon if full of colocations, idioms, compound verbs etc. Verbs and Nouns have a stem and affixes that encode multiple units of meaning. Describing relations and using adjectives  and adjectives is done using phrases, i.e. they are spread across multiple words. But the problem with this term is that semantics of one usint isn't a signle bit of information (i.e. discreate) Some symbols can contain more information, then other. This is other grammatical symbols perhaps less but seem to operate on others.\n",
        " The problem with this term is that semantics of one unit isn't a signls bit of information (i.e. discreate) Some symbols can contain more information, then other. This is other grammatical symbols perhaps less but seem to operate on others.\n",
        "8. **Compositional**- when the meaning of a symbol can be decomposed into the meaning of its parts. This is a compositional way of encoding information. One system that is compositional is first order logic.\n",
        "9. **Generality** - We take here the meaning from machine learning, where a model is said to be general if it can perform well on unseen data. What researchers tend to look at is if agents have have coordinated a signaling system that is a good representation of some subset of pre-linguitic objects which we call the training set, how well would that system generalize to unseen objects? \n",
        "10. **Catagories** - In a simple lewis game we might have multiple states assigned to a single symbol. We call this a homonym and consider it a defect. If though in the framing game we have a survival benefit to acting early, if such a homonym encodes a number of states that require the same action then we might have a benefit to splitting the states into two parts the category and the subcategory. Also we might have additional benefits here by using a shorter signal to encode the category and a longer signal to encode the sub-category. Categories or in more general form Hierarchies are a ubiquitous feature of natural languages. Another facet of this idea is the use of prefixes and suffixes in natural languages. In both cases we have a benefit from using a shorter signal to encode some category and a longer signal to encode the subcategory. But is the prefix perhaps we need prefer to know the category first and in the suffix we might prefer to know the subcategory first.\n",
        "11. Aggregation rule - an aggregation rule is a rule that takes an input with a number of symbols and reconstructs from them a state. In one sense this is what we think of as a grammar, but I'd like to keep them seperate and think about the agrregation rule as something more like a serelization protocol. It takes a number of inputs -- possible from differnt senders and likely each with some meaning or prehaps just a cue - a partial meaning that can't be interpreted without the other cues. \n",
        "Two examples of aggregation rules are \n",
        "    1. the disjunction leading to the bag-of-symbols \n",
        "    2. the idea of serilization of incoming audio signals by the reciever by appending them order in which they are recieved.\n",
        "    3. and serilization converting OOP into a sequence - perhaps for images.\n",
        "We can think about a recursive aggregation rule but I'd like to call these a grammar and keep them seperate. Perhaps later I'll be able to explain why I think this is a good idea.\n",
        "Note that complex signaling systems do not require an aggregation rule or a grammar, but they may benefit from them. Without an aggregation rule we are dealing with a signaling system that is fully entangled and that is little different than a simple signaling system.\n",
        "\n",
        "tip: currently my mental models for Aggregation rules are {disjunction, serialization, prefix coding}.\n",
        "\n",
        "12. **Formal Grammar** - describes which strings drawn from an alphabet are valid according to the language's syntax. Note that the formal grammar is in charge of the syntax and not the semantics. Thus a grammar can be considered as a **language generator**. If the speaker uses such a language generator then the resulting language will have a syntax. And further more is the grammar is unambiguous then the language will be a signaling system. And Aside is that for propositional logic a formal grammar is enough to define the semantics as they arise directly from the syntax. For FOL we need a model theory to define the semantics. However if we construct a Lexicon with semantics from the model of the FOL we end up with a grammar whose semantics are defined from the syntax. \n",
        "My mental model for a formal grammar in the lewis game is propositional logic on its sub-states.\n",
        "\n",
        "The main takeaways is that if we generate the lexicon without using an unambiguous formal grammar we are putting the syntax of the language at risk. And if we don't have a a lexicon for the alphabet we may be putting the semantics of the language at risk. \n",
        "\n",
        "\n",
        "An intersting issue is that we can have differently likelihoods of having different constructs depending on what we include in out lewis game extention....\n",
        "\n",
        "We might want to find a metric that measures the use of categories in a signaling system.\n",
        "\n",
        "Another couple of ideas.\n",
        "\n",
        "We can have examples of complex signaling systems that are on a spectrum from being fully  compositional to being fully entangled. \n",
        "\n",
        "Another thing we can unse is a complex compositional language for pre-linguistic objects that don't have a simple disentangled structure and don't fit with the positivistic view of an objective orthogonal and disentangled dimensions that can be measured with full certainty. Ie we can use a easy to learn compositional complex signaling system to encode the pre-linguistic objects that are not easy to interpret like arrays of raw pixels. But also we might be less likely to learn such a language if this is our only input. We would have to come up with it by change.\n",
        "\n",
        "One way this might happen is that we could learn a grammar (classifier) that \n",
        "\n",
        "## Transcripts:\n",
        "\n",
        "### Baroni Talk\n",
        "\n",
        "I've had this idea about Complex signaling systems, unfortunately. I'm wasted a lot of time, installing a transcription app and lost focus. Let see ---  I'm trying to refocus and get back into it.... Yes, I think I got it. \n",
        "\n",
        "So yesterday after talking with my friend Eyal which got me rethinking about complex signaling systems and trying to realize how despite the many perhaps misguided attempts a growing number of researchers have been able to come up with agents that use complex and even compositional signaling systems. How these agentic systems seem to be a solution to a problem that is hard to frame.  Uh, from deepmind. By, what's your name? I had the second good idea. Uh, which I wanted to write up.\n",
        "01:09\n",
        "For her. Stuff.\n",
        "01:15\n",
        "No, let's let's uh, so so the two happy things. So the the chat with, uh, Got me. To be able to explain entanglement disentanglement. Compositional, non-compositional and generality. In a nice concise way. And that deserves. Good. Write up inside of the baroni post. And maybe inside of the guide to the perplexed. About compositionality. Uh, in fact, I don't think Baroni deserves this because He's just all about making things to make him confusing. Uh, then there is this Gondola girl. Whatever name is. And, Thinking about her work. Got me. To make a bit of progress. I think.\n",
        "02:26\n",
        "What we could do there? Is say this.\n",
        "02:34\n",
        "If? Um,\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "## The Question of Grammar\n",
        "\n",
        "\n",
        "This bit of thinking is about the deep mind emergent languages talk and paper [@]. Um, and what I noticed there. Is that? The talking about emerging languages. And I'm talking about, Engine re-engineering in Duluth game. The change at equilibria. So, as to. Support complex, signaling systems. And this is quite tricky. Particularly. When we have, Both simple and complex.\n",
        "00:45\n",
        "Signals and we have a third kind of thing. Which is the so-called aggregation. And there's yet another thing. Which is the grammar. Is grammar and aggregation the same thing. I don't think so. I think that. The related. But only coincidentally, The aggregation. As scrims describes, it can be conjunctive, which is weak. We go, it leads the weaker representation and the sequential one. Which leads to Richer. Representation. Um,\n",
        "01:38\n",
        "But that said, It doesn't necessarily lead to a grammar. Although, It's definitely sufficient to act as a kind of grammar.\n",
        "01:54\n",
        "Um,\n",
        "01:59\n",
        "What what is an example of a concatitative grammar? Uh, Hungarian. Is a glue. Native language. You simply add. Morphological units to form. Ah, very powerful.\n",
        "02:25\n",
        "Um, representation of a word, which is able to Essentially exist. Or rather resist changes in the order. Or for freeze. And, That doesn't mean that they don't care about that. There isn't extra meaning, due to the order like the word, which is at the focus position. But basically Once we have the markings, in the words with all the affixes that you have in Hungarian, It's quite possible to shuffle the words. And not disrupt. The meaning of the That. Morphology is encoded. So, that's kind of\n",
        "03:24\n",
        "The power of the sequence I suppose. Though. It also, we also have in the sequence this special morphological markers.\n",
        "03:39\n",
        "But we could consider that these morphological markers are just basically. Certain words.\n",
        "03:51\n",
        "Which we put in certain positions. But, So, we're still talking about this idea of\n",
        "04:04\n",
        "Competitive grammar. So another thing in the concoctative grammar is Uh, it could be. Ordered or disordered. If things are marked or unmarked, we can have it. Uh, resistant reordering. So the meaning is preserved and if it isn't like in the example from The unfolding of languages from Old Turkish or Babylonian. You might have a very long sequence of Slots. And, We should be able to. Composite into these slots. A whole wealth of words, a whole wealth of meaning. And, I think we have something similar in German. We basically assemble a whole sentence. By gluing together, bits and pieces.\n",
        "05:06\n",
        "Of.\n",
        "05:12\n",
        "Into one long word. And this actually makes sense. If you think about,\n",
        "05:24\n",
        "About all societies in which The nobody wrote down the language if you don't write down the language. Yes. It's all overall. And, We could think of a word is something. That's just a sequence. Okay. So, all of this Tries to highlight that. Um, We can have grammars just by concatenating. More films or like themes. And,\n",
        "06:06\n",
        "That's what we call compositionality really, or at least. That's a very basic form of Compositionality.\n",
        "06:19\n",
        "So, what is grammar if we have just a compositional\n",
        "06:26\n",
        "Rules. I would say that grammar. Um,\n",
        "06:35\n",
        "Is different ways we use. Create isolate bits of meaning.\n",
        "06:49\n",
        "Yeah, it's Seems to be a hard thing to Define properly, but\n",
        "06:58\n",
        "The kind of ideas I'm thinking about is that we might have this recursive. Recursive set of rules. Because the grammar, Yeah, so in former languages, I think that's the direction that's the direction I'm bent to in. Informal languages, grammar defines. Using recursion, usually.\n",
        "07:30\n",
        "Set of sequences, the sequences are defined by the grammar. You can call them sequences, we control them sets. But I think they usually order set, so we can call them sequences and these sequences I lost to take. Find that alphabet and create. Uh, infinite number of\n",
        "08:00\n",
        "Messages. How do we do that? Quite simply put. We have this operation. With a simple set where we can take the power of the set, the power set. Which is.\n",
        "08:22\n",
        "All the pairs, I think. And all the triplets and so on and so on. Basically, all the subsets rather. Yeah, the power set is the central subsets. If we look at just\n",
        "08:46\n",
        "But the power. Yeah, if we look at all the subsets we can create, Bigger more complex constructs.\n",
        "08:56\n",
        "Um, If we look at, Sequences that we can form. We can also have a grammar for that. And, Some things human grammars have. This thing, this notion of agreement. I see agreement. This Serving two purposes. One is to identify. To maintain a correlation between lexical units. That have a relation. To show us assumings that\n",
        "09:43\n",
        "That the Mexico. Units or even two phrases. Are related using this? Correlation of gender number and so on. Whatever the agreement is keeping, And that way. We can. Poke into it. Into the slots between them. Uh, additional structures. And, The agreement. Allows us to maintain. The relationship. This, of course, breaks down. If we poke in, And they'll literally arbitrarily Large number of, Uh, phrases. He put in a very big tree. This isn't effective. Another way that I'm looking at it. Is that in terms of the pragmatics? If we look at the pragmatic side of communication,\n",
        "10:57\n",
        "This is just the redundancy, which allows To do our correction. I suppose. In the big picture though. These two things. These two phenomena agreement. And error, Corrections are\n",
        "11:22\n",
        "Dual aspects of the same thing. You put. These markers. They allow. They also make the language. More resistant.\n",
        "11:39\n",
        "Errors. They help us. Uh, disambiguate certain messages.\n",
        "11:50\n",
        "Through this types of agreement. And,\n",
        "11:57\n",
        "Uh, we usually don't need these\n",
        "12:03\n",
        "If there is, let's say less chance of an error or less chance of confusing. Some pronoun referencing, some other battle piece. Uh, multiple bits and pieces of the sentence. We might not need to mark this thing with.\n",
        "12:27\n",
        "With an agreement. To understand what's going on and we might end up with a determiner. Which is unmarked. All it says is So what's the difference between a and there or that? Definite and indefinite. All right, this or that.\n",
        "12:54\n",
        "So, The unmarked distance in any way that they don't have. Um, what don't they have? They don't reflect number, they don't reflect. Gender and so on.\n",
        "13:14\n",
        "The other hand we do have one, we have mine. These are.\n",
        "13:24\n",
        "Markers. These are, I don't know, pronouns or particles that.\n",
        "13:31\n",
        "Mark possession. The Mach number, the micro number the marked for person. Right.\n",
        "13:43\n",
        "In Hebrew, they marked with gender. So they get marked with a lot of things and that kind of makes it significantly easier.\n",
        "13:56\n",
        "Makes it much easier. To discuss. There's ambiguous thing. Called position.\n",
        "14:08\n",
        "And in Hungarian we can, The position and the possessor. Using additional information. Which is, uh, stored in the suffix. Along with the singular plural. So, they mark that. But they don't Mark gender. Which is. How do you say? It's useful as it's on a, on a pig or something. Right. So Enough said about that. Um, So, let's get back. So now we've discussed Grammar. Kind of try to Define grammar. And, Three rolls the formal role. Being. That it is.\n",
        "15:13\n",
        "A. That allows us to. To make use of a finite set of symbols. Into an infinite set of messages. But not necessarily saying anything about Their semantics. Although if we look at, I don't know. The grandma. First saw the logic. It does allow us to Define by induction.\n",
        "15:48\n",
        "The Logical.\n",
        "15:53\n",
        "The Logical. The Logical meanings of. I'll betually long phrases. Okay.\n",
        "16:07\n",
        "Um, So, it not only allows us to generate sequences. But to propagate the atomic meanings into more complicated meanings.\n",
        "16:26\n",
        "Aggregate meetings. This.\n",
        "16:34\n",
        "Um, Is more complicated and simple. Aggregation in simple. Aggregation we're saying something after something after something just have this blasting. Logic, we have. And the no with brackets.\n",
        "17:02\n",
        "And we can build with this very specific functions. Of truth functions. And Truth functions correspond to a big chunk of semantics. No doubt about that.\n",
        "17:24\n",
        "So, if you want to look at grammar, Starting with a former language. It's not the worst thing. But if we're thinking about, This thing we call uh, what do we call it? Um,\n",
        "17:44\n",
        "Emergent language. We may be interested in having some additional useful properties, and this is what I collect and the ziturata This. Space. Definitely consider. The issue of learnability. Vulnerability.\n",
        "18:14\n",
        "Becomes Paramount. When we have Collective generational, collectives of Agents,\n",
        "18:27\n",
        "Basically. When we? To transfer. Disability. And The ability to communicate.\n",
        "18:43\n",
        "Uh, between agents and Even if we don't have Generations, even if we have Continuing tasks. Which is. Similar to what's happening in the real world. Uh, we still The. You want? We still would like, to be able to Handle.\n",
        "19:16\n",
        "Distributional shifts. Within the language. So If we. If you and I are talking for a long time. We're gonna have. Who had adaptation of our language will have accent, and then would have a dialect. And then pretty soon. We're gonna ask someone about. This and that and other people. Won't be able to know what we're talking about. We'll have our own jokes. We'll have our own idioms. And,\n",
        "19:58\n",
        "Made upwards. So we'll have Words that other people know, but that now have completely new meanings. So, that's this. Shift. I'm referencing. Distributional shift. And we'd like, to be able to Uh, communicate with other agents. The distributional shift. Is also annoying for researcher. In the sense that Let's say, you understood what the language means now. But What happens after? The our allegiance.\n",
        "20:42\n",
        "Have had another. Half a million turns playing. Let's say some kind of diary game or something. Now, the language might mean. Other things. The words and phrases might be the same but the semantics have changed, that's a big headache or the grammar might have changed.\n",
        "21:08\n",
        "One of the algorithms I came about was to Full languages was Try to swap out. Pairs of signals.\n",
        "21:24\n",
        "And the meanings based on.\n",
        "21:30\n",
        "Distribution of. And this. Get us to a point where Um, We can have. Shared bits and pieces. Shared button pieces. These shirts bits and pieces.\n",
        "21:56\n",
        "Then be able to spread. And then we would have a compositionality. Basically. Should bits and pieces. Become prefixes and prefixes become categories. Vice versa. Let's say, categories. Semantic categories and maybe later they become.\n",
        "22:29\n",
        "Grammatical categories part of speech. And so on and so forth. And That way, we might be able to evolve Um, Rules that simplify learning. We might learn construct. Let the language become. Uh, fixed. So, we don't have A big issue with. With that language needs to evolve, but we do have an issue with this core adaptation.\n",
        "23:10\n",
        "Certain bits and pieces of the language not changed so much. We'd like, To find an optimal grammar. Then stick with it. We'd like to Have close categories that not change those. We'd like to use. Affixes efficiently. And we might want to and prefixes and so on. And we like to stick with those. Otherwise.\n",
        "23:45\n",
        "Our verbs. Might be. More prone to. Withering. And to make the language learnable. It's gonna be much simpler if the verb and the noun, Morphology. Is regular. And more or less constant? And then, We can block in a stem or a root. You can get. A new verb or a new noun, or maybe even both. And all we have to do is learn the meaning of the stem more or less. And we understand all the rest. And that's the power compositionality. So,\n",
        "24:39\n",
        "One old idea, I had. About trying to induct. Buffalologies. In unsupervised or semi-supervised way. Was to try and find some kind of\n",
        "25:04\n",
        "That's a good fit.\n",
        "25:09\n",
        "Handling morphologies for. Handling tables. Did you follow the same structures?\n",
        "25:20\n",
        "This is.\n",
        "25:25\n",
        "Uh, something I don't want to explore here. But this kind of a loss. Might be useful. To encourage.\n",
        "25:43\n",
        "Hola. The agents to coordinate on the language. With a given structure and yeah. A further refinement of this idea. Which I've had more recently. Is to consider that. This morphology. To high degree mirrors a group action. And so we can Define This loss in terms of, Homomorphic loss. I lost that preserves.\n",
        "26:28\n",
        "Structure.\n",
        "26:34\n",
        "But that's pretty much.\n",
        "26:38\n",
        "Just handling structure. You could probably do even better. If we. Also, preserve distances. Distances between. Semantic unit. I suppose. Such.\n",
        "27:02\n",
        "Structure would be. Also a home homeomorphism topologically preserver. Something. To think about.\n",
        "27:18\n",
        "And yet. These are approximate.\n",
        "27:25\n",
        "Why is that? Because, Because, Um, Because of the irregularities irregularities. Are usually going to be deviations from this group structure. From the topology. From the simple distributional forms.\n",
        "27:52\n",
        "For compositionality. We generally assume. Independence. But three languages.\n",
        "28:04\n",
        "Are not uniformly, distributed.\n",
        "28:08\n",
        "Biggest structures are not. Uh, often\n",
        "28:19\n",
        "Show conditional. Probabilities. So, They don't follow.\n",
        "28:33\n",
        "Independent structure.\n",
        "28:38\n",
        "And these are. Aspect of the language, we usually want to preserve In other words, It's often. To our benefit. To have.\n",
        "28:56\n",
        "These other things in the language.\n",
        "29:07\n",
        "Why is that? Because these oddities often encode. Not always, but often. Then code.\n",
        "29:26\n",
        "Frequent the most frequent. Elements. Irregular verbs. To be.\n",
        "29:37\n",
        "To do. In French, a regular.\n",
        "29:46\n",
        "In English. I'm not sure, I think they are also. Is that good makes learning harder? But it's only a little chunk and it probably makes air correction better.\n",
        "30:06\n",
        "Although, I'm sure we could do better than that. Um, So there are all sorts of plays going on here. And, Uh, one thing you can be sure about Is that? To come up with a good grammar. Isn't. Necessarily very difficult.\n",
        "30:38\n",
        "But,\n",
        "30:43\n",
        "It appears to be. Product of planning. Once you have the language, Or laid out in other words.\n",
        "30:56\n",
        "Expand the rules of the grammar. You'll have vast constructs. And to change. And we see the changes in these contracts are often localized. Of the localized. Which means? It's very hard to change the grammar. Just change the grammar, you will have to change.\n",
        "31:25\n",
        "All the application of all these constructs. For that particular, aspect of the rule. There might be subtle changes, that not make big, big, big changes. Usually,\n",
        "31:44\n",
        "Natural language grammars. Nothing. Like The world behaved formal grammars. Which we see, and I think, This is primarily because of what I just described. That it takes a Big effort to make these changes because you have to get these changes to happen in the heads.\n",
        "\n",
        "The most.\n",
        "\n",
        "Oh, at least localized area. Where people are talking? Um, Yeah. Um, but they do happen. So they happen, they happen because The forces. Evolutionary forces. Acting over. Certain time frames. That. We'll do this, because The original system might be inefficient. Of equilibrium in terms of signaling systems. And each interaction entails a tiny amount of of friction. And,\n",
        "If it is likely, the changes will happen in such a way.\n",
        "33:32\n",
        "Follow some kind of path of lift resistance. A domino effect Chain Reaction. We see the vowel, shifts. How can that possibly happen?\n",
        "33:51\n",
        "One wond. But,\n",
        "2\n",
        "Speaker 2\n",
        "33:59\n",
        "I don't know. But\n",
        "1\n",
        "Speaker 1\n",
        "34:07\n",
        "They did happen. They have happened. And, One would think this is. Something that's happening, step by step.\n",
        "34:20\n",
        "Certainly. Words. With a dominant meaning resist changes.\n",
        "34:34\n",
        "To conform with the grammar. Until?\n",
        "34:42\n",
        "Perhaps that word. Is replaced. With another word of the use of that term. Falls into. The shoes. Allowing us to.\n",
        "35:00\n",
        "Uh, assign a new meaning or Reduce the variation. We see in Hebrew.\n",
        "35:10\n",
        "Something even more drastic has happened. Over time certain of the phonemes. Have become.\n",
        "35:25\n",
        "The.\n",
        "35:30\n",
        "Where we have three different sounds in Arabic, we have only one song in Hebrew. And, This is cause number of verbs to become conflated.\n",
        "35:44\n",
        "You can only imagine the chaos. This is created in terms of meanings.\n",
        "35:52\n",
        "If only, we could take a step backwards, Teach everybody, how to Pronounce. These sounds in three distinct ways, everybody. And then, teach everybody. Which verb? Fits with which sound.\n",
        "36:15\n",
        "We could. Make the language, much more.\n",
        "36:23\n",
        "Much less ambiguous. But at the same time, It will probably be. Harder to learn. So, If there's one conclusion from all, this rather long story, Is that all these things? Represent trade-offs. And,\n",
        "36:58\n",
        "Every languages, the equilibria.\n",
        "37:07\n",
        "Corresponding to these different things. Equilibria. That is stable. One change so much, but You can see from the research by schemes and others. That.\n",
        "37:33\n",
        "The. Most of the equilibrio at least in the simple models. Are unstable or semi-stable. And serve the language. Will shift over time.\n",
        "37:53\n",
        "All that you need is the right kind of pressure. I suppose.\n",
        "38:03\n",
        "Enough said. So now we can get back to So,\n",
        "38:15\n",
        "I've covered a lot about. Aspects of grammars. Now, let's get to Signal existence, let's get back to signaling systems. It looks like. Does not look like the simplest games. There's a bunch of. Through specific equilibriums possible. Very specific. They're essentially infinite. Infinite number, and yet very specific. What do I mean? The three types of purequalibia. Perfect, pooling completely useless. Partial pulling. Imperfect.\n",
        "39:05\n",
        "And, And,\n",
        "39:15\n",
        "Separate the equilibria, the so-called signaling systems. That's to, at least. The simple. Games were simple States.\n",
        "39:33\n",
        "So-Called.\n",
        "39:40\n",
        "Uh, simple. The basic.\n",
        "39:50\n",
        "If you. Something more sophisticated.\n",
        "39:58\n",
        "We need to look for some other game. We usually call this. Modification. What we should ask is. Does our modification allow us?\n",
        "40:16\n",
        "To have. Um,\n",
        "2\n",
        "Speaker 2\n",
        "40:22\n",
        "Does it allow us to have new types of equilibrium?\n",
        "40:29\n",
        "I think there's\n",
        "41:18\n",
        "And just upload those.\n",
        "41:36\n",
        "So, the the big, the big question again.\n",
        "41:55\n",
        "Since we do see compositionals, Languages.\n",
        "42:05\n",
        "Can we\n",
        "3\n",
        "Speaker 3\n",
        "42:06\n",
        "say?\n",
        "2\n",
        "Speaker 2\n",
        "42:11\n",
        "Using complex symbols. And not just the coincidence.\n",
        "42:20\n",
        "No, that's not the paper. The big question is\n",
        "42:26\n",
        "If we make some change to the significance,\n",
        "42:41\n",
        "Of the game.\n",
        "42:46\n",
        "To have. New kinds\n",
        "1\n",
        "Speaker 1\n",
        "42:48\n",
        "of Bolivia. Furthermore.\n",
        "42:55\n",
        "This\n",
        "2\n",
        "Speaker 2\n",
        "42:57\n",
        "is equilibrium. Follow approximate, let's say. Use the approximate.\n",
        "43:25\n",
        "We really wish the property. That are close together.\n",
        "43:40\n",
        "Conform to some grammar. To the approximate, some kind of drama. Is there sometimes a genre. Death is. More likely to emerge. And so on.\n",
        "44:00\n",
        "Why do we ask this? Because we don't know. Nice and simple way to extend the game to get this. I don't think.\n",
        "1\n",
        "Speaker 1\n",
        "44:23\n",
        "I\n",
        "2\n",
        "Speaker 2\n",
        "44:24\n",
        "think you would apply this to the agents. They create is such an algorithm These agents. Lexus. No.\n",
        "3\n",
        "Speaker 3\n",
        "44:38\n",
        "Have order of magazine.\n",
        "44:44\n",
        "And it comes. What the future is to transfer.\n",
        "2\n",
        "Speaker 2\n",
        "44:56\n",
        "Oh, it would alone. For these abilities.\n",
        "45:14\n",
        "Policies.\n",
        "45:19\n",
        "The values and action values in terms of, A language where design in terms of getting used to be really interested. Objects. And there's a bit to flip.\n",
        "45:42\n",
        "They will be able to transfer. To have a translation. Between languages.\n",
        "46:02\n",
        "From the line is just one problem.\n",
        "46:15\n",
        "Another I know injection. We've seen in enforcement learning. That when you run the same algorithm, Some seats. So, Just by chance. We might have. Low performance. And another right here.\n",
        "46:45\n",
        "Lord, that pitched.\n",
        "46:54\n",
        "Well, not much except that. When we talk about language, we talk about state that unstable equilibrium and\n",
        "47:11\n",
        "What time vision is?\n",
        "47:16\n",
        "This could do. We understand. The Scorplex signaling systems. You should also be able to figure out How to create?\n",
        "47:35\n",
        "Very many designers including one with three stabilities.\n",
        "47:42\n",
        "If we are able to use this stability of our Policy value functions equivalent function.\n",
        "47:57\n",
        "Because we now have\n",
        "48:02\n",
        "More abstract.\n",
        "48:07\n",
        "We're probably going to be more powerful.\n",
        "48:14\n",
        "Then.\n",
        "48:27\n",
        "So, given all this\n",
        "48:33\n",
        "What did you change? With these so-called.\n",
        "48:48\n",
        "Research.\n",
        "48:56\n",
        "Composable. So complex. To work this country.\n",
        "49:31\n",
        "Because,\n",
        "49:37\n",
        "Uh, because We can easily show that. Languages with these properties.\n",
        "49:51\n",
        "The ability to capture very much and for Concepts using this small relatively small.\n",
        "50:03\n",
        "They're very easy to learn. They're very easy to teach. So if you have all these benefits, maybe some of them deserve distance and structure.\n",
        "50:19\n",
        "And maybe they are also.\n",
        "50:29\n",
        "Easier to To. Translate.\n",
        "50:49\n",
        "So, we would facilitate Uh, the best faciliters of transfer learning. Maybe they. Group instructions for many things that might not be in the learnable in one second. So you might be working with Space Invaders, you might not have. And language.\n",
        "51:23\n",
        "You get that. So if you're working with soccer band, you might have other kinds of Concepts\n",
        "51:34\n",
        "Useful, which\n",
        "51:40\n",
        "And,\n",
        "51:45\n",
        "I, We would like to find some kind of core.\n",
        "51:57\n",
        "A human language, which has\n",
        "52:12\n",
        "Didn't describe States. Well enough.\n",
        "52:22\n",
        "I\n",
        "1\n",
        "Speaker 1\n",
        "52:24\n",
        "have also properties, which bar\n",
        "52:34\n",
        "Uh, gravity stability.\n",
        "52:58\n",
        "And yes, all of this might follow one. Three, two, maybe just one uh, geometrical rule.\n",
        "53:33\n",
        "Yeah. Anyway, anyhow\n",
        "53:49\n",
        "So that's us. What? What does this? What does this?\n",
        "Transcribed by Pixel\n",
        "\n",
        "---\n",
        "\n",
        "## The second question, seems to be in what way is The referential game different from the signaling game?\n",
        "\n",
        "\n",
        "The second question, seems to be \"in what way is The referential game different from the signaling game?\"\n",
        "\n",
        "I postulate that this is due two changes and more significantly, the way these two things come together. The alternative hypothesis is that just one suffices for the emergence of a signaling system.\n",
        "\n",
        "1. There is the classifier.\n",
        "2. Is the Message generator - as described in [@lazaridou2018emergence] can generate any sequence from a given alphabet of symbols.\n",
        "\n",
        "If these symbols are already imbued with semantics from a pre-ante lewis game, then what we now have is a complex signaling system. However, if the symbols are not imbued with semantics then we can still use the lewis game to imbue them with semantics. It uncertain though if these semantics will be different from the semantics due to a simple lewis game.\n",
        "\n",
        "Initially I liked the first hypothesis as it mirrored my thinking of using an extensive game with two steps as my modified lewis game. Soon though I had an small epiphany, and decided to test the second hypothesis, this being more in line with the reductionist approach I had espoused all along. Particularly as this is the smaller and less powerful extension then the first.\n",
        "\n",
        "Now we can think of the lewis signaling game as using a generator with some set of symbols and sequences of length 1. \n",
        "\n",
        "::: {.callout-tip}\n",
        "## To shuffle or not to shuffle? ::cards:: {.unnumbered}\n",
        "\n",
        "More so we might want to shuffle the sequences so they come out in arbitrary orders allowing all the lewis game to unfold in all possible ways. I.e. all forms of symmetry breaking. Alternatively we might want to enumerate the different equilibria and thus only use one canonical order.\n",
        "\n",
        ":::\n",
        "\n",
        "## Some sequences and thier interpretations\n",
        "\n",
        "Here is some code that takes an alphabet $\\mathcal{A}$ and generates all the sequences $\\mathcal{L}$ of length N exactly\n"
      ],
      "id": "ba4f13e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from itertools import product\n",
        "\n",
        "def generate_sequences_generator(alphabet, n):\n",
        "    \"\"\"\n",
        "    A generator to yield all possible sequences of length n from the given alphabet.\n",
        "\n",
        "    Args:\n",
        "        alphabet (list): List of symbols representing the alphabet.\n",
        "        n (int): Length of the sequences to generate.\n",
        "\n",
        "    Yields:\n",
        "        str: A sequence of length n from the alphabet.\n",
        "    \"\"\"\n",
        "    for seq in product(alphabet, repeat=n):\n",
        "        yield ''.join(seq)\n",
        "\n",
        "# Example usage snippet\n",
        "\n",
        "# alphabet = ['A', 'B', 'C']  # Example alphabet\n",
        "# n = 3  # Sequence length\n",
        "\n",
        "# print(\"Generated sequences:\")\n",
        "# for sequence in generate_sequences_generator(alphabet, n):\n",
        "#     print(sequence)"
      ],
      "id": "6b50df03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tenery Sequences\n"
      ],
      "id": "ffa2193c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-tenery-sequences\n",
        "#| fig-cap: Tenery Sequence Generator\n",
        "\n",
        "lang= []\n",
        "print(\"Generated sequences:\")\n",
        "for sequence in generate_sequences_generator(['A','B','C'], 3):\n",
        "    lang.append(sequence)\n",
        "\n",
        "# print lang 3 sequences per line\n",
        "print(\"\\n\".join([\",\".join(lang[i:i+9]) for i in range(0, len(lang), 9)]))"
      ],
      "id": "fig-tenery-sequences",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpreting the sequences.\n",
        "\n",
        "In this case we might interpret each sequence as a message indicating a sub-state A, B, or C took place. If this is their meaning we might need to remove duplicates and order them to decode it. Also we have many alternative messages for equivalent states. This would slow down the learning process.\n",
        "\n",
        "Another way to go is to treat A,B,C as 0,1,2 and we can interpret them as a ternary number. Now each sequence is unique and can be interpreted as a corresponding to some state. If we had again three binary sub-states we could use this system with 3^3 symbols to encode the 2^3 states as follows:\n",
        "\n",
        "A in the first position indicates true for the first sub-state, B indicates False, and the same for the second and third sub-states. We don't need states with C. This is a more efficient encoding and will speed up the learning process. \n",
        "\n",
        "Let say we used the restricted system to start with and 0,1 to encode False and True\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "AAA \\to ['A':T 'B':F,'C':F] \\\\\n",
        "AAB \\to ['A':T 'B':F,'C':T] \\\\\n",
        "ABC \\to ['A':T 'B':F,'C':T] \\\\\n",
        "\\vdots\\\\\n",
        "CCC \\to ['A':F 'B':F,'C':F]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### Binary Strings\n"
      ],
      "id": "d183f575"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fig-binary-sequences\n",
        "#| fig-cap: Binary Sequence Generator\n",
        "\n",
        "\n",
        "def gen_all_sequences(alphabet = ['0', '1'], min_length = 1,max_length = 4,col_size=4):\n",
        "        \n",
        "    lang= []\n",
        "    print(\"Generated sequences:\")\n",
        "    print(f\"From alphabet: {alphabet}\")\n",
        "\n",
        "    for i in range(min_length, max_length+1):\n",
        "        seq=[]\n",
        "        for sequence in generate_sequences_generator(alphabet, i):\n",
        "            seq.append(sequence)\n",
        "        # print lang 3 sequences per line\n",
        "        print(f'Sequences of length {i}:')\n",
        "        print(\"\\n\".join([\",\".join(seq[i:i+col_size]) for i in range(0, len(seq), col_size)]))\n",
        "        print(\"\\n\")\n",
        "        lang=lang+seq\n",
        "    print(lang)\n",
        "    return lang\n",
        "\n",
        "bin_lan_1_4=gen_all_sequences()"
      ],
      "id": "fig-binary-sequences",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes:\n",
        "\n",
        "1. that that the above system might be learned if the Lewis signaling game took place and the sequences were generated in an order corresponding to these states. Otherwise we would get an equivalent system to the first one\n",
        "1. When we generate all sequences of length up to 4 we get in the first row \n",
        "   the two atomic symbols.\n",
        "1. Since these are binary sequence of numbers I naturally interpret these as each being a more general version of the previous one. \n",
        "1. In one sense '0', '01', and '001' mean the same thing to me. Behind it is an aggregation of the digits weighted be powers of 2. This leads to a semantic\n",
        "1. However this interpretation is arbitrary. Binary sequences can encode pretty much anything. We use them for general purpose commutation.\n"
      ],
      "id": "180c4463"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DNA=gen_all_sequences('ACGT', 3,3)"
      ],
      "id": "25b6705f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- if we use four symbols alphabet we can encode the four nucleotides of DNA. \n",
        "- this is the basis of the language of DNA!\n",
        " \n",
        "\n",
        "### Base 4 Sequences \n",
        "\n",
        "![Amino Acids Table](Aminoacids_table.svg){.column-margin}\n",
        "\n",
        "the following generate all the sequences of length 3 comprised of 0,1 and a space symbol\n",
        "\n",
        "### Propositional Logic\n",
        "\n",
        "The following generates all the logical propositions of length 4 using the with three clauses and symbols for negation, conjunction, disjunction for it alphabet\n"
      ],
      "id": "ceeafc1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "brackets='()'\n",
        "modal='‚óª‚óä'\n",
        "std_connectives='‚àß‚à®‚üπ‚ü∫¬¨'\n",
        "connectives=std_connectives+modal\n",
        "props='‚ä§‚ä•ABC'\n",
        "logic_alphabet=brackets+connectives+props"
      ],
      "id": "5983aad1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case we can see that almost all the sequences are not well formed formulas. Clearly if we wanted to use the a language of propositional logic we would need to reject the sequences that are not well formed formulas.\n",
        "\n",
        "Secondly we see that this alphabet which has only three atomic symbols for propositions is bigger - it has 10 symbols. If we wanted to consider `((A‚àßB)‚à®C)`\n",
        "we need to use a sequence of length 7. I f we also need to gengate everything we are at length 10. This is 10^10 sequences to go through. And it is likely to exceed the memory capacity of most computers to store all these sequences. We need a smarter way to generate the sequences.\n",
        "\n",
        "\n",
        "\n",
        "Also we can see that this language is not very efficient. We can encode the same information in a more compact way. For example we can use the following encoding: \n",
        "\n",
        "\n",
        "\n",
        "the following function checks a string for well-formedness\n"
      ],
      "id": "89a1dcb1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def is_well_formed(brackets, connectives, props, s):\n",
        "    \"\"\"\n",
        "    Checks if a given string 's' is a well-formed formula (WFF) under propositional calculus.\n",
        "\n",
        "    Args:\n",
        "        brackets: A string containing the opening and closing bracket characters (e.g., '()').\n",
        "        connectives: A string containing valid connective symbols (e.g., '‚àß‚à®‚üπ‚ü∫¬¨').\n",
        "        props: A string containing valid propositional symbols (e.g., 'ABC‚ä§‚ä•').\n",
        "        s: The string to be checked for WFF status.\n",
        "\n",
        "    Returns:\n",
        "        True if 's' is a WFF, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Reject consecutive negation\n",
        "    if \"¬¨¬¨\" in s:\n",
        "        return False\n",
        "\n",
        "    # 2. Quick check for invalid characters or unbalanced brackets\n",
        "    logic_alphabet = brackets + connectives + props\n",
        "    if not all(ch in logic_alphabet for ch in s):\n",
        "        return False\n",
        "    if not are_brackets_balanced(s, brackets[0], brackets[1]):\n",
        "        return False\n",
        "\n",
        "    # 3. Recursive checker\n",
        "    return check_subformula(s, brackets, connectives, props)\n",
        "\n",
        "def are_brackets_balanced(s, open_br, close_br):\n",
        "    \"\"\"Return True if brackets are balanced in s, otherwise False.\"\"\"\n",
        "    stack = []\n",
        "    for ch in s:\n",
        "        if ch == open_br:\n",
        "            stack.append(ch)\n",
        "        elif ch == close_br:\n",
        "            if not stack:\n",
        "                return False\n",
        "            stack.pop()\n",
        "    return len(stack) == 0\n",
        "\n",
        "def check_subformula(s, brackets, connectives, props):\n",
        "    \"\"\"\n",
        "    Recursively check if 's' is a well-formed formula under the rules:\n",
        "      1. A single proposition in `props` is a WFF.\n",
        "      2. ‚ä§ or ‚ä• is a WFF.\n",
        "      3. If œÜ is a WFF, then ¬¨œÜ is a WFF.\n",
        "      4. If œÜ and œà are WFFs and ¬∞ is a binary connective, then (œÜ¬∞œà) is a WFF.\n",
        "    \"\"\"\n",
        "    # Base cases: single proposition or constant\n",
        "    if s in props:\n",
        "        return len(s) == 1\n",
        "    #if s == '‚ä§' or s == '‚ä•':\n",
        "    #    return True\n",
        "\n",
        "    # Negation\n",
        "    if s.startswith('¬¨'):\n",
        "        # must have something after '¬¨'\n",
        "        if len(s) == 1:\n",
        "            return False\n",
        "        return check_subformula(s[1:], brackets, connectives, props)\n",
        "\n",
        "    # Parenthesized binary formula: must be of the form (X ¬∞ Y)\n",
        "    open_br, close_br = brackets[0], brackets[1]\n",
        "    if s.startswith(open_br) and s.endswith(close_br):\n",
        "        # Strip outer parentheses\n",
        "        inside = s[1:-1]\n",
        "        # Find the main connective at depth 0\n",
        "        depth = 0\n",
        "        main_connective_pos = -1\n",
        "        \n",
        "        for i, ch in enumerate(inside):\n",
        "            if ch == open_br:\n",
        "                depth += 1\n",
        "            elif ch == close_br:\n",
        "                depth -= 1\n",
        "            # A binary connective at top-level (depth == 0)\n",
        "            elif ch in connectives and ch != '¬¨' and depth == 0:\n",
        "                main_connective_pos = i\n",
        "                break\n",
        "\n",
        "        # If we never found a binary connective, not a valid (œÜ¬∞œà) form\n",
        "        if main_connective_pos == -1:\n",
        "            return False\n",
        "        \n",
        "        # Split around the main connective\n",
        "        left_part = inside[:main_connective_pos]\n",
        "        op = inside[main_connective_pos]\n",
        "        right_part = inside[main_connective_pos + 1 :]\n",
        "\n",
        "        # Ensure left and right parts are non-empty and op is truly binary\n",
        "        if not left_part or not right_part:\n",
        "            return False\n",
        "        if op not in connectives or op == '¬¨':  # '¬¨' is unary, so reject it here\n",
        "            return False\n",
        "\n",
        "        # Check each side recursively\n",
        "        return (check_subformula(left_part, brackets, connectives, props) and\n",
        "                check_subformula(right_part, brackets, connectives, props))\n",
        "\n",
        "    # If it doesn't match any of the rules above, it's not a WFF\n",
        "    return False"
      ],
      "id": "b999861a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Cases for the WFF checker\n"
      ],
      "id": "0c72b21c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ====================\n",
        "# Test Cases\n",
        "# ====================\n",
        "\n",
        "test_cases = [\n",
        "    (\"A ‚àß B\", False),       # Reject spaces around binary connective\n",
        "    (\"(A ‚àß B)\", False),     # Reject spaces\n",
        "    (\"(A ‚àß B) ‚üπ C\", False), # Reject spaces\n",
        "    (\"A ‚àß\", False),         # reject spaces and single operand\n",
        "    (\"A‚àß\", False),          # reject bin op with one prop \n",
        "    (\"A‚àßB\", False),         # reject bin op without brackets\n",
        "    (\"(A‚àßB)\", True),\n",
        "    (\"(A‚àßB)‚üπC\", False),   # missing outer brackets\n",
        "    (\"((A‚àßB)‚üπC)\", True),\n",
        "    (\"¬¨¬¨A\", False),         # Reject multiple consecutive nots \n",
        "    (\"‚ä§\", True),\n",
        "    (\"‚ä•\", True),\n",
        "    (\"‚à®\", False),           # Reject single connective not allowed\n",
        "    (\"()\", False),          # Empty brackets\n",
        "    (\"‚ä§‚ä§\", False),          # Reject multiple constants\n",
        "    (\"‚ä§‚ä•‚ä§\", False),         # Multiple constants    \n",
        "    (\"A\", True),            # Accept single propositions\n",
        "    (\"AB\", False),          # Reject adjacent propositions\n",
        "    (\"ABC\", False),         # Reject adjacent propositions\n",
        "    (\"A¬¨B\", False),         # Reject adjacency\n",
        "    (\"¬¨AB\", False),         # Reject adjacency\n",
        "    (\"¬¨A¬¨B\", False),        # Reject adjacency\n",
        "    (\"‚ä•¬¨‚ä§\", False),         # Reject adjacency\n",
        "    (\"‚ä•A\", False),          # Reject adjacency\n",
        "    (\"‚ä•¬¨A\", False),         # Reject adjacency\n",
        "    (\"(B)C\", False),        # Reject adjacency\n",
        "    (\"(A) ‚àß (B)\", False),   # Reject spaces\n",
        "    (\"A‚àß(B)\", False),       # Must have parentheses around entire binary expression, not half\n",
        "    (\"(A)‚àßB\", False),       \n",
        "    (\"A‚àß(B)‚üπC\", False),\n",
        "    (\"(A)‚àß(B)‚üπ(C)\", False),\n",
        "    (\"((A))\", False)        # Reject double parentheses around a single prop\n",
        "]\n",
        "\n",
        "for i, (formula, expected_result) in enumerate(test_cases):\n",
        "    result = is_well_formed(brackets, connectives, props, formula)\n",
        "    print(f\"{i}: {formula}: {result} (Expected: {expected_result})\")\n",
        "    assert result == expected_result"
      ],
      "id": "3179cac5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example of generating all sequences that are WFF up to length 5\n",
        "from itertools import product\n",
        "\n",
        "def generate_sequences_generator(brackets, connectives, props, n, is_wwf=None):\n",
        "    alphabet = brackets + connectives + props\n",
        "    for seq in product(alphabet, repeat=n):\n",
        "        candidate = ''.join(seq)\n",
        "        if is_wwf is None or is_wwf(brackets, connectives, props, candidate):\n",
        "            yield candidate\n",
        "\n",
        "def gen_all_sequences(brackets, connectives, props, \n",
        "                      min_length=1, max_length=4, col_size=4, is_wwf=None):\n",
        "    alphabet = brackets + connectives + props\n",
        "    lang = []\n",
        "    print(\"Generated sequences:\")\n",
        "    print(f\"From alphabet: {alphabet}\\n\")\n",
        "\n",
        "    for length in range(min_length, max_length + 1):\n",
        "        seq_list = []\n",
        "        for sequence in generate_sequences_generator(brackets, connectives, props, length, is_wwf):\n",
        "            seq_list.append(sequence)\n",
        "        print(f\"Sequences of length {length}:\")\n",
        "        for i in range(0, len(seq_list), col_size):\n",
        "            print(\",\".join(seq_list[i : i + col_size]))\n",
        "        print()\n",
        "        lang.extend(seq_list)\n",
        "    return lang\n",
        "\n",
        "print(\"\\nGenerating WFF (is_well_formed) up to length 5:\\n\")\n",
        "wff = gen_all_sequences(brackets, connectives, props, 1, 5, 4, is_wwf=is_well_formed)\n",
        "print(\"\\nAll WFFs up to length 5:\")\n",
        "print(wff)"
      ],
      "id": "b3b2648d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ok lets also consider the following minimal connectives example:\n"
      ],
      "id": "c502c433"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_length=5\n",
        "print(\"\\nGenerating WFF (is_well_formed) up to length 5:\\n\")\n",
        "wff = gen_all_sequences(brackets='()', connectives='‚à®¬¨', props='1ABC', min_length=1, max_length=max_length, col_size=4, is_wwf=is_well_formed)\n",
        "print(f\"\\nAll WFFs up to length {max_length}:\")\n",
        "print(wff)"
      ],
      "id": "b6cd4629",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "as max_length increases the number of sequences we need to generate and test quickly becomes prohibitive.  It takes too long.\n",
        "\n",
        "We can however make our generator more efficient by generating the sequences in a more structured way. We can generate all the sequences of length 1, then all the sequences of length 2, then all the sequences of length 3 and so on. This way we can generate all the sequences of length 5 by first generating all the sequences of length 4 and then adding a new symbol to each of these sequences. This is a more efficient way to generate the sequences. Particularly as we now avoid validating sequences. But we also avoid the exponetial growth inherent the set $|\\mathcal{A}|^N$\n"
      ],
      "id": "6dfd8cd7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def generate_wffs_up_to_length(props, connectives_unary, connectives_binary, max_len):\n",
        "    \"\"\"\n",
        "    Construct all well-formed formulas up to 'max_len' in length.\n",
        "    props: single-character propositions or constants (e.g. {'A','B','C','‚ä§','‚ä•'})\n",
        "    connectives_unary: unary connectives (e.g. {'¬¨'})\n",
        "    connectives_binary: binary connectives (e.g. {'‚àß','‚à®','‚üπ','‚ü∫'})\n",
        "    max_len: maximum length of formula strings to generate\n",
        "\n",
        "    Returns: A dict length -> set of WFF strings of that length\n",
        "    \"\"\"\n",
        "    wffs_by_length = defaultdict(set)\n",
        "    \n",
        "    # 1. base case: length = 1\n",
        "    # any single-character proposition or constant is a WFF\n",
        "    for p in props:\n",
        "        wffs_by_length[1].add(p)\n",
        "\n",
        "    # 2. build up from length=2 to length=max_len\n",
        "    for length in range(2, max_len + 1):\n",
        "        # (a) unary expansions: for each WFF of length-1, prepend '¬¨'\n",
        "        # new length = old length + 1\n",
        "        if length - 1 in wffs_by_length:\n",
        "            for w in wffs_by_length[length - 1]:\n",
        "                for un_op in connectives_unary:\n",
        "                    if not w.startswith(un_op):  # e.g. un_op == '¬¨'\n",
        "                        new_wff = un_op + w\n",
        "                        wffs_by_length[length].add(new_wff)\n",
        "\n",
        "        # (b) binary expansions: for each pair (i, j) with i + j + 3 = length\n",
        "        for i in range(1, length - 2):\n",
        "            j = length - 2 - i\n",
        "            if i in wffs_by_length and j in wffs_by_length:\n",
        "                for left_w in wffs_by_length[i]:\n",
        "                    for right_w in wffs_by_length[j]:\n",
        "                        for bin_op in connectives_binary:\n",
        "                            # (œÜ¬∞œà) => +3 chars for parentheses + operator\n",
        "                            candidate = f\"({left_w}{bin_op}{right_w})\"\n",
        "                            wffs_by_length[length].add(candidate)\n",
        "\n",
        "    return wffs_by_length\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "props = {'A', 'B', 'C', '‚ä§'}           # single-character props/consts\n",
        "unary_connectives = {'¬¨'}                 # just negation\n",
        "binary_connectives = {'‚à®'#,'‚üπ'\n",
        "} # typical binary ops\n",
        "\n",
        "max_len = 8\n",
        "wffs_generated = generate_wffs_up_to_length(\n",
        "    props, \n",
        "    unary_connectives, \n",
        "    binary_connectives, \n",
        "    max_len\n",
        ")\n",
        "\n",
        "tot_wffs =0 \n",
        "for length in range(1, max_len + 1):\n",
        "    print(f\"There are {len(wffs_generated[length])} WFFs of length {length}:\")\n",
        "    print(wffs_generated[length])\n",
        "    print()\n"
      ],
      "id": "54bdf310",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given that.\n",
        "01:47\n",
        "It can.\n",
        "1\n",
        "Speaker 1\n",
        "01:58\n",
        "Generate\n",
        "2\n",
        "Speaker 2\n",
        "01:59\n",
        "compatible patterns.\n",
        "02:05\n",
        "With a certain signaling. System.\n",
        "02:16\n",
        "Patterns, that are incompatible with a certain signaling State.\n",
        "02:25\n",
        "Given this.\n",
        "1\n",
        "Speaker 1\n",
        "02:32\n",
        "The signaling\n",
        "2\n",
        "Speaker 2\n",
        "02:33\n",
        "state that will emerge. Will be. That is compatible.\n",
        "02:42\n",
        "The sequence is generated.\n",
        "1\n",
        "Speaker 1\n",
        "03:01\n",
        "It will\n",
        "2\n",
        "Speaker 2\n",
        "03:02\n",
        "be. Compositional.\n",
        "1\n",
        "Speaker 1\n",
        "03:17\n",
        "If the\n",
        "2\n",
        "Speaker 2\n",
        "03:17\n",
        "patter, Generated.\n",
        "03:22\n",
        "Compatible.\n",
        "03:27\n",
        "With the Of homophism.\n",
        "03:35\n",
        "Homomorphism.\n",
        "03:41\n",
        "In other words, if\n",
        "03:46\n",
        "Reserves the structure of the pre-linguistic object.\n",
        "03:55\n",
        "Preserves the structure. Over the pre-linguistic object.\n",
        "04:05\n",
        "Otherwise.\n",
        "1\n",
        "Speaker 1\n",
        "04:15\n",
        "Despite\n",
        "2\n",
        "Speaker 2\n",
        "04:16\n",
        "using. A language built over in alphabet of\n",
        "04:26\n",
        "Atomic signals. We end up. For the simple noise, sign in the game. That is not discriminate.\n",
        "04:40\n",
        "Between symbols.\n",
        "04:46\n",
        "And that's called the Nissan, the full sequences.\n",
        "1\n",
        "Speaker 1\n",
        "04:55\n",
        "Coordinates\n",
        "2\n",
        "Speaker 2\n",
        "04:56\n",
        "over. Complete sequences rather than considering coordinating on the atomics. Symbols.\n",
        "05:20\n",
        "About that.\n",
        "06:36\n",
        "Now, if we just consider arbitrary symbols,\n",
        "06:46\n",
        "Or rather arbitrary sequences are generated. China, likely. The sequence to the homorphism.\n",
        "07:01\n",
        "This doesn't mean.\n",
        "07:15\n",
        "That we can't ever have. Compositionality baked in. It that boils down. If we're able. Um, Discard bad early choices. And, Make use of. Better ones down the line. What do I?\n",
        "08:07\n",
        "Let's simplify things by imagining the only two possible signaling systems possible. The. It's compositional and the second is,\n",
        "08:24\n",
        "Fully Tangled, for example. Fully entangled means that it is a simple signaling systems. There are no composite signals Rather no complexing no complex signals.\n",
        "08:50\n",
        "So we have a complex system and a simple system.\n",
        "09:01\n",
        "The, the chance of emitting a sequence that's compatible with the complex system is going to be\n",
        "09:14\n",
        "But fixed. Particularly if we don't have a lot of messages.\n",
        "09:28\n",
        "If we do have a lot of messages, that's a different story.\n",
        "09:36\n",
        "Which we should consider.\n",
        "09:46\n",
        "We do realize that we're more likely to have a complex system emerge with larger. Input spaces.\n",
        "10:06\n",
        "Let's suppose we keep emitting random signals. And,\n",
        "10:26\n",
        "Even if the Receiver. Guess is the correct answer. There is a small probability Epsilon. Of emitting. Another sequence.\n",
        "10:48\n",
        "Possibly.\n",
        "10:55\n",
        "What that is already news. What that is already in use.\n",
        "11:13\n",
        "Any given message?\n",
        "11:20\n",
        "On the other hand.\n",
        "11:30\n",
        "The receiver.\n",
        "11:45\n",
        "Well, in this case.\n",
        "11:53\n",
        "Have to make guess. Six. Sequence. Seeing this new sequence.\n",
        "12:16\n",
        "Give it that. There is already. Signaling system in place. And, This side usually gets classifier. This side via its classifier. What the speeds?\n",
        "12:40\n",
        "What this means. What this sequence means, right?\n",
        "12:57\n",
        "Then we might end up by chance.\n",
        "13:04\n",
        "Second signal. Well compatible. More compatible.\n",
        "13:21\n",
        "That. B system. Which is the complex system rather than with the simple system.\n",
        "13:37\n",
        "In other words, It satisfies certain symmetry. Extend with The linguistic object.\n",
        "13:55\n",
        "The classifier. Has already picked up. On the Symmetry. It is.\n",
        "14:18\n",
        "Reinforce this aspect of the classifier,\n",
        "14:27\n",
        "Notice that the classifier. If this case is not killed with the messages, Specifically worked with\n",
        "14:48\n",
        "Decode. The prolinguistic object.\n",
        "15:13\n",
        "Not that this is important. The important thing is, That.\n",
        "15:21\n",
        "The classifier sides. The classifier, assigns These messages.\n",
        "15:37\n",
        "The correct.\n",
        "15:42\n",
        "Pre-Linguistic object. So now, let's imagine we have two of these.\n",
        "16:11\n",
        "The inductive bar of the classifier. The inductive bias of the classifier.\n",
        "16:22\n",
        "Might be. Similar to A3. A decision tree. What does that mean for us?\n",
        "16:48\n",
        "Speeds the Is traditionally. The most likely splitified. For this eyelid.\n",
        "17:48\n",
        "Actually it's a mistake they're trying to dig in into this un. We build it this way specifically. So let's set this aside. Just consider this Stochastic. Let's imagine that we have a stochastic. Decision mechanism.\n",
        "18:17\n",
        "If we have this stochastic decision mechanism,\n",
        "18:25\n",
        "Perhaps. The distribution. The. Possible. Classifiers. And that we are somehow able to pick.\n",
        "18:46\n",
        "Randomly out of a subset of. Um, Classifiers. The ones which Those compatible with the historic data. In such a setting. Our future. Predictions. To be dependent on previous ones. But so long, As we keep on going. We will go to more and more compatible signals. With complex. Signaling rather than with, Simple.\n",
        "19:34\n",
        "How is that? That's actually.\n",
        "19:43\n",
        "It's actually wrong. Yeah, we're gonna get more incompatible ones. That's bad news. But\n",
        "20:02\n",
        "Assuming. That there is. Very large number. Of both types.\n",
        "20:18\n",
        "Or perhaps, even an infinite number of both types doesn't really matter, does it. So we keep on going. Uh, we have\n",
        "20:35\n",
        "We have initially a kind of System was A few complex signals baked in, but mostly Um, Simple entangled representations but we keep on going because our classifier is not so good. It keeps needs to get more good examples. Can only train if it gets a good example. So, it's slow.\n",
        "21:17\n",
        "And during this, we keep on coordinating and what that means is we get a chance to switch Signals with meetings, using the exploration. Okay.\n",
        "21:48\n",
        "This. Yeah. I suppose we Have multiple wins. Multiple ways for some signal, for example. Red cylinder.\n",
        "22:22\n",
        "A bunch of them are.\n",
        "22:29\n",
        "He's compatible.\n",
        "22:34\n",
        "Which the complex signaling system, but most of them are sorry but one of them is good. And let's say this is true.\n",
        "22:51\n",
        "For another. One. And another one. And another one.\n",
        "23:02\n",
        "Does my hypothesis.\n",
        "23:10\n",
        "In such a case. The reinforcement capabilities. In the algorithm. Can be such that in the limit. We will forget the incompatible ones. And remember the compatible ones. How is this possible? Simply stated. Simply stated. This can be possible if.\n",
        "23:54\n",
        "Rather, before we say, simply stated, let's consider that having both of Signaling systems at once. Is a mixed equilibrium. And this is the real hypothesis. Given that you have a mixed equilibrium. For partial. For the partially observed language.\n",
        "24:26\n",
        "Which contains both a composite. And,\n",
        "24:41\n",
        "Simple equilibrium. And, The input space is sufficiently large.\n",
        "24:54\n",
        "That the attraction.\n",
        "25:00\n",
        "Or stability. Yeah. The, the complex system is going to be should be Were attractive more stable and thus attract. Attract. What does that mean? It means as we learn. This process. Continues. The mixed equilibrium should. Converge. To a pure equilibrium on. Complex. Signaling system. Rather than\n",
        "25:51\n",
        "Mixed one. Um,\n",
        "26:01\n",
        "Other one, the Entangled representation. Given that. Given that. The classifier can disentangle things, if it can't. This won't happen. In other words, we're saying that. If the classifier Is able to learn. To represent within itself. The substates.\n",
        "26:44\n",
        "It will be able to reinforce not just the full symbol but the substates. If it can reinforce on the sub States, Will eventually be able to. Reject forget whatever the bad States. The entangled.\n",
        "27:10\n",
        "And keep the good ones. Compound ones. Another interesting thing is, suppose the leading Is entangled.\n",
        "27:28\n",
        "And uses the most. Uses. This an atomic symbol. Just one. This might be. Beneficial. You might not want to learn. A. Compound. Symbol for it. Why is this? It's more efficient. Even given the possibility of full. Regular system. Might be more efficient to code. The signaling system. Ignore by ignoring this.\n",
        "28:13\n",
        "By ignoring us. So, by ignoring The. Regular. Compound form and keeping I'm not by keeping a simple. A simple. Signal. Right, we use an atomic symbol here. But this would be to the detriment.\n",
        "28:48\n",
        "Being unable to use this. As the final. Symbol. In all the other. Encodings. Might still be efficient if you only use this once.\n",
        "29:10\n",
        "And we might be able to use this second atomic symbol. In this fashion.\n",
        "29:22\n",
        "Perhaps for the second, most Common state, assuming that, it's These first three states are significantly. Orders of magnitude more likely than Either states, right? So we give A and B. And then we use. For petting, the rest of the messages.\n",
        "29:55\n",
        "Cc. Ccpa. And Force free seasons, a and three C's with a b. C's.\n",
        "30:15\n",
        "Cab. Sorry CBA. Ccba and so on.\n",
        "30:29\n",
        "Anyhow.\n",
        "30:34\n",
        "I think this is the way that we could have. Complex. Signaling system emerge.\n",
        "30:48\n",
        "A complex signaling systems with a few persistent. A few persistent.\n",
        "31:06\n",
        "A few persistent irregular forms.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## The Third Question - Signals\n",
        "\n",
        "The third question. Third question. Has to do with what? Of this. Referential game. Makes the big Improvement over the plain vanilla Lewis game. But clearly. The multiple choice aspect of it. Speeds are flirting makes the decision. Quicker.\n",
        "00:37\n",
        "Um, but they think that That's not what introduces. In equilibrium.\n",
        "00:53\n",
        "It's not what? Introduces new equilibriums.\n",
        "01:02\n",
        "So, after the notifications, do we have? Well, one bit Is that we have image classifiers?\n",
        "01:18\n",
        "It's that. Steadiness.\n",
        "01:26\n",
        "Maybe.\n",
        "01:31\n",
        "So, introducing a classifier,\n",
        "01:39\n",
        "May well be equivalent. To introducing Grandma.\n",
        "01:49\n",
        "Not Grandma a grammar.\n",
        "02:09\n",
        "Why is that? Because the grammar is Can be viewed as well. Can be viewed as a decision rule. And,\n",
        "02:28\n",
        "What grammar does essentially?\n",
        "2\n",
        "Speaker 2\n",
        "02:30\n",
        "Uh,\n",
        "1\n",
        "Speaker 1\n",
        "02:31\n",
        "taking a bunch of inputs and\n",
        "02:36\n",
        "Classifies. This certain state?\n",
        "02:46\n",
        "Classifies the inputs into a specific. The state is semantic. But,\n",
        "02:59\n",
        "In the world of neural networks that state made the what we need to reconstruct an image. Or some other pretty linguistic options.\n",
        "03:13\n",
        "Three linguistic object.\n",
        "03:19\n",
        "Wow, this is a very bad transcription.\n",
        "03:39\n",
        "Anyway, since we can't transcribe pre-linguistic objects, what we should. Say, is a\n",
        "2\n",
        "Speaker 2\n",
        "03:51\n",
        "So\n",
        "1\n",
        "Speaker 1\n",
        "03:52\n",
        "we can transcribe it so we can.\n",
        "03:58\n",
        "Use discommerce a decision rule, which takes inputs. And, and The coached them into a state.\n",
        "04:10\n",
        "The coach them into a state. Decode. The interstate. Into a state.\n",
        "04:28\n",
        "Okay.\n",
        "04:38\n",
        "If we have a certain, Classify working on the inputs. We should be able to recover the original state, the slowest.\n",
        "04:56\n",
        "The senders encoded them in a faithful way in a way, which we haven't. And the essential information.\n",
        "05:12\n",
        "If the coding also has a\n",
        "05:19\n",
        "Homophism and homomorphismic properties.\n",
        "05:27\n",
        "Then our classifier should be able to learn.\n",
        "05:34\n",
        "We cover the state much more easily. Taking advantage of the symmetries.\n",
        "05:46\n",
        "Inherent in. Message.\n",
        "06:03\n",
        "What else do we have in this signaling game? Or rather.\n",
        "06:17\n",
        "Referential game. So, we have\n",
        "06:28\n",
        "A protocol.\n",
        "06:34\n",
        "No, we have something else. We have this thing called the random. Message generally.\n",
        "06:45\n",
        "Surprisingly enough.\n",
        "06:51\n",
        "This is. A great situation for bl. Blind watchmaker to position themselves.\n",
        "07:03\n",
        "What they mean here is that\n",
        "07:11\n",
        "The sender serendipitously. Generates a Message.\n",
        "07:22\n",
        "That can be easily. Broken down. Into units.\n",
        "07:34\n",
        "The receiver will end. We'll learn this structure. Or rather we learned this message.\n",
        "07:48\n",
        "And since it is a classifier, If it gets a bunch of these,\n",
        "07:57\n",
        "It will be much more likely to. Interpret.\n",
        "08:07\n",
        "Similar messages. Correctly and unlikely to interpret different messages correctly.\n",
        "08:26\n",
        "I considered. Situations. The sender. Makes percentage persistent use. Of the same messages for this, for subsequent States.\n",
        "08:46\n",
        "But if he fails And there is an unlimited number of available signals. He could. Generate the new signal.\n",
        "09:05\n",
        "This makes learning slower.\n",
        "09:14\n",
        "This allows.\n",
        "09:20\n",
        "The receiver. More latitude.\n",
        "09:28\n",
        "In the sense that\n",
        "09:34\n",
        "But randomly generating new signals. If we come with a signal, That's a good fit. For the classifier. Based on what it is, already figured out. We're gonna be more likely.\n",
        "09:55\n",
        "Coordinate.\n",
        "10:00\n",
        "A signal. Following the structure. That we learned and maybe the semantics that we learned.\n",
        "10:13\n",
        "So, we can think of this. S exploration.\n",
        "10:19\n",
        "Another idea.\n",
        "10:24\n",
        "Which I had for simpler signaling systems, is to try to swap out signals. This was inspired by Hoffman Coatings. Frequent.\n",
        "10:44\n",
        "Uh, signals would get shorter, encodings If? Distributions evolved over time. And we realized that the rare signal initially is actually now quite common. We would want to Swap it. With some other signal, which we thought was quite common but now we see us Infrequent.\n",
        "11:23\n",
        "This ability to swap. Would need to be coordinated, too.\n",
        "11:32\n",
        "But we could have a protocol for doing this automatically.\n",
        "11:43\n",
        "We could at every time step. Uh, reconsider All signals.\n",
        "11:57\n",
        "Frequencies and variances. And use these two details. To reorder. Their semantics.\n",
        "12:12\n",
        "In other words to associate shorter signals with Frequent States and longer signals with infrequent States.\n",
        "12:33\n",
        "What about longer signals? How do we optimize these? We need the most sophisticated exploratory strategy.\n",
        "12:50\n",
        "One smart way might be to use some kind of a function So,\n",
        "12:59\n",
        "I would call this. Looking at the search base. Homomorphisms. And homeomorphisms. Both. That would make more sense in the realm of the lie algebra.\n",
        "13:24\n",
        "Suppose we do have. Operators. Which preserve Allah. We might want to. Try to use these.\n",
        "13:38\n",
        "Component of Sequence generator. We will want to use these to map the state to sequence.\n",
        "13:53\n",
        "For the sequence. With the the image of this layout in the loops, Another set.\n",
        "14:09\n",
        "Lie, algebra group.\n",
        "14:14\n",
        "On the set.\n",
        "14:23\n",
        "I don't think my job here is to Solve this, but It appears that.\n",
        "14:36\n",
        "That, what what? What, what? What?\n",
        "14:44\n",
        "Yes, yes. Yes. Yeah, it would appear that. The two beds.\n",
        "15:01\n",
        "Referential game to capture Coptic signals.\n",
        "15:11\n",
        "Complex signals are\n",
        "15:17\n",
        "A generator.\n",
        "15:21\n",
        "Is aminable. To create.\n",
        "15:32\n",
        "Nice.\n",
        "15:37\n",
        "Representations. Nice sequences which are the compass composable.\n",
        "15:49\n",
        "Which are composable.\n",
        "15:54\n",
        "They're not composable, but they are. Amountable. To disentanglement. Of meaning.\n",
        "16:07\n",
        "This is all fine.\n",
        "16:13\n",
        "Via trend and error. As long as we don't have to, We use symbols. If, for yourself, we want to reuse symbols, you really need to have Planning generator. In other words, we want the sender to\n",
        "16:41\n",
        "The messages. So that\n",
        "16:50\n",
        "When he sends blue banana. He's already thought about.\n",
        "16:58\n",
        "Green banana and yellow banana.\n",
        "17:06\n",
        "And reserved. The symbols for banana green, blue, and yellow. And we purchased these symbols. Reserve these symbols.\n",
        "17:28\n",
        "When such States arise in the future.\n",
        "17:35\n",
        "So this is the planning. I've been thinking about, In the late signal games.\n",
        "17:52\n",
        "When it comes to arbitration, when it comes to arbitrary. When it comes to arbitrary States.\n",
        "18:06\n",
        "Planning is more challenging.\n",
        "18:11\n",
        "And still, we will. Similar states to have similar embeddings. In other words to be close.\n",
        "18:24\n",
        "The linguistic manifold as well as\n",
        "18:32\n",
        "Three-Linguistic manifold. Pretty linguistic manifold. The state manifold.\n",
        "19:03\n",
        "Better yet if we can also. Support. Structure. If we support structure, we get learnability. We also capture salience. And I think salience is at the root of semantics. Or at least. One of the corners of semantics.\n",
        "19:34\n",
        "When it comes to, Thinking about threats. Saliency. Captures the greater threats. In that sense. It's similar to the. Reward hypothesis. Perhaps.\n",
        "20:01\n",
        "Okay, so we say this, Suppose we have.\n",
        "20:10\n",
        "Suppose we have this.\n",
        "20:15\n",
        "Smart. Generator. Of messages is from. And suppose.\n",
        "20:30\n",
        "Our Center gauges in planning, he can decompose the state. For example, if it's a dick dictionary of key values.\n",
        "20:42\n",
        "In such a way that It is easy to learn. To recover. These things.\n",
        "20:58\n",
        "Then.\n",
        "21:04\n",
        "We will probably. Have a nice representation.\n",
        "21:13\n",
        "Yes, and\n",
        "21:22\n",
        "Then we can look at learnability.\n",
        "21:32\n",
        "We could request teaching sessions in a teaching session. The sender gets to pick the state. The center gets to pick the state.\n",
        "21:48\n",
        "The sender gets to pick the state.\n",
        "21:55\n",
        "Possibly a partial.\n",
        "22:03\n",
        "And then, Sender and receiver. Exchange, an atomic symbol.\n",
        "22:16\n",
        "We're learning the language. We might not have a state corresponding to a pronoun. Just sentences. But we still\n",
        "2\n",
        "Speaker 2\n",
        "22:25\n",
        "want to be able to teach a pronoun.\n",
        "1\n",
        "Speaker 1\n",
        "22:28\n",
        "So, we want Send a partial.\n",
        "22:38\n",
        "If we can. Teach our student partial States.\n",
        "22:50\n",
        "That's great. Alternatively, we might not be able to do this in this game. Another approach. Would be to give them the capacity to\n",
        "23:03\n",
        "Consider these partial States.\n",
        "23:08\n",
        "This can happen in the classifier. Or it can happen.\n",
        "23:16\n",
        "In a specialized components. What does that mean? Specialist component here. Um, Would be some internally motivated linguist. Perhaps. Something that looks at these sequences.\n",
        "23:41\n",
        "And, Tries to. Assign each bit. Semantic semantics.\n",
        "23:57\n",
        "Anyhow.\n",
        "24:11\n",
        "A third. Aspect. Is that we? Perhaps. The generator. In the classifier.\n",
        "24:29\n",
        "The generator, and the classifier. To have their own actor critic.\n",
        "24:38\n",
        "Architecture.\n",
        "24:45\n",
        "The linguist is the critic. The actor. Is the generator.\n",
        "24:58\n",
        "That's an interesting architecture.\n",
        "25:17\n",
        "This seems to be leading towards. The fourth problem.\n",
        "3\n",
        "Speaker 3\n",
        "25:25\n",
        "The fourth problem.\n",
        "1\n",
        "Speaker 1\n",
        "25:28\n",
        "It's composition. Of noise game.\n",
        "25:39\n",
        "Other games. What we discussed so far has been. In my view. Good position of a loose game. With classifier, perhaps a classification game.\n",
        "26:02\n",
        "We could break it down even further. Perhaps.\n",
        "26:09\n",
        "Serializer. The serializer is.\n",
        "26:16\n",
        "A simple.\n",
        "26:22\n",
        "Converter.\n",
        "26:28\n",
        "The serial.\n",
        "26:46\n",
        "Grammar. Is really what's in charge of?\n",
        "26:59\n",
        "Breaking down and assembling things in a meaning preserving form. So if we talked about homomorphisms, And the hobo milk prisms,\n",
        "27:15\n",
        "This is not arbitrary, but You're responding to some.\n",
        "27:25\n",
        "Semantics some meaning. What's the meaning?\n",
        "27:36\n",
        "In the most General way. The meaning comes from, And then DP, being an MDP. That we need to take decisions that we have states that the transitions leads to rewards and that there is a goal. And, We want to be. Maximizing. These Maximizing. This, the This go.\n",
        "28:14\n",
        "Maximizing. The expected. That's the MDP view in Pac-Man. We shift from\n",
        "28:29\n",
        "Generalities of the mdp2, specifics. Damn. And in this sense,\n",
        "28:43\n",
        "Semantics shift from a specifics on the general to the specific.\n",
        "28:51\n",
        "I think. I'm almost done here. Um,\n",
        "29:03\n",
        "We probably want to be able to, Capture.\n",
        "29:13\n",
        "A framing game Beyond just. Crucification.\n",
        "29:20\n",
        "Which is happening in the MVP. Particularly, in the case of\n",
        "29:30\n",
        "Lifelong agents. There may be multiple\n",
        "29:36\n",
        "Multiple games. Happening.\n",
        "29:42\n",
        "In parallel. We wish we need to interact.\n",
        "29:57\n",
        "Would be good tomorrow.\n",
        "30:02\n",
        "Loose game interacting with this other kind of game.\n",
        "30:08\n",
        "And implement it. Using a simple obstruction.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Grounding \n",
        "The fourth and Final  question is one of grounding\n",
        "\n",
        "THis means aligning semantics coordinated using a lewis game with the semantics of some external system. This could be users who whish to understand and interpret the signals. It might be some language model that uses the lewis game to mediate with some other game. It might be more simple some external framing game, e.g. one where we learn to classify images and use the lewis game to interpret messages that indicate the the right one to pick.\n",
        "\n",
        "One aspect of grounding I got from reviewing [@]\n",
        "\n",
        "Grounding can be thought of as happening in the lewis game. But we might want the semantics to be able to permeate into the framing games neural network at the many levels is uses to learn it's representations. This hypotheticaly might allow the all seeing eye of the CNN or vision transformer to have neurons corresponding to semantic sub-states in layer correspoing to the required receptive fields needed to identify these sub-state represnations.\n",
        "\n",
        "I realy don't have much to say about grounding.\n",
        "\n",
        "Ok tha't a bit of a stretch. There is this:\n",
        "\n",
        "We talk about states and pre-linguistic objects. But where do these come from? Can't say much about it in general. But in RL or MARL we like to think that states come from a  Markov Chain part of out Markov Decision Process. This means something wonderful - we can if we want think of the States coming from an MDP. Learning RL I have been doing so much thinking about MDPs. Tabular states, States with continuous variables. States with action values. States with Features and using them to build Function approximators, temporal abstraction over states. Markov aggregations of states. \n",
        "\n",
        "Sorry to bore you with this long chain of thought -- Lewis games are ofthen considered stateless and can be solved by bandit like algorithms. Sure [Emergence of lanGuage in Games](https://github.com/facebookresearch/EGG), c.f. (@kharitonov:etal:2021) uses [reinforce](), however it still seems overkill. But when we get to complex signals we need the ability to learn aggregation rules, grammars, homomorphisms, homeomorphisms, and the like.\n",
        "\n",
        "\n",
        "## Planning in the Lewis Game {#sec-planning}\n",
        "\n",
        "How can the sender use planing in the lewis signaling game to build a signaling system"
      ],
      "id": "a7a73fd4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/oren/.local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}