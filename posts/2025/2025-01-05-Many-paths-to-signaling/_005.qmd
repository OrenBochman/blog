
## 2. Learning the Saliency distribution.

In this case agents are in an MDP or a PMDP. They are observing the states of prelinguistic objects and we need to assume that this distribution is the same for all agents. I.e. they are learning the distribution of states by sampling. Should they engage in developing a signaling system or wait until they have learned the distribution of states? What if signaling cost is 
fixed like above in the Lewis game with urgency?

::: {.callout-note}

### Story: Creation of the Oracle of Bayes

In another tribe of agents are too busy observing and recording the states of the world to coordinate on a signaling system.
However, it is inevitable that sooner or later as they compare notes they will notice that they have recorded the full empirical distribution of states and that all thier records are in agreement up to an acceptable margin of error. 

The agents can actually use this distribution as a bayesian oracle.

This time each prelinguistic object is assigned a probability of occurrence. The agents order the states by decreasing probability. 

Again if states share probabilities - they will have to be assigned a word sense index to distinguish them using trial and error.

However doing nothing might give these agents a lot of time on thier hands and they might also notice how their Empirical distribution is evolving over time with a slowly increasing number of states (the most frequent ones) getting the same share of the probability mass....

This suggests to the bayesian minded agents that they should estimate the bayesian credible interval for the signals and use the implied signaling systems to communicate about the states
that are common knowledge.

Whenever a state's probability emerges into 'significance' it should be recorded into the lexicon. If the term has entered into
the lexicon by chance it can be dropped if it is no longer within the bayesian credible interval.

:::


1. The main reason I like thinking about thus story is has to do with it relation with corpus linguistics and ir relation to language modeling. We know that language modeling is at the heart of Large language models and this may be a kind of thought experiment about how long in terms of a clock that ticks time in samples collected a RL language modeler would be able to make good inference about increasingly rare states. Look at a sufficiently long n-gram and almost all are sparse. And for a fixed n with even a uniform distribution the probability of most n-grams in an empirical distribution will be very low, unless the corpus is allowed to grow combinatorially.
2. We can also use this to think beyond the lexicon. [Complex signaling systems are built on top of an alphabet of primitive signals. There like our alphabet might be without meaning or they may be used in a huffman code and assigned to the most frequent states.]{.aside} One cause of hallucinations in LLM is called out of distribution queries. This is when there isn't data corresponding to the query and the model tries to construct an response based on an mostly random approximation. We often get hallucination also for queries the LLM has been trained on and even when it can give a good answer to a better prompt. I like to think of these as signals that do not have separating equilibria. 





Another point is to consider that if agents just observe states long enough they should eventually learn to approximate the state distribution. How long would this take ?

Here is a back of the napkin calculation.

If there least common state has probability $\alpha$ and the agents want to know the distribution with confidence $\epsilon$ they would need, according to Hoeffdingâ€™s Inequality

$K\ge\frac{log(2/\epsilon)}{2\alpha^2} \qquad \text{(samples to learn S)}$

also recall that although there is no lower bound on $\alpha$ when $S\sim Uniform[N]$ the upper bound is $1/N$

$K\ge\frac{N^2log(2/\epsilon)}{2} \qquad \text{(samples to learn uniform S)}$

``` {python}
#| label: upper_bound_estimation
import math

# Given values
K = 8 # states
epsilon = 0.34 # confidence


# Calculate time to learn the saliency distribution 
# N using the formula N >= (K^2 * log(2 / epsilon)) / 2
N = (K**2 * math.log(2 / epsilon)) / 2
print(f'Expected time {int(N)} to learn a {K} state distribution with confidence {epsilon}')  

# Expected time to learn a signaling system with N states

T = K * math.log(K)
print(f'Expected time {int(T)} to learn a {K} signaling system  ')
```

So learning a signaling systems is easier then learning the distribution of states. Once they they know how to signal states it is easy to use this system to communicate the distribution to all the receivers.

We have not put a cost on learning the signaling system. But if there was a cost associated with learning we could use it to model when agents would prefer to learn the signaling system or just wait until they can infer the distribution of states and infer they systems from that.

<!-- simulate --> 

A third point is that if they are bayesian they could start to infer the signaling system after viewing a few stats and update thier system as they update their beliefs regarding the distribution of states.

<!-- simulate --> 