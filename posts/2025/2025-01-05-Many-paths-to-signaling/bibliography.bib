@article{bouchacourt2018agents,
  title={How agents see things: On visual representations in an emergent language game},
  author={Bouchacourt, Diane and Baroni, Marco},
  journal={arXiv preprint arXiv:1808.10696},
  year={2018}
}

@misc{lazaridou2018emergencelinguisticcommunicationreferential,
      title={Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input}, 
      author={Angeliki Lazaridou and Karl Moritz Hermann and Karl Tuyls and Stephen Clark},
      year={2018},
      eprint={1804.03984},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1804.03984}, 
}

@inproceedings{chaabouni-etal-2020-compositionality,
    title = "Compositionality and Generalization In Emergent Languages",
    author = "Chaabouni, Rahma  and
      Kharitonov, Eugene  and
      Bouchacourt, Diane  and
      Dupoux, Emmanuel  and
      Baroni, Marco",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.407",
    doi = "10.18653/v1/2020.acl-main.407",
    pages = "4427--4442",
    abstract = "Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.",
}

@misc{mu2022emergentcommunicationgeneralizations,
      title={Emergent Communication of Generalizations}, 
      author={Jesse Mu and Noah Goodman},
      year={2022},
      eprint={2106.02668},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.02668}, 
}

#misc{rita2022emergentcommunicationgeneralizationoverfitting,
#      title={Emergent Communication: Generalization and Overfitting in Lewis Games}, 
#      author={Mathieu Rita and Corentin Tallec and Paul Michel and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux and Florian Strub},
#      year={2022},
#      eprint={2209.15342},
#      archivePrefix={arXiv},
#      primaryClass={cs.MA},
#      url={https://arxiv.org/abs/2209.15342}, 
#}



@book{deutscher2006unfolding,
  title={The Unfolding of Language: An Evolutionary Tour of Mankind's Greatest Invention},
  author={Deutscher, G.},
  isbn={9781466837836},
  url={https://books.google.co.il/books?id=maz9oLIKZKkC},
  year={2006},
  publisher={Henry Holt and Company}
}


@article{kirby2001spontaneous,
  title={Spontaneous evolution of linguistic structure-an iterated learning model of the emergence of regularity and irregularity},
  author={Kirby, Simon},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={5},
  number={2},
  pages={102--110},
  year={2001},
  publisher={IEEE}
}

@article{kottur2017natural,
  title={Natural language does not emerge'naturally'in multi-agent dialog},
  author={Kottur, Satwik and Moura, Jos{\'e} MF and Lee, Stefan and Batra, Dhruv},
  journal={arXiv preprint arXiv:1706.08502},
  year={2017}
}

@article{galke2022emergent,
  title={Emergent communication for understanding human language evolution: What's missing?},
  author={Galke, Lukas and Ram, Yoav and Raviv, Limor},
  journal={arXiv preprint arXiv:2204.10590},
  year={2022}
}

@inproceedings{chaabouni2022emergent,
  title={Emergent communication at scale},
  author={Chaabouni, Rahma and Strub, Florian and Altch{\'e}, Florent and Tarassov, Eugene and Tallec, Corentin and Davoodi, Elnaz and Mathewson, Kory Wallace and Tieleman, Olivier and Lazaridou, Angeliki and Piot, Bilal},
  booktitle={International conference on learning representations},
  year={2022}
}

@article{rita2022role,
  title={On the role of population heterogeneity in emergent communication},
  author={Rita, Mathieu and Strub, Florian and Grill, Jean-Bastien and Pietquin, Olivier and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2204.12982},
  year={2022}
}

@article{rita2020lazimpa,
  title={" LazImpa": Lazy and Impatient neural agents learn to communicate efficiently},
  author={Rita, Mathieu and Chaabouni, Rahma and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2010.01878},
  year={2020}
}

@article{cogswell2019emergence,
  title={Emergence of compositional language with deep generational transmission},
  author={Cogswell, Michael and Lu, Jiasen and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  journal={arXiv preprint arXiv:1904.09067},
  year={2019}
}
@article{cornish2017sequence,
  title={Sequence memory constraints give rise to language-like structure through iterated learning},
  author={Cornish, Hannah and Dale, Rick and Kirby, Simon and Christiansen, Morten H},
  journal={PloS one},
  volume={12},
  number={1},
  pages={e0168532},
  year={2017},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{Wieczorek2024framework,
author = {Wieczorek, Tobias and Tchumatchenko, Tatjana and Wert-Carvajal, Carlos and Eggl, Maximilian},
year = {2024},
month = {08},
pages = {},
title = {A framework for the emergence and analysis of language in social learning agents},
volume = {15},
journal = {Nature Communications},
doi = {10.1038/s41467-024-51887-5}
}


@article{Barrett2006Numerical,
	author = {Jeffrey A. Barrett},
	journal = {Working Paper MBS06–09},
	title = {Numerical Simulations of the Lewis Signaling Game: Learning Strategies, Pooling Equilibria, and the Evolution of Grammar},
	year = {2006},
	abstract = {David Lewis (1969) introduced sender-receiver games as a way of investigating how meaningful language might evolve from initially random signals. In this report I investigate the conditions under which Lewis signaling games evolve to perfect signaling systems under various learning dynamics. While the 2-state/2- term Lewis signaling game with basic urn learning always approaches a signaling system, I will show that with more than two states suboptimal pooling equilibria can evolve. Inhomogeneous state distributions increase the likelihood of pooling equilibria, but learning strategies with negative reinforcement or certain sorts of mutation can decrease the likelihood of, and even eliminate, pooling equilibria. Both Moran and APR learning strategies (Bereby-Meyer and Erev 1998) are shown to promote successful convergence to signaling systems. A model is presented that illustrates how a language that codes state-act pairs in an order-based grammar might evolve in the context of a Lewis signaling game. The terms, grammar, and the corresponding partitions of the state space co-evolve to generate a language whose structure appears to reflect canonical natural kinds. The evolution of these apparent natural kinds, however, is entirely in service of the rewards that accompany successful distinctions between the sender and receiver. Any metaphysics grounded on the structure of a natural language that evolved in this way would track arbitrary, but pragmatically useful distinctions.}

}

@article{Barrett2007Dynamic,
	author = {Jeffrey A. Barrett},
	doi = {10.1086/524714},
	journal = {Philosophy of Science},
	number = {4},
	pages = {527--546},
	publisher = {University of Chicago Press},
	title = {Dynamic Partitioning and the Conventionality of Kinds},
	volume = {74},
	year = {2007}
}

@article{barrett2009evolution,
	author = {Jeffrey A. Barrett},
	doi = {10.1007/s11238-007-9064-0},
	journal = {Theory and Decision},
	number = {2},
	pages = {223--237},
	publisher = {Springer},
	title = {The Evolution of Coding in Signaling Games},
	volume = {67},
	year = {2009}
}

% - MIE alg for XAI
@incollection{skyrms2010signals,
    author = {Skyrms, Brian},
    isbn = {9780199580828},
    title = "{14512 Complex Signals and Compositionality}",
    booktitle = "{Signals: Evolution, Learning, and Information}",
    publisher = {Oxford University Press},
    year = {2010},
    month = {04},
    abstract = "{This chapter focuses on an earlier point in the evolution of signaling. It considers how one might come to have — in the most primitive way — a complex signal composed of simple signals. This is done with the smallest departure possible from signaling models that have been previously examined in this book.}",
    doi = {10.1093/acprof:oso/9780199580828.003.0013},
    url = {https://doi.org/10.1093/acprof:oso/9780199580828.003.0013},
}

@incollection{skyrms2010signalsCh12,
    author = {Skyrms, Brian},
    isbn = {9780199580828},
    title = "{14512 Complex Signals and Compositionality}",
    booktitle = "{Signals: Evolution, Learning, and Information}",
    publisher = {Oxford University Press},
    year = {2010},
    month = {04},
    abstract = "{This chapter focuses on an earlier point in the evolution of signaling. It considers how one might come to have — in the most primitive way — a complex signal composed of simple signals. This is done with the smallest departure possible from signaling models that have been previously examined in this book.}",
    doi = {10.1093/acprof:oso/9780199580828.003.0013},
    url = {https://doi.org/10.1093/acprof:oso/9780199580828.003.0013},
    eprint = {https://academic.oup.com/book/0/chapter/143895157/chapter-ag-pdf/45018345/book\_3092\_section\_143895157.ag.pdf},
}


@book{sutton2018reinforcement,
  title = {Reinforcement Learning: An Introduction},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second},
  keywords = {rl, Rinforcement Learning},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  added-at = {2019-07-13T10:11:53.000+0200},
  url = {http://incompleteideas.net/book/the-book-2nd.html}
}

@misc{rita2022emergent,
      title={Emergent Communication: Generalization and Overfitting in Lewis Games}, 
      author={Mathieu Rita and Corentin Tallec and Paul Michel and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux and Florian Strub},
      year={2022},
      eprint={2209.15342},
      archivePrefix={arXiv},
      primaryClass={cs.MA}
}

@misc{rita2022role,
      title={On the role of population heterogeneity in emergent communication}, 
      author={Mathieu Rita and Florian Strub and Jean-Bastien Grill and Olivier Pietquin and Emmanuel Dupoux},
      year={2022},
      eprint={2204.12982},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      keywords={Emergent communication, population heterogeneity, generalization, overfitting, Lewis games},
}

@book{lewis1969convention,
	address = {Cambridge, MA, USA},
	author = {David Kellogg Lewis},
	editor = {},
	publisher = {Wiley-Blackwell},
	title = {Convention: A Philosophical Study},
	year = {1969}
}


@article{Bereby1998Loser,
title = {On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain},
journal = {Journal of Mathematical Psychology},
volume = {42},
number = {2},
pages = {266-286},
year = {1998},
issn = {0022-2496},
doi = {https://doi.org/10.1006/jmps.1998.1214},
url = {https://www.sciencedirect.com/science/article/pii/S0022249698912147},
author = {Yoella Bereby-Meyer and Ido Erev},
abstract = {One of the main difficulties in the development of descriptive models of learning in repeated choice tasks involves the abstraction of the effect of losses. The present paper explains this difficulty, summarizes its common solutions, and presents an experiment that was designed to compare the descriptive power of the specific quantifications of these solutions proposed in recent research. The experiment utilized a probability learning task. In each of the experiment's 500 trials participants were asked to predict the appearance of one of two colors. The probabilities of appearance of the colors were different but fixed during the entire experiment. The experimental manipulation involved an addition of a constant to the payoffs. The results demonstrate that learning in the loss domain can be faster than learning in the gain domain; adding a constant to the payoff matrix can affect the learning process. These results are consistent with Erev & Roth's (1996) adjustable reference point abstraction of the effect of losses, and violate all other models.}
}


@misc{cn,
  title={Citation Needed},
}

@book{von1999humboldt,
  title={Humboldt: 'On Language': On the Diversity of Human Language Construction and its Influence on the Mental Development of the Human Species},
  author={von Humboldt, W. and Losonsky, M. and Heath, P.},
  isbn={9780521667722},
  series={Cambridge Texts in the History of Philosophy},
  year={1999},
  publisher={Cambridge University Press}
}

@article{barrett2017self,
  title={Self-assembling games},
  author={Barrett, Jeffrey A and Skyrms, Brian},
  journal={The British Journal for the Philosophy of Science},
  year={2017},
  publisher={The University of Chicago Press}
}

@article{meta2022human,
  title={Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  author={Meta Fundamental AI Research Diplomacy Team (FAIR)† and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1067--1074},
  year={2022},
  publisher={American Association for the Advancement of Science}
}


