---
date: "2025-01-05"
title: The Many Path To A Signaling System
bibliography: ./bibliography.bib
categories: [signaling systems, lewis signaling game, reinforcement learning, bayesian games, information theory, game theory, bayesian reinforcement learning,emergent languages]
keywords: [compositionality, partial pooling equilibria, Huffman codes, spontaneous symmetry breaking, Sapir-Whorf hypothesis]
format: 
    html: 
        code-fold: true
---

Although the Lewis signaling game, c.f. [@lewis1969convention] is touted as a mechanism for the emergence of language, [@skyrms2010signals] and others suggest different ways that this can take place, including evolution and reinforcement learning. In many cases languages emerges by spontaneous symmetry breaking  but in other cases a language creator may wear the hat of an engineer. The point is that small changes can have both large or nuanced impact on the languages that arise.

Most of these are from my own thinking but the more I delve into the literature the more I realize that these and many others have been considered by others as well. Accordingly I'll try and add citations, however this is less a literature review and more an exercise in understanding how we may one day get agents to quickly learn to communicate effectively in a broad spectrum of different settings. As such I also consider some cases where the outcomes are less than stellar and what one might want to avoid.


::: {.callout-important}

## TL;DR {.unnumbered}

![Emergent Languages in](/images/in_the_nut_shell_coach_retouched.jpg)

One fascinating aspect the Lewis signaling game [@lewis1969convention] is that although there are many theoretical equilibria initially the agents will inevitably fail to coordinate and they can only reach the optimal signaling systems after some iterations of the game in which they either evolve or use reinforcement learning to coordinate a themselves to a common signaling strategy. In the prisoners dilemma agents can learn to cooperate if the game is iterated. In the Lewis signaling game agents can learn to coordinate on a signaling system if the game is iterated. 


Generally to find a good signaling system requires some kind of algorithm and at least between N and $N^2$ steps as well some number of iterations. I don't recall seeing a discussion of the the minimum or the expected number of iterations required to reach a signaling system under different algorithms. In other words most researchers have considered the complexity of coordination in signaling systems. This is actually a fairly simple problem to solve in the most common settings.

:::


Another two point primarily addressed by the evolutionary game theory community who view evolution in terms of replicator dynamics is that of stability of equilibria and the notions of evolutionarily stable strategies. 

The first has to do with convergence of learning to an optimal signaling system. 

The second has to do with the ability of an equilibrium to resist invasion by a mutant strategy.

### Enumerating the different type of signaling systems and the other types of equilibria.

A related issues is that of enumerating different types of equilibriums in larger games. For basic Lewis Signaling games this is not very difficult as there are N! signaling systems in games with N signals and N states, 

For a complex signaling system with N states and M signals we can enumerate the signals as the first N base M numbers. Once again we deal with N! permutations. However the sender may chose any set of base N base M numbers. This creates an potentially unbounded number of signaling systems. This is perhaps a reflection of Wilhelm von Humboldt characterization of  "infinite use of finite means" meaning that a language as  systems in which a finite number of symbols can be combined in an unbounded number of ways, c.f. [@von1999humboldt]

This perhaps makes the complex signaling game special as a game theoretic problem. At least in the sense of what we consider bounded rationality. It is not at all clear what solution concept could be used to create an optimal signaling system, in this case it should require deep insights into group theory, topology, information theory, category theory, probability theory. Also though I consider the problem of equlibria in terms of an enumeration of states via numeric signals, it is does not at all follow that this is the best way to consider the problem. 
If we use a an alphabet of M phonemes we may for instance run into phonotactic constraints that are not at all present in the numeric representation. Thus another source of complexity may arise in terms of the actual realization of the signaling system.
This is perhaps why this needs to be a working paper -- in which new ideas can be added as they come to me.

For complex signaling systems we need to consider

1. Are the infinite number of signaling systems equivalent up to an isomorphism? I believe that the answer to this is yes by the following rationale. Any signaling system can be viewed as a permutation of signals to states. And according to the Cayley's theorem any groups can be represented as a permutation. Hence we can view any complex signaling system as some group! And groups are equivalent if they are related by a group homomorphism.
However there are still a couple of conundrums to consider. If the prelinguistic objects we call the states have a group structure and this is preserved this seems like a signaling systems that is a faithful representation. But it is also possible that there is a mismatch - that some of the structure is lost or that some additional structure in the language is added that is not in the 
original pre linguistic objects. I think that some of these might be viewed as happy accident while others may be failures in therms of signaling systems.
2. Can the Lewis signaling game together with the pre-linguistic object imbue the language with semantics ?
3. At what point can we view signaling systems universal in terms of the Sapir-Whorf hypothesis. I.e. when does the semantics of the signaling system becomes capable of representing the semantics of any natural language?



IT is entirely possible to 



Another point of interest to me is to consider the emergence of grammar and of a morphology. In [@nowak1999evolution] The authors give a result for the emergence of grammar in a signaling system. This is that there are many more 

I think it worth while to list them in this space --- particularly as I believe that signaling systems are a key for transfer learning in reinforcement learning which together with learning to represent complex states may be the key to AGI.


## Introduction

- Listing number of different scenarios on how signaling systems can arise in the Lewis signaling games. 
- I will start with a story 
- Next add some details like some variants and look some basic analysis. 
- Finally I'll try to place it into the context of MARL. Note that we will be dealing with partially observed multi agent RL. But each scenario can have a different setting.

![lewis signaling game](./lewis_extensive_form.svg){.center}

In The book signals [@skyrms2010signals] the author, Skryms, discusses how Lewis challenged the skepticism of his advisor Quine regarding the  meaning and convention may arise via an arbitrary mechanism like symmetry breaking.

When I considered solving some additional issues surrounding the fundamentals of signaling systems I realized that I had a few different scenarios in mind and that writing them down with some semblance of formalism might be helpful. It turns out that indeed this turns out to be a stepping stone towards developing an optimal algorithms for learning signaling system in different rl settings.

Let's face it under different settings the task of acquiring a signaling system can be easier or harder. In [@skyrms2010signals] the author points out that at symmetry breaking all the different signaling systems that could be learned are equivalent. However if there is an asymmetry in the form of a non-uniform distribution of states or different signaling risks then we we might prefer some signaling systems over others and there might even be a unique optimal signaling system. Furthermore like in reality one would expect that with time distributions of states might change and the optimal signaling system might change as well.

## 1. The Oracle of Saliency

In many cases the arduous task $O(n^2)$ of coordination that makes signaling hard might be avoided. If both agents share some mechanism for enumerating the pre-linguistic objects they encounter, they can systematically enumerate them before the game start. The enumeration is a lexicon and can be used as a signaling system.

You mention that oracles are cryptic - 
 They can now pick use the binary number as canonically they can avoid some or all of the cost of coordination. 

::: {.callout-note}

### Story: The Oracle of Saliency

The sender in an ex-ante step consults an oracle, perhaps the `I Ching` asking what to do for each prelinguistic objects. Each consultation returns a cryptic message not unlike a cryptographic hash. The sender then sorts the objects in increasing order of the cryptic messages, assigning each subsequent binary number to each. 

The receiver also uses the same process with his copy of the oracle. 

Since both the sender and receiver now have access to the same lexicon, they can use it as a signaling system.
:::


1. Even is the oracle does not provide unique enumerations it will reduce the task from $O(N^2)$ to $O(N)$. as once the oracle is consulted the sender and receiver can proceed by trial and error to further coordinate on the ambiguous signals. e.g. by assigning a sub-index to each prelinguistic object that share a signal. This will now look more like a prefix coding scheme.
2. In the lewis game errors cost nothing, but in RL we often encode a sense of urgency by adding a penalty to time wasting moves. If there a penalty e.g. -1 for wasting time on miscoodination, we might call the game the `Lewis game with urgency`. Agents that get to play an infinite nuymber of time would see an infinite rewards past at most $O(n^2)$ penalties and want to play. In this case though even if consulting the oracle has a cost C, so long a the expected cost of trial and error is greater than  k i.e. if there are k^2 uncoordinated states agents would have incentive to consult the oracle. 
3. In the previous points we ignored the reality in which states are not uniformly distributed and that agents may not be able to pay the oracle upfront. If the agents know the most likely states they this knowledge to setup a self-financing scheme to raise funds for consultations of the oracle as well as reduce thier cost of coordination. However it's worth pointing out that our next scenario considers how knowing the distribution of states is just another type of oracle.

Two cases come to mind.

1. They have booth been observing the state space long enough to infer the distribution of states to a high degree of confidence. [**coordinate via the state distribution**]{.column-margin}
2. They can listen to a third party who knows the distribution and learn to signal from them. [**coordinate by imitation**]{.column-margin}
3. They can access a state classifier and send it random noise thus deriving an empirical distribution of states in the classifier (not nature) and use it to learn the signaling system. [**coordinate via a classifier**]{.column-margin}

Once a distribution of states in known it can be used to create huffman codes using 0 and 1. These signals are then ranked.

There is a distribution of the states of the word known to all players.

-   In the easiest case each state has a different probability of occurring. -It is easiest because all players can infer a `canonical signal system` from such a distribution of states.
    - They order states and corresponding actions in decreasing expected value. The canonical system is the one mapping between the states and the actions.
    - Thus the salience distribution breaks the symmetry of all viable signaling systems and leaves just one option.[^1]
-   In each subsequently harder case there are two or more states with equal probability of occurring. These probabilistic symmetry of these states cannot be broken as before and require the use of coordination. The coordinators can break the symmetry by trial and error when that state arises. Once all the symmetries have been coordinated the players can infer the rest via the canonical signal system from the distribution of states.
-   In the worst case all states have equal probability of occurring. This is the hardest case because after each state signal pair the problem is still maximally symmetric. The players need to solve this by using trial and error.

[^1]: This is notion of a most salient mapping acts as an optimal policy for agents who need to quickly avoid the long run costs of a non salient signaling system

::: {.callout-tip}

### MARL Formulation

In terms of the MARL formulation:

- A PMDP has states $S$ and actions $A$. 
    - States are observed by agents of type S whose actions are signals 
    - Actions are performed by agents of type R.
    - Rewards are assigned symmetrically to both All senders and receivers when the receiver action matches the sender observed state.

- States can be uniformly distributed or be drawn from a distribution. 
- We like to call such a distribution the saliency distribution after Schelling notion of a focal point AKA (Schelling point) in his book The Strategy of Conflict. In a lewis signaling game there are n! signaling systems if there are n states, signals and actions. If the states are uniformly distributed then all signaling systems are equivalent. But if the states probabilities are monotonicaly distributed then there is a unique optimal signaling system which is precisely the Schelling point.

- Since saliency 
:::

::: {.callout-caution}
It is also worth noting that many algorithms for MARL use shared parameters, the same critic and so on. And if the agents can access this system the oracle of saliency provides a shortcut to a cannonical emergent language for all agents as well as a general purpose coordination mechanism they might use to coordinate on other tasks. Thus such oracles should be treated with care if we also wish to study the the emergence of a universal language
:::

## 2. Learning the Saliency distribution.

In this case agents are in an MDP or a PMDP. They are observing the states of prelinguistic objects and we need to assume that this distribution is the same for all agents. I.e. they are learning the distribution of states by sampling. Should they engage in developing a signaling system or wait until they have learned the distribution of states? What if signaling cost is 
fixed like above in the Lewis game with urgency?

::: {.callout-note}

### Story: Creation of the Oracle of Bayes

In another tribe of agents are too busy observing and recording the states of the world to coordinate on a signaling system.
However, it is inevitable that sooner or later as they compare notes they will notice that they have recorded the full empirical distribution of states and that all thier records are in agreement up to an acceptable margin of error. 

The agents can actually use this distribution as a bayesian oracle.

This time each prelinguistic object is assigned a probability of occurrence. The agents order the states by decreasing probability. 

Again if states share probabilities - they will have to be assigned a word sense index to distinguish them using trial and error.

However doing nothing might give these agents a lot of time on thier hands and they might also notice how their Empirical distribution is evolving over time with a slowly increasing number of states (the most frequent ones) getting the same share of the probability mass....

This suggests to the bayesian minded agents that they should estimate the bayesian creadibile interval for the signals and use the implied signaling systems to communicate about the states
that are common knowledge.

Whenever a state's probability emerges into 'significance' it should be recorded into the lexicon. If the term has entered into
the lexicon by chance it can be dropped if it is no longer within the bayesian credible interval.

:::


1. The main reason I like thinking about thus story is has to do with it relation with corpus linguistics and ir relation to language modeling. We know that language modeling is at the heart of Large language models and this may be a kind of thought experiment about how long in terms of a clock that ticks time in samples collected a RL language modeler would be able to make good inference about increasingly rare states. Look at a sufficiently long n-gram and almost all are sparse. And for a fixed n with even a uniform distribution the probability of most n-grams in an empirical distribution will be very low, unless the corpus is allowed to grow combinatorially.
2. We can also use this to think beyond the lexicon. [Complex signaling systems are built on top of an alphabet of primative signals. There like our alphabet might be without meaning or they may be used in a huffman code and assigned to the most frequent states.]{.aside} One cause of hellucinations in LLM is called out of distribution queries. This is when there isn't data corresponding to the query and the model tries to construct an response based on an mostly random approximation. We often get helluciantion also for queries the LLM has been trained on and even when it can give a good answer to a better prompt. I like to think of these as signals that do not have seperating equilibria. 





Another point is to consider that if agents just observe states long enough they should eventually learn to approximate the state distribution. How long would this take ?

Here is a back of the napkin calculation.

If there least common state has probability $\alpha$ and the agents want to know the distribution with confidence $\epsilon$ they would need, according to Hoeffdingâ€™s Inequality

$K\ge\frac{log(2/\epsilon)}{2\alpha^2} \qquad \text{(samples to learn S)}$

also recall that although there is no lower bound on $\alpha$ when $S\sim Uniform[N]$ the upper bound is $1/N$

$K\ge\frac{N^2log(2/\epsilon)}{2} \qquad \text{(samples to learn uniform S)}$

``` {python}
#| label: upper_bound_estimation
import math

# Given values
K = 8 # states
epsilon = 0.34 # confidence


# Calculate time to learn the saliency distribution 
# N using the formula N >= (K^2 * log(2 / epsilon)) / 2
N = (K**2 * math.log(2 / epsilon)) / 2
print(f'Expected time {int(N)} to learn a {K} state distribution with confidence {epsilon}')  

# Expected time to learn a signaling system with N states

T = K * math.log(K)
print(f'Expected time {int(T)} to learn a {K} signaling system  ')
```

So learning a signaling systems is easier then learning the distribution of states. Once they they know how to signal states it is easy to use this system to communicate the distribution to all the receivers.

We have not put a cost on learning the signaling system. But if there was a cost associated with learning we could use it to model when agents would prefer to learn the signaling system or just wait until they can infer the distribution of states and infer they systems from that.

<!-- simulate --> 

A third point is that if they are bayesian they could start to infer the signaling system after viewing a few stats and update thier system as they update their beliefs regarding the distribution of states.

<!-- simulate --> 

### Bringing Up Baby

::: {.callout-note}

### Story: Bringing Up Baby 

Here the sender is tha parent and the receiver the child. Each time the child learn a new action a new signal is added to the signaling system. Since the other signals are known the child can learn the new signal in a single step. This is another trivial case where learning is easy.

:::

### Hoppes Urn 

::: {.callout-note}

### Incremental Learning
In RL this is called incremental learning. We can also assign such signals to sequences of actions which we call capabilities. The child can learn a new capability in a single step. This is the most efficient way to learn a signaling system incrementally.

:::


Skryms discusses two methods that agents can use to learn a signaling system incrementally. First is the Chinese restaurant process and the second is the Hoppe urn. He suggest that they are equivalent. I too came up with the Hoppe urn model - as I had already investigated how to codify the most common probability distributions as urn models.

Another way to make learning easier is to always have just one action in context when we need to learn. This allows the receiver to learn the signal system in a single step. It might work with a student learning to signal and act in tandem.[**incremental learning with one new action**]{.column-margin}

In this case urn used in learning have an Hoppe urn with a black stone indicating that a new state action pair is being learned. If the receiver learns the new signal action pair, the agents keep track of it otherwise the new signal and action are discarded.

Note that if the there is only one new state and action a suitable algorithm can learn it immediately. IF there is an exploration - this may cause an error.

We retain this mechanism and might use it for expanding a signaling systems incrementally in the presence of new data.

Note: if there are saliency distributions is being used a new signal would be the last signal in the saliency distribution or in the last group. Over time signals that are not in use might be discarded if thier saliency is bellow the minimum saliency threshold.

## 3. Ship of Fools

::: {.callout-note}

### Story: Ship of Fools

Senders and Recievers lack all prior knowledge. They follow an optimal strategy for a related game the battle of the sexes.
Is a state is uncoordinated senders will explore  randomly pick a signal and recievers will randomly pick an action until they get a reward and exclude the signal action pair from exploration.

:::



This strategy is not the best one for senders, but it is easier to anlyse.

If the state is T and there are N states, signals and actions then are $N\times N$ choices for sender and recievers of which the ones with action A=T get a reward. So the expected reward is 1/N chance of getting a reward. 

The expected rewards are 1/N but since the sender is randomizing each turn is independent. Can they do better?


## 3. The steady navigator

Indeed they can do better. If the sender picks a signal and sticks with it the receiver can eliminate an action each turn. This is the optimal strategy for this, the most common setting of the Lewis signaling game.

::: {.callout-note}

### Story: The Steady navigator

Senders and Receivers lack all prior knowledge. For each new state, the sender picks a signal at random but if the state is the same as the last state the sender sticks to the same signal. The receiver must explore an action at random but if the signal is the same as the a previous seen signal the receiver will explore an an untested action for the signal until they get a reward. 
:::

Lets estimate the expected rewards under this strategy for a state T and N states, signals and actions.

- Sender has 1 signal and
- Since the sender sticks with the same signal the receiver can eliminate an action each turn.
- Receiver has N choices initially with 1 correct choice so we has a expected chance of 1/N of getting a reward.
- Next he can eliminate his first choice and has N-1 choices with 1 correct choice so we has a expected chance of 1/(N-1) of getting a reward.
- And after k tries he has N-k+1 choices with 1 correct choice so we has a expected chance of 1/(N-k+1) of getting a reward.
- In the worst case he will have to try all N actions but
- The Expected number of steps 
$$
\begin{aligned}
\mathbb{E}[steps] &= \sum_{k=1}^{N} \frac{1}{P_{\text{success k}}} \times P_\text{failure up to k} \newline
&= \sum_{k=1}^{N} \frac{1}{{N-(k-1)}} \underbrace{\times \prod_{i=1}^{k-1} \frac{N-i}{N-i+1}}_{\text{telescopic product}} \newline
&= \sum_{k=1}^{N} \frac{1}{\cancel{{N-(k-1)}}} \times \frac{\cancel{{N-(k-1)}}}{N} \newline
\end{aligned}
$$




::: {.callout-tip}

### MARL Formulation

This is basicaly an optimistic initialization strategy. The sender does not explore. The reciever intilizes all signal action pairs optimisticaly with value of 0.5.  This way he will keep exploring untill he gets a reward of 1.0 At this point exploration ends.

:::


So we can expect that the number of steps needed to learn to signal the state T is N.
They should pick a signal for a state and stick with with it. 



## The Guru's Prior

The Sender is a privileged elder who knows the distribution of the states, the associated risk and cost of signaling to sender and receiver and figures our the optimal signaling systems. As such he selects a specific signaling system. This means that students need to coordinate to this system.

-   This means that whenever the state $s_i$ arises we will get signal $sig_i=Send(s_i)$ rather then some random signal. This means that the student for a mistake the *receiver* can use a negative reinforcement for $<sig_i,action_j>$ is the return is 0. This should allow the receiver to narrow down the actions chosen for the next time we he gets that signal.

This is second hardest learning scenario but also most realistic. We don't want to have to learn a new language for every person we meet.

What could happen - the distribution of states could evolve over time.

## The prophet's prior

The sender knows the distribution of the states and how it evolves over time. He choses the currently optimal signaling system. The receivers must learn the signaling system but once a change in the state distribution is observed they will switch to the the new optimal signaling system.

Imagine a world with many predators troubling the signaler. To avoid becoming prey agents must send a risky signals to their neighbors. They should use the signaling with the least expected cost. This cost combines the predator risk and its frequency. Signals can be 1 or 0. 1 is risky and 0 is safe. As frequency of the predators change the optimal signaling system will change as well.

## The Gurus' Posterior

Here there are multiple gurus with knowledge of different distribution. Can they coordinate on the most salient signaling system with respect to thier common knowledge ? 

This should be the signaling system that is most salient for a mixture distribution with weight $w_i$ for each guru.

Lets perhaps assume that there are a very large N and a cutoff $\epsilon$ probability for which the gurus won't bother to include rare sates.


In the second setting two or more students must come up with any signaling systems as fast as possible.


## Babylon Consensus

Multiple senders and receivers take shelter in common ground and need to arrive at a common signaling system.

1. They can want to learn the least costly signaling system in terms of learning.
2. They want to learn the most salient signaling system in terms of the distribution of states.
    3. There is an agent who knows the current distribution of states and the optimal signaling system. 
    4. There isn't such an agent but the senders want to use a 

::: {.callout-note}

### Cost of learning a second dialect


1. for each agent and for each signal that is different from the target signalaling system add a cost of 1.

$$
C = \sum_{i=1}^{N} \sum_{j=1}^{M} \delta_{ij} \\
$$ {#eq-cost}

where $\delta_{ij}$ is 1 if the signal $j$ is different from the target signal for state $i$ and 0 otherwise.   

:::

## POMDP

In this settings one or multiple senders only a partial state. 

Again we consider a hypothetical case where the state describe predators and that it can be partitioned into disjoint parts like <type, proximity> or <type, proximity, number> or <type, proximity, number, direction>. This partioning is also at the basis of compositionality in signaling systems.
 
Skyryms first considers three different settings.

1. **observation one of mutually exclusive partition:** the case where each sender views one part of the partitioned state.
2. **observation of all  mutually exclusive partition** the case where senders see all the parts of the state but don't have a mechanism in place to coordinate who sends which part of the state.
3. **observations of all mutually exclusive partition with coordination** the case where one sender see all the parts of the state but lacks symbols to send the full state and needs to send each part. He must send the parts one at a time resulting in a sequence of signals.

In the first settings the receiver somehow knows that he should first aggregate the signals using a logical and then decode the state.

In the first settings 


where the agent again observe the full state but don't have a a coordination mechanism for picking differnt parts of the message.


They send a partial signal to the receiver who must infer the state and take the appropriate action. The receiver must 

1. aggregate the messages
2. infer the state
3. take the appropriate action

note:


In the first case so long as each part of the state is a unique signal the state can be infered by the reciever using conjunction.
The second case if more problematic and shows us a new way that some signaling systems can be better then others. 

part the agent can't infer the state better then chance. However reinforcement of random partition the senders can learn to send  they both need to learn a decorelated partition for each state the state and send different parts of the state. The issues is if the semantics are composeable.

- An issue here is that there is no guarantte that the senders will send the same part of the state at each turn. If the aggregation rules is conjunction, i.e. logical and, then the receiver will be able to decode the state so long as he gets all the pieces.


## Bayesian Adversarial Signaling

There are multiple senders and each state is known to more than one sender.
Each sender has a voracity parameter $\nu$, this is the probability that they send a faithful signal. 
At one extreme senders make small mistakes and at the other they are completely deceptive.
At the extreme the agents have types (like knights and knaves) and the receivers must learn to classify the agents by type and then learn the signaling system.
Agents need to learn a


## Babbling Bayesian Babies

Babies in the babbling stage of language development are learning to signal. They are sending all possible phonemes and the parents and thier parents either respond or talk to each other. The babies are collecting the feedback and reinforecing both poitively and negatively until they only use the phonemes that are in the language of thier parents. They start with over 300 phonemes and end up with 40-50. 

In this scenario the sender operates at random. Both the sender and the receiver must observe the rewards and reinfoce state signal action triplets.







---



## The evolution of signaling systems

In this section I want to address some of the questions that drive my research on signaling systems.

### When do we expect signaling systems to evolve?

When agents fitness is increasingly predicated on coordination or communication they will get a benefit for evolving signaling systems. I.e. a evolutionary pressure to communicate will lead to the evolution of signaling systems.

### What are the main desiderata for signaling systems?

<!-- this section now has it's own file - consider removing/merging-->

Here are some of the main desiderata for signaling systems:

-   **Efficiency** - the signaling system should be as short as possible. 
-   **Salience** - the signaling system should be most salient for the distribution of states.
-   **Cost** - the signaling system should be as cheap as possible to learn and use.
-   **Robustness** - the signaling system should be robust to noise and deception.
-   **Adaptability** - the signaling system should be able to adapt to changes in the distribution of states.
-   **Compositionality** - the signaling system should be able to be combined with other 
                           RL activities to form
    - more complex signaling system.
    - more complex policies.




This is most clearly illustrated in:

- The **predation scenario** where 
    - Agent's short term survival is predicated on their ability to respond to signals indicating the presence of predators by take the appropriate precautions. Of course signals need a source. 
    - Agents can send a signals for the state they perceive or to stay mute.
    - Agents can repeat signals they receive or stay mute.
    - As predation increases, selection pressure may induce signaling systems to evolve.
- The **Dowery/Courtship scenario** where:
    - The game can be cooperative or competitive.
        - In the competitive case only the fittest agents get a mate.
        - In the cooperative case all agents get to mate but some will mate more often, or with more desirable mates.        
    - Agent must collect resources (e.g. a bill of goods for a dowery) before they can reproducing from a changing landscape.
    - Only the top n dowries will generate an offspring. (bills of goods slowly perish but the size and diversity of is important).
    - Alternatively only the agent that is the the best at courtship n times can generate an offspring. (this time there are smaller bills of good that quickly perish)
    - Resources are plentiful but evanescent.
    - Agent that can signal would be able to collect a dowery faster and increase thier fitness.
    - As competition increases benefits signaling systems should evolve.
    - This is interesting as the exploration/exploitation dilemma caps the rate at which agents can reproduce. Yet signaling will allow agents to over come this cap. 
    - This is also a case where agents may get a benefit from sending false signals if the receiver is a serious contender. So that the receiver will waste time and resources.
    - The agents must learn to discriminate 
    To handle deception agents may also develop a model of the mind of the sender to predict the likelihood of deception. They may also want to tally if the sender has been deceptive in the past.
    - Or 
- The **Knights & Knaves** scenario where:
    - Agents need to: 
        1. Classify agent by type. (knight or knave, monkey, insane, etc.) to interpret the semantics of their signals.
        2. Assemble the state from messages with different semantics to recover the state of the world.
    - This scenario does assumes the agents have an underlying motivation to learn to signal.
    - And now add a selection pressure on the evolution of basic logic and semantics.
    


Agents that communicate can spend less time exploring and more time exploiting.
. In this case the agents will evolve a signaling system that is most salient for the distribution of states. This is the most likely scenario for the evolution of signaling systems.
The reason why agents might want to learn a signaling system is to maximize their fitness


-   What are the main parameters that affect the learning of signaling systems?
    - state distribution (these are the states of the world and signaling is used to share these states with others to maximize fitness - the expected progeny)
    - saliency distribution (weights for states ranking thier risk)
    - voracity of senders.
    - cost of signaling (risk of predation).
-   What are the different settings for learning signaling systems?

Some other questions within these contexts might be:

-   What are the number of signaling systems for a given number of states and actions?
-   What are the number of pooling equilibria for a given number of states and actions?
    -   Let's break these down by the degeneracy of the pooling equilibrium. This might suggest the minimal number of signals needed in an experiment to learn the signaling system. It might also suggest the thresholds of success for optimal signaling systems in different settings.
-   Can we estimate the regret for different RL algorithms ?
    -   What is the expected signaling success for each of the above?
    -   What is the expected and the mean number of steps to acquire a signaling system for a given number of states and actions under different settings?
-   How does having more senders or receivers affect the above?
    -   What is the complexity of n-agents to come up with a common signaling system?
        -   under full communication
        -   under partial communication
-   How does locality affect the time to a universal signaling systems?
    -   if there is full observability
    -   if communications are one to one
    -   if communication are different neighborhood, Von Neuman, Moore, hexagonal, other lattices, chains, rings, random graphs. (need to use optimal dynamics)

Another question that like a lemma on time needed for an agent to become experienced enough to setup an optimal signaling system?

-   Given distribution S of states with k states and some the rarest state $s'$ having probability $p(s') = \alpha$ what is the expected number of observations needed for agents to approximate the distribution of states to within some credible interval $\epsilon<\alpha$?

-   Note while there is no lower bound on alpha the upper bound is $\alpha = 1/k$ for a uniform distribution of states. I think this is the Bayesian version of an empirical distribution. This would be a waiting time for becoming experienced.

-   After this waiting time a steady state distribution should be known to all agents.

Under partial observability the agents need to cooperate to learn the signaling system in a distributed manner. If the agents are on a grid or on a graph what are the bounds on coordination time for learning the signaling system - using a gossip protocol - i.e. each agent can only communicate with its neighbors - using a broadcast protocol - i.e. each agent can communicate with all other agents - using a random walk protocol - i.e. each agent can communicate with a random agent - using a central coordinator - i.e. each agent can communicate with a central coordinator - using an ambassador - i.e. each agent can communicate with an ambassador who can communicate with many other agents per Ramzey's theory

While reviewing a paper of this subject I had realized that there are a number of hypothetical scenarios for signaling systems to arise.

In RL we have different setting for learning optimal strategies. Some of theres different scenarios can be framed in this form.

I wanted to list them here so I can reference them later

But thinking as I list these I notice that some provide an easy solutions to problems that others don't.

One point of interest. If the agents are concerned with picking the right action for each state, they should collapse any states which share the same optimal action into a single signal. This will reduce the number of signals they must be learned and reduce the overall message length and cost of signaling. So in reality we should not be overly concerned with the number of actions exceeding the number of states.

When there are not enough signals agent need to learn to aggregate signals.


add 

1. learning by evolution:
    - replicator dynamics with
    - agents have random signaling systems assigned and the systems with most payoffs is selected through population dynamics.
    - children learn thier parent matrix via sampling.
        - one parent (perfect and imperfect transmission)
        - two parents 
    - pidgins via shared dictionaries
    - creoles shared grammars and dictionaries
    - adding some mutation - adding mutations to the childrerns signaling system.
    - based on paper by (Nowak and Krakauer)
2. learning via reinforcement learning     
1. spontaneous symmetry breaking scenarios vs planning
   1. If there are N signals, states and actions is there an advantage to planning a signaling system vs letting it evolve in terms of the number of steps needed to learn the signaling system? 
    - random signaling means that each step is an independent trial. 
     - Sender can send N signals and
     - Receiver can guess N Actions 
     - So there are N^2 combinations per turn.
     - So there are Only the ones with A=T get a reward so there are N good combinations. So there is a N/N^2 = 1/N chance of getting a reward. So we can expect that the number of steps needed to learn to signal the state T is N.
    - planning means that the sender picks one signal and sticks to it. In this case Receiver gets to systematicaly eliminate an action every time.
    - sender has 1 signal and
    - receiver can guess N at first and N-1 at second and N-k-1 at kth turn.
    - So there are n+1/2  
     actions giving 1*N combinations and only ones with A=T get the payoff. So there is a 1/N chance of getting a reward. So we can expect that the number of steps needed to learn to signal the state T is N.
    - Thus planning is faster than random signaling.
    
    - random signaling means that there are (2n/n*n)^n = 2

    - is agent use positive reinforcement only then 

   2. are there conditions where the signaler/reciever gets to detrmines the signaling system?
     - if Sender sends random signals from L-{coordinated} R must guess the state From L-{coordinated}.
     - if S wants to switch X and Y ? and does so R get 0 . If R is epsilon greedy he will find the new semantics.
     - A meta protocol would require a code switching signal be "Swap X Y"

1. Source coding scenario errors in encoding & decoding -  based on paper by (Nowak and Krakauer)
2. errors in the transmission channel  based on paper by (Nowak and Krakauer)

3. risks - there are signals with monotonically increasing risk.
    - payoffs for signals are symmetric
    - cost associated with the risky signals are borne by the sender 
    - if receivers can respond correctly after getting a partial message they get a bonus.
    - we can also consider sharing cost and rewards symmetrically.
--- creating a complex system with compositionality using self play


### The Bayesian view of the signaling systems

In this section let's consider a view of the Lewis signaling game in terms of a bayesian game theory. 
This is a perspective I used in a article on planning in the complex signaling game and helped me think more precisely about the 
how a signaling system might evolve in a formal setting.

The game has n states and n signals. Internality the a the agents will learn a permutation of the states and signals and its inversion. 
So there are n! signaling systems that can be learned. In the world of bayesian agents each such permutation characterizes an agent type.
THe game starts by nature picking a type for the agent. I say this this is because the agent needs to define a strategy which is a response for each state! Now the same is true for the receiver. The receiver's strategy is to pick an action for each signal. After that it can use bayesian updating to update it's belief about the type of the sender. These probabilities can guide it in the process of learning the signaling system. As the pair make progress, the receiver is able to update it's belief about the sender's type, discarding options that are inconsistent with the signals it has received! Once it has finds n-1 signals it can be certain about the sender's type and it will have an expected payoff of 1.

## Framing games and evolution of domain specific languages

In this scenario we considers if learning a shared language could be a game changer in some strategic interaction like a social dilemma. We may ask be able to not only interpret different signaling system as embodying different semantics derived from the framing game but also consider if these create linguistic relativism where that agent's languages shapes thier perception of the framing game by allowing them to develop newer strategies. We can also consider the beginning of ethics in such a system by considering if the introduction of language allows increases or decreases the overall welfare of the agents.

::: {.callout-note}

### Story: Framing Games


Agents tasked with maximizing a reward signal under conditions of strategic interaction. We call this the framing game and it might be as simple as a 2x2 matrix game like the battle of the sexes, a social language like the iterated prisoner's dilemma or as complex as a Sugarscape simulation . At some point, perhaps at the start send and received actions are introduced into the agents action space. They can now assume the role of the sender and receiver in a Lewis Signaling game.

If the incentives are right e.g. the framing game is cooperative they could learn to signal to each other. Note though that it is conceivable that agents could learn to signal if they are in a competitive game if they are sufficiently driven to explore the send and receive actions. However the resulting equilibrium might not be a perfectly separating one if the agents are not suitably incentivize to use the language to coordinate.

Furthermore these signals may then be incorporated into the planning and allow the agents to coordinate on the framing game. 

One expects that the language that arises under such circumstance would be limited to the domain of the framing game and that its semantics would be inherited from the framing game. However larger framing games with many generation of agents might lead to dynamics that lead to the emergence of a more general language.
:::

This kind of scenario actually contains a rich set of paths to the emergence of many different languages. For agents in the lifelong settings the emergent language might gain additional strata of semantics from multiple domain and then evolve to a more general language. 

A questions then arises what simple framing games can lead to agents to develop languages that are imbued with sufficiently rich semantics that the language has the Sapir-Worf property of being able to express semantics from any other language.


Another idea I have been using liberally in my thinking is that of a framing game for the lewis signaling game. This idea comes from the field of Multi agent  Reinforcement learning however it should also be valid in terms of Game theory.

Simply put an agent may be tasked some general problem like playing chess or solving a maze. In the past I worked on wizards that configure servers or home networks for telecoms. 

I could envision an RL agent learning to do these job by learning from experience. However it seems that if it can play a lewis game and learn a signaling system that is a subset of  english that approximate its domain then it can chat with people rather then relay on a some user interface. 

For a home networks it might need a smaller subset of english and for a server configuration it might larger one. What seems to be the point is that the agent tasked with some external task 
might be able to learn a signaling system that has semantics inherited from the task. If such task is a strategic interaction we may view it as a game. And together we can view the framing game and the lewis signaling game as single iterated game in which the agent learns to play a new variant of the framing game in which it has access to a coordination mechanism that is a domain specific language.

I think that we if we naively combine a game like the battle of the sexes a pure coordination problem with the lewis signaling game the agents will learn a language like 'football', 'opera'. And that these can arise within three iterations and allow the agents to then coordinate on the battle of the sexes so as to score the highest payoffs. This could happen both if the agents alternate signaling or one always gets to signal first, and always picks opera. 

On the other hand with iterated prisoners dilemma signaling might not make a difference as the language may not be able to change the payoffs sufficiently to make the agents act any differently. In this case it is entirely possible that a signaling system will not arise at all regardless of what the agents say they will act in their own best interest. This leads to a completely pooling equilibrium.

So the question that comes to mind is this -
1. can we setup up the signaling game so that the agents will always learn a signaling system if coordination is a benefit.
2. How can we encode the signaling system so that its prelinguistic object will be the states of the world and the actions of the agents. I.e. we want them to be able to talk about the outcomes of the framing game in the signaling game.
3. We want cost and benefits of signaling to be decoupled from the framing game - i.e. we may deduct the payoff for signaling success once the signaling system is learned.
4. We do want the agents in the framing game to aware of the outcome of the signaling game.
5. Finally we want to identify if there are strategies in which coordination increase of decrease overall welfare.

e.g. in the battle of the sexes we should expect perfect rewards
e.g. in a three way traffic junction game we might expect the agents to signal thier intentions to turn left or right and to go straight. This would allow them to avoid accidents. One such mechanism might be a game of paper rock scissors to determine the priority of the agents.
e.g. in the [Braess's Paradox](https://en.wikipedia.org/wiki/Braess%27s_paradox) establishment a high way though a city we might end up increasing the traffic jams. 


## Co-adaptaion and Semantic Drift and grounding.

c.f. [@rita2022emergent] paper of co-adaptation loss!
c.f. [@meta2022human] solving the game of diplomacy
c.f. [@barrett2017self] on templates by skryms


::: {.callout-note}

### Story: Co-adaption

Family members, best friends and members of closely knit societies tend to develop a language that is unique to them. IT can start with in jokes, invented words and phrases and co-opting the meaning of existing words to mean something else. This is a form of co-adaption where the language and the society co-evolve. If allowed to evolve the language can drift so that a stranger would be at odds to understand what the speakers are saying and this is called semantic drift. 

For RL agents it is possible for them to develop a language that is unique to them as suggested above. It is also possible that as conditions change e.g. the framing game is switched from Battle of the sexes to Prisoner's dilemma the languages will remain a 4 state 4 signal language but the meaning of the signals will drift.

:::


this story is more about something we might want to avoid.

1. having more agents should reduce co-adaption.
2. Semantic drift is inherent in the evolution of language. However we may want to allow the language to evolve but for certain aspects to remain fixed. This is one of the desiderata for emergent languages. What we would prefer that grammar and much of to be stable over multiple generation so that great grandfathers can still communicate with thier great grand children. Why is this a problem? In agentic systems we design language emergence to be fast. In most cases every agents need to learn it from scratch, they enjoy the benefits of perfect recall and a noiseless channel. This also means that languages might change very quickly to and that we as researchers will have a tough time understanding the agents over the course of thier simulation. However in natural languages we have a similar situation and for language to work for large populations and for record to make sense for thousands of years we want much of the language to be stable with possiblies for evolution at the fringes....
3. How do we ensure semantics persist over time ? We call this is the idea of grounding. Imagine all the most important ideas were written down in a book and that book was passed down from generation to generation. Everyone might need to learn the book a little after they learned basic language skill in school. soon the book becomes cannon and no one may change it. Over time though it might be permitted to add bits when new concepts were discovered and proved important enough to preserve.
4. Short of starting a religion for our agents we may want good mechanism that will keep the language grounded so that cooadaption and semantic drift are kept in check.

5. Another point here is that if our agents are aware that the framing game has been swapped from battle of the sexes to Prisoners dilemma that they may want to keep their semantics for the Battle of the sexes intact and use them as a template or prior for the Prisoners dilemma. Since prisoners dilemma is non-cooperative, there may not even be a perfectly separating equilibrium for framing game so that assigning a language from a template prior might actually be of benefit. .
