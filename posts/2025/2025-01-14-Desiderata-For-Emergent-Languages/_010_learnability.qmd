
### 1. Learnability - Easy to learn

Despite its importance, learnability may be overlooked in research on emergent languages. While some settings may not prioritize ease of learning, in lifelong learning environments or multi-generational agent interactions, learnability becomes a crucial factor. Many other desiderata either enhance or hinder learnability

> My experience with the Lewis signaling game is that it is easy to learn and that Natural Languages are not.
> The difficulty seems to be in finding the right structure for the state space so that the signaling systems generalize well, allowing learners to pick up the language from a few examples and start to communicate effectively with a limited proficiency.
> At a deeper level if the language arises though a mechanism of spontaneous symmetry breaking, i.e. random choices then will have long term impact on the emergent languages that will increasingly be harder to undo.

### Insights from Natural Languages

Children typically begin learning to talk in stages:

| Age        | Approximate Vocabulary Size | Language Milestones                       |
|------------|-----------------------------|-------------------------------------------|
| 0-6 months | rudimentary signaling       | Crying, cooing, babbling                  |
| 12 months  | ~1-5 words                  | Babbling becomes more complex, First words emerge (e.g., "mama," "dada")        |
| 18 months  | ~50 words                   | Vocabulary expands, simple word combinations begin      |
| 24 months  | ~200-300 words              | Two-word phrases, expanding vocabulary    |
| 30 months  | ~400-600 words              | Three-word sentences, basic grammar structures appear, more verbs/adjectives   |
| 36 months  | ~1,000 words                | Complex sentences, basic grammar          |
| 4 years    | ~1,500-2,000 words          | Storytelling, past/future tense           |
| 5 years    | ~2,500-3,000 words          | Longer conversations, complex grammar     |
| 6 years    | ~5,000+ words               | Adult-like speech patterns emerging       |


Language development varies by individual, but most children can hold simple conversations by age 3 and have a well-developed vocabulary by age 5.

|**Natural Languages**|**Emergent Languages**|  
|---|---|
|proceeds in stages|has one stage|
| takes about 5 years| takes o(n^2/2) steps|
| in 5 years 5000 words | 2,500,000 time steps^[4.7 years at 1 step per minute] |
|Notoriously difficult to learn| Easy to learn$O(N^2/2)$ ^[perhaps not so easy as N increases to millions and there are a a small chance of errors per symbol]|
| learning in stages | learning in one stage |
| learning is a lifelong process | learning is a one time process |
| learning is a social process | learning is one on one |

| Natural languages are notoriously difficult to learn.| On the other hand emergent languages can be learned in the worst case as quickly as in $O(N^2/2)$, that is if one neutralizes the stochasticity of the process by requiring that nature prioritize unlearned states before all the learned ones at any given time.|


It doesn't seem to be much easier to learn a second language once you have already learned the first. However research treats first language acquisition as a different process to second, etc. language acquisition.

In terms of the Lewis signaling game the first language acquisition maps the signals to states, a second language maps the signals to the first language. However in the game the players also engage in the  inventing of the language for the first time simultaneously with learning about the state of the world. 

The exception seems to be that children master one by the time they are 5 years old. Over time they will improve their proficiency and may learn additional languages.
Students of a language may require many examples to learn. Having a dictionary is of limited help. ^[for lewis signaling games where agents learn a lexicon this is is all an agent needs to learn the signaling system.]
There isn't an objective metric for tracking how difficult it is to learn a particular language.
There is plenty of anecdotal evidence that some languages are easier to learn, as well as some languages are harder for native speakers of a second language.
There are many challenges along the way.

Part of [Nature v.s. Nurture debate](https://en.wikipedia.org/wiki/Nature_versus_nurture) is to what degree is  the language instinct hard coded into us. c.f. Pinker's books the blank slate and the language instinct. It 


I often get hung up on why are natural languages (AKA man's greatest invention) are such a challenge to learn and what this might mean for my investigation into emergence of language.

Hypothesis: Complex signaling that fulfill enough desiderata may suffer from reduced learnability. I think that the core desiderata might actually allow for languages with graded burden of learnability.

Questions: Howe can we evaluate the learnability of a signaling system? 
What are the metrics that we can use to evaluate the learnability of a signaling system?

#### Metrics for Learnability

::: {.callout-info collapse="true"}
##### TODO:

I'm uncertain if other have studied learnability in the context of emergent languages. So there is an open challenge in defining good metrics that can guide progress in this area.

1. [ ] research this in the litrature.

:::

In RL we do however have metrics that are associated with learning.
These are:
1. The number of examples needed to learn a task.
2. The generalization of the learned task.
3. The stability of the learned task.


1. Evidence that agents have learned the signaling systems is that they can communicate effectively. This is measures in terms of expected success rate. During learning this may take a long time to converge to 1.0. 
2. In the tabular setting $n^2/s$ seems to be the worst case for agents engaged purely with learning a signaling system.
3. The best case is O(n) - i.e. if the receiver could see the state it would still need to see 
The faster the agent reach 1.0 the better they 

1. The most obvious metric is the success rate for the agents in carrying out communications. However this by itself is not enough. In the original game the best case path to learning generally requires at least n^2/2 tries and the algorithms used do not usually need to  generalize. In the complex signaling systems we might have infinite or prohibitively large state spaces and even for finite one potentially unbounded number of signals.
2. A second idea is how many examples are needed to be seen before agent get a good grasp of the language.

- In [@goldsmith-2001-unsupervised] the author considered the induction of morphology using Minimal description length MDL - the number of bits needed to describe the signaling system is what agents need to coordinate between them to learn a shared communication system. In Goldsmith's work he considered a corpus and then compared it to to the a corpus compressed + the binary of an encoder built from templates and a frequency lexicon. Goldsmith showed that by learning a morphology it was possible to compressed corpus + the binary of the encoder into less then the original corpur. his is a good metric for the learnability of a signaling system. In the case of emergent languages
- I should make a review of this great paper but the gist is that there is a lexicon and a morphology based on a set of templates that are used to generate the words. ^[As it overgenerates, it might be necessary to store a bit-list of which lexicon items can be used with which templates. though I don't actually recall this being the case as the might be an infinite number.]

- We like to consider two cases: ^[I have already written an article on some different ways that signaling systems can be arise.]

1. there is a sender/teacher with a good signaling system and a receiver/student learning it.
2. there is no sender/teacher and the agents have to construct such a signaling system from scratch.

the above notion if MDL is a good metric for the first case but not the second. In the second case we need to consider the complexity of the state space as well as the algorithmic complexity arriving at a common communication system. The cost of coordination of an MDL is subsumed by the cost due to complexity of constructing optimal signaling system that faithfully represent the structure of the state space.

Another two points:

- Learning a partial system should give agents better benefits than not.
- Learning as a group should be easier and quicker than learning individually.
- e.g. Learning of rules (grammar/morphology) should amplify the learning and generalization of the speaker wrt the structure of the state space.

RL based metrics for learnability are:

1. cumulative reward $G_T$ if normalized becomes the average reward per time step or expected success rate.
1. Sample efficiency
1. convergence rate - how long till policy stabilizes or stops improving
1. regret - Measures how much reward is lost due to suboptimal actions compared to an optimal policy. Defined as $R_t = G^*-G_t$ where $G^*$  is the optimal cumulative reward
1. entropy of the policy- Measures randomness in action selection
1. policy stability 
1. success rate/ task completion rate
1. generalization
1. exploration vs exploitation ballance


For hierarchical RL additional metrics may include subtask efficiency, hierarchical consistency, and intrinsic reward utilization to assess the learning of macro-actions and task decomposition.

### References 

In the beautiful review paper [@Nowak2002ComputationalAE], the authors discuss Learnability. This shows that this at least in the evolutionary context this has been considered

#### Gold's Theorem

One of the main points in the paper is Gold's Theorem, c.f.[@gold1967language] concerning the impossibility of learning an unrestricted set of languages. The authors also discuss the necessity of innate expectations in language acquisition, arguing that the human brain's learning algorithm can learn existing human languages but not all computable languages.

> Gold’s theorem formally states there exists no algorithm that can learn a set of ‘super-finite’ languages. Such a set includes all finite languages and at least one infinite language. Intuitively, if the learner infers that the target language is an infinite language, whereas the actual target language is a finite language that is contained in the infinite language, then the learner will not encounter any contradicting evidence, and will never converge onto the correct language. This result holds in greatest possible generality: ‘algorithm’ here includes any function from text to language.

#### The paradox of language acquisition.

The authors describe Chomsky's concept of "poverty of the stimulus" and the proposed solution of "universal grammar" (UG) as a restricted set of candidate grammars. They explain the controversy surrounding the notion of an innate UG and highlight the role of learning theory in demonstrating its logical necessity. 

If memory serves me correctly, in [@bod2003probabilistic], the authors argue that the `poverty of the stimulus` is not a problem for language acquisition. They show that if Grammars are defined using probabilistic rules then the poverty of the stimulus is not a problem. This is because the child can learn the language by observing the world and the language spoken by the adults. T think that they make a case against the necessity for an innate UG.

I have not thought everything through but I believe that Gold Theorem may not be a show stopper for learning grammars in MARL.

- One reason is that saliency and other factors may restrict the sets of all possible languages that may be learned to just one.
- A second is that spontaneous symmetry breaking can also reduce the number of possible languages to just one.
- I cannot however say this is a general refutation of Gold's Theorem, I think that it depends of the choice of State space and it's implicit structure and how that might be captured in terms of a signaling system.
- If 
- Another is that
One reasons is my view that bayesian agents can learn to update thier beliefs about the relevant equilibria.


This is another point they make that I wanted to bear upon. In the book probablistic linguistic the authors
if agents can learn an approximation of a language then they can learn the language. This is a point that I have made in my own work.

I drew the following sources for this section: 
[@bloom2013one], 
[@fenson2000variability],
 [@hoff2009language],
 [@hart1995meaningful]
