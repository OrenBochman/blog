---
date: "2025-01-14"
title: Emergent Languages - A Desiderata
categories: [emergent languages, reinforcement learning, information theory, linguistics, complex signaling system]
keywords: [compositionality, transfer learning]
bibliography: ./bibliography.bib
image: /images/cover.png
---

> [coordinate decision making], this is a difficult problem, because:<br>
> Firstly, most coordination problems cannot be solved in polynomial time.<br>
> Secondly, it's difficult because as we decentralized our systems we need to know who, what, and when we want to communicate, right? <br>
> And thirdly, it's difficult because it's not at all clear what strategy we want to follow when things don't actually work as expected <br> 
> -- [@RoboticsToday_2021] Amanda Prorok

This working paper aims to **collect and develop** desiderata for emergent languages. Inspired by the reductionist approach taken by [@Skyrms2010signals], my investigations of emergent languages began with minimal extensions to the Lewis Signaling Game.

In a  a companion working paper, "[The Many Path To A Signaling System](../2025-01-05-Many-paths-to-signaling/index.qmd)" I investigate different setting under which emergent languages can arise. These two papers help cement my intuitions on how one may develop agents with the ability to learn emergent languages with such desirable properties. I am putting this draft out to aid other researchers in the field of emergent languages as well as to solicit feedback on the desiderata I have compiled.

<!--
The Lewis game  is not understood very well when we extend it to complex signals.

Thus my desiderata for emergent languages remains minimal and shaped in part by my intuition of the simple and complex variants. 
-->

::: {.callout-tip collapse="true" }
## Simulacra projects for Low resource Languages {.unnumbered}

Since writing the first draft I also took an online course on Multi-Lingual NLP.

In the course the instructors suggest that if there is no data for the language we can use a related language to bootstrap the process and if that is not available they suggest using Turkish. One issue raised is that having one such language can lead to overfitting to that language. Some NLP models can be fit on multiple languages allowing them to fit to the structure of the languages rather than one specific language. 

One of my main takeaways from that course is that we may be able to evolve a variety of emergent languages that are approximations of Low Resource Languages.
This may be possible by imbuing them with properties drawn from sources like [WALS](https://wals.info/) the World Atlas of Language Structures, c.f [@wals]. Such emergent languages may then be useful as a priors for building model for transfer learning between high resource languages and the low resource ones. They may be used with prototype of NLP models to assist collection of actual data most efficiently from speakers of these languages.
:::


::: {.callout-tip }
## TL;DR: A desiderata For Emergent Languages {.unnumbered}

![Emergent Languages in a Nutshell](/images/in_the_nut_shell_coach_retouched.jpg)

This is a list of properties that I and others have suggested for emergent languages. 

:::

[@Skyrms2010signals] and [@NIPS2016_c7635bfd] have shown that we can also add a lot of structure to the signaling system.

In his book he considers, learning to reason, learning to form efficient communication networks, learning to use better communication protocols. Others have considers inducting communication protocols from a meta protocol. [@NIPS2016_c7635bfd] And this has been coupled with the idea of learning to represent the structure of the state space.

I think that **the coordination problem is the easy part** and I have devoted some time to solve it in a number of settings. I think that the next challenge is to learn a reductionist concept for signal aggregation. Something that allows bags of words, ordered sequences and recursive structures to be learned in much the same way. I think this we have not been asking the right questions to find it yet since aggregation is a hard problem. However I think that at this point a starting point exists and the next problem is to learn the structure of the state space. I think that here we can apply some existing algorithms that may be a good fit for Lewis signaling games but it is quite important because the structure of the state space is what will determine the optimality of the language to a particular domain. A second facet is that we should think of a minimal state space that captures the essence of the real world. This should be a model that we can consider sufficiently powerful for real world agents. But in terms of the emergent signaling system I don't think it will be qualitative or quantitatively all that different from other systems other than this it could act as a good [interlingua]() for transferring between natural languages as well as between tasks in RL. This is therefore the holy grail of emergent languages at this point.

<!-- move these in to NLP section for each desiderata 
::: {.callout-warning}

## Natural Languages 

Most of the desiderata for emergent languages are sourced from natural languages. However when we look at natural languages we see that the desiderata are not a feature of such languages but rather an idealization. More alarming is that these are not provided with a base line metric from different languages as a baseline. 

A second point is that natural language is not optimal in any of these desiderata and this is a point well worth remembering. 


- Natural languages are not easy to learn. 
- Natural languages combine regularity with much irregularity and this happens at all levels of the language from phonology, orthography through morphology and also syntax.
- Students, particularly children, are prone to misgeneralize and need to be corrected with the right forms. 
Instead of learning one base form of a word in most languages you have to learn a number of additional forms.
- Natural languages contain numerous homophones (different words tha sound the same) and homonyms (single words with multiple senses).
- Written languages often require punctuation to make the semantics precise. (The spoken version may often be subject to misinterpretation.)
- Natural languages contain lots of redundancy that is not particularly useful for better communication and makes them hard to learn.
- Natural languages are rife with ambiguity and though one can make a case that we can disambiguate them from context. This is true when we want to communicate. It is not true when people want to dissemble - listen to any politician or lawyer on the spot and you will discover that they are using a lot of words but to say very little. this is not the case, it is just the way we parse them. Given a number of parse tree of an ambiguous sentence we are told we can usually pick the right one. However there are more sentences in the language that have many many valid parse trees then just one or two. 

:::


::: {.callout-warning}

## Metrics

1. Ideally item in the desiderata should come with some metric that can be used to formalize it and to rank different signaling systems.
1. Also it would be nice if there were examples.

:::

-->

## The Desiderata

So the desiderata for emergent languages are 

Important:

{{< include _010_learnability.qmd >}} 

{{< include _020_communication.qmd >}}

{{< include _030_resilience.qmd >}}

{{< include _040_salience.qmd >}}


<!--

{{ < include _050_generalization.qmd >}}

{{ < include _060_hierarchical.qmd >}}

{{ < include _070_faithful.qmd >}}

{{ < include _080_distributional.qmd >}}

{{ < include _090_compositionality.qmd >}}

{{ < include _100_disentanglement.qmd >}}

{{ < include _110_entanglement.qmd >}}

{{ < include _120_stability.qmd >}}

{{ < include _130_zipfian.qmd >}}
-->


### 5. Salient wrt. the distribution of states

Salience means that not all equilibria are the same. 

e.g. a lognormal or exponential distribution or any distribution other than the uniform distribution of states will have a different optimal signaling system, particularly under adaptive prefix coding. This suggest that out of the numerous signaling systems that can arise from the Lewis signaling game some will be more efficient and this may well translate directly into greater fitness for the agents.

However we might as well be clear that for larger state spaces the signals further down the long tail play a less important role in the fitness of agents and therefore one a signaling system or even a partial pooling equilibria is found that captures sufficient quantiles of the state distribution, may confer similar fitness benefits. 

And so it seems that while saliency places an order on all possible signaling equilibria it may have to compete with less salient equilibria that are much easier to learn and use. H

However I think that learnability demands much more on the structure of the state space in terms of prelinguistic objects while saliency is more about the distribution of states. So that if we try to disregard most pathological cases and consider state spaces resembling reality we might expect that the most salient signaling system will also be very learnable.


e.g. is syllables have energy/clarity costs or then the otherwise symmetrical equilibria will now have an order and there is a notion of the most salient signaling system.

Salience is not the same but may also encompass the notions of 

- risk minimizing wrt risks associated with signaling - particularly in the case of risks affecting agent's fitness!
- minimize costs/overhead associated with signaling (in rl there should be a cost associated with each marginal bit that the they send across the channel) 
- Minimizing signal length - This may be the reason why the most common states are the shortest signals - using the unmarked case as the default. This is a form of source coding. (Perhaps this items is more fundamental then risk and salience) It may also be the reason why we have vowel harmony in some languages and why there are other types of redundant agreement in different languages.
- A theorem: if a (natural) language arising via evolution has a redundancy that may be removed without loss of information or via context then it will be compressed and eroded or eliminated given time. Thus such features are that exist and are stable are will have a measurable benefits in terms of communication.

### 6. The signaling system should be able to generalize to new states

I'm not sure if this should be included. Generelization implies two or three things
1. replacing tabular lexicons with linear fuction approximators
2. compositionality over multiple analogies.
3. Recursive grammar.
This is one needs to be promoted or at least combined with learnability. Generalization means beinng able to learn base forms and generelizing to derived forms autoamtical. It also means being able to compose signals. IT also being able to capture hierarchies and other structures in the state space.

1. Morphology 
    - inflections- 
        - verb - conjugations
            - Tense and Aspect and Mood systems
        - nouns - declensions
    - derivations to other parts of speech
    - paradigms for each (e.g. posseives in English)
2. Relations 
    - Sematic Roles
    - Coverbs and Complements extend the verbs to cover many related concepts
2. Syntax for Logical and set theory as a basis for
    - Conjunctions and Disjunctions, 
    - First order/Contrafactual logic Quantifiers 
    - Subordination and Coordination
3. Spatio-temporal events     
    - Adverbs and Prepositions
    - Adjectives and Adverbs

7. ### 7. Hierarchical structures

Hierarchical structures arise naturally out of rule based systems, particularly when the rules allow a symbol on the left to be replaced by the same symbol on the right. It can be more complicated if this happen via an intermediate symbol

$$
S \rightarrow aSb
$$ {#eq-simple recursive}

## an example or recursive rules with noun phrases.

$$
\begin{align*}
S &\rightarrow NP VP \\
NP &\rightarrow (Det) N |    \\
VP &\rightarrow V (NP) (PP)
\end{align*}    

$$ 
\begin{align*}
S &\rightarrow a|b \\
a &\rightarrow bb \\
b &\rightarrow aa
\end{align*}
$$

I think this is an integral part of generalization and of learnability.
I think that this is a grammar that can generate nested structures like nested clauses, nested phrases, or nested dependency trees. 

A Recursive grammar seems more compact than one that is not recursive. It is also more expressive. Recursive rules are more learnable in the sense that learning one rule leads to possibly infinite number of sentences.

Another property is that the recursive grammar should be well formed (syntax) and avoid generating ungrammatical sentences (semantics) This may be complicated and I'd rather avoid hashing the details at this time. If necessary this item be broken down.

This is nice to have  because a grammar with non recursive rules is less expressive. We might run into problems for expressive basic recursive structures like numbers. 


- every time an agents learns another part of the system it should have. My solution here leans on using group actions to structure the state space. Either one big one group action like for hebrew or a number of smaller ones like for english.


## Nice to have:


### 7. Hierarchical structures



### 8. Signaling systems should be able to faithfully encode Spacio-temporal and hierarchical structures in the state space.

In this section I consider the progression from a signaling system, to a domain specific language to a natural language.
Actually I like to think that the main structures that might lead to a natural language are:




#### 7.1 Domain Specific Language

A signaling system 
A Domain specific language

- arithmetic or at least a number systems 
- basic relations e.g. equality, and order relations.
- physics or at least spacio-temporal events
- semantic hierarchies aka thesauri
- first order logic + modal + contrafactual logic or at least propositional logic
- set theory
- basic notions from probability.

- its worth noting that set theory or logic might be all we need to get the rest, but we are not building mathematics but a language that needs to communicate.


### 8. Distributional Semantics

This central property does not arise in many signaling systems. 
However distibutional semantics is a property of natural languages. It is the good part of entanglement and disentanglement. It makes it easier to understand things by thier context^[a word is characterized by the company it keeps] & Distributed Representations

- Signaling systems should be alignable with 2000 discourse atoms c.f. [@arora2018linearalgebraicstructureword], or a subset if they come from a much simpler state structure.
- In fact a major point to research on emergent languages get to see if they manifest distributional semantics. I hypothesize that this will happen if the the state space is has a semantic basis - i.e. the state space is a vector space with dimensions that are semantically orthogonal. 
I now revised the above strong requirements to the following but perhaps they are even equivalent.
- prefix codes with compositionality
- prefix codes learned with loss onanalogies

### 9. compositionality - is state has structure the languages it should be preserved/mirrored by the language

significant increase in what it can express and understand. I lean towards adding a topology to captures semantic hierarchies. The different signaling system are associated with a lattice of topological groups with the complete pooling equilibrium at the top and the unstructured separating equilibrium. In between are partial pooling equilibria and the various structured separating equilibria. For compositionality we want to pick certain structured pooling equilibria over the structured separating ones.

### 10. Disentanglement 

- we like most morphmes to have unique semantics and not require context to disambiguate them. This places too much of a congnitive burden on speakers and requires learning by learners.


### 11. Entanglement

- the language should be able to encode multiple concepts in a single signal when binding moephems etc is clearly more efficent (we never use parts in isolation) 

- when language encode two or more semantics in a single signal. e.g. 'They' encodes (third)  person and plural (number) as one signal. This is a pronoun but it is not inflected and is not made of two bound morphemes, it is a single morpheme.

I want to come up with a information theoretic notions behind driving Entanglement and Disentanglement. 
1. I think they are based on the mutual information between the signals and the states and relative entropy. 
2. THe number of sub-states in the structure is high it best encoded as a group action i.e. a rule
3. If the sub-states are a few it is best encoded as a dictionary
4. If like a pronoun a complex signal is high frequency and high entropy there is a greater fitness to compress them into a single signal. And we might want to reduce errors by intentionally boosting the [phonemic contrast]^[explain!?]. 

In reality natural languages are not optimal in any of these desiderata. They are the result of a long evolutionary process that has been shaped by many factors. However I think that the desiderata are a good starting point for designing a language that is optimal for a particular task.


## 12. stability of regularity and irregularity (resilience to errors and to evolution)

consider that a language that generates entangled structures to compress and reduce mistakes for something like its pronouns these should be stable over and not replaced by a more regular system that is less efficient....
    i.e. the loss for having such a pronouns should be less then a the gain from having a more regular system.

Morpho-syntax should be stable over time and be composable with the abstract morphology  structure of the state space.

Languages change over time but not the core structure of the language. This is a form of stability.

## 13. Zipfian distributions

 
## An evolving desiderata of Emergent Languages

1. mappings between states and signals
    1. morphosyntax mappings preserve partial states (Homomorphism of normal subgroups) 
    2. mappings preserve semantic topologies (if a is close to b then f(a) should be close to f(b))    
1. Stability
    1. Regularity is stable (mappings used in syntax and morphology are stable over time)
    2. Irregularity is stable (mappings used in irregular verbs and nouns are also stable over time) In english we maintain many irregular borrowings from other languages and thus preserve thier regularity - making such exceptions easier to learn too.
1. Compositionality
1. Brevity (source coding)
1. Self correcting (channel coding to detect errors and correction them through redundancies like agreement, vowel harmony, etc.)
1. Learnability - how many things need to be coordinated; the complexity of the structures, the [Hoffding bound](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality) on rates of learning distribution when there are errors. The [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction) for  multiple learners.^[multiple learners has similar logic as a multiple hypothesis testing problem, for each learner postulating different signaling system with each failure or success in a Lewis game. More so when learners get to observe each other's actions and rewards.]

1. Stable irregularities 
1. Zipfian distributions - the most common signals should be the shortest and the least common the longest. This is a form of source coding and would arrise naturally from huffman coding, except that this isn't practical for several reasons. It could also arise out of laziness in the sender
1. Faithfulness 
1. Distributional stability
1. Decidebility - easy to disambiguate ambiguous signals from thier context
1. Expressivity - the ability to express a wide range of concepts
1. Generalization - learning the language is possible from just a few signal state pairs.

Some metrics

1. Compositionality
    1. Topographic similarity
    1. 
2. Source coding 
    1. Compression ratio
    1. Entropy
    1. Mutual information
3. Error detection and correction
    1. Error rate
    1. Hamming distance
4. Learnability
    1. Number of examples to learn a new word
    1. Number of examples to learn a new rule


Another random thought or two:


### VO and OV via symmetry breaking 


If we use Huffman coding like process to organize the order of the morphological and syntactical elements (effectively making the fixing the on average most surprising partial signals before the next on average most surprising ones) we should have emergent languages that are rather similar and fairly easy to learn 
Like Turkish and Japanese. However there is at the start the question of how to apply aggregations. If action is first we get V O languages if it is second we get OV languages. I think that V carries more entropy in Predation and Resource gathering games so that VO should be more prevalent. However once this decision is made most practical algorithms will not be able to reverse it.

### Vowel Harmony

If agents backprogogate with topographic similarity in mind and the 
basic signals (phonemes) are endowed with a similarity they may end up with
systems with vowel harmony and alternation of consonants to capture sets normal subgroups with greater similarity.

if these regular configuration also lead to better channel coding the benefits should persist. 


## Compositionality in Lewis signaling games

So here is a sketch idea for an algorithm for learning a compositional language in a lewis game.


We need a language designer. This is can be the sender, the receiver or implicit. Without loss of generality we can assume that the sender is the language designer.

THe language designer needs 
1. to a 'semantic' metric to decide when two state are close or distant.
2. a  way to deconstruct states into atomic orthogonal/independent parts. I am thinking of normal subgroups.

Note that we can define the metric on the parts and aggregate them to get the metric on the whole. This is a form of compositionality.

More abstractly we can just say that the state is an algebraic topological group. 

So the language designer can use a template with n part (on for each of the subgroups) Ideal ordered with by the decreasing size to prefix code the sub-states. If they there are duplicate sizes this will yield multiple equilibria to be selected via spontaneous symmetry breaking.

The designer now can allocate the states to one of the points in the topology. By picking the system with lowest overall distances we get a regular compositional language. 

Since there are many ways to do this the designer needs to coordinate with the receiver. 
However since there is greater regularity they only need to coordinate a minimal set with each atomic  substate apearing once.


## Is deep RL needed for language emergence? 

Regarding deep learning - I think that the best work in deep learning is done by researcher who have a clear view of the problem and use the DNN to approximate the function they cannot extend or scale beyond thier simple model. In the case of Emergent Languages people have built kits and adapted other architectures to that is overkill instead of focusing on the real problem at hand. 

So I think that most of the work in that space if flawed even if some the results are interesting. 

I don't know yet but I think that in may ways the complex signaling systems are not so different from simple ones. 

So far I also do not see the need for deep RL in this space. It seems to be mudding the waters rather then clarifying the problem. It clear thar RNNs and and newer seq2seq models are possible solutions but it seems that in most cases little effort is made to understand the problem mathematically and find the simplest solutions. I think that once we do this we should have both an elementary model of complex signaling. I currently think this may be a form of linear function approximation from RL. However I do not discount later use of deep learning models to learn non-linear variants. It is worth noting that by moving from a linear function approximation to a non-linear ones one expects to lose much and it is unclear that we gain anything at all since languages are discrete.

 and possible function approximation


## Language that can evolve under changing conditions without losing the core structure