---
date: "2025-01-14"
title: Emergent Languages - A Desiderata
categories: [emergent languages, reinforcement learning, information theory, linguistics, complex signaling system]
keywords: [compositionality, transfer learning]
bibliography: ./bibliography.bib
image: /images/cover.png
---

> [coordinate decision making], this is a difficult problem, because:<br>
> Firstly, most coordination problems cannot be solved in polynomial time.<br>
> Secondly, it's difficult because as we decentralized our systems we need to know who, what, and when we want to communicate, right? <br>
> And thirdly, it's difficult because it's not at all clear what strategy we want to follow when things don't actually work as expected <br> 
> -- [@RoboticsToday_2021] Amanda Prorok

This working paper aims to **collect and develop** desiderata for emergent languages. Inspired by the reductionist approach taken by [@Skyrms2010signals], my investigations of emergent languages began with minimal extensions to the Lewis Signaling Game.

In a  a companion working paper, "[The Many Path To A Signaling System](../2025-01-05-Many-paths-to-signaling/index.qmd)" I investigate different setting under which emergent languages can arise. These two papers help cement my intuitions on how one may develop agents with the ability to learn emergent languages with such desirable properties. I am putting this draft out to aid other researchers in the field of emergent languages as well as to solicit feedback on the desiderata I have compiled.

<!--
The Lewis game  is not understood very well when we extend it to complex signals.

Thus my desiderata for emergent languages remains minimal and shaped in part by my intuition of the simple and complex variants. 
-->

::: {.callout-tip collapse="true" }
## Simulacra projects for Low resource Languages {.unnumbered}

Since writing the first draft I also took an online course on Multi-Lingual NLP.

In the course the instructors suggest that if there is no data for the language we can use a related language to bootstrap the process and if that is not available they suggest using Turkish. One issue raised is that having one such language can lead to overfitting to that language. Some NLP models can be fit on multiple languages allowing them to fit to the structure of the languages rather than one specific language. 

One of my main takeaways from that course is that we may be able to evolve a variety of emergent languages that are approximations of Low Resource Languages.
This may be possible by imbuing them with properties drawn from sources like [WALS](https://wals.info/) the World Atlas of Language Structures, c.f [@wals]. Such emergent languages may then be useful as a priors for building model for transfer learning between high resource languages and the low resource ones. They may be used with prototype of NLP models to assist collection of actual data most efficiently from speakers of these languages.
:::


::: {.callout-tip }
## TL;DR: A desiderata For Emergent Languages {.unnumbered}

![Emergent Languages in a Nutshell](/images/in_the_nut_shell_coach_retouched.jpg)

This is a list of properties that I and others have suggested for emergent languages. 

:::

[@Skyrms2010signals] and [@NIPS2016_c7635bfd] have shown that we can also add a lot of structure to the signaling system.

In his book he considers, learning to reason, learning to form efficient communication networks, learning to use better communication protocols. Others have considers inducting communication protocols from a meta protocol. [@NIPS2016_c7635bfd] And this has been coupled with the idea of learning to represent the structure of the state space.

I think that **the coordination problem is the easy part** and I have devoted some time to solve it in a number of settings. I think that the next challenge is to learn a reductionist concept for signal aggregation. Something that allows bags of words, ordered sequences and recursive structures to be learned in much the same way. I think this we have not been asking the right questions to find it yet since aggregation is a hard problem. However I think that at this point a starting point exists and the next problem is to learn the structure of the state space. I think that here we can apply some existing algorithms that may be a good fit for Lewis signaling games but it is quite important because the structure of the state space is what will determine the optimality of the language to a particular domain. A second facet is that we should think of a minimal state space that captures the essence of the real world. This should be a model that we can consider sufficiently powerful for real world agents. But in terms of the emergent signaling system I don't think it will be qualitative or quantitatively all that different from other systems other than this it could act as a good [interlingua]() for transferring between natural languages as well as between tasks in RL. This is therefore the holy grail of emergent languages at this point.

<!-- move these in to NLP section for each desiderata 
::: {.callout-warning}

## Natural Languages 

Most of the desiderata for emergent languages are sourced from natural languages. However when we look at natural languages we see that the desiderata are not a feature of such languages but rather an idealization. More alarming is that these are not provided with a base line metric from different languages as a baseline. 

A second point is that natural language is not optimal in any of these desiderata and this is a point well worth remembering. 


- Natural languages are not easy to learn. 
- Natural languages combine regularity with much irregularity and this happens at all levels of the language from phonology, orthography through morphology and also syntax.
- Students, particularly children, are prone to misgeneralize and need to be corrected with the right forms. 
Instead of learning one base form of a word in most languages you have to learn a number of additional forms.
- Natural languages contain numerous homophones (different words tha sound the same) and homonyms (single words with multiple senses).
- Written languages often require punctuation to make the semantics precise. (The spoken version may often be subject to misinterpretation.)
- Natural languages contain lots of redundancy that is not particularly useful for better communication and makes them hard to learn.
- Natural languages are rife with ambiguity and though one can make a case that we can disambiguate them from context. This is true when we want to communicate. It is not true when people want to dissemble - listen to any politician or lawyer on the spot and you will discover that they are using a lot of words but to say very little. this is not the case, it is just the way we parse them. Given a number of parse tree of an ambiguous sentence we are told we can usually pick the right one. However there are more sentences in the language that have many many valid parse trees then just one or two. 

:::


::: {.callout-warning}

## Metrics

1. Ideally item in the desiderata should come with some metric that can be used to formalize it and to rank different signaling systems.
1. Also it would be nice if there were examples.

:::

-->

## The Desiderata

So the desiderata for emergent languages are 

Important:

{{< include _010_learnability.qmd >}} 

{{< include _020_communication.qmd >}}

{{< include _030_resilience.qmd >}}

{{< include _040_decodable.qmd >}}

{{< include _050_salience.qmd >}}

{{< include _060_generalization.qmd >}}

{{< include _070_hierarchical.qmd >}}

{{< include _080_distributional_semantics.qmd >}}

{{< include _090_dsl.qmd >}}

<!--

## Nice to have:

{{ < include _070_faithful.qmd >}}

{{ < include _090_compositionality.qmd >}}

{{ < include _100_disentanglement.qmd >}}

{{ < include _110_entanglement.qmd >}}

{{ < include _120_stability.qmd >}}

{{ < include _130_zipfian.qmd >}}
-->



## 9. compositionality - is state has structure the languages it should be preserved/mirrored by the language

significant increase in what it can express and understand. I lean towards adding a topology to captures semantic hierarchies. The different signaling system are associated with a lattice of topological groups with the complete pooling equilibrium at the top and the unstructured separating equilibrium. In between are partial pooling equilibria and the various structured separating equilibria. For compositionality we want to pick certain structured pooling equilibria over the structured separating ones.

### 10. Disentanglement 

- we like most morphmes to have unique semantics and not require context to disambiguate them. This places too much of a congnitive burden on speakers and requires learning by learners.


### 11. Entanglement

- the language should be able to encode multiple concepts in a single signal when binding moephems etc is clearly more efficent (we never use parts in isolation) 

- when language encode two or more semantics in a single signal. e.g. 'They' encodes (third)  person and plural (number) as one signal. This is a pronoun but it is not inflected and is not made of two bound morphemes, it is a single morpheme.

I want to come up with a information theoretic notions behind driving Entanglement and Disentanglement. 
1. I think they are based on the mutual information between the signals and the states and relative entropy. 
2. THe number of sub-states in the structure is high it best encoded as a group action i.e. a rule
3. If the sub-states are a few it is best encoded as a dictionary
4. If like a pronoun a complex signal is high frequency and high entropy there is a greater fitness to compress them into a single signal. And we might want to reduce errors by intentionally boosting the [phonemic contrast]^[explain!?]. 

In reality natural languages are not optimal in any of these desiderata. They are the result of a long evolutionary process that has been shaped by many factors. However I think that the desiderata are a good starting point for designing a language that is optimal for a particular task.


## 12. stability of regularity and irregularity (resilience to errors and to evolution)

consider that a language that generates entangled structures to compress and reduce mistakes for something like its pronouns these should be stable over and not replaced by a more regular system that is less efficient....
    i.e. the loss for having such a pronouns should be less then a the gain from having a more regular system.

Morpho-syntax should be stable over time and be composable with the abstract morphology  structure of the state space.

Languages change over time but not the core structure of the language. This is a form of stability.

## 13. Zipfian distributions

 
## An evolving desiderata of Emergent Languages

1. mappings between states and signals
    1. morpho-syntax mappings preserve partial states (Homomorphism of normal subgroups) 
    2. mappings preserve semantic topologies (if a is close to b then f(a) should be close to f(b))    
1. Stability
    1. Regularity is stable (mappings used in syntax and morphology are stable over time)
    2. Irregularity is stable (mappings used in irregular verbs and nouns are also stable over time) In english we maintain many irregular borrowings from other languages and thus preserve thier regularity - making such exceptions easier to learn too.
1. Compositionality
1. Brevity (source coding)
1. Self correcting (channel coding to detect errors and correction them through redundancies like agreement, vowel harmony, etc.)
1. Learnability - how many things need to be coordinated; the complexity of the structures, the [Hoffding bound](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality) on rates of learning distribution when there are errors. The [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction) for  multiple learners.^[multiple learners has similar logic as a multiple hypothesis testing problem, for each learner postulating different signaling system with each failure or success in a Lewis game. More so when learners get to observe each other's actions and rewards.]

1. Stable irregularities 
1. Zipfian distributions - the most common signals should be the shortest and the least common the longest. This is a form of source coding and would arrise naturally from huffman coding, except that this isn't practical for several reasons. It could also arise out of laziness in the sender
1. Faithfulness 
1. Distributional stability
1. Decidebility - easy to disambiguate ambiguous signals from thier context
1. Expressivity - the ability to express a wide range of concepts
1. Generalization - learning the language is possible from just a few signal state pairs.

Some metrics

1. Compositionality
    1. Topographic similarity
    1. 
2. Source coding 
    1. Compression ratio
    1. Entropy
    1. Mutual information
3. Error detection and correction
    1. Error rate
    1. Hamming distance
4. Learnability
    1. Number of examples to learn a new word
    1. Number of examples to learn a new rule


Another random thought or two:



<!--

## Compositionality in Lewis signaling games

So here is a sketch idea for an algorithm for learning a compositional language in a lewis game.


We need a language designer. This is can be the sender, the receiver or implicit. Without loss of generality we can assume that the sender is the language designer.

THe language designer needs 
1. to a 'semantic' metric to decide when two state are close or distant.
2. a  way to deconstruct states into atomic orthogonal/independent parts. I am thinking of normal subgroups.

Note that we can define the metric on the parts and aggregate them to get the metric on the whole. This is a form of compositionality.

More abstractly we can just say that the state is an algebraic topological group. 

So the language designer can use a template with n part (on for each of the subgroups) Ideal ordered with by the decreasing size to prefix code the sub-states. If they there are duplicate sizes this will yield multiple equilibria to be selected via spontaneous symmetry breaking.

The designer now can allocate the states to one of the points in the topology. By picking the system with lowest overall distances we get a regular compositional language. 

Since there are many ways to do this the designer needs to coordinate with the receiver. 
However since there is greater regularity they only need to coordinate a minimal set with each atomic  sub-astate appearing once.


## Is deep RL needed for language emergence? 

Regarding deep learning - I think that the best work in deep learning is done by researcher who have a clear view of the problem and use the DNN to approximate the function they cannot extend or scale beyond thier simple model. In the case of Emergent Languages people have built kits and adapted other architectures to that is overkill instead of focusing on the real problem at hand. 

So I think that most of the work in that space if flawed even if some the results are interesting. 

I don't know yet but I think that in may ways the complex signaling systems are not so different from simple ones. 

So far I also do not see the need for deep RL in this space. It seems to be mudding the waters rather then clarifying the problem. It clear thar RNNs and and newer seq2seq models are possible solutions but it seems that in most cases little effort is made to understand the problem mathematically and find the simplest solutions. I think that once we do this we should have both an elementary model of complex signaling. I currently think this may be a form of linear function approximation from RL. However I do not discount later use of deep learning models to learn non-linear variants. It is worth noting that by moving from a linear function approximation to a non-linear ones one expects to lose much and it is unclear that we gain anything at all since languages are discrete.

 and possible function approximation

-->

## Language that can evolve under changing conditions without losing the core structure


- this is being open to extension and to change.
- I have already demonstrated above how a language can use a simple template yet be still be open to extension using open categories of nouns and verbs by applying a stress pattern to the final syllable. 

- I am thinking about a stronger form of this property akin to what might be used by agents that undergoes metamorphosis and now perceives a new MDP with different states and actions. There may be a need to adapt the language and memories for the states of the world in terms of the old language. If the agent is now color blind it may benefit in the short run from a language that lacks colors. If however it regains it sight it would be better if these words were not lost. 

So we might want to preserve the old semantics using new terms. This suggest that we may want to have a general purpose language but only use a small easy to learn subset of it to get going.


{{< include _300_simulcra.qmd >}}