---
title: "Planning in the Complex Lewis Game"
date: "2025-01-14"
categories: [compositionality, emergent languages, reinforcement learning, transfer learning, information theory, linguistics]
keywords: [compositionality, emergent languages, reinforcement learning, transfer learning]
bibliography: ./bibliography.bib
#draft: true
---

Today I want to consider a couple of weak assumption about planning in the Complex Lewis game. It seems that before I do I ought to revisit the basic form of the Lewis Signaling game and the referential version of the game and the complex version of the game, and how these are formalized in Game Theory and in Reinforcement Learning. This is perhaps an onerous detour but it might be useful to help make progress that the weak assumptions I want to consider.

To avoid confusion lets state these assumptions up front. Lets code the function that maps the states to singals as $e$ for endoing and the function that maps the signals to states as $d$ for decoding.


- In the simple game the sender may pick from an combinatorial large set of lexicons if there are N states and N signals there are N! lexicons. The receiver in this case has to learn the full lexicon by trial and error.
- In the referential game the same is true about the lexicon, only now the receiver picks from a restricted set of states, though these are not know in advance if they are remebered learning can proceed much faster as each trial eliminates most lexicons.
- In the complex game the sender has N states by $M<N$ signals, but can send a sequence of signals. If we convert the signals to digits the sender is sending a base M number or a string of base M digits. While there are an infinite number of possible numbers the sender can send, only the first N of these are need to encode the N states... If the sender picks the first N numbers there are again N! different lexicons and we can use the same algorithm to solve the game.  
- If the channel is noisy the sender might want to use the first contiguous N numbers but pick the first N numbers that are each at a hamming distance of K from each other. This suggest that the receiver cannot assume the first bane M numbers are the signals but rather that there are N distinct signals. If the channel is known to be noisy then there may be many more corrupted signals that should be considered.
- If there is a inherent structure in the states, the sender might select an encoding $e(s)$ should preserve this structure. For example if S has a group structure the sender might use an encoding that is a group homomorphism. If there is are partial orders in the states the sender might use an encoding that if monotonic with respect to the partial order. 
    - We may call a minimal set of states as a base lexicon if knowledge of the base lexicon and the encoding function is sufficient to recover the rest of the lexicon automatically. In terms of linguistics we might call these the parts of the lexicon that is not in the base as the derived lexicon as it contains inflections, declensions, and derivations of the base forms.
    - In such a case though the sender might not want to pick the first N numbers but might want to pick a different set of numbers that are more suited to the structure of the states! The receiver who decodes some example might use analogies to infer additional states and eventually learn the decoder and the base lexicon. 
    - If the receiver is like a child it may see many analogies and learn the decoder. If the reciever is less bright it could still learn the decoder the hardway (i.e. by trial and error over the full lexicon)





In the Lewis signaling game, the sender and receiver have to coordinate their actions to maximize their reward. The sender has to choose a signal that will help the receiver identify the correct state. The receiver has to interpret the signal and choose the correct state. This requires planning and coordination between the two agents.

::: {.callout-caution}

## Bayesian view of the Lewis game

What do the agents know pre-ante?

1. do they know the full state space?
1. do they know the distribution of states?
1. do they know all possible signaling systems?
1. do they know the other possible equilibria?

The receiver in game theory needs a strategy and in that strategy is a response to all possible actions by the sender. If there are sufficient rounds the receiver has a strategy for solving the full coordination problem before the game starts.

If there are N states and signals, there are N! ways to map the states to signals we can consider that these are the different possible lexicons. While the sender might pick a signal at random for each new state, in terms of game theory she too must have a strategy, i.e. a choice of signal for each contingent state. And we can follow the convention of Bayesian games and that this choice is made before the game starts by nature picking a type for the sender from the N! possible types. We may also assume that nature has picked a vector of states to be presented to the sender and receiver but this is not necessary as once the strategies are pre-determined we can use bayesian game theory to work out the actions of the agents that will lead to the equilibria with the highest payoffs. Since this is a bayesian game the solution concept is a bayesian Nash equilibrium.

Each time the sender solves on of the senders signals she can eliminate all the incompatible types until only one type remains and the receiver has inferred the mapping that decodes the sender's signals for all but one state and thus knows the senders type.

In reality this changes very little in terms of the game, it's just a more precise way of thinking about the game in terms of what Game theory formalism requires.

:::

::: {.callout-caution}

## Reinforcement Learning view of the Lewis game
Note that when we think about the same game in reinforcement learning we may not require the agents to decide on a full strategy before the game starts. In RL settings they are in a PMDP a partially observed Markov Decision Process.

:::


We could consider these as different lexicons. In a bayesian settings we might go further and consider that there are N! senders each with a different lexicon and that the receiver must infer type of sender rather than the the correct signal for the current state. 

If the sender picks a lexicon for n

:::


The state space can be simple or structured and anything in between. If the state space is simple the sender will use the original Lewis game to create a lexicon. If the state space is structured the sender and receiver may do better - particularly if the number of simple signals is smaller than the number of states and sequences of signals must be used.



If the state space is a list of binary fields then the sender can use a fixed template of length n and two signals encodeing on or off.
If the 
Planning in the Lewis game is a complex problem 

It seems a daunting task to plan the whole language when presented with say the first state or perhaps a million states.
We can assume from the game theoretic nature of the Lewis Game that the sender and receivers know the full states space even if not the distribution of states. I say this because how could the receiver know from what options to respond to the sender if it did not know the full state space. 

In the referential game the receiver only seed a few states and must infer the right one from the signal. In this case we may relax the previous assumption and say that the sender and receiver may have a partial view of the state space.

## 

More so what if the sender must start without even being exposed to the full state space or it 

1. morphology

## Analogies

