---
date: 2025-04-02
title: "Base line Morphology Model" 
subtitle: "A baseline morphology and syntax for emergent languages"
description: "Some thoughts and code for a baseline morphology and syntax for emergent languages"
categories: ['code']
---

So in this note I'd like to create a vanilla implementation of a morphology and syntax that might be used as a inductive bias for the emergent language.

Morphological states are based on primitives. More generally we also need a syntax to combine many morphological states into a larger state.

grounding means at the bare minimum mapping state-features to morphological-features. If we have complex structure we would need trees of morphological states. 
a feature 

## Morphology

Let's:

1. [x] code a regular morphology for the emergent language.
1. [ ] look at using it as a part of the inductive bias for the emergent language.
1. [x] add inputs:
    - part of speech
        - open
        - closed
    - nouns
    - cases
    - tenses
    - aspects
    - moods  
1. [x] export lexicon to CSV
1. [ ] make features like a data-frame or at least a dict.
1. [ ] export for use with [egg] ?
1. [ ] an algorithm for an agent to associate the morphology with the states (for senders perhaps) what is the minimal steps to identify the morphology to the states. (a form of grounding)
1. [ ] add support for closed categories like pronouns and articles
1. create an option for replacing phonology with a base-n numbers. 
1. add a mapping back and forth from IPA to phonemes to Base-n numbers.
1. test on Prisoners Dilemma
1. create a linear function approximator from the features for morphological states


### A baseline generative model for morphology

```{python}
import random
import csv


random.seed(45)
# start with a simple morphology and then add more complexity.

## TODO: check we don't over flow the phoneme space

class base_morphology:

    def __init__(self,
                    vowels=None, 
                    consonants=None,
                    parts_of_speech_closed=None,
                    parts_of_speech_open=None,
                    declensions=None, 
                    nouns=None,
                    ):

        # define the phonemes
        if not vowels:
            self.vowels = ['a','e', 'i','o', 'u', 'aa','ee', 'ii','oo', 'uu','ai','au','ei','ou','ia','ua']
        else:
            self.vowels = vowels
        if not consonants:
            self.consonants = ['b', 'c','cs','ch', 'd','dh', 'dzh','f', 'g','gh', 'h','hw','ny', 'j', 'k','kw' 'l','ld','lh', 'm','mb', 'n','nc', 'nd','ng','ngw','nqu','nqt','nt', 'p', 'q', 'r','rd','rh', 's', 'sh', 't','tsh' 'v', 'w', 'x', 'y', 'z']
        else:
            self.consonants = consonants

        # define the parts of speech
        if not parts_of_speech_closed:
            self.parts_of_speech_closed = ['pronoun','article','preposition','conjunction','numeral']
        else:
            self.parts_of_speech_closed = parts_of_speech_closed
        
        if not parts_of_speech_open:
            self.parts_of_speech_open = ['noun','verb','adjective','adverb']
        else:
            self.parts_of_speech_open = parts_of_speech_open

        if not declensions:
            self.declensions = [
                'nominative',  # subject
                'accusative',  # direct object
                'dative',      # indirect object
                'instrumental',# with, by means of
                'causal',      # for, for the purpose of
                'translative', # into
                'terminative', # as far as, up to
                'essive',      # as 
                'inessive',    # in
                'superessive', # on  
                'adessive',    # by, at
                'illative',    # into
                'sublative',   # onto
                'allative',    # to
                'elative',     # out of
                'delative',    # off, about
                'ablative',    # from, away from
                'genitive',    # of, 's 
                'locative',    # location
                'vocative',    # object being addressed 
                'partitive',   # partialness
                'abessive',   # without
                'comitative', # with
            ]
        else:
            self.declensions = declensions

        # define the nouns
        if not nouns:
            self.nouns = ['monkey','falcon','puma','conda','tilapia','banana','kiwi','coconut','pear','river','mountain','ocean','lake','forest','clearing','valley','one','two','many',]
        else:
            self.nouns = nouns
        
        self.gen_parts_of_speech_dict()
        self.gen_dec_dict()
        self.gen_noun_dict()
        self.gen_plurals_dict()
        self.gen_inf_markers_dict()
        self.gen_tense_dict()
        self.gen_mood_dict()
        self.gen_aspect_dict()

    def generate_rnd_phone(self):   
        # generate a random phoneme
        return random.choice(self.consonants) + random.choice(self.vowels)

    def generate_num_phoneme(self,consonant, vowel):
        # pick a consonants cons from consonants
        c = self.consonants[consonant % len(self.consonants)]
        # pick a vowel from vowels
        v = self.vowels[vowel % len(self.vowels)]
        return c + v 

    def generate_rnd_stem(self,k=3):
        # generate a random word with k phonemes
        word = ''
        for i in range(k):
            word += self.generate_rnd_phone()
        return word 

    def gen_parts_of_speech_dict(self):
        # generate a dictionary of parts of speech        
        pos_markers = [""]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.parts_of_speech_open)-1)]
        self.pos_dict = {pos_markers[i]:self.parts_of_speech_open[i] for i in range(len(self.parts_of_speech_open))}
        # currently the closed pos are ignored

    # The criterion for an ending to be a case (according to today's generative linguistic grammars of Hungarian) is that a word with that ending can be a compulsory argument of a verb. This difference is usually unimportant for average learners of the language.

    def gen_dec_dict(self):
        # generate a dictionary of declensions        
        markers = [""]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.declensions)-1)]
        self.declenations_dict = {markers[i]:self.declensions[i] for i in range(len(self.declensions))}

    def gen_plurals_dict(self):
        # generate a dictionary for plurals affixes
        ## TODO make a parameter
        self.numbers = ['singular','plural']    
        markers = [""]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.numbers)-1)]
        self.plu_markers_dict = {markers[i]:self.numbers[i] for i in range(len(self.numbers))}

    def gen_inf_markers_dict(self):
        # generate a dictionary for plurals affixes
        ## TODO make a parameter
        self.inflections = ['1ps','2ps','3ps','1pp','2pp','3pp'] 
        markers = [""]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.inflections)-1)]
        self.inf_markers_dict = {markers[i] : self.inflections[i] for i in range(len(self.inflections))}

    def gen_tense_dict(self):
        # generate a dictionary for tenses affixes
        ## TODO make a parameter
        self.tenses = ['past','present','future']
        markers = [""]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.tenses)-1)]
        self.tense_markers_dict = {markers[i] : self.tenses[i] for i in range(len(self.tenses))}

    def gen_mood_dict(self):
        # generate a dictionary for tenses affixes
        ## TODO make a parameter
        self.moods = ['indicative','subjunctive','imperative','conditional','optative','jussive','interrogative','exclamatory']
        markers = [""]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.moods)-1)]
        self.mood_markers_dict = {markers[i] : self.moods[i] for i in range(len(self.moods))}

    def gen_aspect_dict(self):
        # generate a dictionary for tenses affixes
        ## TODO make a parameter
        self.aspects = ['perfective','imperfective','progressive','habitual','frequentative','iterative']        
        markers = [""]+ [self.generate_num_phoneme(i, 0) for i in range(len(self.aspects)-1)]
        self.aspects_dict = {markers[i] : self.aspects[i] for i in range(len(self.aspects))}


    def gen_noun_dict(self):

        self.nouns = ['monkey','falcon','puma','conda','tilapia','banana','kiwi','coconut','pear','river','mountain','ocean','lake','forest','clearing','valley','one','two','many',]

        ## 1. generate a stem for each noun
        stems = [self.generate_rnd_stem(3) for i in range(len(self.nouns))]

        ## 2. a dictionary of nouns

        self.nouns_dict = {stems[i]:self.nouns[i] for i in range(len(self.nouns))}


    def gen_lexicon(self):

        self.lexicon = {}

        for stem in (self.nouns_dict):
            print(f'\n\nlemma: {stem} = {self.nouns_dict[stem]}')
            for pos in self.pos_dict: 
                #print(pos)
                if self.pos_dict[pos] == 'noun': 
                    for declension in self.declenations_dict: 
                        for plural in self.plu_markers_dict: 
                            lexeme = f'{stem}\'{pos}{declension}{plural}'
                            features =  f'{self.nouns_dict[stem]},{self.pos_dict[pos]},{self.declenations_dict[declension]},{self.plu_markers_dict[plural]}'
                            self.lexicon[lexeme] = features
                            print(f'{lexeme} = {features}')
                elif self.pos_dict[pos] == 'verb':
                    for mood in self.mood_markers_dict:
                        for aspect in self.aspects_dict:
                            for tense in self.tense_markers_dict:
                                for inflection in self.inf_markers_dict:
                                    lexeme = f'{stem}\'{pos}{mood}{aspect}{tense}{inflection}'
                                    features = f'{self.nouns_dict[stem]},{self.pos_dict[pos]},{self.mood_markers_dict[mood]},{self.aspects_dict[aspect]},{self.tense_markers_dict[tense]},{self.inf_markers_dict[inflection]}'
                                    self.lexicon[lexeme] = features
                                    print(f'{lexeme} = {features}')
                else:
                    lexeme= f'{stem}\'{pos}'
                    features = f'{self.nouns_dict[stem]},{self.pos_dict[pos]}'
                    self.lexicon[lexeme] = features
                    print(f'{lexeme} = {features}')
        
    def export_lemmas(self,filename='lexicon.csv'):
        # export the lexicon to a csv file
        with open(filename, 'w') as f:
            writer = csv.writer(f)
            writer.writerow(['lemma', 'features'])
            for lemma, features in self.lexicon.items():
                writer.writerow([lemma, features])       

base = base_morphology(
    vowels=['a','e', 'i','o', 'u',],
    consonants=['b', 'c', 'd','f', 'g','h', 'j', 'k', 'l','m','n', 'p', 'q'],
    parts_of_speech_closed=['pronoun','article','preposition'],
    parts_of_speech_open=['noun','verb'],
    declensions=['nominative'],
    nouns=['monkey','falcon'],
)

print(f'{base.generate_rnd_phone()=}')
print(f'{base.generate_num_phoneme(3, 2)}')
print(f'{base.generate_rnd_stem(3)}')
print (f'{base.pos_dict=}')
print (f'{base.declenations_dict=}')
print (f'{base.nouns_dict=}')
print (f'{base.plu_markers_dict=}')
print (f'{base.inf_markers_dict=}')
print (f'{base.tense_markers_dict=}')
print (f'{base.mood_markers_dict=}')
print (f'{base.aspects_dict=}')
print (f'{base.gen_lexicon()=}')
base.export_lemmas()

#export_lemmas(lexicon)
# f = open('dict.csv','wb')
# w = csv.DictWriter(f,mydict.keys())
# w.writerow(mydict)
# f.close()

```

## Morphology and States

Morphology refers to morphemes. 
These are the smallest packages of semantic meaning.
Although not particularly clear cases, inflection and derivations extend the 
semantics of the root in many systematic ways that can be viewed as part of a
powerful ontology. Adjectives and Adverbs are also ways to extend the semantics of the root.

What struck me as I developed this baseline morphology is that while it is derived as a sum of products of related morphemes from a list or factor (product of lists) of features.

While these features are defined in terms of linguistic properties of natrural languages, we do not neccessarily need to derive the morphemes from these specific features. We could look at this in a more abstract form as construct from sums of products of feature factors. I think the baseline overgenerates the verbs. 

One For example

```{python}
from itertools import product

def constrained_product(feature_sets, constraint_fn):
    """
    Generates product combinations satisfying constraint_fn.
    """
    return [combo for combo in product(*feature_sets) if constraint_fn(combo)]

# Example
colors = ['red', 'blue']
sizes = ['S', 'M', 'L']
styles = ['casual', 'formal']

def constraint(c):
# This yields only combinations where red ≠ formal
    color, size, style = c
    return not (color == 'red' and style == 'formal')  # example constraint

valid = constrained_product([colors, sizes, styles], constraint)
for item in valid : 
    print(item)

```

So if we wanted to generalize we might add a spec that defines sums of products of factors with constraints. This can be useful for phonotactics and other features.

- e.g. 
    1. declensions go with pos = noun 
    1. inflections tense mood aspect go with pos = verb


adjectives

- hope
    - hopeful 
    - hopeless 
- care
    - careful (-ful)
    - careless (-less)
- fast
    - fastness (-ness)    
    - fastlike (-like)
    - fasted (-ed)
    - faster / fastest (comparative/superlative suffixes)    
    - fasten (-en)
    - fasting (-ing)
- base
    - basic (-ic)
    - baseless (-less)
    - baser / basest (comparative/superlative suffixes)
    - baselike (-like)
    - basial (rare/technical, -ial)
    - based (-ed)
    - baser / basest (comparative/superlative suffixes)
- race
    - racial (-al)
    - raceless (-less)
    - raced (-ed)
    - racer / raciest (comparative/superlative suffixes)
    - racelike (-like)
    - racial (-al)
    - racism (-ism)
    - racist (-ist)


nouns should be:
<stem> +  <plural>+ <possessor> + <possession-person> + <possession-plural>
<declension> + 

in your houses

## Bayesian Morphology ?

```{python}
from itertools import product

def constrained_dict_product(dicts, value_constraint):
    """
    Given a list of dicts, returns (key_tuple, value_tuple)
    for each product, filtering on values.
    """
    key_sets = [d.keys() for d in dicts]
    val_maps = [d for d in dicts]

    results = []
    for key_combo in product(*key_sets):
        val_combo = tuple(d[k] for d, k in zip(val_maps, key_combo))
        if value_constraint(val_combo):
            results.append((key_combo, val_combo))
    return results

# Example dicts
size = {1: 'S', 2: 'M'}
color = {10: 'red', 20: 'blue'}
style = {100: 'casual', 200: 'formal'}

# Constraint on values only
def constraint(vals):
    c, s, t = vals
    return not (c == 'red' and t == 'formal')

output = constrained_dict_product([color, size, style], constraint)
print(f'{output}')
```


A more general version ...

design:

1. we generate using features. using a generator.
1. we generate a dataframe or a dict with all the features.
1. we apply a constraint function to filter the features.
1. we use a dictionary or a function to map features to morphemes.

```{python}
from itertools import product
import pandas as pd
#import numpy as np
import random
import string
import csv


# class abstract_morphology:

#     # init takes a dict of factors and a constraint
#     def __init__(self,
#                     factors = None,
#                     constraint_fn = None,
#                     ):
#         if not factors:
#             self.factors = {
                            
#                  'pos' : ['noun','verb','adjective','adverb'],
#                  'tense' : ['past','present','future'],
#                  'mood' : ['indicative','subjunctive','imperative'],
#                  'aspect' : ['perfective','imperfective','progressive'],
#                  'inflection' : ['1ps','2ps','3ps','1pp','2pp','3pp'],
#                  'declension' : ['nominative','accusative','dative','instrumental','causal','translative','terminative','essive','inessive','superessive','adessive','illative','sublative','allative','elative','delative','ablative','genitive','locative','vocative']
#             }                                    
#         else:
#             self.factors = factors

#         if not constraints:
#             self.constraints = return (pos == 'noun' and declension not None)
#         else:
#             self.constraint_fn = constraint_fn



#         self.lexicon = self.gen_lexicon()

#     def gen_lexicon(self):

#         return [combo for combo in product(*feature_sets) if constraint_fn(combo)]


from itertools import product

def constrained_product(feature_sets, constraint_fn):
    """
    Generates product combinations satisfying constraint_fn.
    """
    return [combo for combo in product(*feature_sets) if constraint_fn(combo)]

def morph_constraint(c):
    """
    Enforces different slot requirements depending on `pos` value.
    """
    stem = c[0]
    pos = c[1]

    if pos == 'noun':
        # Expected: (pos, stem, decl, possessor, possessed, number)
        return stem is not None and all(f is not None for f in c[2:6]) and all(f is None for f in c[6:])
    elif pos == 'verb':
        # Expected: (pos, stem, inflection, mood, aspect, tense)
        return stem is not None and all(f is not None for f in c[6:10]) and all(f is None for f in c[2:6])
    elif pos not in {'noun', 'verb'}:
        # Expected: (pos, stem, None, None, None, None)
        return stem is not None and all(f is None for f in c[2:])
    return False

pos = ['noun', 'verb', 'adjective']
stem = ['falcon', 'cat']
decl = ['I', 'II']
possessor = ['1PS', '2PS', '3PS', '1PP', '2PP', '3PP']
possessed = ['0', '1']
number = ['sg', 'pl']
inflection = ['1PS', '2PS', '3PS', '1PP', '2PP', '3PP']
tense = ['past', 'present', 'future']
mood = ['indicative', 'subjunctive', 'imperative']
aspect = ['perfective', 'imperfective', 'progressive']

# Same arity for all: (pos, stem, slot3, slot4, slot5, slot6)
features = [stem, 
            pos, 
            decl + [None], 
            possessed + [None],
            number + [None],
            possessor + [None], 
            inflection + [None],
            aspect + [None],
            mood + [None], 
            tense + [None],
            ]

valid = constrained_product(features, morph_constraint)

for item in valid:
    print(item)
```
    

## Syntax

a fixed template syntax to go with the morphology that can be generalized to a rule based grammar.

children start with one word sentence then learn two sentences and then three word sentences and so on.



