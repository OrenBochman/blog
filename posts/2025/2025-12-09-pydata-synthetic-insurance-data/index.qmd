---
title: "Harnessing Generative Models for Synthetic Non-Life Insurance Data"
subtitle: "PyData Global 2025 Recap"
date: 2025-12-09
categories: ["PyData", "Insurance", "Generative Models", "Synthetic Data", "Machine Learning"]
keywords: ["PyData", "Insurance", "Generative Models", "Synthetic Data", "Machine Learning"]
description: "An in-depth exploration of using various generative models to create synthetic non-life insurance premium data, including validation techniques and model comparisons."
image: pydata_logo.png
---

![pydata global](pydata_logo.png){.column-margin}


::: callout-tip
## Lecture Overview

This study is oriented to a synthetic non-life insurance premium dataset generated using several Generative Models.\
As a benchmark, a Conditional Gaussian Mixture Model has been employed.\
The validation of the generated data involved several steps: data visualization, comparison with univariate analysis, PCA and UMAP representations between the trained data and the generated samples.\
In addition, check the consistency of data produced, the statistical Kolmogorov–Smirnov test and predictive modeling of frequency and severity with Generalised Linear Models (GLMs) exploited by [Tweedie distribution](https://en.wikipedia.org/wiki/Tweedie_distribution) as a measure of the generated data's quality, followed by the evidence of features importance.\
For further comparison, advanced Deep Learning architectures have been employed:

-   Conditional Variational Autoencoders (CVAEs),
-   CVAEs enhanced with a Transformer Decoder,
-   a Conditional Diffusion Model, and Large Language Models.

The analysis assesses each model’s ability to capture the underlying distributions, preserve complex dependencies, and maintain relationships intrinsic to the premium data.\
These findings provide insightful directions for enhancing synthetic data generation in insurance, with potential applications in risk modeling, pricing strategies with data scarcity, and regulatory compliance.

In classification and regression tasks, generative models aim to learn the joint probability distribution of data.\
These models focus on generating data points similar to the training data.\
Open insurance datasets are rare because they encode proprietary risk structures of the Company, limiting researchers’ access to comprehensive data for analysis and assessing new approaches.\
Generative models enable reproducible experimentation and innovation today. In the talk I explore several generative models used to produce synthetic data.
:::

::: callout-tip
## What You'll Learn:

In the talk I explore several generative models used to produce synthetic data.

1.  Conditional Gaussian Mixture Models used as a benchmark;
2.  Conditional Variational Autoencoders;
3.  Conditional Variational Autoencoders with a Transformer Decoder;
4.  Conditional Diffusion Model;
5.  Large Language Models.

Finally, I gave the overall results, followed by different approaches.
:::

::: callout-tip
## Prerequisites:

-   Basic Python and PyTorch
-   Some familiarity with neural networks (e.g., feed-forward, softmax)
-   No need for prior experience in building models from scratch
:::

## Tools and Frameworks:

We will introduce you to certain modern frameworks in the workshop but the emphasis be on first principles and using vanilla Python and LLM calls to build AI-powered systems.

[workshop repo](https://github.com/hugobowne/AI-for-SWEs)

::: callout-tip
## Speakers:

### Claudio Giorgio Giancaterino

-   Statistics & Actuarial background
-   Actuary during the day
-   Data Scientist in the free time
-   c.f [links](https://sites.google.com/view/claudio-links/home)
:::

---

## Outline

![About the Speaker](slide01.png)

![Agenda](slide02.png)

![Motivations](slide03.png)

![Data scarcity](slide04.png)

![Anatomy of Insurence Non-Life Risk Data](slide05.png)

### The Data

![Datasets used](slide06.png)

-   These are kind of similar.

-   I thought that non-life insurance was very broad.

<!-- -->

-   Should look into other datasets

![Unlocking data Quality](slide07.png)

![Synthetic Data Generation Trials](slide08.png)

### The Models

this is like a party ?

![Conditional Gaussian Mixture Model (CGMM)](slide09.png)

- I covered Gaussian mixtures in my notes on Bayesian Mixture Models
-  EM is fine for Point estimates but It seems that using a Hierarcial MCMC the ClaimOcc would be handled just as well without conditioning.

![Conditional Variational Auto-Encoder (CVAE)](slide10.png)

This is analogous to a forger

![ Conditional Variational Auto-Encoder with a Transformer based Decoder (CTVAE)](slide11.png)

Create a story but start with an outline - i.e. to make it a richer story.

![Conditional Difusion Model (CDM)](slide13.png)

Add noise to an image and then learn to denoise it.

![LLM](slide14.png)

- Unclear how using an LLM would be applicable to the tabular datasets discussed above.
- What is the context,
- what are the sequences ?

Perhaps this is just a feed forward Neural network doing regression or a transformer doing regression.

### Validation

![Validation by Consistency records](slide15.png)

![Validation by Koglomorov-Smirnov Test](slide16.png)

![Validation by data Visualization - Univariate analysis](slide17.png)

![Validation by data Visualization - Correleation matrix](slide18.png)

![Validation by data Visualization - 3D PCA](slide19.png)

![Validation by data Visualization - 3D UMAP](slide20.png)

![Validation by predictive modeling - Frequency and Severity prediction](slide21.png)

![Validation by Feature Importance - SHAP Feature Importance](slide22.png)

### Results and Conclusions

![Overall Results](slide23.png)

![Conclusions](slide24.png)

## My Reflections:

-   The speaker is a very smart/accomplished person.
-   The validation parts is very interesting - it would be interesting to see what he can say about model validation in general.
-   The model intuitions are neat. Worth reviewing and noting down!
-   How was the data used with LLM ?
-   How is the LLM trained?