---
title: "Where Have All the Metrics Gone?"
subtitle: "PyData Global 2025 Recap"
date: 2025-12-09
categories: ["PyData", "Metrics", "Machine Learning", "AI", "Data Science", "Evaluation"]
keywords: ["PyData", "Metrics", "Machine Learning", "AI", "Data Science", "Evaluation"]
description: "Exploring the challenges and strategies for defining and measuring metrics in modern AI systems, especially when traditional metrics are unavailable or inadequate."
image: pydata_logo.png
---

::: {.callout-tip}
## Lecture Overview

How exactly does one validate the factuality of answers from a Retrieval-Augmented Generation (RAG) system? Or measure the impact of the new system prompt for your customer service agent? What do you do when stakeholders keep asking for "accuracy" metrics that you simply don't have? In this talk, we’ll learn how to define (and measure) what “good” looks like when traditional model metrics don’t apply.

In the good old supervised learning days, standard measures like accuracy, F1, and MSE were like blazes on the data science trail, showing us how to descend the gradient towards "better". But now we're in uncharted analytics territory, where our work increasingly involves unlabeled data and generative AI outputs, and metrics are either unavailable or undefined.

The key to every successful trek is preparation. We have to move from thinking about “metrics as defaults” to “metrics as design choices." We also need to be ready to design those metrics before we even start testing, because when we devise metrics post-training, we risk HARKing (Hypothesizing After Results are Known) and losing our scientific footing.
:::




::: {.callout-tip}
## Learning Objectives

This talk will provide a field guide for translating different kinds of modern research questions into clearly-defined metrics, including:

- Metrics of the past and why they aren't as useful now (~5 min)
Common failure modes when attempting to evaluate generative AI outputs and other unlabeled data (~8 min)
- Techniques for identifying proxies when labels are missing (~8 min)
- Defining criteria for open-ended outputs (~8 min)
- Open source Python libraries (including new tools like outlines and dspy as well as old favorites like hypothesis and pytest) to equip you for your next data science adventure (~8 min)
- Come learn how to define and adapt new metrics so that you'll be prepared for wherever your modeling journey takes you.
:::


## Tools and Frameworks:

We will introduce you to certain modern frameworks in the workshop but the emphasis be on first principles and using vanilla Python and LLM calls to build AI-powered systems.


[workshop repo](https://github.com/hugobowne/AI-for-SWEs)


:::: {.callout-tip}
## Speakers: 

### Dr. Rebecca Bilbro

Wrote [Yellowbrick: Machine Learning Visualization
](https://www.scikit-yb.org/en/latest/)

Rebecca is an active contributor to the open source community and has conducted research on natural language processing, semantic network extraction, entity resolution, and high dimensional information visualization. She earned her doctorate from the University of Illinois, Urbana-Champaign, where her research centered on communication and visualization practices in engineering. Rebecca is co-founder and CTO of Rotational Labs.
:::



## Outline

![](slide01.png)

![](slide02.png)

![](slide03.png)

![](slide04.png)

![](slide05.png)

![](slide06.png)

![](slide07.png)

![](slide08.png)

![](slide09.png)

![](slide10.png)

![](slide11.png)

![](slide12.png)

![](slide13.png)

![](slide14.png)

![](slide15.png)

![](slide16.png)

![](slide17.png)

![](slide18.png)

![](slide19.png)

![](slide20.png)

![](slide21.png)

![](slide22.png)

![](slide23.png)

![](slide24.png)

![](slide25.png)

![](slide26.png)

![](slide27.png)

![](slide28.png)

![](slide29.png)

![](slide30.png)

![](slide31.png)