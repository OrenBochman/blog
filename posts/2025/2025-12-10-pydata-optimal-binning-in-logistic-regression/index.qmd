---
title: "Optimal Variable Binning in Logistic Regression"
subtitle: "PyData Global 2025 Recap"
date: 2025-12-10
categories: ["PyData", "Logistic Regression", "Feature Engineering", "Optimal Binning"]
keywords: ["PyData", "Logistic Regression", "Feature Engineering", "Optimal Binning"]
description: "A practical guide to optimal variable binning techniques for enhancing logistic regression models in regulated industries."
image: pydata_logo.png
---

![pydata global](pydata_logo.png){.column-margin}


::: {.callout-tip}
## Lecture Overview

In many regulated industries—finance, healthcare, insurance—logistic regression remains the model of choice for its interpretability and regulatory acceptability. 

Yet capturing non-linear effects and interactions often requires variable binning, and naive approaches (equal-width or quantile cuts) can either wash out signal or invite overfitting. 

In this 30-minute session, data scientists and risk analysts with a working knowledge of logistic regression and Python will learn to:

- Diagnose the weaknesses of basic binning strategies.
- Select and apply optimal-binning algorithms for different use cases.
- Assess bin stability and guard against model overfit.

All code, data samples, and a notebook will be available on GitHub.
:::

Despite the rise of complex “black-box” models, regulated environments still demand transparency. [Properly binned variables not only improve model fit but also yield coefficients that the business and auditors can interpret]{.mark}.

However, determining cut-points that preserve true signal while avoiding data-snooping bias is non-trivial.

::: {.callout-tip}
## What You'll Learn:

- Understand the basic idea behind binning (the what)
- To know in which contexts variable binning makes sense (the when and why).
- Choose among popular optimal-binning techniques (e.g., ChiMerge, MDLP, decision-tree-based) based on data size, feature type, and operational constraints (the how).
:::


::: {.callout-tip}
## Audience & Prerequisites:

- Data scientists and risk analysts who use logistic regression in regulated settings and need a reproducible, explainable feature-engineering pipeline.
- Prerequisites: Basic Python (pandas, scikit-learn) and logistic-regression familiarity
- Materials: GitHub repo with notebook, data samples, will be shared during the talk

:::


::: {.callout-important}
## Tools and Frameworks:

- [OptBinning](https://gnpalencia.org/optbinning/)
:::

:::: {.callout-tip}
## Speakers: 

### Charaf Zguiouar

Quantitative Finance and Econometrics Gradutate from Sorbonne's University. 
Currently working as Data Scientist at BNP Paribas & as lecturer at Sorbonne's University.


- [website](zgcharaf.github.io)
- [workshop repo](https://github.com/zgcharaf/Pydata-Global-Talk-December-2025)
:::


## Outline

![Optimal  Binning in Logistic Regression](slide01.png)

![Agenda](slide02.png)

![Who am I](slide03.png)

![Modeling under uncertainty](slide05.png)

![From model risks to modeling choices](slide06.png)

![Logistic regression recap](slide07.png)

![what is binning](slide08.png)

![WoE and IV](slide09.png)

Weight of Evidence (WoE) and Information Value (IV) are two key concepts in variable binning for logistic regression.

$$
WoE_j = \ln\left(\frac{Good_j / Total\ Good}{Bad_j / Total\ Bad   }\right) 
$$ {#eq-woe-bin}

$$
IV = \sum_j \left(\frac{Good_j}{Total\ Good} - \frac{Bad_j}{Total\ Bad}\right) \times WoE_j
$$ {#eq-iv-bin}

![IV as a feature selection metric](slide10.png)

![When log-odds are not linear](slide11.png)

![What is binning?](slide12.png)

![Model A vs Model B: What is wrong here?](slide13.png)

![Investigating like Sherlock Holmes](slide14.png)

![Case study dataset](slide15.png)

![Feature Overview](slide16.png)

### Four Binning Strategies

![Age vs CHD risk – Decile (Quantile) Binning](slide17.png)

![Age vs CHD risk – Equal-Width Binning](slide18.png)

![Age vs CHD risk – Tree-Based Binning](slide19.png)

![Age vs CHD risk – Optimized Binning](slide20.png)

![Four modeling approaches we will compare](slide21.png)

![AUC & ROC comparison](slide22.png)

![How Boosting Algorithms Handle Binning 1](slide23.png)

![How Boosting Algorithms Handle Binning 2](slide24.png)

![Optimal binning as an optimisation problem](slide25.png) 

![MDLP: Entropy-based Binning](slide26.png) 

![Mathematical programming-based optimal binning](slide27.png)

![Stochastic optimal binning](slide28.png)

![What "good" looks like](slide29.png)

![Conclusion & how to explore further](slide30.png)

![OptBinning library](slide31.png)

- OptBinning is a Python library for optimal binning and scorecard modelling.
- Created and maintained by Guillermo Navas-Palencia.
- Implements mathematical programming formulations for:
    - Binary, continuous and multiclass targets.
    - Monotonicity, minimum size, and other business constraints

- Documentation: gnpalencia.org/optbinningGitHub 
- repository: github.com/guillermo-navas-palencia/optbinning

![Question](slide32.png)

![Thanks](slide33.png)

### Reflection

We looked at what we mean by binning in Logistic Regression, why and when to use it, and how to choose an optimal binning technique based on data and operational constraints. 

We also saw how to implement these techniques in Python using the OptBinning library.