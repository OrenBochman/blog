Interesting, I knew some, I found almost all of this material and more on Wikipedia. https://en.wikipedia.org/wiki/AI-assisted_targeting_in_the_Gaza_Strip
. Also, this is not an official source from the IDF, but someone from acedemia. Also, I realise that many of the things he presents as facts are  opinions, interpretations, and analogies. He said that these tools underwent changes throughout the war, but this was used back in 2021, so it must be older.

1. The chief of staff decided to set this up, but this is something that started with someone who is an expert in 8200, who proposed and planned this. 
2. While this stuff is modern it is not very hard to develop, there are very many people who learned it for a year that can put this together and the main difference between most application is to ingest different type of table and put it in the table with 2+ columns.
3. We know that 8200 put all the calls for Gaza, etc, on Microsoft Azure Clouds. One system that used this is not so much to know what they said but to find the location. What he didn't say, which anyone in this knows, is that they don't need the call to find the location - the cellphone tower knows this. What the project did with the voice is to put a voice fingerprint on the person.
2. They had problematic goals (Massive damage - not a legitimate goal of war, rather than removing all mid-low-level enemy combatants) Except that the only person who said it was another researcher not some General or AirFoce Commander - c.f. the wikipedia article.
3. They developed the protocols  with top-tier lawyers 
4. It sounds like the algorithms were not set up in a way that the commanders can see ...
5. He says IDF did not check the outcomes. But he immediately admits that they did - but not in a way that he approves of. I don't really care what he thinks. In the artillery, there was a small department that did stats on the accuracy. For an ML/AI, there are clearly IDF protocols to check because you need them to improve the system. 

Some alarming points...

1. Having a system that prioretises damage is a blatent war crime - so even if the "general" said this I don't think this says anything about the degisn of the system but on how it is used.
2. The system used 50 historical targets as a training set ? *If it is true, it is very bad* .... We know that there were many, many targets from the past. Was there no data? Did 8200, etc, Mossad, Shavak erase their databases before this project started ? So I don't buy this claim. What he actually said was that they had 50 active/quality targets before the system was created, and then suggested that they used this to jump to all the data they bought/stole from Facebook, etc.
4. He says that more accurate weapons kill more people. Very true. Non-lethal weapons are used 10x+ than lethal weapons.
5. He says nothing about Hamas being embedded in the population.
6. The criteria for collateral damage are also alarming, except that it must be put in the context of other conflicts. He says nothing about this, but he is a researcher. I hear claims by other experts that the collateral damage is lower than in other conflicts.
7. In sophisticated models, there is a "dial" for tweaking things like collateral damage and other war crimes-related criteria. He says nothing about it. This is not something that you show the decision maker - he needs to see the bottom lines if he must make a decision in 20 seconds. If there were 2 hours to decide, then it would make a long report and complex dashboards. So this again shows his bias.
8. He suggests that they use ChatGPT to make life and death decisions. Sounds bad. A reality check is that the new GPT is used to check for very specific things, such as whether there are two speakers in a call who are children. Or if this is a teacher talking to a student. ChatGPT is much better than anything else for this. But yes, it can get things wrong.... but the error rates add up and the model then decides how sure it is that the target is a legitimate target and that it is not in a school, etc.
9. The Russians have generals famous for carpet bombing the enemy. We may have done the same, but justified it with A.I., or we know about so many enemy combatants that no place is safe. It might seem semantics, but in the end, we hit about 1 in 3 buildings in Gaza. I am pretty sure there are many other buildings we never hit with new terrorists inside. So this is an alarming number but expected if the war goes on and on.