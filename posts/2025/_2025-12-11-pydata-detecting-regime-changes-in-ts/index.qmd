---
title: "How Big are SLMs"
subtitle: "PyData Global 2025 Recap"
date: 2025-12-11
image: pydata_logo.png
---

![pydata global](pydata_logo.png){.column-margin}


::: {.callout-tip}
## Lecture Overview

Want to understand how transformers actually work without wading through 10,000 lines of framework code or drowning in tensor shapes? This talk walks you through building a transformer model from scratch ‚Äî no pre-trained shortcuts, no black-box abstractions ‚Äî just clean PyTorch code and good old-fashioned curiosity. You'll walk away with a clearer mental model of how attention, encoders, decoders, and masking really work.
:::

Transformers power modern large language models, but their inner workings are often buried under complex libraries and unreadable abstractions. In this talk, we‚Äôll peel back the layers and build the original Transformer architecture (Vaswani et al., 2017) step by step in PyTorch, from input embeddings to attention masks to the full encoder-decoder stack.

This talk is designed for attendees with a basic understanding of deep learning and PyTorch who want to go beyond surface-level blog posts and get a hands-on, conceptual grasp of what happens under the hood. You'll see how each part of the transformer connects back to the equations in the original paper, how to debug common implementation pitfalls, and how to avoid getting lost in tensor dimension hell.


::: {.callout-tip}
## What You'll Learn:

- üîç A walkthrough of key components: attention, positional encoding, encoder/decoder stack
- üß† Visual explanations of attention masks, shapes, and residuals
- ‚ö†Ô∏è Common bugs and debugging strategies (like handling shape mismatches and masking errors)
- ‚úÖ Real-world implementation tips and tricks that demystify the architecture

By the end of the talk, attendees will:

- Understand the full forward pass of a transformer
- Know how each component connects to the original paper
- Feel more confident reading or writing custom model architectures
:::


::: {.callout-tip}
## Prerequisites:

- Basic Python and PyTorch
- Some familiarity with neural networks (e.g., feedforward, softmax)
- No need for prior experience in building models from scratch
:::

## Tools and Frameworks:

We will introduce you to certain modern frameworks in the workshop but the emphasis be on first principles and using vanilla Python and LLM calls to build AI-powered systems.


[workshop repo](https://github.com/hugobowne/AI-for-SWEs)


:::: {.callout-tip}
## Speakers: 

### Robin Troesch

Data Engineer trying to reduce the impact of computing on the climate and helping the energy transition.
Working at Electricity Maps in Copenhagen (DK) since 2022 first in the data platform team responsible for acquiring grid data. Joined the grid forecast team in 2023.
:::



## Outline

![](slide01.png)

![](slide02.png)

![](slide03.png)

![](slide04.png)

![](slide05.png)

![](slide06.png)

![](slide07.png)

![](slide08.png)

![](slide09.png)

![](slide10.png)

![](slide11.png)

![](slide12.png)

![](slide13.png)

![](slide14.png)

![](slide15.png)

![](slide16.png)

![](slide17.png)

![](slide18.png)

![](slide19.png)

![](slide20.png)

![](slide21.png)

![](slide22.png)

![](slide23.png)

![](slide24.png)

![](slide25.png)