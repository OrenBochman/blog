<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Oren Bochman&#39;s Blog</title>
<link>https://orenbochman.github.io/reviews.html</link>
<atom:link href="https://orenbochman.github.io/reviews.xml" rel="self" type="application/rss+xml"/>
<description>Personal website, portfolio and blog</description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Sat, 14 Sep 2024 10:59:04 GMT</lastBuildDate>
<item>
  <title>Multi-column Deep Neural Networks for Image Classification</title>
  <link>https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/</link>
  <description><![CDATA[ 




<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="cireşan2012multicolumndeepneuralnetworks">(Cireşan, Meier, and Schmidhuber 2012)</span> titled “Multi-column Deep Neural Networks for Image Classification”, the authors, Dan Cireşan, Ueli Meier, Juergen Schmidhuber introduce a biologically plausible deep artificial neural network architecture that achieves near-human performance on tasks such as the recognition of handwritten digits or traffic signs. The method uses small receptive fields of convolutional winner-take-all neurons to yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. The authors demonstrate that their approach outperforms humans on a traffic sign recognition benchmark and improves the state-of-the-art on various image classification benchmarks.</p>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks. — <span class="citation" data-cites="cireşan2012multicolumndeepneuralnetworks">(Cireşan, Meier, and Schmidhuber 2012)</span></p>
</blockquote>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="cireşan2012multicolumndeepneuralnetworks">(Cireşan, Meier, and Schmidhuber 2012)</span> the authors make significant strides in the field of image classification by demonstrating the effectiveness of multi-column deep neural networks (DNNs). <mark>This work is noteworthy for its pioneering approach in applying deep learning techniques to image classification tasks, which have since become the foundation of modern computer vision systems.</mark></p>
</section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<p>The authors present a system that uses several deep neural networks, each operating as a “column,” which are trained independently. The outputs of these networks are then averaged to form the final prediction. This multi-column approach exploits the diversity between different networks and boosts classification accuracy, reducing the impact of overfitting and improving generalization. Notably, the method achieved state-of-the-art results on several image classification benchmarks at the time, including the MNIST digit recognition task.</p>
<p>One of the central contributions of this paper is the demonstration of how <mark>combining multiple deep networks can outperform single networks in complex image classification tasks</mark>. The authors trained their models on NVIDIA GPUs, which allowed them to scale deep networks efficiently—a relatively new practice when this paper was published, underscoring its innovative edge.</p>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Improvement on Benchmarks</strong>: The multi-column DNN approach delivered unprecedented accuracy on datasets like MNIST, achieving an error rate of just 0.23%. This represents one of the early breakthroughs that paved the way for deep learning in computer vision.</p></li>
<li><p><strong>Effective Use of Parallelism</strong>: The paper highlights the use of modern GPUs to efficiently train deep networks, illustrating how hardware advancements can accelerate research progress.</p></li>
<li><p><strong>Generalizability</strong>: While the paper focuses on MNIST and other datasets, the multi-column DNN framework offers a flexible approach to other image classification tasks. The general architecture and training methodology could be adapted to more complex datasets, making this work highly relevant across a variety of image recognition problems.</p></li>
<li><p><strong>Robustness</strong>: By averaging outputs from multiple networks, the system reduces the sensitivity to the specific architecture or initialization of a single network. This ensemble-like approach increases robustness and reduces error rates.</p></li>
</ul>
</section>
<section id="weaknesses" class="level2">
<h2 class="anchored" data-anchor-id="weaknesses">Weaknesses</h2>
<ul>
<li><p><strong>Lack of Theoretical Insight</strong>: Although the empirical results are impressive, the paper does not delve deeply into the theoretical reasons behind the success of multi-column architectures. It remains unclear how much of the performance gain is due to ensembling versus the intrinsic strength of the individual networks.</p></li>
<li><p><strong>Computational Cost</strong>: The approach requires training multiple deep neural networks independently, which could be computationally expensive for larger datasets or higher-dimensional inputs. While GPUs mitigate this to an extent, scaling the multi-column approach to larger tasks would demand significant computational resources.</p></li>
<li><p><strong>Limited Applicability to Other Modalities</strong>: The paper focuses solely on image classification. While it hints at the potential for multi-column networks in other domains (e.g., audio or text), the paper doesn’t explore these extensions or provide empirical evidence beyond the image domain.</p></li>
</ul>
</section>
<section id="impact-and-relevance" class="level2">
<h2 class="anchored" data-anchor-id="impact-and-relevance">Impact and Relevance</h2>
<p>This paper marked a turning point for deep learning in computer vision, showing the power of combining deep networks for complex tasks like image classification. Its success on benchmarks like MNIST helped popularize deep learning as a dominant method for pattern recognition and set the stage for more advanced techniques. Although it primarily focuses on image classification, the insights regarding ensemble learning through independent deep networks have since inspired various approaches in different machine learning areas, including speech recognition and natural language processing.</p>
<p>The paper is particularly significant when viewed in the context of its time (2012), as it predated the massive adoption of deep learning across industries. Its methods were fundamental to later developments in deep convolutional neural networks, which have become a cornerstone of state-of-the-art models in computer vision tasks today.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Ciresan, Meier, and Schmidhuber’s work on multi-column deep neural networks represents a crucial step forward in the development of image classification techniques. Its impact on deep learning, especially in terms of model ensembling and parallelization using GPUs, cannot be overstated. While it comes with some computational challenges and lacks deep theoretical explanation, the paper’s practical results and novel approach have solidified its place as a landmark contribution in the history of deep learning.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-cireşan2012multicolumndeepneuralnetworks" class="csl-entry">
Cireşan, Dan, Ueli Meier, and Juergen Schmidhuber. 2012. <span>“Multi-Column Deep Neural Networks for Image Classification.”</span> <a href="https://arxiv.org/abs/1202.2745">https://arxiv.org/abs/1202.2745</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors</title>
  <link>https://orenbochman.github.io/reviews/2012/dropout/</link>
  <description><![CDATA[ 




<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="hinton2012improvingneuralnetworkspreventing">(Hinton et al. 2012)</span> titled “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors”, the authors, Hinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov introduces a new regularization technique called “Dropout” that helps to prevent overfitting in neural networks. Dropout is a simple and effective way to improve the performance of neural networks by preventing co-adaptation of feature detectors. The authors show that dropout can be used to improve the performance of a wide range of neural networks, including deep networks, convolutional networks, and recurrent networks.</p>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification, and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
</blockquote>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>, introduces the dropout technique as an innovative method to prevent overfitting in neural networks. Overfitting occurs when a model performs well on training data but poorly on unseen test data, particularly when dealing with a large number of parameters and limited training samples. The paper addresses this by proposing the use of dropout, a regularization technique that randomly omits units (neurons) during training.</p>
</section>
<section id="core-ideas" class="level2">
<h2 class="anchored" data-anchor-id="core-ideas">Core Ideas</h2>
<p>The central concept behind dropout is to prevent co-adaptation of feature detectors. In a traditional neural network, feature detectors can co-adapt to specific patterns in the training data, which leads to poor generalization. By randomly omitting neurons with a probability of 0.5 during training, each neuron is forced to contribute independently to the final output. This reduces the reliance on specific sets of neurons and ensures that each feature detector learns useful patterns.</p>
<p>Another significant advantage of dropout is that it acts as an efficient form of model averaging. Training with dropout can be seen as training an ensemble of neural networks that share parameters, making it computationally feasible to obtain better generalization without having to train multiple models.</p>
</section>
<section id="experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results</h2>
<p>The authors demonstrate the effectiveness of dropout on several benchmark datasets, including MNIST, CIFAR-10, ImageNet, TIMIT, and the Reuters corpus.</p>
<ul>
<li>MNIST: Dropout reduced the error rate from 160 errors to around 110 by applying 50% dropout to hidden units and 20% dropout to input units.</li>
<li>TIMIT: Dropout improved frame classification accuracy in speech recognition tasks, reducing the error rate by 3% in comparison to standard training methods.</li>
<li>CIFAR-10: The authors achieved a 16.6% error rate without dropout and 15.6% with dropout, outperforming previous state-of-the-art results.</li>
<li>ImageNet: Dropout applied to deep convolutional neural networks (CNNs) reduced the error rate from 48.6% to 42.4%.</li>
<li>Reuters Corpus: Dropout reduced classification error from 31.05% to 29.62%.</li>
</ul>
</section>
<section id="theoretical-contributions" class="level2">
<h2 class="anchored" data-anchor-id="theoretical-contributions">Theoretical Contributions</h2>
<p>The theoretical underpinning of dropout is grounded in model averaging and regularization. In standard practice, model averaging is performed by training multiple models and averaging their predictions, but this approach is computationally expensive. Dropout provides a far more efficient alternative by implicitly training an ensemble of models that share parameters, thus achieving the benefits of model averaging without the overhead of training separate models.</p>
<p>Additionally, dropout mitigates the problem of overfitting by introducing noise during training, making the model more robust. At test time, all units are used, but their outgoing weights are scaled to reflect the fact that fewer units were active during training.</p>
</section>
<section id="discussion-and-impact" class="level2">
<h2 class="anchored" data-anchor-id="discussion-and-impact">Discussion and Impact</h2>
<p>The introduction of dropout represents a major step forward in the development of deep learning models, as it allows for better generalization across a variety of tasks. Its simplicity, coupled with its effectiveness, has made dropout a standard tool in neural network training. The experiments conducted in the paper demonstrate its utility across a wide range of tasks, from image recognition to speech processing, providing compelling evidence of its broad applicability.</p>
<p>The idea of preventing co-adaptation of feature detectors to improve generalization is an elegant solution to a longstanding problem in neural network training. By ensuring that each neuron must work independently, dropout forces the model to learn more robust features that generalize well to unseen data.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This paper is a highly influential paper that introduced a novel technique for improving the generalization of deep learning models. The results speak for themselves, with dropout achieving state-of-the-art performance across multiple datasets and tasks. The technique has since become a standard part of neural network training, revolutionizing the field and contributing to the success of deep learning in real-world applications.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-hinton2012improvingneuralnetworkspreventing" class="csl-entry">
Hinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. <span>“Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.”</span> <a href="https://arxiv.org/abs/1207.0580">https://arxiv.org/abs/1207.0580</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/dropout/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>ImageNet Classification with Deep Convolutional Neural Networks</title>
  <link>https://orenbochman.github.io/reviews/2012/imagenet/</link>
  <description><![CDATA[ 




<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p><span class="citation" data-cites="krizhevsky2012imagenet">(Krizhevsky, Sutskever, and Hinton 2012)</span> is a seminal paper in the field of deep learning. It introduced the AlexNet architecture, which won the ImageNet Large Scale Visual Recognition Challenge in 2012. The paper is a great starting point for anyone interested in deep learning, as it provides a detailed explanation of the architecture and training process of the network.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>
<p>In <span class="citation" data-cites="krizhevsky2012imagenet">(Krizhevsky, Sutskever, and Hinton 2012)</span> titled “ImageNet Classification with Deep Convolutional Neural Networks”, the authors Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton presents the development and training of a large deep convolutional neural network (CNN) for image classification using the ImageNet dataset.</p>
<p>Thus paper marks a pivotal point in the development of deep learning, particularly in the realm of computer vision. The authors introduced a large convolutional neural network (CNN) trained on the ImageNet dataset, which significantly outperformed previous models, winning the <a href="https://www.image-net.org/challenges/LSVRC">ImageNet Large Scale Visual Recognition Challenge</a> (ILSVRC) in 2012 with a top-5 test error rate of 15.3%.</p>
<section id="key-contributions" class="level3">
<h3 class="anchored" data-anchor-id="key-contributions">Key Contributions:</h3>
<ul>
<li><p><strong>Architecture</strong>: The CNN consists of five convolutional layers and three fully connected layers, with the final layer being a softmax classifier that distinguishes between 1000 categories. This architecture involves a total of 60 million parameters and 650,000 neurons.</p></li>
<li><p><strong>GPU Utilization</strong>: Training was performed on two GTX 580 GPUs to speed up the process, allowing them to handle the large network size and dataset. This took approximately 5-6 days to complete.</p></li>
<li><p><strong>Techniques to Improve Performance</strong>: The network used a variety of novel techniques to improve both performance and training time:</p>
<ul>
<li><strong>Rectified Linear Units</strong> (ReLUs): These non-saturating neurons were employed to speed up training, which was crucial for dealing with such a large model.</li>
<li><strong>Dropout</strong>: A regularization method was used in fully connected layers to prevent overfitting by randomly dropping some neurons during training.</li>
<li><strong>Data Augmentation</strong>: The authors employed various forms of data augmentation, including random crops, horizontal flips, and color variation via principal component analysis (PCA), which greatly expanded the size of the training set and further reduced overfitting.</li>
</ul></li>
</ul>
<p>overfitting.</p>
</section>
</section>
<section id="results-and-impact" class="level2">
<h2 class="anchored" data-anchor-id="results-and-impact">Results and Impact</h2>
<p>The network trained for the ILSVRC 2010 and 2012 challenges achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, for ILSVRC 2010, far surpassing previous methods based on feature extraction and boosting. In the ILSVRC 2012 competition, the network reduced the top-5 error to 15.3%, compared to the 26.2% achieved by the second-best entry. This result not only established CNNs as the state-of-the-art model for image classification tasks but also cemented the importance of deep learning in the broader machine learning community.</p>
</section>
<section id="challenges-and-limitations" class="level2">
<h2 class="anchored" data-anchor-id="challenges-and-limitations">Challenges and Limitations</h2>
<p>The authors acknowledge that their network size was constrained by the available GPU memory and that improvements in both hardware and larger datasets could potentially improve the performance of such models in the future.</p>
<p>The CNN’s architecture and optimization techniques pioneered by this paper have set a foundation for subsequent advances in deep learning, particularly in image recognition tasks.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The paper demonstrated the feasibility and efficacy of training deep networks on large-scale datasets and provided key insights into architectural choices, regularization, and optimization. This work has since inspired a plethora of follow-up research, leading to advancements such as transfer learning, fine-tuning on smaller datasets, and the further development of GPU-based training methods. The innovations introduced in this paper laid the groundwork for the modern AI revolution in image recognition and beyond.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-krizhevsky2012imagenet" class="csl-entry">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“Imagenet Classification with Deep Convolutional Neural Networks.”</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/imagenet/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>NIN — Network in Network</title>
  <link>https://orenbochman.github.io/reviews/2013/NIN/</link>
  <description><![CDATA[ 




<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="lin2014networknetwork">(Lin, Chen, and Yan 2014)</span> the authors, Lin, Min, Qiang Chen, and Shuicheng Yan, of this paper titled “Network in Network” paper came up with a way of connencting somee ideas on improving CNNs which had been mostly getting bigger <img src="https://latex.codecogs.com/png.latex?(VGG%20%3E%20AlexNet%20%3E%20LeNet)"> . They replaced traditional linear filters in convolutional neural networks (CNNs) with multilayer perceptrons (MLPs) to enhance local feature abstraction. <mark>This new architecture, called NIN, also introduces <strong>global average pooling</strong> in place of <em>fully connected layers</em> thus reducing overfitting, improving model interpretability and more significantly reducing the size of the network.</mark></p>
<p>It took a while for the idea to catch on, but in computer vision, most of the parameters are in the fully connected layers, and the NIN architecture enables us to to reduce the number of parameters in the fully connected layers thereby <mark>breaking the curse of dimensionality in CNN</mark>. Once people realized this the NIN architecture became more widely adopted and influenced the development of more sophisticated deep learning architectures like the <mark>Inception architecture</mark> and further refined into the Resnet architecture</p>
<p>The NIN architecture has a significant impact on the design of CNNs by demonstrating that local feature abstraction can be enhanced with MLPs, leading to better performance with fewer parameters. Global average pooling, which replaces fully connected layers, makes the architecture more robust to overfitting and spatial translations, making it a powerful tool for image classification tasks. This combination of techniques has influenced the development of more sophisticated deep learning architectures, particularly in domains where model interpretability and reduced overfitting are critical.</p>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>We propose a novel deep network structure called “Network In Network” (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking multiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.</p>
<p>– <span class="citation" data-cites="lin2014networknetwork">(Lin, Chen, and Yan 2014)</span></p>
</blockquote>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="lin2014networknetwork">(Lin, Chen, and Yan 2014)</span> the authors introduced a novel deep learning architecture that aims to improve the abstraction capabilities of convolutional neural networks (CNNs) by incorporating multilayer perceptrons (MLPs) into the convolution layers. This approach, termed “Network in Network,” replaces the conventional linear filters used in CNNs with small neural networks, allowing for better local feature modeling. The NIN architecture also introduces global average pooling as a substitute for traditional fully connected layers to reduce overfitting and improve the interpretability of the model.</p>
</section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<p>The NIN paper makes several key contributions to the deep learning landscape:</p>
<ol type="1">
<li><p><strong>Mlpconv Layer</strong>: Instead of using traditional linear filters, NIN proposes the use of multilayer perceptrons (MLPs) within the convolutional layers (termed mlpconv layers). These layers act as universal function approximators, capable of modeling more complex representations within local receptive fields. This structure allows for better abstraction of non-linear latent concepts, overcoming the limitations of traditional linear filters in CNNs.</p>
<p><img src="https://orenbochman.github.io/reviews/2013/NIN/fig1.png" class="img-fluid"> <img src="https://orenbochman.github.io/reviews/2013/NIN/fig2.png" class="img-fluid"></p></li>
<li><p><strong>Global Average Pooling</strong>: NIN introduces global average pooling as an alternative to fully connected layers in the final classification stage. This technique computes the spatial average of each feature map, feeding the result directly into a softmax layer for classification. By avoiding fully connected layers, the model becomes less prone to overfitting, thus improving generalization performance. Furthermore, this method provides more interpretable results by establishing a direct correspondence between feature maps and class labels.</p></li>
<li><p><strong>State-of-the-Art Performance</strong>: The authors demonstrate that NIN achieves state-of-the-art performance on several benchmark datasets, including CIFAR-10, CIFAR-100, and SVHN, without the need for extensive data augmentation or model ensembling. The architecture consistently outperforms other methods, such as maxout networks and CNNs with dropout regularization, especially in terms of classification accuracy.</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/reviews/2013/NIN/results.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Cifar-10 error rates</figcaption>
</figure>
</div>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Innovative Architecture</strong>: The introduction of MLPs into convolutional layers is a simple yet effective modification that significantly enhances the representational power of the model. This makes NIN a powerful alternative to traditional CNNs, especially for tasks that require fine-grained feature extraction and abstraction.</p></li>
<li><p><strong>Reduced Overfitting</strong>: The use of global average pooling not only replaces the computationally expensive fully connected layers but also serves as a built-in regularizer, reducing the need for additional techniques like dropout. This structural regularization helps to prevent overfitting, particularly on datasets with limited training examples, such as CIFAR-100.</p></li>
<li><p><strong>Better Interpretability</strong>: The global average pooling layer allows for easier interpretation of the learned feature maps, as each map is directly associated with a class. This increases the transparency of the network’s the decision-making process compared to conventional CNNs.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/reviews/2013/NIN/fig4.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Visulization</figcaption>
</figure>
</div>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<ul>
<li><p><strong>Limited Novelty in Pooling</strong>: While global average pooling is effective, the concept is not entirely new, and its novelty is limited. Previous works have proposed similar techniques for specific tasks. However NIN certainly demonstrates the concepts efficacy.</p></li>
<li><p><strong>Scalability</strong>: The paper focuses primarily on relatively small datasets like CIFAR-10, CIFAR-100, SVHN, and MNIST. While NIN excels in these scenarios, it would be interesting to see how the architecture performs on larger, more complex datasets such as ImageNet, where the size and variety of the data might pose additional challenges.</p></li>
<li><p><strong>Lack of Depth Exploration</strong>: While the architecture consists of three stacked mlpconv layers, the paper does not deeply explore the impact of adding more layers or experimenting with deeper NIN networks. Such exploration could provide insight into how well the architecture scales with increased model depth.</p></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>NIN architecture is an elegant and effective solution to improving feature abstraction and reducing overfitting in CNNs. By embedding MLPs within convolutional layers and using global average pooling for classification, NIN achieves state-of-the-art performance across a variety of tasks. NIN presented a strong case for the importance of local feature modeling and interpretable classification mechanisms in modern deep learning.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>
<ul>
<li><a href="https://www.youtube.com/watch?v=QfNvhP6k6ZM">Alex Smola Course Video on NIN</a>, his <a href="https://c.d2l.ai/stanford-cs329p/">course</a> and <a href="https://www.d2l.ai/">book</a></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-lin2014networknetwork" class="csl-entry">
Lin, Min, Qiang Chen, and Shuicheng Yan. 2014. <span>“Network in Network.”</span> <a href="https://arxiv.org/abs/1312.4400">https://arxiv.org/abs/1312.4400</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2013/NIN/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
  <link>https://orenbochman.github.io/reviews/2014/Dropout/</link>
  <description><![CDATA[ 




<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="JMLR:v15:srivastava14a">(Srivastava et al. 2014)</span> the authors, present a novel regularization technique for deep neural networks called “dropout.” The key idea behind dropout is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much and significantly reduces overfitting. The authors show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification, and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
<p>The technique had been in use in some earlier works, but this paper popularized it and showed its effectiveness on a wide range of tasks. The idea behind drop out is pretty simple and people have since come up with many variations of it. It has become a standard technique in the deep learning toolbox.</p>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
<p>– <span class="citation" data-cites="JMLR:v15:srivastava14a">(Srivastava et al. 2014)</span></p>
</blockquote>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="JMLR:v15:srivastava14a">(Srivastava et al. 2014)</span> the authors presents a regularization technique called Dropout, aimed at addressing the critical problem of overfitting in deep neural networks (DNNs). Dropout randomly drops units (neurons) during training to prevent co-adaptation of units, which can lead to overfitting. This novel technique is demonstrated to significantly improve the performance of neural networks across a wide range of tasks, including computer vision, speech recognition, and natural language processing.</p>
</section>
<section id="core-idea" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="core-idea">Core Idea</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig1.png" class="img-fluid" width="640"></p>
</div></div><p>Dropout works by randomly removing units from the network during each training iteration. This prevents the network from becoming overly reliant on specific units, thus reducing overfitting. During testing, all units are used, but their weights are scaled to account for the dropout during training. This approximates the averaging of an exponential number of thinned networks that would otherwise be computationally infeasible.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig2.png" class="img-fluid" width="640"></p>
</div></div></section>
<section id="methodology-and-theoretical-motivation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="methodology-and-theoretical-motivation">Methodology and Theoretical Motivation</h2>
<p>Dropout’s theoretical foundation stems from biological principles, specifically the idea of genetic robustness in sexual reproduction. In the analogy, a network’s hidden units act like genes that must learn to function independently of one another, preventing complex co-adaptations that may not generalize well to unseen data. The stochastic nature of dropout introduces noise during training, which acts as a form of model averaging and regularization.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig3.png" class="img-fluid" width="640"></p>
</div></div></section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<ul>
<li>Model Averaging: Dropout enables the training of many subnetworks (thinned networks) simultaneously, which leads to a more robust model.</li>
<li>Regularization Effect: Dropout reduces overfitting more effectively than other methods such as L1/L2 regularization or early stopping.</li>
<li>Efficiency: Dropout provides a computationally feasible approximation of model averaging by scaling weights at test time, as opposed to maintaining an ensemble of networks.</li>
</ul>
</section>
<section id="experimental-results" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results</h2>
<p>The authors demonstrate the efficacy of dropout across several benchmark datasets:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig4.png" class="img-fluid" width="640"></p>
</div></div><ul>
<li>MNIST: Error rates are reduced from 1.60% (standard neural network) to 0.95% using dropout with additional max-norm regularization.</li>
<li>CIFAR-10 and CIFAR-100: Dropout networks outperform previous methods, with an error reduction to 12.6% on CIFAR-10 and 37.2% on CIFAR-100.</li>
<li>TIMIT (Speech Data): Dropout reduces the phone error rate from 23.4% to 21.8%, showing significant improvements over non-dropout models.</li>
<li>ImageNet: Dropout helps achieve state-of-the-art results in image classification tasks, significantly lowering the top-5 error rate.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig5.png" class="img-fluid" width="640"></p>
</div></div></section>
<section id="advantages-of-dropout" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-dropout">Advantages of Dropout</h2>
<ul>
<li>Generality: Dropout works across a variety of architectures, including fully connected, convolutional, and recurrent neural networks.</li>
<li>Ease of Use: Dropout is simple to implement, requiring only one additional hyperparameter (the dropout rate, typically 0.5 for hidden layers).</li>
<li>Compatibility with Other Methods: Dropout can be combined with techniques like unsupervised pretraining, max-norm regularization, and momentum, further improving model performance.</li>
</ul>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<ul>
<li>Increased Training Time: Dropout can significantly slow down training, typically requiring 2-3 times more iterations to converge.</li>
<li>Tuning of Hyperparameters: While simple, the dropout rate must be carefully selected, and higher learning rates and momentum are generally required for optimal performance.</li>
<li>Application-Specific Benefits: Although dropout improves performance in vision and speech recognition tasks, the improvements in certain domains like text classification (e.g., Reuters RCV1 dataset) are less pronounced. <sup>1</sup></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The paper introduces dropout as a powerful and simple regularization technique that significantly reduces overfitting in deep neural networks. Dropout provides a computationally efficient method to approximate model averaging and works across a range of architectures and tasks, achieving state-of-the-art results on several benchmarks. However, it comes at the cost of increased training time, and some tuning is required for optimal performance.</p>
<p>Dropout represents a substantial advancement in neural network training, and its adoption has since become widespread in the deep learning community.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf" class="col-page" style="width:8.5in;height:11in"></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-JMLR:v15:srivastava14a" class="csl-entry">
Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <span>“Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”</span> <em>Journal of Machine Learning Research</em> 15 (56): 1929–58. <a href="http://jmlr.org/papers/v15/srivastava14a.html">http://jmlr.org/papers/v15/srivastava14a.html</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I think this is because if we drop a few words from a sentence a reader can often guess them from the context and redundancy in natural languages.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2014/Dropout/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings</title>
  <link>https://orenbochman.github.io/reviews/2015/sense2vec/</link>
  <description><![CDATA[ 




<p><strong>Sense2Vec</strong> <span class="citation" data-cites="trask2015sense2vecfastaccurate">(Trask, Michalak, and Liu 2015)</span> is an interesting deep learning model based on word2vec that can learn more interesting and detailed word vectors from large corpora. Sense2Vec embeddings are for word senses rather than for tokens.</p>
<p>The shortcoming of word2vec is that it only learns one vector per word, which is not enough to capture the multiple meanings of a word. Sense2Vec addresses this issue by learning multiple embeddings for each word based on supervised disambiguation. This allows a consuming NLP model to select a sense-disambiguated embedding quickly and accurately.</p>
<p>I thought that the next step would be to cluster these embeddings by contexts and thus arrive at a wordsense version - however it turns out this was computationally expensive and difficult to apply in a scalable fashion. The idea of this paper is to use a supervised approach to disambiguate the senses of words. This means that the words need to be tagged using thier part-of-speech (POS) tags or named entity resolution. This approach is faster and more accurate than the clustering approach. However there is a limitation that the supervised labels are required and there can be multiple wordsenses within a signle POS tag.</p>
<p>Abstract</p>
<blockquote class="blockquote">
<p>Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or “senses”. Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8% average error reduction in unlabeled attachment scores across 6 languages.</p>
<p>— <span class="citation" data-cites="trask2015sense2vecfastaccurate">(Trask, Michalak, and Liu 2015)</span></p>
</blockquote>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>The paper <span class="citation" data-cites="trask2015sense2vecfastaccurate">(Trask, Michalak, and Liu 2015)</span> the autors introduced a novel method for word sense disambiguation in neural word embeddings. Traditional word embedding techniques such as Word2Vec represent each word by a single vector, regardless of its multiple meanings or senses, which often leads to ambiguities. This challenge is known as the “superposition” problem, where multiple meanings of a word are combined into a single vector, potentially leading to suboptimal performance in downstream NLP tasks. SENSE2VEC addresses this limitation by generating multiple embeddings for each word, effectively disambiguating different senses.</p>
</section>
<section id="motivation-and-related-work" class="level2">
<h2 class="anchored" data-anchor-id="motivation-and-related-work">Motivation and Related Work</h2>
<p>The motivation behind SENSE2VEC stems from the shortcomings of earlier models like Word2Vec and Wang2Vec. Word2Vec, while highly successful, does not consider the order of words in a sentence and struggles with polysemy—the phenomenon where a single word can have multiple meanings. Wang2Vec improves upon Word2Vec by incorporating word order, making it more suitable for syntactic tasks, yet still relies on single embeddings per word, making it less effective for handling polysemic words.</p>
<p>Other approaches, such as multi-prototype vector-space models by Reisinger and Mooney (2010), attempt to tackle polysemy by clustering the contexts in which a word appears and generating different embeddings for each cluster. However, these methods often require unsupervised clustering, making the process computationally expensive and difficult to apply in a scalable fashion. SENSE2VEC circumvents these challenges by leveraging supervised learning with part-of-speech (POS) tagging or named entity resolution, reducing the computational overhead while providing more accurate disambiguation.</p>
</section>
<section id="the-sense2vec-model" class="level2">
<h2 class="anchored" data-anchor-id="the-sense2vec-model">The SENSE2VEC Model</h2>
<p>The core innovation of SENSE2VEC lies in its use of supervised labels, such as POS tags, to disambiguate the senses of words. Unlike previous models that rely on unsupervised clustering methods, SENSE2VEC uses these labels to generate separate embeddings for each sense of a word. For example, the word “bank” can have distinct embeddings for its noun and verb forms.</p>
<p>The model can be trained using traditional methods like Continuous Bag of Words (CBOW) or Skip-gram, with a key difference: instead of predicting words given surrounding words, SENSE2VEC predicts word senses given surrounding senses. This approach leads to more accurate representations of polysemic words in context, reducing the negative impact of superposition.</p>
</section>
<section id="evaluation-and-results" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-and-results">Evaluation and Results</h2>
<p>The authors evaluated SENSE2VEC on various NLP tasks, including dependency parsing and named entity resolution (NER). Notably, the disambiguation achieved by SENSE2VEC led to significant improvements in performance. For example, in dependency parsing across six languages (Bulgarian, German, English, French, Italian, and Swedish), SENSE2VEC embeddings resulted in an average error reduction of over 8% compared to the Wang2Vec baseline.</p>
<p>The model also demonstrated its effectiveness in handling sentiment disambiguation, as shown in Table 5 of the paper, where the word “bad” was successfully disambiguated into a negative and positive sentiment sense. The positive sense captured sarcastic uses of “bad,” which is often interpreted as “good” in informal language, while the negative sense retained the more classical meaning of “bad.”</p>
</section>
<section id="strengths-and-contributions" class="level2">
<h2 class="anchored" data-anchor-id="strengths-and-contributions">Strengths and Contributions</h2>
<p>SENSE2VEC’s strengths lie in its efficiency and accuracy. By utilizing supervised labels, the model eliminates the need for expensive clustering algorithms, making it both faster and easier to scale. The ability to disambiguate nuanced senses of words, such as sentiment and named entities, showcases the flexibility and robustness of the approach.</p>
<p>Additionally, the model’s performance improvements in downstream tasks like dependency parsing and NER demonstrate its practical applicability in real-world NLP systems. The fact that SENSE2VEC outperforms earlier models like Wang2Vec and other clustering-based approaches highlights its contribution to the field of word sense disambiguation.</p>
</section>
<section id="limitations-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-future-work">Limitations and Future Work</h2>
<p>One potential limitation of SENSE2VEC is its reliance on labeled data. While supervised learning offers many advantages in terms of accuracy, it also introduces a dependency on the availability of high-quality labels. For languages or domains where such labels are scarce, applying SENSE2VEC may be more challenging.</p>
<p>The authors acknowledge this limitation and suggest that future work could explore the use of other types of supervised labels or investigate ways to combine both supervised and unsupervised methods to further enhance word sense disambiguation.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Overall, SENSE2VEC presents a compelling and efficient solution to the problem of word sense disambiguation in neural word embeddings. By leveraging supervised NLP labels, the model significantly improves the accuracy of embeddings for polysemic words, leading to better performance in NLP tasks like dependency parsing and NER. Its contribution to the field is clear, and it paves the way for future advancements in sense-disambiguated word embeddings.</p>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also:</h2>
<ul>
<li><a href="https://arxiv.org/abs/1511.06388">paper</a></li>
<li><a href="https://github.com/explosion/sense2vec">code</a> by Explosion AI</li>
<li><a href="https://demos.explosion.ai/sense2vec">demo</a></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="./paper.pdf" class="col-page" width="800" height="1000"></p>
<figcaption>paper</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sense2vec <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Sense2Vec</span>
<span id="cb1-2"></span>
<span id="cb1-3">s2v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Sense2Vec().from_disk(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/path/to/s2v_reddit_2015_md"</span>)</span>
<span id="cb1-4">query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"natural_language_processing|NOUN"</span></span>
<span id="cb1-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> query <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> s2v</span>
<span id="cb1-6">vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v[query]</span>
<span id="cb1-7">freq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v.get_freq(query)</span>
<span id="cb1-8">most_similar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v.most_similar(query, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [('machine_learning|NOUN', 0.8986967),</span></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  ('computer_vision|NOUN', 0.8636297),</span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  ('deep_learning|NOUN', 0.8573361)]</span></span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sense2vec <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Sense2Vec</span>
<span id="cb2-2">s2v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Sense2Vec().from_disk(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./s2v_reddit_2015_md"</span>)</span>
<span id="cb2-3">vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"natural_language_processing|NOUN"</span>]</span>
<span id="cb2-4">most_similar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v.most_similar(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"duck|VERB"</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb2-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(most_similar)</span></code></pre></div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-trask2015sense2vecfastaccurate" class="csl-entry">
Trask, Andrew, Phil Michalak, and John Liu. 2015. <span>“Sense2vec - a Fast and Accurate Method for Word Sense Disambiguation in Neural Word Embeddings.”</span> <a href="https://arxiv.org/abs/1511.06388">https://arxiv.org/abs/1511.06388</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2015/sense2vec/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>ViT — An Image is worth 16x16 words: Transformers for Image Recognition at scale</title>
  <link>https://orenbochman.github.io/reviews/2020/ViT-an-image-is-worth-16x16-words/</link>
  <description><![CDATA[ 




<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</p>
<p>— <span class="citation" data-cites="DBLP:journals/corr/abs-2010-11929">(Dosovitskiy et al. 2020)</span></p>
</blockquote>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also</h2>
<ul>
<li><a href="https://arxiv.org/abs/2010.11929">Paper</a></li>
<li><a href="https://github.com/google-research/vision_transformer">Code - Vision Transformer and MLP-Mixer Architectures</a></li>
<li><a href="https://iclr.cc/virtual/2021/oral/3458">ICLR - Video &amp; Slides</a></li>
<li><a href="https://research.google/pubs/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/">Blog post</a></li>
<li>Third-party reviews:
<ul>
<li><a href="https://www.youtube.com/@YannicKilcher">Review by Yannic Kilcher</a></li>
<li><a href="https://www.youtube.com/watch?v=aD-D8-D-ZyY">Sahil Khose</a></li>
<li><a href="https://www.youtube.com/watch?v=DVoHvmww2lQ">AI Coffee Break with Letitia</a></li>
<li><a href="https://medium.com/@ManishChablani/vision-transformer-vit-an-image-is-worth-16x16-words-transformers-for-image-recognition-at-a4bd5c6f17a7">Manish Chablani — Review</a></li>
</ul></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-DBLP:journals/corr/abs-2010-11929" class="csl-entry">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>CoRR</em> abs/2010.11929. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2020/ViT-an-image-is-worth-16x16-words/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>ConvNeXt - A ConvNet for the 2020</title>
  <link>https://orenbochman.github.io/reviews/2022/A ConvNet for the 2020/</link>
  <description><![CDATA[ 




<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>The “Roaring 20s” of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually “modernize” a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.</p>
<p>in <span class="citation" data-cites="liu2022convnet2020s">(Liu et al. 2022)</span></p>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=OpfxPj2AIo4">Video</a></li>
<li><a href="https://arxiv.org/abs/2201.03545">Paper</a></li>
<li><a href="https://github.com/facebookresearch/ConvNeXt">Official PyTorch implementation of ConvNeXt, from the following paper</a><br>
</li>
<li><a href="https://github.com/AlassaneSakande/A-ConvNet-of-2020s">Code</a></li>
<li><a href="https://www.youtube.com/watch?v=QzCjXqFnWPE">Zhuang Liu official video</a></li>
<li>Third Party Coverage:
<ul>
<li><a href="https://www.youtube.com/watch?v=OpfxPj2AIo4">AI Bites video</a></li>
<li><a href="https://www.youtube.com/watch?v=idiIllIQOfU">Aleksa Gordić - The AI Epiphany</a></li>
<li><a href="https://medium.com/aiguys/a-convnet-for-the-2020s-or-2561c9e946e1">Summary</a></li>
</ul></li>
<li><a href="https://docs.google.com/presentation/d/1J9">Slides</a></li>
<li><a href="https://www.youtube.com/watch?v=JN6H4rQvwgM">Talk</a></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-liu2022convnet2020s" class="csl-entry">
Liu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. <span>“A ConvNet for the 2020s.”</span> <a href="https://arxiv.org/abs/2201.03545">https://arxiv.org/abs/2201.03545</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2022/A ConvNet for the 2020/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review</title>
  <link>https://orenbochman.github.io/reviews/2022/Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review/</link>
  <description><![CDATA[ 







 ]]></description>
  <guid>https://orenbochman.github.io/reviews/2022/Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</title>
  <link>https://orenbochman.github.io/reviews/2024/LLM2Vec/</link>
  <description><![CDATA[ 




<p>In <span class="citation" data-cites="behnamghader2024llm2veclargelanguagemodels">(BehnamGhader et al. 2024)</span> the authors consider using LLMs which are mostly decoder only transformers as text encoders. This allows them to use the LLMs for NLP tasks like chunking, NEW and POS. Recall that T5 <span class="citation" data-cites="raffel2020exploring">(Raffel et al. 2020)</span> can do this is a decoder encode model.</p>
<p>Tricks:</p>
<ol type="1">
<li>enabling bidirectional attention,</li>
<li>masked next token prediction, and</li>
<li>unsupervised contrastive learning.</li>
</ol>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-behnamghader2024llm2veclargelanguagemodels" class="csl-entry">
BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. <span>“LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders.”</span> <a href="https://arxiv.org/abs/2404.05961">https://arxiv.org/abs/2404.05961</a>.
</div>
<div id="ref-raffel2020exploring" class="csl-entry">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. <span>“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.”</span> <em>Journal of Machine Learning Research</em> 21 (140): 1–67.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/LLM2Vec/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Flexora: A Flexible Low-Rank Approach to Learning Graph Representations</title>
  <link>https://orenbochman.github.io/reviews/2024/flexora/</link>
  <description><![CDATA[ 




<p>in <span class="citation" data-cites="wei2024flexoraflexiblelowrank">(Wei et al. 2024)</span></p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-wei2024flexoraflexiblelowrank" class="csl-entry">
Wei, Chenxing, Yao Shu, Ying Tiffany He, and Fei Richard Yu. 2024. <span>“Flexora: Flexible Low Rank Adaptation for Large Language Models.”</span> <a href="https://arxiv.org/abs/2408.10774">https://arxiv.org/abs/2408.10774</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/flexora/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>MambaVision A Hybrid Mamba-Transformer Vision Backbone</title>
  <link>https://orenbochman.github.io/reviews/2024/mamba-vision/</link>
  <description><![CDATA[ 




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In <span class="citation" data-cites="hatamizadeh2024mambavision">(Hatamizadeh and Kautz 2024)</span>, the authors apply the State Space Model (SSM) inherent in recently introduced Mamba architecture, <span class="citation" data-cites="gu2023mamba">(Gu and Dao 2023)</span>, for vision tasks. They point out that prior work on using the Mamba architecture for vision was ill-suited these tasks and propose a remedy in the form of a hybrid Mamba-Transformer architecture which they call MambaVision. Thier experiment show that MambaVision outperforms other vision architectures on ImageNet-1K, MS COCO and ADE20K datasets.</p>
<p>The paper’s main innovation is <mark>more self-attention blocks in the final layers of the transformer which improves the models ability to capture long-range spatial dependencies</mark>.</p>
</section>
<section id="the-problems-with-mamba-for-vision-tasks" class="level2">
<h2 class="anchored" data-anchor-id="the-problems-with-mamba-for-vision-tasks">The problems with Mamba for vision tasks</h2>
<p>A <em>dilettante reader</em> like myself might be interested in the author’s outline of the shortcomings of the Mamba architecture for vision tasks and earlier attempt in <span class="citation" data-cites="zhu2024vision">(Zhu et al. 2024)</span> <em>vision mamba</em> model which directed thier efforts the right direction.</p>
<blockquote class="blockquote">
<p>… the Mamba’s autoregressive formulation, while effective for tasks requiring sequential data processing, faces limitations in computer vision tasks that benefit from a <strong>full receptive field</strong><sup>1</sup>:</p>
<ol type="1">
<li><p>Unlike sequences where order matters, image pixels do not have a sequential dependency in the same way. Instead, spatial relationships are often local and need to be considered in a more parallel and integrated manner. Hence, this results in inefficiency for processing spatial data</p></li>
<li><p>an autoregressive model like Mamba processes data step-by-step, limiting its ability to capture and utilize global context in one forward pass. In contrast, vision tasks often require understanding the global context to make accurate predictions about local regions</p></li>
</ol>
</blockquote>
<blockquote class="blockquote">
<p>Vision Mamba (Vim) and others have proposed modifications such as bidirectional SSMs to address lack of global context and spatial understanding. While bidirectional SSMs have the potential to capture more comprehensive context, they introduce significant latency due to the need to process the entire sequence before making predictions. Additionally, the increased complexity can lead to challenges in training, risk of overfitting, and may not always result in better accuracy. Due to these pitfalls, backbones with Vision Transformer (ViT) and Convolutional Neural Network (CNN) architectures still outperform best Mamba-based vision models on different vision tasks. — <span class="citation" data-cites="hatamizadeh2024mambavision">(Hatamizadeh and Kautz 2024, 2)</span></p>
</blockquote>
<p>To sum all this up - Mamba’s auto regressive nature is well suited to temporal and sequential data like text and speech but is ill suited to handle spatial data like images where order manifests as a hierarchy of spatial neighborhoods which should be processed in parallel. Thus for vision, mamba suffer a loss in the efficiency of the flow of information both locally and globally. As such pre mamba vision models fare better.</p>
<p>The next section outlines the ideas espoused in prior work both pre and post mamba. This section summarizes both the earlier work on computer vision models since the introduction of Transformers and some results since the introduction of the Mamba architecture.</p>
<ul>
<li>Vision Transformer (ViT) <span class="citation" data-cites="dosovitskiy2021imageworth16x16words">(Dosovitskiy et al. 2021)</span> showed that CNNs can be replaced with self-attention, but wasn’t data efficient.</li>
<li>Data-efficient Image Transformer (DeiT) <span class="citation" data-cites="touvron2021training">(Touvron et al. 2021)</span> used distillation to train ViT more efficient.</li>
<li>LeViT model <span class="citation" data-cites="graham2021levit">(Graham et al. 2021)</span> introduced a redesign for MLP and self-attention with a Lenet like pyramid pooling structure.</li>
<li>Cross-covariance Image Transformer (XCiT) <span class="citation" data-cites="ali2021xcit">(Ali et al. 2021)</span> introduced transposed self-attention mechanism more effectively modeling interactions between feature channels.</li>
<li>The Pyramid Vision Transformer (PVT) <span class="citation" data-cites="wang2021pyramid">(Wang et al. 2021)</span> improving efficiency by adopting a hierarchical structure with patch embedding at the start of each stage and spatial dimension reduction.</li>
<li>Swin Transformer <span class="citation" data-cites="liu2021swin">(Liu et al. 2021)</span> used shifted windows to improve the efficiency of self-attention computation.</li>
<li>Twins Transformer <span class="citation" data-cites="chu2021twins">(Chu et al. 2021)</span> featured spatially separable self-attention that significantly enhanced efficiency.</li>
<li>Focal Transformer <span class="citation" data-cites="yang2021focal">(Yang et al. 2021)</span> used a focal mechanism to improve the efficiency of self-attention computation for capturing long-range interactions.</li>
</ul>
</section>
<section id="the-mambavision-architecture---macro" class="level2">
<h2 class="anchored" data-anchor-id="the-mambavision-architecture---macro">3.1 The MambaVision Architecture - Macro</h2>
<p>MambaVision has a hierarchical architecture consisting of 4 different stages. The first two stages consist of CNN-based layers for fast feature extraction at higher input resolutions, while stage 3 and 4 include the proposed MambaVision and Transformer blocks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://orenbochman.github.io/reviews/2024/mamba-vision/fig2.png" class="img-fluid figure-img"></p>
<figcaption>Architecture of hierarchical MambaVision</figcaption>
</figure>
</div>
<p>The first two blocks in stages 1 and 2</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%20z%20=%20GELU(BN(Conv_%7B3%C3%973%7D(z)))%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Az%20=%20BN(Conv_%7B3%C3%973%7D(%5Chat%20z))%20+%20z%0A"></p>
<p>Where GELU is the Gaussian Error Linear Unit activation function, a modern alternative to the rectified linear unit (ReLU) function, and BN is good old batch normalization layer which transforms the inputs to have zero mean and unit variance which speeds up training.</p>
</section>
<section id="the-mambavision-architecture---micro" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-mambavision-architecture---micro">3.2 The MambaVision Architecture - Micro</h2>

<div class="no-row-height column-margin column-container"><div id="fig-micro" class="quarto-float quarto-figure quarto-figure-left anchored" data-group="my-gallery" data-fig-align="left">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-micro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://orenbochman.github.io/reviews/2024/mamba-vision/fig3.png" class="img-fluid quarto-figure quarto-figure-left figure-img" data-group="my-gallery" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-micro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Architecture of MambaVision block
</figcaption>
</figure>
</div></div><p>The authors redesigned the original Mamba mixer to make it more suitable for vision tasks.</p>
<ol type="1">
<li>regular convolution replaces causal convolution</li>
<li>added a symmetric branch without SSM , consisting of an additional convolution and SiLU activation, to compensate for any content lost due to the sequential constraints of SSMs.</li>
<li>These branches are concatenated and project via a final linear layer.</li>
</ol>
<p>This combination ensures that the final feature representation incorporates both the sequential and spatial information, leveraging the strengths of both branches.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AX_1%20&amp;=%20Scan(%CF%83(Conv(Linear(C,%20%5Cfrac%7BC%7D%7B2%7D%20)(X_%7Bin%7D))))%20%5C%5C%0AX_2%20&amp;=%20%CF%83(Conv(Linear(C,%20%5Cfrac%7BC%7D%7B2%7D%20)(X_%7Bin%7D)))%20%5C%5C%0AX_%7Bout%7D%20&amp;=%20Linear(%20%5Cfrac%7BC%7D%7B2%7D%20,%20C)(Concat(X_1,%20X_2))%20%5C%5C%0A%5Cend%7Balign*%7D%0A"></p>
</section>
<section id="ablation-studies" class="level2">
<h2 class="anchored" data-anchor-id="ablation-studies">Ablation Studies</h2>
<p>Section 4 the experiment looks at MambaVision’s performance in image classification as well as other downstream tasks like, object detection, instance segmentation and semantic segmentation tasks. The authors note that the model was equipped with the model with specialized heads for different tasks and required fine tuning the original model. I am a somewhat critical of calling this the performance on downstream tasks when we are talking about models with different layers that were fine tuned using different optimizers on task specific datasets.</p>
<p>The results section outline an <strong>ablation study</strong><sup>2</sup> used to identify the optimal way to integrate the Vision Transformer (ViT) with the Mamba architecture.</p>
<p>As usual, the authors provide a family of models with different sizes to gauge the performance characteristics for scaling the model.</p>
<p>The various models</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://arxiv.org/pdf/2407.08083">paper</a></li>
<li><a href="https://github.com/NVlabs/MambaVision">code</a></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-ali2021xcit" class="csl-entry">
Ali, Alaaeldin, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, et al. 2021. <span>“Xcit: Cross-Covariance Image Transformers.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 20014–27.
</div>
<div id="ref-chu2021twins" class="csl-entry">
Chu, Xiangxiang, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. 2021. <span>“Twins: Revisiting the Design of Spatial Attention in Vision Transformers.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 9355–66.
</div>
<div id="ref-dosovitskiy2021imageworth16x16words" class="csl-entry">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div>
<div id="ref-graham2021levit" class="csl-entry">
Graham, Benjamin, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and Matthijs Douze. 2021. <span>“Levit: A Vision Transformer in Convnet’s Clothing for Faster Inference.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 12259–69.
</div>
<div id="ref-gu2023mamba" class="csl-entry">
Gu, Albert, and Tri Dao. 2023. <span>“Mamba: Linear-Time Sequence Modeling with Selective State Spaces.”</span> <em>arXiv Preprint arXiv:2312.00752</em>.
</div>
<div id="ref-hatamizadeh2024mambavision" class="csl-entry">
Hatamizadeh, Ali, and Jan Kautz. 2024. <span>“MambaVision: A Hybrid Mamba-Transformer Vision Backbone.”</span> <em>arXiv Preprint arXiv:2407.08083</em>.
</div>
<div id="ref-liu2021swin" class="csl-entry">
Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. <span>“Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 10012–22.
</div>
<div id="ref-touvron2021training" class="csl-entry">
Touvron, Hugo, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. 2021. <span>“Training Data-Efficient Image Transformers &amp; Distillation Through Attention.”</span> In <em>International Conference on Machine Learning</em>, 10347–57. PMLR.
</div>
<div id="ref-wang2021pyramid" class="csl-entry">
Wang, Wenhai, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. 2021. <span>“Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 568–78.
</div>
<div id="ref-yang2021focal" class="csl-entry">
Yang, Jianwei, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. 2021. <span>“Focal Attention for Long-Range Interactions in Vision Transformers.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 30008–22.
</div>
<div id="ref-zhu2024vision" class="csl-entry">
Zhu, Lianghui, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. 2024. <span>“Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model.”</span> <em>arXiv Preprint arXiv:2401.09417</em>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>seeing the full picture or at least big parts of it↩︎</p></li>
<li id="fn2"><p>investigating the effects of removing parts of a model↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/mamba-vision/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</title>
  <link>https://orenbochman.github.io/reviews/2024/smaller-weaker-yet-better/</link>
  <description><![CDATA[ 




<p>in <span class="citation" data-cites="bansal2024smallerweakerbettertraining">(Bansal et al. 2024)</span> the authors consider the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. They evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. They then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Their findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.</p>
<p>Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bansal2024smallerweakerbettertraining" class="csl-entry">
Bansal, Hritik, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, and Mehran Kazemi. 2024. <span>“Smaller, Weaker, yet Better: Training LLM Reasoners via Compute-Optimal Sampling.”</span> <a href="https://arxiv.org/abs/2408.16737">https://arxiv.org/abs/2408.16737</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/smaller-weaker-yet-better/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>TheoremLlama An End-To-End Framework to Train a General-Purpose Large Language Model to Become a Lean4 Expert</title>
  <link>https://orenbochman.github.io/reviews/2024/theorem-llama/</link>
  <description><![CDATA[ 




<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. Similar methods have shown promising results in code generation. However, most modern LLMs exhibit suboptimal performance due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data. This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address the challenges, this paper proposes TheoremLlama, an end-to-end framework to train a general-purpose LLM to become a Lean4 expert. This framework encompasses NL-FL aligned dataset generation methods, training approaches for the LLM formal theorem prover, and techniques for LLM Lean4 proof writing. Using the dataset generation method, we provide <a href="https://huggingface.co/datasets/RickyDeSkywalker/OpenBootstrappedTheorem">Open Bootstrapped Theorems</a> (OBT), an NL-FL aligned and bootstrapped dataset. A key innovation in this framework is the NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leveraging the NL reasoning ability of LLMs for formal reasoning. The TheoremLlama framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. We have also open-sourced our model checkpoints and generated dataset1, and will soon make all the code publicly available2.</p>
</blockquote>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p><span class="citation" data-cites="wang2024theoremllamatransforminggeneralpurposellms">(Wang et al. 2024)</span> the authors present an end-to-end framework to train a Large Language Model (LLM) to become a <a href="https://github.com/leanprover/lean4">Lean4</a> expert. <sup>1</sup> <mark>The framework includes NL-FL aligned dataset generation methods, training approaches for the LLM formal theorem prover, and techniques for LLM Lean4 proof writing.</mark> The authors demonstrate the effectiveness of TheoremLlama by achieving cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%.<sup>2</sup></p>
<p>TheoremLlama is a significant step towards using LLMs’ natural language abilities to formalize theorem proving in Lean4, improving mathematical reasoning, and tackling major issues with data alignment and training approaches.</p>
<p>However, <mark>the lack of aligned NL and Formal Language (FL) theorem-proving data frequently makes it difficult for contemporary LLMs to operate efficiently.</mark> The lack of available resources impedes the advancement of efficient training approaches and strategies to fully utilize LLMs’ potential in creating formal mathematical proofs. In order to overcome these limitations, a team of researchers from The Hong Kong University of Science and Technology and the University of Illinois Urban-Champagin has introduced TheoremLlama, an end-to-end framework created to specialize a general-purpose LLM in Lean4 theorem proving.</p>
<p>TheoremLlama is made up of various important parts, which are as follows:</p>
<ul>
<li><p>NL-FL Aligned Dataset Generation: TheoremLlama presents techniques for creating an NL-FL-aligned dataset in order to get over data shortage. This dataset, called Open Bootstrapped Theorems (OBT), uses a bootstrapping technique to include NL proofs into Lean4 code. By integrating NL reasoning into Lean4 scenarios, the framework improves LLMs’ comprehension and execution of formal reasoning.</p></li>
<li><p>Formal Training for LLM Theorem Provers: The system applies new training strategies to help LLMs become successful Lean4 theorem provers. Methods like block training and curriculum data sorting have been utilized to enhance the LLM’s in-context learning and guarantee reliable training on the OBT dataset. LLM Lean4 Proof Writing: This part is about improving the LLM’s capacity to write formal proofs in Lean4 on its own. The LLM refines its formal reasoning abilities iteratively by using correctly generated proofs as examples.</p></li>
<li><p>TheoremLlama’s NL-FL bootstrapping approach is a significant invention that enables efficient training by coordinating natural language reasoning with formal mathematical language constraints. The framework’s efficiency has been demonstrated by experimental findings, which on the MiniF2F-Valid and Test datasets, respectively, yielded cumulative accuracies of 36.48% and 33.61%. These outcomes outperformed GPT-4’s baseline findings, which on the same datasets yielded accuracies of 22.95% and 25.41%.</p></li>
</ul>
<p>In conclusion, TheoremLlama is an important step towards using LLMs’ natural language abilities to formalize theorem proving in Lean4, improving mathematical reasoning, and tackling major issues with data alignment and training approaches</p>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also:</h2>
<ul>
<li><a href="https://arxiv.org/abs/2407.03203">paper</a></li>
<li>https://www.marktechpost.com/2024/07/10/theoremllama-an-end-to-end-framework-to-train-a-general-purpose-large-language-model-to-become-a-lean4-expert/</li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-wang2024theoremllamatransforminggeneralpurposellms" class="csl-entry">
Wang, Ruida, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, and Tong Zhang. 2024. <span>“TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts.”</span> <a href="https://arxiv.org/abs/2407.03203">https://arxiv.org/abs/2407.03203</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Note: lean has a serious learning curve and is used in many current mathematics research projects.↩︎</p></li>
<li id="fn2"><p>But is this sufficient to make a difference in the real world? GPT-4 isn’t expected to do well on lean…. ↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/theorem-llama/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters</title>
  <link>https://orenbochman.github.io/reviews/2024/tree-attention-topology-aware-decoding/</link>
  <description><![CDATA[ 




<p>in <span class="citation" data-cites="shyam2024treeattentiontopologyawaredecoding">(Shyam et al. 2024)</span> the authors</p>
<p>Self-attention is the core mathematical operation of modern transformer architectures and is also a significant computational bottleneck due to its quadratic complexity in the sequence length. In this work, we derive the scalar energy function whose gradient computes the self-attention block, thus elucidating the theoretical underpinnings of self-attention, providing a Bayesian interpretation of the operation and linking it closely with energy-based models such as Hopfield Networks. Our formulation reveals that the reduction across the sequence axis can be efficiently computed in parallel through a tree reduction. Our algorithm, for parallelizing attention computation across multiple GPUs enables cross-device decoding to be performed asymptotically faster (up to 8× faster in our experiments) than alternative approaches such as Ring Attention, while also requiring significantly less communication volume and incurring 2× less peak memory.</p>
<p><a href="https://github.com/Zyphra/tree_attention">code</a></p>
<ol type="1">
<li>Introduction</li>
</ol>
<p>The self-attention operation is the core computational building block of the transformer architecture [1, 2], which has become an ubiquitous and highly effective workhorse architecture currently applied at scale to language [3–7], vision [8], audio [9], and decision-making [10, 11]. Nonetheless, the quadratic time complexity of self-attention means that significant resources are required to train and generate from transformer-based Large Language Models (LLMs), especially for models with large context lengths.</p>
<p>During inference, the attention block largely determines the computational and memory requirements, which become more demanding as the input sequence length increases. Although LLMs generate one token at a time, the entire sequence of past tokens must still be stored in memory and used to compute attention scores during generation. Since attention performs a similarity matching of every token representation with every other, it incurs quadratic computational complexity in terms of flops.</p>
<p>There have been recent advances in training LLMs to handle extremely long contexts (up to 1M tokens)[12–14].Such models attain qualitatively new capabilities such as extremely large-scale in-context learning of entire small datasets held in the prompt [15–17]. They can also avoid putting multi-modal continuous data through a lossy tokenization scheme [15, 18] by directly operating at the byte level [19, 20]. The issue however is that performing inference on such long contexts is very expensive.</p>
<p>To speed up inference and alleviate memory requirements, recent works have attempted to alter the attention mechanism itself, either by linearizing it [21], or approximating it by a kernel map [22–24], which reduces the complexity to linear at the cost of reduced expressiveness.</p>
<p>Correspondence to: vasu@zyphra.com, jonathan@zyphra.com Others have invented alternative sequence mixing archi- tectures such as state-space models which are designed to be efficiently computable in linear time and constant memory [25–29]. Other approaches utilize efficient algo- rithms to reduce the computational burden of attention while keeping the core computation the same. These in- clude memory-efficient attention [30], Flash Attention [31] and Flash Decoding [32], which provide a set of IO- aware kernels to map the attention operation to the GPU hardware resources in an extremely efficient way, signifi- cantly reducing the memory overhead required. Further works [33–36] explore compressing or otherwise reduc- ing the KV cache required in generation. Finally, Ring Attention [37] proposes a way to parallelize the atten- tion computation across the sequence axis between GPUs, thus enabling significantly longer contexts than can be served on a single GPU. This is the regime of primary interest of this paper. By leveraging the exact energy function for the self-attention block, we develop a method to speed up inference for long context use-cases when keys and values are sharded across multiple GPUs along the sequence axis.</p>
<p>Our proposed algorithm for computing attention via the gradient of the energy function is built on top of an efficient parallel computation and tree reduction commu- nication strategy. In particular, this formulation lets us devise an asymptotically faster algorithm for performing decoding in which the number of communication steps scales logarithmically with the number of devices, instead of linearly in alternatives such as Ring Attention [37]. Our topology-aware approach illustrated in Fig. 1 sig- nificantly outperforms leading attention parallelization methods such as Ring Attention on multiple devices. In this work, we make three core contributions: •We provide a mathematical form for the energy function of self-attention.</p>
<p>•From this theory, we develop an algorithm for par- allelizing the attention computation across devices, leveraging tree-reduction topology. arXiv:2408.04093v3 [cs.LG] 14 Aug 2024</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-shyam2024treeattentiontopologyawaredecoding" class="csl-entry">
Shyam, Vasudev, Jonathan Pilault, Emily Shepperd, Quentin Anthony, and Beren Millidge. 2024. <span>“Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters.”</span> <a href="https://arxiv.org/abs/2408.04093">https://arxiv.org/abs/2408.04093</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/tree-attention-topology-aware-decoding/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>2BP: 2-Stage Backpropagation</title>
  <link>https://orenbochman.github.io/reviews/2024/two-stage-backpropagation/</link>
  <description><![CDATA[ 




<p>in <span class="citation" data-cites="shyam2024treeattentiontopologyawaredecoding">(Shyam et al. 2024)</span> the authors …</p>
<p>As Deep Neural Networks (DNNs) grow in size and complexity, they often exceed the memory capacity of a single accelerator, necessitating the sharding of model parameters across multiple accelerators. Pipeline parallelism is a commonly used sharding strategy for training large DNNs. However, current implementations of pipeline parallelism are being unintentionally bottlenecked by the automatic differentiation tools provided by ML frameworks. This paper introduces 2-stage backpropagation (2BP). By splitting the backward propagation step into two separate stages, we can reduce idle compute time. We tested 2BP on various model architectures and pipelining schedules, achieving increases in throughput in all cases. Using 2BP, we were able to achieve a 1.70x increase in throughput compared to traditional methods when training a LLaMa-like transformer with 7 billion parameters across 4 GPUs.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-shyam2024treeattentiontopologyawaredecoding" class="csl-entry">
Shyam, Vasudev, Jonathan Pilault, Emily Shepperd, Quentin Anthony, and Beren Millidge. 2024. <span>“Tree Attention: Topology-Aware Decoding for Long-Context Attention on GPU Clusters.”</span> <a href="https://arxiv.org/abs/2408.04093">https://arxiv.org/abs/2408.04093</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/two-stage-backpropagation/</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Why bother reviewing papers?</title>
  <link>https://orenbochman.github.io/reviews/meta/meta.html</link>
  <description><![CDATA[ 




<p>Why bother reviewing papers?</p>
<section id="ground-breaking-v.s.-record-breaking" class="level2">
<h2 class="anchored" data-anchor-id="ground-breaking-v.s.-record-breaking">Ground Breaking v.s. Record Breaking</h2>
<p>While all papers should have break new ground, I follow <span class="citation" data-cites="Kuhn1962">(Kuhn 1962)</span> in differentiating between what in … considered a jump and a gradual improvement. Most papers that announce a new SOTA results are gradual improvements - essentially tweaks to well understood experiments while the truly ground-breaking papers are rare. Often, we see the authors taking an innovation from another field (perhaps a theoretical result) and applying it to a new domain. Less frequently, the author connects several previously unrelated results or techniques and does something considered impossible. Finally, there are papers where the authors ignore prior art and develop several original ideas, creating a new field. (Like Nash did with non-cooperative game theory)</p>
<p>They introduce new capabilities that aren’t widely used. e.g.&nbsp;in<span class="citation" data-cites="arora2018linearalgebraicstructureword">(Arora et al. 2018)</span> the authors introduce sparse coding to embeddings. However, research from information science indicates that it can take a few years for results to circulate, become cited, and adopted. This paper often seems irrelevant later on when there is new research that can get more powerful results. Record-breaking papers (most common) beat the SOTAs by a few fractions of a point and typically arrive in black-box models, and their ‘black magic’ is neither fully understood nor transferable and hard to apply. So, going back to the paper that broke the ground, I find it interesting.</p>
</section>
<section id="learning-to-think-like-a-researcher" class="level2">
<h2 class="anchored" data-anchor-id="learning-to-think-like-a-researcher">Learning to think like a researcher</h2>
<p>The way top-tier researchers like Christopher Manning, Geoffrey Hinton, Christopher Bishop and David MacKay came up with their breakthroughs is frequently motivated and outlined in their papers. These can be as simple as making a PCA that works on Neural Networks for TSNE <sup>1</sup>. In <span class="citation" data-cites="pennington-etal-2014-glove">(Pennington, Socher, and Manning 2014)</span> such as getting embedding to work using a covariance matrix rather than a moving window. Or as complicated as introducing causal regret for credit assignment for agents solving social dilemmas. In many ways, the intellectual journey is more fascinating than any specific magic trick.</p>
<p>Reviewing paper it not as good as taking a class with them. If you can do that. However, but teaching is often not the forte for most gifted researchers - sometimes their finest hours are when writing papers. Another point is that in the class teachers are very much limited by the material they can present - the students can only cover so much new mathematics in a class. In a paper they are writing to their peers so that all bets are off and the material can be as advanced as needed and touch on many disparate fields. In Skrym’s book on the evolution of signaling systems routinely touch on game theory, evolution, information theory, sociology, and classical philosophy. The reader left to catch up on their own if they can and to dive into the literature if they are dissatisfied with some of the the author’s claim. but most courses are not as advanced as most papers. I took courses by Christopher Manning or Geoffrey Hinton. However And yet, most of their publications are just as useful to review. It is not enough to read them if you want to assimilate some of their creativity or problem solving approaches. away in the sense that they provide a unique insight into the workings of their minds. It’s fascinating and inspiring to see how these influential figures think about problems and how they approach them. Their papers offer a valuable perspective that can’t be found elsewhere, and this understanding can be a great source of motivation for your own research. What I like about these two authors is that they can take some old ideas/techniques and figure out how to use them in new settings. T-SNE came from PCA, and GLOVE came from Topic Modelling.</p>
<p>I won’t say that ground-breaking papers are easy to read—yet some are written with great clarity, and others, like the LSTM papers, are notoriously difficult to understand. But you may discover that reading through the paper beats watching videos or blog posts by others.</p>
<p>Some points to consider when reviewing the paper.</p>
<ol type="1">
<li>the big picture</li>
<li>what is the main innovation</li>
<li>what is new for you as a reader</li>
<li>anything you feel left out.</li>
<li>anything you disagree with or would have done differently.</li>
</ol>
</section>
<section id="literature-reviews" class="level2">
<h2 class="anchored" data-anchor-id="literature-reviews">Literature Reviews</h2>
<p>Reading some papers in fast-moving areas like ML or deep learning can provide a good overview of recent developments. These tell how the field has changed and delineate the landmark approaches and the papers in which they arrived.</p>
</section>
<section id="more-techniques" class="level2">
<h2 class="anchored" data-anchor-id="more-techniques">More Techniques</h2>
<p>Some talented authors come with diverse backgrounds. They will list many fascinating ideas, algorithms and techniques. Taking a few minutes to check these out Another interesting aspect of many papers is use of algorithms or techniques that I am unfamiliar with.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-arora2018linearalgebraicstructureword" class="csl-entry">
Arora, Sanjeev, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2018. <span>“Linear Algebraic Structure of Word Senses, with Applications to Polysemy.”</span> <a href="https://arxiv.org/abs/1601.03764">https://arxiv.org/abs/1601.03764</a>.
</div>
<div id="ref-Kuhn1962" class="csl-entry">
Kuhn, Thomas S. 1962. <em>The Structure of Scientific Revolutions</em>. Chicago: University of Chicago Press.
</div>
<div id="ref-pennington-etal-2014-glove" class="csl-entry">
Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. <span>“<span>G</span>lo<span>V</span>e: Global Vectors for Word Representation.”</span> In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (<span>EMNLP</span>)</em>, edited by Alessandro Moschitti, Bo Pang, and Walter Daelemans, 1532–43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>you do know how regular PCA work right↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/meta/meta.html</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Flexora – Flexible Low Rank Adaptation for Large Language Models</title>
  <link>https://orenbochman.github.io/reviews/more.html</link>
  <description><![CDATA[ 







 ]]></description>
  <guid>https://orenbochman.github.io/reviews/more.html</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Dependency Parsing using Deep Learning</title>
  <link>https://orenbochman.github.io/reviews/parsing/parsing.html</link>
  <description><![CDATA[ 




<section id="questions" class="level2">
<h2 class="anchored" data-anchor-id="questions">Questions</h2>
<div id="rem-transition-based-approach" class="proof remark">
<p><span class="proof-title"><em>Remark 2</em>. </span>Explina the two approches to parsing</p>
<ol type="1">
<li><p>Constituency Grammar uses phrase structure grammar to organize words into nested constituents.</p></li>
<li><p>Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate)</p></li>
</ol>
<p>Mathematicaly hhis means that there is an preorder relation on the words of a sentence. If needed we add a <mark>fake root element</mark> which makes the relation a tree.</p>
<p>Also from a category theory point of view we are subordinate the the more specific Linguisticaly relations unifing them under the ageis of order to get a more homogenious view of the sentence</p>
<p>Understainng this definition:</p>
<ol type="1">
<li><p>Can there be more than one relation between two words?</p></li>
<li><p>Are there cases where a word has the same relations with multiple parents?</p></li>
<li><p>Can words modify multiple words ? &gt; [[Jack [and Jill]] ascended] here we resolve this issue by</p></li>
<li><p>How do we choose the root of a sentence or decicide to add a fake one?</p></li>
<li></li>
</ol>
</div>
<div id="rem-transition-based-approach" class="proof remark">
<p><span class="proof-title"><em>Remark 2</em>. </span>what is the transition based approach from Nivre 2003</p>
</div>
<div id="rem-key-challenges" class="proof remark">
<p><span class="proof-title"><em>Remark 3</em>. </span></p>
</div>
<p>can we visualize the textual parse tree as a tree</p>
<p>(went (nsubj (Jack) (cc and) (conj Jill)) (prep up (pobj (det the) (hill))))</p>
<p>(S (NP (NP (NNP Jack)) (CC and) (NP (NNP Jill))) (VP (VBD went) (PP (IN up) (NP (DT the) (NN hill)))))</p>
<ol type="1">
<li><p><strong>Learning</strong>: Given a training set D of sentences annotated with dependency graphs, induce a parsing model M that can be used to parse new sentences.ven a training set D of sentences annotated with dependency graphs, induce a parsing model M that can be used to parse new sentences.</p></li>
<li><p><strong>Parsing</strong>: Given a parsing model <img src="https://latex.codecogs.com/png.latex?M"> and a sentence <img src="https://latex.codecogs.com/png.latex?S">, derive the optimal dependency graph <img src="https://latex.codecogs.com/png.latex?D"> for <img src="https://latex.codecogs.com/png.latex?S"> according to <img src="https://latex.codecogs.com/png.latex?M">. :::</p></li>
</ol>


</section>

 ]]></description>
  <guid>https://orenbochman.github.io/reviews/parsing/parsing.html</guid>
  <pubDate>Sat, 14 Sep 2024 10:59:04 GMT</pubDate>
</item>
<item>
  <title>Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries</title>
  <link>https://orenbochman.github.io/reviews/2020/Why Overfitting Isn’t Always Bad/</link>
  <description><![CDATA[ 




<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>This paper, “Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries,” challenges the traditional view that overfitting is inherently detrimental when developing cross-lingual word embeddings (CLWE).</p>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting the training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.computational resources to train.</p>
<p>— <span class="citation" data-cites="Zhang2020XLingEmbedd">(Zhang et al. 2020)</span></p>
</blockquote>
</section>
<section id="key-points" class="level2">
<h2 class="anchored" data-anchor-id="key-points">Key Points:</h2>
<ol type="1">
<li>Traditional Evaluation of CLWE:
<ul>
<li>Cross-lingual word embeddings (CLWE) typically aim to map words from different languages into a shared vector space.</li>
<li>They are commonly evaluated using Bilingual Lexicon Induction (BLI), which tests the model’s ability to translate words based on a set of test words.</li>
</ul></li>
<li>Underfitting in Projection-Based CLWE:
<ul>
<li>Current CLWE methods, particularly linear projection-based ones, underfit the training dictionary. This means they don’t perfectly align all translation pairs in the training data.</li>
<li>The paper argues that this underfitting, while beneficial for BLI test accuracy, can hinder performance on downstream tasks where words from the training dictionary play a critical role.</li>
</ul></li>
<li>Retrofitting Approach:
<ul>
<li>The authors propose retrofitting as a post-processing step to bring training translation pairs closer in the embedding space, essentially overfitting the training dictionary.</li>
<li>This retrofitting involves modifying the embeddings to minimize distances between training translation pairs while retaining as much of the original structure as possible.</li>
</ul></li>
<li>Retrofitting to Synthetic Dictionaries:
<ul>
<li>In addition to the training dictionary, the paper introduces retrofitting to a synthetic dictionary induced from the original CLWE using the Cross-Domain Similarity Local Scaling (CSLS) heuristic.</li>
<li>This helps balance the need to fit the training dictionary while maintaining some generalization capability for unseen words.</li>
</ul></li>
</ol>
</section>
<section id="experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results:</h2>
<ol type="1">
<li>BLI Accuracy:
<ul>
<li>Retrofitting the embeddings to the training dictionary results in perfect alignment of training pairs, leading to decreased BLI test accuracy since the embeddings overfit the training data.</li>
<li>However, retrofitting to a synthetic dictionary can achieve a balance, improving BLI test accuracy somewhat while still fitting the training data better.</li>
</ul></li>
<li>Downstream Tasks:
<ul>
<li>The paper evaluates the retrofitted embeddings on two downstream tasks: document classification and dependency parsing.</li>
<li>Despite lower BLI test accuracy, retrofitted embeddings often lead to improved performance on these tasks. This underscores the importance of fully exploiting the training dictionary for downstream performance.</li>
</ul></li>
</ol>
</section>
<section id="main-contributions" class="level2">
<h2 class="anchored" data-anchor-id="main-contributions">Main Contributions:</h2>
<ol type="1">
<li>Challenge to BLI as a Sole Metric:
<ul>
<li>The paper argues that BLI accuracy does not always correlate with downstream task performance, revealing the limitations of relying solely on BLI for evaluating CLWE.</li>
</ul></li>
<li>Retrofitting as a Beneficial Overfitting:
<ul>
<li>It shows that overfitting to the training dictionary through retrofitting can be beneficial, enhancing the performance of downstream tasks even if it harms BLI test accuracy.</li>
</ul></li>
<li>Synthetic Dictionary for Balance:
<ul>
<li>Introducing the use of a synthetic dictionary for retrofitting provides a middle ground, balancing the need to fit the training dictionary while retaining some generalization ability.</li>
</ul></li>
</ol>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion:</h2>
<p>The authors suggest that the overemphasis on BLI as an evaluation metric for CLWE should be reconsidered, advocating instead for a focus on downstream tasks that better reflect the utility of the embeddings. They propose that future work might explore more complex non-linear projections to better fit the dictionary without compromising on generalization.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion:</h2>
<p>The paper’s main takeaway is that overfitting to the training dictionary via retrofitting is not inherently harmful. In fact, it can lead to better performance on downstream NLP tasks. This insight invites a reconsideration of the evaluation metrics for cross-lingual word embeddings and opens the door for future work on more sophisticated retrofitting and projection methods.</p>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also</h2>
<ul>
<li><a href="http://users.umiacs.umd.edu/~jbg/docs/2020_acl_refine.pdf">Paper</a></li>
<li><a href="https://www.youtube.com/watch?v=yVN47wGkCko">Video</a></li>
<li><a href="https://github.com/zhangmozhi/retrofit_clwe">Code</a></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-Zhang2020XLingEmbedd" class="csl-entry">
Zhang, Mozhi, Yoshinari Fujinuma, Michael J. Paul, and Jordan Boyd-Graber. 2020. <span>“Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries.”</span> In <em>Association for Computational Linguistics</em>. The Cyberverse Simulacrum of Seattle. <a href="http://umiacs.umd.edu/~jbg//docs/2020_acl_refine.pdf">http://umiacs.umd.edu/~jbg//docs/2020_acl_refine.pdf</a>.
</div>
</div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2020/Why Overfitting Isn’t Always Bad/</guid>
  <pubDate>Tue, 11 Jun 2024 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
