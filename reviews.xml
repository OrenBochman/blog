<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Oren Bochman&#39;s Blog</title>
<link>https://orenbochman.github.io/reviews.html</link>
<atom:link href="https://orenbochman.github.io/reviews.xml" rel="self" type="application/rss+xml"/>
<description>Personal website, portfolio and blog</description>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Thu, 02 Jan 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>On Learning To Become a Successful Loser</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/</link>
  <description><![CDATA[ 





<section id="on-learning-to-become-a-successful-loser-a-comparison-of-alternative-abstractions-of-learning-processes-in-the-loss-domain" class="level1 page-columns page-full">
<h1>On Learning To Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain</h1>
<p>I tracked this paper due to it being highlighted in <span class="citation" data-cites="Skyrms2010signals">(Skyrms 2010)</span> as the source of a model that learns a signaling systems faster. I got me started with the loss domain. I was eventually able to find how to speed up learning by in the Lewis signaling game by considering the much more likely mistakes. Once I got on this algorithm I was thinking that using this idea for Bayesian updating of beliefs. This eventually led me to a second algorithm that was able to rapidly adjust a belief regarding the the state of the world in lewis signaling game with changing distributions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Skyrms2010signals" class="csl-entry">
Skyrms, Brian. 2010. <span>“<span class="nocase">Signals: Evolution, Learning, and Information</span>.”</span> In. Oxford University Press. <a href="https://doi.org/10.1093/acprof:oso/9780199580828.003.0013">https://doi.org/10.1093/acprof:oso/9780199580828.003.0013</a>.
</div></div><div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../../../images/in_the_nut_shell_coach_retouched.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="On Learning to become a successful loser in a nutshell"><img src="https://orenbochman.github.io/images/in_the_nut_shell_coach_retouched.jpg" class="img-fluid figure-img" alt="On Learning to become a successful loser in a nutshell"></a></p>
<figcaption>On Learning to become a successful loser in a nutshell</figcaption>
</figure>
</div>
<p>Besides having this amazing title this research paper compares five mathematical models that predict student behavior in repeated decision-making tasks involving gains and losses. <mark>The core issue is how to accurately represent the effect of losses on learning, as observed deviations from expected utility theory exist</mark> They conducted an experiment and find that learning in the loss domain can be faster than in the gain domain. <mark>The main results suggest that adding a constant to the payoff matrix can accelerate the learning process</mark>, supporting the adjustable reference point (ARP) abstraction of the effect of losses proposed by <span class="citation" data-cites="Roth1995Learning">(Roth and Erev 1995)</span>.</p>
<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<div class="no-row-height column-margin column-container"></div><section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>One of the main difficulties in the development of descriptive models of learning in repeated choice tasks involves the abstraction of the effect of losses. The present paper explains this difficulty, summarizes its common solutions, and presents an experiment that was designed to compare the descriptive power of the specific quantifications of these solutions proposed in recent research. The experiment utilized a probability learning task. In each of the experiment’s 500 trials participants were asked to predict the appearance of one of two colors. The probabilities of appearance of the colors were different but fixed during the entire experiment. The experimental manipulation involved an addition of a constant to the payoffs. The results demonstrate that learning in the loss domain can be faster than learning in the gain domain; adding a constant to the payoff matrix can affect the learning process. These results are consistent with by <span class="citation" data-cites="Roth1995Learning">(Roth and Erev 1995)</span> adjustable reference point abstraction of the effect of losses, and violate all other models</p>
<p>— <span class="citation" data-cites="bereby1998learning">(Bereby-Meyer and Erev 1998)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-Roth1995Learning" class="csl-entry">
Roth, Alvin, and Ido Erev. 1995. <span>“Learning in Extensive-Form Games: Experimental Data and Simple Dynamic Models in the Intermediate Term.”</span> <em>Games and Economic Behavior</em> 8 (1): 164–212. <a href="https://EconPapers.repec.org/RePEc:eee:gamebe:v:8:y:1995:i:1:p:164-212">https://EconPapers.repec.org/RePEc:eee:gamebe:v:8:y:1995:i:1:p:164-212</a>.
</div><div id="ref-bereby1998learning" class="csl-entry">
Bereby-Meyer, Yoella, and Ido Erev. 1998. <span>“On Learning to Become a Successful Loser: A Comparison of Alternative Abstractions of Learning Processes in the Loss Domain.”</span> <em>Journal of Mathematical Psychology</em> 42 (2-3): 266–86.
</div></div></div>
</section>
<section id="outline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="outline">Outline:</h2>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>Highlights the difficulty in developing descriptive models of learning in repeated choice tasks that involve potential losses.</li>
<li>Presents the main goal of the paper: to compare the descriptive power of five distinct solutions to this difficulty and to identify a robust approximation of learning in simple decision tasks.</li>
</ul>
</section>
<section id="the-challenge-and-alternative-solutions" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-challenge-and-alternative-solutions">The Challenge and Alternative Solutions</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="table-01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Models and Solutions"><img src="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/table-01.png" class="img-fluid figure-img" alt="Models and Solutions"></a></p>
<figcaption>Models and Solutions</figcaption>
</figure>
</div></div><ul>
<li>Discusses the difficulty in abstracting the effect of losses given the approximately linear relationship between choice probabilities and the ratio of accumulated payoffs observed in the gain domain.</li>
<li>Introduces probability learning tasks used to derive and compare the predictions of different models.</li>
<li>Presents five alternative solutions to the problem:
<ul>
<li>Low Reference Point (LRP):
<ul>
<li>Transforms objective payoffs into non-negative rewards by subtracting the worst possible outcome</li>
</ul></li>
<li>Adjustable Reference Point and Truncation (ARP)
<ul>
<li>Uses an evolving reference point to distinguish gains and losses and truncates negative values to ensure positive propensities</li>
</ul></li>
<li>Exponential Response Rule (EDS, EFP, EWA)
<ul>
<li>Applies an exponential function to propensities, eliminating the need for handling negative values directly. Examples include Exponential Discounted Sum (EDS), Exponential Fictitious Play (EFP), and Experience Weighted Attractions (EWA) models</li>
</ul></li>
<li>Cumulative Normal Response Rule (CNFP)
<ul>
<li>Uses a cumulative normal distribution to model the relationship between payoffs and propensities- Employs the cumulative normal distribution function to map propensities (which can be negative) to choice probabilities (which are always between 0 and 1). The CNFP model exemplifies this.</li>
</ul></li>
<li>Relative Reinforcement solutions (CLO)
<ul>
<li>Uses outcome-specific parameters to determine the impact of different outcomes on choice probabilities. The Cardinal Linear Operator (CLO) model demonstrates this.</li>
</ul></li>
</ul></li>
<li>Describes the specific implementations of these solutions through different models, including their assumptions and parameterizations.</li>
</ul>
</section>
<section id="experiment" class="level3">
<h3 class="anchored" data-anchor-id="experiment">Experiment</h3>
<ul>
<li>Describes the experimental method, including the participants, the procedure, and the payoff structure of the probability learning task across three conditions.</li>
</ul>
</section>
<section id="results" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="results">Results</h3>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="table-01.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Models and Solutions"><img src="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/table-01.png" class="img-fluid figure-img" alt="Models and Solutions"></a></p>
<figcaption>Models and Solutions</figcaption>
</figure>
</div></div><ul>
<li>Presents the aggregated experimental results, showing a significant effect of the reward condition on the proportion of optimal choices.</li>
<li>Compares the quantitative predictive and descriptive power of the models using correlation and mean squared deviation (MSD) measures.</li>
<li>Discusses the between-subject variability observed in the data and the limitations of the models in capturing this variability.</li>
<li>Conducts a model-based analysis to evaluate the robustness of the condition effect.</li>
<li>Performs a sensitivity analysis to assess the robustness of the ARP model’s predictions to changes in parameter values.</li>
</ul>
</section>
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">Discussion</h3>
<ul>
<li>Discusses the main finding that the addition of constants to payoffs affects the speed of learning, highlighting the role of the distinction between gains and losses.</li>
<li>Notes the advantage of the ARP model in capturing the observed results and acknowledges the potential validity of other solutions under specific assumptions or parameterizations.</li>
<li>Addresses the generality of the findings by discussing:
<ul>
<li>Settings where the ARP model’s predictions are consistent with previous research (probability learning, signal detection).</li>
<li>Settings where the model might fail (learning among only positive outcomes, influence of other players’ payoffs).</li>
</ul></li>
</ul>
</section>
<section id="conclusions" class="level3">
<h3 class="anchored" data-anchor-id="conclusions">Conclusions</h3>
<ul>
<li>Concludes that human learning is affected by the distinction between gains and losses.</li>
<li>Emphasizes that modeling this distinction, particularly through the adjustable reference point approach, improves the descriptive power of adaptive learning models.</li>
<li>Acknowledges the need for further research to refine the quantification of the reference point for a more accurate and generalizable model.</li>
</ul>
</section>
</section>
<section id="key-takeaways" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ul>
<li>In most RL settings rewards are sparse. One way to speed up learning is to try and increase our reward signal.</li>
<li>This is the basis for seeling out to decompose the reward signal into an internal motivation for the agent and an external motivation for the problem designer.</li>
<li>Another approach though is to consider the loss domain. If we can get signals out of losses we can speed up learning and RL agents are slow learners - especially deep RL agents.</li>
<li>A third approach that I was able to make use of is to use both the loss domain and the gain domain to update beliefs about the possible states of the world. This was allowed me to speed up Bayesian learning algorithm for a coordination task.</li>
</ul>
<p>Besides this the paper has a lot of possible options for potential update rules to get this potential speed up.</p>
<ol type="1">
<li>How does adding a constant to all payoffs in a decision task affect learning, and which model best explains this effect?</li>
</ol>
<p>One of the results I learned is that adding a constant to the payoff matrix doesn’t change it. In fact linear transformations of the payoff matrix don’t change the outcomes. In policy gradient methods this we call this trick learning with baselines. What we see is that it doesn’t bias the estimator but can drastically reduce the variance of the estimator. And this variance is the noise that slows down learning by the agent. So adding a constant can surprisingly impact learning speed. The ARP model uniquely predicts this: subtracting a constant to introduce losses speeds learning compared to a purely gain-based scenario. This highlights the psychological impact of the gain-loss framing.</p>
<p>Another insight I had about this is while trying to abstract the RL algorithms. Was that under some conditions we can convert the reward function into a distance metric. Having a metric makes navigation states space much simpler. I really can’t think of a better feature.</p>
<ol start="2" type="1">
<li>What are the limitations of the ARP model?</li>
</ol>
<ul>
<li>The ARP model, with its current parameters, assumes an initial reference point of zero and a slow adjustment process. This might not hold when:
<ul>
<li>All options are positive: The model would predict slow learning even when clear differences exist.</li>
<li>Social comparison exists: People may adjust their reference point based on other players’ payoffs, a factor not currently incorporated in the model.</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li>How would you define the loss domain and the gain domain ?</li>
</ol>
<ul>
<li><p>The gain domain is when choice probabilities are approximately linearly related to the ratio of accumulated reinforcement.</p></li>
<li><p>The loss domain is when negative payoffs are possible.</p></li>
<li><p>In the gain domain, the probabilities of choosing an alternative match the ratio of accumulated reinforcement, meaning that individuals are more likely to choose options that have yielded higher rewards in the past.</p></li>
<li><p>Descriptive models have to assume that choice probabilities are determined by a function of the accumulated reinforcements, which must have strictly positive values. However, this presents a problem when losses are possible because negative payoffs can result in negative values for the function.</p></li>
</ul>
<ol start="4" type="1">
<li>In the paper the autors mention the value function from prospect theory c.f. <span class="citation" data-cites="kahneman1979econ">(Kahneman 1979)</span>. How does this relate to the ARP model?</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-kahneman1979econ" class="csl-entry">
Kahneman, Daniel. 1979. <span>“Econ Ometrica i Ci.”</span> <em>Econometrica</em> 47 (2): 263–91.
</div></div><p>The authors state that models that use solutions other than the adjustable reference point can account for the results of the study under the assumption that the model’s parameters can be affected by the payoffs. One way to account for this is to use reinforcement functions with the characteristics of Prospect Theory’s value function. Prospect theory, developed by Kahneman and Tversky, suggests that individuals make decisions based on the potential value of losses and gains rather than the final outcome, and that losses have a greater impact on individuals than gains do. This relates to the ARP model because it also assumes that reinforcements are evaluated relative to a reference point, meaning outcomes above the reference point are perceived as gains (reinforcements) and outcomes below the reference point are perceived as losses (punishments).</p>
<ol start="5" type="1">
<li>Is there a formal definition of this prospect theoretic value function?</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="value-function.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="a value function"><img src="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/value-function.png" class="img-fluid figure-img" alt="a value function"></a></p>
<figcaption>a value function</figcaption>
</figure>
</div></div><p>A key element of this theory is the value function, which exhibits these characteristics: - It’s defined on deviations from a reference point. - It’s generally concave for gains and convex for losses. - It’s steeper for losses than for gains, meaning an equivalent loss has a greater psychological impact than the corresponding gain.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2025,
  author = {Bochman, Oren},
  title = {On {Learning} {To} {Become} a {Successful} {Loser}},
  date = {2025-01-02},
  url = {https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2025" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2025. <span>“On Learning To Become a Successful
Loser.”</span> January 2, 2025. <a href="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/">https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/</a>.
</div></div></section></div> ]]></description>
  <category>RL</category>
  <category>Reinforcement Learning</category>
  <guid>https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/</guid>
  <pubDate>Thu, 02 Jan 2025 00:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/1998/becoming-a-successful-loser/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Emergent Communication of Generalizations</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>I think this is an amazing paper. I read it critically and made copious notes to see what I could learn from it. The paper point out some limitations of Lewis referential games^[In this game the sender sees images, it needs to classify them into some representation and sends a message. The reciever gets the same or similar images + distractors, it needs to run a classifier and needs to select the correct one. Learning an image classifier per agent is expensive and requires access to both an an image classification is likely shared. This however presents a problem…. It allows the agents? ] and suggest a couple of extentions that can over come these limitations. There is</p>
<p><span class="citation" data-cites="mu2022emergentcomms">(Mu and Goodman 2021)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-mu2022emergentcomms" class="csl-entry">
Mu, Jesse, and Noah D. Goodman. 2021. <span>“Emergent Communication of Generalizations.”</span> <em>CoRR</em> abs/2106.02668. <a href="https://arxiv.org/abs/2106.02668">https://arxiv.org/abs/2106.02668</a>.
</div></div></section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>To build agents that can collaborate effectively with others, recent research has trained artificial agents to communicate with each other in Lewis-style referential games. However, this often leads to successful but uninterpretable communication. We argue that this is due to the game objective: communicating about a single object in a shared visual context is prone to overfitting and does not encourage language useful beyond concrete reference. In contrast, human language conveys a rich variety of abstract ideas. To promote such skills, we propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. We find that these games greatly improve systematicity and interpretability of the learned languages, according to several metrics in the literature. Finally, we propose a method for identifying logical operations embedded in the emergent languages by learning an approximate compositional reconstruction of the language.</p>
</section>
<section id="the-video" class="level2">
<h2 class="anchored" data-anchor-id="the-video">The Video</h2>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/LVW_t7p42X0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<p>Here is the paper with my annotation and highlights.</p>
</section>
<section id="annotations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="annotations">Annotations</h2>

<div class="no-row-height column-margin column-container"><div class="">
<blockquote class="blockquote">
<p>To promote such skills, propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. <sup>1</sup></p>
<div id="fn1"><p><sup>1</sup>&nbsp;Significant modification the game: tweak payoffs, assign categories to symbols and allow sending of categories.</p></div></blockquote>
<blockquote class="blockquote">
<p>We argue that the reference games typically used in these studies are ill-suited to drive linguistic <em>systematicity</em> for two reasons <sup>2</sup></p>
<div id="fn2"><p><sup>2</sup>&nbsp;The best the original Lewis signaling game can do is establish a one to one convention between a sender’s siganal of states and reciever action per states. This is just a coordination part of communication.</p></div></blockquote>
<blockquote class="blockquote">
<p>These tasks are more difficult <sup>3</sup></p>
<div id="fn3"><p><sup>3</sup>&nbsp;adding categories can result in a combinatorial increase the total messages. So that the agents need to coordinate on one of many more equilibria. Also you now want the agents to learn a much narrower subset of those possible equilibrium i.e.&nbsp;those that are are faithfull to certain structures in of the states. This is essentially a new problem which could be embodies as a second step after the Lewis game. There is no guarantee in general that such a structure exists. And as the authors suggest other structures are not considered</p></div></blockquote>
<blockquote class="blockquote">
<p>In the <strong>set reference</strong> (setref) game, a teacher must communicate to a student not just a single object, but rather a group of objects belonging to a concept <sup>4</sup></p>
<div id="fn4"><p><sup>4</sup>&nbsp;this is an interesting game - and also similar to <span class="cn">add reference</span> one modifies the game to refer to set of states by adding and, or and not operators giving an agent an basic reasoning ability. The new signaling systems allows specifying many more states. This can be useful in many applications. Of course learning to send additional operators becomes trivial conceptually if not in the practical sense</p></div></blockquote>
<blockquote class="blockquote">
<p>These tasks are more difficult than traditional reference games <sup>5</sup></p>
<div id="fn5"><p><sup>5</sup>&nbsp;a “concept” like a red triangle is a specific type of a set. so this should be a easier task than the set reference. The difficulty seems to be not in the language or the categories but in the added classification of varied visual representation of seagulls</p></div></blockquote>
</div></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf#page=1" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf#page=1" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
<hr>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf#page=2" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="paper"><embed src="./paper.pdf#page=2" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Emergent {Communication} of {Generalizations}},
  date = {2024-10-09},
  url = {https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Emergent Communication of
Generalizations.”</span> October 9, 2024. <a href="https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/">https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/</a>.
</div></div></section></div> ]]></description>
  <category>signaling games</category>
  <category>stub</category>
  <guid>https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/</guid>
  <pubDate>Wed, 09 Oct 2024 00:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2021/emergent-communication-of-generelization/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Evolutionary dynamics of Lewis signaling games: signaling systems vs. partial pooling</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p><span class="citation" data-cites="huttegger2010evolutionary">(Huttegger et al. 2010)</span></p>
<div class="no-row-height column-margin column-container"><div id="ref-huttegger2010evolutionary" class="csl-entry">
Huttegger, Simon M, Brian Skyrms, Rory Smead, and Kevin JS Zollman. 2010. <span>“Evolutionary Dynamics of Lewis Signaling Games: Signaling Systems Vs. Partial Pooling.”</span> <em>Synthese</em> 172: 177–91.
</div></div></section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>Abstract Transfer of information between senders and receivers, of one kind or another, is essential to all life. David Lewis introduced a game theoretic model of the simplest case, where one sender and one receiver have pure common interest. How hard or easy is it for evolution to achieve information transfer in Lewis signaling?. The answers involve surprising subtleties. We discuss some if these in terms of evolutionary dynamics in both finite and infinite populations, with and without mutation.</p>
</section>
<section id="this-paper" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="this-paper">This paper</h2>
<p>I’ve not looked too deeply into this paper as it needs a deep dive into evolutionary dynamics which I’ve yet to study in depth. My intererst are RL and complex signaling systems. Some of the results in evolutionary dynamics can be directly applied to reinforcement learning.</p>
<p>The paper looks at moran processes and mutations. This is interesting in the open ended setting of language evolution.</p>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Because this is a partnership game, average payoff is a <strong>Lyapunov function</strong> <sup>1</sup> for the system [in fact, it is even a potential function; see <span class="citation" data-cites="hofbauer1998evolutionary">(Hofbauer and Sigmund 1998)</span> ].</p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Lyapunov functions</strong> are scalar functions that may be used to prove the stability of an equilibrium of an ODE</p></div><div id="ref-hofbauer1998evolutionary" class="csl-entry">
Hofbauer, Josef, and Karl Sigmund. 1998. <em>Evolutionary Games and Population Dynamics</em>. Cambridge university press.
</div></div></div>
<p>The look at Lyapunov functions and the replicator dynamics. This is interesting in the context stability of equilibria in signaling games.</p>
<blockquote class="blockquote">
<p>raises the same questions in the context of evolution in finite populations with fixed population size (via the Moran process with and without mutation)</p>
</blockquote>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Evolutionary Dynamics of {Lewis} Signaling Games: Signaling
    Systems Vs. Partial Pooling},
  date = {2024-10-08},
  url = {https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Evolutionary Dynamics of Lewis Signaling
Games: Signaling Systems Vs. Partial Pooling.”</span> October 8, 2024.
<a href="https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/">https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/</a>.
</div></div></section></div> ]]></description>
  <category>signaling games</category>
  <category>evolutionary dynamics</category>
  <category>stub</category>
  <guid>https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/</guid>
  <pubDate>Tue, 08 Oct 2024 00:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2009/HutteggerSkryms2009/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2020/Why Overfitting Isn’t Always Bad/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>This paper, “Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries,” challenges the traditional view that overfitting is inherently detrimental when developing cross-lingual word embeddings (CLWE)<sup>1</sup> and that the evaluation of CLWE using Bilingual Lexicon Induction (BLI) is flawed.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;see my note below regarding this point.</p></div></div><div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cross Language Word Embeddings (CLWE) in a nutshell
</div>
</div>
<div class="callout-body-container callout-body">
<p>Since I have not looked into CLWE before I add this outline on how CLWE are being learned from in the video lined below.</p>
<ol type="1">
<li>embeddings are learned for each language</li>
<li>a Bilingual dictionary provides a mapping from word pairs which is used to tweak the embeddings so they align across languages.</li>
</ol>
</div>
</div>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting the training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.computational resources to train.</p>
<p>— <span class="citation" data-cites="Zhang2020XLingEmbedd">(Zhang et al. 2020)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-Zhang2020XLingEmbedd" class="csl-entry">
Zhang, Mozhi, Yoshinari Fujinuma, Michael J. Paul, and Jordan Boyd-Graber. 2020. <span>“Why Overfitting Isn’t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries.”</span> In <em>Association for Computational Linguistics</em>. The Cyberverse Simulacrum of Seattle. <a href="http://umiacs.umd.edu/~jbg//docs/2020_acl_refine.pdf">http://umiacs.umd.edu/~jbg//docs/2020_acl_refine.pdf</a>.
</div></div></div>
</section>
<section id="key-points" class="level2">
<h2 class="anchored" data-anchor-id="key-points">Key Points:</h2>
<ol type="1">
<li><strong>Traditional Evaluation of CLWE</strong>:
<ul>
<li>Cross-lingual word embeddings (CLWE) typically aim to map words from different languages into a shared vector space.</li>
<li>They are commonly evaluated using Bilingual Lexicon Induction (BLI), which tests the model’s ability to translate words based on a set of test words.</li>
</ul></li>
<li><strong>Underfitting in Projection-Based CLWE</strong>:
<ul>
<li>Current CLWE methods, particularly linear projection-based ones, underfit the training dictionary. This means they don’t perfectly align all translation pairs in the training data.</li>
<li>The paper argues that this underfitting, while beneficial for BLI test accuracy, can hinder performance on downstream tasks where words from the training dictionary play a critical role.</li>
</ul></li>
<li><strong>Retrofitting Approach</strong>:
<ul>
<li>The authors propose retrofitting as a post-processing step to bring training translation pairs closer in the embedding space, essentially overfitting the training dictionary.</li>
<li>This retrofitting involves modifying the embeddings to minimize distances between training translation pairs while retaining as much of the original structure as possible.</li>
</ul></li>
<li>Retrofitting to Synthetic Dictionaries:
<ul>
<li>In addition to the training dictionary, the paper introduces retrofitting to a synthetic dictionary induced from the original CLWE using the Cross-Domain Similarity Local Scaling (CSLS) heuristic.</li>
<li>This helps balance the need to fit the training dictionary while maintaining some generalization capability for unseen words.</li>
</ul></li>
</ol>
</section>
<section id="experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results:</h2>
<ol type="1">
<li><strong>BLI Accuracy</strong>:
<ul>
<li>Retrofitting the embeddings to the training dictionary results in perfect alignment of training pairs, leading to decreased BLI test accuracy since the embeddings overfit the training data.</li>
<li>However, retrofitting to a synthetic dictionary can achieve a balance, improving BLI test accuracy somewhat while still fitting the training data better.</li>
</ul></li>
<li><strong>Downstream Tasks</strong>:
<ul>
<li>The authors evaluates the retrofitted embeddings on two downstream tasks: <strong>document classification</strong> and <strong>dependency parsing</strong>.</li>
<li>Despite lower BLI test accuracy, retrofitted embeddings often lead to improved performance on these tasks. This underscores the importance of fully exploiting the training dictionary for downstream performance.</li>
</ul></li>
</ol>
</section>
<section id="main-contributions" class="level2">
<h2 class="anchored" data-anchor-id="main-contributions">Main Contributions:</h2>
<ol type="1">
<li><strong>Challenge to BLI as a Sole Metric</strong>:
<ul>
<li>The authors argues that BLI accuracy does not always correlate with downstream task performance, revealing the limitations of relying solely on BLI for evaluating CLWE.</li>
</ul></li>
<li><strong>Retrofitting as a Beneficial Overfitting</strong>:
<ul>
<li>It shows that overfitting to the training dictionary through retrofitting can be beneficial, enhancing the performance of downstream tasks even if it harms BLI test accuracy.</li>
</ul></li>
<li>Synthetic Dictionary for Balance:
<ul>
<li>Introducing the use of a synthetic dictionary for retrofitting provides a middle ground, balancing the need to fit the training dictionary while retaining some generalization ability.</li>
</ul></li>
</ol>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion:</h2>
<p>The authors suggest that the overemphasis on BLI as an evaluation metric for CLWE should be reconsidered, advocating instead for a focus on downstream tasks that better reflect the utility of the embeddings. They propose that future work might explore more complex non-linear projections to better fit the dictionary without compromising on generalization.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion:</h2>
<p>The paper’s main takeaway is that overfitting to the training dictionary via retrofitting is not inherently harmful. In fact, it can lead to better performance on downstream NLP tasks. This insight invites a reconsideration of the evaluation metrics for cross-lingual word embeddings and opens the door for future work on more sophisticated retrofitting and projection methods.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
overfitting
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>It is not surprising that the authors ‘discovered’ that fitting the model on <code>test</code> + <code>train</code> + <code>validation</code> gives better results then fitting on <code>train</code> part only. After all the best way to reduce overfitting is to give it more data.</li>
<li>This is a common practice in machine learning to train on the full dataset once we have used to get the best results when the model is deployed to production.</li>
<li>Bayesian LOO Cross validation also allows one to measure the generalization error of the model without having to make the sacrifice of a train/test split.</li>
<li>Considering that the dataset is a dictionary means there is probably little noise to overfit on.</li>
</ul>
<p>On the other hand their criticism of BLI as a metric is valid. There are lots of bad metrics and it is more so when we wish to use it to estimate performance on a different task. In RL one uses importance sampling to make the correction between on policy and off policy - perhaps this is something to look into, though it would be easier to do this if this was a RL problem rather than supervised ML.</p>
<p>It is a common practice in machine learning to use a proxy metric to evaluate the model. The problem is that the proxy metric is not always a good indicator of the model’s performance on the real task. This is why it is important to evaluate the model on the real task as well.</p>
</div>
</div>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also</h2>
<ul>
<li><a href="http://users.umiacs.umd.edu/~jbg/docs/2020_acl_refine.pdf">Paper</a></li>
<li><a href="https://www.youtube.com/watch?v=yVN47wGkCko">Video</a></li>
<li><a href="https://github.com/zhangmozhi/retrofit_clwe">Code</a></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2024,
  author = {Bochman, Oren},
  title = {Why {Overfitting} {Isn’t} {Always} {Bad:} {Retrofitting}
    {Cross-Lingual} {Word} {Embeddings} to {Dictionaries}},
  date = {2024-06-11},
  url = {https://orenbochman.github.io/reviews/2020/Why Overfitting Isn’t Always Bad/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2024" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2024. <span>“Why Overfitting Isn’t Always Bad:
Retrofitting Cross-Lingual Word Embeddings to Dictionaries.”</span> June
11, 2024. <a href="https://orenbochman.github.io/reviews/2020/Why Overfitting Isn’t Always Bad/">https://orenbochman.github.io/reviews/2020/Why
Overfitting Isn’t Always Bad/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2020/Why Overfitting Isn’t Always Bad/</guid>
  <pubDate>Tue, 11 Jun 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2015/sense2vec/</link>
  <description><![CDATA[ 





<p><strong>Sense2Vec</strong> <span class="citation" data-cites="trask2015sense2vecfastaccurate">(Trask, Michalak, and Liu 2015)</span> is an interesting deep learning model based on word2vec that can learn more interesting and detailed word vectors from large corpora. Sense2Vec embeddings are for word senses rather than for tokens.</p>
<div class="no-row-height column-margin column-container"></div><p>The shortcoming of word2vec is that it only learns one vector per word, which is not enough to capture the multiple meanings of a word. Sense2Vec addresses this issue by learning multiple embeddings for each word based on supervised disambiguation. This allows a consuming NLP model to select a sense-disambiguated embedding quickly and accurately.</p>
<p>I thought that the next step would be to cluster these embeddings by contexts and thus arrive at a wordsense version - however it turns out this was computationally expensive and difficult to apply in a scalable fashion. The idea of this paper is to use a supervised approach to disambiguate the senses of words. This means that the words need to be tagged using thier part-of-speech (POS) tags or named entity resolution. This approach is faster and more accurate than the clustering approach. However there is a limitation that the supervised labels are required and there can be multiple wordsenses within a signle POS tag.</p>
<p>Abstract</p>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or “senses”. Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8% average error reduction in unlabeled attachment scores across 6 languages.</p>
<p>— <span class="citation" data-cites="trask2015sense2vecfastaccurate">(Trask, Michalak, and Liu 2015)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>The paper <span class="citation" data-cites="trask2015sense2vecfastaccurate">(Trask, Michalak, and Liu 2015)</span> the autors introduced a novel method for word sense disambiguation in neural word embeddings. Traditional word embedding techniques such as Word2Vec represent each word by a single vector, regardless of its multiple meanings or senses, which often leads to ambiguities. This challenge is known as the “superposition” problem, where multiple meanings of a word are combined into a single vector, potentially leading to suboptimal performance in downstream NLP tasks. SENSE2VEC addresses this limitation by generating multiple embeddings for each word, effectively disambiguating different senses.</p>
<div class="no-row-height column-margin column-container"><div id="ref-trask2015sense2vecfastaccurate" class="csl-entry">
Trask, Andrew, Phil Michalak, and John Liu. 2015. <span>“Sense2vec - a Fast and Accurate Method for Word Sense Disambiguation in Neural Word Embeddings.”</span> <a href="https://arxiv.org/abs/1511.06388">https://arxiv.org/abs/1511.06388</a>.
</div></div></section>
<section id="motivation-and-related-work" class="level2">
<h2 class="anchored" data-anchor-id="motivation-and-related-work">Motivation and Related Work</h2>
<p>The motivation behind SENSE2VEC stems from the shortcomings of earlier models like Word2Vec and Wang2Vec. Word2Vec, while highly successful, does not consider the order of words in a sentence and struggles with polysemy—the phenomenon where a single word can have multiple meanings. Wang2Vec improves upon Word2Vec by incorporating word order, making it more suitable for syntactic tasks, yet still relies on single embeddings per word, making it less effective for handling polysemic words.</p>
<p>Other approaches, such as multi-prototype vector-space models by Reisinger and Mooney (2010), attempt to tackle polysemy by clustering the contexts in which a word appears and generating different embeddings for each cluster. However, these methods often require unsupervised clustering, making the process computationally expensive and difficult to apply in a scalable fashion. SENSE2VEC circumvents these challenges by leveraging supervised learning with part-of-speech (POS) tagging or named entity resolution, reducing the computational overhead while providing more accurate disambiguation.</p>
</section>
<section id="the-sense2vec-model" class="level2">
<h2 class="anchored" data-anchor-id="the-sense2vec-model">The SENSE2VEC Model</h2>
<p>The core innovation of SENSE2VEC lies in its use of supervised labels, such as POS tags, to disambiguate the senses of words. Unlike previous models that rely on unsupervised clustering methods, SENSE2VEC uses these labels to generate separate embeddings for each sense of a word. For example, the word “bank” can have distinct embeddings for its noun and verb forms.</p>
<p>The model can be trained using traditional methods like Continuous Bag of Words (CBOW) or Skip-gram, with a key difference: instead of predicting words given surrounding words, SENSE2VEC predicts word senses given surrounding senses. This approach leads to more accurate representations of polysemic words in context, reducing the negative impact of superposition.</p>
</section>
<section id="evaluation-and-results" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-and-results">Evaluation and Results</h2>
<p>The authors evaluated SENSE2VEC on various NLP tasks, including dependency parsing and named entity resolution (NER). Notably, the disambiguation achieved by SENSE2VEC led to significant improvements in performance. For example, in dependency parsing across six languages (Bulgarian, German, English, French, Italian, and Swedish), SENSE2VEC embeddings resulted in an average error reduction of over 8% compared to the Wang2Vec baseline.</p>
<p>The model also demonstrated its effectiveness in handling sentiment disambiguation, as shown in Table 5 of the paper, where the word “bad” was successfully disambiguated into a negative and positive sentiment sense. The positive sense captured sarcastic uses of “bad,” which is often interpreted as “good” in informal language, while the negative sense retained the more classical meaning of “bad.”</p>
</section>
<section id="strengths-and-contributions" class="level2">
<h2 class="anchored" data-anchor-id="strengths-and-contributions">Strengths and Contributions</h2>
<p>SENSE2VEC’s strengths lie in its efficiency and accuracy. By utilizing supervised labels, the model eliminates the need for expensive clustering algorithms, making it both faster and easier to scale. The ability to disambiguate nuanced senses of words, such as sentiment and named entities, showcases the flexibility and robustness of the approach.</p>
<p>Additionally, the model’s performance improvements in downstream tasks like dependency parsing and NER demonstrate its practical applicability in real-world NLP systems. The fact that SENSE2VEC outperforms earlier models like Wang2Vec and other clustering-based approaches highlights its contribution to the field of word sense disambiguation.</p>
</section>
<section id="limitations-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-future-work">Limitations and Future Work</h2>
<p>One potential limitation of SENSE2VEC is its reliance on labeled data. While supervised learning offers many advantages in terms of accuracy, it also introduces a dependency on the availability of high-quality labels. For languages or domains where such labels are scarce, applying SENSE2VEC may be more challenging.</p>
<p>The authors acknowledge this limitation and suggest that future work could explore the use of other types of supervised labels or investigate ways to combine both supervised and unsupervised methods to further enhance word sense disambiguation.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Overall, SENSE2VEC presents a compelling and efficient solution to the problem of word sense disambiguation in neural word embeddings. By leveraging supervised NLP labels, the model significantly improves the accuracy of embeddings for polysemic words, leading to better performance in NLP tasks like dependency parsing and NER. Its contribution to the field is clear, and it paves the way for future advancements in sense-disambiguated word embeddings.</p>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also:</h2>
<ul>
<li><a href="https://arxiv.org/abs/1511.06388">paper</a></li>
<li><a href="https://github.com/explosion/sense2vec">code</a> by Explosion AI</li>
<li><a href="https://demos.explosion.ai/sense2vec">demo</a></li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sense2vec <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Sense2Vec</span>
<span id="cb1-2"></span>
<span id="cb1-3">s2v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Sense2Vec().from_disk(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/path/to/s2v_reddit_2015_md"</span>)</span>
<span id="cb1-4">query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"natural_language_processing|NOUN"</span></span>
<span id="cb1-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> query <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> s2v</span>
<span id="cb1-6">vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v[query]</span>
<span id="cb1-7">freq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v.get_freq(query)</span>
<span id="cb1-8">most_similar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v.most_similar(query, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [('machine_learning|NOUN', 0.8986967),</span></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  ('computer_vision|NOUN', 0.8636297),</span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  ('deep_learning|NOUN', 0.8573361)]</span></span></code></pre></div>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sense2vec <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Sense2Vec</span>
<span id="cb2-2">s2v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Sense2Vec().from_disk(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./s2v_reddit_2015_md"</span>)</span>
<span id="cb2-3">vector <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"natural_language_processing|NOUN"</span>]</span>
<span id="cb2-4">most_similar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> s2v.most_similar(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"duck|VERB"</span>, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb2-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(most_similar)</span></code></pre></div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Sense2vec - {A} {Fast} and {Accurate} {Method} for {Word}
    {Sense} {Disambiguation} {In} {Neural} {Word} {Embeddings}},
  date = {2022-06-26},
  url = {https://orenbochman.github.io/reviews/2015/sense2vec/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Sense2vec - A Fast and Accurate Method for
Word Sense Disambiguation In Neural Word Embeddings.”</span> June 26,
2022. <a href="https://orenbochman.github.io/reviews/2015/sense2vec/">https://orenbochman.github.io/reviews/2015/sense2vec/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2015/sense2vec/</guid>
  <pubDate>Sun, 26 Jun 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Variational Inference with Normalizing Flows</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="figure 1"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/fig_1.png" class="img-fluid figure-img" width="800" alt="figure 1"></a></p>
<figcaption>figure 1</figcaption>
</figure>
</div>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.</p>
<p>— <span class="citation" data-cites="rezende2016vinflows">(Rezende and Mohamed 2016)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-rezende2016vinflows" class="csl-entry">
Rezende, Danilo Jimenez, and Shakir Mohamed. 2016. <span>“Variational Inference with Normalizing Flows.”</span> <a href="https://arxiv.org/abs/1505.05770">https://arxiv.org/abs/1505.05770</a>.
</div></div></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig_2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="figure 2"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/fig_2.png" class="col-page img-fluid figure-img" width="800" alt="figure 2"></a></p>
<figcaption>figure 2</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./alg_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="algorithm 1"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/alg_1.png" class="col-page img-fluid figure-img" width="800" alt="algorithm 1"></a></p>
<figcaption>algorithm 1</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig_3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="figure 3"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/fig_3.png" class="col-page img-fluid figure-img" width="800" alt="figure 3"></a></p>
<figcaption>figure 3</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./table_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="table 1"><img src="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/table_1.png" class="col-page img-fluid figure-img" width="800" alt="table 1"></a></p>
<figcaption>table 1</figcaption>
</figure>
</div>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Variational {Inference} with {Normalizing} {Flows}},
  date = {2022-06-26},
  url = {https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Variational Inference with Normalizing
Flows.”</span> June 26, 2022. <a href="https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/">https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/</a>.
</div></div></section></div> ]]></description>
  <category>variational inference</category>
  <category>normalizing flows</category>
  <category>probabilistic modeling</category>
  <category>stub</category>
  <guid>https://orenbochman.github.io/reviews/2015/vi-with-normalizing-flows/</guid>
  <pubDate>Sun, 26 Jun 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/1989/skeletonization/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>This Nips 1988 paper is about simplifying neural networks by removing redundant units. The authors’ approach is systematically identifying and removing redundant or less-relevant units without losing functionality.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="figure 1"><img src="https://orenbochman.github.io/reviews/1989/skeletonization/fig_1.png" class="img-fluid figure-img" width="800" alt="figure 1"></a></p>
<figcaption>figure 1</figcaption>
</figure>
</div>
</section>
<section id="summary" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>In <span class="citation" data-cites="mozer1988skeletonization">(Mozer and Smolensky 1988)</span> the authors presents a novel approach to simplifying neural networks by systematically identifying and removing redundant or less-relevant units. The authors address a key challenge in connectionist models: understanding and optimizing the network’s behavior by trimming unnecessary components without losing functionality.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mozer1988skeletonization" class="csl-entry">
Mozer, Michael C, and Paul Smolensky. 1988. <span>“Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment.”</span> <em>Advances in Neural Information Processing Systems</em> 1.
</div></div><p>The core idea of the paper is to iteratively train a network, compute a relevance metric for each input or hidden unit, and eliminate the units that are least relevant to the network’s performance. The skeletonization process offers several practical benefits:</p>
<ul>
<li><p><strong>Constraining Generalization</strong>: By reducing redundant units, skeletonization effectively limits the network’s complexity, which can enhance its generalization capability.</p></li>
<li><p><strong>Improving Learning Performance</strong>: Networks often require many hidden units to quickly learn a dataset. Skeletonization accelerates initial learning with excess units and trims unnecessary ones later, leading to better generalization without sacrificing learning speed.</p></li>
<li><p><strong>Simplifying Network Behavior</strong>: The authors argue that skeletonization can transform complex networks into simpler models, effectively capturing core decision-making rules.</p></li>
</ul>
<p>The technique contrasts with other approaches such as weight decay methods by opting for an all-or-none removal of units, motivated by the desire to identify the most critical components through explicit relevance metrics. These metrics are determined by computing the network’s error with and without specific units, using a time-averaged relevance estimate.</p>
</section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<ol type="1">
<li><p><strong>Relevance Metric and Error Propagation</strong>: Mozer and Smolensky introduce a <mark>relevance measure derived from the network’s error response to the removal of a unit</mark>. The metric can be computed via an error propagation procedure similar to backpropagation, making the approach computationally feasible for larger networks.</p></li>
<li><p><strong>Practical Application in Simple Examples</strong>:</p>
<ul>
<li>In the <strong>cue salience problem</strong>, the relevance metric highlights the most critical input unit, effectively eliminating irrelevant units.</li>
<li>In the <strong>rule-plus-exception problem</strong>, the method identifies the hidden unit responsible for most general cases, while distinguishing another unit that handles exceptions, reflecting the nuanced task structure.</li>
<li>The <strong>train problem</strong> demonstrates the technique’s ability to reduce inputs to a minimal set of features needed to differentiate categories.</li>
</ul></li>
<li><p><strong>Skeletonization in More Complex Tasks</strong>: Mozer and Smolensky apply skeletonization to more sophisticated problems like the <strong>four-bit multiplexor</strong> and the random mapping problem, showing that skeletonized networks can outperform standard ones in terms of both failure rate and learning speed.</p></li>
</ol>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Effective Reduction of Network Size</strong>: One of the most impressive outcomes is the ability of skeletonized networks to match or exceed the performance of larger networks with fewer units. The networks show resilience in maintaining their functionality even as units are removed.</p></li>
<li><p><strong>Improvement in Learning Time</strong>: The authors provide evidence that learning with an initially large network and then trimming it can lead to faster convergence than training a smaller network from the start. This result challenges conventional thinking that fewer hidden units should always be preferable from the outset.</p></li>
<li><p><strong>Rule Abstraction</strong>: The skeletonization process successfully identifies essential “rules” that govern network behavior, making it easier to interpret a network’s decisions in a simplified and concise manner.</p></li>
</ul>
</section>
<section id="limitations-and-open-questions" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-open-questions">Limitations and Open Questions</h2>
<ul>
<li><p><strong>Predefined Trimming Limit</strong>: One limitation of the paper’s approach is the need to predefine how much of the network to trim. While the authors acknowledge that magnitudes of relevance values may offer insights, an automatic stopping criterion based on these values is not fully explored.</p></li>
<li><p><strong>Error Function Sensitivity</strong>: The paper highlights an issue with using quadratic error functions to compute relevance. The authors propose an alternative linear error function, but the sensitivity of results to different error metrics could benefit from further investigation.</p></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Mozer and Smolensky’s skeletonization technique represents a significant step toward optimizing neural networks by removing redundant units without losing core functionality. The method not only improves learning performance but also offers valuable insights into the internal workings of neural networks. By focusing on relevance metrics, the authors pave the way for more interpretable, efficient, and generalized neural models.</p>
<p>Overall, skeletonization remains an influential contribution to the study of neural network optimization, providing both theoretical insights and practical improvements in learning systems.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Skeletonization: {A} {Technique} for {Trimming} the {Fat}
    from a {Network} via {Relevance} {Assessment}},
  date = {2022-06-22},
  url = {https://orenbochman.github.io/reviews/1989/skeletonization/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Skeletonization: A Technique for Trimming
the Fat from a Network via Relevance Assessment.”</span> June 22, 2022.
<a href="https://orenbochman.github.io/reviews/1989/skeletonization/">https://orenbochman.github.io/reviews/1989/skeletonization/</a>.
</div></div></section></div> ]]></description>
  <category>neural networks</category>
  <category>pruning</category>
  <guid>https://orenbochman.github.io/reviews/1989/skeletonization/</guid>
  <pubDate>Wed, 22 Jun 2022 00:00:00 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/1989/skeletonization/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Simplifying Neural Networks by soft weight sharing</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/</link>
  <description><![CDATA[ 





<p>This paper was mentioned in Geoffrey Hinton’s <a href="https://www.coursera.org/learn/neural-networks-deep-learning/">Coursera course</a> as a way to simplify neural networks.</p>
<p>The main takeaway is that of modeling the loss using a mixture of Gaussians to cluster the weights and penalize the complexity of the model.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
To Dos
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><label><input type="checkbox">add categories</label></li>
<li><label><input type="checkbox">add a citation for the course.</label></li>
<li><label><input type="checkbox">It would be worth-while to look through what he says about this paper if only to ensure the main result is made to stand out from some of the others.</label></li>
<li><label><input type="checkbox">Do a from scratch implementation of the paper</label></li>
<li><label><input type="checkbox">Do a Python implementation of the paper.</label></li>
<li><label><input type="checkbox">Why isn’t this technique easier to implement in practice?</label></li>
</ol>
</div>
</div>
</div>
<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>The primary aim of the paper <span class="citation" data-cites="nowlan2018simplifying">(Nowlan and Hinton 1992)</span> is reducing the complexity of neural networks by employing a mixture of Gaussian priors to the weights, creating a “soft” weight-sharing mechanism. Instead of simply penalizing large weights (as in L2 regularization), this method clusters the weights, allowing some to stay close to zero and others to remain non-zero, depending on their usefulness. Soft weight sharing along with weight decay, improving generalization and making the model more interpretable.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p><mark>One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity</mark>. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple Gaussians. <mark>A set of weights is <em>simple</em> if the weights have high probability density under the mixture model</mark>. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms</p>
<p>– <span class="citation" data-cites="nowlan2018simplifying">(Nowlan and Hinton 1992)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Weight clustering - a perplexing idea &amp; Other quandaries
</div>
</div>
<div class="callout-body-container callout-body">
<p>This notion of clustering weights is odd to say the least as these are just numbers in a data structure. Viewed as a method to reduce the effective number of parameters in the model, it makes some convoluted sense. What this idea seems to boil down to is that we are prioritizing neural net architectures with some abstract symmetry in the weights and thus a lower capacity and thus less prone to overfitting.</p>
<ul>
<li>We shall shall soon see that the authors have attempted to motivate this idea in at least two ways:
<ol type="1">
<li><strong>Weight decay</strong> - the penalty is a function of the weights themselves based on <span class="citation" data-cites="plaut1986experiments">(Plaut, Nowlan, and Hinton 1986)</span></li>
<li><strong>A Bayesian perspective</strong> - is a negative log density of the weights under a Gaussian prior.</li>
</ol></li>
<li>it might also help if we learned that mixture models are often used to do clustering in unsupervised learning.</li>
</ul>
<p>A few quandaries then arise:</p>
<ol type="1">
<li>How can we figure for different layers having weights, gradients and learning rates being more correlated then between layers.</li>
<li>That there may be other structure so that the weights are not independent of each other.
<ol type="1">
<li>In classifiers the are continuous approximation of logic gates.</li>
<li>In regression settings their values approximate continuous variables ?</li>
</ol></li>
<li>In many networks most of the weights are in the last layer, so we can use a different penalty for the last layer.</li>
<li>Is there a way to impose an abstract symmetry on the weights of a neural network such that is commensurate with the problem?</li>
<li>Can we impose multiple such symmetries on the network to give it other advantages?
<ul>
<li>Invariance to certain transformations,</li>
<li>using it for initialization,</li>
<li>making the model more interpretable,</li>
<li>Once we learn these mixture distribution of weights, can we use its parameters in, batch normalization, layer norm and with other regularization techniques like dropout?</li>
</ul></li>
</ol>
</div>
</div>
<div class="no-row-height column-margin column-container"></div></section>
<section id="the-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-problem">The problem:</h2>
<p>This main problem in this paper is that of supervised ML</p>
<blockquote class="blockquote">
<p>How to train a model so it will generalize well on unseen data?</p>
</blockquote>
<p>In deep learning this problem is exacerbated by the fact that neural networks require fitting lots of parameters while the data for training is limited. This naturally leads to overfitting - memorizing the data and noise rather than learning the underlying data generating process.</p>
</section>
<section id="related-work" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="related-work">Related work</h2>
<p>How have others tried to solve this problem?</p>
<ul>
<li><strong>Weight Sharing</strong>: Reducing effective numbers of <em>free</em> parameters in a neural network by using a prior distribution over the weights. This is equivalent to adding a penalty term to the loss function. c.f. <span class="citation" data-cites="lang1990time">(Lang, Waibel, and Hinton 1990)</span></li>
<li><strong>Early Stopping</strong>: Does not restricts the parameters instead detecting the point when training and test scores diverge and stopping the training early. In reality weights are saved after each epoch and once we are certain that a divergence in say accuracy we restore the earlier weights c.f. <span class="citation" data-cites="morgan1989generalization">(Morgan and Bourlard 1989)</span>, <span class="citation" data-cites="weigend1990predicting">(Weigend, Huberman, and Rumelhart 1990)</span>.</li>
<li><strong>Pruning</strong>: Removing weight from the network <span class="citation" data-cites="mozer1989using">(Mozer and Smolensky 1989)</span>,
<ol type="1">
<li>keep track of the importance of each unit and drop the least important ones - could work well in RL where we keep track of the importance of each state/action/features we might also care more about prioritizing certain states and discarding others. c.f. <span class="citation" data-cites="mozer1989using">(Mozer and Smolensky 1989)</span></li>
<li>Use second order gradient information to estimate network sensitivity to weight changes and prune based on that. c.f. <span class="citation" data-cites="lecun-90b">(LeCun et al. 1990)</span></li>
</ol></li>
<li><strong>Penalty Term</strong>: Adding a term in the loss penelizing for the network’s complexity. c.f. <span class="citation" data-cites="mozer1989using">(Mozer and Smolensky 1989)</span> <span class="citation" data-cites="lecun-90b">(LeCun et al. 1990)</span><sup>1</sup>.
<ol type="1">
<li>Complexity can be approximated using sum of the squares of the weights.</li>
<li>Differentiating the sum of the squares of the weights leads to weight decay. <span class="citation" data-cites="plaut1986experiments">(Plaut, Nowlan, and Hinton 1986)</span></li>
</ol></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-lang1990time" class="csl-entry">
Lang, Kevin J, Alex H Waibel, and Geoffrey E Hinton. 1990. <span>“A Time-Delay Neural Network Architecture for Isolated Word Recognition.”</span> <em>Neural Networks</em> 3 (1): 23–43.
</div><div id="ref-morgan1989generalization" class="csl-entry">
Morgan, Nelson, and Hervé Bourlard. 1989. <span>“Generalization and Parameter Estimation in Feedforward Nets: Some Experiments.”</span> <em>Advances in Neural Information Processing Systems</em> 2.
</div><div id="ref-weigend1990predicting" class="csl-entry">
Weigend, Andreas S, Bernardo A Huberman, and David E Rumelhart. 1990. <span>“Predicting the Future: A Connectionist Approach.”</span> <em>International Journal of Neural Systems</em> 1 (03): 193–209.
</div><div id="ref-mozer1989using" class="csl-entry">
Mozer, Michael C, and Paul Smolensky. 1989. <span>“Using Relevance to Reduce Network Size Automatically.”</span> <em>Connection Science</em> 1 (1): 3–16.
</div><div id="ref-lecun-90b" class="csl-entry">
LeCun, Yann, J. S. Denker, S. Solla, R. E. Howard, and L. D. Jackel. 1990. <span>“Optimal Brain Damage.”</span> In <em>Advances in Neural Information Processing Systems (NIPS 1989)</em>, edited by David Touretzky. Vol. 2. Denver, CO: Morgan Kaufman. <a href="https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;in the paper this citation is ambiguous, but I think this is the correct one - based on the abstract</p></div><div id="ref-nowlan2018simplifying" class="csl-entry">
Nowlan, Steven J., and Geoffrey E. Hinton. 1992. <span>“<span class="nocase">Simplifying Neural Networks by Soft Weight-Sharing</span>.”</span> <em>Neural Computation</em> 4 (4): 473–93. <a href="https://doi.org/10.1162/neco.1992.4.4.473">https://doi.org/10.1162/neco.1992.4.4.473</a>.
</div></div><p>Just a few ideas from this paper: <span class="citation" data-cites="nowlan2018simplifying">(Nowlan and Hinton 1992)</span></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bcost%7D%20=%20%5Ctext%7Bdata-misfit%7D%20+%20%5Clambda%20%5Ctimes%20%5Ctext%7Bcomplexity%7D%20%5Cqquad%0A"></p>
<p>penalties:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi%7D%20w_i%5E2%20%5Cqquad%20%5Ctext%7B(L2%20penalty)%7D%0A"></p>
<p>the authors provide two ways to think about this penalty:</p>
<ol type="1">
<li><strong>Weight decay</strong> - the penalty is a function of the weights themselves based on <span class="citation" data-cites="plaut1986experiments">(Plaut, Nowlan, and Hinton 1986)</span></li>
<li><strong>a Bayesian perspective</strong> - is a negative log density of the weights under a Gaussian prior.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-plaut1986experiments" class="csl-entry">
Plaut, D. C., S. J. Nowlan, and G. E. Hinton. 1986. <span>“Experiments on Learning by Back-Propagation.”</span> CMU-CS-86-126. Pittsburgh, PA: Carnegie–Mellon University. <a href="https://ni.cmu.edu/~plaut/papers/pdf/PlautNowlanHinton86TR.backprop.pdf">https://ni.cmu.edu/~plaut/papers/pdf/PlautNowlanHinton86TR.backprop.pdf</a>.
</div></div><p>The authors point out that the problem with L2 penalty term is that it prefers two weak interactions over a strong one.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft(%5Cfrac%7Bw%7D%7B2%7D%5Cright)%5E2+%5Cleft(%5Cfrac%7Bw%7D%7B2%7D%5Cright)%5E2%20%3C%20w%5E2%20+%200%20%5Cqquad%0A"></p>
<p>we want to keep larger weights and drop the smaller ones</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(w)%20=%20%5Cpi_n%20%5Cfrac%7B1%7D%7B2%20%5Cpi%5Csigma_n%7De%5E%7B-w%5E2/2%5Csigma%5E2_n%7D%20+%20%5Cpi_b%20%5Cfrac%7B1%7D%7B2%20%5Cpi%5Csigma_b%7De%5E%7B-w%5E2/2%5Csigma%5E2_b%7D%20%5Cqquad%0A"></p>
<p>This is equivalent to the L2 penalty. <span class="citation" data-cites="mackay1991bayesian">(MacKay 1991)</span> showed that we can do better with:</p>
<div class="no-row-height column-margin column-container"><div id="ref-mackay1991bayesian" class="csl-entry">
MacKay, David JC. 1991. <span>“Bayesian Modeling and Neural Networks.”</span> <em>PhD Thesis, Dept. Of Computation and Neural Systems, CalTech</em>. <a href="https://www.inference.org.uk/mackay/thesis.pdf">https://www.inference.org.uk/mackay/thesis.pdf</a>.
</div></div><p><img src="https://latex.codecogs.com/png.latex?%0Ap(w)%20=%5Csum_i%20%5Clambda_i%20%5Csum_%7Bj%7D%20w_j%5E2%20%5Cqquad%20%5Ctext%7B(L2%20penalty)%7D%0A"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?i"> is the layer and</li>
<li><img src="https://latex.codecogs.com/png.latex?j"> is the weight in that layer</li>
</ul>
<p>Different layers has a different penalty, which is equivalent to a Gaussian prior with different variances for each layer.<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;note: this answers the first of my question above.</p></div></div><p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi%7D%20%5Cdelta(w%7B_i%5Cne%200%7D)%20%5Cqquad%20%5Ctext%7B(L0%20penalty)%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(w)%20=%20%5Csum_%7Bi%7D%20%5Cdelta(w_i%20%5Cne%200)%20+%20%5Clambda%20%5Csum_%7Bi%7D%20w_i%5E2%0A"></p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li>Article <a href="https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/">using weight constraints to reduce generalization</a></li>
<li>The paper is available at https://www.cs.utoronto.ca/~hinton/absps/sunspots.pdf</li>
</ul>
</section>
<section id="an-after-thought" class="level2">
<h2 class="anchored" data-anchor-id="an-after-thought">An after thought</h2>
<p>Can we use a Bayesian RL to tune the hyper-parameters of model and dataset. We can perhaps create an RL alg that controls the many aspects of training of a model. It can explore/exploit different setups on subsets of the data. Find variants that converge faster and are more robust by adding constraints at different levels. It can identify problems in the datasets (possible bad labels etc) . Ensambles, mixtures of experts, different regularization strategies. Different Learning rates and schedules globaly or per layer.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2022,
  author = {Bochman, Oren},
  title = {Simplifying {Neural} {Networks} by Soft Weight Sharing},
  date = {2022-06-22},
  url = {https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2022" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2022. <span>“Simplifying Neural Networks by Soft Weight
Sharing.”</span> June 22, 2022. <a href="https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/">https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/</a>.
</div></div></section></div> ]]></description>
  <category>neural networks</category>
  <category>regularization</category>
  <category>stub</category>
  <guid>https://orenbochman.github.io/reviews/1991/simplifing-NN-by-soft-weight-sharing/</guid>
  <pubDate>Wed, 22 Jun 2022 00:00:00 GMT</pubDate>
</item>
<item>
  <title>VGGNet: Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2014/VGGNet/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In this paper <span class="citation" data-cites="simonyan2015deepconvolutionalnetworkslargescale">(Simonyan and Zisserman 2015)</span> the authors, Karen Simonyan and Andrew Zisserman from the Visual Geometry Group at Oxford, investigated the effect of increasing the convolutional network depth on the accuracy in the large-scale image recognition setting. The authors show that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers.</p>
<div class="no-row-height column-margin column-container"></div><ul>
<li><p>By using 3x3 convolution filters with stride of 1 instead of larger ones like 5x5 or 7x7 the authors were able to reduce the number of parameters in the network which allowed them to use deeper networks (16-19) layers with a similar capacity to earlier networks. This is possible as stack of three 3x3 convolutional layers has an effective receptive field of 7x7 with 81% fewer parameters than a single 7x7 convolutional layer. Once this was understood 3x3 became the standard convolutional filter size in modern CNN architectures.</p></li>
<li><p>The authors introduced a data augmentation method called ‘image jittering’ which varying image scales.</p></li>
<li><p>The authors later tweaked their model further including using 1x1 convolutional layers and Local Response Normalization (LRN) which improved the performance of the model as well as Xaiver initialization. And they were able to achieve state-of-the-art results on the ImageNet dataset.</p></li>
<li><p>The authors released weights for <strong>VGG16</strong> and <strong>VGG19</strong> Called D and E in the table below which were the basis of their ImageNet Challenge 2014 submission. And it is is these two models that are most commonly used in practice as thier weight are available in the Keras library <span class="citation" data-cites="chollet2015keras">(Chollet et al. 2015)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-chollet2015keras" class="csl-entry">
Chollet, François et al. 2015. <span>“Keras.”</span> <a href="https://keras.io" class="uri">https://keras.io</a>.
</div></div></section>
<section id="the-abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-abstract">The abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>In this work <mark>we investigate the effect of the convolutional network depth on its accuracy</mark> in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a <mark>significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers</mark>. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. — <span class="citation" data-cites="simonyan2015deepconvolutionalnetworkslargescale">(Simonyan and Zisserman 2015)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-simonyan2015deepconvolutionalnetworkslargescale" class="csl-entry">
Simonyan, Karen, and Andrew Zisserman. 2015. <span>“Very Deep Convolutional Networks for Large-Scale Image Recognition.”</span> <a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a>.
</div></div></div>
</section>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>The paper has a table with some network architectures and their performance on the ImageNet dataset. In many cases data scientist etc. like to copy the architectures of well known models and use them in their own work. So this paper is a good reference for giving a few more options for architectures to use.</p>
<p>The paper uses 3x3 convolution filters which is a common practice in modern CNN architectures.</p>
<blockquote class="blockquote">
<p>We use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 × 5; three such layers have a 7 × 7 effective receptive field. <mark>So what have we gained by using, for instance, a stack of three 3 × 3 conv. layers instead of a single 7 × 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters</mark>: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by 3 (32C2) = 27C^2 weights; at the same time, a single 7 × 7 conv. layer would require 72C2 = 49C2 parameters, i.e.&nbsp;81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between).</p>
</blockquote>
<p>The authors also reference 1 × 1 convolutions from [NiN] paper which also have large FC layers at the end.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./table1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="architecture"><img src="https://orenbochman.github.io/reviews/2014/VGGNet/table1.png" class="img-fluid figure-img" width="800" alt="architecture"></a></p>
<figcaption>architecture</figcaption>
</figure>
</div>
<p>Where: - A is 11 layered. - A-LRN is 11 layered but have Local Response Normalization. - B is 13 layered. - C is 16 layered but has 1x1 convolutional layers. - D is 16 layered but 1x1 convolutional layers in C are replaced with 3x3 convolutional layers. - E is 19 layered</p>
<p>Training</p>
<p>The result were state of the art but by 2018 <span class="citation" data-cites="DBLP:journals/corr/GoyalDGNWKTJH17">(Goyal et al. 2017)</span> it would be possible to train a ResNet-50 imagenet classifier in under an hour of compute with just using 256 GPUs. There is little novelty in the methods. The authors simply increased the depth of the network and increase the umber of parameters.(but they also used them more efficiently).</p>
<div class="no-row-height column-margin column-container"><div id="ref-DBLP:journals/corr/GoyalDGNWKTJH17" class="csl-entry">
Goyal, Priya, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. <span>“Accurate, Large Minibatch <span>SGD:</span> Training ImageNet in 1 Hour.”</span> <em>CoRR</em> abs/1706.02677. <a href="http://arxiv.org/abs/1706.02677">http://arxiv.org/abs/1706.02677</a>.
</div><div id="ref-BibEntry2024Sep" class="csl-entry">
Appalapuri, Prabhu. 2016. <span>“<span class="nocase">Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition</span>.”</span> <a href="https://github.com/Prabhu204/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition" class="uri">https://github.com/Prabhu204/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition</a>; GitHub.
</div></div><p>At <span class="citation" data-cites="BibEntry2024Sep">(Appalapuri 2016)</span> I found a Pytorch implementation of this paper.</p>
<p>Many People ask what is the difference between VGG16 and VGG19. The difference is that VGG19 has 3 more convolutional layers than VGG16. Since these extra convolutional layers are stacked after two other layers, the receptive field of VGG19 is larger than that of VGG16. Also the CNN also have a RELU so that the network also has increased discriminative power. This means that VGG19 can capture more complex patterns in the input image than VGG16. However, this comes at the cost of more parameters and more computation. In practice, VGG16 is often used because it is simpler and faster to train than VGG19.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./vgg16.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="VGG16"><img src="https://orenbochman.github.io/reviews/2014/VGGNet/vgg16.png" class="img-fluid figure-img" width="800" alt="VGG16"></a></p>
<figcaption>VGG16</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./vgg19.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="VGG19"><img src="https://orenbochman.github.io/reviews/2014/VGGNet/vgg19.png" class="img-fluid figure-img" width="800" alt="VGG19"></a></p>
<figcaption>VGG19</figcaption>
</figure>
</div>
<!--
::: callout-warning

:::: {.column-margin}

![](captain_abvious.jpg){width="250px"} 




@meme2013captainobvious

> More layers $\implies$ more parameters <br>
> more parameters $\implies$ more capacity <br>
> more capacity $\implies$ better fit. <br>
> BaZinga! 🚀

::::

It seems that the authors were aware of methods for speeding up training but they did not bother with them. They as using 4 GPUs gave them the results in 2-3 weeks per model.

:::
-->
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<ul>
<li>The authors did not use any data augmentation methods like random cropping, flipping, etc. which are common in modern CNN architectures. They also did not use any regularization methods like dropout, L2 regularization, etc. which are also common in modern CNN architectures.</li>
<li>The networks are pretty massive and require a lot of GPU memory in inference.</li>
</ul>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://www.robots.ox.ac.uk/~vgg/research/very_deep/">home page</a></li>
<li>https://medium.com/<span class="citation" data-cites="siddheshb008/vgg-net-architecture-explained-71179310050f">(<strong>siddheshb008/vgg-net-architecture-explained-71179310050f?</strong>)</span></li>
<li>https://karan3-zoh.medium.com/paper-summary-very-deep-convolutional-networks-for-large-scale-image-recognition-e7437959d856</li>
<li>https://safakkbilici.github.io/summary-vggnet/</li>
<li>https://www.cs.toronto.edu/~frossard/post/vgg16/</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman2015,
  author = {Bochman, Oren},
  title = {VGGNet: {Very} {Deep} {Convolutional} {Networks} for
    {Large-Scale} {Image} {Recognition}},
  date = {2015-12-10},
  url = {https://orenbochman.github.io/reviews/2014/VGGNet/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman2015" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. 2015. <span>“VGGNet: Very Deep Convolutional Networks for
Large-Scale Image Recognition.”</span> December 10, 2015. <a href="https://orenbochman.github.io/reviews/2014/VGGNet/">https://orenbochman.github.io/reviews/2014/VGGNet/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2014/VGGNet/</guid>
  <pubDate>Thu, 10 Dec 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Multi-column Deep Neural Networks for Image Classification</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="cireşan2012multicolumndeepneuralnetworks">(Cireşan, Meier, and Schmidhuber 2012)</span> titled “Multi-column Deep Neural Networks for Image Classification”, the authors, Dan Cireşan, Ueli Meier, Juergen Schmidhuber introduce a biologically plausible deep artificial neural network architecture that achieves near-human performance on tasks such as the recognition of handwritten digits or traffic signs. The method uses small receptive fields of convolutional winner-take-all neurons to yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. The authors demonstrate that their approach outperforms humans on a traffic sign recognition benchmark and improves the state-of-the-art on various image classification benchmarks.</p>
<div class="no-row-height column-margin column-container"></div></section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks. — <span class="citation" data-cites="cireşan2012multicolumndeepneuralnetworks">(Cireşan, Meier, and Schmidhuber 2012)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
</section>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="cireşan2012multicolumndeepneuralnetworks">(Cireşan, Meier, and Schmidhuber 2012)</span> the authors make significant strides in the field of image classification by demonstrating the effectiveness of multi-column deep neural networks (DNNs). <mark>This work is noteworthy for its pioneering approach in applying deep learning techniques to image classification tasks, which have since become the foundation of modern computer vision systems.</mark></p>
<div class="no-row-height column-margin column-container"><div id="ref-cireşan2012multicolumndeepneuralnetworks" class="csl-entry">
Cireşan, Dan, Ueli Meier, and Juergen Schmidhuber. 2012. <span>“Multi-Column Deep Neural Networks for Image Classification.”</span> <a href="https://arxiv.org/abs/1202.2745">https://arxiv.org/abs/1202.2745</a>.
</div></div></section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<p>The authors present a system that uses several deep neural networks, each operating as a “column,” which are trained independently. The outputs of these networks are then averaged to form the final prediction. This multi-column approach exploits the diversity between different networks and boosts classification accuracy, reducing the impact of overfitting and improving generalization. Notably, the method achieved state-of-the-art results on several image classification benchmarks at the time, including the MNIST digit recognition task.</p>
<p>One of the central contributions of this paper is the demonstration of how <mark>combining multiple deep networks can outperform single networks in complex image classification tasks</mark>. The authors trained their models on NVIDIA GPUs, which allowed them to scale deep networks efficiently—a relatively new practice when this paper was published, underscoring its innovative edge.</p>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Improvement on Benchmarks</strong>: The multi-column DNN approach delivered unprecedented accuracy on datasets like MNIST, achieving an error rate of just 0.23%. This represents one of the early breakthroughs that paved the way for deep learning in computer vision.</p></li>
<li><p><strong>Effective Use of Parallelism</strong>: The paper highlights the use of modern GPUs to efficiently train deep networks, illustrating how hardware advancements can accelerate research progress.</p></li>
<li><p><strong>Generalizability</strong>: While the paper focuses on MNIST and other datasets, the multi-column DNN framework offers a flexible approach to other image classification tasks. The general architecture and training methodology could be adapted to more complex datasets, making this work highly relevant across a variety of image recognition problems.</p></li>
<li><p><strong>Robustness</strong>: By averaging outputs from multiple networks, the system reduces the sensitivity to the specific architecture or initialization of a single network. This ensemble-like approach increases robustness and reduces error rates.</p></li>
</ul>
</section>
<section id="weaknesses" class="level2">
<h2 class="anchored" data-anchor-id="weaknesses">Weaknesses</h2>
<ul>
<li><p><strong>Lack of Theoretical Insight</strong>: Although the empirical results are impressive, the paper does not delve deeply into the theoretical reasons behind the success of multi-column architectures. It remains unclear how much of the performance gain is due to ensembling versus the intrinsic strength of the individual networks.</p></li>
<li><p><strong>Computational Cost</strong>: The approach requires training multiple deep neural networks independently, which could be computationally expensive for larger datasets or higher-dimensional inputs. While GPUs mitigate this to an extent, scaling the multi-column approach to larger tasks would demand significant computational resources.</p></li>
<li><p><strong>Limited Applicability to Other Modalities</strong>: The paper focuses solely on image classification. While it hints at the potential for multi-column networks in other domains (e.g., audio or text), the paper doesn’t explore these extensions or provide empirical evidence beyond the image domain.</p></li>
</ul>
</section>
<section id="impact-and-relevance" class="level2">
<h2 class="anchored" data-anchor-id="impact-and-relevance">Impact and Relevance</h2>
<p>This paper marked a turning point for deep learning in computer vision, showing the power of combining deep networks for complex tasks like image classification. Its success on benchmarks like MNIST helped popularize deep learning as a dominant method for pattern recognition and set the stage for more advanced techniques. Although it primarily focuses on image classification, the insights regarding ensemble learning through independent deep networks have since inspired various approaches in different machine learning areas, including speech recognition and natural language processing.</p>
<p>The paper is particularly significant when viewed in the context of its time (2012), as it predated the massive adoption of deep learning across industries. Its methods were fundamental to later developments in deep convolutional neural networks, which have become a cornerstone of state-of-the-art models in computer vision tasks today.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Ciresan, Meier, and Schmidhuber’s work on multi-column deep neural networks represents a crucial step forward in the development of image classification techniques. Its impact on deep learning, especially in terms of model ensembling and parallelization using GPUs, cannot be overstated. While it comes with some computational challenges and lacks deep theoretical explanation, the paper’s practical results and novel approach have solidified its place as a landmark contribution in the history of deep learning.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Multi-Column {Deep} {Neural} {Networks} for {Image}
    {Classification}},
  url = {https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Multi-Column Deep Neural Networks for Image
Classification.”</span> <a href="https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/">https://orenbochman.github.io/reviews/2012/Multi-column
deep neural networks for image classi cation/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2012/Multi-column deep neural networks for image classi cation/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2012/dropout/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="hinton2012improvingneuralnetworkspreventing">(Hinton et al. 2012)</span> titled “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors”, the authors, Hinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov introduces a new regularization technique called “Dropout” that helps to prevent overfitting in neural networks. Dropout is a simple and effective way to improve the performance of neural networks by preventing co-adaptation of feature detectors. The authors show that dropout can be used to improve the performance of a wide range of neural networks, including deep networks, convolutional networks, and recurrent networks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hinton2012improvingneuralnetworkspreventing" class="csl-entry">
Hinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. <span>“Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.”</span> <a href="https://arxiv.org/abs/1207.0580">https://arxiv.org/abs/1207.0580</a>.
</div></div></section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification, and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
</blockquote>
</section>
<section id="review" class="level2">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>, introduces the dropout technique as an innovative method to prevent overfitting in neural networks. Overfitting occurs when a model performs well on training data but poorly on unseen test data, particularly when dealing with a large number of parameters and limited training samples. The paper addresses this by proposing the use of dropout, a regularization technique that randomly omits units (neurons) during training.</p>
</section>
<section id="core-ideas" class="level2">
<h2 class="anchored" data-anchor-id="core-ideas">Core Ideas</h2>
<p>The central concept behind dropout is to prevent co-adaptation of feature detectors. In a traditional neural network, feature detectors can co-adapt to specific patterns in the training data, which leads to poor generalization. By randomly omitting neurons with a probability of 0.5 during training, each neuron is forced to contribute independently to the final output. This reduces the reliance on specific sets of neurons and ensures that each feature detector learns useful patterns.</p>
<p>Another significant advantage of dropout is that it acts as an efficient form of model averaging. Training with dropout can be seen as training an ensemble of neural networks that share parameters, making it computationally feasible to obtain better generalization without having to train multiple models.</p>
</section>
<section id="experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results</h2>
<p>The authors demonstrate the effectiveness of dropout on several benchmark datasets, including MNIST, CIFAR-10, ImageNet, TIMIT, and the Reuters corpus.</p>
<ul>
<li>MNIST: Dropout reduced the error rate from 160 errors to around 110 by applying 50% dropout to hidden units and 20% dropout to input units.</li>
<li>TIMIT: Dropout improved frame classification accuracy in speech recognition tasks, reducing the error rate by 3% in comparison to standard training methods.</li>
<li>CIFAR-10: The authors achieved a 16.6% error rate without dropout and 15.6% with dropout, outperforming previous state-of-the-art results.</li>
<li>ImageNet: Dropout applied to deep convolutional neural networks (CNNs) reduced the error rate from 48.6% to 42.4%.</li>
<li>Reuters Corpus: Dropout reduced classification error from 31.05% to 29.62%.</li>
</ul>
</section>
<section id="theoretical-contributions" class="level2">
<h2 class="anchored" data-anchor-id="theoretical-contributions">Theoretical Contributions</h2>
<p>The theoretical underpinning of dropout is grounded in model averaging and regularization. In standard practice, model averaging is performed by training multiple models and averaging their predictions, but this approach is computationally expensive. Dropout provides a far more efficient alternative by implicitly training an ensemble of models that share parameters, thus achieving the benefits of model averaging without the overhead of training separate models.</p>
<p>Additionally, dropout mitigates the problem of overfitting by introducing noise during training, making the model more robust. At test time, all units are used, but their outgoing weights are scaled to reflect the fact that fewer units were active during training.</p>
</section>
<section id="discussion-and-impact" class="level2">
<h2 class="anchored" data-anchor-id="discussion-and-impact">Discussion and Impact</h2>
<p>The introduction of dropout represents a major step forward in the development of deep learning models, as it allows for better generalization across a variety of tasks. Its simplicity, coupled with its effectiveness, has made dropout a standard tool in neural network training. The experiments conducted in the paper demonstrate its utility across a wide range of tasks, from image recognition to speech processing, providing compelling evidence of its broad applicability.</p>
<p>The idea of preventing co-adaptation of feature detectors to improve generalization is an elegant solution to a longstanding problem in neural network training. By ensuring that each neuron must work independently, dropout forces the model to learn more robust features that generalize well to unseen data.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This paper is a highly influential paper that introduced a novel technique for improving the generalization of deep learning models. The results speak for themselves, with dropout achieving state-of-the-art performance across multiple datasets and tasks. The technique has since become a standard part of neural network training, revolutionizing the field and contributing to the success of deep learning in real-world applications.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Improving {Neural} {Networks} by {Preventing} {Co-Adaptation}
    of {Feature} {Detectors}},
  url = {https://orenbochman.github.io/reviews/2012/dropout/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Improving Neural Networks by Preventing
Co-Adaptation of Feature Detectors.”</span> <a href="https://orenbochman.github.io/reviews/2012/dropout/">https://orenbochman.github.io/reviews/2012/dropout/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/dropout/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
</item>
<item>
  <title>ImageNet Classification with Deep Convolutional Neural Networks</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2012/imagenet/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p><span class="citation" data-cites="krizhevsky2012imagenet">(Krizhevsky, Sutskever, and Hinton 2012)</span> is a seminal paper in the field of deep learning. It introduced the AlexNet architecture, which won the ImageNet Large Scale Visual Recognition Challenge in 2012. The paper is a great starting point for anyone interested in deep learning, as it provides a detailed explanation of the architecture and training process of the network.</p>
<div class="no-row-height column-margin column-container"></div><div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
<p>In <span class="citation" data-cites="krizhevsky2012imagenet">(Krizhevsky, Sutskever, and Hinton 2012)</span> titled “ImageNet Classification with Deep Convolutional Neural Networks”, the authors Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton presents the development and training of a large deep convolutional neural network (CNN) for image classification using the ImageNet dataset.</p>
<div class="no-row-height column-margin column-container"><div id="ref-krizhevsky2012imagenet" class="csl-entry">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“Imagenet Classification with Deep Convolutional Neural Networks.”</span> <em>Advances in Neural Information Processing Systems</em> 25.
</div></div><p>Thus paper marks a pivotal point in the development of deep learning, particularly in the realm of computer vision. The authors introduced a large convolutional neural network (CNN) trained on the ImageNet dataset, which significantly outperformed previous models, winning the <a href="https://www.image-net.org/challenges/LSVRC">ImageNet Large Scale Visual Recognition Challenge</a> (ILSVRC) in 2012 with a top-5 test error rate of 15.3%.</p>
<section id="key-contributions" class="level3">
<h3 class="anchored" data-anchor-id="key-contributions">Key Contributions:</h3>
<ul>
<li><p><strong>Architecture</strong>: The CNN consists of five convolutional layers and three fully connected layers, with the final layer being a softmax classifier that distinguishes between 1000 categories. This architecture involves a total of 60 million parameters and 650,000 neurons.</p></li>
<li><p><strong>GPU Utilization</strong>: Training was performed on two GTX 580 GPUs to speed up the process, allowing them to handle the large network size and dataset. This took approximately 5-6 days to complete.</p></li>
<li><p><strong>Techniques to Improve Performance</strong>: The network used a variety of novel techniques to improve both performance and training time:</p>
<ul>
<li><strong>Rectified Linear Units</strong> (ReLUs): These non-saturating neurons were employed to speed up training, which was crucial for dealing with such a large model.</li>
<li><strong>Dropout</strong>: A regularization method was used in fully connected layers to prevent overfitting by randomly dropping some neurons during training.</li>
<li><strong>Data Augmentation</strong>: The authors employed various forms of data augmentation, including random crops, horizontal flips, and color variation via principal component analysis (PCA), which greatly expanded the size of the training set and further reduced overfitting.</li>
</ul></li>
</ul>
<p>overfitting.</p>
</section>
</section>
<section id="results-and-impact" class="level2">
<h2 class="anchored" data-anchor-id="results-and-impact">Results and Impact</h2>
<p>The network trained for the ILSVRC 2010 and 2012 challenges achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, for ILSVRC 2010, far surpassing previous methods based on feature extraction and boosting. In the ILSVRC 2012 competition, the network reduced the top-5 error to 15.3%, compared to the 26.2% achieved by the second-best entry. This result not only established CNNs as the state-of-the-art model for image classification tasks but also cemented the importance of deep learning in the broader machine learning community.</p>
</section>
<section id="challenges-and-limitations" class="level2">
<h2 class="anchored" data-anchor-id="challenges-and-limitations">Challenges and Limitations</h2>
<p>The authors acknowledge that their network size was constrained by the available GPU memory and that improvements in both hardware and larger datasets could potentially improve the performance of such models in the future.</p>
<p>The CNN’s architecture and optimization techniques pioneered by this paper have set a foundation for subsequent advances in deep learning, particularly in image recognition tasks.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The paper demonstrated the feasibility and efficacy of training deep networks on large-scale datasets and provided key insights into architectural choices, regularization, and optimization. This work has since inspired a plethora of follow-up research, leading to advancements such as transfer learning, fine-tuning on smaller datasets, and the further development of GPU-based training methods. The innovations introduced in this paper laid the groundwork for the modern AI revolution in image recognition and beyond.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {ImageNet {Classification} with {Deep} {Convolutional}
    {Neural} {Networks}},
  url = {https://orenbochman.github.io/reviews/2012/imagenet/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“ImageNet Classification with Deep
Convolutional Neural Networks.”</span> <a href="https://orenbochman.github.io/reviews/2012/imagenet/">https://orenbochman.github.io/reviews/2012/imagenet/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2012/imagenet/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
</item>
<item>
  <title>Handwriting beautification using token means</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2013/HandwritingBeautification/</link>
  <description><![CDATA[ 





<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: beautification"><img src="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/fig_1.png" class="img-fluid figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: beautification
</figcaption>
</figure>
</div>
<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="Zitnick2013Beautification">(Zitnick 2013)</span> the author shows how we can use a model for beautifying handwriting. The problem raised is that there is lots of variation in handwriting for a single individual and come up this a method to reduce this by a clever form of avaraging.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Zitnick2013Beautification" class="csl-entry">
Zitnick, C. Lawrence. 2013. <span>“Handwriting Beautification Using Token Means.”</span> <em>ACM Trans. Graph.</em> 32 (4). <a href="https://doi.org/10.1145/2461912.2461985">https://doi.org/10.1145/2461912.2461985</a>.
</div></div><p>The data is captured from a tablet and thus had a three dimensional structure. Central to this paper is are two ideas:</p>
<ol type="1">
<li>How to effectively average similar tokens to get a suitable mean token.
<ul>
<li>they use a moving window</li>
<li>the sample using a curvature based sampling
<ul>
<li><span class="citation" data-cites="Whitney1937Regular">(Whitney 1937)</span></li>
<li><span class="citation" data-cites="mokhtarian1992theory">(Mokhtarian and Mackworth 1992)</span></li>
<li><span class="citation" data-cites="dudek1997shape">(Dudek and Tsotsos 1997)</span></li>
</ul></li>
<li>they use a fine alignment using
<ul>
<li>affinity matrix</li>
<li>dynamic programming to find the best warping between two sequences.</li>
</ul></li>
<li>they also use a visual similarity metric to make the model more <strong>robust</strong> to graphemes with similar strokes but different shapes.</li>
</ul></li>
<li>How to decide which tokens are similar.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-Whitney1937Regular" class="csl-entry">
Whitney, Hassler. 1937. <span>“On Regular Closed Curves in the Plane.”</span> <em>Compositio Mathematica</em> 4: 276–84. <a href="http://www.numdam.org/item/CM_1937__4__276_0/">http://www.numdam.org/item/CM_1937__4__276_0/</a>.
</div><div id="ref-mokhtarian1992theory" class="csl-entry">
Mokhtarian, Farzin, and Alan K Mackworth. 1992. <span>“A Theory of Multiscale, Curvature-Based Shape Representation for Planar Curves.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 14 (8): 789–805.
</div><div id="ref-dudek1997shape" class="csl-entry">
Dudek, Gregory, and John K Tsotsos. 1997. <span>“Shape Representation and Recognition from Multiscale Curvature.”</span> <em>Computer Vision and Image Understanding</em> 68 (2): 170–89.
</div></div><p>Once these are solved, it becomes a matter of clustering tokens by similarity and then averaging the tokens in each cluster to get a mean token.</p>
<p>The mean token are then used to replace the original token in the handwriting data. The authors show that this method can be used to improve the quality of handwriting data.</p>
<p>Q. As time goes by there is more data and the replacements pool towards the cluster avarage. It seems that replacement might be more uniform if the earlier replacements were updated as their cluster avarage drifts…</p>
<p>This naturally leads to a kind of time series.</p>
<p>Perhaps the key idea is how the authors convert the text to a sequence of vectors use a token mean to represent the data. This is a simple idea but</p>
</section>
<section id="the-approach" class="level2">
<h2 class="anchored" data-anchor-id="the-approach">3. The approach</h2>
<p>We represent the stylus’s samples by storing the difference vectors between the stylus positions</p>
<p>i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5CPhi%20=%20%5C%7B%5Cphi_1,%20%5Cldots%20,%20%5Cphi_a%5C%7D"> with</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cphi_i%20=%20%5C%7Bx_i,%20y_i,%20p_i%5C%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?(x_i,%20y_i)"> is the difference in the stylus’s pixel position between samples i − 1 and i.</p>
<p><img src="https://latex.codecogs.com/png.latex?p_i"> is the stylus’s pressure.</p>
</section>
<section id="stroke-resampling" class="level2">
<h2 class="anchored" data-anchor-id="stroke-resampling">3.1 Stroke resampling</h2>
<p>As I understand it data is captured uniformly from a tablet and thus had a three dimensional structure. The authors then to resample the data to more faithfully represent the the curvature that is the building block of strokes within the handwriting.</p>
<div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: stroke resampling - uniform v.s. curvature based"><img src="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/fig_2.png" class="img-fluid figure-img" style="width:80.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: stroke resampling - uniform v.s. curvature based
</figcaption>
</figure>
</div>
<p>They represent samples taken at regular distance intervals using <img src="https://latex.codecogs.com/png.latex?%5CPhi%5Ed%20=%20%5C%7B%20%5Cphi%5Ed%20_1%20,%20%5Cldots,%20%5Cphi%5Ed_n%20%5C%7D"> where the sample magnitude <img src="https://latex.codecogs.com/png.latex?r_i"> is constant for all samples. c.f. Figure&nbsp;2</p>
<p>Curvature based sampling:</p>
<p>We compute a stroke representation <img src="https://latex.codecogs.com/png.latex?%CE%A6%5Ec%20=%20%5C%7B%5Cphi%5Ec_1,%20%5Cldots%20,%20%5Cphi%5Ec_n%5C%7D"> with the same parameterization as <img src="https://latex.codecogs.com/png.latex?%CE%A6">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%CF%86%5Ec_i%20=%5C%7Bx_i,%20y_i,%20p_i%20%5C%7D"></p>
<p><span id="eq-stroke"><img src="https://latex.codecogs.com/png.latex?%0Az_i=z_%7Bi-1%7D+min(1,%20%5Cfrac%7B%5Calpha%20%5CDelta_%5Ctheta%20%5Cbeta_j%7D%7B2%5Cpi%7D)%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<p>where:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?z_i"> is the point on the curve.</li>
<li><img src="https://latex.codecogs.com/png.latex?z_%7Bi-1%7D"> is the previous point on the curve.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha"> is the sampling density parameter (minimum value = 24)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CDelta_%5Ctheta%20%5Cin%20(0,%5Cpi%5D"> is the angle between samples <img src="https://latex.codecogs.com/png.latex?%CF%86_%7Bi%E2%88%921%7D"> and <img src="https://latex.codecogs.com/png.latex?%CF%86_i"></li>
<li><img src="https://latex.codecogs.com/png.latex?r_j"> is the stroke magnitude.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%20=%20max(0,%20min(1,%20r_j%20%E2%88%92%20%5Csqrt%7B2%7D))."> is a parameter that controls for discretization of the stylus.</li>
</ul>
<!-- todo try to understand this better find/create an implementation -->
</section>
<section id="refining-strokes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="refining-strokes">3.2 Refining strokes</h2>
<p>When a user writes they generate a large set of stroke samples, denoted <img src="https://latex.codecogs.com/png.latex?%CE%A6"> (for the rest of the paper we assume a curvature-based sampling and drop the superscript c.) From <img src="https://latex.codecogs.com/png.latex?%CE%A6"> we create overlapping fixed length sequences of stroke samples called tokens c.f. Figure&nbsp;1. Each token contains n stroke samples.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
matrix profiles
</div>
</div>
<div class="callout-body-container callout-body">
<p>Using matrix profiles math behind stumpy might be usefull in making this work faster and better, not sure about real time.</p>
</div>
</div>
<section id="fine-scale-alignment" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="fine-scale-alignment">Fine-scale alignment</h3>

<div class="no-row-height column-margin column-container"><div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: fine alignment"><img src="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/fig_3.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: fine alignment
</figcaption>
</figure>
</div></div><p>The <strong>match cost</strong> <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7Bk,l%7D"> is found using a linear combination of three features,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbeta_%7Bk,l%7D%20=%20%5CDelta_%7B%5Chat%20r%7D%20+%20%5CDelta_%5Ctheta%20+%20%5Cdelta_p%20%5Cqquad%0A"></p>
<p>computed from:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?t_%7Bi,k%7D"> and <img src="https://latex.codecogs.com/png.latex?t_%7Bj,l%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Chat%20r"> is the absolute difference between <img src="https://latex.codecogs.com/png.latex?%5Chat%20r_k"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%20r_l">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5CDelta_%5Ctheta"> is the absolute angular distance between <img src="https://latex.codecogs.com/png.latex?%CE%B8_k"> and <img src="https://latex.codecogs.com/png.latex?%CE%B8_l">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdelta_p"> measures if both strokes have consistent visibility. That is, <img src="https://latex.codecogs.com/png.latex?%5Cdelta_p%20=%201"> if <img src="https://latex.codecogs.com/png.latex?p_k%20=%200"> and <img src="https://latex.codecogs.com/png.latex?p_l%20=%200">, or <img src="https://latex.codecogs.com/png.latex?p_k%20%3E%200"> and <img src="https://latex.codecogs.com/png.latex?p_l%20%3E%200">, and <img src="https://latex.codecogs.com/png.latex?%5Cdelta_p%20=%200"> otherwise.</li>
</ul>
</section>
<section id="merging-stroke-sets" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="merging-stroke-sets">Merging stroke sets</h3>
<blockquote class="blockquote">
<p>Once two or more tokens are aligned, we can merge them by averaging the stroke samples.</p>
</blockquote>

<div class="no-row-height column-margin column-container"><div id="fig-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: matches"><img src="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/fig_4.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: matches
</figcaption>
</figure>
</div></div></section>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also</h2>
<ul>
<li>http://larryzitnick.org/</li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Handwriting Beautification Using Token Means},
  url = {https://orenbochman.github.io/reviews/2013/HandwritingBeautification/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Handwriting Beautification Using Token
Means.”</span> <a href="https://orenbochman.github.io/reviews/2013/HandwritingBeautification/">https://orenbochman.github.io/reviews/2013/HandwritingBeautification/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2013/HandwritingBeautification/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
</item>
<item>
  <title>NIN — Network in Network</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2013/NIN/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="lin2014networknetwork">(Lin, Chen, and Yan 2014)</span> the authors, Lin, Min, Qiang Chen, and Shuicheng Yan, of this paper titled “Network in Network” paper came up with a way of connencting somee ideas on improving CNNs which had been mostly getting bigger <img src="https://latex.codecogs.com/png.latex?(VGG%20%3E%20AlexNet%20%3E%20LeNet)"> . They replaced traditional linear filters in convolutional neural networks (CNNs) with multilayer perceptrons (MLPs) to enhance local feature abstraction. <mark>This new architecture, called NIN, also introduces <strong>global average pooling</strong> in place of <em>fully connected layers</em> thus reducing overfitting, improving model interpretability and more significantly reducing the size of the network.</mark></p>
<div class="no-row-height column-margin column-container"></div><p>It took a while for the idea to catch on, but in computer vision, most of the parameters are in the fully connected layers, and the NIN architecture enables us to to reduce the number of parameters in the fully connected layers thereby <mark>breaking the curse of dimensionality in CNN</mark>. Once people realized this the NIN architecture became more widely adopted and influenced the development of more sophisticated deep learning architectures like the <mark>Inception architecture</mark> and further refined into the Resnet architecture</p>
<p>The NIN architecture has a significant impact on the design of CNNs by demonstrating that local feature abstraction can be enhanced with MLPs, leading to better performance with fewer parameters. Global average pooling, which replaces fully connected layers, makes the architecture more robust to overfitting and spatial translations, making it a powerful tool for image classification tasks. This combination of techniques has influenced the development of more sophisticated deep learning architectures, particularly in domains where model interpretability and reduced overfitting are critical.</p>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>We propose a novel deep network structure called “Network In Network” (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking multiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.</p>
<p>– <span class="citation" data-cites="lin2014networknetwork">(Lin, Chen, and Yan 2014)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
</section>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="lin2014networknetwork">(Lin, Chen, and Yan 2014)</span> the authors introduced a novel deep learning architecture that aims to improve the abstraction capabilities of convolutional neural networks (CNNs) by incorporating multilayer perceptrons (MLPs) into the convolution layers. This approach, termed “Network in Network,” replaces the conventional linear filters used in CNNs with small neural networks, allowing for better local feature modeling. The NIN architecture also introduces global average pooling as a substitute for traditional fully connected layers to reduce overfitting and improve the interpretability of the model.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lin2014networknetwork" class="csl-entry">
Lin, Min, Qiang Chen, and Shuicheng Yan. 2014. <span>“Network in Network.”</span> <a href="https://arxiv.org/abs/1312.4400">https://arxiv.org/abs/1312.4400</a>.
</div></div></section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<p>The NIN paper makes several key contributions to the deep learning landscape:</p>
<ol type="1">
<li><p><strong>Mlpconv Layer</strong>: Instead of using traditional linear filters, NIN proposes the use of multilayer perceptrons (MLPs) within the convolutional layers (termed mlpconv layers). These layers act as universal function approximators, capable of modeling more complex representations within local receptive fields. This structure allows for better abstraction of non-linear latent concepts, overcoming the limitations of traditional linear filters in CNNs.</p>
<p><img src="https://orenbochman.github.io/reviews/2013/NIN/fig1.png" class="img-fluid"> <img src="https://orenbochman.github.io/reviews/2013/NIN/fig2.png" class="img-fluid"></p></li>
<li><p><strong>Global Average Pooling</strong>: NIN introduces global average pooling as an alternative to fully connected layers in the final classification stage. This technique computes the spatial average of each feature map, feeding the result directly into a softmax layer for classification. By avoiding fully connected layers, the model becomes less prone to overfitting, thus improving generalization performance. Furthermore, this method provides more interpretable results by establishing a direct correspondence between feature maps and class labels.</p></li>
<li><p><strong>State-of-the-Art Performance</strong>: The authors demonstrate that NIN achieves state-of-the-art performance on several benchmark datasets, including CIFAR-10, CIFAR-100, and SVHN, without the need for extensive data augmentation or model ensembling. The architecture consistently outperforms other methods, such as maxout networks and CNNs with dropout regularization, especially in terms of classification accuracy.</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./results.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Cifar-10 error rates"><img src="https://orenbochman.github.io/reviews/2013/NIN/results.png" class="img-fluid figure-img" style="width:80.0%" alt="Cifar-10 error rates"></a></p>
<figcaption>Cifar-10 error rates</figcaption>
</figure>
</div>
</section>
<section id="strengths" class="level2">
<h2 class="anchored" data-anchor-id="strengths">Strengths</h2>
<ul>
<li><p><strong>Innovative Architecture</strong>: The introduction of MLPs into convolutional layers is a simple yet effective modification that significantly enhances the representational power of the model. This makes NIN a powerful alternative to traditional CNNs, especially for tasks that require fine-grained feature extraction and abstraction.</p></li>
<li><p><strong>Reduced Overfitting</strong>: The use of global average pooling not only replaces the computationally expensive fully connected layers but also serves as a built-in regularizer, reducing the need for additional techniques like dropout. This structural regularization helps to prevent overfitting, particularly on datasets with limited training examples, such as CIFAR-100.</p></li>
<li><p><strong>Better Interpretability</strong>: The global average pooling layer allows for easier interpretation of the learned feature maps, as each map is directly associated with a class. This increases the transparency of the network’s the decision-making process compared to conventional CNNs.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./fig4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Visulization"><img src="https://orenbochman.github.io/reviews/2013/NIN/fig4.png" class="img-fluid figure-img" style="width:80.0%" alt="Visulization"></a></p>
<figcaption>Visulization</figcaption>
</figure>
</div>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<ul>
<li><p><strong>Limited Novelty in Pooling</strong>: While global average pooling is effective, the concept is not entirely new, and its novelty is limited. Previous works have proposed similar techniques for specific tasks. However NIN certainly demonstrates the concepts efficacy.</p></li>
<li><p><strong>Scalability</strong>: The paper focuses primarily on relatively small datasets like CIFAR-10, CIFAR-100, SVHN, and MNIST. While NIN excels in these scenarios, it would be interesting to see how the architecture performs on larger, more complex datasets such as ImageNet, where the size and variety of the data might pose additional challenges.</p></li>
<li><p><strong>Lack of Depth Exploration</strong>: While the architecture consists of three stacked mlpconv layers, the paper does not deeply explore the impact of adding more layers or experimenting with deeper NIN networks. Such exploration could provide insight into how well the architecture scales with increased model depth.</p></li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>NIN architecture is an elegant and effective solution to improving feature abstraction and reducing overfitting in CNNs. By embedding MLPs within convolutional layers and using global average pooling for classification, NIN achieves state-of-the-art performance across a variety of tasks. NIN presented a strong case for the importance of local feature modeling and interpretable classification mechanisms in modern deep learning.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
<ul>
<li><a href="https://www.youtube.com/watch?v=QfNvhP6k6ZM">Alex Smola Course Video on NIN</a>, his <a href="https://c.d2l.ai/stanford-cs329p/">course</a> and <a href="https://www.d2l.ai/">book</a></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {NIN -\/-\/- {Network} in {Network}},
  url = {https://orenbochman.github.io/reviews/2013/NIN/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“NIN --- Network in Network.”</span> <a href="https://orenbochman.github.io/reviews/2013/NIN/">https://orenbochman.github.io/reviews/2013/NIN/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2013/NIN/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
</item>
<item>
  <title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2014/Dropout/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="JMLR:v15:srivastava14a">(Srivastava et al. 2014)</span> the authors, present a novel regularization technique for deep neural networks called “dropout.” The key idea behind dropout is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much and significantly reduces overfitting. The authors show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification, and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
<div class="no-row-height column-margin column-container"></div><p>The technique had been in use in some earlier works, but this paper popularized it and showed its effectiveness on a wide range of tasks. The idea behind drop out is pretty simple and people have since come up with many variations of it. It has become a standard technique in the deep learning toolbox.</p>
</section>
<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.</p>
<p>– <span class="citation" data-cites="JMLR:v15:srivastava14a">(Srivastava et al. 2014)</span></p>
</blockquote><div class="no-row-height column-margin column-container"></div></div>
</section>
<section id="review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="review">Review</h2>
<p>In <span class="citation" data-cites="JMLR:v15:srivastava14a">(Srivastava et al. 2014)</span> the authors presents a regularization technique called Dropout, aimed at addressing the critical problem of overfitting in deep neural networks (DNNs). Dropout randomly drops units (neurons) during training to prevent co-adaptation of units, which can lead to overfitting. This novel technique is demonstrated to significantly improve the performance of neural networks across a wide range of tasks, including computer vision, speech recognition, and natural language processing.</p>
<div class="no-row-height column-margin column-container"><div id="ref-JMLR:v15:srivastava14a" class="csl-entry">
Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <span>“Dropout: A Simple Way to Prevent Neural Networks from Overfitting.”</span> <em>Journal of Machine Learning Research</em> 15 (56): 1929–58. <a href="http://jmlr.org/papers/v15/srivastava14a.html">http://jmlr.org/papers/v15/srivastava14a.html</a>.
</div></div></section>
<section id="core-idea" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="core-idea">Core Idea</h2>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig1.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;1: the effect of dropout"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig1.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: the effect of dropout
</figcaption>
</figure>
</div>
</div></div><p>Dropout works by randomly removing units from the network during each training iteration. This prevents the network from becoming overly reliant on specific units, thus reducing overfitting. During testing, all units are used, but their weights are scaled to account for the dropout during training. This approximates the averaging of an exponential number of thinned networks that would otherwise be computationally infeasible.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig2.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;2: Training vs Test"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig2.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Training vs Test
</figcaption>
</figure>
</div>
</div></div></section>
<section id="methodology-and-theoretical-motivation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="methodology-and-theoretical-motivation">Methodology and Theoretical Motivation</h2>
<p>Dropout’s theoretical foundation stems from biological principles, specifically the idea of genetic robustness in sexual reproduction. In the analogy, a network’s hidden units act like genes that must learn to function independently of one another, preventing complex co-adaptations that may not generalize well to unseen data. The stochastic nature of dropout introduces noise during training, which acts as a form of model averaging and regularization.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig3.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;3: figure 3"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig3.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: figure 3
</figcaption>
</figure>
</div>
</div></div></section>
<section id="key-contributions" class="level2">
<h2 class="anchored" data-anchor-id="key-contributions">Key Contributions</h2>
<ul>
<li>Model Averaging: Dropout enables the training of many subnetworks (thinned networks) simultaneously, which leads to a more robust model.</li>
<li>Regularization Effect: Dropout reduces overfitting more effectively than other methods such as L1/L2 regularization or early stopping.</li>
<li>Efficiency: Dropout provides a computationally feasible approximation of model averaging by scaling weights at test time, as opposed to maintaining an ensemble of networks.</li>
</ul>
</section>
<section id="experimental-results" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="experimental-results">Experimental Results</h2>
<p>The authors demonstrate the efficacy of dropout across several benchmark datasets:</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig4.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;4: figure 4"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig4.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: figure 4
</figcaption>
</figure>
</div>
</div></div><ul>
<li>MNIST: Error rates are reduced from 1.60% (standard neural network) to 0.95% using dropout with additional max-norm regularization.</li>
<li>CIFAR-10 and CIFAR-100: Dropout networks outperform previous methods, with an error reduction to 12.6% on CIFAR-10 and 37.2% on CIFAR-100.</li>
<li>TIMIT (Speech Data): Dropout reduces the phone error rate from 23.4% to 21.8%, showing significant improvements over non-dropout models.</li>
<li>ImageNet: Dropout helps achieve state-of-the-art results in image classification tasks, significantly lowering the top-5 error rate.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig5.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;5: image samples"><img src="https://orenbochman.github.io/reviews/2014/Dropout/fig5.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: image samples
</figcaption>
</figure>
</div>
</div></div></section>
<section id="advantages-of-dropout" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-dropout">Advantages of Dropout</h2>
<ul>
<li>Generality: Dropout works across a variety of architectures, including fully connected, convolutional, and recurrent neural networks.</li>
<li>Ease of Use: Dropout is simple to implement, requiring only one additional hyperparameter (the dropout rate, typically 0.5 for hidden layers).</li>
<li>Compatibility with Other Methods: Dropout can be combined with techniques like unsupervised pretraining, max-norm regularization, and momentum, further improving model performance.</li>
</ul>
</section>
<section id="limitations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<ul>
<li>Increased Training Time: Dropout can significantly slow down training, typically requiring 2-3 times more iterations to converge.</li>
<li>Tuning of Hyperparameters: While simple, the dropout rate must be carefully selected, and higher learning rates and momentum are generally required for optimal performance.</li>
<li>Application-Specific Benefits: Although dropout improves performance in vision and speech recognition tasks, the improvements in certain domains like text classification (e.g., Reuters RCV1 dataset) are less pronounced. <sup>1</sup></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I think this is because if we drop a few words from a sentence a reader can often guess them from the context and redundancy in natural languages.</p></div></div></section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The paper introduces dropout as a powerful and simple regularization technique that significantly reduces overfitting in deep neural networks. Dropout provides a computationally efficient method to approximate model averaging and works across a range of architectures and tasks, achieving state-of-the-art results on several benchmarks. However, it comes at the cost of increased training time, and some tuning is required for optimal performance.</p>
<p>Dropout represents a substantial advancement in neural network training, and its adoption has since become widespread in the deep learning community.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The Paper</h2>
<p><a href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">link to the paper</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="embeded paper"><embed src="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" class="col-page" style="width:8.5in;height:11in"></a></p>
<figcaption>embeded paper</figcaption>
</figure>
</div>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks}
    from {Overfitting}},
  url = {https://orenbochman.github.io/reviews/2014/Dropout/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Dropout: A Simple Way to Prevent Neural
Networks from Overfitting.”</span> <a href="https://orenbochman.github.io/reviews/2014/Dropout/">https://orenbochman.github.io/reviews/2014/Dropout/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2014/Dropout/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
</item>
<item>
  <title>ViT — An Image is worth 16x16 words: Transformers for Image Recognition at scale</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2020/ViT/</link>
  <description><![CDATA[ 





<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</p>
<p>— <span class="citation" data-cites="DBLP:journals/corr/abs-2010-11929">(Dosovitskiy et al. 2020)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-DBLP:journals/corr/abs-2010-11929" class="csl-entry">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>CoRR</em> abs/2010.11929. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div></div></div>
</section>
<section id="see-also" class="level2">
<h2 class="anchored" data-anchor-id="see-also">See also</h2>
<ul>
<li><a href="https://arxiv.org/abs/2010.11929">Paper</a></li>
<li><a href="https://github.com/google-research/vision_transformer">Code - Vision Transformer and MLP-Mixer Architectures</a></li>
<li><a href="https://iclr.cc/virtual/2021/oral/3458">ICLR - Video &amp; Slides</a></li>
<li><a href="https://research.google/pubs/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/">Blog post</a></li>
<li>Third-party reviews:
<ul>
<li><a href="https://www.youtube.com/@YannicKilcher">Review by Yannic Kilcher</a></li>
<li><a href="https://www.youtube.com/watch?v=aD-D8-D-ZyY">Sahil Khose</a></li>
<li><a href="https://www.youtube.com/watch?v=DVoHvmww2lQ">AI Coffee Break with Letitia</a></li>
<li><a href="https://medium.com/@ManishChablani/vision-transformer-vit-an-image-is-worth-16x16-words-transformers-for-image-recognition-at-a4bd5c6f17a7">Manish Chablani — Review</a></li>
</ul></li>
</ul>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {ViT -\/-\/- {An} {Image} Is Worth 16x16 Words: {Transformers}
    for {Image} {Recognition} at Scale},
  url = {https://orenbochman.github.io/reviews/2020/ViT/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“ViT --- An Image Is Worth 16x16 Words:
Transformers for Image Recognition at Scale.”</span> <a href="https://orenbochman.github.io/reviews/2020/ViT/">https://orenbochman.github.io/reviews/2020/ViT/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2020/ViT/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2020/ViT/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Temporal Abstraction in Reinforcement Learning with the Successor Representation</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2023/temporal-abstraction/</link>
  <description><![CDATA[ 





<p>This paper review is an extended introduction to temporal abstraction using options. It covers lots of advanced concepts in reinforcement learning that were introduced in Doina’s Precup’s talk in the Coursera Specialization on Reinforcement Learning by Martha White and Adam White. The paper is a deep dive into the topic of options and the successor representation. It is a long paper with lots of advanced concepts and algorithms. The paper is a great resource for anyone interested in reinforcement learning and temporal abstraction.</p>
<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Ever since I saw <span class="citation" data-cites="Martha2022SubTasks">(White 2022)</span> the video lecture on subtasks by Martha White about learning tasks in parallel. However the video does not address the elephant in the room - how to discover the options.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Martha2022SubTasks" class="csl-entry">
White, Martha. 2022. <span>“Developing Reinforcement Learning Agents That Learn Many Subtasks.”</span> <em>YouTube</em>. <a href="https://www.youtube.com/watch?v=GmGL9cVfJG4&amp;t=6s">https://www.youtube.com/watch?v=GmGL9cVfJG4&amp;t=6s</a>.
</div><div id="fig-subtasks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/GmGL9cVfJG4" title="Martha White - Developing Reinforcement Learning Agents that Learn Many Subtasks?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-subtasks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
</figcaption>
</figure>
</div><div id="fig-option-discovery" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-option-discovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/m9gYmYEYuIs" title="Marlos C. Machado - Representation-driven Option Discovery in Reinforcement Learning?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-option-discovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Talk at Cohere.AI by Marlos C. Machado on Representation-driven Option Discovery in Reinforcement Learning. He discusses the Representation-driven Option Discovery (ROD) cycle and how it can be used to discover options in reinforcement learning. The talk covers much of the material in the paper as well as some more recent follow up work.
</figcaption>
</figure>
</div><div id="ref-Machado2024Cohere" class="csl-entry">
———. 2024. <span>“Representation-Driven Option Discovery in Reinforcement Learning.”</span> <em>YouTube</em>. <a href="https://www.youtube.com/watch?v=m9gYmYEYuIs">https://www.youtube.com/watch?v=m9gYmYEYuIs</a>.
</div></div>

<p>This is a hefty paper 70 pages with 8 algorithms many figures and citations from research spanning thirty years. It is filled to the brim with fascinating concepts that are developed by the authors but builds on lots of work by earlier researchers. It may seem to cover a niche topic but <span class="citation" data-cites="Machado2024Cohere">(c.f. Machado 2024, time 773)</span> makes an eloquent argument that this paper deals with a fundamental question of where options come and if we put aside the jargon for a second we are trying to capture a form of intelingence that includes elements of generalization, planning, problem solving, learning at a level much closer what we are familiar with. And these familiar forms of mental abstractions much harder to consider in the context of Supervised or Unsupervised learning which lack the ineraction with the environment that is the hallmark of reinforcement learning.</p>
<p>I came about this paper by accident. I a quick summary before I realized how long it was and I put out my first pass, and I hope to flesh it including perhaps a bit of code.</p>
<p>I’ve been developing my own ideas regarding the creation and aggregation of options in reinforcement learning. My thinking to date has been different. I am exploring a Bayesian based tasks. I’ve considered creating shared semantics via emergent symbolic semantics and looking at a number of composability mechanisms for state, language and of options including using hierarchial bayesian models. While working on coding environments for this subjects a search led to this amazing paper!</p>
<p>In <span class="citation" data-cites="reinforcement_2024">(<strong>reinforcement_2024?</strong>)</span> Marlos C. Machado, has given a talk that explains many of the complex ideas within this paper. This talk is available on YouTube.</p>
<p>Marlos C. Machado is a good speaker and going over that paper and the video certainly helps to understand the challenges of temporal abstractions as well as the solutions that the paper proposes.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TL;DR
</div>
</div>
<div class="callout-body-container callout-body">
<p>This paper posits that <strong>successor representations</strong>, which encode states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstraction like options if these are not known.</p>
<p>Options are a powerful form of temporal abstraction that allows agents to make predictions and to operate at different levels of abstraction within an environment in ways idiosyncratic of human approach to tackle many problems. One of the key questions has been how to discover good options. The paper presents a rather simple yet powerful answer to this.</p>
<p>Here is a lighthearted Deep Dive into the paper generated by notebooklm</p>
<audio controls="1">
<source src="deepdive.wav" data-external="1" type="audio/mpeg">

</audio>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Abstract
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation, which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the successor representation can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent’s representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we also discuss how the successor representation allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the successor representation to combine them. Our results shed light on important design decisions involved in the definition of options and demonstrate the synergy of different methods based on the successor representation, such as eigenoptions and the option keyboard.</p>
<p>— <span class="citation" data-cites="machado2023temporal">(Machado et al. 2023)</span></p>
</blockquote>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"></div></section>
<section id="the-review" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-review">The Review</h2>
<section id="introduction-1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="introduction-1">Introduction</h3>

<div class="no-row-height column-margin column-container"><div id="fig-option-framework" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-option-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/GntIVgNKkCI" title="DeepHack.RL: Doina Precup - Temporal abstraction in reinforcement learning" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-option-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Doina Precup’s Talk at DeepHack.RL on Temporal abstraction in reinforcement learning covers both the intro and the background material on options.
</figcaption>
</figure>
</div></div><p>In this section, the authors introduce the reinforcement learning problem and the options framework. Next they discuss the benefits of using options and highlight the option discovery problem. Next they present the successor representation (SR) as a representation learning method that is conducive to option discovery, summarizing its use cases and connecting it to neuroscience They go on to describe the paper’s focus on temporally-extended exploration and the use of eigenoptions and covering options. The finnish the introduction by highlight the paper’s evaluation methodology and the use of toy domains and navigation tasks for clarity and intuition.</p>
<p>In <span class="citation" data-cites="Doina2017DeepHack">(Precup 2017)</span> Doina precup gives a talk on temporal abstraction in reinforcement learning. This talk covers both the introduction and the background material on options and is on YouTube.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Doina2017DeepHack" class="csl-entry">
Precup, Doina. 2017. <span>“DeepHack.RL: Temporal Abstraction in Reinforcement Learning.”</span> <em>YouTube</em>. <a href="https://www.youtube.com/watch?v=GntIVgNKkCI">https://www.youtube.com/watch?v=GntIVgNKkCI</a>.
</div></div></section>
</section>
<section id="background" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<ul>
<li><p>Defines the reinforcement learning problem, covering Markov Decision Processes, policies, value functions, and common algorithms such as Q-learning.</p></li>
<li><p>Introduces the options framework <span class="citation" data-cites="Sutton1999BetweenMA">(Richard S. Sutton, Precup, and Singh 1999)</span>, <span class="citation" data-cites="precup2000temporal">(Precup and Sutton 2000)</span>, defining its components (initiation set, policy, termination condition), execution models, and potential benefits.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-Sutton1999BetweenMA" class="csl-entry">
Sutton, Richard S., Doina Precup, and Satinder Singh. 1999. <span>“Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning.”</span> <em>Artificial Intelligence</em> 112 (1): 181–211. https://doi.org/<a href="https://doi.org/10.1016/S0004-3702(99)00052-1">https://doi.org/10.1016/S0004-3702(99)00052-1</a>.
</div><div id="ref-precup2000temporal" class="csl-entry">
Precup, Doina, and Richard S. Sutton. 2000. <span>“Temporal Abstraction in Reinforcement Learning.”</span> PhD thesis, University of Massachusetts Amherst.
</div></div><p>An option <img src="https://latex.codecogs.com/png.latex?%5Comega%20%5Cin%20%5COmega"> is a 3-tuple</p>
<p><span id="eq-6"><img src="https://latex.codecogs.com/png.latex?%0A%5Comega%20=%20%3CI_%5Comega%20,%20%5Cpi_%5Comega%20,%20%5Cbeta_%5Comega%20%3E%20%5Cqquad%0A%5Ctag%7B1%7D"></span></p>
<ul>
<li>where
<ul>
<li><img src="https://latex.codecogs.com/png.latex?I_%5Comega%20%E2%8A%86%20S"> the options’s initiation set,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Comega%20:%20%5Cmathcal%7BS%7D%20%5Ctimes%20%5Cmathcal%7BA%7D%20%5Crightarrow%20%5B0,%201%5D"> the option’s policy, such that <img src="https://latex.codecogs.com/png.latex?%5Csum_a%20%5Cpi_%5Comega%20(%C2%B7,%20a)%20=%201">, and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta_%5Comega%20:%20%5Cmathcal%7BS%7D%20%5Crightarrow%20%5B0,%201%5D"> the option’s termination condition <sup>1</sup></li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;the probability that option <img src="https://latex.codecogs.com/png.latex?%CF%89"> will terminate at a given state.</p></div></div></section>
<section id="a-framework-for-option-discovery-from-representation-learning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-framework-for-option-discovery-from-representation-learning">A Framework for Option Discovery from Representation Learning</h2>

<div class="no-row-height column-margin column-container"><div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures" width="250px">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;4: Representation-driven Option Discovery (ROD) cycle [@Machado2019EfficientEI]. The option discovery algorithms discussed in this paper can be seen as instantiating this cycle. The incoming arrow to Collect samples depicts the start of the process. The arrow from Define option to Option set highlights the output generated by the ROD cycle. Note that other generated artifacts can also be used by the agent outside the ROD cycle, such as the learned representation."><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/fig_1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Representation-driven Option Discovery (ROD) cycle <span class="citation" data-cites="Machado2019EfficientEI">(Machado 2019)</span>. The option discovery algorithms discussed in this paper can be seen as instantiating this cycle. The incoming arrow to Collect samples depicts the start of the process. The arrow from Define option to Option set highlights the output generated by the ROD cycle. Note that other generated artifacts can also be used by the agent outside the ROD cycle, such as the learned representation.
</figcaption>
<div id="ref-Machado2019EfficientEI" class="csl-entry">
Machado, Marlos C. 2019. <span>“Efficient Exploration in Reinforcement Learning Through Time-Based Representations.”</span> <em>Revue De Geographie Alpine-Journal of Alpine Research</em>. PhD thesis.
</div></figure>
</div></div><ul>
<li><p>Introduces a general framework for option discovery driven by representation learning, named the <strong>Representation-driven Option Discovery</strong> (ROD) cycle.</p>
<ul>
<li>Collect samples</li>
<li>Learn a representation</li>
<li>Derive an intrinsic reward function from the representation</li>
<li>Learn to maximize intrinsic reward</li>
<li>Define option</li>
</ul></li>
<li><p>Presents a step-by-step explanation of the ROD cycle, outlining its iterative and constructivist nature, as depicted in the figure.</p></li>
</ul>
</section>
<section id="the-successor-representation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-successor-representation">The Successor Representation</h2>

<div class="no-row-height column-margin column-container"><div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures" width="250px">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;5: Example similar to Dayans 1993 of the SR, w.r.t. the uniform random policy, of state A (left). Consider a navigation task where the agent has access to its (x, y) coordinates. It is tempting to use some distance metric such as the Euclidean distance to define distance between states. However, if one considers the gray tiles to be walls, an agent in point A can reach point B much quicker than point C. The SR captures this distinction, ensuring that, in this representation, point A is closer to point B than it is to point C. The plots of the SR were generated using a discretization of the grid, where each tile is a state. Red represents larger values while blue represents smaller values (states that are temporally further away). Recall the SR of a state, in the tabular case, is an |S|-dimensional representation, thus allowing us to depict it as a heatmap over the state space."><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/fig_3.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Example similar to Dayans 1993 of the SR, w.r.t. the uniform random policy, of state A (left). Consider a navigation task where the agent has access to its (x, y) coordinates. It is tempting to use some distance metric such as the Euclidean distance to define distance between states. However, if one considers the gray tiles to be walls, an agent in point A can reach point B much quicker than point C. The SR captures this distinction, ensuring that, in this representation, point A is closer to point B than it is to point C. The plots of the SR were generated using a discretization of the grid, where each tile is a state. Red represents larger values while blue represents smaller values (states that are temporally further away). Recall the SR of a state, in the tabular case, is an |S|-dimensional representation, thus allowing us to depict it as a heatmap over the state space.
</figcaption>
</figure>
</div></div><ul>
<li><p>Presents the successor representation (SR) as a method to extract representations from observations.</p></li>
<li><p>In <span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 4.1)</span> Defines the SR in the tabular setting, explaining its ability to capture environment dynamics by encoding expected future state visitation, as shown in Equation 7 and the figure.</p></li>
</ul>
<div class="no-row-height column-margin column-container"></div><p><span id="eq-7"><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi_%7B%5Cpi%7D(s,s')%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi,p%7D%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20%20%CE%B3%5Et%20%5Cmathbb%7B1%7D_%7B%5C%7BS_t=s'%7D%5C%7D%20%7C%20%7BS_0%20=%20s%20%7D%20%5Cqquad%0A%5Ctag%7B2%7D"></span></p>

<div class="no-row-height column-margin column-container"><div id="fig-4" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures" width="250px">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;6: First three PVFs in the (a) four-room domain. Gray squares represent walls and white squares represent accessible states. Four actions are available: up, down, right, and left. The transitions are deterministic and the agent is not allowed to move into a wall. (b-d) These plots depict the first, second, and third eigenvectors associated with each state. The axes are rotated for clarity. The bottom left corner of the four-room domain is the state closer to the reade"><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/fig_4.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: First three PVFs in the (a) four-room domain. Gray squares represent walls and white squares represent accessible states. Four actions are available: up, down, right, and left. The transitions are deterministic and the agent is not allowed to move into a wall. (b-d) These plots depict the first, second, and third eigenvectors associated with each state. The axes are rotated for clarity. The bottom left corner of the four-room domain is the state closer to the reade
</figcaption>
</figure>
</div></div><ul>
<li>Discusses the estimation of the SR with temporal-difference learning, its connection to general value functions <sup>2</sup>, and its relationship to the transition probability matrix Equation 9.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;“the SR can be estimated from samples with temporal-difference learning methods <span class="citation" data-cites="sutton1988learning">(Richard S. Sutton 1988)</span>, where the reward function is replaced by the state occupancy”</p><div id="ref-sutton1988learning" class="csl-entry">
Sutton, Richard S. 1988. <span>“Learning to Predict by the Methods of Temporal Differences.”</span> <em>Machine Learning</em> 3: 9–44.
</div></div></div><p><span id="eq-8"><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi(S_t,j)%20%5Cleftarrow%20%5Chat%7B%5CPsi%7D(S_t,%20j)%20+%20%5Ceta%20%5B%5Cmathbb%7B1%7D_%7B%5C%7BS_t=j%5C%7D%7D%20+%20%5Cgamma%20%5Chat%7B%5CPsi%7D(S_%7Bt+1%7D,%20j)%20%E2%88%92%20%5Chat%7B%5CPsi%7D(S_t,%20j)%5D%0A%5Ctag%7B3%7D"></span></p>
<p><span id="eq-9"><img src="https://latex.codecogs.com/png.latex?%0A%5CPsi_%5Cpi%20=%20%20%5Csum_%7Bt=0%7D%5E%5Cinfty%20(%5Cgamma%20P_%5Cpi)%5Et%20=%20(I-%5Cgamma%20P_%5Cpi)%5E%7B-1%7D%5Cqquad%0A%5Ctag%7B4%7D"></span></p>
<ul>
<li><p>Introduces successor features (SFs) as a generalization of the SR to the function approximation setting, extending the definition of the SR to arbitrary features, as shown in Equation 11.</p></li>
<li><p>Highlights the relationship between the SR and PVFs.</p></li>
</ul>
</section>
<section id="temporally-extended-exploration" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="temporally-extended-exploration">Temporally-Extended Exploration</h2>
<ul>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 5)</span> discusses temporally-extended exploration with options and its potential to enhance exploration in RL.</p></li>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 5.1)</span> introduces eigenoptions, which are options defined by the eigenvectors of the SR. &gt; “Eigenoptions are options defined by the eigenvectors of the SR.2 Each eigenvector assigns an intrinsic reward to every state in the environment.”</p>
<ul>
<li><p>Explains the concept of eigenoptions using the four-room domain as an example (Figure 5).</p></li>
<li><p>Describes how to learn eigenoptions’ policies using an intrinsic reward function derived from the eigenvectors of the SR.</p></li>
<li><p>Defines the initiation set and termination condition of eigenoptions, as shown in Equation 16.</p></li>
<li><p>Presents Theorem 1, which guarantees the existence of at least one terminal state for each eigenoption.</p></li>
</ul></li>
<li><p>Introduces covering options, which are point options defined by the bottom eigenvector of the graph Laplacian and aim to minimize the environment’s cover time.</p>
<ul>
<li>Explains the concept of covering options using the four-room domain (Figure 7).</li>
<li>Describes how to learn covering options’ policies using a simplified intrinsic reward function.</li>
<li>Defines the initiation set and termination condition of covering options.</li>
<li>Highlights the iterative nature of covering option discovery, where options are added one by one at each iteration.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"></div></section>
<section id="evaluation-of-temporally-extended-exploration-with-options" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evaluation-of-temporally-extended-exploration-with-options">Evaluation of Temporally-Extended Exploration with Options</h2>
<ul>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 6)</span>Evaluates eigenoptions and covering options in the context of temporally-extended exploration.</p></li>
<li><p>Uses the diffusion time, a task-agnostic metric, to quantify exploration effectiveness by measuring the expected number of decisions required to navigate between states.</p></li>
<li><p>Presents results comparing eigenoptions and covering options:</p>
<ul>
<li>Shows that both approaches can reduce diffusion time in the four-room domain when computed in closed form (Figure 8).</li>
<li>Discusses the impact of different initiation set sizes, highlighting the trade-off between avoiding sink states and ensuring option availability.</li>
</ul></li>
<li><p>Investigates the effectiveness of eigenoptions and covering options in an online setting:</p>
<ul>
<li>Demonstrates the robustness of eigenoptions to online SR estimation (Figure 11).</li>
<li>Reveals the challenges of using covering options online, particularly due to their restrictive initiation set and reliance on a single eigenvector (Figure 12).</li>
</ul></li>
<li><p>Explores the impact of using options on reward maximization in a fixed task:</p>
<ul>
<li>Shows that eigenoptions can accelerate reward accumulation when used for temporally-extended exploration in Q-learning (Figure 9).</li>
<li>Observes that covering options do not consistently improve reward maximization in this setting, likely due to their sparse initiation set.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fig-9" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./fig_9.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;7: figure 9"><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/fig_9.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-9-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: figure 9
</figcaption>
</figure>
</div></div>
</section>
<section id="iterative-option-discovery-with-the-rod-cycle" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="iterative-option-discovery-with-the-rod-cycle">Iterative Option Discovery with the ROD Cycle</h2>
<ul>
<li><p><span class="citation" data-cites="machado2023temporal">(Machado et al. 2023, sec. 7)</span> introduces Covering Eigenoptions (CEO), a new algorithm that performs multiple iterations of the ROD cycle for option discovery.</p></li>
<li><p>Describes the steps of CEO, emphasizing its use of eigenoptions and online SR estimation, as outlined in Algorithm 2.</p></li>
<li><p>Demonstrates the benefits of multiple ROD cycle iterations with CEO, showing a significant reduction in the number of steps needed to visit all states in the four-room domain (Figure 14).</p></li>
<li><p>Illustrates the behavior of CEO over multiple iterations, highlighting its ability to progressively discover more complex options (Figure 14).</p></li>
<li><p>Combining Options with the Option Keyboard</p></li>
<li><p>Discusses the option keyboard as a way to combine existing options to create new options without additional learning, potentially expanding the agent’s behavioral repertoire.</p></li>
<li><p>Introduces Generalized Policy Evaluation (GPE) and Generalized Policy Improvement (GPI), generalizations of standard policy evaluation and improvement.</p></li>
<li><p>Explains how to use GPE and GPI to synthesize options from linear combinations of rewards induced by eigenvectors of the SR, as outlined in Algorithm 3.</p></li>
<li><p>Combining Eigenoptions with the Option Keyboard</p></li>
<li><p>Demonstrates the synergy of eigenoptions and the option keyboard.</p></li>
<li><p>Presents a qualitative analysis of options generated by combining eigenoptions with the option keyboard (Figures 16 and 17).</p></li>
<li><p>Shows that the option keyboard leads to a combinatorial explosion of new options, as evidenced by the number of unique options generated (Figure 18).</p></li>
<li><p>Demonstrates the diversity of options generated by the option keyboard through heatmaps showing the frequency of termination in different states (Figures 19 and 20).</p></li>
<li><p>Presents a quantitative analysis of the diffusion time induced by eigenoptions combined with the option keyboard, highlighting the improvement in exploration effectiveness (Figures 21 and 22).</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-machado2023temporal" class="csl-entry">
Machado, Marlos C., Andre Barreto, Doina Precup, and Michael Bowling. 2023. <span>“Temporal Abstraction in Reinforcement Learning with the Successor Representation.”</span> <em>Journal of Machine Learning Research</em> 24 (80): 1–69. <a href="http://jmlr.org/papers/v24/21-1213.html">http://jmlr.org/papers/v24/21-1213.html</a>.
</div></div></section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">Related Work</h2>
<ul>
<li>Discusses option discovery methods for planning and bottleneck options, including those based on spectral clustering and normalized cuts.</li>
<li>Mentions other option discovery methods for temporally-extended exploration, such as diffusion options.</li>
<li>Outlines extensions of the SR and option discovery methods to function approximation, including linear and non-linear function approximation techniques.</li>
<li>Discusses the connection of the SR to other reinforcement learning concepts, such as proto-value functions, slow-feature analysis, and dual representations.</li>
<li>Highlights the relationship of the SR to neuroscience, including its potential to model hippocampal place fields and grid cell activations.</li>
<li>Mentions the SR’s application to explaining human behavior and decision-making.</li>
</ul>
</section>
<section id="conclusion" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li>Highlights the potential of using the SR as the main substrate for temporal abstraction, pointing out promising directions for future work.</li>
<li>Emphasizes the importance of iterative option discovery and its role in building intelligent agents capable of continual learning and complex skill acquisition.</li>
</ul>
<p>Here are the successor representations algorithms from the paper:</p>

<div class="no-row-height column-margin column-container"><div id="fig-11" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./alg_1.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;8: successor representations algorithms"><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/alg_1.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-11-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: successor representations algorithms
</figcaption>
</figure>
</div></div><p>Next is the covering eigenoptions algorithm:</p>

<div class="no-row-height column-margin column-container"><div id="fig-12" class="quarto-float quarto-figure quarto-figure-center anchored" data-group="figures">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="./alg_2.png" class="lightbox" data-gallery="figures" title="Figure&nbsp;9: Covering Eigenoptions algorithm"><img src="https://orenbochman.github.io/reviews/2023/temporal-abstraction/alg_2.png" class="img-fluid figure-img" width="250"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-12-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Covering Eigenoptions algorithm
</figcaption>
</figure>
</div></div></section>
<section id="study-guide-for-the-paper" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="study-guide-for-the-paper">Study guide for the paper</h2>
<ol type="1">
<li>What is an option in reinforcement learning?</li>
</ol>
<p>We actually took the definition from the paper. But here is another from the video. This is perhaps a more elegant definition. It comes from []</p>
<p>In reinforcement learning, an <strong>option</strong> is a temporally extended course of actions that allows an agent to operate at different levels of abstraction within an environment. Options are a form of temporal abstraction that enables agents to make predictions and execute actions over extended time horizons, providing a way to structure and organize the agent’s behavior. <img src="https://latex.codecogs.com/png.latex?%0Av_%7B%5Cpi,%5Cbeta%7D%5E%7Bc,z%7D(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi,%5Cbeta%7D%20%5Cleft%5B%20%5Csum_%7Bj=1%7D%5EK%20c(S_j)%20+%20%5Cgamma%5E%7BK-1%7D%20z(S_k)%20%7C%20S_0%20=%20s%20%5Cright%5D%20%5Cqquad%20%5Ctext%7Bfor%20all%20%7D%20s%20%5Cin%20S%0A"></p>
<ul>
<li>where
<ul>
<li><img src="https://latex.codecogs.com/png.latex?v_%7B%5Cpi,%5Cbeta%7D%5E%7Bc,z%7D(s)"> is the value function of the option,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi"> is the policy,</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is the termination condition,</li>
<li><img src="https://latex.codecogs.com/png.latex?c"> is the extrinsic reward function, <img src="https://latex.codecogs.com/png.latex?z"> is the intrinsic reward function,</li>
<li><img src="https://latex.codecogs.com/png.latex?S_j"> is the state at time <img src="https://latex.codecogs.com/png.latex?j">,</li>
<li><img src="https://latex.codecogs.com/png.latex?K"> is the duration of the option, and</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor.</li>
</ul></li>
</ul>
<ol type="1">
<li>How can options be used ?</li>
</ol>
<ul>
<li>For planning: you can use eigenvectors of the SR to identify bottleneck, states that are difficult to reach under a random walk, and then use options to guide the agent to those states. c.f. <span class="citation" data-cites="Solway2014OptimalBH">(Solway et al. 2014)</span></li>
<li>For exploration: you can use eigenoptions to encourage exploration by driving the agent toward states that are difficult to reach under a random walk.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-Solway2014OptimalBH" class="csl-entry">
Solway, Alec, Carlos Diuk, N. Córdova, Debbie M. Yee, A. Barto, Y. Niv, and M. Botvinick. 2014. <span>“Optimal Behavioral Hierarchy.”</span> <em>PLoS Computational Biology</em> 10.
</div></div><ol type="1">
<li>Explain the successor representation</li>
</ol>
<p>The <strong>successor representation (SR)</strong> is a method in reinforcement learning that represents states based on their expected future visits under a given policy. It captures the environment’s dynamics by encoding how likely an agent is to visit each state in the future, starting from a particular state.</p>
<ul>
<li>The SR is denoted as <img src="https://latex.codecogs.com/png.latex?%CE%A8_%CF%80">, where <img src="https://latex.codecogs.com/png.latex?%CF%80"> represents the agent’s policy.</li>
<li>It can be estimated online using <strong>temporal difference learning</strong> and generalized to function approximation using <strong>successor features</strong>.</li>
</ul>
<p>The SR allows for <strong>Generalized Policy Evaluation (GPE)</strong>: once the SR is learned, an agent can immediately evaluate its performance under any reward function that can be expressed as a linear combination of the features used to define the SR.</p>
<p>The SR offers a powerful tool for discovering and using temporal abstractions in reinforcement learning, enabling the development of more intelligent and efficient agents. It is used in option discovery methods like eigenoptions and covering options, providing a natural framework for identifying and leveraging temporally extended courses of actions.</p>
<p>Here is a breakdown of the mathematical definition of the SR:</p>
<p><span id="eq-SR"><img src="https://latex.codecogs.com/png.latex?%0A%CE%A8_%5Cpi%20(s,%20s')%20=%20%20%5Cmathbb%7BE%7D_%7B%CF%80,p%7D%20%5B%5Csum%5E%5Cinfty_%7Bt=0%7D%20%CE%B3%5Et%5Cmathbb%7B1%7D_%7BS_t%20=%20s'%7D%20%7C%20S_0%20=%20s%20%5D%20%5Cqquad%0A%5Ctag%7B5%7D"></span></p>
<ul>
<li>Where:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?s,%20s'"> are states in the environment.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is the discount factor, determining the weight of future rewards.</li>
<li>The <strong>expectation (E)</strong> is taken over the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and the transition probability kernel <img src="https://latex.codecogs.com/png.latex?p">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7B1%7D_%7BS_t%20=%20s'%7D"> is an indicator function that equals 1 if the agent is in state s’ at time <img src="https://latex.codecogs.com/png.latex?t">, and 0 otherwise.</li>
</ul></li>
</ul>
<p>This equation calculates the expected discounted number of times the agent will visit state s’ in the future, given that it starts in state s and follows policy π. The SR matrix stores these expected visitations for all state pairs.</p>
<ol start="2" type="1">
<li>Explain what is an eigenoption a covering option and the difference</li>
</ol>
<p><strong>Eigenoptions</strong> and <strong>covering options</strong> are two methods for option discovery in RL that use the successor representation (SR). Options represent temporally extended courses of actions.</p>
<p><strong>Eigenoptions</strong> are options defined by the eigenvectors of the SR.</p>
<ul>
<li>Each eigenvector of the SR assigns an intrinsic reward to every state in the environment.</li>
<li>An eigenoption aims to reach the state with the highest (or lowest) value in the corresponding eigenvector.</li>
<li>They encourage exploration by driving the agent toward states that are difficult to reach under a random walk.</li>
<li>Eigenoptions have a broad initiation set, meaning they can be initiated from many states.</li>
<li>They terminate when the agent reaches a state with a (locally) maximum value in the eigenvector, meaning the agent can’t accumulate more positive intrinsic reward.</li>
<li>Eigenoptions tend to have different durations based on the eigenvalue they are derived from, allowing the agent to operate at different timescales.</li>
</ul>
<p><strong>Covering options</strong> are defined by the bottom eigenvector of the graph Laplacian, which is equivalent to the top eigenvector of the SR under certain conditions.</p>
<ul>
<li>They aim to minimize the environment’s expected cover time, which is the number of steps needed for a random walk to visit every state.</li>
<li>Each covering option connects two specific states: one with the lowest value and one with the highest value in the corresponding eigenvector.</li>
<li>They are discovered iteratively. After each option is discovered, the environment’s graph is updated, and the process repeats.</li>
<li>Covering options have a restrictive initiation set, containing only the single state with the lowest value in the eigenvector.</li>
<li>They terminate when they reach the state with the highest value in the eigenvector.</li>
</ul>
<p>Here’s a table summarizing the <strong>key differences</strong> between eigenoptions and covering options:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 37%">
<col style="width: 53%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Eigenoption</th>
<th style="text-align: left;">Covering Option</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Definition</strong></td>
<td style="text-align: left;">Based on any eigenvector of the SR</td>
<td style="text-align: left;">Based on the bottom eigenvector of the graph Laplacian (equivalent to the top eigenvector of the SR under certain conditions)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Goal</strong></td>
<td style="text-align: left;">Reach states with high/low values in the corresponding eigenvector</td>
<td style="text-align: left;">Minimize environment’s cover time</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Initiation Set</strong></td>
<td style="text-align: left;">Broad (many states)</td>
<td style="text-align: left;">Restrictive (single state)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Termination Condition</strong></td>
<td style="text-align: left;">Reaching a (local) maximum in the eigenvector</td>
<td style="text-align: left;">Reaching the state with the highest value in the eigenvector</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Discovery Process</strong></td>
<td style="text-align: left;">Can be discovered in parallel, in a single iteration</td>
<td style="text-align: left;">Discovered iteratively, one option at a time</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Timescale</strong></td>
<td style="text-align: left;">Different eigenoptions can have different durations</td>
<td style="text-align: left;">Generally have similar durations</td>
</tr>
</tbody>
</table>
<p>Both eigenoptions and covering options can be effective for exploration, but they have different strengths and weaknesses. Eigenoptions can learn more diverse behaviors and capture different timescales, while covering options may be simpler to implement and can guarantee improvement in the environment’s cover time.</p>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="paper"><embed src="./paper.pdf" class="col-page" width="800" height="1000"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>There are a few blog posts that dive deeper into some of the concepts in the paper.</p>
<ul>
<li><a href="https://medium.com/@marlos.cholodovskis/the-representation-driven-option-discovery-cycle-e3f5877696c2">The Representation-driven Option Discovery</a></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {Temporal {Abstraction} in {Reinforcement} {Learning} with the
    {Successor} {Representation}},
  url = {https://orenbochman.github.io/reviews/2023/temporal-abstraction/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“Temporal Abstraction in Reinforcement
Learning with the Successor Representation.”</span> <a href="https://orenbochman.github.io/reviews/2023/temporal-abstraction/">https://orenbochman.github.io/reviews/2023/temporal-abstraction/</a>.
</div></div></section></div> ]]></description>
  <category>review</category>
  <category>Reinforcement learning</category>
  <guid>https://orenbochman.github.io/reviews/2023/temporal-abstraction/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2023/temporal-abstraction/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/FontCLIP/</link>
  <description><![CDATA[ 





<section id="abstract" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Acquiring the desired font for various design tasks can be challenging and requires professional typographic knowledge. While previous font retrieval or generation works have alleviated some of these difficulties, they often lack support for multiple languages and semantic attributes beyond the training data domains. To solve this problem, we present FontCLIP: a model that connects the semantic understanding of a large vision-language model with typographical knowledge. We integrate typography-specific knowledge into the comprehensive vision-language knowledge of a pretrained CLIP model through a novel finetuning approach. We propose to use a compound descriptive prompt that encapsulates adaptively sampled attributes from a font attribute dataset focusing on Roman alphabet characters. FontCLIP’s semantic typographic latent space demonstrates two unprecedented generalization abilities. First, FontCLIP generalizes to different languages including Chinese, Japanese, and Korean (CJK), capturing the typographical features of fonts across different languages, even though it was only finetuned using fonts of Roman characters. Second, FontCLIP can recognize the semantic attributes that are not presented in the training data. FontCLIP’s dual-modality and generalization abilities enable multilingual and cross-lingual font retrieval and letter shape optimization, reducing the burden of obtaining desired fonts.</p>
<p>— <span class="citation" data-cites="Tatsukawa2024Fontclip">(Tatsukawa et al. 2024)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-Tatsukawa2024Fontclip" class="csl-entry">
Tatsukawa, Yuki, I‐Chao Shen, Anran Qi, Yuki Koyama, Takeo Igarashi, and Ariel Shamir. 2024. <span>“FontCLIP: A Semantic Typography Visual‐language Model for Multilingual Font Applications.”</span> <em>Computer Graphics Forum</em> 43 (2). <a href="https://doi.org/10.1111/cgf.15043">https://doi.org/10.1111/cgf.15043</a>.
</div></div></div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {FontCLIP: {A} {Semantic} {Typography} {Visual-Language}
    {Model} for {Multilingual} {Font} {Applications}},
  url = {https://orenbochman.github.io/reviews/2024/FontCLIP/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“FontCLIP: A Semantic Typography
Visual-Language Model for Multilingual Font Applications.”</span> <a href="https://orenbochman.github.io/reviews/2024/FontCLIP/">https://orenbochman.github.io/reviews/2024/FontCLIP/</a>.
</div></div></section></div> ]]></description>
  <category>paper-review</category>
  <category>typography</category>
  <category>vision-language-model</category>
  <category>CLIP</category>
  <category>multilingual</category>
  <category>cross-lingual</category>
  <category>semantic attributes</category>
  <guid>https://orenbochman.github.io/reviews/2024/FontCLIP/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2024/FontCLIP/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/LLM2Vec/</link>
  <description><![CDATA[ 





<section id="tldr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>In <span class="citation" data-cites="behnamghader2024llm2veclargelanguagemodels">(BehnamGhader et al. 2024)</span> the authors consider using LLMs which are mostly decoder only transformers as text encoders. This allows them to use the LLMs for NLP tasks like chunking, NEW and POS. Recall that T5 <span class="citation" data-cites="raffel2020exploring">(Raffel et al. 2020)</span> can do this is a decoder encode model.</p>
<div class="no-row-height column-margin column-container"><div id="ref-behnamghader2024llm2veclargelanguagemodels" class="csl-entry">
BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. <span>“LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders.”</span> <a href="https://arxiv.org/abs/2404.05961">https://arxiv.org/abs/2404.05961</a>.
</div><div id="ref-raffel2020exploring" class="csl-entry">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. <span>“Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.”</span> <em>Journal of Machine Learning Research</em> 21 (140): 1–67.
</div></div></section>
<section id="tricks" class="level2 callout-info">
<h2 class="anchored" data-anchor-id="tricks">Tricks</h2>
<ol type="1">
<li>enabling bidirectional attention,</li>
<li>masked next token prediction, and</li>
<li>unsupervised contrastive learning.</li>
</ol>
</section>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<blockquote class="blockquote">
<p>Large decoder-only language models (LLMs) are the state-of-the-art models on most of today’s NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 4 popular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data (as of May 24, 2024). Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="The 3 steps of LLM2Vec"><img src="https://orenbochman.github.io/reviews/2024/LLM2Vec/fig1.png" class="img-fluid figure-img" alt="The 3 steps of LLM2Vec"></a></p>
<figcaption>The 3 steps of LLM2Vec</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Evaluation on word level tasks"><img src="https://orenbochman.github.io/reviews/2024/LLM2Vec/fig2.png" class="img-fluid figure-img" alt="Evaluation on word level tasks"></a></p>
<figcaption>Evaluation on word level tasks</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Unsupervised results"><img src="https://orenbochman.github.io/reviews/2024/LLM2Vec/fig3.png" class="img-fluid figure-img" alt="Unsupervised results"></a></p>
<figcaption>Unsupervised results</figcaption>
</figure>
</div>
</section>
<section id="the-paper" class="level2">
<h2 class="anchored" data-anchor-id="the-paper">The paper</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="paper.pdf" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="paper"><embed src="paper.pdf" class="col-page" width="800"></a></p>
<figcaption>paper</figcaption>
</figure>
</div>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://github.com/McGill-NLP/llm2vec">code</a></li>
</ul>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/44OukEJyRsU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {LLM2Vec: {Large} {Language} {Models} {Are} {Secretly}
    {Powerful} {Text} {Encoders}},
  url = {https://orenbochman.github.io/reviews/2024/LLM2Vec/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“LLM2Vec: Large Language Models Are Secretly
Powerful Text Encoders.”</span> <a href="https://orenbochman.github.io/reviews/2024/LLM2Vec/">https://orenbochman.github.io/reviews/2024/LLM2Vec/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/LLM2Vec/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2024/LLM2Vec/cover.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>MambaVision A Hybrid Mamba-Transformer Vision Backbone</title>
  <dc:creator>Oren Bochman</dc:creator>
  <link>https://orenbochman.github.io/reviews/2024/mamba-vision/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In <span class="citation" data-cites="hatamizadeh2024mambavision">(Hatamizadeh and Kautz 2024)</span>, the authors apply the State Space Model (SSM) inherent in recently introduced Mamba architecture, <span class="citation" data-cites="gu2023mamba">(Gu and Dao 2023)</span>, for vision tasks. They point out that prior work on using the Mamba architecture for vision was ill-suited these tasks and propose a remedy in the form of a hybrid Mamba-Transformer architecture which they call MambaVision. Thier experiment show that MambaVision outperforms other vision architectures on ImageNet-1K, MS COCO and ADE20K datasets.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gu2023mamba" class="csl-entry">
Gu, Albert, and Tri Dao. 2023. <span>“Mamba: Linear-Time Sequence Modeling with Selective State Spaces.”</span> <em>arXiv Preprint arXiv:2312.00752</em>.
</div></div><p>The paper’s main innovation is <mark>more self-attention blocks in the final layers of the transformer which improves the models ability to capture long-range spatial dependencies</mark>.</p>
</section>
<section id="the-problems-with-mamba-for-vision-tasks" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-problems-with-mamba-for-vision-tasks">The problems with Mamba for vision tasks</h2>
<p>A <em>dilettante reader</em> like myself might be interested in the author’s outline of the shortcomings of the Mamba architecture for vision tasks and earlier attempt in <span class="citation" data-cites="zhu2024vision">(Zhu et al. 2024)</span> <em>vision mamba</em> model which directed thier efforts the right direction.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhu2024vision" class="csl-entry">
Zhu, Lianghui, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. 2024. <span>“Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model.”</span> <em>arXiv Preprint arXiv:2401.09417</em>.
</div></div><div class="page-columns page-full"><blockquote class="blockquote">
<p>… the Mamba’s autoregressive formulation, while effective for tasks requiring sequential data processing, faces limitations in computer vision tasks that benefit from a <strong>full receptive field</strong><sup>1</sup>:</p>
<ol type="1">
<li><p>Unlike sequences where order matters, image pixels do not have a sequential dependency in the same way. Instead, spatial relationships are often local and need to be considered in a more parallel and integrated manner. Hence, this results in inefficiency for processing spatial data</p></li>
<li><p>an autoregressive model like Mamba processes data step-by-step, limiting its ability to capture and utilize global context in one forward pass. In contrast, vision tasks often require understanding the global context to make accurate predictions about local regions</p></li>
</ol>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;seeing the full picture or at least big parts of it</p></div></div></div>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>Vision Mamba (Vim) and others have proposed modifications such as bidirectional SSMs to address lack of global context and spatial understanding. While bidirectional SSMs have the potential to capture more comprehensive context, they introduce significant latency due to the need to process the entire sequence before making predictions. Additionally, the increased complexity can lead to challenges in training, risk of overfitting, and may not always result in better accuracy. Due to these pitfalls, backbones with Vision Transformer (ViT) and Convolutional Neural Network (CNN) architectures still outperform best Mamba-based vision models on different vision tasks. — <span class="citation" data-cites="hatamizadeh2024mambavision">(Hatamizadeh and Kautz 2024, 2)</span></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="ref-hatamizadeh2024mambavision" class="csl-entry">
Hatamizadeh, Ali, and Jan Kautz. 2024. <span>“MambaVision: A Hybrid Mamba-Transformer Vision Backbone.”</span> <em>arXiv Preprint arXiv:2407.08083</em>.
</div></div></div>
<p>To sum all this up - Mamba’s auto regressive nature is well suited to temporal and sequential data like text and speech but is ill suited to handle spatial data like images where order manifests as a hierarchy of spatial neighborhoods which should be processed in parallel. Thus for vision, mamba suffer a loss in the efficiency of the flow of information both locally and globally. As such pre mamba vision models fare better.</p>
<p>The next section outlines the ideas espoused in prior work both pre and post mamba. This section summarizes both the earlier work on computer vision models since the introduction of Transformers and some results since the introduction of the Mamba architecture.</p>
<ul>
<li>Vision Transformer (ViT) <span class="citation" data-cites="dosovitskiy2021imageworth16x16words">(Dosovitskiy et al. 2021)</span> showed that CNNs can be replaced with self-attention, but wasn’t data efficient.</li>
<li>Data-efficient Image Transformer (DeiT) <span class="citation" data-cites="touvron2021training">(Touvron et al. 2021)</span> used distillation to train ViT more efficient.</li>
<li>LeViT model <span class="citation" data-cites="graham2021levit">(Graham et al. 2021)</span> introduced a redesign for MLP and self-attention with a Lenet like pyramid pooling structure.</li>
<li>Cross-covariance Image Transformer (XCiT) <span class="citation" data-cites="ali2021xcit">(Ali et al. 2021)</span> introduced transposed self-attention mechanism more effectively modeling interactions between feature channels.</li>
<li>The Pyramid Vision Transformer (PVT) <span class="citation" data-cites="wang2021pyramid">(Wang et al. 2021)</span> improving efficiency by adopting a hierarchical structure with patch embedding at the start of each stage and spatial dimension reduction.</li>
<li>Swin Transformer <span class="citation" data-cites="liu2021swin">(Liu et al. 2021)</span> used shifted windows to improve the efficiency of self-attention computation.</li>
<li>Twins Transformer <span class="citation" data-cites="chu2021twins">(Chu et al. 2021)</span> featured spatially separable self-attention that significantly enhanced efficiency.</li>
<li>Focal Transformer <span class="citation" data-cites="yang2021focal">(Yang et al. 2021)</span> used a focal mechanism to improve the efficiency of self-attention computation for capturing long-range interactions.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-dosovitskiy2021imageworth16x16words" class="csl-entry">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div><div id="ref-touvron2021training" class="csl-entry">
Touvron, Hugo, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. 2021. <span>“Training Data-Efficient Image Transformers &amp; Distillation Through Attention.”</span> In <em>International Conference on Machine Learning</em>, 10347–57. PMLR.
</div><div id="ref-graham2021levit" class="csl-entry">
Graham, Benjamin, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and Matthijs Douze. 2021. <span>“Levit: A Vision Transformer in Convnet’s Clothing for Faster Inference.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 12259–69.
</div><div id="ref-ali2021xcit" class="csl-entry">
Ali, Alaaeldin, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, et al. 2021. <span>“Xcit: Cross-Covariance Image Transformers.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 20014–27.
</div><div id="ref-wang2021pyramid" class="csl-entry">
Wang, Wenhai, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. 2021. <span>“Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 568–78.
</div><div id="ref-liu2021swin" class="csl-entry">
Liu, Ze, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. <span>“Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 10012–22.
</div><div id="ref-chu2021twins" class="csl-entry">
Chu, Xiangxiang, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. 2021. <span>“Twins: Revisiting the Design of Spatial Attention in Vision Transformers.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 9355–66.
</div><div id="ref-yang2021focal" class="csl-entry">
Yang, Jianwei, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. 2021. <span>“Focal Attention for Long-Range Interactions in Vision Transformers.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 30008–22.
</div></div></section>
<section id="the-mambavision-architecture---macro" class="level2">
<h2 class="anchored" data-anchor-id="the-mambavision-architecture---macro">3.1 The MambaVision Architecture - Macro</h2>
<p>MambaVision has a hierarchical architecture consisting of 4 different stages. The first two stages consist of CNN-based layers for fast feature extraction at higher input resolutions, while stage 3 and 4 include the proposed MambaVision and Transformer blocks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="fig2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Architecture of hierarchical MambaVision"><img src="https://orenbochman.github.io/reviews/2024/mamba-vision/fig2.png" class="img-fluid figure-img" alt="Architecture of hierarchical MambaVision"></a></p>
<figcaption>Architecture of hierarchical MambaVision</figcaption>
</figure>
</div>
<p>The first two blocks in stages 1 and 2</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%20z%20=%20GELU(BN(Conv_%7B3%C3%973%7D(z)))%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Az%20=%20BN(Conv_%7B3%C3%973%7D(%5Chat%20z))%20+%20z%0A"></p>
<p>Where GELU is the Gaussian Error Linear Unit activation function, a modern alternative to the rectified linear unit (ReLU) function, and BN is good old batch normalization layer which transforms the inputs to have zero mean and unit variance which speeds up training.</p>
</section>
<section id="the-mambavision-architecture---micro" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-mambavision-architecture---micro">3.2 The MambaVision Architecture - Micro</h2>

<div class="no-row-height column-margin column-container"><div id="fig-micro" class="quarto-float quarto-figure quarto-figure-left anchored" data-group="my-gallery" data-fig-align="left">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-micro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="fig3.png" class="lightbox" data-gallery="my-gallery" title="Figure&nbsp;1: Architecture of MambaVision block"><img src="https://orenbochman.github.io/reviews/2024/mamba-vision/fig3.png" class="img-fluid quarto-figure quarto-figure-left figure-img" width="300"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-micro-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Architecture of MambaVision block
</figcaption>
</figure>
</div></div><p>The authors redesigned the original Mamba mixer to make it more suitable for vision tasks.</p>
<ol type="1">
<li>regular convolution replaces causal convolution</li>
<li>added a symmetric branch without SSM , consisting of an additional convolution and SiLU activation, to compensate for any content lost due to the sequential constraints of SSMs.</li>
<li>These branches are concatenated and project via a final linear layer.</li>
</ol>
<p>This combination ensures that the final feature representation incorporates both the sequential and spatial information, leveraging the strengths of both branches.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AX_1%20&amp;=%20Scan(%CF%83(Conv(Linear(C,%20%5Cfrac%7BC%7D%7B2%7D%20)(X_%7Bin%7D))))%20%5C%5C%0AX_2%20&amp;=%20%CF%83(Conv(Linear(C,%20%5Cfrac%7BC%7D%7B2%7D%20)(X_%7Bin%7D)))%20%5C%5C%0AX_%7Bout%7D%20&amp;=%20Linear(%20%5Cfrac%7BC%7D%7B2%7D%20,%20C)(Concat(X_1,%20X_2))%20%5C%5C%0A%5Cend%7Balign*%7D%0A"></p>
</section>
<section id="ablation-studies" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ablation-studies">Ablation Studies</h2>
<p>Section 4 the experiment looks at MambaVision’s performance in image classification as well as other downstream tasks like, object detection, instance segmentation and semantic segmentation tasks. The authors note that the model was equipped with the model with specialized heads for different tasks and required fine tuning the original model. I am a somewhat critical of calling this the performance on downstream tasks when we are talking about models with different layers that were fine tuned using different optimizers on task specific datasets.</p>
<p>The results section outline an <strong>ablation study</strong><sup>2</sup> used to identify the optimal way to integrate the Vision Transformer (ViT) with the Mamba architecture.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;investigating the effects of removing parts of a model</p></div></div><p>As usual, the authors provide a family of models with different sizes to gauge the performance characteristics for scaling the model.</p>
<p>The various models</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://arxiv.org/pdf/2407.08083">paper</a></li>
<li><a href="https://github.com/NVlabs/MambaVision">code</a></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bochman,
  author = {Bochman, Oren},
  title = {MambaVision {A} {Hybrid} {Mamba-Transformer} {Vision}
    {Backbone}},
  url = {https://orenbochman.github.io/reviews/2024/mamba-vision/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bochman" class="csl-entry quarto-appendix-citeas">
Bochman, Oren. n.d. <span>“MambaVision A Hybrid Mamba-Transformer Vision
Backbone.”</span> <a href="https://orenbochman.github.io/reviews/2024/mamba-vision/">https://orenbochman.github.io/reviews/2024/mamba-vision/</a>.
</div></div></section></div> ]]></description>
  <guid>https://orenbochman.github.io/reviews/2024/mamba-vision/</guid>
  <pubDate>Mon, 10 Mar 2025 11:54:46 GMT</pubDate>
  <media:content url="https://orenbochman.github.io/reviews/2024/mamba-vision/cover.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
