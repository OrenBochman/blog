@article{Buntine1991BayesianB,
  title={Bayesian Back-Propagation},
  author={Wray L. Buntine and Andreas S. Weigend},
  journal={Complex Syst.},
  year={1991},
  volume={5},
  url={https://api.semanticscholar.org/CorpusID:14814125}
}

@article{baum1988size,
  title={What size net gives valid generalization?},
  author={Baum, Eric and Haussler, David},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society: series B (methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}

@article{lang1990time,
  title={A time-delay neural network architecture for isolated word recognition},
  author={Lang, Kevin J and Waibel, Alex H and Hinton, Geoffrey E},
  journal={Neural networks},
  volume={3},
  number={1},
  pages={23--43},
  year={1990},
  publisher={Elsevier}
}

@article{mackay1991bayesian,
  title={Bayesian modeling and neural networks},
  author={MacKay, David JC},
  journal={PhD thesis, Dept. of Computation and Neural Systems, CalTech},
  year={1991},
  url={https://www.inference.org.uk/mackay/thesis.pdf}

}

@article{morgan1989generalization,
  title={Generalization and parameter estimation in feedforward nets: Some experiments},
  author={Morgan, Nelson and Bourlard, Herv{\'e}},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{mozer1988skeletonization,
  title={Skeletonization: A technique for trimming the fat from a network via relevance assessment},
  author={Mozer, Michael C and Smolensky, Paul},
  journal={Advances in neural information processing systems},
  volume={1},
  year={1988}
}

@article{mozer1989using,
  title={Using relevance to reduce network size automatically},
  author={Mozer, Michael C and Smolensky, Paul},
  journal={Connection Science},
  volume={1},
  number={1},
  pages={3--16},
  year={1989},
  publisher={Taylor \& Francis},
  context={regularization via loss term for complexity of weights}
}

@article{nowlan2018simplifying,
    author = {Nowlan, Steven J. and Hinton, Geoffrey E.},
    title = "{Simplifying Neural Networks by Soft Weight-Sharing}",
    journal = {Neural Computation},
    volume = {4},
    number = {4},
    pages = {473-493},
    year = {1992},
    month = {07},
    abstract = "{One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.4.473},
    url = {https://doi.org/10.1162/neco.1992.4.4.473},
    eprint = {https://direct.mit.edu/neco/article-pdf/4/4/473/812332/neco.1992.4.4.473.pdf},
}




@TechReport{plaut1986experiments,
  address	= {Pittsburgh, PA},
  author	= {Plaut, D. C. and Nowlan, S. J. and Hinton, G. E.},
  institution	= {Carnegie--Mellon University},
  number	= {CMU-CS-86-126},
  title		= {Experiments on learning by back-propagation},
  year		= {1986},
  url	= {https://ni.cmu.edu/~plaut/papers/pdf/PlautNowlanHinton86TR.backprop.pdf}
}


@article{weigend1990predicting,
  title={Predicting the future: A connectionist approach},
  author={Weigend, Andreas S and Huberman, Bernardo A and Rumelhart, David E},
  journal={International journal of neural systems},
  volume={1},
  number={03},
  pages={193--209},
  year={1990},
  publisher={World Scientific}
}


@phdthesis{lecun-87,
original =  "orig/lecun-87.tiff",
author  =   "LeCun, Y.",
title   =   "Modeles connexionnistes de l'apprentissage (connectionist learning models)",
school  =   "Universit\'e P. et M. Curie (Paris 6)",
year    =   "1987",
month   =   "June",
}

@inproceedings	{lecun-89,
original =      "orig/lecun-89.tiff",
key	=	"Lecun" ,
author	=	"LeCun, Y." ,
title	=	"Generalization and Network Design Strategies",
booktitle=	"Connectionism in Perspective",
address	=	"Zurich, Switzerland" ,
year	=	"1989" ,
editor  =       "Pfeifer, R. and Schreter, Z. and Fogelman, F. and Steels, L.",
publisher =     "Elsevier",
note    =       "an extended version was published as a technical report of the University of Toronto"
}

@inproceedings     {lecun-90c,
original =    "orig/lecun-90c.ps.gz",
author  =       "LeCun, Y. and Boser, B.  and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel,  L. D.",
title   =       "Handwritten digit recognition with a back-propagation network",
booktitle=      "Advances in Neural Information Processing Systems (NIPS 1989)", 
volume =        "2",
address =       "Denver, CO", 
year    =       "1990",
editor  =       "Touretzky, David",
publisher=      "Morgan Kaufman",
video = "<a href='http://youtu.be/FwFduRA_L6Q'>Video</a>",
url= {https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
abstract= {We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.},
}

@inproceedings     {lecun-90b,
original =      "orig/lecun-90b.ps.gz",
author  =       "LeCun, Yann and Denker, J. S. and Solla, S. and Howard, R. E. and Jackel, L. D.",
title   =       "Optimal Brain Damage",
booktitle=      "Advances in Neural Information Processing Systems (NIPS 1989)", 
volume =        "2",
address =       "Denver, CO", 
year    =       "1990",
editor  =       "Touretzky, David",
publisher=      "Morgan Kaufman",
abstract= {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes 
          for adapting the size of a neural network. By removing unimportant weights from a network, several 
          improvements can be expected: better generalization, fewer training examples required, and improved 
          speed of learning and/or classification. The basic idea is to use second-derivative information to make a 
          tradeoff between network complexity and training set error. Experiments confirm the usefulness of the 
          methods on a real-world application.},
url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
context= {regularization via loss term for complexity of weights}
}