
- Introduction
  - Describes the importance of inductive inference in human cognition, highlighting the challenge of generalizing from sparse data.
  - Mentions that traditional accounts of induction emphasize either statistical learning or domain-specific knowledge.
  - Argues for a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.
- Theory-based Bayesian Models
  - Introduces the core idea of theory-based Bayesian models, where prior knowledge shapes the hypothesis space and probability distributions.
  - Notes the distinction between domain-general statistical mechanisms and domain-specific knowledge representations.
  - Presents Bayes' rule as a framework for combining prior knowledge (P(h|T)) and likelihood (P(x|h,T)) to calculate posterior probabilities (P(h|x,T)).
  - Emphasizes the role of domain theory in generating hypothesis spaces, prior probabilities, and likelihoods, forming a probabilistic version of intuitive theories.
- Learning Names for Things
  - Describes the classic category learning task as an abstraction of word learning for object kinds.
  - Presents limitations of traditional statistical models for word learning that assume simple notions of categories and label-category mappings.
  - Introduces a Bayesian model for word learning that utilizes a tree-structured taxonomy of objects as the hypothesis space of word meanings.
  - Discusses how the model explains children's generalization patterns, showing that generalization follows a gradient according to taxonomic distance, which sharpens with more examples.
  - Mentions the "size principle" as a general principle of Bayesian learning, leading to the preference for smaller, more specific hypotheses with increasing evidence.
- Reasoning about Hidden Properties
  - Introduces the task of property induction, where learners must generalize a novel property observed in one or more categories to other categories.
  - Presents a theory-based Bayesian model for property induction in the domain of biological species, assuming a tree-structured taxonomy and a mutation process for property generation.
  - Highlights the model's ability to account for generalizations of "blank" biological properties, surpassing models based on generic knowledge representations.
  - Discusses the need for different priors to account for generalizations of different kinds of predicates, such as anatomical, behavioral, and disease properties.
  - Presents examples of Bayesian models using different structured graphs and stochastic processes to capture the specific inductive biases for different property types.
- Learning Theories to Support Property Induction
  - Addresses the problem of learning the taxonomic tree structure from raw data (species-property pairs) using Bayesian inference.
  - Explains how the best tree structure maximizes the likelihood of the observed properties, reflecting the smooth variation of features over the tree.
  - Discusses the role of a "taxonomic principle" as an abstract domain principle that guides the learning process.
  - Notes that the Bayesian framework allows for learning abstract domain principles themselves, choosing the best structural form (e.g., tree, linear order, clusters) based on the trade-off between complexity and fit to the data.
  - Emphasizes the ability to incorporate explicit instructions at any level of abstraction within the hierarchical Bayesian framework, which can lead to dramatic changes in inferences.
- Causal Learning and Reasoning
  - Highlights the central role of causal cognition in intuitive theories, suggesting that causality is often seen as defining a theory.
  - Presents the idea of causal graphical models as a particular kind of structured probabilistic model used to make inferences about observable events.
  - Discusses the learning of causal models, contrasting bottom-up statistical cues with top-down approaches relying on abstract domain knowledge about causal mechanisms.
  - Introduces the concept of "causal grammars" as probabilistic models that generate causal graphical models based on an ontology and a set of causal laws.
  - Provides examples of theory-based Bayesian causal induction, showing how abstract knowledge about risk factors, diseases, and symptoms can guide the learning of causal networks in a medical domain, and how domain principles such as the "activation law" can explain causal inferences in the "blicket detector" paradigm.
- Learning Abstract Causal Theories
  - Acknowledges the open question of how abstract causal principles or "framework theories" are learned.
  - Mentions the infinite relational model (IRM) as an example of a Bayesian model that can infer the number of classes, class membership, and relationships between classes from data.
  - Notes that while IRM can learn some simple framework knowledge, more powerful methods for learning in probabilistic logical systems are needed to account for richer causal theories.
- Conclusion
  - Reiterates the potential of the theory-based Bayesian framework for understanding human cognition, emphasizing the integration of both sophisticated inference processes and knowledge representations.
  - Acknowledges the limitations of current models and the need to address algorithmic and psychological plausibility issues.
  - Concludes that probabilistic inference over hierarchies of increasingly abstract and flexibly structured representations is a crucial idea for explaining inductive learning and reasoning.