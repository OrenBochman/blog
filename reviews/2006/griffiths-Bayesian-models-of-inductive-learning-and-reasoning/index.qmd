---
date: 2022-03-10
title: "ðŸ§  Theory-Based Bayesian Models of Inductive Learning and Reasoning"
subtitle: "paper review"
description: "How do humans make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world?"
categories: [review,induction]
keywords: [review,paper,tutorial]
image: /images/lit-review-cover.jpg
bibliography: "./bibliography.bib"
---

![cover](/images/lit-review-cover.jpg){.column-margin .nolightbox}


<!-- VIDEOS GO HERE 

::: {.column-margin #fig-subtasks}
{{< video https://youtu.be/GmGL9cVfJG4
    title='Martha White - Developing Reinforcement Learning Agents that Learn Many Subtasks?' >}}

Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
:::

-->

<!-- A QUOTE by someone more famous than the author of the paper for context, add highlighting for emphasis, verse is a nice touch!  -->
> "The ideal market completely disregards those spikesâ€”but a realistic model cannot." [Mandelbrot highlights the inadequacy of models ignoring extreme price movements, emphasizing the need for a framework that can accommodate them.]{.mark}


<!-- LEDE personal context why I reviewed this source 

{{< lipsum 1 >}}

-->

::: callout-note
## TL;DR - Too Long; Didn't Read about Inductive Learning 

![Inductive Learning in a nutshell](/images/in_the_nut_shell_coach_retouched.jpg)

Inductive learning can be used a framework for learning to reason about the world

<!-- 1. What are the research questions? -->
<!-- 2. What are the main findings? -->
<!-- 3. In historical context why was this important? -->
:::

<!-- 
Here is a lighthearted Deep Dive into the paper:

<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">
</source>
</audio>
-->

### Abstract

> Inductive inference allows humans to make powerful generalizations from sparse data when learning about word meanings, unobserved properties, causal relationships, and many other aspects of the world. Traditional accounts of induction emphasize either the power of statistical learning, or the importance of strong constraints from structured domain knowledge, intuitive theories or schemas. We argue that both components are necessary to explain the nature, use and acquisition of human knowledge, and we introduce a theory-based Bayesian framework for modeling inductive learning and reasoning as statistical inferences over structured knowledge representations.
>
> --- [@tenenbaum2006theory] <!--CITATION HERE-->

## Glossary

This paper uses lots of big terms so let's break them down so we can understand them better

{{< include _glossary.qmd >}}

## Outline

{{< include _outline.qmd >}}


## Reflection on the paper and beyond.

This seems to be one of the way to capture the issues of aggregation rules for signals in the transitions from simple to complex signaling systems. 

In reality these are two different things

1. learning a model from some state space.
2. aggregating symbols using a rule like a grammar.

The two problems are related but only tangentially. This is one reason that transitioning from the simple Lewis signaling model to the more complex ones is so difficult.

## The paper

![paper](./paper.pdf){.col-page width="800px" height="1000px"}