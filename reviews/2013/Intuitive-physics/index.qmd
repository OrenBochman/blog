---
date: 2025-03-31
title: "Simulation as an engine of physical scene understanding"
subtitle: "paper review"
description: "Model for a cognitive mechanism similar to computer engines that simulate rich physics in video games and graphics, but that uses approximate, probabilistic simulations to make robust and fast inferences in complex natural scenes where crucial information is unobserved."
categories: [draft,review]
keywords: [review,paper,intuitive physics engine]
image: /images/lit-review-cover.jpg
bibliography: "./bibliography.bib"
---

![cover](/images/lit-review-cover.jpg){.column-margin .nolightbox}

<!-- 
- TODO:
  - [x] folder-name
  - [x] date
  - [x] title / subtitle
  - [ ] description
  - [x] categories
  - [x] keywords
  - [x] paper - download rename
  - [x] abstract
  - [x] citation
  - [x] outline
  - [x] glossary
  - [x] podcast
  - [ ] Lede paragraph
  - [ ] tl;dr
  - [x] figures
  - [x] reflections
  - [ ] video - locate/remove
  - [ ] quote
  - [x] remove draft
-->


![mindmap](mindmap.png){.column-margin}

<!-- VIDEOS GO HERE -->

::: {.column-margin #vid-01}
{{< video https://www.youtube.com/watch?v=TFyAEHk5asY&ab_channel=MITCBMM
    title='Computational Models of Cognition: Part 1' >}}

Josh Tenenbaum, MIT BMM Summer Course 2018 Computational Models of Cognition: Part 1
:::

::: {.column-margin #vid-02}
{{< video https://www.youtube.com/watch?v=lD2tkuRm8fc&ab_channel=MITCBMM
    title='Computational Models of Cognition: Part 2' >}}

Josh Tenenbaum, MIT BMM Summer Course 2018 Computational Models of Cognition: Part 2
:::

::: {.column-margin #vid-03}
{{< video https://www.youtube.com/watch?v=VPT73em9Nuc&ab_channel=MITCBMM
    title='Computational Models of Cognition: Part 3' >}}

Josh Tenenbaum, MIT BMM Summer Course 2018 Computational Models of Cognition: Part 3 
:::

<!-- A QUOTE by someone more famous than the author of the paper for context, add highlighting for emphasis, verse is a nice touch!  
> "The ideal market completely disregards those spikes—but a realistic model cannot." [Mandelbrot highlights the inadequacy of models ignoring extreme price movements, emphasizing the need for a framework that can accommodate them.]{.mark}

-->

<!-- LEDE personal context why I reviewed this source -->

I considered adding a step in `pretraining` of RL agents to capture semantics of Newtonian Physics so they could learn to interpret a sentence like "the egg hit the wall and then it broke." as a sequence of events that are related by a causal relation some physics and some common knowledge. This paper suggest that this might be a good idea and goes further to suggest that the agent might be equipped with a elementary physics engine that would allow it to simulate the physical world and learn from its interactions with it. We see that language models can help agents plan complex task much better and faster than by learning from pixel data alone. So giving access to a physics engine might be a good idea, particularly as this paper suggest that using few shot interactions with the engine approximates how we humans intuit the physical world.

The experiment in this paper seem to be a great addition to the curriculum I had in mind for the agent (block world and sokoban  which are 2d worlds). However @fig-3 and @fig-4 suggest using MDPs with 3D objects and more complex interactions.

::: callout-note
## TL;DR - Too Long; Didn't Click on intuitive physics

![intuitive physics  in a nutshell](/images/in_the_nut_shell_coach_retouched.jpg)

Here's an outline summarizing the research paper:

1.  **Research Questions:**
    *   What are the **computational underpinnings of rapid physical inferences** that allow people to understand and interact with the physical world?
    *   More specifically, the research aims to develop and test a **computational framework for intuitive physical inference** suitable for everyday scene understanding, focusing on reasoning about multiple incompletely observed objects interacting in complex ways and making coarse, approximate, short-term predictions.
    *   The authors investigate whether a model based on an "**intuitive physics engine**" (IPE), using approximate, probabilistic simulations, can explain human physical scene understanding.

2.  **Main Findings:**
    *   The IPE model **fits data from five distinct psychophysical tasks**, including judging whether a tower will fall, predicting the direction of a fall, and determining which colored blocks are more likely to fall off a bumped table with obstacles.
    *   The model **captures several illusions and biases** in human physical judgments, such as the perception of delicately balanced objects as unstable, which a deterministic ground truth physics model cannot explain.
    *   **Simpler, non-simulation-based accounts** relying on geometric features alone consistently fared worse at predicting people's judgments than the IPE model.
    *   People's judgments appear to be consistent with having been based on a **relatively small number of stochastic simulation samples** (roughly three to seven).

3.  **In historical context why was this important?**
    *   Early studies of intuitive physics suggested that human intuitions are **fundamentally incompatible with Newtonian mechanics** based on errors in explicit reasoning about simple systems. However, later work revised this interpretation, showing that intuitions are often accurate in concrete dynamic contexts.
    *   While the idea of the brain building "**mental models**" to support inference through mental simulations had been proposed, these systems **had not attempted to engage with physical scene understanding** in a quantitative and probabilistic way, focusing more on qualitative or propositional representations suited for symbolic reasoning.
    *   The work challenged purely **model-free, data-driven approaches** in computer vision as a complete explanation for physical scene understanding, suggesting that simulation-based reasoning plays a crucial role.

:::

Here is a lighthearted Deep Dive into the paper:

<audio controls="1">
<source src="podcast.mp3" data-external="1" type="audio/mpeg">
</source>
</audio>

### Abstract

> In a glance, we can perceive whether a stack of dishes will topple, a branch will support a child’s weight, a grocery bag is poorly packed and liable to tear or crush its contents, or a tool is firmly attached to a table or free to be lifted. Such rapid physical inferences are central to how people interact with the world and with each other, yet their computational underpinnings are poorly understood. We propose a model based on an “intuitive physics engine,” a cognitive mechanism similar to computer engines that simulate rich physics in video games and graphics, but that uses approximate, probabilistic simulations to make robust and fast inferences in complex natural scenes where crucial information is unobserved. This single model fits data from five distinct psychophysical tasks, captures several illusions and biases, and explains core aspects of human mental models and common-sense reasoning that are instrumental to how humans understand their everyday world
>
> --- [@battaglia2013simulation] <!--CITATION HERE-->

## Glossary

{{< include _glossary.qmd >}}

## Outline

![Everyday scenes, activities, and art that evoke strong physical intuitions. (A) A cluttered workshop that exhibits many nuanced physical properties. (B) A 3D object-based representation of the scene in A that can support physical inferences based on simulation. (C) A precarious stack of dishes looks like an accident waiting to happen. (D) A child exercises his physical reasoning by stacking blocks. (E) Jenga puts players' physical intuitions to the test. (F) "Stone balancing" exploits our powerful physical expectations (Photo and stone balance by Heiko Brinkmann).](./fig_01.png){#fig-1 .column-margin width="250px" group="figures"}

![(A) The IPE model takes inputs (e.g., perception, language, memory, imagery, etc.) that instantiate a distribution over scenes (1), then simulates the effects of physics on the distribution (2), and then aggregates the results for output to other sensorimotor and cognitive faculties (3). (B) Exp. 1 (Will it fall?) tower stimuli. The tower with the red border is actually delicately balanced, and the other two are the same height, but the blue-bordered one is judged much less likely to fall by the model and people. (C) Probabilistic IPE model (x axis) vs. human judgment averages (y axis) in Exp. 1. See Fig. S3 for correlations for other values of σ and φ. Each point represents one tower (with SEM), and the three colored circles correspond to the three towers in B. (D) Ground truth  (nonprobabilistic) vs. human judgments (Exp. 1). Because it does not represent uncertainty, it cannot capture people's judgments for a number of our stimuli, such as the red-bordered tower in B. (Note that these cases may be rare in natural scenes, where configurations tend to be more clearly stable or unstable and the IPE would be expected to correlate better with ground truth than it does on our stimuli.)](./fig_02.png){#fig-2 .column-margin width="250px" group="figures"}

![(A) Exp. 2 (In which direction?). Subjects viewed the tower (Upper), predicted the direction in which it would fall by adjusting the white line with the mouse, and received feedback (Lower). (B) Exp. 2: Angular differences between the probabilistic IPE model’s and subjects’ circular mean judgments for each tower (blue points), where 0 indicates a perfect match. The gray bars are circular histograms of the differences. The red line indicates the tower in A. (C) The same as B, but for the ground truth model. (D) Exp. 3 (Will it fall?: mass): State pair stimuli (main text). Light blocks are green, and heavy ones are dark. (E) Exp. 3: The mass-sensitive IPE model’s vs. people’s judgments, as in Fig. 2C. The black lines connect state pairs. Both model and people vary their judgments similarly within each state pair (lines' slopes near 1). (F) Exp. 4: The mass-insensitive model vs. people. Here the model cannot vary its judgments within state pairs (lines are near vertical). (G) Exp. 4 (In which direction?: mass): State pair stimuli. (H) Exp. 4: The mass-sensitive IPE model’s vs. people’s judgments, as in B. The black lines connect state pairs. The model’s and people’s judgments are closely matched within state pairs (short black lines). (I) Exp. 4: The mass-insensitive IPE model vs. people. Here again, the model cannot vary its judgments per state pair (longer black lines)](./fig_03.png){#fig-3 .column-margin width="250px" group="figures"}

![Exp. 5 (Bump?). (A) Scene stimuli, whose tables have different obstacles (T0–T4). (B) In the uncued bump condition, subjects were not informed about the direction from which the bump would strike the scene; in the cued bump conditions, a blue arrowhead indicated the bump’s direction. (C) The disk plot shows IPE model predictions per bump direction (angle) and φ (radius) for the stimulus in the image; the blue arrowheads/arcs indicate the range of bump angles simulated per bump cue, and the green circle and arrowheads represent the uncued condition. Inset bar graphs show the model’s and people’s responses, per cue/condition. (D) The same block configuration as in C, with different obstacles (T1). (E–J) IPE model’s (x axis) vs. people’s (y axis) mean judgments (each point is one scene, with SEM). The lines in G–J indicate cue-wise pairs. Each subplot show one cue condition and IPE model variant (correlations in parentheses, with P value of difference from full IPE): (E) Uncued, full IPE. (F) Uncued, obstacle insensitive (model assumes T0). (G) Cued, full IPE. (H) Cued, obstacle insensitive. (I) Cued, cue insensitive (model averages over all bump angles). (J) Cued, obstacle and cue insensitive.](./fig_04.png){#fig-4 .column-margin width="250px" group="figures"}


{{< include _outline.qmd >}}


## Reflections <!-- Criticism & Own Thoughts  -->

### Bibliography

To start these sources cited in this paper by Josh Tenenbaum and his group that I seem to warrant some attention:

11. Sanborn AN, Mansinghka VK, Griffiths TL (2013) Reconciling intuitive physics and
Newtonian mechanics for colliding objects. Psychol Rev 120(2):411–437.

12. Gerstenberg T, Goodman N, Lagnado D, Tenenbaum J (2012) Noisy newtons: Unifying process and dependency accounts of causal attribution. Proceedings of the 34th Conference of the Cognitive Science Society, eds Miyake N, Peebles D, Cooper RP (Cognitive Science Society, Austin, TX), pp 378–383.

13. Smith KA, Vul E (2013) Sources of uncertainty in intuitive physics. Top Cogn Sci 5(1):185–199.

14. Smith K, Battaglia P, Vul E (2013) Consistent physics underlying ballistic motion prediction. Proceedings of the 35th Conference of the Cognitive Science Society, eds Knauff M, Pauen M, Sebanz N, Wachsmuth I (Cognitive Science Society, Austin, TX), pp 3426–3431.

21. Tenenbaum JB, Kemp C, Griffiths TL, Goodman ND (2011) How to grow a mind: Statistics, structure, and abstraction. Science 331(6022):1279–1285.

23. Vul E, Goodman N, Griffiths T, Tenenbaum J (2009) [One and done? Optimal decisions from very few samples.]{.mark} Proceedings of the 31st Conference of the Cognitive Science Society, eds Taatgen N, van Rijn H (Cognitive Science Society, Austin, TX), pp 66–72.

25. Vul E, Frank M, Alvarez G, Tenenbaum J (2009) Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model. Adv NIPS 22:1955–1963.

The paper also has some further sources on **child development** and **language** that may be wroth a quick scan:

1. Marr D (1982) Vision (Freeman, San Francisco).

2. Baillargeon R (2002) The acquisition of [physical knowledge in infancy]{.mark}: A summary in eight lessons. Blackwell Handbook of Childhood Cognitive Development (Blackwell, Oxford), Vol 1, pp 46–83.

4. Talmy L (1988) Force dynamics in [language]{.mark} and cognition. Cogn Sci 12(1):49–100.

15. Craik K (1943) The Nature of Explanation (Cambridge Univ Press, Cambridge, UK).

16. Gentner D, Stevens A (1983) Mental Models (Lawrence Erlbaum, Hillsdale, NJ).

17. Hegarty M (2004) Mechanical reasoning by mental simulation. Trends Cogn Sci 8(6):280–285.

18. Johnson-Laird P (1983) Mental Models: Towards a Cognitive Science of Language, Inference, and Consciousness (Cambridge Univ Press, Cambridge, UK), Vol 6.

19. De Kleer J, Brown J (1984) A qualitative physics based on confluences. Artif Intell 24(1):7–83.

30. Téglás E, et al. (2011) [Pure reasoning in 12-month-old infants as probabilistic inference]{.mark}. Science 332(6033):1054–1059

### Ontology

So my own thought based on a course on Cognitive A.I. that followed Winston's Classic textbook on Artificial Intelligence, were that to empower an agent that could learn a signaling system, to acquire a more general purpose language, it would be necessary to expose it to multiple MDP in which it would learn man different things and be able to generalize across them. The immediate ideas seemed to be a variant of "Blocksworld" where the agent would be tasked to manipulate blocks and other objects in a 2D world. This would be a good scenario in which it would learn to develop preposition or at least to represent the spatial relations between objects. A second idea was to follow up with games like "Sokoban". In which it would benefit from symbolic representation of the objects and their relations and might also learn
more about physical constraints.

All this suggested that that such agents might benefit from a curriculum consisting of:

- "Logic Structure" - for logic, sets, relations.
- "Narrative Structure" - events, turning points, exposition, point of view, dialogue, description, and action. This would allow it to tell stories and explain plans that are grounded in a symbolic/linguistic representation of the MDP. 
- "Physics" - relations and common sense reasoning about the physical world. e.g. "the egg hit the wall and then it broke." 
- "Causation"
- "Probability and Uncertainty" 
- "Game Theory" - strategic reasoning about other agents, and the ability to represent the MDP in terms of a game. Note that there is an interpretation of probability in terms of game theory. So perhaps this module might subsume the Probability and Uncertainty as well as the Causation module. Making agents play in game theory scenarios would be very easy and quick part of the curriculum to implement and also a relatively simple to integrate with language.  The module can be used to develop a symbolic representation.
  - Personal pronouns, Social dynamics, Interests, Incentives, Coalitions, Alliances, Trust, Reputation, Deception, Manipulation, and Exploitation, Cooperation, Coordination, Competition, Conflict, Cheap talk, are all semantics that might arise in these modules. 
  - Utility and welfare functions. 
  - Micro-Economics can also be built using game theory.



One of the problems I had been considering was how to facilitate transfer learning between these different domains. Mapping state space to symbolic representation seems to be too much a hit or miss approach. Using a reductionist language approach seems to be key. 
I had thought of mechanisms like temporal abstractions, like the options framework, and generalized values functions, from RL but also Hierarchical Bayesian models for things like logic. 

The big problem seems to be that I was thinking about the agent learning just one complex signaling system using a linear function approximator with many features drawn from all the above curriculum. 

Another approach seems to be to use an abstraction that I have called at one time lobe formation. This is a process in which the agent learns to group together similar features and to iteratively learn more abstract representations of the MDP in terms of symbols, relations and constraints.

### Reincarnation and Metamorphosis

I realized another interesting point though. In different incarnation of this kind of agent it would need to handle different states and action spaces for different MDPs. So it should be able to learn many models from previous MDPs and then be able to generate Hypothesis by instantiating different models based on the different model it has. Lobe formation suggest that initially very simple models might appear more useful, but as experiences accumulates in a new MDP it might be able to use more sophisticated model. Ideally it should be able to use distributional semantics and a grounding procedure to match the new MDP to what it knows from previous MDPs. e.g. features for approximation, temporal and spatial abstractions, strategic reasoning, and augmenting the linguistic representation with a good symbolic representation for the new MDP. In fact it seems that the agent might perhaps assemble a hierarchical model for the MDP based on building blocks with established semantics (symbolic and distributional)

There are likely many different ways to do this reincarnation. But it seems that organisms undergoing metamorphosis seem to be able to make use of previous experiences despite a radical change in their body and brain. 

One idea is that we have a encoder that can be used to bootstrap a more sophisticated decoder. Another idea is that we can use evidence to handle model selection and switch to better models. 

Another based on sugarscape agents is that we might want to incorporate these models into the agent's DNA, i.e. the agent would have access to the different models, thier priors, semantics, and mappings to states and actions. This would all be arranged using a forrest structure. The agent might then use a tree, multiple trees or create a random forest based on its older models to match the current MDP to the many different inductive biases it has learned.

## The paper

![paper](./paper.pdf){.col-page width="800px" height="1000px"}

