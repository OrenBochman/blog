
---
title: "Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space"
subtitle: "paper review"
bibliography: "./bibliography.bib"
image: cover.jpg
categories: [draft,review]
keywords: [review]
draft: true
---


## TL;DR

This paper updates the word2vec skip-gram to a **Multi-Sense Skip-Gram** (MSSG) which performs word-sense discrimination and embedding simultaneously. It improving its training time, while assuming a specific number of senses for each word. In addition a second variant **Non-Parametric Multi-Sense Skip-Gram** (NP-MSSG) the number of senses can vary for each differnt word.

### Abstract

> There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly
all this work, however, assumes a single vector per word typeâ€”ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present
an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense
discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new
state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less
than 6 hours
>
> --- [@neelakantan2015efficient]

![](fig_1.png)

![](fig_2.png)

![](alg_1.png)

![](tab_2.png)

## The paper
![paper](./paper.pdf){.col-page width="800px" height="1000px"}
