
---
title: "Natural Language Does Not Emerge *naturally* in Multi-Agent Dialog"
subtitle: "paper review"
bibliography: "./bibliography.bib"
image: cover.jpg
categories: [review,language evolution, MARL]
keywords: [language evolution, information theory]
draft: true
---


::: {.callout-note}
## TL-DR Emergent Languages In a Nutshell

![Emergent Languages In a Nutshell](/images/in_the_nut_shell_coach_retouched.jpg)


This time the they did not like the emergent language so they decided to add constraints - this is just on of the many ways to engineer an outcome in game theory especially if you don't understand the many possible equilibria. 

Theoretical and numerous papers on Emergent languages indicate the many challenging in getting rudimentary languages to emerge. How hard they are to interpret and how different they are from natural languages. In reality if a new english or turkish emerged I'm pretty sure it would be even harder to decode. 

:::

### Abstract

> A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision!  
> In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not  interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’, despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.
> --- [@kottur2017natural]


## Review

Intialy, I was very critical of this paper. It was presented at the 2017 Conference on Empirical Methods in Natural Language Processing, and got an award for best short paper. I got to is from a citation indicating an path to compositionality via 

Even though its easier to drive the emmergence of language using EGG fro example I don't recall lots of papers claiming this is easy to do, get basic stuff like generalization, compositionality or interpretability. Perhaps I should look at the papers they are citing. But off hand many seem rather familiar already! Is this perhaps a strawman argument rather than admiting that so far this is a hard problem to solve and you try many times you will get a few working results?

Anyhow lets try and salvage something from this paper, as it has been cited a few times.

![](./fig_1.png){#fig-1 .column-margin width="250px" group="figures"}

{{< lipsum 2 >}}


![](./fig_2.png){#fig-2 .column-margin width="250px" group="figures"}

{{< lipsum 2 >}}

![](./table_1.png){#tbl-1 .column-margin width="250px" group="figures"}


{{< lipsum 2 >}}

![](./table_2.png){#tbl-2 .column-margin width="250px" group="figures"}

{{< lipsum 2 >}}

## The paper

![paper](./paper.pdf){.col-page width="800px" height="1000px"}
