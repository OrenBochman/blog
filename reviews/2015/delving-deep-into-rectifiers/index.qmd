---
date: "2016-12-10"
title: "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"
subtitle: "paper review"
keywords: [deep learning, neural networks, image recognition, computer vision, ImageNet, PReLU, MSRA, rectifiers, activation functions] 
bibliography: ./bibliography.bib
draft: true
---

## TL;DR

In this paper [@DBLP:journals/corr/HeZR015] the authors, Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun, proposed a new activation function, the Parametric Rectified Linear Unit (PReLU), which learns the parameters of the rectifier during training. This method improves the accuracy of deep neural networks and reduces the training time. The authors also introduced a new weight initialization method, the "MSRA" initialization, which further improves the training of deep neural networks. The authors demonstrated that their method outperforms the previous state-of-the-art on the ImageNet classification task, achieving a top-5 error rate of 3.57%, surpassing human-level performance.


## Abstract

> Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.
>
> --- [@DBLP:journals/corr/HeZR015]

## The paper

![paper](./paper.pdf){.col-page width="800px" height="1000px"}
