---
title: "Inception - Going Deeper with Convolutions"
date: "2016-12-10"
subtitle: "paper review"
bibliography: ./bibliography.bib
keywords: [ "convolutional neural networks", "deep learning", "image recognition", "computer vision", Inception , GoogLeNet] 
draft: true
image: cover.jpg

---

## TL;DR

In this paper [@Szegedy_2015_CVPR] the authors, Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich from Google, Mountain View, CA, USA,  investigated the effect of increasing the convolutional network depth on the accuracy in the large-scale image recognition setting. The authors show that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 22 weight layers.

The inception architecture is based on the idea of using multiple filters of different sizes in parallel. This allows the network to capture features at different scales. The authors also introduced a new type of layer called the inception module which is a combination of different types of filters. The inception module is used to reduce the number of parameters in the network while increasing its depth.

::: callout-warning

This paper lays out lots of ideas and explain the logic behind them. It is a good reference for understanding the inception architecture but many of the notions are broader in scope than just CNNs. On the other hand this paper has lots of engineering details indicating how the authors made many optimizations to make the network work in practice. The result called "Google-LeNet" is a very efficient network that can be trained on a single GPU but also a monster to understand.

For example the network has three SoftMax classifiers are interspaced within the model. This seems insane as the fully connected and the massive softmax require the most resources to train and compute. The short answer is that for this deep model there is a vanishing gradient problem and the authors found that the intermediate classifiers helped to propagate the gradients back through the network. By using classifiers at different depths the authors were able to train a very deep network without the gradients vanishing. The early classifiers can be thought as a feature extractor and the final classifier as a classifier. However the earlier classifiers contain information no longer present in the final classifier. This is a common issue in CNNs and the authors found a clever way to deal with it. Resnets are a more modern solution to this problem. But the inception architecture may be useful for cases where we may want to 

:::


The authors also used a new type of regularization called "label smoothing" which helps to prevent overfitting.

## Abstract 

> We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.
>
> --- [@Szegedy_2015_CVPR]


## Review

{{<lipsum>}}

## The paper

![paper](./paper.pdf){.col-page width="800px" height="1000px"}

## Resources
