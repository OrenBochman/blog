
---
title: "RAND-WALK: A latent variable model approach to word embeddings"
subtitle: "paper review"
bibliography: "./bibliography.bib"
image: cover.jpg
categories: [draft,review,distributional semantics,word embeddings,nlp]
keywords: [review]
draft: true
---

My interest in this paper is to try to get a better mathematical intuition on word embeddings. This model is mentioned in the follow up papers on word sense embeddings. A bayesian formulation of the log-linear model can be useful to develop better representations for Bayesian or RL agents that can derive distributional semantics of thier environment.

::: {.callout-note}

## Some questions

some questions before reading this paper about word embeddings:

Word embeddings are defined in live in a high-dimensional space with (300-2000) dimensions, how do they overcome the curse of dimensionality? ^[Hint: perhaps they exist in much lower-dimensional manifolds within the high-dimensional space]


:::


## TL;DR

This paper introduces a Bayesian dynamic log-linear topic generative model for word embeddings. This paper is long (33 pages) and contains mathematical derivations. 


### Abstract

>Semantic word embeddings represent the meaning of a word via a vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, and have hand-tuned hyperparameters and reweighting methods.
This paper proposes a new generative model, a dynamic version of the log-linear topic model of Mnih and Hinton (2007). The methodological novelty is to use the prior to compute closed form expressions for word statistics. This provides a theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as well as some hyperparameter choices. It also helps explain why low-dimensional semantic embeddings contain linear algebraic structure that allows solution of word analogies, as shown by Mikolov et al. (2013a) and many subsequent papers. Experimental support is provided for the generative model assumptions, the most important of which is that latent word vectors are fairly uniformly dispersed in space.
>
> --- [@citation] <!--THE PAPER'S CITATION GOES HERE-->



## The Review 

This paper posits a model that can explain why popular word embedding methods like PMI, word2vec, and GloVe work as well as the reason why these embeddings can solve word analogies.


let's recall PMI (Pointwise Mutual Information) for two words $w_1$ and $w_2$:

$$
PMI(w_1,w_1) = \log \frac{P(w_1,w_2)}{P(w_1)P(w_2)}
$$

{{< lipsum 2 >}}

![figure 1](./fig_1.jpg){#fig-1 .column-margin width="250px" group="figures"}

{{< lipsum 2 >}}

## The paper

![paper](./paper.pdf){.col-page width="800px" height="1000px"}
