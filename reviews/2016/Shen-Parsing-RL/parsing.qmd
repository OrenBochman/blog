---
title: "Dependency Parsing using Deep Learning"
date:  "2021-05-01"
draft: true
---

## Questions

Explain the two approaches to parsing

1. Constituency Grammar uses phrase structure grammar to organize words into nested constituents.

1. Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the head (or governor, superior, regent) to the dependent (or modifier, inferior, subordinate)

Mathematically this means that there is an preorder relation on the words of a sentence. If needed we add a [fake root element]{.mark} which makes the relation a tree.

Also from a category theory point of view we are subordinate the the more specific Linguistically relations unifying them under the aegis of order to get a more homogenous view of the sentence 

Understating this definition:

1. Can there be more than one relation between two words?
1. Are there cases where a word has the same relations with multiple parents? 
1. Can words modify multiple words ?
    > [[Jack [and Jill]] ascended] 
    here we resolve this issue by 
1. How do we choose the root of a sentence or decicide to add a fake one?





what is the transition based approach from [@Nivre2003]

can we visualize the textual parse tree as a tree

(went
  (nsubj (Jack)
    (cc and)
    (conj Jill))
  (prep up
    (pobj (det the) (hill))))

(S
  (NP (NP (NNP Jack)) (CC and) (NP (NNP Jill)))
  (VP (VBD went)
    (PP (IN up)
      (NP (DT the) (NN hill)))))



1. **Learning**: Given a training set D of sentences annotated with dependency graphs, induce a parsing model M that can be used to parse new sentences.ven a training set D of sentences annotated with dependency graphs, induce a parsing model M that can be used to parse new sentences.

2. **Parsing**: Given a parsing model $M$ and a sentence $S$, derive the optimal dependency graph $D$ for $S$ according to $M$. 





