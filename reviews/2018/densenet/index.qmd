---
title: "DenseNet: Densely Connected Convolutional Networks"
subtitle: "paper review"
bibliography: "./bibliography.bib"
image: cover.jpg
categories: [draft,review]
keywords: [review]
draft: true
---

## TL;DR

{{< lipsum 1 >}} <!--PLACE THE TL;DR HERE SHORT AND OPINIONATED-->


### Abstract

> Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections—one between each layer and its subsequent layer—our network has $\frac{L(L+1)}{2}$ direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.
>
> --- [@huang2017densely] <!--THE PAPER'S CITATION GOES HERE-->

## Outline

- Introduction
  - Describes the vanishing-gradient problem in deep CNNs.
  - Presents the idea of creating short paths from early layers to later layers to facilitate the gradient flow.
  - Highlights the main contribution of the paper: the Dense Convolutional Network (DenseNet), where each layer is connected to every other layer in a feed-forward fashion.

- Related Work
  - Discusses previous research on network architectures, focusing on the challenges of training very deep networks.
  - Mentions Highway Networks and Residual Networks (ResNets) as examples of architectures that use bypassing paths to ease training.
  - Notes the use of multi-level features, increased network width, and innovations like Network in Network (NIN), Deeply Supervised Network (DSN), and Ladder Networks.

- DenseNets
  - Describes the dense connectivity pattern of DenseNets, where each layer receives feature maps from all preceding layers.
  - Explains the use of dense blocks and transition layers to facilitate downsampling.
  - Introduces the growth rate parameter $k$, which controls the number of new feature maps added by each layer.
  - Presents the DenseNet-B, DenseNet-C, and DenseNet-BC variations, which incorporate bottleneck layers, compression, or both, to improve model compactness and efficiency.

- Experiments
  - Presents empirical evaluation of DenseNet on four benchmark datasets: CIFAR-10, CIFAR-100, SVHN, and ImageNet.
  - Details the training procedures used for each dataset.
  - Mentions the use of data augmentation and dropout for some datasets.

- Classification Results on CIFAR and SVHN
  - Discusses the results obtained on CIFAR and SVHN, highlighting DenseNet's superior accuracy and parameter efficiency compared to state-of-the-art architectures, especially ResNets.
  - Notes DenseNet-BC's consistent outperformance on all CIFAR datasets and its ability to achieve comparable accuracy to ResNets with significantly fewer parameters.
  - Analyzes the effect of increasing model capacity by varying depth $L$ and growth rate $k$, showing that DenseNets can utilize the increased representational power effectively.
  - Highlights the regularizing effect of dense connections, making DenseNets less prone to overfitting, especially on datasets without data augmentation.

- Classification Results on ImageNet
  - Presents results on the ImageNet classification task, comparing DenseNet-BC with state-of-the-art ResNet architectures using the same experimental setup.
  - Compares the performance of DenseNets and ResNets in terms of top-1 and top-5 validation errors, as well as the number of parameters and FLOPs.
  - Demonstrates that DenseNets achieve comparable accuracy to ResNets while requiring significantly fewer parameters and less computation.

- Discussion
  - Compares DenseNets with ResNets, highlighting the differences in connectivity patterns and their implications for model compactness and performance.
  - Discusses the implicit deep supervision in DenseNets, similar to Deeply Supervised Nets (DSN), where each layer receives additional supervision from the loss function through shorter connections.
  - Draws a connection between DenseNets and stochastic depth regularization in ResNets, providing a possible explanation for the success of this technique.
  - Presents an experiment on feature reuse in DenseNets, analyzing the weight assigned to connections between layers and demonstrating that features from early layers are used directly by deep layers throughout the network.

- Conclusion
  - Summarizes the key contributions and advantages of DenseNets, emphasizing their scalability, accuracy, parameter efficiency, and implicit deep supervision.
  - Suggests further research directions, such as hyperparameter optimization and exploring the use of DenseNets as feature extractors for other computer vision tasks.


## The Review 

![A 5-layer dense block with a growth rate of k = 4.
Each layer takes all preceding feature-maps as input.](./fig_1.png){#fig-1 .column-margin width="250px" group="figures"}

## The paper

![paper](./paper.pdf){.col-page width="800px" height="1000px"}
