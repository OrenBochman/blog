This paper uses lots of big terms so let's break them down so we can understand them better

Multi-Agent Reinforcement Learning (MARL)
: A subfield of reinforcement learning where multiple agents learn and interact within a shared environment to achieve their individual or collective goals.

Intrinsic Motivation
: Reward signals generated by the agent itself, rather than from the external environment, used to drive exploration and learning, especially in the absence of or in addition to extrinsic rewards.

Causal Influence
: The degree to which one agent's actions directly affect the subsequent behavior or policy of another agent.

Counterfactual Reasoning
: A cognitive process of considering alternative scenarios or past events and their potential outcomes ("What if I had done...?"). In this context, agents simulate alternative actions to assess their impact.

Mutual Information (MI)
: A measure of the statistical dependence between two random variables. In this paper, it quantifies how much information one agent's actions provide about another agent's actions.

Sequential Social Dilemma (SSD)
: A type of multi-agent environment characterized by a conflict between individual short-term incentives to defect and the long-term collective benefit of cooperation, played over multiple timesteps.

Markov Game
: A formal framework for modeling multi-agent environments, extending Markov Decision Processes to include multiple interacting agents.

Partially Observable
: A setting where agents only have access to a limited subset of the true state of the environment.

Distributed Asynchronous Advantage Actor-Critic (A3C)
: A reinforcement learning algorithm that uses multiple parallel agents to asynchronously collect experience and update a global policy and value function.

Policy
: A mapping from an agent's observation of the environment to a probability distribution over its possible actions.

Reward Function
: A function that defines the incentive structure for an agent by assigning a numerical value (reward) to the agent's actions or the resulting state of the environment.

Kullback-Leibler Divergence (DKL)
: A measure of how one probability distribution differs from a second, reference probability distribution. Used here to quantify the change in another agent's policy due to a counterfactual action.

Model of Other Agents (MOA)
: An internal model learned by an agent to predict the behavior (actions) of other agents in the environment.

Emergent Communication
: Communication behaviors that arise spontaneously among agents as a result of their learning process, without being explicitly programmed or predefined.

Empowerment
: In single-agent RL, an intrinsic reward signal that encourages agents to take actions that maximize their ability to influence future states of the environment (high mutual information between actions and future states).

Gini Coefficient
: A measure of statistical dispersion intended to represent the income or wealth distribution of a nation's residents, but used here to quantify the inequality of individual rewards among agents.

