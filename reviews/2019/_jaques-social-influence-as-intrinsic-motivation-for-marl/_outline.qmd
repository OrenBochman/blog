
- Introduction
    - Describes intrinsic social motivation in multi-agent reinforcement learning (MARL) and its importance.
    - Presents social influence as a novel intrinsic motivation mechanism that rewards agents for their causal influence on other agents' actions.
    - Discusses challenges in MARL, such as coordination and communication, and how the influence reward can address them.
- Sequential Social Dilemmas
    - Describes Cleanup and Harvest, two Sequential Social Dilemmas (SSDs) used to test the social influence reward, highlighting their game-theoretic payoff structure that encourages defection while rewarding cooperation.
    - Mentions the importance of cooperation in these games and the challenges they pose to traditional RL agents.
- Multi-Agent RL for SSDs   
    - Presents the MARL Markov game setting with independent agents aiming to maximize individual rewards.
    - Describes the state, action, transition, and reward structure of the game and the agents' policies.
    - Mentions the use of a distributed asynchronous advantage actor-critic (A3C) approach for training.
- Basic Social Influence
    - Introduces the basic formulation of the social influence reward, which rewards agents for the causal influence they exert on other agents' actions.
    - Explains how influence is calculated using counterfactual reasoning by simulating alternative actions and assessing their impact on other agents' behavior using KL-divergence.
    - Discusses the relationship between the influence reward and maximizing mutual information between agents' actions, arguing that this encourages coordinated behavior.
- Experiment I: Basic Influence
    - Presents experimental results demonstrating that agents trained with the social influence reward achieve significantly higher collective reward in both Cleanup and Harvest compared to baseline agents.
    - Includes a case study demonstrating emergent communication in Cleanup, where the influencer agent learns to signal the presence or absence of apples through its movements.
    - Discusses limitations of the influence reward, such as the possibility of non-cooperative influence depending on the environment and reward balance.
- Influential Communication
    - Discusses the connection between influence and communication in human learning, suggesting that influence is a key driver of emergent communication in cooperative tasks.
    - Describes an extension of the model where agents are equipped with an explicit communication channel, allowing them to emit discrete symbols that other agents can observe.
    - Explains how the influence reward is used to train the communication policy, arguing that it incentivizes meaningful communication that benefits the listener.
- Experiment II: Influential Communication
    - Presents experimental results demonstrating that agents incentivized to communicate via the social influence reward achieve significantly higher collective reward in both games compared to baseline agents.
    - Shows that in Cleanup, the communication head could be trained effectively with zero extrinsic reward, suggesting that influence alone can suffice for training.
    - Analyzes communication behavior using metrics like speaker consistency and instantaneous coordination (IC), finding that influence agents communicate more consistently and exhibit higher IC during moments of high influence.
- Modeling Other Agents
    - Discusses the limitations of centralized training for computing the influence reward and proposes a solution: equipping each agent with its own Model of Other Agents (MOA).
    - Describes how the MOA, implemented as a neural network, is trained to predict other agents' actions based on observed trajectories and used to compute the influence reward.
    - Mentions that influence is only rewarded when the target agent is within the influencer's field-of-view to compensate for potential inaccuracies in the MOA's predictions.
- Experiment III: Modeling Other Agents
    - Presents experimental results demonstrating that agents with MOA modules achieve higher collective reward compared to baseline agents.
    - Concludes that the influence reward can be effectively computed using an internal MOA, allowing agents to learn socially but independently.
- Related Work
    - Provides a comprehensive review of related work on intrinsic social rewards, highlighting the limitations of previous approaches.
    - Discusses the relationship to work on emergent communication, machine theory of mind, and empowerment in single-agent RL.
- Details on Causal Inference
    - Provides a detailed explanation of how the causal influence reward is assessed using counterfactual reasoning.
    - Describes the causal diagrams for computing the influence reward in both the basic and MOA cases, emphasizing the importance of conditioning on the correct set of variables to avoid confounds.
- Conclusions and Future Work
    - Summarizes the key findings of the paper, emphasizing that the social influence reward consistently leads to higher collective return across different experimental setups.
    - Discusses potential extensions, such as developing a form of "empathy" in agents, using influence for coordinated behavior in robots, and applying the influence reward to hierarchical RL.
