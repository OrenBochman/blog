---
title: "Social Influence as Intrinsic Motivation"
subtitle: "paper review"
description: "Social Influence as Intrinsic Motivation for Multi-Agent Deep RL"
categories: [review, rl]
keywords: [review,paper]
image: /images/lit-review-cover.jpg
bibliography: "./bibliography.bib"

---

![cover](/images/lit-review-cover.jpg){.column-margin .nolightbox}

<!-- 
- TODO:
  - [ ] folder-name
  - [ ] date
  - [ ] title / subtitle
  - [ ] description
  - [ ] categories
  - [ ] keywords
  - [ ] paper - download rename
  - [ ] abstract
  - [ ] citation
  - [ ] outline
  - [ ] glossary
  - [ ] podcast
  - [ ] Lede paragraph
  - [ ] tl;dr
  - [ ] figures
  - [ ] reflections
  - [ ] video - locate/remove
  - [ ] quote
  - [ ] remove draft
-->

https://www.facebook.com/share/v/1EHxoBQirE/

<!-- VIDEOS GO HERE 


::: {.column-margin #fig-subtasks}
{{< video https://youtu.be/GmGL9cVfJG4
    title='Martha White - Developing Reinforcement Learning Agents that Learn Many Subtasks?' >}}

Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
:::

-->

<!-- A QUOTE by someone more famous than the author of the paper for context, add highlighting for emphasis, verse is a nice touch!  
> "The ideal market completely disregards those spikes—but a realistic model cannot." [Mandelbrot highlights the inadequacy of models ignoring extreme price movements, emphasizing the need for a framework that can accommodate them.]{.mark}

-->

<!-- LEDE personal context why I reviewed this source -->

This is the paper in which agents learned to communicate with each other in a decentralized manner to jointly solve social dillemas. These are games in which agents are both cooperating and competing with each other. It presents some new and deep ideas.

::: callout-note
## TL;DR - Too Long; Didn't Read about Social Influence <!-- Short Catchy title -->

![Social Influence in a nutshell](/images/in_the_nut_shell_coach_retouched.jpg)

{{< lipsum 1 >}} 

<!-- A good prompt for RAG is

Using a markdown list outline:

1. [ ] What are the research questions?
2. [ ] What are the main findings?
3. [ ] In historical context why was this important?

-->

:::

Here is a lighthearted Deep Dive into the paper:

<!-- convert to mp3 via:
ffmpeg -i podcast.wav -b:a 192k podcast.mp3
-->

<audio controls="1">
  <source src="podcast.mp3" data-external="1" type="audio/mpeg"></source>
</audio>

### Abstract

> We propose a unified mechanism for achieving  coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents’ actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents’ behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area. 
>
> --- [@jaques2019social] 

## Glossary

{{< include _glossary.qmd >}}

## Outline

![Total collective reward obtained in Experiment 1. Agents trained with influence (red) significantly outperform the baseline and ablated agents. In Harvest, the influence reward is essential to achieve any meaningful learning.](./fig_01.png){#fig-1 .column-margin width="250px" group="figures"}

![A moment of high influence when the purple influencer signals the presence of an apple (green tiles) outside the yellow influencee’s field-of-view (yellow outlined box).](./fig_02.png){#fig-2 .column-margin width="250px" group="figures"}


![The communication model has two heads, which learn the environment policy, πe, and a policy for emitting communication symbols, πm. Other agents’ communication messages mt−1 are input to the LSTM.](./fig_03.png){#fig-3 .column-margin width="250px" group="figures"}


![Total collective reward for deep RL agents with communication channels. Once again, the influence reward is essential to improve or achieve any learning](./fig_04.png){#fig-4 .column-margin width="250px" group="figures"}

![Metrics describing the quality of learned communication protocols. The models trained with influence reward exhibit more consistent communication and more coordination, especially in moments where influence is high5](./fig_05.png){#fig-5 .column-margin width="250px" group="figures"}

![The Model of Other Agents (MOA) architecture learns both an RL policy πe, and a supervised model that predicts the actions of other agents, at+1. The supervised model is used for internally computing the influence reward](./fig_06.png){#fig-6 .column-margin width="250px" group="figures"}

![Total collective reward for MOA models. Again, intrinsic influence consistently improves learning, with the powerful A3C agent baselines not being able to learn.](./fig_07.png){#fig-7 .column-margin width="250px" group="figures"}

![Causal diagrams of agent k’s effect on j’s action. Shaded nodes are conditioned on, and we intervene on $a^k_t$ (blue node) by replacing it with counterfactuals. Nodes with a green background must be modeled using the MOA module. Note that there is no backdoor path between $a^k_t$ and $s_t$ in the MOA case, since it would require traversing a collider that is not in the conditioning set.](./fig_08.png){#fig-8 .column-margin width="250px" group="figures"}

{{< include _outline.qmd >}}


## Reflections <!-- Criticism & Own Thoughts & Bibliography  -->

{{< lipsum 2 >}}

## The paper

![paper](./paper.pdf){.col-page width="800px" height="1000px"}

