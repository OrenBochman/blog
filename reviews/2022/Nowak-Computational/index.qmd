---
title: "Computational and evolutionary aspects of language"
subtitle: "paper review"
description: "teaser for reading this paper"
categories: [review]
keywords: [review,paper]
image: /images/lit-review-cover.jpg
bibliography: "./bibliography.bib"
draft: true
---

![cover]{/images/lit-review-cover.jpg}{.column-margin .nolightbox}

<!-- VIDEOS GO HERE 

::: {.column-margin #fig-subtasks}
{{< video https://youtu.be/GmGL9cVfJG4
    title='Martha White - Developing Reinforcement Learning Agents that Learn Many Subtasks?' >}}

Talk at Waterloo.AI by Martha White on Developing Reinforcement Learning Agents that Learn Many Subtasks. She makes the case for the life long problem setting and discusses recent research on learning multiple tasks (options and GVFs) in parallel.
:::

-->

<!-- A QUOTE by someone more famous than the author of the paper for context, add highlighting for emphasis, verse is a nice touch! 

> "The ideal market completely disregards those spikes—but a realistic model cannot." [Mandelbrot highlights the inadequacy of models ignoring extreme price movements, emphasizing the need for a framework that can accommodate them.]{.mark}

-->

<!-- LEDE personal context why I reviewed this source --> 

{{< lipsum 1 >}}


::: {.callout-note}
## TL;DR - Computational and evolutionary of language
![XXX in a nutshell](/images/in_the_nut_shell_coach_retouched.jpg)

{{< lipsum 1 >}} <!-- SHORT & OPINIONATED-->

<!-- 1. What are the research questions? -->
<!-- 2. What are the main findings? -->
<!-- 3. In historical context why was this important? -->

:::

Here is a lighthearted Deep Dive into the paper:

<audio controls="1">
  <source src="podcast.mp3" data-external="1" type="audio/mpeg">
  </source>
</audio>


### Abstract

> Language is our legacy. It is the main evolutionary contribution of humans, and perhaps the most interesting trait that has emerged in the past 500 million years. Understanding how darwinian evolution gives rise to human language requires the integration of formal language theory, learning theory and evolutionary dynamics. Formal language theory provides a mathematical description of language and grammar. Learning theory formalizes the task of language acquisition—it can be shown that no procedure can learn an unrestricted set of languages. Universal grammar specifies the restricted set of languages learnable by the human brain. Evolutionary dynamics can be formulated to describe the cultural evolution of language and the biological evolution of universal grammar. 
>
> --- [@nowak2002computational] 

## Glossary

This book/paper uses lots of big terms so let's break them down so we can understand them better

<!-- GLOSSARY HERE-->

## Outline

- Introduction
  - Briefly introduces human language as a recently emerged generative system for the transfer of non-genetic information.
  - Presents a series of questions to be addressed in the paper, focusing on the nature of language, grammar, learning, and the role of universal grammar
  - Notes the paper's aim to synthesize formal language theory, learning theory, and evolutionary dynamics.
- Formal Language Theory
  - Defines the fundamental components of formal language theory, including alphabets, sentences, languages, and grammars.
  - Provides examples of grammars and explains how they generate languages.
  - Discusses the relationship between languages, grammars, and machines, highlighting the Chomsky hierarchy.
- The Structure of Natural Languages
  - Notes that natural languages are infinite and exceed the capacity of finite-state grammars.
  - Discusses the debate regarding the adequacy of context-free grammars for representing natural languages.
  - Highlights the importance of tree structures for understanding natural language syntax.
- Learning Theory
  - Defines learning as inductive inference and contrasts it with memorization, emphasizing the ability to generalize to novel instances.
  - Presents the "paradox of language acquisition", noting that the linguistic input children receive is insufficient to uniquely determine the grammar of their native language.
- The Paradox of Language Acquisition
  - Describes Chomsky's concept of "poverty of the stimulus" and the proposed solution of "universal grammar" (UG) as a restricted set of candidate grammars.
  - Explains the controversy surrounding the notion of an innate UG and highlights the role of learning theory in demonstrating its logical necessity.
- Learnability
  - Defines the concept of "learnability" in formal terms, using the concepts of algorithms, texts, and languages.
  - Presents Gold's Theorem, a key result in learning theory demonstrating the impossibility of learning an unrestricted set of languages, including regular languages.
- Probably Almost Correct
  - Addresses criticisms of Gold's framework, particularly the requirement for exact language identification.
  - Discusses extensions of the framework, such as statistical learning theory, which allow for approximate language identification and confirm the need for UG.
- Learning Finite Languages
  - Discusses the challenges of learning finite languages, showing that even in this restricted case, a restricted search space is needed for generalization beyond memorized sentences.
- The Necessity of Innate Expectations
  - Argues for the necessity of an innate UG based on the human brain's learning algorithm being able to learn existing human languages but not all computable languages.
  - Discusses different approaches to UG, including "principles and parameters theory" and "optimality theory".
  - Notes that the debate surrounding UG should focus on its form rather than its existence.
- What is Special About Language Acquisition?
  - Highlights the unique aspect of language acquisition occurring without explicit instruction about grammatical rules, contrasting it with learning other generative systems like chess or arithmetic.
- Evolutionary Language Theory
  - Discusses the evolutionary origins of human language, emphasizing the need to understand the genetic modifications that led to the capacity for language.
  - Briefly mentions the reuse of pre-existing cognitive features in the evolution of language.
  - Outlines a minimalist program for studying language evolution by combining formal language theory, learning theory, and evolutionary theory.
- Cultural Evolution of Language with Constant Universal Grammar
  - Explains the concept of language as an extended phenotype of a population and considers a scenario with a constant UG.
  - Presents a language dynamical equation that models the selection of languages based on communicative function and learnability.
  - Discusses the concept of a "coherence threshold" - the minimum specificity of UG required for linguistic coherence in a population.
- Evolution of Universal Grammar
  - Introduces a model for the evolution of UG, incorporating mutation and selection among different UGs.
  - Discusses the interplay between the biological evolution of UG and the cultural evolution of language, noting the selective pressure for linguistic coherence.
  - Briefly mentions the trade-off between learnability and adaptability in the evolution of UG.
- Historical Linguistics
  - Discusses the application of the language dynamical equation to the study of language change in historical linguistics.
  - Mentions different mechanisms of language change, including `parameter resetting`, grammaticalization, and creolization.
  - Notes the prevalence of selectively neutral language changes and the potential for incorporating neutral evolution models.
- Outlook
  - Summarizes the need to integrate formal language theory, learning theory, and evolutionary theory in the study of language.
  - Lists some theoretical and empirical questions for future research, emphasizing the interdisciplinary nature of the field.



## The Review 

<!-- Beyond the outline -->

{{< lipsum 2 >}}

::: {#fig-1 .column-margin width="250px" group="figures"}
![figure 1](./fig91.jpg){#fig-1 .column-margin width="250px" group="figures"}

The basic objects of formal language theory are alphabets, sentences, languages and grammars. Grammars consist of rewrite rules: a particular string can be rewritten as another string. Such rules contain symbols of the alphabet (here 0 and 1), and so-called ‘non-terminals’ (here S, A, B and F), and a null-element, $\epsilon$. The grammar in this figure works as follows: each sentence begins with the symbol S. S is rewritten as 0A. Now there are two choices: A can be rewritten as 1A or 0B. B can be rewritten as 1B or 0F. F always goes to e. This grammar generates sentences of the form 01m01n0, which means that every sentence begins with 0, followed by a sequence of m 1s, followed by a 0, followed by a sequence of n 1s, followed by 0.
:::


{{< lipsum 2 >}}

## The paper

![paper](./paper.pdf){.col-page width="800px" height="1000px"}
