[
  {
    "objectID": "posts/2024/post-with-code/index.html",
    "href": "posts/2024/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n2\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {Post {With} {Code}},\n  date = {2024-01-28},\n  url = {https://orenbochman.github.io/blog//posts/2024/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. “Post With Code.” January 28, 2024. https://orenbochman.github.io/blog//posts/2024/post-with-code."
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/migration notes.html",
    "href": "posts/2023/2023-05-10-migration-notes/migration notes.html",
    "title": "Quarto Migration Notes",
    "section": "",
    "text": "I was able to stand on the shoulders of Giants, (Navarro 2022) when I migrated this blog.\nThere are lots of details that should be in the guide that are scattered all over the quarto site.\nI decided that all posts should have the following fields in their front matter:"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/migration notes.html#virtual-environments",
    "href": "posts/2023/2023-05-10-migration-notes/migration notes.html#virtual-environments",
    "title": "Quarto Migration Notes",
    "section": "Virtual Environments",
    "text": "Virtual Environments\nare documented here\nideal one can have one virtual environment for the whole site"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/migration notes.html#extras",
    "href": "posts/2023/2023-05-10-migration-notes/migration notes.html#extras",
    "title": "Quarto Migration Notes",
    "section": "Extras",
    "text": "Extras\n\nthe about page is based on postcards package\nicons for navigation come from bootstrap\ncover images are from pexels"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/migration notes.html#references",
    "href": "posts/2023/2023-05-10-migration-notes/migration notes.html#references",
    "title": "Quarto Migration Notes",
    "section": "references:",
    "text": "references:\nhttps://blog.djnavarro.net/posts/2022-04-20_porting-to-quarto/"
  },
  {
    "objectID": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html",
    "href": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html",
    "title": "Tidy Text Mining With R",
    "section": "",
    "text": "Computational Linguistics tasks:"
  },
  {
    "objectID": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html#text-preprocessing",
    "href": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html#text-preprocessing",
    "title": "Tidy Text Mining With R",
    "section": "Text preprocessing",
    "text": "Text preprocessing\n\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %&gt;%\n  ungroup()\n\noriginal_books\n\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   &lt;chr&gt;                   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt;\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# ℹ 73,412 more rows\n\nlibrary(tidytext)\ntidy_books &lt;- original_books %&gt;%\n  unnest_tokens(word, text)\n\ntidy_books\n\n# A tibble: 725,055 × 4\n   book                linenumber chapter word       \n   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 and        \n 3 Sense & Sensibility          1       0 sensibility\n 4 Sense & Sensibility          3       0 by         \n 5 Sense & Sensibility          3       0 jane       \n 6 Sense & Sensibility          3       0 austen     \n 7 Sense & Sensibility          5       0 1811       \n 8 Sense & Sensibility         10       1 chapter    \n 9 Sense & Sensibility         10       1 1          \n10 Sense & Sensibility         13       1 the        \n# ℹ 725,045 more rows\n\n\n\ndata(stop_words)\n\ntidy_books &lt;- tidy_books %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_books\n\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# ℹ 217,599 more rows\n\n\n\nthis removes stop words\n\n\ntidy_books %&gt;%\n  count(word, sort = TRUE) \n\n# A tibble: 13,914 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 miss    1855\n 2 time    1337\n 3 fanny    862\n 4 dear     822\n 5 lady     817\n 6 sir      806\n 7 day      797\n 8 emma     787\n 9 sister   727\n10 house    699\n# ℹ 13,904 more rows\n\n\n\nlibrary(ggplot2)\n\ntidy_books %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(n &gt; 600) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\n#devtools::install_github(\"ropensci/gutenbergr\")\nlibrary(gutenbergr)\n\n#hgwells &lt;- gutenberg_download(c(35, 36,  159, 456, 1047, 3691, 5230, 11870, 12163, 23218, 28218, 35461,39585))\nhgwells &lt;- gutenberg_download(c(35, 36,  159))\n\nDetermining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest\n\n\n`curl` package not installed, falling back to using `url()`\nUsing mirror http://aleph.gutenberg.org\n\n\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/5/159/159.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n\n\n\ntidy_hgwells &lt;- hgwells %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\n\ntidy_hgwells %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 8,146 × 2\n   word         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 time       328\n 2 people     205\n 3 martians   165\n 4 black      152\n 5 night      140\n 6 machine    133\n 7 found      110\n 8 white      108\n 9 road       105\n10 day        102\n# ℹ 8,136 more rows\n\n\n\nbronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767))\n\n\ntidy_bronte &lt;- bronte %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_bronte %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 23,213 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 time    1065\n 2 miss     854\n 3 day      825\n 4 don’t    780\n 5 hand     767\n 6 eyes     714\n 7 night    648\n 8 heart    638\n 9 looked   601\n10 door     591\n# ℹ 23,203 more rows\n\n\n\nlibrary(tidyr)\n\nfrequency &lt;- bind_rows(mutate(tidy_bronte, author = \"Brontë Sisters\"),\n                       mutate(tidy_hgwells, author = \"H.G. Wells\"), \n                       mutate(tidy_books, author = \"Jane Austen\")) %&gt;% \n  mutate(word = str_extract(word, \"[a-z']+\")) %&gt;%\n  count(author, word) %&gt;%\n  group_by(author) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;% \n  select(-n) %&gt;% \n  pivot_wider(names_from = author, values_from = proportion) %&gt;%\n  pivot_longer(`Brontë Sisters`:`H.G. Wells`,\n               names_to = \"author\", values_to = \"proportion\")\n\nfrequency\n\n# A tibble: 54,120 × 4\n   word      `Jane Austen` author          proportion\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1 a            0.00000919 Brontë Sisters  0.0000665 \n 2 a            0.00000919 H.G. Wells      0.0000293 \n 3 aback       NA          Brontë Sisters  0.00000391\n 4 aback       NA          H.G. Wells     NA         \n 5 abaht       NA          Brontë Sisters  0.00000391\n 6 abaht       NA          H.G. Wells     NA         \n 7 abandon     NA          Brontë Sisters  0.0000313 \n 8 abandon     NA          H.G. Wells      0.0000293 \n 9 abandoned    0.00000460 Brontë Sisters  0.0000899 \n10 abandoned    0.00000460 H.G. Wells      0.000234  \n# ℹ 54,110 more rows\n\n\n\nlibrary(scales)\n\n# expect a warning about rows with missing values being removed\nggplot(frequency, aes(x = proportion, y = `Jane Austen`, \n                      color = abs(`Jane Austen` - proportion))) +\n  geom_abline(color = \"gray40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  scale_color_gradient(limits = c(0, 0.001), \n                       low = \"darkslategray4\", high = \"gray75\") +\n  facet_wrap(~author, ncol = 2) +\n  theme(legend.position=\"none\") +\n  labs(y = \"Jane Austen\", x = NULL)\n\nWarning: Removed 39274 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 39276 rows containing missing values (`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n  cor.test(data = frequency[frequency$author == \"Brontë Sisters\",], ~ proportion + `Jane Austen`)\n\n\n    Pearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 110.73, df = 10275, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7286645 0.7462983\nsample estimates:\n      cor \n0.7376071 \n\n\n\ncor.test(data = frequency[frequency$author == \"H.G. Wells\",], \n         ~ proportion + `Jane Austen`)\n\n\n    Pearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 29.497, df = 4567, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3753856 0.4241064\nsample estimates:\n      cor \n0.4000286 \n\n\nkwik and kwok\n\nlibrary(quanteda)\nlibrary(gutenbergr)\n\nausten_works = gutenberg_works(author == \"Austen, Jane\")\nausten = gutenberg_download(austen_works$gutenberg_id)\n\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/3/4/1342/1342.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n\nhead(hgwells)\n\n# A tibble: 6 × 2\n  gutenberg_id text              \n         &lt;int&gt; &lt;chr&gt;             \n1           35 \"The Time Machine\"\n2           35 \"\"                \n3           35 \"An Invention\"    \n4           35 \"\"                \n5           35 \"by H. G. Wells\"  \n6           35 \"\"                \n\n# tidy_hgwells &lt;- hgwells %&gt;%\n#   unnest_tokens(word, text) %&gt;%\n#   anti_join(stop_words)\n\n#head(tidy_hgwells)\n\nthe_corpus &lt;- corpus(austen)\nthe_tokens &lt;- tokens(the_corpus,case_insensitive = TRUE)\n\nWarning: case_insensitive argument is not used.\n\nkwic_table &lt;- kwic(the_tokens,pattern = \"lady\",index = 1:100)\n#kwic_table &lt;- kwic(tokens(tidy_hgwells$word),pattern = \"time\")\n\n#kwic_table &lt;- kwic(tokens(tidy_hgwells$word),pattern = \"machine\",index = 1:400, case_insensitive = TRUE)\nnrow(kwic_table)\n\n[1] 2008\n\nhead(kwic_table,10)\n\nKeyword-in-context with 10 matches.                                                          \n  [text61, 5]                Gloucester, by which | lady |\n  [text98, 7]                deserved by his own. | Lady |\n [text100, 8] youthful infatuation which made her | Lady |\n [text112, 6]            her kindness and advice, | Lady |\n [text118, 4]                   passed away since | Lady |\n [text122, 2]                                That | Lady |\n [text142, 2]                                  To | Lady |\n [text143, 8]              favourite, and friend. | Lady |\n [text169, 1]                                     | Lady |\n [text177, 3]                   immediately after | Lady |\n                                \n ( who died 1800 )              \n Elliot had been an excellent   \n Elliot, had never              \n Elliot mainly relied for the   \n Elliot’s death, and they       \n Russell, of steady age         \n Russell, indeed, she           \n Russell loved them all;        \n Russell’s temples had long been\n Russell out of all the"
  },
  {
    "objectID": "posts/2011/2011-11-29-npl-python/index.html",
    "href": "posts/2011/2011-11-29-npl-python/index.html",
    "title": "Text Mining With Python",
    "section": "",
    "text": "import numpy as np                           # library for scientific computing and matrix \nimport matplotlib.pyplot as plt              # visualization library\nimport string\nimport re\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer    \n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /home/oren/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\n\n1nltk.download('stopwords')\n\n\ndef process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english') \n    \n2    tweet = re.sub(r'\\$\\w*', '', tweet)\n3    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n4    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n5    tweet = re.sub(r'#', '', tweet)\n6    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n    tweets_clean = []\n    for word in tweet_tokens:\n7        if (word not in stopwords_english and\n8                word not in string.punctuation):\n            # tweets_clean.append(word)\n9            stem_word = stemmer.stem(word)\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n\n\n1\n\ndownload the stopwords\n\n2\n\nremove stock market tickers like $GE\n\n3\n\nremove old style retweet text “RT”\n\n4\n\nremove hyperlinks\n\n5\n\nremove hashtags\n\n6\n\ntokenize tweets\n\n7\n\nremove stopwords\n\n8\n\nremove punctuation\n\n9\n\nstemming word\n\n\n\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = defaultdict(int)\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs\n\ndef build_vocab(freqs):\n    vocab = [k for k, v in freq.items() if (v &gt; 1 and k != '\\n')]\n    vocab.sort()\n    return vocab\n\nprocessing unknown tokens\n\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n\n\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nNumber of tweets:  10000\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2011,\n  author = {Bochman, Oren},\n  title = {Text {Mining} {With} {Python}},\n  date = {2011-11-29},\n  url = {https://orenbochman.github.io/blog//posts/2011/2011-11-29-npl-python},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2011. “Text Mining With Python.” November\n29, 2011. https://orenbochman.github.io/blog//posts/2011/2011-11-29-npl-python."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html",
    "href": "posts/2011/2011-08-11-time-management/index.html",
    "title": "Time management Tips",
    "section": "",
    "text": "Effective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance. In this article, we’ll explore various time management strategies, focusing on learning from others, project management techniques, and effective meetings."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#introduction",
    "href": "posts/2011/2011-08-11-time-management/index.html#introduction",
    "title": "Time management Tips",
    "section": "",
    "text": "Effective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance. In this article, we’ll explore various time management strategies, focusing on learning from others, project management techniques, and effective meetings."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#learning-from-others",
    "href": "posts/2011/2011-08-11-time-management/index.html#learning-from-others",
    "title": "Time management Tips",
    "section": "Learning from Others",
    "text": "Learning from Others\nTo improve your time management skills, consider learning from different sources:\n\nWisdom from experienced individuals: Look for insights and advice from people who have been successful in managing their time. They can provide valuable tips and practical approaches to help you make the most of your time.\nFresh perspectives from younger generations: Younger people often have unique and innovative ideas on how to approach time management. Their perspectives can provide fresh insights on the world and how to navigate it efficiently.\nCassandra’s prophecies: Although they may seem pessimistic or overly cautious, pay attention to people who predict potential problems or obstacles. By considering their warnings, you can proactively prepare for possible challenges, preventing self-fulfilling prophecies."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#project-management",
    "href": "posts/2011/2011-08-11-time-management/index.html#project-management",
    "title": "Time management Tips",
    "section": "Project management",
    "text": "Project management\n\nMeasure work by output, not input: Focus on the results produced rather than the amount of time spent on a task. This will encourage efficiency and productivity.\nDefine specific goals: Clearly outline the objectives, responsibilities, and tasks associated with each project or job.\nTrack goal progress: Regularly monitor the progress of your goals to ensure they are on track for completion.\nProvide periodic progress reports: Keep stakeholders informed with brief, regular updates on project status.\nSet high-quality performance objectives: Ensure that your goals are challenging, attainable, and aligned with your overall mission.\nMaintain a project list: Keep a running list of all projects, breaking them down into smaller, manageable tasks if necessary.\nFocus on the next step, not the final goal: Concentrate on completing the immediate task at hand rather than getting overwhelmed by the overall project.\nPrioritize tasks effectively: Determine which tasks are most important and tackle them first.\nSchedule your day realistically: Plan your day in a way that allows for adequate time to complete tasks without feeling rushed or overwhelmed."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#conducting-efficient-meetings",
    "href": "posts/2011/2011-08-11-time-management/index.html#conducting-efficient-meetings",
    "title": "Time management Tips",
    "section": "Conducting Efficient Meetings:",
    "text": "Conducting Efficient Meetings:\n\nEstablish clear meeting objectives: Ensure that every meeting has a specific purpose, such as group bonding, reaching a group decision, or conducting peer-to-peer negotiations.\nUse alternative communication methods when appropriate: Utilize emails and letters for unidirectional information flow or for maintaining records, as they can be more efficient than meetings for certain tasks."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#conclusion",
    "href": "posts/2011/2011-08-11-time-management/index.html#conclusion",
    "title": "Time management Tips",
    "section": "Conclusion:",
    "text": "Conclusion:\nBy implementing these time management strategies, you can increase your productivity, achieve your goals, and maintain a healthy work-life balance. Remember, the key to success is consistently refining and adapting your approach to time management as you encounter new challenges and opportunities."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Quarto Migration Notes\n\n\nI have moved this blog from blogger to Jekyl and finaly to quarto. \n\n\n\n\n\nJan 30, 2024\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nBeacuase auto-ml is a superpoer\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\nWikisym 2012\n\n\nConference Report\n\n\n\n\n\nJul 26, 2022\n\n\n\n\n\n\n\nText Mining With Python\n\n\n\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nText Mining With R\n\n\na number of NLP tasks in R\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\nan update on NLP with R\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nR Books\n\n\nbooks reviews & recommendations\n\n\n\n\n\nAug 26, 2011\n\n\n\n\n\n\n\nTime management Tips\n\n\nWe could all use a productivity boost\n\n\n\n\n\nAug 11, 2011\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a place to share my insights about my interests.\nI’ve decided to migrate to Quarto and see what the platform can do."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "This is a place to share my insights about my interests.\nI’ve decided to migrate to Quarto and see what the platform can do."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oren Bochman’s Blog",
    "section": "",
    "text": "Quarto Migration Notes\n\n\nI have moved this blog from blogger to Jekyl and finaly to quarto. \n\n\n\nQuarto\n\n\nBlogging\n\n\nReproducibility\n\n\n\nsome migration notes from Blooger to Jekyl to Quarto blog.\n\n\n\n\n\nJan 30, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nBeacuase auto-ml is a superpoer\n\n\n\ncheatsheets\n\n\ncode\n\n\ndata science\n\n\nauto-ml\n\n\n\nAutogluon is a auto-ml framework, here are three cheetsheet for accellerating data science workloads\n\n\n\n\n\nDec 20, 2023\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWikisym 2012\n\n\nConference Report\n\n\n\nreport\n\n\nwikisym\n\n\nconference\n\n\n\n\n\n\n\n\n\nJul 26, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With Python\n\n\n\n\n\n\nPython\n\n\nNLP\n\n\nText Mining\n\n\n\n\n\n\n\n\n\nNov 29, 2011\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With R\n\n\na number of NLP tasks in R\n\n\n\nR\n\n\nNLP\n\n\nText Mining\n\n\n\n\n\n\n\n\n\nNov 29, 2011\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\nan update on NLP with R\n\n\n\nR\n\n\nNLP\n\n\nText Mining\n\n\n\n\n\n\n\n\n\nNov 29, 2011\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nR Books\n\n\nbooks reviews & recommendations\n\n\n\nreviews\n\n\ndata analysis\n\n\nR\n\n\n\nIn this updated post I included some R books you might want to look at if you are getting started with R for data science.\n\n\n\n\n\nAug 26, 2011\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTime management Tips\n\n\nWe could all use a productivity boost\n\n\n\ntime management\n\n\nproductivity\n\n\n\nEffective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance\n\n\n\n\n\nAug 11, 2011\n\n\nOren Bochman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html",
    "href": "posts/2011/2011-08-26-R-books/index.html",
    "title": "R Books",
    "section": "",
    "text": "In this updated post I included some R books you might want to look at if you are getting started with R for data science\nFellow data scientists, do not be overwhelmed by vast multiplicities of numbers, for they are but symbols of the natural order. Embrace their uncertainty and seek to understand the patterns within it. I offer you some of the first R books I came across. I got started with R in 2011, and I decided to update it to focus on stats and ml books I’ve come across with an attempt to list my favorites with their lecture notes and video lectures where available."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#software-for-data-analysis-programming-with-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#software-for-data-analysis-programming-with-r",
    "title": "R Books",
    "section": "Software for data analysis: programming with R",
    "text": "Software for data analysis: programming with R\n\n\n\n\n\nSoftware for data analysis\n\n\nIn (Chambers 2008) the author presents the essential guidebook for those who wish to learn how to use the R programming language for data analysis. Chambers is a renowned statistician, and he shares his expertise in the field of data analysis through this book. The book covers a wide range of topics related to data analysis, including data structures, object-oriented programming, graphics, and statistical modeling. It also offers a practical approach to understanding R programming, with an emphasis on building applications that can handle large datasets. Overall, this is a valuable resource for those who want to learn about R programming for data analysis. It is a comprehensive guide that covers all the essential aspects of data analysis and provides hands-on experience with R programming. The book is written in a clear and concise manner, making it easy to follow even for beginners."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#principles-of-statistical-data-handling",
    "href": "posts/2011/2011-08-26-R-books/index.html#principles-of-statistical-data-handling",
    "title": "R Books",
    "section": "Principles of Statistical Data Handling",
    "text": "Principles of Statistical Data Handling\n\n\n\n\n\nPrinciples of Statistical Data Handling\n\n\nIn (Davidson 1996) the author offers a guide to the foundations of this field, including exploratory data analysis, hypothesis testing, and model building. Through careful attention and disciplined study, one can cultivate a deep understanding of the methods and techniques that underlie statistical data handling, and by following, we shall approach data with a rational and objective mindset, illuminating with our analytical skills the meaningful insights and so make more informed decisions.\nA Stoic would say “Remember, the data is not what you see, but what you make of it. So, approach it with a clear mind, free from bias and preconceptions, and seek the truth that lies hidden within.” By applying oneself to these principles with diligence and perseverance, we may yet unlock the full potential of statistical data handling, and make a valuable contribution to the world of data science."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#probability-statistics-for-engineers-and-scientists",
    "href": "posts/2011/2011-08-26-R-books/index.html#probability-statistics-for-engineers-and-scientists",
    "title": "R Books",
    "section": "Probability & Statistics for Engineers and Scientists",
    "text": "Probability & Statistics for Engineers and Scientists\n\n\n\n\n\nProbability & Statistics for Engineers and Scientists\n\n\nIn (Walpole et al. 2007), the autor presents a path to comprehension of probability and statistics is laid out before you.\nIt is a journey of discovery that will require patience, diligence, and a willingness to learn. The author presents the tools and techniques needed to analyze data and draw meaningful conclusions. By using R, one can unlock the secrets hidden in the data.\nFear not mistakes, for they are but stepping stones towards deeper understanding, only take care to learn from them, and use the knowledge gained to improve your understanding daily.\nWith each chapter, you will gain a greater understanding of the complex and interconnected world of probability and statistics.\nEmbrace the journey, and may the numbers guide you towards enlightenment."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#introduction-to-probability-and-statistics-using-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#introduction-to-probability-and-statistics-using-r",
    "title": "R Books",
    "section": "Introduction to Probability and Statistics Using R",
    "text": "Introduction to Probability and Statistics Using R\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\n\n\nIn (Kerns 2018), recommended by my old friend Adam Hyland, the author covers the basic concepts of probability and statistics using the R programming language.\nIt is a useful resource for data scientists who wish to gain a deeper understanding of probability and statistics and how to apply them.\nStarting with basic probability, distributions, hypothesis testing, regression analysis, it then proceeds to more advanced topics such as Bayesian statistics, machine learning, and time series analysis.\nEach chapter presents clear explanations, examples, and R code to help the reader grasp the theoretical concepts and apply them in practice. By including a wide range of real-world examples and datasets, it helps the readers conect the concepts and techniques with thier application to real data.\nA complimentry copy is available at this link"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#statistical-computing-with-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#statistical-computing-with-r",
    "title": "R Books",
    "section": "Statistical Computing with R",
    "text": "Statistical Computing with R\n\n\n\n\n\nStatistical Computing with R\n\n\nIn (Rizzo 2019) the author presents is a comprehensive guide to the analysis and manipulation of data using R. Within, we are introduced to a wide variety of statistical concepts and tools that enable us to explore and understand complex datasets.\nStandard statistical techniques used in data analysis, such as probability, hypothesis testing are covered. We learn about the normal distribution and its importance in statistical analysis, as well as the Poisson distribution, which is used to model counts of events.\nThe book introduces us to the use of statistical transformations, such as the log transformation, which is often used to make skewed data more normal. We also learn about density estimation and the use of histograms and kernel density estimates to visualize data.\nThe concepts of sampling and random variables are explored, as well as the calculation of sample means and standard errors. We also learn about the use of random samples from Monte Carlo simulation to approximate probabilities and calculate statistics.\nThe book covers the use of algorithms and samplers, such as the Metropolis-Hastings algorithm, to explore parameter space and to generate samples from distributions of interest. We learn about the importance of convergence and the use of proposals in Monte Carlo sampling.\nThe concepts of bias and variance are explored, as well as the calculation of confidence intervals and the use of hypothesis testing to evaluate statistical significance. We also learn about the use of the bootstrap and jackknife methods to find the level of uncertainty in our estimates.\nThroughout the book, we are introduced to the use of R for statistical computing. We learn about the use of formulas to specify statistical models, as well as the use of packages for data manipulation and visualization.\nOverall, “Statistical Computing with R” is an essential resource for anyone interested in using statistical methods to analyze data. It provides a lucid and comprehensive treatment of statistical concepts and their practical implementation using R."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#bayesian-methods-of-data-analysis",
    "href": "posts/2011/2011-08-26-R-books/index.html#bayesian-methods-of-data-analysis",
    "title": "R Books",
    "section": "Bayesian methods of Data Analysis",
    "text": "Bayesian methods of Data Analysis\n\n\n\n\n\nBayesian methods of Data Analysis\n\n\nIn (Carlin and Louis 2008) the author presents us with a framework that is grounded in the philosophy of probability theory. We learn to seek a baseline model then approach the problem at hand with a Bayesian perspective.\nThrough the use of Bayesian models, we can compute the conditional distributions of our data and evaluate the error and loss functions. We must consider convergence, the choice of priors, and how they are specified. We use Bayes’ rule to compute the posterior distribution and marginal likelihood, and we obtain point estimates and credible intervals.\nThe use of the Gibbs sampler and the Metropolis-Hastings algorithm in MCMC methods are presented as tools to obtain a sample from the posterior distribution. We use WinBUGS code and Monte Carlo simulations to produce results that are in line with the data observed.\nWe are introduced to the concept of the Bayes factor, and how it is used to compare models. We also understand how the use of the Jeffreys prior, the hyperprior, and the conjugate prior can be used to simplify our computations.\nIn Bayesian methods, we use the full conditional distributions to obtain the joint posterior distribution of our parameters. We also compute the marginal posterior distribution, which can be used to obtain a credible interval.\nWe are shown how to deal with univariate and multivariate data, and how to model the random effects and covariate effects. We also understand how to evaluate the performance of our models through histograms, percentiles, and plots.\nIn this work, we are presented with a practical and useful guide to Bayesian methods that can be applied to a variety of problems."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#information-theory-inference-and-learning-algorithms",
    "href": "posts/2011/2011-08-26-R-books/index.html#information-theory-inference-and-learning-algorithms",
    "title": "R Books",
    "section": "Information Theory, Inference and Learning Algorithms",
    "text": "Information Theory, Inference and Learning Algorithms\n\n\n\n\n\nInformation Theory, Inference and Learning Algorithms\n\n\nInformation Theory, Inference and Learning Algorithms (MacKay 2003) by David J.C. MacKay FRS - Course notes: Information Theory, Pattern Recognition, and Neural Networks.\nThis is not an R book as far as I recall but it is available online, together with lectures by the author. I recommend this book and videos for anyone interested bayesian data analysis. The author was a physicist, a leading bayesian and pioneer in Bayesian Neural Networks whose work is very relavant even today (2024)"
  },
  {
    "objectID": "posts/2011/2011-11-29-text-mining-with-r/index.html",
    "href": "posts/2011/2011-11-29-text-mining-with-r/index.html",
    "title": "Text Mining With R",
    "section": "",
    "text": "Computational Linguistics tasks:"
  },
  {
    "objectID": "posts/2011/2011-11-29-text-mining-with-r/index.html#text-preprocessing",
    "href": "posts/2011/2011-11-29-text-mining-with-r/index.html#text-preprocessing",
    "title": "Text Mining With R",
    "section": "Text preprocessing",
    "text": "Text preprocessing\n\n4tm_corpus &lt;- tm_map(tm_corpus, tolower)\ninspect(tm_corpus)\n\n\n4\n\nthis makes all the tokens lowercase\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs, hospitals, doctors                                                                                                                                      \n [2] smog, pollution, micro-plastics, environment.                                                                                                                  \n [3] doctors, hospitals, healthcare                                                                                                                                 \n [4] pollution, environment, water.                                                                                                                                 \n [5] i love nlp with deep learning.                                                                                                                                 \n [6] i love machine learning.                                                                                                                                       \n [7] he said he was keeping the wolf from the door.                                                                                                                 \n [8] time flies like an arrow, fruit flies like a banana.                                                                                                           \n [9] pollution, greenhouse gasses, ghg, hydrofluorocarbons, ozone hole, global warming. montreal protocol.                                                          \n[10] greenhouse gasses, hydrofluorocarbons, perfluorocarbons, sulfur hexafluoride, carbon dioxide, carbon monoxide, co2, hydrofluorocarbons, methane, nitrous oxide.\n\n\n\n5tm_corpus &lt;- tm_map(tm_corpus, content_transformer(removePunctuation))\ninspect(tm_corpus)\n\n\n5\n\nthis removes punctuation tokens\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                              \n [2] smog pollution microplastics environment                                                                                                             \n [3] doctors hospitals healthcare                                                                                                                         \n [4] pollution environment water                                                                                                                          \n [5] i love nlp with deep learning                                                                                                                        \n [6] i love machine learning                                                                                                                              \n [7] he said he was keeping the wolf from the door                                                                                                        \n [8] time flies like an arrow fruit flies like a banana                                                                                                   \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                       \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co2 hydrofluorocarbons methane nitrous oxide\n\n\n\n6tm_corpus &lt;- tm_map(tm_corpus, removeWords, stopwords(\"english\"))\ninspect(tm_corpus)\n\n\n6\n\nthis removes stop words\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                              \n [2] smog pollution microplastics environment                                                                                                             \n [3] doctors hospitals healthcare                                                                                                                         \n [4] pollution environment water                                                                                                                          \n [5]  love nlp  deep learning                                                                                                                             \n [6]  love machine learning                                                                                                                               \n [7]  said   keeping  wolf   door                                                                                                                         \n [8] time flies like  arrow fruit flies like  banana                                                                                                      \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                       \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co2 hydrofluorocarbons methane nitrous oxide\n\n\n\n7tm_corpus &lt;- tm_map(tm_corpus, removeNumbers)\ninspect(tm_corpus)\n\n\n7\n\nthis removes numbers\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                             \n [2] smog pollution microplastics environment                                                                                                            \n [3] doctors hospitals healthcare                                                                                                                        \n [4] pollution environment water                                                                                                                         \n [5]  love nlp  deep learning                                                                                                                            \n [6]  love machine learning                                                                                                                              \n [7]  said   keeping  wolf   door                                                                                                                        \n [8] time flies like  arrow fruit flies like  banana                                                                                                     \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                      \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co hydrofluorocarbons methane nitrous oxide\n\n\n\n8tm_corpus &lt;- tm_map(tm_corpus, stemDocument, language=\"english\")\ninspect(tm_corpus)\n\n\n8\n\nthis stems the words\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drug hospit doctor                                                                                                                       \n [2] smog pollut microplast environ                                                                                                           \n [3] doctor hospit healthcar                                                                                                                  \n [4] pollut environ water                                                                                                                     \n [5] love nlp deep learn                                                                                                                      \n [6] love machin learn                                                                                                                        \n [7] said keep wolf door                                                                                                                      \n [8] time fli like arrow fruit fli like banana                                                                                                \n [9] pollut greenhous gass ghg hydrofluorocarbon ozon hole global warm montreal protocol                                                      \n[10] greenhous gass hydrofluorocarbon perfluorocarbon sulfur hexafluorid carbon dioxid carbon monoxid co hydrofluorocarbon methan nitrous oxid\n\n\n\n9tm_corpus &lt;- tm_map(tm_corpus, stripWhitespace)\ninspect(tm_corpus)\n\n\n9\n\nRemoving Whitespaces - a single white space or group of whitespaces may be considered to be a token within a corpus. This is how we remove these token\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drug hospit doctor                                                                                                                       \n [2] smog pollut microplast environ                                                                                                           \n [3] doctor hospit healthcar                                                                                                                  \n [4] pollut environ water                                                                                                                     \n [5] love nlp deep learn                                                                                                                      \n [6] love machin learn                                                                                                                        \n [7] said keep wolf door                                                                                                                      \n [8] time fli like arrow fruit fli like banana                                                                                                \n [9] pollut greenhous gass ghg hydrofluorocarbon ozon hole global warm montreal protocol                                                      \n[10] greenhous gass hydrofluorocarbon perfluorocarbon sulfur hexafluorid carbon dioxid carbon monoxid co hydrofluorocarbon methan nitrous oxid\n\n\n\ndtm &lt;- DocumentTermMatrix(tm_corpus)\ninspect(dtm)\n\n&lt;&lt;DocumentTermMatrix (documents: 10, terms: 43)&gt;&gt;\nNon-/sparse entries: 53/377\nSparsity           : 88%\nMaximal term length: 17\nWeighting          : term frequency (tf)\nSample             :\n    Terms\nDocs doctor environ fli gass hospit hydrofluorocarbon learn like love pollut\n  1       1       0   0    0      1                 0     0    0    0      0\n  10      0       0   0    1      0                 2     0    0    0      0\n  2       0       1   0    0      0                 0     0    0    0      1\n  3       1       0   0    0      1                 0     0    0    0      0\n  4       0       1   0    0      0                 0     0    0    0      1\n  5       0       0   0    0      0                 0     1    0    1      0\n  6       0       0   0    0      0                 0     1    0    1      0\n  7       0       0   0    0      0                 0     0    0    0      0\n  8       0       0   2    0      0                 0     0    2    0      0\n  9       0       0   0    1      0                 1     0    0    0      1\n\n\n\nfindFreqTerms(dtm, 2)\n\n [1] \"doctor\"            \"hospit\"            \"environ\"          \n [4] \"pollut\"            \"learn\"             \"love\"             \n [7] \"fli\"               \"like\"              \"gass\"             \n[10] \"greenhous\"         \"hydrofluorocarbon\" \"carbon\"           \n\n\n\nfindAssocs(dtm, \"polution\", 0.8)\n\n$polution\nnumeric(0)\n\n\n\nas.matrix(dtm)\n\n    Terms\nDocs doctor drug hospit environ microplast pollut smog healthcar water deep\n  1       1    1      1       0          0      0    0         0     0    0\n  2       0    0      0       1          1      1    1         0     0    0\n  3       1    0      1       0          0      0    0         1     0    0\n  4       0    0      0       1          0      1    0         0     1    0\n  5       0    0      0       0          0      0    0         0     0    1\n  6       0    0      0       0          0      0    0         0     0    0\n  7       0    0      0       0          0      0    0         0     0    0\n  8       0    0      0       0          0      0    0         0     0    0\n  9       0    0      0       0          0      1    0         0     0    0\n  10      0    0      0       0          0      0    0         0     0    0\n    Terms\nDocs learn love nlp machin door keep said wolf arrow banana fli fruit like time\n  1      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  2      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  3      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  4      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  5      1    1   1      0    0    0    0    0     0      0   0     0    0    0\n  6      1    1   0      1    0    0    0    0     0      0   0     0    0    0\n  7      0    0   0      0    1    1    1    1     0      0   0     0    0    0\n  8      0    0   0      0    0    0    0    0     1      1   2     1    2    1\n  9      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  10     0    0   0      0    0    0    0    0     0      0   0     0    0    0\n    Terms\nDocs gass ghg global greenhous hole hydrofluorocarbon montreal ozon protocol\n  1     0   0      0         0    0                 0        0    0        0\n  2     0   0      0         0    0                 0        0    0        0\n  3     0   0      0         0    0                 0        0    0        0\n  4     0   0      0         0    0                 0        0    0        0\n  5     0   0      0         0    0                 0        0    0        0\n  6     0   0      0         0    0                 0        0    0        0\n  7     0   0      0         0    0                 0        0    0        0\n  8     0   0      0         0    0                 0        0    0        0\n  9     1   1      1         1    1                 1        1    1        1\n  10    1   0      0         1    0                 2        0    0        0\n    Terms\nDocs warm carbon dioxid hexafluorid methan monoxid nitrous oxid perfluorocarbon\n  1     0      0      0           0      0       0       0    0               0\n  2     0      0      0           0      0       0       0    0               0\n  3     0      0      0           0      0       0       0    0               0\n  4     0      0      0           0      0       0       0    0               0\n  5     0      0      0           0      0       0       0    0               0\n  6     0      0      0           0      0       0       0    0               0\n  7     0      0      0           0      0       0       0    0               0\n  8     0      0      0           0      0       0       0    0               0\n  9     1      0      0           0      0       0       0    0               0\n  10    0      2      1           1      1       1       1    1               1\n    Terms\nDocs sulfur\n  1       0\n  2       0\n  3       0\n  4       0\n  5       0\n  6       0\n  7       0\n  8       0\n  9       0\n  10      1\n\n\nload(url(“https://cbail.github.io/Trump_Tweets.Rdata”)) head(trumptweets$text)"
  },
  {
    "objectID": "posts/2012/2012-07-26-wikisym-2012_files/index.html",
    "href": "posts/2012/2012-07-26-wikisym-2012_files/index.html",
    "title": "Wikisym 2012",
    "section": "",
    "text": "Due to a kind grant by the WikiMedia Foundation I was able to attend Wikisym 2012 in Linz Austria what follows is my report on the event.\n\n\n\n\n\n\nBackground\n\n\n\n\n\nThe renovated Ars Electronica Center at Linz, seen from the bridge across the Danube at night\n\n\n\nHPaul, CC BY-SA 3.0 via Wikimedia Commons\n\nI am a Wikipedian based in Budapest Hungary. I have been active for the last year with WM.HU and participated in a number of the local event’s chapters ever since being introduced to them at Wikimania 2011 in Haifa. During such a meeting I, Bence Damkos, and other chapter luminaries got to discussing many apparent cultural paradoxes taking place in a virtual community. Since I was studying the theory of games at the time I began to notice that some of the situations were very similar to a classic game such as the prisoner’s dilemma and the battle of the sexes while others resembled second-price sealed actions and bargaining games. I was intrigued and I started publishing some analysis on a page on Meta.\nAt this time I came across some interesting ideas from another researcher, Jodi Schneider who introduced me to the field of Computer Supported Collaborative Work (CSCW) and to her area of research - the deletion process. Eventually, she suggested that I should attend wikisym. However, I had no background in writing a conference paper I asked her for help and she copy-edited my work guiding me through a number of tricky issues. I eventually submitted the paper and to my surprise, it was accepted. So I took a train to Linz - I was surprised when after boarding the train that I had to reserve a seat and accordingly had to stand for the duration of the five-hour journey. By the time I arrived at the little town it was late and I was exhausted. I took a bus and ended in a hotel by the Danube.\n\n\nAt the Conference\nOn the morning of the conference, I took breakfast and met some of my favorite wikipedians - Maryna Pinchuk and Ryan Faulkner who were preparing to give a paper on their work in running editor engagement experiment - in which I had unwittingly participated. After a short chat I made my way to the venue the Ars Electronica and I could not believe my eyes - the conference was hosted by one of the most amazing technology museums in Europe. In the evening, the building would completely dominate the riverside’s view with its digital animation installations.\n\n\n\n\n\nR. Stuart Geiger\n\n\n\nAnne Helmond, CC by-nc-nd 2.0 via flicker\n\nThe Conference began with a number of presentations. I was impressed by most of the presentation but my sentiments were clearly not shared by everyone at the conference. I later learned that some of the more vehement voices were doctoral students who were out to prove their mettle. The papers that most struck my fancy used a number of novel techniques. Ranging from actuarial, survival analysis through SNA to sentiment analysis. Classifying Wikipedia Articles Using Network Motif Counts and Ratios by Guangyu Wu, Martin Harrigan and Pádraig Cunningham was one of the hardest to understand. It used a novel SNA technique to classify Wikipedia articles. However, it seemed that the other participant did not like the level of detail that the researchers had provided. Dr. Bernie Hoagan a Research Fellow from the Oxford Internet Institute asked the researchers why they had not tried to use ERGMs which might give more accurate results. I would later correspond with Dr. Hoagan and he helped me get started with Social network analysis. A paper by Michela Ferron and Paolo Massa titled Psychological processes underlying Wikipedia representations of natural and man-made disasters. It showcased the use of sentiment analysis. I was already familier with this method from my work in a Natural Language Programming outfit in Israel for which I wrote a search engine for the Hebrew Wikipedia. But I had consider this technique as very complex to set-up. On reviewing the paper I realised that an off the shelf tool called LIWC (Linguistic Inquiry and Word Count) can do the job. LIWC was developed by a team lead by James W. Pennebaker whose book The Secret Life of Pronouns is a gentle introduction to the intricacies of sentiment analysis. What remained difficult to grasp was a three-dimensional model of sentiment. I was unfamiliar with the terminology so I would end up rereading this paper a couple of times. But this was not the only paper to use sentiment analysis or natural language technology. Manypedia: Comparing Language Points of View of Wikipedia Communities by Paolo Massa and Federico Scrinzi which showed a tool that allows users to compare different language edition version of the same article in their own language using machine translation. A second paper to discuss sentiment analysis, this time focusing on talk pages was: Emotions and dialogue in a peer-production community: the case of Wikipedia. This paper used an even more complex paradigm than the previous one. It utilized Margaret M. Bradley & Peter J. Lang’s ANEW (Affective Norms for English Words) word list to create a three-dimensional model of sentiment (valence, arousal and dominance). Even more interesting were its conclusions regarding participation of women and its implication on Wikipedia’s growing gender gap.\n\n\n\n\n\nHeather Ford, Jimmy Wales\n\n\n\nMessedrocker, CC BY 1.0 via Wikimedia Commons\n\nI would discuss some of my ideas to some of the participants over dinner. One amusing debate included [Stuart Geiger] and when I quoted a point from an excellent paper he pointed out that he had written it. I also met with heather ford who co-authored a paper with Mr Geiger. Heather Ford told us about her blog Ethnography matters which I started to follow because it turns out that ethnography really matters These include work by Stuart Geiger and on the lives of robots using trace ethnography. During the conference I met with Jodi Schneider but we had little opportunity to chat due to an upcoming deadline. I enjoy following her research on deletion as well as on Argumentation in collaborative deliberations. I decided to help Wikipedia’s research newsletter by abstracting and providing laymen’s summaries to CSCW related research.\n\n\nPanels, Demos and Posters\n\n\n\n\n\nPhoebe Ayers at WikiSym\n\n\n\nRagesoss, CC BY-SA 3.0 via Wikimedia Commons\n\nI found out that the Wikisym conference had a colourful history and participated in a discussion mediated by the delectable Phoebe Ayers on the conference’s future. I suggested that the conference should be collocated with Wikimania since this would help reduce cost of community members who attend the Wikimania conference. A second conundrum being debated being the issue of open academy. This was an issue of growing urgency since the WMF, one of Wikisym’s chief sponsors prefers to support open access open research work. I think that Phoebe Ayers is a wonderful person and was sad to hear she was no longer on the foundation board of directors. Another serendipitous facet of the Wikisym conference is the demo and poster session which allow hackers to present their latest breakthroughs and innovations in technology, of Wikis. This had once been the cornerstone of the conference. I met the developers of TikiWiki as well as the a Java based XWiki. I decided that one day I would implement my own version of the wiki.\n\n\nJimmy Wales’ Keynote Address\nWikisym’s keynote was given by Wikipedia’s co-founder Jimmy Wales. He explained how this talk was one of the ticket he would give this year. However, this was a much better talk than he gave at Wikimania. He mentioned research possibilities and he responded to my question. I was and still am considering if population dynamics could affect phase changes within the community. My question was if a Wiki’s community dropped below a certain size if it would no longer be viable to maintain it. One example of a Wiki being shut down was the 9-11 wiki. I found Wales’ answer enlightening - he said that big or small the community should have little problem adapting to take care of it’s Wiki. Another point worth mentioning was his recommendation to use Wiki data sets of smaller wikis in research. He recommended Muppet wiki as an example of a wiki with a significantly different governance structure than Wikipedia.\n\n\nAfter the conference\nFollowing the conference, I kept in touch with a number of the participants. I applied myself to study social network analysis as well as data analysis with R. I increased my participation in the research newsletter. I hope to expand my research further using population dynamics on graphs and evolutionary game theory. However, with all the new research methods, I’ve gleaned. I am uncertain what direction my future investigations will take only that they will be even more exciting than before.\n\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Wikisym 2012},\n  date = {2022-07-26},\n  url = {https://orenbochman.github.io/blog//posts/2012/2012-07-26-wikisym-2012_files},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Wikisym 2012.” July 26, 2022. https://orenbochman.github.io/blog//posts/2012/2012-07-26-wikisym-2012_files."
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html",
    "href": "posts/2023/2023-12-20-autogluon/index.html",
    "title": "AutoGluon Cheetsheets",
    "section": "",
    "text": "AutoGluon is a powerful framework for auto-ML."
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "href": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "title": "AutoGluon Cheetsheets",
    "section": "Tabular",
    "text": "Tabular\n\n\n\nTabular"
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "href": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "title": "AutoGluon Cheetsheets",
    "section": "Time Series",
    "text": "Time Series"
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "href": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "title": "AutoGluon Cheetsheets",
    "section": "Multimodal",
    "text": "Multimodal"
  },
  {
    "objectID": "posts/2024/welcome/index.html",
    "href": "posts/2024/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\n\nthumbnail\n\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2024-01-26},\n  url = {https://orenbochman.github.io/blog//posts/2024/welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. “Welcome To My Blog.” January 26,\n2024. https://orenbochman.github.io/blog//posts/2024/welcome."
  }
]