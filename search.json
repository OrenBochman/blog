[
  {
    "objectID": "posts/2024/welcome/index.html",
    "href": "posts/2024/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\n\nthumbnail\n\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nthumbnail\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2024-01-26},\n  url = {https://orenbochman.github.io/blog//posts/2024/welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. “Welcome To My Blog.” January 26,\n2024. https://orenbochman.github.io/blog//posts/2024/welcome."
  },
  {
    "objectID": "posts/2024/BBdM/BBdM.html",
    "href": "posts/2024/BBdM/BBdM.html",
    "title": "Political Scenario Prediction Using Game theory",
    "section": "",
    "text": "A review of MESQUITA (2011)\nThis technical paper is fairly challenging to understand. The main thrust of this work is when conducting an inteligence assement of some future political scenario, assuming one can identify the main actors thier relative influence, their positions and thier belief about other players we can use a formal model to assess how they will behave and interpret various outcomes.\nNow there are any number of ways this can go bad:\nSo now that we named the elephants in the room, we can talk about can address the least interesting part of the paper - the fact that the models perform very well when fed with data from expert analysts, assuming that the raw data is accurate.\nPrior work by Tatelock outlines the notion of comparing forecasting skill of analysts AKA experts who engage in competition charectrises thier abilities. Tatelock call the best Superforcastors suggesting that there is method behind all this madness. A second claim is that formal Models tend to outperform the experts given thier raw data. Mesiqua explains that this is due to reduced variance.\nMy current interest are:\nI suppose that if you get good at structuring the problem then interpretation can be much easier.\nIn some senses the old model is very simple considering how well it performs. Yet Bayesian games are the kind of games we all played in preschool or at least not to solve them for a perfect Bayesian equilibrium. So there is that inherent mathmatical complexity to deal with. If I’m not mistaken the author is bent on reporting his successes without revealing too much about how to reproduce his work. I havent reviewd the relevant papers. A second big chunk of this paper deals with the performance of the old model and responding to criticism of his work. So the readers need to decipher as much as they can. However it is not very much motivated. BBdM is an eloquent speaker and an excellent author of several book."
  },
  {
    "objectID": "posts/2024/BBdM/BBdM.html#structure-of-the-new-model",
    "href": "posts/2024/BBdM/BBdM.html#structure-of-the-new-model",
    "title": "Political Scenario Prediction Using Game theory",
    "section": "Structure of the New Model",
    "text": "Structure of the New Model\nThe new model’s structure is much more complex than the expected utility model and so it will be important for it to outperform that model meaningfully to justify its greater computational complexity. Inputs are, in contrast, only modestly more complicated or demanding although what is done with them is radically different.\nEach player is uncertain whether the other player is a hawk or a dove and whether the other player is pacific or retaliatory. By hawk I mean a player who prefers to try to coerce a rival to give in to the hawk’s demands even if this means imposing (and enduring) costs rather than compromising on the policy outcome. A dove prefers to compromise rather than engage in costly coercion to get the rival to give in. A retaliatory player prefers to defend itself (potentially at high costs), rather than allow itself to be bullied into giving in to the rival, while a pacific player prefers to give in when coerced in order to avoid further costs associated with self-defense. The priors on types are set at 0.5 at the game’s outset and are updated according to Bayes’ Rule. This element is absent in Bueno de Mesquita and Lalman (1992). In fact, the model here is an iterated, generalized version of their model, integrating results across N(N–1) player dyads, introducing a range of uncertainties and an indeterminate number of iterations as well as many other features as discussed below. Of course, uncertainty is not and cannot be limited to information about player types when designing an applied model. We must also be concerned that there is uncertainty in the estimates of values on input variables whether the data are derived, as in the tests here, from experts or, as in cases reported on in the final two\nthere are 4 type of players: &lt;[hawk|dove],[pacific,retalitory]&gt;\n\n\n\nold-model\nnew model"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html",
    "href": "posts/2023/2023-05-10-migration-notes/index.html",
    "title": "Quarto Migration Notes",
    "section": "",
    "text": "I was able to stand on the shoulders of :giants when I migrated this blog."
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#giants",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#giants",
    "title": "Quarto Migration Notes",
    "section": ":giants",
    "text": ":giants\n(Rapp 2022) (Navarro 2022), (Hill 2022), (Kaye 2022)\n\nRapp, Albert. 2022. “The Ultimate Guide to Starting a Quarto Blog.” June 24, 2022. https://albert-rapp.de/posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html.\n\nNavarro, Danielle. 2022. “Porting a Distill Blog to Quarto.” April 20, 2022. https://blog.djnavarro.net/posts/2022-04-20_porting-to-quarto.\n\nHill, Alison. 2022. “We Don’t Talk about Quarto.” April 4, 2022. https://www.apreshill.com/blog/2022-04-we-dont-talk-about-quarto/.\n\nKaye, Ella. 2022. “Welcome to My Quarto Website!” December 11, 2022. https://ellakaye.co.uk/posts/2022-12-11_welcome-quarto."
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#markdown",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#markdown",
    "title": "Quarto Migration Notes",
    "section": "Markdown",
    "text": "Markdown\n\nQuarto’s markdown isn’t my favorite markdown implementation.\nIt is based on pandoc spec"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#the-devil-is-in-the-details",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#the-devil-is-in-the-details",
    "title": "Quarto Migration Notes",
    "section": "The devil is in the details",
    "text": "The devil is in the details\nThere are lots of details that should be in the guide that are scattered all over the quarto site.\nI decided that all posts should have the following fields in their front matter:\n\ntitle\nsubtitle\ndescription\ndate\ncategories\nimage\nimage-description"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#virtual-environments",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#virtual-environments",
    "title": "Quarto Migration Notes",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nare documented here\nideal one can have one virtual environment for the whole site"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#lightbox-galleries",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#lightbox-galleries",
    "title": "Quarto Migration Notes",
    "section": "Lightbox Galleries",
    "text": "Lightbox Galleries\nso far I used this only in the this page\nthe light box plugin was integrated into Quarto in the version 4.1 which I migrated to. I have been using light box to make notes of talks and so on. So in for this blog adding light boxes is a breeze.\nAll that’s realy needed is to change setting in the frontmatter:\nlightbox: true\nwhich I did for all posts by adding the setting to the _metadata.yaml in the posts directory. And now all images default to opening within their own lightbox when clicked upon.\nto disable the feature say, on a logo for example just add .no-lightbox css style to the image like this:\n![caption](filename.png){.no-lightbox}\nif you want to be able to scroll through a series of images we need to decorate each images as follows:\n![caption](filename.png){group=\"my-gallery\"}\nAn added bonus is that it is possible to zoom into these light-boxed images"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#extras",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#extras",
    "title": "Quarto Migration Notes",
    "section": "Extras",
    "text": "Extras\n\nthe about page is based on postcards package\nicons for navigation come from bootstrap\ncover images are from pexels"
  },
  {
    "objectID": "posts/2023/2023-05-10-migration-notes/index.html#nutshell-addin",
    "href": "posts/2023/2023-05-10-migration-notes/index.html#nutshell-addin",
    "title": "Quarto Migration Notes",
    "section": "Nutshell Addin",
    "text": "Nutshell Addin\n:Nutshells: seems like an interesting way to reduce the clutter by hiding unessential exposition.\n\nWill users realize they can interact with these nutshells 1 ?\nNot sure if the implementation is good enough to be worth the hassle.\nThey also seem to be unwieldy, 2?\nCan we get the page to keep track of the nutshell state so we can bookmarkit ?\nI’m going to try it out and see if they are pages for which they are suitable.\n\n1 in landing pages I used to have an animation to get users to engage with in-lined content 2 could we loose the callout/baloon\n:Nutshells\nWhich are links that expand inline into content\n\n\nOpen issues:\n\ncan I readily integrate books and presentation into this blog ?\n\ncan I drop them in or do I need to build them in another repo\nthen deploy\nthen link!?\n\nhow about embedding repls\nhow about embedding shiny live apps\n\nhttps://github.com/shafayetShafee\n\n\nEmbedding PDF\n\nplugin repo\ndocumentation\n\ninstallation\nquarto add jmgirard/embedpdf\n{{&lt; pdf dummy.pdf &gt;}}\n{{&lt; pdf dummy.pdf width=100% height=800 &gt;}}\n{{&lt; pdf dummy.pdf border=1 &gt;}}\n{{&lt; pdf dummy.pdf class=myclass &gt;}}"
  },
  {
    "objectID": "posts/2017/dnn-glossery/2017-09-10-deep-neural-networks-glossery.html",
    "href": "posts/2017/dnn-glossery/2017-09-10-deep-neural-networks-glossery.html",
    "title": "Glossary of terms for Deep Neural Networks",
    "section": "",
    "text": "Glossary of terms in Deep leaning and ML\n\nAccuracy\n\nThe fraction of predictions that a classification model got right.\n\nactivation\n\nemphasizes that neuron like a real neuron may be on or off. In reality a negative bias will create a threshold to activation, otherwise, the neuron will always produce output. Also called [value] or [output].\n\nactivation function\n\nThe activation function is an attempt to mimic the biological neuron’s output in response to it input. This is generally a non-linear function. Some examples are RELU, Sigmoid, Tanh, Leaky RELU, Maxout and there are many others. All other things being equal RELU has emerged as the preferred activation function to start with.\n\nAdaGrad\n\nA gradient descent learning algorithm that re-scales the gradients of each parameter, effectively giving each parameter an independent learning rate. c.f. (Duchi, Hazan, and Singer 2011).\n\n\nDuchi, John, Elad Hazan, and Yoram Singer. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” Journal of Machine Learning Research 12 (7).\nAnomaly detection\n\nThe process of identifying outliers that are considered candidates for removal from a dataset, Typically for being nonrepresentative high leverage points.\n\nAttention\n\nA mechanism that aggregate information from a set of inputs in a data-dependent manner. An attention mechanism might consist of a weighted sum over a set of inputs, where the weight for each input is computed by another part of the neural network.\n\nAttribute\n\nSynonym for feature.\n\nAutomation bias\n\nWhen a human decision-maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors.\n\nBackpropagation\n\nThe main algorithm for performing gradient descent on neural networks. First, the output values of each node are calculated (and cached) in a forward pass. Then, the partial derivative of the error with respect to each parameter is calculated in a backward pass through the graph.\n\nBagging\n\nA method to train an ensemble where each constituent model trains on a random subset of training examples sampled with replacement. E.g. a random forest is a collection of decision trees trained with bagging. The term bagging is short for bootstrap aggregating.\n\nBatch normalization\n\nNormalizing the input or output of the activation functions in a hidden layer. Batch normalization increases a network’s stability by protecting against outlier weights, enable higher learning rates and reduce **overfitting`.\n\nBatch size\n\nThe number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000. Batch size is usually fixed during training and inference by GPU memory constraints. Some frameworks like TensorFlow allow using dynamic batch sizes.\n\nBias term\n\na term that allows for the identification of the neuron threshold as the weight on a special, constant input.\n\nBayesian neural network\n\nA probabilistic neural network that accounts for uncertainty in weights and outputs. A Bayesian neural network relies on Bayes’ Theorem to calculate uncertainties in weights and predictions. A Bayesian neural network can be useful when it is important to quantify uncertainty, such as in models related to pharmaceuticals. Bayesian neural networks can also help prevent overfitting.\n\nBayesian optimization\n\nA probabilistic regression model technique for optimizing computationally expensive objective functions by instead optimizing a surrogate that quantifies the uncertainty via a Bayesian learning technique. Since Bayesian optimization is itself very expensive, it is usually used to optimize expensive-to-evaluate tasks that have a small number of parameters, such as selecting hyperparameters.\n\nBinning\n\nsynonym for bucketing\n\nBoltzmann machine\n\nan algorithm for learning the probability distribution on a set of inputs by means of weight changes using noisy responses.\n\nBoosting\n\nA machine learning technique that iteratively combines a set of simple and not very accurate classifiers (referred to as “weak” classifiers) into a classifier with high accuracy (a “strong” classifier) by upweighting the examples that the model is currently misclassifying.\n\nbucketing\n\nConverting a (usually continuous) feature into multiple binary features called buckets or bins, typically based on value range. For example, instead of representing temperature as a single continuous floating-point feature, you could chop ranges of temperatures into discrete bins.\n\ncategorical\n\nFeatures or columns in the data with a discrete set of possible values.\n\nConnection weight\n\nThe parameter which is used to set the importance to an input coming to a given neuron from another one.\n\nDelta rule\n\nthe simplest learning rule, in which weights are changed proportionally to the discrepancy between actual output and desired output.\n\nError surface\n\nthe surface in the weight space indicating how the error in the output of a neural network depends on these weights.\n\nFeature\n\na column in a training case Feed-in\n\n\nthe number of inputs for a unit\n\nFan out\n\nthe amount of spread in output from a neuron.\n\nHebb learning law\n\nmodification of a connection weight proportional to the activities of the input and output neurons.\n\nHopfield network\n\na network with symmetric connection weights and thresholding of neural response.\n\nInput\n\nis ambiguous, because more often, input is short for **input neuron`.\n\nInput unit\n\nspecial neuron receiving only input activity which is fed on to the rest of the network.\n\nLayer\n\na collection of neurons all of which receive input from a preceding set of neurons (or inputs), and send their outputs to other neurons or outside the net.\n\nLearning law\n\nrule for changing the connection weights in a neural network.\n\nLearning rate\n\namount by which the connection weights change at each learning step.\n\nMomentum\n\na term added to the weight change in back-propagation to achieve better learning by jumping out of local minima.\n\nNeuron\n\na synonym for unit emphasizing the analogy with real brains.\n\nOutput\n\nlike value but emphasizing that it’s different from the input.\n\nParameter\n\nthe weights and biases learned by the network. Additional parameters - which are not necessarily learned or not directly part of the network are called hyperparameters\n\nRecurrent neural network\n\none in which output activity is fed back into the input or hidden layers. Also called RNN Reinforcement training\n\n\nmodification of connection weights.\n\nTest set\n\nthe set of input and output patterns used to test if a neural network has been trained effectively.\n\nTraining set\n\nthe set of input-output patterns provided to train the network.\n\nTraining case\n\na row in the dataset is the most commonly used and is quite generic. Also called input and training example Training example\n\n\nemphasizes the analogy with human learning: we learn from examples.\n\nTraining point\n\nemphasizes that it’s a location in a high-dimensional space.\n\nUnit\n\na node in a neural network`. Nodes consists of an activation function, a weight, an input and output called the activation. The term unit emphasizes that it’s one component of a large network. Also referred to as a neuron** .\n\nValue\n\na synonym for activation, referencing the output value of the activation function (RELU, sigmoid, tanh, etc.) when acting on its input.\n\nWeight space\n\nA high dimensional space with each dimension corresponding to the weight of a single neuron`. Weight space corresponds to the space of all possible weights. Each point in the space is a collection of weights and each training case can be represented as a hyper-plane** passing through the origin. See also error surface\n\nloss function\n\nemphasizes that we’re minimizing it, without saying much about what the meaning of the number is.\n\nerror function\n\nemphasizes that it’s the extent to which the network gets things wrong.\n\nobjective function\n\nis very generic. This is the only one where it’s not clear whether we’re minimizing or maximizing it.\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Glossary of Terms for {Deep} {Neural} {Networks}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//posts/2017/dnn-glossery/2017-09-10-deep-neural-networks-glossery.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Glossary of Terms for Deep Neural\nNetworks.” August 6, 2017. https://orenbochman.github.io/blog//posts/2017/dnn-glossery/2017-09-10-deep-neural-networks-glossery.html."
  },
  {
    "objectID": "posts/2017/dnn-15/2017-08-06-deep-neural-networks-notes-15.html",
    "href": "posts/2017/dnn-15/2017-08-06-deep-neural-networks-notes-15.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course\n\nNeural Networks for Machine Learning\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} {From} {Hinton’s}\n    {Course}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//posts/2017/dnn-15/2017-08-06-deep-neural-networks-notes-15.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes From Hinton’s\nCourse.” August 6, 2017. https://orenbochman.github.io/blog//posts/2017/dnn-15/2017-08-06-deep-neural-networks-notes-15.html."
  },
  {
    "objectID": "posts/2017/dnn-13/2017-08-06-deep-neural-networks-notes-13.html",
    "href": "posts/2017/dnn-13/2017-08-06-deep-neural-networks-notes-13.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course\n\nNeural Networks for Machine Learning\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} {From} {Hinton’s}\n    {Course}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//posts/2017/dnn-13/2017-08-06-deep-neural-networks-notes-13.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes From Hinton’s\nCourse.” August 6, 2017. https://orenbochman.github.io/blog//posts/2017/dnn-13/2017-08-06-deep-neural-networks-notes-13.html."
  },
  {
    "objectID": "posts/2017/dnn-11/2017-08-06-deep-neural-networks-notes-11.html",
    "href": "posts/2017/dnn-11/2017-08-06-deep-neural-networks-notes-11.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course\n\nNeural Networks for Machine Learning\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} {From} {Hinton’s}\n    {Course}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//posts/2017/dnn-11/2017-08-06-deep-neural-networks-notes-11.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes From Hinton’s\nCourse.” August 6, 2017. https://orenbochman.github.io/blog//posts/2017/dnn-11/2017-08-06-deep-neural-networks-notes-11.html."
  },
  {
    "objectID": "posts/2017/dnn-09/2017-08-06-deep-neural-networks-notes-09.html",
    "href": "posts/2017/dnn-09/2017-08-06-deep-neural-networks-notes-09.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course TODO: make a jupyter notebook which takes input images and augments them."
  },
  {
    "objectID": "posts/2017/dnn-09/2017-08-06-deep-neural-networks-notes-09.html#lecture-9a-overview-of-ways-to-improve-generalization",
    "href": "posts/2017/dnn-09/2017-08-06-deep-neural-networks-notes-09.html#lecture-9a-overview-of-ways-to-improve-generalization",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "Lecture 9a: Overview of ways to improve generalization",
    "text": "Lecture 9a: Overview of ways to improve generalization\nIn the discussion of overfitting, we assume that the bottleneck of our ability to do machine learning is the\namount of data that we have; not the amount of training time or computer power that we have. ### Preventing overfitting Four approaches to reduce overfitting due to too many parameters to training rows: 1. Get more data - the best option. 2. Use a model that has the right capacity: - enough to fit the true regularities. - not enough to fit spurious data. (a bit cheeky consider the massive capacity of most NN) 3. Average models. - Ensambling use models with different forms. - Bagging train the model on different subsets the training data. 4.: Bayesian: - use a single nn architecture but average the predictions made by many weight vectors 1. Getting more data via augmentation (increases signal to noise)\n1. consider normalization 2. Sample-wise\n3. Feature wise pixel standardization 4. PCA whitening - reduces dimension + whiting 5. ZCA the idea is to reducing effect of correlation in adjacent pixels by normalizing feature variance and reducing correlation at features. (does not reduce dimensions of the data)\n\n\n\n\n\n\n\ntransform\nimage\n\n\n\n\nOriginal\n\n\n\nFeature Standardization\n\n\n\nZCA whitening\n\n\n\nRandom Rotations\n\n\n\nRandom shifts\n\n\n\nRandom Flips\n\n\n\nRandom affine transforms\n\n\n\nContrast Stretching\n\n\n\nHistogram Equalization\n\n\n\nAdaptive Histogram Equalization \n\n\n\nCLAHE contrast stretching + adaptive histogram equalization = Contrast limited adaptive histogram equalization\n\n\n\n\nconsider augmentation. random crop/rotation/shear/mirroring/flip scaling blocking out rectangles elastic deformation mesh (used in Unet) contrast stretching + adaptive histogram equalization = Contrast limited adaptive histogram equalization (CLAHE) ZCA whitening transform \n10 15 20 25 0 5 10 15 20 25 10 15 20 25 0 5 10 15 2025 0 5 10 1520 25 0 5 10 1520 25 10 15 20 25 10 15 20 25\n10 15 20 25 0 10 15 20 25 0 5 5 10 15 20 25 10 15 20 25 15 20 25 0 10 15 20 25 0 5 5 10 15 20 25 10 15 20 25 10 15 20 25 10 15 20 25 I폐言 하디결\nStandardized Feature MNIST Images ZCA Whitening MNIST Images random affine transforms\nContrast Stretching Histogram Equalization Adaptive Histogram Equalization \ncode: image-augmentation-deep-learning-keras (on nminst) data augmentation with elastic deformations\n(Reduce) model capacity early stopping. regularization schemes. Average different models (architectures/algs/ partitions of the data aka bagging) - see lecture 10 Bayesian train same model many times and then use multiple weights to predict.\nA new issue - is that we now need to fit hyperparameters. This introduces a new idea - a three way split of the data training - for learning model parameters validation - for fitting hyper parameters test - for a final unbiased estimate of the network A further refinement is n-way cross validation:"
  },
  {
    "objectID": "posts/2017/dnn-09/2017-08-06-deep-neural-networks-notes-09.html#lecture-9b-limiting-the-size-of-the-weights",
    "href": "posts/2017/dnn-09/2017-08-06-deep-neural-networks-notes-09.html#lecture-9b-limiting-the-size-of-the-weights",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "Lecture 9b: Limiting the size of the weights",
    "text": "Lecture 9b: Limiting the size of the weights\nLimiting the size of the weigh The standard L2 weight penalty involves adding an extra term to the cost function that penalizes the squared weights. — This keeps the weights small unless they have big error derivatives. öWi\nThere is some math in this video. It’s not complicated math. You should make sure to understand it. ## Lecture 9c: Using noise as a regularizer adding noise to the input can have a regularizing effect. Think lets add noise to a picture - it drowns out most of the small features say blurring them. But for large items - we are trying to learn - they look mostly the same. The advantage is that adding noise is easy. Anyhow - this is more of a buildup of an abstract idea that will later be interpreted using full Bayesian learning. L2 weight-decay via noisy inp Suppose we add Gaussian noise to the inputs. — The variance of the noise is amplified by the squared weight before going into the next layer. In a simple net with a linear output unit directly connected to the inputs, the amplified noise gets added to the output. This makes an additive contribution to the This slide serves to show that noise is not a crazy idea. The penalty strength can be thought of as being\nσ2i , or twice that (to compensate for the 1/2 in the weight decay cost function), but that detail is not important here. Second slide (the math slide)\nThe reason why the middle term is zero is that all of the epsilons have mean zero. You may notice that the result is not exactly like the L2 penalty of the previous video: the factor 1/2 is missing. Or equivalently, the strength of the penalty is not sigma i squared, but twice that. The main point, however, is that this noise is equivalent to an L2 penalty. Jargon: overfitting, underfitting, generalization, and regularization Overfitting can be thought of as the model being too confident about what the data is like: more confident than would be justified, given the limited amount of training data that it was trained on. If an alien from outer space would take one look at a street full of cars (each car being a training case), and it so happens that there were only two Volkswagens there, one dark red and one dark blue, then the alien might conclude “all Volkswagens on Earth are of dark colours.” That would be overfitting. If, on the other hand, the alien would be so reluctant to draw conclusions that he even fails to conclude that cars typically have four wheels, then that would be underfitting. We seek the middle way, where we don’t draw more than a few unjustified conclusions, but we do draw most of the conclusions that really are justified. Regularization means forcing the model to draw fewer conclusions, thus limiting overfitting. If we overdo it, we end up underfitting. Jargon: “generalization” typically means the successful avoidance of both overfitting and underfitting. Since overfitting is harder to avoid, “generalization” often simply means the absence of (severe) overfitting. The “accidental regularities” that training data contains are often complicated patterns. However, NNs can learn complicated patterns quite well. Jargon: “capacity” is learning capacity. It’s the amount of potential (artificial) brain power in a model, and it mostly depends on the number of learned parameters (weights & biases). ## Lecture 9d: Introduction to the full Bayesian approach The full Bayesian approach could provide an alternative to using SGD. However with the exception of very simple models it is usually computationally intractable as it requires finding the prior distribution for all the parameters.\nWe can start with an prior P(params) - and adjust it given each training item.\nGiven some data we would have to calculate its likelihood i.e. p(data)\nBut to do this we would need to see how it effects all parameter settings - this is the real issue as for 10 settings for 100 nodes we would need to test 10^100 weight combinations…\nthis is an outline of the Bayesian approach.\nthere is a prior distribution over parameters, there is data, say the training data and we can calculate its likelihood and combine it with the prior to get a posterior.\nWith sufficient Bayesian updating will in the limit beat an uninformative prior.\nbut he does not go into how much data. The Bayesian framework The Bayesian framework assumes that we distribution for everything. — The prior may be very vague. — When we see some data, we combine our with a likelihood term to get a posterior distr — The likelihood term takes into account how observed data is given the parameters of th\na 100 coin tosses motivates the frequentist approach which uses the (ML) maximal likelihood estimate of the probability.\nNext calculates the ml is 0.53 by differentiating and setting the derivative equal to zero. Next asks what if we have only one coin toss. which is a kin to asking “what if the experiment is too small and there are unobserved outcomes?” in which case we cannot account for their likelihood in a ML estimate. A coin tossing example Suppose we know nothing about coins excep tossing event produces a head with some unl probability p and a tail with probability I-p.  — Our model of a coin has one parameter, p Suppose we observe 100 tosses and there al What is p? here D is the data and W is a set of weights.\nBayes Theorem joint probability prior probability of weight vector W probabilit data give p(W) p(DlW) ID)\nHowever, it may be possible to approximate a prior. The terms “prior”, “likelihood term”, and “posterior” are explained in a more mathematical way at the end of the video, so if you’re confused, just keep in mind that a mathematical explanation follows. For the coin example, try not to get confused about the difference between “p” (the probability of seeing heads) and “P” (the abbreviation for “probability”). Jargon: “maximum likelihood” means maximizing the likelihood term, without regard to any prior that there may be. At 8:22 there’s a slightly incorrect statement in the explanation, though not in the slide. The mean is not at .53 (although it is very close to that). What’s really at .53 is the mode, a.k.a. the peak, a.k.a. the most likely value. The Bayesian approach is to average the network’s predictions, at test time, where “average” means that we use network parameters according to the posterior distribution over parameter settings given the training data. Essentially, we’re averaging the predictions from many predictors: each possible parameter setting is a predictor, and the weight for that weighted average is the posterior probability of that parameter setting. “It’s helpful to know that whenever you see a squared error being minimized, you can make a probabilistic interpretation of what’s going on, and in that probabilistic interpretation, you’ll be maximizing the  log probability under a Gausian.” So the proper Bayesian approach, is to find the full posterior distribution over all possible weight vectors. If there’s more than a handful of weights, that’s hopelessly difficult when you have  a non-linear net. Bayesians have a lot of ways of  approximating this distribution, often using Monte Carlo methods.  But for the time being, let’s try and do something simpler.  Let’s just try to find the most probable weight vector.  So the single setting of the weights that’s most probable given the prior knowledge we have and given the data. So what we’re going to try and do is find  an optimal value of W by starting with some random weight vector, and then  adjusting it in the direction that improves the probability of that weight  factor given the data. It will only be a local optimum. The Bayesian interpretation of weight -log I D) = —logp(D I W) 1 (yc -tc)2 1)\nassuming that the model makes a Gaussian prediction — log p(W) 20w t assuming a for the weig"
  },
  {
    "objectID": "posts/2017/dnn-09/2017-08-06-deep-neural-networks-notes-09.html#lecture-9e-the-bayesian-interpretation-of-weight-decay",
    "href": "posts/2017/dnn-09/2017-08-06-deep-neural-networks-notes-09.html#lecture-9e-the-bayesian-interpretation-of-weight-decay",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "Lecture 9e: The Bayesian interpretation of weight decay",
    "text": "Lecture 9e: The Bayesian interpretation of weight decay\nIn this video, we use Bayesian thinking (which is widely accepted as very reasonable) to justify weight decay (which may sound like an arbitrary hack). Maximum A Posteriori (MAP) learning means looking for that setting of the network parameters that has greatest posterior probability given the data. As such it’s somewhat different from the simpler “Maximum Likelihood” learning, where we look for the setting of the parameters that has the greatest likelihood term: there, we don’t have a prior over parameter settings, so it’s not very Bayesian at all. Slide 1 introduces Maximum Likelihood learning. Try to understand well what that has to do with the Bayesian “likelihood term”, before going on to the next slide. The reason why we use Gaussians for our likelihood and prior is that that makes the math simple, and fortunately it’s not an insane choice to make. However, it is somewhat arbitrary. 10:15: Don’t worry about the absence of the factor 1/2 in the weight decay strength. It doesn’t change the story in any essential way."
  },
  {
    "objectID": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html",
    "href": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course"
  },
  {
    "objectID": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html#lecture-7a-modeling-sequences-a-brief-overview",
    "href": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html#lecture-7a-modeling-sequences-a-brief-overview",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "Lecture 7a: Modeling sequences: A brief overview",
    "text": "Lecture 7a: Modeling sequences: A brief overview\nThis video talks about some advanced material that will make a lot more sense after you complete the course: it introduces some generative models for unsupervised learning (see video 1e), namely Linear Dynamical Systems and Hidden Markov Models. These are neural networks, but they’ve very different in nature from the deterministic feedforward networks that we’ve been studying so far. For now, don’t worry if those two models feel rather mysterious. However, Recurrent Neural Networks are the next topic of the course, so make sure that you understand them. ## Lecture 7b: Training RNNs with back propagation Most important prerequisites to perhaps review: videos 3d and 5c (about backprop with weight sharing). After watching the video, think about how such a system can be used to implement the brain of a robot as it’s producing a sentence of text, one letter at a time. What would be input; what would be output; what would be the training signal; which units at which time slices would represent the input & output? ## Lecture 7c: A toy example of training an RNN Clarification at 3:33: there are two input units. Do you understand what each of those two is used for? The hidden units, in this example, as in most neural networks, are logistic. That’s why it’s somewhat reasonable to talk about binary states: those are the extreme states. ## Lecture 7d: Why it is difficult to train an RNN This is all about backpropagation with logistic hidden units. If necessary, review video 3d and the example that we studied in class. Remember that Geoffrey explained in class how the backward pass is like an extra long linear network? That’s the first slide of this video. Echo State Networks: At 6:36, “oscillator” describes the behavior of a hidden unit (i.e. the activity of the hidden unit oscillates), just like we often use the word “feature” to functionally describe a hidden unit. Echo State Networks: like when we were studying perceptrons, the crucial question here is what’s learned and what’s not learned. ESNs are like perceptrons with randomly created inputs. At 7:42: the idea is good initialization with subsequent learning (using backprop’s gradients and stochastic gradient descent with momentum as the optimizer). ## Lecture 7e: Long-term Short-term-memory This video is about a solution to the vanishing or exploding gradient problem. Make sure that you understand that problem first, because otherwise this video won’t make much sense. The material in this video is quite advanced. In the diagram of the memory cell, there’s a somewhat new type of connection: a multiplicative connection. It’s shown as a triangle. It can be thought of as a connection of which the strength is not a learned parameter, but is instead determined by the rest of the neural network, and is therefore probably different for different training cases. This is the interpretation that Mr Hinton uses when he explains backpropagation through time through such a memory cell. That triangle can, alternatively, be thought of as a multiplicative unit: it receives input from two different places, it multiplies those two numbers, and it sends the product somewhere else as its output. Which two of the three lines indicate input and which one indicates output is not shown in the diagram, but is explained. In Geoffrey’s explanation of row 4 of the video, “the most active character” means the character that the net, at this time, consider most likely to be the next character in the character string, based on what the pen is doing. ## Lecture 9a: Overview of ways to improve generalization In the discussion of overfitting, we assume that the bottleneck of our ability to do machine learning is the\namount of data that we have; not the amount of training time or computer power that we have. Preventing overfitting Approach 1: Get more data! — Almost always the best bet if you have enough compute power to train on more data. Approach 2: Use a model that has the right capacity: — enough to fit the true regularities. • Approach 3: Avel models. — Use models v — Or train the rx subsets of tlu is called “bag Approach 4: (Ba) Four approaches to reduce overfitting due to too many parameters to training rows: Getting more data (increases signal to noise) consider normalization Sample-wise\nFeature wise pixel standardization PCA whitening - reduces dimension + whiting ZCA - http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf - the idea is to reducing effect of correlation in adjacent pixels by normalizing feature variance and reducing correlation at features. (does not reduce dimensions of the data) consider augmentation. random crop/rotation/shear/mirroring/flip scaling blocking out rectangles elastic deformation mesh (used in Unet) contrast stretching + adaptive histogram equalization = Contrast limited adaptive histogram equalization (CLAHE) ZCA whitening transform \n10 15 20 25 0 5 10 15 20 25 10 15 20 25 0 5 10 15 2025 0 5 10 1520 25 0 5 10 1520 25 10 15 20 25 10 15 20 25\n10 15 20 25 0 10 15 20 25 0 5 5 10 15 20 25 10 15 20 25 15 20 25 0 10 15 20 25 0 5 5 10 15 20 25 10 15 20 25 10 15 20 25 10 15 20 25 I폐言 하디결\nStandardized Feature MNIST Images ZCA Whitening MNIST Images random affine transforms\nContrast Stretching Histogram Equalization Adaptive Histogram Equalization \ncode: image-augmentation-deep-learning-keras (on nminst) data augmentation with elastic deformations\n(Reduce) model capacity early stopping. regularization schemes. Average different models (architectures/algs/ partitions of the data aka bagging) - see lecture 10 Bayesian train same model many times and then use multiple weights to predict.\nA new issue - is that we now need to fit hyperparameters. This introduces a new idea - a three way split of the data training - for learning model parameters validation - for fitting hyper parameters test - for a final unbiased estimate of the network A further refinement is n-way cross validation:"
  },
  {
    "objectID": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html#lecture-9b-limiting-the-size-of-the-weights",
    "href": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html#lecture-9b-limiting-the-size-of-the-weights",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "Lecture 9b: Limiting the size of the weights",
    "text": "Lecture 9b: Limiting the size of the weights\nLimiting the size of the weigh The standard L2 weight penalty involves adding an extra term to the cost function that penalizes the squared weights. — This keeps the weights small unless they have big error derivatives. öWi\nThere is some math in this video. It’s not complicated math. You should make sure to understand it. ## Lecture 9c: Using noise as a regularizer adding noise to the input can have a regularizing effect. Think lets add noise to a picture - it drowns out most of the small features say blurring them. But for large items - we are trying to learn - they look mostly the same. The advantage is that adding noise is easy. Anyhow - this is more of a buildup of an abstract idea that will later be interpreted using full Bayesian learning. L2 weight-decay via noisy inp Suppose we add Gaussian noise to the inputs. — The variance of the noise is amplified by the squared weight before going into the next layer. In a simple net with a linear output unit directly connected to the inputs, the amplified noise gets added to the output. This makes an additive contribution to the This slide serves to show that noise is not a crazy idea. The penalty strength can be thought of as being\nσ2i , or twice that (to compensate for the 1/2 in the weight decay cost function), but that detail is not important here. Second slide (the math slide)\nThe reason why the middle term is zero is that all of the epsilons have mean zero. You may notice that the result is not exactly like the L2 penalty of the previous video: the factor 1/2 is missing. Or equivalently, the strength of the penalty is not sigma i squared, but twice that. The main point, however, is that this noise is equivalent to an L2 penalty. Jargon: overfitting, underfitting, generalization, and regularization Overfitting can be thought of as the model being too confident about what the data is like: more confident than would be justified, given the limited amount of training data that it was trained on. If an alien from outer space would take one look at a street full of cars (each car being a training case), and it so happens that there were only two Volkswagens there, one dark red and one dark blue, then the alien might conclude “all Volkswagens on Earth are of dark colours.” That would be overfitting. If, on the other hand, the alien would be so reluctant to draw conclusions that he even fails to conclude that cars typically have four wheels, then that would be underfitting. We seek the middle way, where we don’t draw more than a few unjustified conclusions, but we do draw most of the conclusions that really are justified. Regularization means forcing the model to draw fewer conclusions, thus limiting overfitting. If we overdo it, we end up underfitting. Jargon: “generalization” typically means the successful avoidance of both overfitting and underfitting. Since overfitting is harder to avoid, “generalization” often simply means the absence of (severe) overfitting. The “accidental regularities” that training data contains are often complicated patterns. However, NNs can learn complicated patterns quite well. Jargon: “capacity” is learning capacity. It’s the amount of potential (artificial) brain power in a model, and it mostly depends on the number of learned parameters (weights & biases). ## Lecture 9d: Introduction to the full Bayesian approach The full Bayesian approach could provide an alternative to using SGD. However with the exception of very simple models it is usually computationally intractable as it requires finding the prior distribution for all the parameters.\nWe can start with an prior P(params) - and adjust it given each training item.\nGiven some data we would have to calculate its likelihood i.e. p(data)\nBut to do this we would need to see how it effects all parameter settings - this is the real issue as for 10 settings for 100 nodes we would need to test 10^100 weight combinations…\nthis is an outline of the Bayesian approach.\nthere is a prior distribution over parameters, there is data, say the training data and we can calculate its likelihood and combine it with the prior to get a posterior.\nWith sufficient Bayesian updating will in the limit beat an uninformative prior.\nbut he does not go into how much data. The Bayesian frameworl The Bayesian framework assumes that we distribution for everything. — The prior may be very vague. — When we see some data, we combine our with a likelihood term to get a posterior distr — The likelihood term takes into account how observed data is given the parameters of th\na 100 coin tosses motivates the frequentist approach which uses the (ML) maximal likelihood estimate of the probability.\nNext calculates the ml is 0.53 by differentiating and setting the derivative equal to zero. Next asks what if we have only one coin toss. which is a kin to asking “what if the experiment is too small and there are unobserved outcomes?” in which case we cannot account for their likelihood in a ML estimate. A coin tossing example Suppose we know nothing about coins excep tossing event produces a head with some unl probability p and a tail with probability I-p.  — Our model of a coin has one parameter, p Suppose we observe 100 tosses and there al What is p? here D is the data and W is a set of weights.\nBayes Theorem joint probability prior probability of weight vector W probabilit data give p(W) p(DlW) ID)\nHowever, it may be possible to approximate a prior. The terms “prior”, “likelihood term”, and “posterior” are explained in a more mathematical way at the end of the video, so if you’re confused, just keep in mind that a mathematical explanation follows. For the coin example, try not to get confused about the difference between “p” (the probability of seeing heads) and “P” (the abbreviation for “probability”). Jargon: “maximum likelihood” means maximizing the likelihood term, without regard to any prior that there may be. At 8:22 there’s a slightly incorrect statement in the explanation, though not in the slide. The mean is not at .53 (although it is very close to that). What’s really at .53 is the mode, a.k.a. the peak, a.k.a. the most likely value. The Bayesian approach is to average the network’s predictions, at test time, where “average” means that we use network parameters according to the posterior distribution over parameter settings given the training data. Essentially, we’re averaging the predictions from many predictors: each possible parameter setting is a predictor, and the weight for that weighted average is the posterior probability of that parameter setting. “It’s helpful to know that whenever you see a squared error being minimized, you can make a probabilistic interpretation of what’s going on, and in that probabilistic interpretation, you’ll be maximizing the  log probability under a Gausian.” So the proper Bayesian approach, is to find the full posterior distribution over all possible weight vectors. If there’s more than a handful of weights, that’s hopelessly difficult when you have  a non-linear net. Bayesians have a lot of ways of  approximating this distribution, often using Monte Carlo methods.  But for the time being, let’s try and do something simpler.  Let’s just try to find the most probable weight vector.  So the single setting of the weights that’s most probable given the prior knowledge we have and given the data. So what we’re going to try and do is find  an optimal value of W by starting with some random weight vector, and then  adjusting it in the direction that improves the probability of that weight  factor given the data. It will only be a local optimum. The Bayesian interpretation of weight -log I D) = —logp(D I W) 1 (yc -tc)2 1)\nassuming that the model makes a Gaussian prediction — log p(W) 20w t assuming a for the weig"
  },
  {
    "objectID": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html#lecture-9e-the-bayesian-interpretation-of-weight-decay",
    "href": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html#lecture-9e-the-bayesian-interpretation-of-weight-decay",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "Lecture 9e: The Bayesian interpretation of weight decay",
    "text": "Lecture 9e: The Bayesian interpretation of weight decay\nIn this video, we use Bayesian thinking (which is widely accepted as very reasonable) to justify weight decay (which may sound like an arbitrary hack). Maximum A Posteriori (MAP) learning means looking for that setting of the network parameters that has greatest posterior probability given the data. As such it’s somewhat different from the simpler “Maximum Likelihood” learning, where we look for the setting of the parameters that has the greatest likelihood term: there, we don’t have a prior over parameter settings, so it’s not very Bayesian at all. Slide 1 introduces Maximum Likelihood learning. Try to understand well what that has to do with the Bayesian “likelihood term”, before going on to the next slide. The reason why we use Gaussians for our likelihood and prior is that that makes the math simple, and fortunately it’s not an insane choice to make. However, it is somewhat arbitrary. 10:15: Don’t worry about the absence of the factor 1/2 in the weight decay strength. It doesn’t change the story in any essential way."
  },
  {
    "objectID": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html#lecture-10a-why-it-helps-to-combine-models",
    "href": "posts/2017/dnn-07/2017-08-06-deep-neural-networks-notes-07.html#lecture-10a-why-it-helps-to-combine-models",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "Lecture 10a: Why it helps to combine models",
    "text": "Lecture 10a: Why it helps to combine models\nThe papers: Improving neural networks by preventing co-adaptation of feature detectors 2012 G. E. Hinton , N. Srivastava, A. Krizhevsky, I. Sutskever and R. R. Salakhutdinov Abstract When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This “overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorically large variety of internal contexts in which it must operate. Random “dropout” gives big improvements on many benchmark tasks and sets new records for speech and object recognition. Adaptive Mixtures of Local Experts 1998 Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan and Geoffrey E. Hinton Abstract “We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.” My Notes: The main prior idea is to utilize or learn to partition the training data so that one can train specialized models that are local experts on the problem space and then use some linear combination of the expert’s predictions to make predictions. The paper points out that prior work for used an error functions that does not encourage cooperation rather than specialization, which required using many experts in each prediction. Later work added penalty terms in the objective function to gate a single active exert in the prediction.(Jacobs, Jordan, and Barton, 1990). The paper offers an alternative error function that encourages specialization.\nThe difference difference between the error functions. The cooperative error function: does not encourage ion. They assume that the output 01 linear combination of the outputs of the local experts, with the gatinl the proportion of each local output in the linear combination. So the where öiC is the output vector of expert i on case c, p: is the propo expert i to the combined output vector, and is the desired output This error measure compares the desired output with a blend of t] experts, so, to minimize the error, each local expert must make its out\nThe competitive error function\nThe error defined in (3) is simply the negative log probability of generating the desired output vector under the mixture of gaussians model described at the end of the next section.\nTo see why this error function works better, it is helpful to compare the derivatives of the two error functions with respect to the output of an expert. From (2) we get\nIn equation 4 the term pci is used to weight the derivative for expert i. while from equation 3 we get ell’ In equation 5 the weighting term takes into account how well expert i does relative to other experts, which is a more useful measure of the relevance of expert i to training case c, especially early in the training. Suppose, for example, that the gating network initially gives equal weights to all experts and k ~d c − ~o c i k &gt; 1 for all the experts. Equation 4 will adapt the best-fitting expert the slowest, whereas equation 5 will adapt it the fastest. My wrap up Game theoretic framework have to formalize cooperative and competitive aspects of learning and how these might influence network architectures. c.f. Semantics, Representations and Grammars for Deep Learning (2015) David Balduzzi. There has been lots of progress in training single models for multiple tasks. c.f. One Model To Learn Them All (2017) Lukasz Kaiser et all. - covered in this video: One Neural network learns EVERYTHING ?! which uses mixture of expert layer which come from later work:\nOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017) Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean in which mixture of experts is used within large neural networks This lecture is about using a mixture of experts to reduce overfitting. The notion is to train lower capacity models specializing on subsets of the data and learn to predict which one would be the best predictor. Then use the best model for prediction. Alternatively we might average the results of the simpler models. The lecture is challenging as it skims the prior work failing to sufficiently motivate why the different error function arise (they depend on the way the learning scheme are set up) as the paper tries to bridge between competitive learning and a modular neural network.\nThere’s, again, a lot of math, although it’s less difficult than in videos 9d and 9e. Be sure to understand the formulas before moving on. We’re going to combine many models, by using the average of their predictions, at test time. 5:38: There’s a mistake in the explanation of why that term disappears. The mistake is that -2(t-ybar) is not a random variable, so it makes no sense to talk about its variance, mean, correlations, etc. The real reason why the term disappears is simply that the right half of the term, i.e. i, is zero, because ybar is the mean of the yi values. ## Lecture 10b: Mixtures of Experts This is a different way of combining multiple models. “Nearest neighbor” is a very simple regression method that’s not a neural network. 7:22: The formula is confusing. The idea is a weighted average of squared errors (weighted by those probabilities p_i). That can be written as an weighted expectation, with weights p_i, of (t-y_i)^2; or as a sum of p_i  (t-y_i)^2. The formula on the slide mixes those two notations. On the next slide it’s written correctly. 10:03: This formula is not trivial to find, but if you differentiate and simplify, you will find it. ## Lecture 10c: The idea of full Bayesian learning In this video you learn what exactly we want to do with that difficult-to-compute posterior distribution. We learn about doing which is so time-consuming that we can never do it for normal-size neural networks. This is a theory video. We average the predictions from many weight vectors on test data, with averaging weights coming from the posterior over weight vectors given the training data. That sounds simple and is indeed, in a sense, what happens. However, there’s more to be said about what this “averaging” entails. The Bayesian approach is all about probabilities, so the idea of producing a single number as output has no place in the Bayesian approach. Instead, the output is a distribution, indicating how likely the net considers every possible output value to be. In video 9e we introduced the idea that the scalar output from a network really is the mean of such a predictive distribution. We need that idea again here. That is what Geoffrey means at 6:37. “Adding noise to the output” is a way of saying that the output is simply the centre of a predictive distribution. What’s averaged is those distributions: the predictive distribution of the Bayesian approach is the weighted mean of all those Gaussian predictive distributions of the various weight vectors. By the way, the result of this averaging of many such Gaussian distributions is not a Gaussian distribution. However, if we’re only interested in the mean of the predictive distribution (which would not be very Bayesian in spirit), then we can simply average the outputs of the networks to get that mean. You can mathematically verify this for yourself. ## Lecture 10d: Making full Bayesian learning practical Maximum Likelihood is the least Bayesian. Maximum A Posteriori (i.e. using weight decay) is slightly more Bayesian. This video introduces a feasible method that’s even closer to the Bayesian ideal. However, it’s necessarily still an approximation. 4:22: “save the weights” means recording the current weight vector as a sampled weight vector. ## Lecture 10e: Dropout This is not Bayesian. This is a specific way of adding noise (that idea was introduced in general in video 9c). It’s a recent discovery and it works very, very well. Dropout can be viewed in different ways: One way to view this method is that we add noise. Another more complicated way, which is introduced first in the video, is about weight sharing and different models. That second way to view it serves as the explanation of why adding noise works so well. The first slide in other words: a mixture of models involves taking the arithmetic mean (a.k.a. “the mean”) of the outputs, while a product of models involves taking the geometric mean of the outputs, which is a different kind of mean. ## Lecture 11a: Hopfield Nets Neural networks and physical systems with emergent collective computational abilities J. J. HOPFIELD 1982 Now, we leave behind the feedforward deterministic networks that are trained with backpropagation gradients. We’re going to see quite a variety of different neural networks now. These networks do not have output units. These networks have units that can only be in states 0 and 1. These networks do not have units of which the state is simply a function of the state of other units. These networks are, instead, governed by an “energy function”. Best way to really understand Hopfield networks: Go through the example of the Hopfield network finding a low energy state, by yourself. Better yet, think of different weights, and do the exercise with those. Typically, we’ll use Hopfield networks where the units have state 0 or 1; not -1 or 1. ## Lecture 11b: Dealing with spurious minima The last in-video question is not easy. Try to understand how the perceptron learning procedure is used in a Hopfield net; it’s not very thoroughly explained. ## Lecture 11c: Hopfield nets with hidden units This video introduces some sophisticated concepts, and is not entirely easy. An “excitatory connection” is a connection of which the weight is positive. “inhibitory”, likewise, means a negative weight. We look for an energy minimum, “given the state of the visible units”. That means that we look for a low energy configuration, and we’ll consider only configurations in which the visible units are in the state that’s specified by the data. So we’re only going to consider flipping the states of the hidden units. Be sure to really understand the last two sentences that Geoffrey speaks in this video. ## Lecture 11d: Using stochastic units to improve search We’re still working with a mountain landscape analogy. This time, however, it’s not an analogy for parameter space, but for state space. A particle is, therefore, not a weight vector, but a configuration. What’s the same is that we’re, in a way, looking for low points in the landscape. We’re also using the physics analogy of systems that can be in different states, each with their own energy, and subject to a temperature. This analogy is introduced in slide 2. This is the analogy that originally inspired Hopfield networks. The idea is that at a high temperature, the system is more inclined to transition into configurations with high energy, even though it still prefers low energy. 3:25: “the amount of noise” means the extent to which the decisions are random. 4:20: If T really were 0, we’d have division by zero, which is not good. What we really mean here is “as T gets really, really small (but still positive)”. For mathematicians: it’s the limit as T goes to zero from above. Thermal equilibrium, and this whole random process of exploring states, is much like the exploration of weight vectors that we can use in Bayesian methods. It’s called a Markov Chain, in both cases. ## Lecture 11e: How a Boltzmann machine models data Now, we’re making a generative model of binary vectors. In contrast, mixtures of Gaussians are a generative model of real-valued vectors. 4:38: Try to understand how a mixture of Gaussians is also a causal generative model. 4:58: A Boltzmann Machine is an energy-based generative model. 5:50: Notice how this is the same as the earlier definition of energy. What’s new is that it’s mentioning visible and hidden units separately, instead of treating all units the same way. ## Lecture 12a: Boltzmann machine learning 6:50: Clarification: The energy is linear in the weights, but quadratic in the states. What matters for this argument is just that it’s linear in the weights. ## Lecture 12c: Restricted Boltmann Machines 3:02. Here, a “particle” is a configuration. These particles are moving around the configuration space, which, when considered with the energy function, is our mountain landscape. 4:58. It’s called a reconstruction because it’s based on the visible vector at t=0 (via the hidden vector at t=0). It will, typically, be quite similar to the visible vector at t=0. A “fantasy” configuration is one drawn from the model distribution by running a Markov Chain for a long time. The word “fantasy” is chosen as part of the analogy of a Boltzmann Machine vs. a brain that learned several memories. ## Lecture 12d: An example of RBM learning This is not an easy video. Prerequisite is a rather extensive understanding of what an RBM does. Be sure to understand video 12c quite well before proceeding with 12d. Prerequisite for this video is that you understand the “reconstruction” concept of the previous video. The first slide is about an RBM, but uses much of the same phrases that we previously used to talk about deterministic feedforward networks. The hidden units are described as feature detectors, or “features” for short. The weights are shown as arrows, even though a Boltzmann Machine has undirected connections. That’s because calculating the probability of the hidden units turning on, given the state of the visible units, is exactly like calculating the real-valued state of a logistic hidden unit, in a deterministic feedforward network. However, in a Boltzmann Machine, that number is then treated as a probability of turning on, and an actual state of 1 or 0 is chosen, randomly, based on that probability. We’ll make further use of that similarity next week. 2:30. That procedure for changing energies, that was just explained, is a repeat (in different words) of the Contrastive Divergence story of the previous video. If you didn’t fully realize that, then review. ## Lecture 13a: The ups and downs of back propagation 6:15: Support Vector Machines are a popular method for regression: for learning a mapping from input to output, as we have been doing with neural networks during the first half of the course. ## Lecture 13b: Belief Nets 7:43. For this slide, keep in mind Boltzmann Machines. There, too, we have hidden units and visible units, and it’s all probabilistic. BMs and SBNs have more in common than they have differences. 9:16. Nowadays, “Graphical Models” are sometimes considered as a special category of neural networks, but in the history that’s described here, they were considered to be very different types of systems. ## Lecture 13c: Learning sigmoid belief nets It would be good to read the first part of “The math of Sigmoid Belief Nets” before watching this video. 4:39. The second part of “The math of Sigmoid Belief Nets” mathematically derives this formula. Read it after finishing this video. 7:04. Actually, those numbers aren’t quite correct, although they’re not very far off. The take-home message, however, is correct: p(0,1) and p(1,0) are large, while the other two are small. 7:33. Here’s “explaining away” rephrased in a few more ways: If the house jumps, everybody starts wondering what might have caused that. Was there an earthquake? Did a truck hit the house? We’re not at all sure. When the wind then carries, through the open window, the voice of an upset truck driver bemoaning his bad luck, we know that a truck hit the house. That finding “explains away” the possibility that there might have been an earthquake: all of a sudden, we no longer suspect that there might have been an earthquake, even though we haven’t consulted the seismological office. In other words: as soon as we learn something about one possible cause (truck hits house), we can make an inference about other possible causes (earthquake). ## Lecture 13d: The wake-sleep algorithm 4:38. Another way to say this is that the multiple units behave independently: the probability of unit 2 turning on has nothing to do with whether or not unit 1 turned on. 5:30. The green weights are the weights of the Sigmoid Belief Net. An “unbiased sample” from some distribution is a sample that’s really drawn from that distribution. A “biased sample” is a sample that’s not quite from the intended distribution. We don’t really do maximum likelihood learning. We just use the maximum likelihood learning rule, while substituting “a sample from the posterior” by “a sample from the approximate posterior”. The only “maximum likelihood” part of it is that the formula for going from that sample to delta w is the same. ## Lecture 15a: From PCA to autoencoders Remember how, in assignment 4, we’re use unsupervised learning to obtain a different representation of each data case? PCA is another example of that, but for PCA, there’s even greater emphasis on obtaining that different representation. Chapter 15 is about unsupervised learning using deterministic feedforward networks. By contrast, the first part of the course was about supervised learning using deterministic feedforward networks, and the second part was about unsupervised learning using very different types of networks. 0:26. A linear manifold is a hyperplane. 1:25. A curved manifold is no longer a hyperplane. One might say it’s a bent hyperplane, but really, “hyperplane” means that it’s not bent. 1:37. “N-dimensional data” means that the data has N components and is therefore handled in a neural network by N input units. 1:58. Here, that “lower-dimensional subspace” is yet another synonym for “linear manifold” and “hyperplane”. 2:46 and 3:53. Geoffrey means the squared reconstruction error. 4:43. Here, for the first time, we have a deterministic feedforward network with lots of output units that are not a softmax group. An “autoencoder” is a neural network that learns to encode data in such a way that the original can be approximately reconstructed. ## Lecture 15b: Deep autoencoders 2:51. “Gentle backprop” means training with a small learning rate for not too long, i.e. not changing the weights a lot. ## Lecture 15c: Deep autoencoders for document retrieval “Latent semantic analysis” and “Deep Learning” sound pretty good as phrases… there’s definitely a marketing component in choosing such names :) 1:14. The application for the method in this video is this: “given one document (called the query document), find other documents similar to it in this giant col## Lection of documents.” 2:04. Some of the text on this slide is still hidden, hence for example the count of 1 for “reduce”. 3:09. This slide is a bit of a technicality, not very central to the story. If you feel confused, postpone focusing on this one until you’ve understood the others well. 6:49. Remember t-SNE? ## Lecture 15d: Semantic Hashing We’re continuing our attempts to find documents (or images), in some huge given pile, that are similar to a single given document (or image). Last time, we focused on making the search produce truly similar documents. This time, we focus on simply making the search fast (while still good). This video is one of the few times when machine learning goes hand in hand very well with intrinsically discrete computations (the use of bits, in this case). We’ll still use a deep autoencoder. This video is an example of using noise as a regularizer (see video 9c). Crucial in this story is the notion that units of the middle layer, the “bottleneck”, are trying to convey as much information as possible in their states to base the reconstruction on. Clearly, the more information their states contain, the better the reconstruction can potentially be. ## Lecture 15e: Learning binary codes for image retrieval It is essential that you understand video 15d before you try 15e. 7:13. Don’t worry if you don’t understand that last comment. ## Lecture 15f: Shallow autoencoders for pre-training This video is quite separate from the others of chapter 15.\nCNN Architecture & hyper parameters\nConvolutional Neural Network example INPUT [F,F,3]\nCONV [F,F,K] - basis sensor RELU [F,F,K ] - elementwise activation POOL [F/2,F/2,S] - down sampling\nFC - convers volume to class probability Hyper parameters: K – depth is the number of filters/kernels to use say 12 F - the RECEPTIVE FIELD or spatial extent of the filters – pixels width and height a neuron sees say 32x32 S – the STRIDE = step size for the offset used for sliding the filters so that there is an overlap neurons – say 1 P the amount of PADDING= padding round input with zeros, used because output and input might otherwise have different sizes\nAs of 2015 per STRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET the recommendation is to Removing\nPooling Removing normalization also recommended\nINPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]M -&gt; [FC -&gt; RELU]K -&gt; FC\nSeems FC and CONV are functionally equivalent and can be interchanged. Some other techniques/layers types: 1x1 convolution Dilated convolutions (acting on spaced out pixels) Replacing Max Pooling with ROI region of interrest pooling Loss layer – represent the overall error Dropout layer - Regularization by droping a unit with probabpility p DropConnect - Regularization by dropping connections instead of units\nStochastic pooling\nWeight decay = 0.001 Image whitening and contrast normalization in preprocessing"
  },
  {
    "objectID": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html",
    "href": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html",
    "title": "Notes for Lesson 5 of Deep Neural Networks",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html#lecture-5a-why-object-recognition-is-difficult",
    "href": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html#lecture-5a-why-object-recognition-is-difficult",
    "title": "Notes for Lesson 5 of Deep Neural Networks",
    "section": "Lecture 5a: Why object recognition is difficult",
    "text": "Lecture 5a: Why object recognition is difficult\n\n  \n\n \n\nWe’re switching to a different application of neural networks: computer vision, i.e. having a computer really understand what an image is showing.\nThis video explains why it is difficult for a computer to go from an image (i.e. the color and intensity for each pixel in the image) to an understanding of what it’s an image of.\nSome of this discussion is about images of 2-dimensional objects (writing on paper), but most of it is about photographs of 3-D real-world scenes.\nMake sure that you understand the last slide:\nIt explains how switching age and weight is like an object moving over to a different part of the image (to different pixels).\nThese two might sound like very different situations, but the analogy is in fact quite good: they’re not really very different.\nUnderstanding this is prerequisite for especially the next video."
  },
  {
    "objectID": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html#lecture-5b-achieving-viewpoint-invariance",
    "href": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html#lecture-5b-achieving-viewpoint-invariance",
    "title": "Notes for Lesson 5 of Deep Neural Networks",
    "section": "Lecture 5b: Achieving viewpoint invariance",
    "text": "Lecture 5b: Achieving viewpoint invariance\n“invariant” means, literally, that it doesn’t vary: it doesn’t change as a result of a change of viewpoint.\nThis means that if the neuron for the feature detector is fairly active (say it’s a logistic neuron and it has a value close to 1) for one input image, then if we give the neural network a image of that same scene from a somewhat different viewpoint, that same neuron will still be fairly active. Its activity is invariant under viewpoint changes.\n“invariant” is a matter of degrees: there’s very little that’s completely invariant, or that has no invariance at all, but some things are more invariant than others.\nThe invariant features are things like “there’s a red circle somewhere in the image”, and the neuron for that feature detector should somehow learn to turn on when there is indeed a red circle in the input, and turn off if there isn’t.\nTry to come up with examples of features that are largely invariant under viewpoint changes, and examples of features that don’t have that property."
  },
  {
    "objectID": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html#lecture-5c-convolutional-nets-for-digit-recognition",
    "href": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html#lecture-5c-convolutional-nets-for-digit-recognition",
    "title": "Notes for Lesson 5 of Deep Neural Networks",
    "section": "Lecture 5c: Convolutional nets for digit recognition",
    "text": "Lecture 5c: Convolutional nets for digit recognition\nLike many of the stories which we tell with the application of recognizing handwritten digits,\nthis one, too, is applicable to a great variety of vision tasks. It’s just that handwritten digit recognition is a standard example for neural networks - it used to be .\nLENET 1998 Architecture\nRecognizing the digit 3 C3•. f. maps 16$10x10 S4: f. maps 16$5x5 INPUT Cl feature maps 6$28x28 S2•. f. maps 6$14x14 1 C5•. layer F6: layer 120 84 Full con ec\nConvolutional nets are still very much used.\nThe replicated feature approach\nBackpropagation with weight constraints\nUse many different copies of the same feature detector with different positions.\n\nCould also replicate across scale and orientation (tricky and expensive)\nReplication greatly reduces the number of free parameters to be learned.\n\nUse several different feature types, each with its own map of replicated detectors.\n\nAllows each patch of image to be represented in several ways.\n\nThe red connections all have the same weight.\nIt’s easy to modify the backpropagation algorithm to incorporate linear constraints between the weights.\nWe compute the gradients as usual, and then modify the gradients so that they satisfy the constraints.\n\nSo if the weights started off satisfying the constraints, they will continue to satisfy them.\n\nTo constrain: WI = M22 we need: AWI = AW2 and compute : öW2 for WI and w2 use OWI OW2\nPriors and Prejudice\nThe brute force approach\nWe can put our prior knowledge about the task into the network by designing appropriate:\n\nConnectivity.\nWeight constraints.\nNeuron activation functions\n\nThis is less intrusive than hand-designing the features. But it still prejudices the network towards the particular way of solving the problem that we had in mind. Alternatively, we can use our prior knowledge to create a whole lot more training data.\n\nThis may require a lot of work (Hofman&Tresp, 1993)\nIt may make learning take much longer. It allows optimization to discover clever ways of using the multi-layer network that we did not think of.\nAnd we may never fully understand how it does it.\nLeNet uses knowledge about the invariances to design:\n\nthe local connectivity the weight-sharing\nthe pooling.\n\n\nThis achieves about 80 errors.\n\nThis can be reduced to about 40 errors by using many different transformations of the input and other tricks (Ranzato 2008)\nCiresan et. al. (2010) inject knowledge of invariances by creating a huge amount of carefully designed extra training data:\nFor each training image, they produce many new training examples by applying many different transformations.\nThey can then train a large, deep, dumb net on a GPI-J without much overfitting. They achieve about 35 errors.\n\nThe slide “Backpropagation with weight constraints” can be confusing.\nBackpropagation uses the chain rule to calculate error gradients for updating the weights. However it cannot enforce weights constraints we need here. These are enforced by the optimizer: the system that, updates the weights & biases of the network to reduce the error, and that uses the gradient (obtained by backprop) to figure out in which direction to change the weights.\nThe gradient for two weights will typically not be the same, even if they’re two weights that we’d like to keep equal.\nThe optimizer can keep the “tied” weights the same in at least two ways.\nTo use the sum of the gradients of the various “instances” of the tied weights as if it were the gradient for each of the instances. That’s what the video describes.\nAnother way is to use the mean instead of the sum.\nThe main point of this is that it’s not the gradients that change if we have convolution; what changes is what we do with the gradients.\nA more accurate interpretation is that there really aren’t two weights that we’re trying to keep equal, but rather there’s only one parameter that shows up in two (or more) places in the network.\nThat’s the more mathematical interpretation. It favors using the sum of gradients instead of the mean (you can try to figure out why, if you’re feeling mathematical). This interpretation is also closer to what typically happens in the computer program that runs the convolutional neural net.\nHow to detect a significant drop in the error rate\nBig difference vs. Small difference\nIs 30 errors in 10,000 test cases significantly better than 40 errors?\nIt all depends on the particular errors!\nThe McNemar test uses the particular errors and can be much more powerful than a test that just uses the number of errors.\nmodel 2 wrong model 2 right model 1 wrong 29 11 model 1 right 9959 model 2 wrong model 2 right model 1 wrong 15 25 model 1 right 15 9945"
  },
  {
    "objectID": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html#lecture-5d-convolutional-nets-for-object-recognition",
    "href": "posts/2017/dnn-05/2017-08-06-deep-neural-networks-notes-05.html#lecture-5d-convolutional-nets-for-object-recognition",
    "title": "Notes for Lesson 5 of Deep Neural Networks",
    "section": "Lecture 5d: Convolutional nets for object recognition",
    "text": "Lecture 5d: Convolutional nets for object recognition\nThis video is more a collection of interesting success stories than a thorough introduction to new concepts.\nAlex Krizhevsky (NIPS 2012)\nImageNet Classification with Deep CNN Architecture 128 Max pooling of 4 Max pooling Max pooling Figure 2: An illustration Of the architecture Of Our CNN, explicitly showing the delineation Of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, and the number of neurons in the network ’s remaining layers is given by 253,440—186,624—64,896—64,896—43,264—\n\ntrained on 2 NVIDA GTX 580 = 1000 cores for 2 weeks 7 hidden layers 5 layers are convolutional\n2 are fully connected,\nUsed RELU which train faster than logistic Data augmentation rnd 224x224 patches from 256x256\nl/r-reflection, alter RGB channels using PCA Dropout (omit half the hidden units for each example) ILSVRC-2012 image classification - find class in top 5 localization - find location Vlad Mnih (ICLM 2012) used non conv net to learn roads from plane photos (utilized map data).\n\nTerminology: “backpropagation” is often used as the name for the combination of two systems:\n\nSystem 1: the error backpropagation system that computes gradients.\nSystem 2: the gradient descent system that uses those gradients to gradually improve the weights and biases of a neural network.\n\nMost researchers, including Hinton, usually mean this combination, when they say “backpropagation”."
  },
  {
    "objectID": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html",
    "href": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html",
    "title": "Notes for Lesson 3 of Deep Neural Networks",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3a-learning-the-weights-of-a-linear-neuron",
    "href": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3a-learning-the-weights-of-a-linear-neuron",
    "title": "Notes for Lesson 3 of Deep Neural Networks",
    "section": "Lecture 3a: Learning the weights of a linear neuron",
    "text": "Lecture 3a: Learning the weights of a linear neuron\n\nWhy is a new algorithm needed?\n\nThe Perceptron learning procedure cannot be generalized to more layers because for those the mean of two good solutions may not be a good set of weights.\nBefore we showed that we were approximating better sets of weights in this algorithm we want to improve the output as a response of the input. Motivation: We want to use this to learn prices. Our procedure is to reduce overall error, unlike with a perceptron we cannot guarantee we will get better individual estimates.\n\n\nA toy example\n\nEach day you get lunch at the cafeteria.\n\nYour diet consists of fish, chips, and ketchup.\nYou get several portions of each.\n\nThe cashier only tells you the total price of the meal\n\nAfter several days, you should be able to figure out the price of each portion.\n\nThe iterative approach: Start with random guesses for the prices and then adjust them to get a better fit to the observed prices of whole meals.\n\n\n\nSolving the equations iteratively\n\nEach meal price gives a linear constraint on the prices of the portions:\n\n\\[\n\\text{price} = X_\\text{fish} W_\\text{fish} + X_\\text{chips} W_\\text{chips} + X_\\text{ketchup}W_\\text{ketchup}      \n\\] - The prices of the portions are like the weights in of a linear neuron.\n\\[\nW = (w_\\text{fish} , W_\\text{ chips} , W_\\text{ketchup} )\n\\]\n\n\nThe true weights used by the cashier\n\nWe will start with guesses for the weights and then adjust the guesses slightly to give a better fit to the prices given by the cashier.\n\n\n\nA model of the cashier with arbitrary initial weights\n\n\n\na toy problem\n\n\n\nResidual error = 350\nThe “delta-rule” for learning is: \\[\n\\Delta w_i = \\epsilon x_i (t - y)\n\\]\nWith a learning rate \\(\\epsilon\\) of 1/35, the weight changes are:+20, +50, +30\nThis gives new weights of: 70, 100, 80.\nThe weight for chips got worse, but over all the weights are better\n\n\n\n\nafter the first update\n\n\nby reducing errors, individual weight estimate may be getting worse\nCalculating the change in the weights:\ncalculate our output using forward prop\n\\[\ny = \\sum_{n \\in train} w_i x_i= \\vec{w}^T\\vec{x}\n\\]\nDefine the error as the squared residuals summed over all training cases:\n\\[\nE = \\frac{1}{2}\\sum_{n \\in train} (t_n−y_n)^2\n\\]\nuse the chain rule to get error derivatives for weights\n\\[\n\\frac{d E}{\\partial w_i}=\\frac{1}{2}\\sum_{n \\in train}\\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n}=\\frac{1}{2}\\sum_{n \\in train}(t^n−y^n)\n\\]\nthe batch delta rule changes the weight in proportion to their error derivative summed on all training cases times the learning rate\n\\[\n\\Delta w_i=−\\frac{d E}{\\partial w_i}\\epsilon  + \\sum_{n \\in train}(t^n−y^n)\\epsilon\n\\]\n\nresidual error\n\nit’s the amount by which we got the answer wrong.\n\n\nA very central concept is introduced without being made very explicit: we use derivatives for learning, i.e. for making the weights better. Try to understand why those concepts are indeed very related.\n\non-line learning\n\nmeans that we change the weights after every training example that we see, and we typically cycle through the collection of available training examples."
  },
  {
    "objectID": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3b-the-error-surface-for-a-linear-neuron",
    "href": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3b-the-error-surface-for-a-linear-neuron",
    "title": "Notes for Lesson 3 of Deep Neural Networks",
    "section": "Lecture 3b: The error surface for a linear neuron",
    "text": "Lecture 3b: The error surface for a linear neuron\n\n\n\n\nerror surface of a linear neuron\n\n\n\nThe error surface lies in a space with a horizontal axis for each weight and one vertical axis for the error.\n\nFor a linear neuron with a squared error, it is a quadratic bowl.\nVertical cross-sections are parabolas.\nHorizontal cross-sections are ellipses.\n\nFor multi-layer, non-linear nets the error surface is much more complicated.\n\n\nOnline versus batch learning\n\n\n\nWhy learning can be slow\n\n\nIf the ellipse is very elongated, the direction of steepest descent is almost perpendicular to the direction towards the minimum!\nThe red gradient vector has a large component along the short axis of the ellipse and a small component along the long axis of the ellipse.\nThis is just the opposite of what we want."
  },
  {
    "objectID": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3c-learning-the-weights-of-a-logistic-output-neuron",
    "href": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3c-learning-the-weights-of-a-logistic-output-neuron",
    "title": "Notes for Lesson 3 of Deep Neural Networks",
    "section": "Lecture 3c: Learning the weights of a logistic output neuron",
    "text": "Lecture 3c: Learning the weights of a logistic output neuron\n\nLogistic neurons AKA linear filters - useful to understand the algorithm but in reality we need to use non linear activation function.\n\nLogistic neurons\nThese give a real-valued output that is a smooth and bounded function of their total input. They have nice derivatives which make learning easy.\n\\[\nz = b + \\sum _i x_i w_i\n\\]\n\\[\ny=\\frac{1}{1+e^{-z}}\n\\]\n\n\n\nlogistic activation function\n\n\n\n\nThe derivatives of a logistic neuron\nThe derivatives of the logit, z, with respect to the inputs and the weights are very simple:\n\\[\nz = b + \\sum _i x_i w_i\n\\]\n\\[\n\\frac{\\partial z}{\\partial w_i} = x_i \\;\\;\\;\\;\\; \\frac{\\partial z}{\\partial x_i} = w_i\n\\]\nThe derivative of the output with respect to the logit is simple if you express it in terms of the output:\n\\[\ny=\\frac{1}{1+e^{-z}}\n\\]\n\\[\n\\frac{d y}{d z} = y( 1-y)\n\\]\nsince\n\\[\ny=\\frac{1}{1+e^{-z}}=(1+e^{-z})^-1\n\\]\n\\[ \\frac{d y}{d z} = \\frac{-1(-e^{-z})}{(1+e^{-z})^2} =\\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}  = y( 1-y) \\] Using the chain rule to get the derivatives needed for learning the weights of a logistic unit To learn the weights we need the derivative of the output with respect to each weight:\n\\[\n\\frac{d y}{\\partial w_i}  =\\frac{\\partial z}{\\partial w_i} \\frac{dy}{dz}  = x_iy( 1-y)\n\\]\n\\[\n\\frac{d E}{\\partial w_i}  = \\frac{\\partial y^n}{\\partial w_i} \\frac{dE}{dy^n} = - \\sum \\green{x_i^n}\\red{ y^n( 1-y^n)}\\green{(t^n-y^n)}\n\\]\nwhere the green part corresponds to the delta rule and the extra term in red is simply the slope of the logistic.\nThe error function is still:\n\\[\nE =\\frac{1}{2}(y−t)^2\n\\]\nNotice how after Hinton explained what the derivative is for a logistic unit, he considers the job to be done. That’s because the learning rule is always simply some learning rate multiplied by the derivative."
  },
  {
    "objectID": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3d-the-back-propagation-algorithm",
    "href": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3d-the-back-propagation-algorithm",
    "title": "Notes for Lesson 3 of Deep Neural Networks",
    "section": "Lecture 3d: The back-propagation algorithm",
    "text": "Lecture 3d: The back-propagation algorithm\n[ \ntitle=\"Lecture 3d : The backpropagation algorithm\" \nwidth=\"1024\" \nheight=\"720\" ]{.quarto-shortcode__ data-is-shortcode=\"1\" data-raw=\"\"}\nHere, we start using hidden layers. To train them, we need the back propagation algorithm. Hidden layers, and this algorithm, are very important. They are the layers between the input layer and the output.\nThe story of training by perturbations also makes an appearance in the course by David MCcay, serving primarily as motivation for the back propagation algorithm.\nThis computation, just like the forward propagation, can be vectorized across multiple units in every layer, and multiple training cases.\n\nLearning with hidden units (again)\n\nNetworks without hidden units are very limited in the input-output mappings they can model.\n\nAdding a layer of hand-coded features (as in a Perceptrons) makes them much more powerful but the hard bit is designing the features.\n\nWe would like to find good features without requiring insights into the task or repeated trial and error where we guess some features and see how well they work.\n\nWe need to automate the loop of designing features for a particular task and seeing how well they work.\n\n\n\nLearning by perturbing weights\n\nRandomly perturb one weight and see if it improves performance. If so, save the change.\n\nThis is a form of reinforcement learning.\nVery inefficient. We need to do multiple forward passes on a representative set of training cases just to change one weight. Back propagation is much better.\nTowards the end of learning, large weight perturbations will nearly always make things worse, because the weights need to have the right relative values. (so we should adapt a decreasing learning rate).\n\nWe could randomly perturb all the weights in parallel and correlate the performance gain with the weight changes.\n\nNot any better because we need lots of trials on each training case to “see” the effect of changing one weight through the noise created by all the changes to other weights.\n\nA better idea: Randomly perturb the activities of the hidden units.\n\nOnce we know how we want a hidden activity to change on a given training case, we can compute how to change the weights.\nThere are fewer activities than weights, but backpropagation still wins by a factor of the number of neurons.\n\n\n\n\nThe idea behind backpropagation\n\nWe don’t know what the hidden units ought to do, but we can compute how fast the error changes as we change a hidden activity.\n\nInstead of using desired activities to train the hidden units, use error derivatives w.r.t. hidden activities.\n\nEach hidden activity can affect many output units and can therefore have many separate effects on the error. These effects must be combined.\n\n\nWe can compute error derivatives for all the hidden units efficiently at the same time.\n\nOnce we have the error derivatives for the hidden activities, its easy to get the error derivatives for the weights going into a hidden unit."
  },
  {
    "objectID": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#sketch-of-back-propagation-on-a-single-case",
    "href": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#sketch-of-back-propagation-on-a-single-case",
    "title": "Notes for Lesson 3 of Deep Neural Networks",
    "section": "Sketch of back propagation on a single case",
    "text": "Sketch of back propagation on a single case\n\nFirst convert the discrepancy between each output and its target value into an error derivative.\nThen compute error derivatives in each hidden layer from error derivatives in the layer above.\nThen use error derivatives w.r.t. activities to get error derivatives w.r.t. the incoming weights.\n\n\\[E =\\frac{1}{2}(t_i-y_i)^2\\]\n\\[\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)\\]\n\n\n\nback proogations of erros\n\n\n\n\n\nbackproogating"
  },
  {
    "objectID": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3e-using-the-derivatives-computed-by-backpropagation",
    "href": "posts/2017/dnn-03/2017-08-06-deep-neural-networks-notes-03.html#lecture-3e-using-the-derivatives-computed-by-backpropagation",
    "title": "Notes for Lesson 3 of Deep Neural Networks",
    "section": "Lecture 3e: Using the derivatives computed by backpropagation",
    "text": "Lecture 3e: Using the derivatives computed by backpropagation\n\n\nThe backpropagation algorithm is an efficient way of computing the error derivative \\(\\frac{dE}{dw}\\) for every weight on a single training case. There are many decisions needed on how to derive new weights using there derivatives.\n\nOptimization issues: How do we use the error derivatives on individual cases to discover a good set of weights? (lecture 6)\nGeneralization issues: How do we ensure that the learned weights work well for cases we did not see during training? (lecture 7)\n\nWe now have a very brief overview of these two sets of issues.\nHow often to update weights ?\n\nOnline - after every case.\nMini Batch - after a small sample of training cases.\nFull Batch - after a full sweep of training data.\n\nHow much to update? (c.f. lecture 6)\n\nfixed learning rate\nadaptable global learning rate\nadaptable learning rate per weight\ndon’t use steepest descent (velocity/momentum/second order methods)\n\n\n\nOverfitting: The downside of using powerful models\n\nRegularization - How to ensure that learned weights work well for cases we did not see during training?\n\nThe training data contains information about the regularities in the mapping from input to output. But it also contains two types of noise.\n\nThe target values may be unreliable (usually only a minor worry).\nThere is sampling error. There will be accidental regularities just because of the particular training cases that were chosen.\n\nWhen we fit the model, it cannot tell which regularities are real and which are caused by sampling error.\n\nSo it fits both kinds of regularity.\nIf the model is very flexible it can model the sampling error really well. This is a disaster.\n\n\n\n\nA simple example of overfitting\n\n\nWhich output value should you predict for this test input?\nWhich model do you trust?\n\nThe complicated model fits the data better.\nBut it is not economical.\n\nA model is convincing when it fits a lot of data surprisingly well.\nIt is not surprising that a complicated model can fit a small amount of data well.\nModels fit both signal and noise.\n\n\n\nHow to reduce overfitting\n\nA large number of different methods have been developed.\n\nWeight-decay\nWeight-sharing - reduce model flexibility by adding constraints on weights\nEarly stopping - stop training when by monitoring the Test error.\nModel averaging - use an ensemble of models\nBayesian fitting of neural nets - like averaging but weighed\nDropout - (hide data from half the net)\nGenerative pre-training - (more data)\n\nMany of these methods will be described in lecture 7.\n\n\n\n\na toy problem\nafter the first update\nerror surface of a linear neuron\nlogistic activation function\nback proogations of erros\nbackproogating"
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nThese is the first installment of notes to the course “Deep Neural Networks” by Geffory Hinton I took on Coursera\nThis was one of the first course online on the subject.\nHinton was one of the leading researchers on deep learning, his students are some of the most important reaserchers today. He introduced some algorithms and methods that were not published.\nThis course is now outdated - it does not cover transformers and probably all the results have been beaten as this is a fast moving field.\nStill this is an interesting, if mathematicaly sophisticated introduction to deep learning.\n{{&lt; video https://www.youtube.com/watch?v=2fRnHVVLf1Y     class=column-margin     title=\"Lecture 1\"      width=\"1024\"      height=\"720\" &gt;}}"
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1a-why-do-we-need-machine-learning",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1a-why-do-we-need-machine-learning",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Lecture 1a: Why do we need machine learning?",
    "text": "Lecture 1a: Why do we need machine learning?\n\n\nWhat is Machine Learning?\n\nIt is very hard to write programs that solve problems like ==recognizing a 3d object== from a novel viewpoint in new lighting conditions in a cluttered scene.\n\nWe don’t know what program to write because we don’t know how its done in our brain.\nEven if we had a good idea about how to do it, the program might be horrendously complicated.\n\nIt is hard to write a program to compute the probability that a credit card transaction is fraudulent.\n\nThere may not be any rules that are both simple and reliable. We need to combine a very large number of weak rules.\nFraud is a moving target. The program needs to keep changing.\n\n\n\n\nThe Machine Learning Approach\n\nInstead of writing a program by hand for each specific task, we collect lots of examples that specify the correct output for a given input.\nA machine learning algorithm then takes these examples and produces a program that does the job.\n\nThe program produced by the learning algorithm may look very different from a typical hand-written program. It may contain millions of numbers.\nIf we do it right, the program works for new cases as well as the ones we trained it on.\nIf the data changes the program can change too by training on the new data.\n\nMassive amounts of computation are now cheaper than paying someone to write a task-specific program.\n\n\n\nSome examples of tasks best solved by learning\n\nRecognizing patterns:\n\nObjects in real scenes\nFacial identities or facial expressions\nSpoken words\n\nRecognizing anomalies:\n\nUnusual sequences of credit card transactions\nUnusual patterns of sensor readings in a nuclear power plant\n\nPrediction:\n\nFuture stock prices or currency exchange rates\nWhich movies will a person like?\n\n\n\n\nA standard example of machine learning\n\nA lot of genetics is done on fruit flies.\n\nThey are convenient because they breed fast.\nWe already know a lot about them.\n\nThe MNIST database of hand-written digits is the the machine learning equivalent of fruit flies.\n\nThey are publicly available and we can learn them quite fast in a moderate-sized neural net.\nWe know a huge amount about how well various machine learning methods do on MNIST.\n\nWe will use MNIST as our standard task.\n\n\n\nBeyond MNIST: The ImageNet task\n\n1000 different object classes in 1.3 million high-resolution training images from the web.\n\nBest system in 2010 competition got 47% error for its first choice and 25% error for its top 5 choices.\n\nJitendra Malik, an eminent neural net sceptic, said that this competition is a good test of whether deep neural networks work well for object recognition.\nA very deep neural net Krizhevsky, Sutskever, and Hinton (2012) gets less that 40% error for its first choice and less than 20% for its top 5 choices.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” Advances in Neural Information Processing Systems 25.\n\n\nThe Speech Recognition Task\n\nA speech recognition system has several stages:\n\nPre-processing: Convert the sound wave into a vector of acoustic coefficients. Extract a new vector about every 10 mille seconds.\nThe acoustic model: Use a few adjacent vectors of acoustic coefficients to place bets on which part of which phoneme is being spoken.\nDecoding: Find the sequence of bets that does the best job of fitting the acoustic data and also fitting a model of the kinds of things people say.\n\nDeep neural networks pioneered by George Dahl and Abdel-rahman Mohamed are now replacing the previous machine learning method for the acoustic model.\n\n\n\nPhone recognition on the TIMIT benchmark\n He discusses work from from Mohamed, Dahl, and Hinton (2012) - After standard post-processing using a bi-phone model, a deep net with 8 layers gets 20.7% error rate. - The best previous speaker independent result on TIMIT was 24.4% and this required averaging several models. - Li Deng (at MSR) realized that this result could change the way speech recognition was done.\nMohamed, Abdel-rahman, George E. Dahl, and Geoffrey Hinton. 2012. “Acoustic Modeling Using Deep Belief Networks.” IEEE Transactions on Audio, Speech, and Language Processing 20 (1): 14–22. https://doi.org/10.1109/TASL.2011.2109382."
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1b-what-are-neural-networks",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1b-what-are-neural-networks",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Lecture 1b: What are neural networks?",
    "text": "Lecture 1b: What are neural networks?\n\nSome tasks that are easy or humans, like vision, are hard for software, and vice versa (chess).\n\nReasons to study neural computation\n\nTo understand how the brain actually works.\n\nIts very big and very complicated and made of stuff that dies when you poke it around. So we need to use computer simulations.\n\nTo understand a style of parallel computation inspired by neurons and their adaptive connections.\n\nVery different style from sequential computation.\nshould be good for things that brains are good at (e.g. vision)\nShould be bad for things that brains are bad at (e.g. 23 x 71)\n\nTo solve practical problems by using novel learning algorithms inspired by the brain (this course)\n\nLearning algorithms can be very useful even if they are not how the brain actually works.\n\n\n\n\nA typical cortical neuron\n\nGross physical structure:\n\nThere is one axon that branches\nThere is a dendritic tree that collects input from other neurons.\n\nAxons typically contact dendritic trees at synapses\n\nA spike of activity in the axon causes charge to be injected into the post-synaptic neuron.\n\nSpike generation:\n\nThere is an axon hillock that generates outgoing spikes whenever enough charge has flowed in at synapses to depolarize the cell membrane.\n\n\n\n\nSynapses\n\nWhen a spike of activity travels along an axon and arrives at a synapse it causes vesicles of transmitter chemical to be released.\n\nThere are several kinds of transmitter.\n\nThe transmitter molecules diffuse across the synaptic cleft and bind to receptor molecules in the membrane of the post-synaptic neuron thus changing their shape.\n\nThis opens up holes that allow specific ions in or out.\n\n\n\n\nHow synapses adapt\n\nThe effectiveness of the synapse can be changed:\n\nvary the number of vesicles of transmitter.\nvary the number of receptor molecules.\n\nSynapses are slow, but they have advantages over RAM\n\nThey are very small and very low-power.\nThey adapt using locally available signals\n\nBut what rules do they use to decide how to change?\n\n\n\n\n\nHow the brain works on one slide!\n\nEach neuron receives inputs from other neurons\n\nA few neurons also connect to receptors.\nCortical neurons use spikes to communicate.\n\nThe effect of each input line on the neuron is controlled by a synaptic weight\n\nThe weights can be positive or negative.\n\nThe synaptic weights adapt so that the whole network learns to perform useful computations\n\nRecognizing objects, understanding language, making plans, controlling the body.\n\nYou have about neurons each with about weights.\n\nA huge number of weights can affect the computation in a very short time. Much better bandwidth than a workstation.\n\n\n\n\nModularity and the brain\n\nDifferent bits of the cortex do different things.\n\nLocal damage to the brain has specific effects.\nSpecific tasks increase the blood flow to specific regions.\n\nBut cortex looks pretty much the same all over.\n\nEarly brain damage makes functions relocate.\n\nCortex is made of general purpose stuff that has the ability to turn into special purpose hardware in response to experience.\n\nThis gives rapid parallel computation plus flexibility.\nConventional computers get flexibility by having stored sequential programs, but this requires very fast central processors to perform long sequential computations."
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1c-some-simple-models-of-neurons",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1c-some-simple-models-of-neurons",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Lecture 1c: Some simple models of neurons",
    "text": "Lecture 1c: Some simple models of neurons\n\n\nIdealized neurons\n\nTo model things we have to idealize them (e.g. atoms)\n\nIdealization removes complicated details that are not essential for understanding the main principles.\nIt allows us to apply mathematics and to make analogies to other, familiar systems.\nOnce we understand the basic principles, its easy to add complexity to make the model more faithful.\n\nIt is often worth understanding models that are known to be wrong (but we must not forget that they are wrong!)\n\nE.g. neurons that communicate real values rather than discrete spikes of activity.\n\n\n\n\nLinear neurons\n\nThese are simple but computationally limited\n\nIf we can make them learn we may get insight into more complicated neurons.\n\n\n\\[\ny=b+\\sum_i{ x_i \\times w_i}\n\\]\nwhere:\n\n\\(y\\) is the output\n\n\\(b\\) is the bias\n\\(i\\) is the index over input connectinos\n\n\\(x_i\\) is the ith input\n\\(w_i\\) is the weight on ith input\n\nBias is often conveniently chosen to be 0 which is odd considering that it is the constraint on the activation. This is handled formally by a technique called batch normalization\n\n\n\nlinear activation function\n\n\nThese are simple but computationally limited.\n\nIf we can make them learn we may get insight into more complicated neurons.\n\n\\[\ny=b+\\sum_i{ x_i \\times w_i}\n\\]"
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#binary-threshold-units",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#binary-threshold-units",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Binary threshold units",
    "text": "Binary threshold units\n Binary threshold units are due to Warren McCulloch and Walter Pitts from their McCulloch and Pitts (1943). They were in turn influenced by earlier work by John Von Neumann the father of modern computer and game theory.\n\n\n\n\n\n\n\n\nWarren Sturgis Mcculloch\nWalter Pitts\nJohnvon Neumann\n\n\n\n\n\n\n\n\n\n\n\nFirst compute a weighted sum of the inputs.\nThen send out a fixed size spike of activity if the weighted sum exceeds a threshold.\nMcCulloch and Pitts thought that each spike is like the truth value of a proposition and each neuron combines truth values to compute the truth value of another proposition!\n\nThere are two ways to write these mathematicaly:\n\\[\nz = \\sum_i{ x_i w_i}\\\\\n\\theta = -b \\\\\ny = \\left\\{\n   \\begin{array}{ll}\n       1 & \\text{if} \\space z \\ge \\theta \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n    \\right.\n\\]\nusing bias\n\\[\nz = b+ \\sum_i{ x_i w_i}\\\\\ny = \\left\\{\n   \\begin{array}{ll}\n       1 & \\text{if} \\space z \\ge 0 \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n    \\right.\n\\]"
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#relu---rectified-linear-neurons-aka-linear-threshold-neurons",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#relu---rectified-linear-neurons-aka-linear-threshold-neurons",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "RELU - REctified Linear Neurons AKA Linear Threshold neurons",
    "text": "RELU - REctified Linear Neurons AKA Linear Threshold neurons\n\n\n\nRELU activation function\n\n\n\nThey compute a linear weighted sum of their inputs.\nThe output is a non-linear function of the total input.\n\n\\[\nz = b + \\sum _i x_iw_i \\\\\n\\]\n\\[\ny = \\left\\{\n   \\begin{array}{ll}\n       z & \\text{if} \\space z \\gt 0 \\\\\n       0 & \\text{otherwise}\n   \\end{array}\n   \\right.\n\\]"
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#sigmoid-neurons",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#sigmoid-neurons",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Sigmoid neurons",
    "text": "Sigmoid neurons\n\n\n\nSigmoid activation function\n\n\n\nThese give a real-valued output that is a smooth and bounded function of their total input.\nTypically they use the logistic function\nHave nice derivatives which make learning easy.\n\n\\[\nz = b + \\sum _i x_iw_i \\\\\n\\space\\\\\ny = \\frac{1}{1+e^{-z}}\n\\]"
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#stochastic-binary-neurons",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#stochastic-binary-neurons",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Stochastic binary neurons",
    "text": "Stochastic binary neurons\n\n\n\nbinary activation function\n\n\nThese use the same equations as logistic units. - But they treat the output of the logistic as the probability of producing a spike in a short time window.\nWe can do a similar trick for rectified linear units:\n\nThe output is treated as the Poisson rate for spikes.\n\n\\[\nz = b + \\sum _i x_iw_i \\\\\n\\space\\\\\np(s=1) = \\frac{1}{1+e^{-z}}\n\\]"
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#choosing-an-activation-function",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#choosing-an-activation-function",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Choosing an activation function",
    "text": "Choosing an activation function\nFirst let us note that many other activation function exist, this table list the following:\n\n\n\nactivation functions\n\n\nAt this point in the course we do not go into how one should pick a preferred activation function for the given problem. Some ideas for this are mentioned during the course. If we look at this from an engineering perspective some units tend to work well with other units and there are some other constraints like the range of inputs.\n\nLinear units\nTheir main benefit is that they help us write down the mathematically familiar linear model which is great for getting a basic insight into the problem. We can analyze this model in term of linear and or abstract algebra using concepts like spaces, subspace, solutions, eigenvectors, eigenvalues and so on. Unfortunately linear units they are not expressive enough to perform as a basis of an efficient universal approximator. A linear model is equivalent to a large logistic regression as each variable will effect all other variables. So once we developed some intuition about our linear model we would want to switch to a non-liner units and make use of the full power of neural networks.\n\n\nBinary threshold units\nTheir main benefit seem to be for modeling logical gates or logical circuits. Cons: have only zero and infinite gradients so are unsuitable for use in networks that are trained using gradient descent. They are used however in Hopfield networks. We will also consider later using a fully baysian approch to neural networks where we don’t need stochastic gradient descent - instead using MCMC search. It would seem that is such a settings using binary threshold units would dramatically decrease the search space.\n\n\nRELU\nThis is the simplest non linear units - using it is essentially introducing constraints in the form of inequalities. It should only be used in a hidden layer. A classification will need to add a Softmax and a regression a linear function. RELUs can die - so a Leaky RELU can be a better choice. \n\n\nSigmoid\nThis is continuous and has a gradient between 0 and 1 - pros: sigmoid with weight initialized to zero behave like a linear system. As the weights increase towards they networks\n- cons: saturate and kill gradients also when output is not centered about 0 then gradients tend to go to far to 0 or 1. They converge slowly.\n\n\nTANH\npros: very high values are similar (~1) and very low values are also similar (~1) cons: sub optimal for a deep network, as gradient diminish in the deeper parts of the model. RMSProp will compensate for that, but still changing to RELU will improve convergence speed c.f. user8272359 (2017). It is better then sigmoid as it avoids the exploding gradient problem\n\nuser8272359. 2017. “Deep Neural Network Using Keras/Tensorflow Solves Spiral Dataset Classification. But Accuracy Is Stuck Around 50.” August 5, 2017. https://datascience.stackexchange.com/questions/22830/deep-neural-network-using-keras-tensorflow-solves-spiral-dataset-classification."
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1d-a-simple-example-of-learning",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1d-a-simple-example-of-learning",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Lecture 1d: A simple example of learning",
    "text": "Lecture 1d: A simple example of learning\n\nVisualization of neural networks is one of the few methods to get some insights into what is going on inside the black box.\n• Consider a neural network with two layers of neurons. – neurons in the top layer represent known shapes. – neurons in the bottom layer represent pixel intensities. • A pixel gets to vote if it has ink on it. – Each inked pixel can vote for several different shapes. • The shape that gets the most votes wins.\n\nHow to display the weights\nGive each output unit its own “map” of the input image and display the weight coming from each pixel in the location of that pixel in the map.\nUse a black or white blob with the area representing the magnitude of the weight and the color representing the sign.\n\n\nHow to learn the weights\nShow the network an image and increment the weights from active pixels to the correct class.\nThen decrement the weights from active pixels to whatever class the network guesses\n\n\nThe learned weights\nThe details of the learning algorithm will be explained in future lectures.\n\n\nWhy the simple learning algorithm is insufficient\n\nA two layer network with a single winner in the top layer is equivalent to having a rigid template for each shape.\nThe winner is the template that has the biggest overlap with the ink.\nThe ways in which hand-written digits vary are much too complicated to be captured by simple template matches of whole shapes.\nTo capture all the allowable variations of a digit we need to learn the features that it is composed of."
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1e-three-types-of-learning",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#lecture-1e-three-types-of-learning",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Lecture 1e: Three types of learning",
    "text": "Lecture 1e: Three types of learning\n\nThe three main types of learning machine learning:\n\nSupervised learning\n\nLearn to predict an output given an input vector\n\nReinforcement learning\n\nLearn to select an action to maximize payoff.\n\nUnsupervised learning\n\nDiscover a good internal representation of the input.\n\nSemi supervised learning\n\nSemi-supervised uses a small amount of supervised data and large amount of unsupervised elarning\n\nFew/one shot learning\n\nSupervised learning with inference from one or a few examples\n\nZero shot learning\n\nSupervised learning with inference for inputs not seen in training - usually based on learned structrure\n\nTransfer learning\n\nLearning something from one data set and use it on another"
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#two-types-of-supervised-learning",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#two-types-of-supervised-learning",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "Two types of supervised learning",
    "text": "Two types of supervised learning\n\nEach training case consists of an input vector x and a target output t.\nRegression: The target output is a real number or a whole vector of real numbers.\n\nThe price of a stock in 6 months time.\nThe temperature at noon tomorrow.\n\nClassification: The target output is a class label.\n\nThe simplest case is a choice between 1 and 0.\nWe can also have multiple alternative labels."
  },
  {
    "objectID": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#how-supervised-learning-typically-works",
    "href": "posts/2017/dnn-01/2017-08-06-deep-neural-networks-notes-01.html#how-supervised-learning-typically-works",
    "title": "Notes for Lesson 1 of Deep Neural Networks",
    "section": "How supervised learning typically works",
    "text": "How supervised learning typically works\n\nWe start by choosing a model-class:\n\nA model-class, f, is a way of using some numerical \\(y=f(x;W)\\) parameters, W, to map each input vector, x, into a predicted output y.\n\nLearning usually means adjusting the parameters to reduce the discrepancy between the target output, t, on each training case and the actual output, y, produced by the model.\n\nFor regression, \\(\\frac{1}{2}(y-t)^2\\)is often a sensible measure of the discrepancy.\nFor classification there are other measures that are generally more sensible (they also work better).\n\n\n\nReinforcement learning\n\nIn reinforcement learning, the output is an action or sequence of actions and the only supervisory signal is an occasional scalar reward.\n\nThe goal in selecting each action is to maximize the expected sum of the future rewards.\nWe usually use a discount factor for delayed rewards so that we don’t have to look too far into the future.\n\nReinforcement learning is difficult:\n\nThe rewards are typically delayed so its hard to know where we went wrong (or right).\nA scalar reward does not supply much information.\n\nThis course cannot cover everything and reinforcement learning is one of the important topics we will not cover.\n\n\n\nUnsupervised learning\n\nFor about 40 years, unsupervised learning was largely ignored by the machine learning community\n\nSome widely used definitions of machine learning actually excluded it.\nMany researchers thought that clustering was the only form of unsupervised learning.\n\nIt is hard to say what the aim of unsupervised learning is.\n\nOne major aim is to create an internal representation of the input that is useful for subsequent supervised or reinforcement learning.\nYou can compute the distance to a surface by using the disparity between two images. But you don’t want to learn to compute disparities by stubbing your toe thousands of times.\n\n\n\n\nOther goals for unsupervised learning\n\nIt provides a compact, low-dimensional representation of the input.\n\nHigh-dimensional inputs typically live on or near a lowdimensional manifold (or several such manifolds).\nPrincipal Component Analysis is a widely used linear method for finding a low-dimensional representation.\n\nIt provides an economical high-dimensional representation of the input in terms of learned features.\n\nBinary features are economical. – So are real-valued features that are nearly all zero.\n\nIt finds sensible clusters in the input.\n\nThis is an example of a very sparse code in which only one of the features is non-zero.\n\n\n\n\n\nlinear activation function\nRELU activation function\nSigmoid activation function\nbinary activation function\nactivation functions"
  },
  {
    "objectID": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html",
    "href": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html",
    "title": "A/B testing cost and risks?",
    "section": "",
    "text": "While it is not a forgone conclusion that CRO driven by A/B test will be a major disaster but it is likely that without some expert supervision it can end up a costing more and taking longer. The statistical concepts are not fairly basic but most of them are misunderstood."
  },
  {
    "objectID": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html#setup-costs",
    "href": "posts/2017/2017-07-30-experimental-design/2017-07-30-experimental-design.html#setup-costs",
    "title": "A/B testing cost and risks?",
    "section": "Setup costs",
    "text": "Setup costs\nYou generally need to pay a developer to code the alternatives, and to setup the code that does the the sampling and book keeping. An IT guy to deploy it and an analyst or data scientist to analyse it and then a marketing manager to decide to take some action (ok the intervention and decide to do more testing.) ## Exploration costs and diminishing return When we want to romanticize researching a new intervention we call it Exploration and then we use the less romantic term Exploitation to refer to making using our current best intervention to get outcomes from our web marketing efforts. A/B tests shunts a percentage of you traffic (often 50%) for the duration of the experiment to an intervention we call B we expect to beat out current best effort which we call A. But since pretty smart people chose A it can take many attempts to find a B that better. Also each time you find a better alternative it get harder and you end up getting smaller improvements this is due to what is called a smaller effect size for the intervention. Consider that you will eventually get to an optimum and need to look at a different strategy to make improvements. The good news is that each time your exploration works out and you get a better outcome the test will end up driving up your kpi and goals. But the bad news is that most of the time you wont be getting a win and conducting the test will cost you in lost action. Most A/B testing platforms can track your goals and will try to minimize the negative impact of the test using power of Bayesian statistics. You should make use of these if they are an option. But if you are getting started you might not have the benefit of integrating this type of stats into your experiment. Also even the amazingly capable people who build these tools can get it wrong and have had to rewriting their systems. The main reason that testing has diminishing returns is that you it is much easier to test for big changes early but small subtle changes which have small effects take longer to achieve statistical significance because we are trying to separate two very similar signals coming from A and B. Each win generally means that the next test will take longer be less likely to be a win and therefore cost more on average.\nAs the are usually diminishing returns from running A/B test. If your landing page has a low conversion rate, say 1.5%. You can do experiments and you might be able to reduce bounce rate by 40% percent which should increase your conversion rate to 1.2%. Next you might be able to increase time on page by 300% using better videos that might boost your conversion rate to 3.5, your might be able to pick a more effective cal to action and get around to 3.7% conversion rates. And you might also add an aggressive exit popup and get to 4.2%. That is a dream scenario that CTR consultants dream about. But if your are not working in a startup and you have a solid marketing team you will probably start with many of the choices near optimum and end up at 3% conversion rate. And each successful test will give you a .1% improvement. That might be ok but such a small improvement will take much longer to reach significance and therefore end up costing more. With all the test you ever do you might never reach 4% Over time you should expect that A/B test will be measuring ever smaller effects and will require more traffic to arrive at a statistically significant result. Longer test will cost more."
  },
  {
    "objectID": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html",
    "href": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html",
    "title": "Tidy Text Mining With R",
    "section": "",
    "text": "Computational Linguistics tasks:"
  },
  {
    "objectID": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html#text-preprocessing",
    "href": "posts/2011/2011-11-29-tidy-text-mining-with-r/index.html#text-preprocessing",
    "title": "Tidy Text Mining With R",
    "section": "Text preprocessing",
    "text": "Text preprocessing\n\nlibrary(janeaustenr)\nlibrary(dplyr)\nlibrary(stringr)\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %&gt;%\n  ungroup()\n\noriginal_books\n\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   &lt;chr&gt;                   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt;\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# ℹ 73,412 more rows\n\nlibrary(tidytext)\ntidy_books &lt;- original_books %&gt;%\n  unnest_tokens(word, text)\n\ntidy_books\n\n# A tibble: 725,055 × 4\n   book                linenumber chapter word       \n   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 and        \n 3 Sense & Sensibility          1       0 sensibility\n 4 Sense & Sensibility          3       0 by         \n 5 Sense & Sensibility          3       0 jane       \n 6 Sense & Sensibility          3       0 austen     \n 7 Sense & Sensibility          5       0 1811       \n 8 Sense & Sensibility         10       1 chapter    \n 9 Sense & Sensibility         10       1 1          \n10 Sense & Sensibility         13       1 the        \n# ℹ 725,045 more rows\n\n\n\ndata(stop_words)\n\ntidy_books &lt;- tidy_books %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_books\n\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# ℹ 217,599 more rows\n\n\n\nthis removes stop words\n\n\ntidy_books %&gt;%\n  count(word, sort = TRUE) \n\n# A tibble: 13,914 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 miss    1855\n 2 time    1337\n 3 fanny    862\n 4 dear     822\n 5 lady     817\n 6 sir      806\n 7 day      797\n 8 emma     787\n 9 sister   727\n10 house    699\n# ℹ 13,904 more rows\n\n\n\nlibrary(ggplot2)\n\ntidy_books %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(n &gt; 600) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\n#devtools::install_github(\"ropensci/gutenbergr\")\nlibrary(gutenbergr)\n\n#hgwells &lt;- gutenberg_download(c(35, 36,  159, 456, 1047, 3691, 5230, 11870, 12163, 23218, 28218, 35461,39585))\nhgwells &lt;- gutenberg_download(c(35, 36,  159))\n\nDetermining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest\n\n\n`curl` package not installed, falling back to using `url()`\nUsing mirror http://aleph.gutenberg.org\n\n\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/5/159/159.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n\n\n\ntidy_hgwells &lt;- hgwells %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\n\n\ntidy_hgwells %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 8,146 × 2\n   word         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 time       328\n 2 people     205\n 3 martians   165\n 4 black      152\n 5 night      140\n 6 machine    133\n 7 found      110\n 8 white      108\n 9 road       105\n10 day        102\n# ℹ 8,136 more rows\n\n\n\nbronte &lt;- gutenberg_download(c(1260, 768, 969, 9182, 767))\n\n\ntidy_bronte &lt;- bronte %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\nJoining with `by = join_by(word)`\n\ntidy_bronte %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 23,213 × 2\n   word       n\n   &lt;chr&gt;  &lt;int&gt;\n 1 time    1065\n 2 miss     854\n 3 day      825\n 4 don’t    780\n 5 hand     767\n 6 eyes     714\n 7 night    648\n 8 heart    638\n 9 looked   601\n10 door     591\n# ℹ 23,203 more rows\n\n\n\nlibrary(tidyr)\n\nfrequency &lt;- bind_rows(mutate(tidy_bronte, author = \"Brontë Sisters\"),\n                       mutate(tidy_hgwells, author = \"H.G. Wells\"), \n                       mutate(tidy_books, author = \"Jane Austen\")) %&gt;% \n  mutate(word = str_extract(word, \"[a-z']+\")) %&gt;%\n  count(author, word) %&gt;%\n  group_by(author) %&gt;%\n  mutate(proportion = n / sum(n)) %&gt;% \n  select(-n) %&gt;% \n  pivot_wider(names_from = author, values_from = proportion) %&gt;%\n  pivot_longer(`Brontë Sisters`:`H.G. Wells`,\n               names_to = \"author\", values_to = \"proportion\")\n\nfrequency\n\n# A tibble: 54,120 × 4\n   word      `Jane Austen` author          proportion\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n 1 a            0.00000919 Brontë Sisters  0.0000665 \n 2 a            0.00000919 H.G. Wells      0.0000293 \n 3 aback       NA          Brontë Sisters  0.00000391\n 4 aback       NA          H.G. Wells     NA         \n 5 abaht       NA          Brontë Sisters  0.00000391\n 6 abaht       NA          H.G. Wells     NA         \n 7 abandon     NA          Brontë Sisters  0.0000313 \n 8 abandon     NA          H.G. Wells      0.0000293 \n 9 abandoned    0.00000460 Brontë Sisters  0.0000899 \n10 abandoned    0.00000460 H.G. Wells      0.000234  \n# ℹ 54,110 more rows\n\n\n\nlibrary(scales)\n\n# expect a warning about rows with missing values being removed\nggplot(frequency, aes(x = proportion, y = `Jane Austen`, \n                      color = abs(`Jane Austen` - proportion))) +\n  geom_abline(color = \"gray40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  scale_color_gradient(limits = c(0, 0.001), \n                       low = \"darkslategray4\", high = \"gray75\") +\n  facet_wrap(~author, ncol = 2) +\n  theme(legend.position=\"none\") +\n  labs(y = \"Jane Austen\", x = NULL)\n\nWarning: Removed 39274 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 39276 rows containing missing values (`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n  cor.test(data = frequency[frequency$author == \"Brontë Sisters\",], ~ proportion + `Jane Austen`)\n\n\n    Pearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 110.73, df = 10275, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7286645 0.7462983\nsample estimates:\n      cor \n0.7376071 \n\n\n\ncor.test(data = frequency[frequency$author == \"H.G. Wells\",], \n         ~ proportion + `Jane Austen`)\n\n\n    Pearson's product-moment correlation\n\ndata:  proportion and Jane Austen\nt = 29.497, df = 4567, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3753856 0.4241064\nsample estimates:\n      cor \n0.4000286 \n\n\nkwik and kwok\n\nlibrary(quanteda)\nlibrary(gutenbergr)\n\nausten_works = gutenberg_works(author == \"Austen, Jane\")\nausten = gutenberg_download(austen_works$gutenberg_id)\n\nWarning: ! Could not download a book at http://aleph.gutenberg.org/1/3/4/1342/1342.zip.\nℹ The book may have been archived.\nℹ Alternatively, You may need to select a different mirror.\n→ See https://www.gutenberg.org/MIRRORS.ALL for options.\n\nhead(hgwells)\n\n# A tibble: 6 × 2\n  gutenberg_id text              \n         &lt;int&gt; &lt;chr&gt;             \n1           35 \"The Time Machine\"\n2           35 \"\"                \n3           35 \"An Invention\"    \n4           35 \"\"                \n5           35 \"by H. G. Wells\"  \n6           35 \"\"                \n\n# tidy_hgwells &lt;- hgwells %&gt;%\n#   unnest_tokens(word, text) %&gt;%\n#   anti_join(stop_words)\n\n#head(tidy_hgwells)\n\nthe_corpus &lt;- corpus(austen)\nthe_tokens &lt;- tokens(the_corpus,case_insensitive = TRUE)\n\nWarning: case_insensitive argument is not used.\n\nkwic_table &lt;- kwic(the_tokens,pattern = \"lady\",index = 1:100)\n#kwic_table &lt;- kwic(tokens(tidy_hgwells$word),pattern = \"time\")\n\n#kwic_table &lt;- kwic(tokens(tidy_hgwells$word),pattern = \"machine\",index = 1:400, case_insensitive = TRUE)\nnrow(kwic_table)\n\n[1] 2008\n\nhead(kwic_table,10)\n\nKeyword-in-context with 10 matches.                                                          \n  [text61, 5]                Gloucester, by which | lady |\n  [text98, 7]                deserved by his own. | Lady |\n [text100, 8] youthful infatuation which made her | Lady |\n [text112, 6]            her kindness and advice, | Lady |\n [text118, 4]                   passed away since | Lady |\n [text122, 2]                                That | Lady |\n [text142, 2]                                  To | Lady |\n [text143, 8]              favourite, and friend. | Lady |\n [text169, 1]                                     | Lady |\n [text177, 3]                   immediately after | Lady |\n                                \n ( who died 1800 )              \n Elliot had been an excellent   \n Elliot, had never              \n Elliot mainly relied for the   \n Elliot’s death, and they       \n Russell, of steady age         \n Russell, indeed, she           \n Russell loved them all;        \n Russell’s temples had long been\n Russell out of all the"
  },
  {
    "objectID": "posts/2011/2011-11-29-npl-python/index.html",
    "href": "posts/2011/2011-11-29-npl-python/index.html",
    "title": "Text Mining With Python",
    "section": "",
    "text": "import numpy as np                           # library for scientific computing and matrix \nimport matplotlib.pyplot as plt              # visualization library\nimport string\nimport re\n\nimport nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer    \n\nnltk.download('twitter_samples')\n\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /home/oren/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n\n\nTrue\n\n\n\n1nltk.download('stopwords')\n\n\ndef process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english') \n    \n2    tweet = re.sub(r'\\$\\w*', '', tweet)\n3    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n4    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n5    tweet = re.sub(r'#', '', tweet)\n6    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n    tweets_clean = []\n    for word in tweet_tokens:\n7        if (word not in stopwords_english and\n8                word not in string.punctuation):\n            # tweets_clean.append(word)\n9            stem_word = stemmer.stem(word)\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n\n\n1\n\ndownload the stopwords\n\n2\n\nremove stock market tickers like $GE\n\n3\n\nremove old style retweet text “RT”\n\n4\n\nremove hyperlinks\n\n5\n\nremove hashtags\n\n6\n\ntokenize tweets\n\n7\n\nremove stopwords\n\n8\n\nremove punctuation\n\n9\n\nstemming word\n\n\n\n\n[nltk_data] Downloading package stopwords to /home/oren/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = defaultdict(int)\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs\n\ndef build_vocab(freqs):\n    vocab = [k for k, v in freq.items() if (v &gt; 1 and k != '\\n')]\n    vocab.sort()\n    return vocab\n\nprocessing unknown tokens\n\ndef assign_unk(word):\n    \"\"\"\n    Assign tokens to unknown words\n    \"\"\"\n    \n    # Punctuation characters\n    # Try printing them out in a new cell!\n    punct = set(string.punctuation)\n    \n    # Suffixes\n    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n\n    # Loop the characters in the word, check if any is a digit\n    if any(char.isdigit() for char in word):\n        return \"--unk_digit--\"\n\n    # Loop the characters in the word, check if any is a punctuation character\n    elif any(char in punct for char in word):\n        return \"--unk_punct--\"\n\n    # Loop the characters in the word, check if any is an upper case character\n    elif any(char.isupper() for char in word):\n        return \"--unk_upper--\"\n\n    # Check if word ends with any noun suffix\n    elif any(word.endswith(suffix) for suffix in noun_suffix):\n        return \"--unk_noun--\"\n\n    # Check if word ends with any verb suffix\n    elif any(word.endswith(suffix) for suffix in verb_suffix):\n        return \"--unk_verb--\"\n\n    # Check if word ends with any adjective suffix\n    elif any(word.endswith(suffix) for suffix in adj_suffix):\n        return \"--unk_adj--\"\n\n    # Check if word ends with any adverb suffix\n    elif any(word.endswith(suffix) for suffix in adv_suffix):\n        return \"--unk_adv--\"\n    \n    # If none of the previous criteria is met, return plain unknown\n    return \"--unk--\"\n\n\n# select the lists of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets\n\n# let's see how many tweets we have\nprint(\"Number of tweets: \", len(tweets))\n\nNumber of tweets:  10000\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2011,\n  author = {Bochman, Oren},\n  title = {Text {Mining} {With} {Python}},\n  date = {2011-11-29},\n  url = {https://orenbochman.github.io/blog//posts/2011/2011-11-29-npl-python},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2011. “Text Mining With Python.” November\n29, 2011. https://orenbochman.github.io/blog//posts/2011/2011-11-29-npl-python."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html",
    "href": "posts/2011/2011-08-11-time-management/index.html",
    "title": "Time management Tips",
    "section": "",
    "text": "Effective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance. In this article, we’ll explore various time management strategies, focusing on learning from others, project management techniques, and effective meetings."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#introduction",
    "href": "posts/2011/2011-08-11-time-management/index.html#introduction",
    "title": "Time management Tips",
    "section": "",
    "text": "Effective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance. In this article, we’ll explore various time management strategies, focusing on learning from others, project management techniques, and effective meetings."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#learning-from-others",
    "href": "posts/2011/2011-08-11-time-management/index.html#learning-from-others",
    "title": "Time management Tips",
    "section": "Learning from Others",
    "text": "Learning from Others\nTo improve your time management skills, consider learning from different sources:\n\nWisdom from experienced individuals: Look for insights and advice from people who have been successful in managing their time. They can provide valuable tips and practical approaches to help you make the most of your time.\nFresh perspectives from younger generations: Younger people often have unique and innovative ideas on how to approach time management. Their perspectives can provide fresh insights on the world and how to navigate it efficiently.\nCassandra’s prophecies: Although they may seem pessimistic or overly cautious, pay attention to people who predict potential problems or obstacles. By considering their warnings, you can proactively prepare for possible challenges, preventing self-fulfilling prophecies."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#project-management",
    "href": "posts/2011/2011-08-11-time-management/index.html#project-management",
    "title": "Time management Tips",
    "section": "Project management",
    "text": "Project management\n\nMeasure work by output, not input: Focus on the results produced rather than the amount of time spent on a task. This will encourage efficiency and productivity.\nDefine specific goals: Clearly outline the objectives, responsibilities, and tasks associated with each project or job.\nTrack goal progress: Regularly monitor the progress of your goals to ensure they are on track for completion.\nProvide periodic progress reports: Keep stakeholders informed with brief, regular updates on project status.\nSet high-quality performance objectives: Ensure that your goals are challenging, attainable, and aligned with your overall mission.\nMaintain a project list: Keep a running list of all projects, breaking them down into smaller, manageable tasks if necessary.\nFocus on the next step, not the final goal: Concentrate on completing the immediate task at hand rather than getting overwhelmed by the overall project.\nPrioritize tasks effectively: Determine which tasks are most important and tackle them first.\nSchedule your day realistically: Plan your day in a way that allows for adequate time to complete tasks without feeling rushed or overwhelmed."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#conducting-efficient-meetings",
    "href": "posts/2011/2011-08-11-time-management/index.html#conducting-efficient-meetings",
    "title": "Time management Tips",
    "section": "Conducting Efficient Meetings:",
    "text": "Conducting Efficient Meetings:\n\nEstablish clear meeting objectives: Ensure that every meeting has a specific purpose, such as group bonding, reaching a group decision, or conducting peer-to-peer negotiations.\nUse alternative communication methods when appropriate: Utilize emails and letters for unidirectional information flow or for maintaining records, as they can be more efficient than meetings for certain tasks."
  },
  {
    "objectID": "posts/2011/2011-08-11-time-management/index.html#conclusion",
    "href": "posts/2011/2011-08-11-time-management/index.html#conclusion",
    "title": "Time management Tips",
    "section": "Conclusion:",
    "text": "Conclusion:\nBy implementing these time management strategies, you can increase your productivity, achieve your goals, and maintain a healthy work-life balance. Remember, the key to success is consistently refining and adapting your approach to time management as you encounter new challenges and opportunities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oren Bochman’s Blog",
    "section": "",
    "text": "Quarto Migration Notes\n\n\nI have moved this blog from blogger to Jekyl and finaly to quarto. \n\n\n\nquarto\n\n\nblogging\n\n\ncode\n\n\n\nsome migration notes from Blooger to Jekyl to Quarto blog.\n\n\n\n\n\nJan 30, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPolitical Scenario Prediction Using Game theory\n\n\n\n\n\n\nBayesian updating\n\n\nforecasting\n\n\ngame theory\n\n\nprediction\n\n\npolicy engineering\n\n\npolicy analysis\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nBeacuase auto-ml is a Superpower\n\n\n\ncheatsheets\n\n\ncode\n\n\ndata science\n\n\nauto-ml\n\n\n\nAutogluon is a auto-ml framework, here are three cheetsheet for accellerating data science workloads\n\n\n\n\n\nDec 20, 2023\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nWikisym 2012\n\n\nConference Report\n\n\n\nreport\n\n\nwikisym\n\n\nconference\n\n\n\n\n\n\n\n\n\nJul 26, 2022\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nGlossary of terms for Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\ndeep learning\n\n\nglossary\n\n\nnotes\n\n\nneural networks\n\n\n\nGlossary of terms in Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for Lesson 1 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursera\n\n\n\ndeep learning\n\n\nneural networks\n\n\nnotes\n\n\ncoursera\n\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for Lesson 2 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\ndeep learning\n\n\nneural networks\n\n\nnotes\n\n\ncoursera\n\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for Lesson 3 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\ndeep learning\n\n\nneural networks\n\n\nnotes\n\n\ncoursera\n\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for Lesson 4 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\ndeep learning\n\n\nneural networks\n\n\nnotes\n\n\ncoursera\n\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for Lesson 5 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\ndeep learning\n\n\nneural networks\n\n\nnotes\n\n\ncoursera\n\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for Lesson 10 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursera\n\n\n\ndeep learning\n\n\nneural networks\n\n\nnotes\n\n\ncoursera\n\n\n\nNotes on Deep leaning and ML from Neural Networks for Machine Learning by Geoffrey Hintonon on Coursera\n\n\n\n\n\nAug 6, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nA/B testing cost and risks?\n\n\n\n\n\n\nPPC,\n\n\ndata science,\n\n\ndigital marketing,\n\n\nquantitative marketing,\n\n\nCRO,\n\n\nexperimental design,\n\n\nA/B testing\n\n\n\nA/B testing cost and risks and some recommendation.\n\n\n\n\n\nJul 29, 2017\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With Python\n\n\na number of NLP tasks in Python\n\n\n\npython\n\n\nNLP\n\n\ntext mining\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 29, 2011\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining With R\n\n\na number of NLP tasks in R\n\n\n\nR\n\n\nNLP\n\n\ntext Mining\n\n\ncode\n\n\n\n\n\n\n\n\n\nNov 29, 2011\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\nan update on NLP with R\n\n\n\nR\n\n\nNLP\n\n\nText Mining\n\n\n\n\n\n\n\n\n\nNov 29, 2011\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nR Books\n\n\nbooks reviews & recommendations\n\n\n\nreviews\n\n\ndata analysis\n\n\nR\n\n\n\nIn this updated post I included some R books you might want to look at if you are getting started with R for data science.\n\n\n\n\n\nAug 26, 2011\n\n\nOren Bochman\n\n\n\n\n\n\n\n\n\n\n\n\nTime management Tips\n\n\nWe could all use a productivity boost\n\n\n\ntime management\n\n\nproductivity\n\n\n\nEffective time management is crucial for success in both personal and professional life. With the right approach, you can achieve more in less time while maintaining a healthy work-life balance\n\n\n\n\n\nAug 11, 2011\n\n\nOren Bochman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a space to share my insights about my interests.\nI’ve decided to migrate to Quarto and see what the platform can do."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "This is a space to share my insights about my interests.\nI’ve decided to migrate to Quarto and see what the platform can do."
  },
  {
    "objectID": "about.html#newsletter",
    "href": "about.html#newsletter",
    "title": "About",
    "section": "Newsletter",
    "text": "Newsletter\nIf you enjoyed this post, then don’t miss out on any future posts by subscribing to my email newsletter"
  },
  {
    "objectID": "about.html#buy-me-coffe",
    "href": "about.html#buy-me-coffe",
    "title": "About",
    "section": "Buy Me Coffe",
    "text": "Buy Me Coffe"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Quarto Migration Notes\n\n\nI have moved this blog from blogger to Jekyl and finaly to quarto. \n\n\n\n\n\nJan 30, 2024\n\n\n\n\n\n\n\nPolitical Scenario Prediction Using Game theory\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\n\n\n\n\n\nAutoGluon Cheetsheets\n\n\nBeacuase auto-ml is a Superpower\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\nWikisym 2012\n\n\nConference Report\n\n\n\n\n\nJul 26, 2022\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nGlossary of terms for Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nDeep Neural Networks - Notes From Hinton’s Course\n\n\n\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nNotes for Lesson 1 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursera\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nNotes for Lesson 2 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nNotes for Lesson 3 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nNotes for Lesson 4 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nNotes for Lesson 5 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursa\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nNotes for Lesson 10 of Deep Neural Networks\n\n\ncourse by Geffory Hinton on Coursera\n\n\n\n\n\nAug 6, 2017\n\n\n\n\n\n\n\nA/B testing cost and risks?\n\n\n\n\n\n\n\n\nJul 29, 2017\n\n\n\n\n\n\n\nText Mining With Python\n\n\na number of NLP tasks in Python\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nText Mining With R\n\n\na number of NLP tasks in R\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nTidy Text Mining With R\n\n\nan update on NLP with R\n\n\n\n\n\nNov 29, 2011\n\n\n\n\n\n\n\nR Books\n\n\nbooks reviews & recommendations\n\n\n\n\n\nAug 26, 2011\n\n\n\n\n\n\n\nTime management Tips\n\n\nWe could all use a productivity boost\n\n\n\n\n\nAug 11, 2011\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html",
    "href": "posts/2011/2011-08-26-R-books/index.html",
    "title": "R Books",
    "section": "",
    "text": "In this updated post I included some R books you might want to look at if you are getting started with R for data science\nFellow data scientists, do not be overwhelmed by vast multiplicities of numbers, for they are but symbols of the natural order. Embrace their uncertainty and seek to understand the patterns within it. I offer you some of the first R books I came across. I got started with R in 2011, and I decided to update it to focus on stats and ml books I’ve come across with an attempt to list my favorites with their lecture notes and video lectures where available."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#software-for-data-analysis-programming-with-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#software-for-data-analysis-programming-with-r",
    "title": "R Books",
    "section": "Software for data analysis: programming with R",
    "text": "Software for data analysis: programming with R\n\n\n\n\n\nSoftware for data analysis\n\n\n\nChambers, John M. 2008. Software for Data Analysis Programming with r. New York; London: Springer. http://www.amazon.de/Software-Data-Analysis-Programming-Statistics/dp/0387759352.\nIn (Chambers 2008) the author presents the essential guidebook for those who wish to learn how to use the R programming language for data analysis. Chambers is a renowned statistician, and he shares his expertise in the field of data analysis through this book. The book covers a wide range of topics related to data analysis, including data structures, object-oriented programming, graphics, and statistical modeling. It also offers a practical approach to understanding R programming, with an emphasis on building applications that can handle large datasets. Overall, this is a valuable resource for those who want to learn about R programming for data analysis. It is a comprehensive guide that covers all the essential aspects of data analysis and provides hands-on experience with R programming. The book is written in a clear and concise manner, making it easy to follow even for beginners."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#principles-of-statistical-data-handling",
    "href": "posts/2011/2011-08-26-R-books/index.html#principles-of-statistical-data-handling",
    "title": "R Books",
    "section": "Principles of Statistical Data Handling",
    "text": "Principles of Statistical Data Handling\n\n\n\n\n\nPrinciples of Statistical Data Handling\n\n\n\nDavidson, Fred. 1996. Principles of Statistical Data Handling. https://doi.org/10.4135/9781483348902.\nIn (Davidson 1996) the author offers a guide to the foundations of this field, including exploratory data analysis, hypothesis testing, and model building. Through careful attention and disciplined study, one can cultivate a deep understanding of the methods and techniques that underlie statistical data handling, and by following, we shall approach data with a rational and objective mindset, illuminating with our analytical skills the meaningful insights and so make more informed decisions.\nA Stoic would say “Remember, the data is not what you see, but what you make of it. So, approach it with a clear mind, free from bias and preconceptions, and seek the truth that lies hidden within.” By applying oneself to these principles with diligence and perseverance, we may yet unlock the full potential of statistical data handling, and make a valuable contribution to the world of data science."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#probability-statistics-for-engineers-and-scientists",
    "href": "posts/2011/2011-08-26-R-books/index.html#probability-statistics-for-engineers-and-scientists",
    "title": "R Books",
    "section": "Probability & Statistics for Engineers and Scientists",
    "text": "Probability & Statistics for Engineers and Scientists\n\n\n\n\n\nProbability & Statistics for Engineers and Scientists\n\n\n\nWalpole, Ronald E., Raymond H. Myers, Sharon L. Myers, and Keying Ye. 2007. Probability & Statistics for Engineers and Scientists. 8th ed. Upper Saddle River: Pearson Education.\nIn (Walpole et al. 2007), the autor presents a path to comprehension of probability and statistics is laid out before you.\nIt is a journey of discovery that will require patience, diligence, and a willingness to learn. The author presents the tools and techniques needed to analyze data and draw meaningful conclusions. By using R, one can unlock the secrets hidden in the data.\nFear not mistakes, for they are but stepping stones towards deeper understanding, only take care to learn from them, and use the knowledge gained to improve your understanding daily.\nWith each chapter, you will gain a greater understanding of the complex and interconnected world of probability and statistics.\nEmbrace the journey, and may the numbers guide you towards enlightenment."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#introduction-to-probability-and-statistics-using-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#introduction-to-probability-and-statistics-using-r",
    "title": "R Books",
    "section": "Introduction to Probability and Statistics Using R",
    "text": "Introduction to Probability and Statistics Using R\n\n\n\n\n\nIntroduction to Probability and Statistics Using R\n\n\n\nKerns, G. Jay. 2018. Introduction to Probability and Statistics Using r.\nIn (Kerns 2018), recommended by my old friend Adam Hyland, the author covers the basic concepts of probability and statistics using the R programming language.\nIt is a useful resource for data scientists who wish to gain a deeper understanding of probability and statistics and how to apply them.\nStarting with basic probability, distributions, hypothesis testing, regression analysis, it then proceeds to more advanced topics such as Bayesian statistics, machine learning, and time series analysis.\nEach chapter presents clear explanations, examples, and R code to help the reader grasp the theoretical concepts and apply them in practice. By including a wide range of real-world examples and datasets, it helps the readers conect the concepts and techniques with thier application to real data.\nA complimentry copy is available at this link"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#statistical-computing-with-r",
    "href": "posts/2011/2011-08-26-R-books/index.html#statistical-computing-with-r",
    "title": "R Books",
    "section": "Statistical Computing with R",
    "text": "Statistical Computing with R\n\n\n\n\n\nStatistical Computing with R\n\n\n\nRizzo, Maria L. 2019. Statistical Computing with r Maria l. Rizzo. Second edition. Chapman & Hall/CRC the r Series. Boca Raton: CRC Press, Taylor & Francis Group.\nIn (Rizzo 2019) the author presents is a comprehensive guide to the analysis and manipulation of data using R. Within, we are introduced to a wide variety of statistical concepts and tools that enable us to explore and understand complex datasets.\nStandard statistical techniques used in data analysis, such as probability, hypothesis testing are covered. We learn about the normal distribution and its importance in statistical analysis, as well as the Poisson distribution, which is used to model counts of events.\nThe book introduces us to the use of statistical transformations, such as the log transformation, which is often used to make skewed data more normal. We also learn about density estimation and the use of histograms and kernel density estimates to visualize data.\nThe concepts of sampling and random variables are explored, as well as the calculation of sample means and standard errors. We also learn about the use of random samples from Monte Carlo simulation to approximate probabilities and calculate statistics.\nThe book covers the use of algorithms and samplers, such as the Metropolis-Hastings algorithm, to explore parameter space and to generate samples from distributions of interest. We learn about the importance of convergence and the use of proposals in Monte Carlo sampling.\nThe concepts of bias and variance are explored, as well as the calculation of confidence intervals and the use of hypothesis testing to evaluate statistical significance. We also learn about the use of the bootstrap and jackknife methods to find the level of uncertainty in our estimates.\nThroughout the book, we are introduced to the use of R for statistical computing. We learn about the use of formulas to specify statistical models, as well as the use of packages for data manipulation and visualization.\nOverall, “Statistical Computing with R” is an essential resource for anyone interested in using statistical methods to analyze data. It provides a lucid and comprehensive treatment of statistical concepts and their practical implementation using R."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#bayesian-methods-of-data-analysis",
    "href": "posts/2011/2011-08-26-R-books/index.html#bayesian-methods-of-data-analysis",
    "title": "R Books",
    "section": "Bayesian methods of Data Analysis",
    "text": "Bayesian methods of Data Analysis\n\n\n\n\n\nBayesian methods of Data Analysis\n\n\n\nCarlin, B. P., and T. A. Louis. 2008. Bayesian Methods for Data Analysis. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://books.google.co.il/books?id=GTJUt8fcFx8C.\nIn (Carlin and Louis 2008) the author presents us with a framework that is grounded in the philosophy of probability theory. We learn to seek a baseline model then approach the problem at hand with a Bayesian perspective.\nThrough the use of Bayesian models, we can compute the conditional distributions of our data and evaluate the error and loss functions. We must consider convergence, the choice of priors, and how they are specified. We use Bayes’ rule to compute the posterior distribution and marginal likelihood, and we obtain point estimates and credible intervals.\nThe use of the Gibbs sampler and the Metropolis-Hastings algorithm in MCMC methods are presented as tools to obtain a sample from the posterior distribution. We use WinBUGS code and Monte Carlo simulations to produce results that are in line with the data observed.\nWe are introduced to the concept of the Bayes factor, and how it is used to compare models. We also understand how the use of the Jeffreys prior, the hyperprior, and the conjugate prior can be used to simplify our computations.\nIn Bayesian methods, we use the full conditional distributions to obtain the joint posterior distribution of our parameters. We also compute the marginal posterior distribution, which can be used to obtain a credible interval.\nWe are shown how to deal with univariate and multivariate data, and how to model the random effects and covariate effects. We also understand how to evaluate the performance of our models through histograms, percentiles, and plots.\nIn this work, we are presented with a practical and useful guide to Bayesian methods that can be applied to a variety of problems."
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#information-theory-inference-and-learning-algorithms",
    "href": "posts/2011/2011-08-26-R-books/index.html#information-theory-inference-and-learning-algorithms",
    "title": "R Books",
    "section": "Information Theory, Inference and Learning Algorithms",
    "text": "Information Theory, Inference and Learning Algorithms\n\n\n\n\n\nInformation Theory, Inference and Learning Algorithms\n\n\n\nMacKay, David J. C. 2003. Information Theory, Inference, and Learning Algorithms. Copyright Cambridge University Press.\nInformation Theory, Inference and Learning Algorithms (MacKay 2003) by David J.C. MacKay FRS - Course notes: Information Theory, Pattern Recognition, and Neural Networks.\nThis is not an R book as far as I recall but it is available online, together with lectures by the author. I recommend this book and videos for anyone interested bayesian data analysis. The author was a physicist, a leading bayesian and pioneer in Bayesian Neural Networks whose work is very relavant even today (2024)"
  },
  {
    "objectID": "posts/2011/2011-08-26-R-books/index.html#references",
    "href": "posts/2011/2011-08-26-R-books/index.html#references",
    "title": "R Books",
    "section": "References",
    "text": "References\n\n\n\nSoftware for data analysis\nPrinciples of Statistical Data Handling\nProbability & Statistics for Engineers and Scientists\nIntroduction to Probability and Statistics Using R\nStatistical Computing with R\nBayesian methods of Data Analysis\nInformation Theory, Inference and Learning Algorithms"
  },
  {
    "objectID": "posts/2011/2011-11-29-text-mining-with-r/index.html",
    "href": "posts/2011/2011-11-29-text-mining-with-r/index.html",
    "title": "Text Mining With R",
    "section": "",
    "text": "Computational Linguistics tasks:"
  },
  {
    "objectID": "posts/2011/2011-11-29-text-mining-with-r/index.html#text-preprocessing",
    "href": "posts/2011/2011-11-29-text-mining-with-r/index.html#text-preprocessing",
    "title": "Text Mining With R",
    "section": "Text preprocessing",
    "text": "Text preprocessing\n\n4tm_corpus &lt;- tm_map(tm_corpus, tolower)\ninspect(tm_corpus)\n\n\n4\n\nthis makes all the tokens lowercase\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs, hospitals, doctors                                                                                                                                      \n [2] smog, pollution, micro-plastics, environment.                                                                                                                  \n [3] doctors, hospitals, healthcare                                                                                                                                 \n [4] pollution, environment, water.                                                                                                                                 \n [5] i love nlp with deep learning.                                                                                                                                 \n [6] i love machine learning.                                                                                                                                       \n [7] he said he was keeping the wolf from the door.                                                                                                                 \n [8] time flies like an arrow, fruit flies like a banana.                                                                                                           \n [9] pollution, greenhouse gasses, ghg, hydrofluorocarbons, ozone hole, global warming. montreal protocol.                                                          \n[10] greenhouse gasses, hydrofluorocarbons, perfluorocarbons, sulfur hexafluoride, carbon dioxide, carbon monoxide, co2, hydrofluorocarbons, methane, nitrous oxide.\n\n\n\n5tm_corpus &lt;- tm_map(tm_corpus, content_transformer(removePunctuation))\ninspect(tm_corpus)\n\n\n5\n\nthis removes punctuation tokens\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                              \n [2] smog pollution microplastics environment                                                                                                             \n [3] doctors hospitals healthcare                                                                                                                         \n [4] pollution environment water                                                                                                                          \n [5] i love nlp with deep learning                                                                                                                        \n [6] i love machine learning                                                                                                                              \n [7] he said he was keeping the wolf from the door                                                                                                        \n [8] time flies like an arrow fruit flies like a banana                                                                                                   \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                       \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co2 hydrofluorocarbons methane nitrous oxide\n\n\n\n6tm_corpus &lt;- tm_map(tm_corpus, removeWords, stopwords(\"english\"))\ninspect(tm_corpus)\n\n\n6\n\nthis removes stop words\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                              \n [2] smog pollution microplastics environment                                                                                                             \n [3] doctors hospitals healthcare                                                                                                                         \n [4] pollution environment water                                                                                                                          \n [5]  love nlp  deep learning                                                                                                                             \n [6]  love machine learning                                                                                                                               \n [7]  said   keeping  wolf   door                                                                                                                         \n [8] time flies like  arrow fruit flies like  banana                                                                                                      \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                       \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co2 hydrofluorocarbons methane nitrous oxide\n\n\n\n7tm_corpus &lt;- tm_map(tm_corpus, removeNumbers)\ninspect(tm_corpus)\n\n\n7\n\nthis removes numbers\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drugs hospitals doctors                                                                                                                             \n [2] smog pollution microplastics environment                                                                                                            \n [3] doctors hospitals healthcare                                                                                                                        \n [4] pollution environment water                                                                                                                         \n [5]  love nlp  deep learning                                                                                                                            \n [6]  love machine learning                                                                                                                              \n [7]  said   keeping  wolf   door                                                                                                                        \n [8] time flies like  arrow fruit flies like  banana                                                                                                     \n [9] pollution greenhouse gasses ghg hydrofluorocarbons ozone hole global warming montreal protocol                                                      \n[10] greenhouse gasses hydrofluorocarbons perfluorocarbons sulfur hexafluoride carbon dioxide carbon monoxide co hydrofluorocarbons methane nitrous oxide\n\n\n\n8tm_corpus &lt;- tm_map(tm_corpus, stemDocument, language=\"english\")\ninspect(tm_corpus)\n\n\n8\n\nthis stems the words\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drug hospit doctor                                                                                                                       \n [2] smog pollut microplast environ                                                                                                           \n [3] doctor hospit healthcar                                                                                                                  \n [4] pollut environ water                                                                                                                     \n [5] love nlp deep learn                                                                                                                      \n [6] love machin learn                                                                                                                        \n [7] said keep wolf door                                                                                                                      \n [8] time fli like arrow fruit fli like banana                                                                                                \n [9] pollut greenhous gass ghg hydrofluorocarbon ozon hole global warm montreal protocol                                                      \n[10] greenhous gass hydrofluorocarbon perfluorocarbon sulfur hexafluorid carbon dioxid carbon monoxid co hydrofluorocarbon methan nitrous oxid\n\n\n\n9tm_corpus &lt;- tm_map(tm_corpus, stripWhitespace)\ninspect(tm_corpus)\n\n\n9\n\nRemoving Whitespaces - a single white space or group of whitespaces may be considered to be a token within a corpus. This is how we remove these token\n\n\n\n\n&lt;&lt;SimpleCorpus&gt;&gt;\nMetadata:  corpus specific: 1, document level (indexed): 0\nContent:  documents: 10\n\n [1] drug hospit doctor                                                                                                                       \n [2] smog pollut microplast environ                                                                                                           \n [3] doctor hospit healthcar                                                                                                                  \n [4] pollut environ water                                                                                                                     \n [5] love nlp deep learn                                                                                                                      \n [6] love machin learn                                                                                                                        \n [7] said keep wolf door                                                                                                                      \n [8] time fli like arrow fruit fli like banana                                                                                                \n [9] pollut greenhous gass ghg hydrofluorocarbon ozon hole global warm montreal protocol                                                      \n[10] greenhous gass hydrofluorocarbon perfluorocarbon sulfur hexafluorid carbon dioxid carbon monoxid co hydrofluorocarbon methan nitrous oxid\n\n\n\ndtm &lt;- DocumentTermMatrix(tm_corpus)\ninspect(dtm)\n\n&lt;&lt;DocumentTermMatrix (documents: 10, terms: 43)&gt;&gt;\nNon-/sparse entries: 53/377\nSparsity           : 88%\nMaximal term length: 17\nWeighting          : term frequency (tf)\nSample             :\n    Terms\nDocs doctor environ fli gass hospit hydrofluorocarbon learn like love pollut\n  1       1       0   0    0      1                 0     0    0    0      0\n  10      0       0   0    1      0                 2     0    0    0      0\n  2       0       1   0    0      0                 0     0    0    0      1\n  3       1       0   0    0      1                 0     0    0    0      0\n  4       0       1   0    0      0                 0     0    0    0      1\n  5       0       0   0    0      0                 0     1    0    1      0\n  6       0       0   0    0      0                 0     1    0    1      0\n  7       0       0   0    0      0                 0     0    0    0      0\n  8       0       0   2    0      0                 0     0    2    0      0\n  9       0       0   0    1      0                 1     0    0    0      1\n\n\n\nfindFreqTerms(dtm, 2)\n\n [1] \"doctor\"            \"hospit\"            \"environ\"          \n [4] \"pollut\"            \"learn\"             \"love\"             \n [7] \"fli\"               \"like\"              \"gass\"             \n[10] \"greenhous\"         \"hydrofluorocarbon\" \"carbon\"           \n\n\n\nfindAssocs(dtm, \"polution\", 0.8)\n\n$polution\nnumeric(0)\n\n\n\nas.matrix(dtm)\n\n    Terms\nDocs doctor drug hospit environ microplast pollut smog healthcar water deep\n  1       1    1      1       0          0      0    0         0     0    0\n  2       0    0      0       1          1      1    1         0     0    0\n  3       1    0      1       0          0      0    0         1     0    0\n  4       0    0      0       1          0      1    0         0     1    0\n  5       0    0      0       0          0      0    0         0     0    1\n  6       0    0      0       0          0      0    0         0     0    0\n  7       0    0      0       0          0      0    0         0     0    0\n  8       0    0      0       0          0      0    0         0     0    0\n  9       0    0      0       0          0      1    0         0     0    0\n  10      0    0      0       0          0      0    0         0     0    0\n    Terms\nDocs learn love nlp machin door keep said wolf arrow banana fli fruit like time\n  1      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  2      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  3      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  4      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  5      1    1   1      0    0    0    0    0     0      0   0     0    0    0\n  6      1    1   0      1    0    0    0    0     0      0   0     0    0    0\n  7      0    0   0      0    1    1    1    1     0      0   0     0    0    0\n  8      0    0   0      0    0    0    0    0     1      1   2     1    2    1\n  9      0    0   0      0    0    0    0    0     0      0   0     0    0    0\n  10     0    0   0      0    0    0    0    0     0      0   0     0    0    0\n    Terms\nDocs gass ghg global greenhous hole hydrofluorocarbon montreal ozon protocol\n  1     0   0      0         0    0                 0        0    0        0\n  2     0   0      0         0    0                 0        0    0        0\n  3     0   0      0         0    0                 0        0    0        0\n  4     0   0      0         0    0                 0        0    0        0\n  5     0   0      0         0    0                 0        0    0        0\n  6     0   0      0         0    0                 0        0    0        0\n  7     0   0      0         0    0                 0        0    0        0\n  8     0   0      0         0    0                 0        0    0        0\n  9     1   1      1         1    1                 1        1    1        1\n  10    1   0      0         1    0                 2        0    0        0\n    Terms\nDocs warm carbon dioxid hexafluorid methan monoxid nitrous oxid perfluorocarbon\n  1     0      0      0           0      0       0       0    0               0\n  2     0      0      0           0      0       0       0    0               0\n  3     0      0      0           0      0       0       0    0               0\n  4     0      0      0           0      0       0       0    0               0\n  5     0      0      0           0      0       0       0    0               0\n  6     0      0      0           0      0       0       0    0               0\n  7     0      0      0           0      0       0       0    0               0\n  8     0      0      0           0      0       0       0    0               0\n  9     1      0      0           0      0       0       0    0               0\n  10    0      2      1           1      1       1       1    1               1\n    Terms\nDocs sulfur\n  1       0\n  2       0\n  3       0\n  4       0\n  5       0\n  6       0\n  7       0\n  8       0\n  9       0\n  10      1\n\n\nload(url(“https://cbail.github.io/Trump_Tweets.Rdata”)) head(trumptweets$text)"
  },
  {
    "objectID": "posts/2012/2012-07-26-wikisym-2012_files/index.html",
    "href": "posts/2012/2012-07-26-wikisym-2012_files/index.html",
    "title": "Wikisym 2012",
    "section": "",
    "text": "Due to a kind grant by the WikiMedia Foundation I was able to attend Wikisym 2012 in Linz Austria what follows is my report on the event.\n\n\n\n\n\n\nBackground\n\n\n\n\n\nThe renovated Ars Electronica Center at Linz, seen from the bridge across the Danube at night\n\n\n\nHPaul, CC BY-SA 3.0 via Wikimedia Commons\n\nI am a Wikipedian based in Budapest Hungary. I have been active for the last year with WM.HU and participated in a number of the local event’s chapters ever since being introduced to them at Wikimania 2011 in Haifa. During such a meeting I, Bence Damkos, and other chapter luminaries got to discussing many apparent cultural paradoxes taking place in a virtual community. Since I was studying the theory of games at the time I began to notice that some of the situations were very similar to a classic game such as the prisoner’s dilemma and the battle of the sexes while others resembled second-price sealed actions and bargaining games. I was intrigued and I started publishing some analysis on a page on Meta.\nAt this time I came across some interesting ideas from another researcher, Jodi Schneider who introduced me to the field of Computer Supported Collaborative Work (CSCW) and to her area of research - the deletion process. Eventually, she suggested that I should attend wikisym. However, I had no background in writing a conference paper I asked her for help and she copy-edited my work guiding me through a number of tricky issues. I eventually submitted the paper and to my surprise, it was accepted. So I took a train to Linz - I was surprised when after boarding the train that I had to reserve a seat and accordingly had to stand for the duration of the five-hour journey. By the time I arrived at the little town it was late and I was exhausted. I took a bus and ended in a hotel by the Danube.\n\n\nAt the Conference\nOn the morning of the conference, I took breakfast and met some of my favorite wikipedians - Maryna Pinchuk and Ryan Faulkner who were preparing to give a paper on their work in running editor engagement experiment - in which I had unwittingly participated. After a short chat I made my way to the venue the Ars Electronica and I could not believe my eyes - the conference was hosted by one of the most amazing technology museums in Europe. In the evening, the building would completely dominate the riverside’s view with its digital animation installations.\n\n\n\n\n\nR. Stuart Geiger\n\n\n\nAnne Helmond, CC by-nc-nd 2.0 via flicker\n\nThe Conference began with a number of presentations. I was impressed by most of the presentation but my sentiments were clearly not shared by everyone at the conference. I later learned that some of the more vehement voices were doctoral students who were out to prove their mettle. The papers that most struck my fancy used a number of novel techniques. Ranging from actuarial, survival analysis through SNA to sentiment analysis. Classifying Wikipedia Articles Using Network Motif Counts and Ratios by Guangyu Wu, Martin Harrigan and Pádraig Cunningham was one of the hardest to understand. It used a novel SNA technique to classify Wikipedia articles. However, it seemed that the other participant did not like the level of detail that the researchers had provided. Dr. Bernie Hoagan a Research Fellow from the Oxford Internet Institute asked the researchers why they had not tried to use ERGMs which might give more accurate results. I would later correspond with Dr. Hoagan and he helped me get started with Social network analysis. A paper by Michela Ferron and Paolo Massa titled Psychological processes underlying Wikipedia representations of natural and man-made disasters. It showcased the use of sentiment analysis. I was already familier with this method from my work in a Natural Language Programming outfit in Israel for which I wrote a search engine for the Hebrew Wikipedia. But I had consider this technique as very complex to set-up. On reviewing the paper I realised that an off the shelf tool called LIWC (Linguistic Inquiry and Word Count) can do the job. LIWC was developed by a team lead by James W. Pennebaker whose book The Secret Life of Pronouns is a gentle introduction to the intricacies of sentiment analysis. What remained difficult to grasp was a three-dimensional model of sentiment. I was unfamiliar with the terminology so I would end up rereading this paper a couple of times. But this was not the only paper to use sentiment analysis or natural language technology. Manypedia: Comparing Language Points of View of Wikipedia Communities by Paolo Massa and Federico Scrinzi which showed a tool that allows users to compare different language edition version of the same article in their own language using machine translation. A second paper to discuss sentiment analysis, this time focusing on talk pages was: Emotions and dialogue in a peer-production community: the case of Wikipedia. This paper used an even more complex paradigm than the previous one. It utilized Margaret M. Bradley & Peter J. Lang’s ANEW (Affective Norms for English Words) word list to create a three-dimensional model of sentiment (valence, arousal and dominance). Even more interesting were its conclusions regarding participation of women and its implication on Wikipedia’s growing gender gap.\n\n\n\n\n\nHeather Ford, Jimmy Wales\n\n\n\nMessedrocker, CC BY 1.0 via Wikimedia Commons\n\nI would discuss some of my ideas to some of the participants over dinner. One amusing debate included [Stuart Geiger] and when I quoted a point from an excellent paper he pointed out that he had written it. I also met with heather ford who co-authored a paper with Mr Geiger. Heather Ford told us about her blog Ethnography matters which I started to follow because it turns out that ethnography really matters These include work by Stuart Geiger and on the lives of robots using trace ethnography. During the conference I met with Jodi Schneider but we had little opportunity to chat due to an upcoming deadline. I enjoy following her research on deletion as well as on Argumentation in collaborative deliberations. I decided to help Wikipedia’s research newsletter by abstracting and providing laymen’s summaries to CSCW related research.\n\n\nPanels, Demos and Posters\n\n\n\n\n\nPhoebe Ayers at WikiSym\n\n\n\nRagesoss, CC BY-SA 3.0 via Wikimedia Commons\n\nI found out that the WikiSym conference had a colourful history and participated in a discussion mediated by the delectable Phoebe Ayers on the conference’s future. I suggested that the conference should be collocated with Wikimania since this would help reduce cost of community members who attend the Wikimania conference. A second conundrum being debated being the issue of open academy. This was an issue of growing urgency since the WMF, one of Wikisym’s chief sponsors prefers to support open access open research work. I think that Phoebe Ayers is a wonderful person and was sad to hear she was no longer on the foundation board of directors. Another serendipitous facet of the Wikisym conference is the demo and poster session which allow hackers to present their latest breakthroughs and innovations in technology, of Wikis. This had once been the cornerstone of the conference. I met the developers of TikiWiki as well as the a Java based XWiki. I decided that one day I would implement my own version of the wiki.\n\n\nJimmy Wales’ Keynote Address\nWikisym’s keynote was given by Wikipedia’s co-founder Jimmy Wales. He explained how this talk was one of the ticket he would give this year. However, this was a much better talk than he gave at Wikimania. He mentioned research possibilities and he responded to my question. I was and still am considering if population dynamics could affect phase changes within the community. My question was if a Wiki’s community dropped below a certain size if it would no longer be viable to maintain it. One example of a Wiki being shut down was the 9-11 wiki. I found Wales’ answer enlightening - he said that big or small the community should have little problem adapting to take care of it’s Wiki. Another point worth mentioning was his recommendation to use Wiki data sets of smaller wikis in research. He recommended Muppet wiki as an example of a wiki with a significantly different governance structure than Wikipedia.\n\n\nAfter the conference\nFollowing the conference, I kept in touch with a number of the participants. I applied myself to study social network analysis as well as data analysis with R. I increased my participation in the research newsletter. I hope to expand my research further using population dynamics on graphs and evolutionary game theory. However, with all the new research methods, I’ve gleaned. I am uncertain what direction my future investigations will take only that they will be even more exciting than before.\n\n\n\nThe renovated Ars Electronica Center at Linz, seen from the bridge across the Danube at night\nR. Stuart Geiger\nHeather Ford, Jimmy Wales\nPhoebe Ayers at WikiSym\n\n\n\nCitationBibTeX citation:@online{bochman2022,\n  author = {Bochman, Oren},\n  title = {Wikisym 2012},\n  date = {2022-07-26},\n  url = {https://orenbochman.github.io/blog//posts/2012/2012-07-26-wikisym-2012_files},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2022. “Wikisym 2012.” July 26, 2022. https://orenbochman.github.io/blog//posts/2012/2012-07-26-wikisym-2012_files."
  },
  {
    "objectID": "posts/2017/2017-08-06-dropout/2017-08-06-dropout.html",
    "href": "posts/2017/2017-08-06-dropout/2017-08-06-dropout.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "My thoughts are that we should be able to do better than this version of dropout. - Shortcoming: - Dropout on units can render the net very poor. - Drop out slows training down - since we don’t update half the units and probably a large number of the weights. - For different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures. - We should track dropout related stats to better understand the confidence of the model. - A second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout? 1. Start with some combinations \\(\\binom k n\\) of the weights. where \\(k = | {training\\; set}|*{minibatch\\_size}\\). We use the same dropout for each mini-batch, then switch. 2. Each epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ? 3. When we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net. 4. Ideally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step. 5. We could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism. 6. We should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "posts/2017/2017-08-06-dropout/2017-08-06-dropout.html#dropout",
    "href": "posts/2017/2017-08-06-dropout/2017-08-06-dropout.html#dropout",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "My thoughts are that we should be able to do better than this version of dropout. - Shortcoming: - Dropout on units can render the net very poor. - Drop out slows training down - since we don’t update half the units and probably a large number of the weights. - For different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures. - We should track dropout related stats to better understand the confidence of the model. - A second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout? 1. Start with some combinations \\(\\binom k n\\) of the weights. where \\(k = | {training\\; set}|*{minibatch\\_size}\\). We use the same dropout for each mini-batch, then switch. 2. Each epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ? 3. When we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net. 4. Ideally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step. 5. We could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism. 6. We should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the"
  },
  {
    "objectID": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html",
    "href": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html",
    "title": "Notes for Lesson 2 of Deep Neural Networks",
    "section": "",
    "text": "Feed forward networks are the subject of the first half of the course.\n\n\n\n\n\n\n\n\nFeed forward nets - regression and classication for images and tabular data.\n\n\n\nRecurrent nets - sequence to sequence\n\n\n\nHopfield nets - associative memory using symmetric nets with no hidden units\n\n\n\nBoltzmann machines - symmetric nets with hidden units\n\n\n\ncredit: images from The Neural Network Zoo\n\n\n\n\n\n\nperceptrons\n\n\n\nwhy the bias can be implemented as a special input unit?\nbiases can be treated using weights using an input that is always one.\na threshold is equivalent to having a negative bias.\nwe can avoid having to figure out a separate learning rule for the bias by using a trick:\nA bias is exactly equivalent to a weight on an extra input line that always has an activation of 1.\n\n\n\ncode and image from: Implementing the Perceptron Algorithm in Python  In english:\n\nAdd an extra component with value 1 to each input vector. The “bias” weight on this component is minus the threshold. Now we can forget the threshold.\nPick training cases using any policy that ensures that every training case will keep getting picked.\n\nIf the output unit is correct, leave its weights alone.\nIf the output unit incorrectly outputs a zero, add the input vector to the weight vector.\nIf the output unit incorrectly outputs a 1, subtract the input vector from the weight vector. This is guaranteed to find a set of weights that gets the right answer for all the training cases if any such set exists. a full implementation of a perceptrons:\n\n\ndef perceptron(X, y, lr, epochs):\n    '''\n    X: inputs\n    y: labels\n    lr: learning rate\n    epochs: Number of iterations\n    m: number of training examples\n    n: number of features \n    '''\n    m, n = X.shape    \n    # Initializing parapeters(theta) to zeros.\n    # +1 in n+1 for the bias term.\n    theta = np.zeros((n+1,1))\n    \n    # list with misclassification count per iteration.\n    n_miss_list = []\n    \n    # Training.\n    for epoch in range(epochs):\n        # variable to store misclassified.\n        n_miss = 0\n        # looping for every example.\n        for idx, x_i in enumerate(X):\n            # Inserting 1 for bias, X0 = 1.\n            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          \n            # Calculating prediction/hypothesis.\n            y_hat = step_func(np.dot(x_i.T, theta))\n            # Updating if the example is misclassified.\n            if (np.squeeze(y_hat) - y[idx]) != 0:\n                theta += lr*((y[idx] - y_hat)*x_i)\n                # Incrementing by 1.\n                n_miss += 1\n        # Appending number of misclassified examples\n        # at every iteration.\n        n_miss_list.append(n_miss)\n    return theta, n_miss_list\n\n\n\n\n\n\n\nA point (a.k.a. location) and an arrow from the origin to that point, are often used interchangeably.\nA hyperplane is the high-dimensional equivalent of a plane in 3-D.\nThe scalar product or inner product between two vectors\n\nsum of element-wise products.\nThe scalar product between two vectors that have an angle of less than 90 degrees between them is positive.\n\nFor more than 90 degrees it’s negative.\n\n\n\n\n\n\n\nHas one dimension per weight.\nA point in the space represents a particular setting of all the weights.\nAssuming that we have eliminated the threshold, each training case can be represented as a hyperplane through the origin.\n\nThe weights must lie on one side of this hyperplane to get the answer correct.\n\nEach training case defines a plane (shown as a black line)\n\nThe plane goes through the origin and is perpendicular to the input vector.\nOn one side of the plane the output is wrong because the scalar product of the weight vector with the input vector has the wrong sign.\n\n\n\n\n\n\nWe look at the geometrical interpretation which is the proof for the convergence of the Perceptron learning algorithm works. We are trying to find a decision surface by solving a convex optimization problem. The surface is a hyperplane represented by a line where on side is the correct set and the other is incorrect. The weight vectors form a cone: - This means that wights are closed under addition and positive scaler product. - At zero it is zero.\n\n\n To get all training cases right we need to find a point on the right side of all the planes. But there may not be any such point! If there are any weight vectors that get the right answer for all cases, they lie in a hyper-cone with its apex at the origin.\n\nThe average of two good weight vectors is a good weight vector.\nThe problem is convex.\n\nTwo training case form two hyper planes (shown in black). the good weights lie between them average of good weights is good.\nGeometry of learning using weight space:\n\nit has one dimension per weight (vertex in the graph).\n\na point in the space represents a particular setting of all the weights.\neach training case (once we eliminate the threshold/bias) is a hyper plane through the origin\nonce the threshold is eliminated each training case can be represented as a hyper place through the origin\nthe weight must lie on one side of this place to get the answer correct .\n\nCaveats:\n\nconvergence depends on the picking the right features\ndeep nets don’t use this procedure - as it only converges for single layer perceptrons - but for more than one layer sum of a solution is not necessarily also a solution.\n\n\n\n\n\n\nThis story motivates the need for more powerful networks.\nThese ideas will be important in future lectures, when we’re working on moving beyond these limitations."
  },
  {
    "objectID": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2a-types-of-neural-network-architectures",
    "href": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2a-types-of-neural-network-architectures",
    "title": "Notes for Lesson 2 of Deep Neural Networks",
    "section": "",
    "text": "Feed forward networks are the subject of the first half of the course.\n\n\n\n\n\n\n\n\nFeed forward nets - regression and classication for images and tabular data.\n\n\n\nRecurrent nets - sequence to sequence\n\n\n\nHopfield nets - associative memory using symmetric nets with no hidden units\n\n\n\nBoltzmann machines - symmetric nets with hidden units\n\n\n\ncredit: images from The Neural Network Zoo"
  },
  {
    "objectID": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2b-perceptrons-the-first-generation-of-neural-networks",
    "href": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2b-perceptrons-the-first-generation-of-neural-networks",
    "title": "Notes for Lesson 2 of Deep Neural Networks",
    "section": "",
    "text": "perceptrons\n\n\n\nwhy the bias can be implemented as a special input unit?\nbiases can be treated using weights using an input that is always one.\na threshold is equivalent to having a negative bias.\nwe can avoid having to figure out a separate learning rule for the bias by using a trick:\nA bias is exactly equivalent to a weight on an extra input line that always has an activation of 1.\n\n\n\ncode and image from: Implementing the Perceptron Algorithm in Python  In english:\n\nAdd an extra component with value 1 to each input vector. The “bias” weight on this component is minus the threshold. Now we can forget the threshold.\nPick training cases using any policy that ensures that every training case will keep getting picked.\n\nIf the output unit is correct, leave its weights alone.\nIf the output unit incorrectly outputs a zero, add the input vector to the weight vector.\nIf the output unit incorrectly outputs a 1, subtract the input vector from the weight vector. This is guaranteed to find a set of weights that gets the right answer for all the training cases if any such set exists. a full implementation of a perceptrons:\n\n\ndef perceptron(X, y, lr, epochs):\n    '''\n    X: inputs\n    y: labels\n    lr: learning rate\n    epochs: Number of iterations\n    m: number of training examples\n    n: number of features \n    '''\n    m, n = X.shape    \n    # Initializing parapeters(theta) to zeros.\n    # +1 in n+1 for the bias term.\n    theta = np.zeros((n+1,1))\n    \n    # list with misclassification count per iteration.\n    n_miss_list = []\n    \n    # Training.\n    for epoch in range(epochs):\n        # variable to store misclassified.\n        n_miss = 0\n        # looping for every example.\n        for idx, x_i in enumerate(X):\n            # Inserting 1 for bias, X0 = 1.\n            x_i = np.insert(x_i, 0, 1).reshape(-1,1)          \n            # Calculating prediction/hypothesis.\n            y_hat = step_func(np.dot(x_i.T, theta))\n            # Updating if the example is misclassified.\n            if (np.squeeze(y_hat) - y[idx]) != 0:\n                theta += lr*((y[idx] - y_hat)*x_i)\n                # Incrementing by 1.\n                n_miss += 1\n        # Appending number of misclassified examples\n        # at every iteration.\n        n_miss_list.append(n_miss)\n    return theta, n_miss_list"
  },
  {
    "objectID": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2c-a-geometrical-view-of-perceptrons",
    "href": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2c-a-geometrical-view-of-perceptrons",
    "title": "Notes for Lesson 2 of Deep Neural Networks",
    "section": "",
    "text": "A point (a.k.a. location) and an arrow from the origin to that point, are often used interchangeably.\nA hyperplane is the high-dimensional equivalent of a plane in 3-D.\nThe scalar product or inner product between two vectors\n\nsum of element-wise products.\nThe scalar product between two vectors that have an angle of less than 90 degrees between them is positive.\n\nFor more than 90 degrees it’s negative.\n\n\n\n\n\n\n\nHas one dimension per weight.\nA point in the space represents a particular setting of all the weights.\nAssuming that we have eliminated the threshold, each training case can be represented as a hyperplane through the origin.\n\nThe weights must lie on one side of this hyperplane to get the answer correct.\n\nEach training case defines a plane (shown as a black line)\n\nThe plane goes through the origin and is perpendicular to the input vector.\nOn one side of the plane the output is wrong because the scalar product of the weight vector with the input vector has the wrong sign."
  },
  {
    "objectID": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2d-why-the-learning-works",
    "href": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2d-why-the-learning-works",
    "title": "Notes for Lesson 2 of Deep Neural Networks",
    "section": "",
    "text": "We look at the geometrical interpretation which is the proof for the convergence of the Perceptron learning algorithm works. We are trying to find a decision surface by solving a convex optimization problem. The surface is a hyperplane represented by a line where on side is the correct set and the other is incorrect. The weight vectors form a cone: - This means that wights are closed under addition and positive scaler product. - At zero it is zero.\n\n\n To get all training cases right we need to find a point on the right side of all the planes. But there may not be any such point! If there are any weight vectors that get the right answer for all cases, they lie in a hyper-cone with its apex at the origin.\n\nThe average of two good weight vectors is a good weight vector.\nThe problem is convex.\n\nTwo training case form two hyper planes (shown in black). the good weights lie between them average of good weights is good.\nGeometry of learning using weight space:\n\nit has one dimension per weight (vertex in the graph).\n\na point in the space represents a particular setting of all the weights.\neach training case (once we eliminate the threshold/bias) is a hyper plane through the origin\nonce the threshold is eliminated each training case can be represented as a hyper place through the origin\nthe weight must lie on one side of this place to get the answer correct .\n\nCaveats:\n\nconvergence depends on the picking the right features\ndeep nets don’t use this procedure - as it only converges for single layer perceptrons - but for more than one layer sum of a solution is not necessarily also a solution."
  },
  {
    "objectID": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2e-what-perceptrons-cant-do",
    "href": "posts/2017/dnn-02/2017-08-06-deep-neural-networks-notes-02.html#lecture-2e-what-perceptrons-cant-do",
    "title": "Notes for Lesson 2 of Deep Neural Networks",
    "section": "",
    "text": "This story motivates the need for more powerful networks.\nThese ideas will be important in future lectures, when we’re working on moving beyond these limitations."
  },
  {
    "objectID": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html",
    "href": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html",
    "title": "Notes for Lesson 4 of Deep Neural Networks",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4a-learning-to-predict-the-next-word",
    "href": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4a-learning-to-predict-the-next-word",
    "title": "Notes for Lesson 4 of Deep Neural Networks",
    "section": "Lecture 4a: Learning to predict the next word",
    "text": "Lecture 4a: Learning to predict the next word\nWe have the basic method for creating hidden layers (backprop), we’re going to see what can be achieved with them. We start to ask how the network learns to use its hidden units, with a toy application to family trees and a real application to language modeling.\nA simple example of relational information\nAnother way to express the same information\nChristopher = Penelope\nAndrew = Christine\nMargaret = Arthur\nColin\nRoberto = Maria\nGina = Emilio\nAlfonso\nVictoria = James\nCharlotte\nJennifer = Charles\nLucia = Marco\nPierro = Francesca\nAngela = Tomaso\nSophia\nMake a set of propositions using the 12 relationships:\n\nson, daughter, nephew, niece, father, mother, uncle, aunt\nbrother, sister, husband, wife (colin has-father james) (colin has-mother victoria) (james has-wife victoria) this follows from the two above (charlotte has-brother colin) (victoria has-brother arthur) (charlotte has-uncle arthur) this follows from the above A relational learning task\nThe structure of the neural net\nGiven a large set of triples that come from some family trees, figure out the regularities.\n\nThe obvious way to express the regularities is as symbolic rules:\nHasMother(x,y) & HasHusband(y,z)⇒HasFather(x, z)\nFinding the symbolic rules involves a difficult search through a very large discrete space of possibilities.\nCan a neural network capture the same knowledge by searching through a continuous space of weights?\nlocal encoding of person 2 output distributed encoding of person 2 units that learn to predict features of the output from features of the inputs distributed encoding of person 1 distributed encoding of relationship inputs local encoding of relationship local encoding of person 1\nArchitecture: the net has bottle necks to force it to learn relations using distributed encoding. bottle necks are too small What the network learns\nWhat the network learns\no 3 o o 3 Christopher Andrew Arthur James Charles Colin Penelope Christine Victoria Jennifer Margaret Charlotte Christopher Andrew Arthur James Charles Colin Christine Victoria Jennifer Margaret Charlotte\nThe six hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer. - Nationality, generation, branch of the family tree. These features are only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. For example: Input person is of generation 3 and relationship requires answer to be one generation up implies Output person is of generation 2 This video introduces distributed representations. It’s not actually about predicting words, but it’s building up to that. It does a great job of looking inside the brain of a neural network. That’s important, but not always easy to do."
  },
  {
    "objectID": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4b-a-brief-diversion-into-cognitive-science",
    "href": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4b-a-brief-diversion-into-cognitive-science",
    "title": "Notes for Lesson 4 of Deep Neural Networks",
    "section": "Lecture 4b: A brief diversion into cognitive science",
    "text": "Lecture 4b: A brief diversion into cognitive science\nThis video is part of the course, i.e. it’s not optional, despite what Geoff says in the beginning of the video. This video gives a high-level interpretation of what’s going on in the family tree network. This video contrasts two types of inference:\n\nConscious inference, based on relational knowledge.\nUnconscious inference, based on distributed representations."
  },
  {
    "objectID": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4c-another-diversion-the-softmax-output-function",
    "href": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4c-another-diversion-the-softmax-output-function",
    "title": "Notes for Lesson 4 of Deep Neural Networks",
    "section": "Lecture 4c: Another diversion: The Softmax output function",
    "text": "Lecture 4c: Another diversion: The Softmax output function\nA Softmax cost function is a general-purpose ML component/technique for combining binary discriminators into a probability distribution to construct a classifier We’ve seen binary threshold output neurons and logistic output neurons. This video presents a third type. This one only makes sense if we have multiple output neurons. Problems with squared error The first “problem with squared error” is a problem that shows up when we’re combining the squared error loss function with logistic output units. The logistic has small gradients, if the input is very positive or very negative. When assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but multiple binaries with square error do not encode this constraint.\nSoftmax\n\nCross-entropy\n\nthe right cost function to use with SoftMax\n\n\nThe output units in a softmax group use a non-local non-linearity: softmax group this is called the “logit” zj jegroup öYi\nThe right cost function is the negative log probability of the right answer. C has a very big gradient when the target value is 1 and the output is almost zero. — A value of 0.000001 is much better than 0.000000001 — The steepness of dC/dy exactly balances the flatness of dy/dz öZi t j log target value öyj öZi\nthe cross entropy cost function - is the correct cost function to use with SoftMax\nArchitectural Note: SoftMax unit +Cross-Entropy loss function =&gt; for classification"
  },
  {
    "objectID": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4d-neuro-probabilistic-language-models",
    "href": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4d-neuro-probabilistic-language-models",
    "title": "Notes for Lesson 4 of Deep Neural Networks",
    "section": "Lecture 4d: Neuro-probabilistic language models",
    "text": "Lecture 4d: Neuro-probabilistic language models\nThis is the first of several applications of neural networks that we’ll studying in some detail, in this course. Synonyms: word embedding; word feature vector; word encoding. All of these describe the learned collection of numbers that is used to represent a word. “embedding” emphasizes that it’s a location in a high-dimensional space: it’s where the words are embedded in that space. When we check to see which words are close to each other, we’re thinking about that embedding. “feature vector” emphasizes that it’s a vector instead of a scalar, and that it’s componential, i.e. composed of multiple feature values. “encoding” is very generic and doesn’t emphasize anything specific. looks at the trigram model"
  },
  {
    "objectID": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4e-ways-to-deal-with-the-large-number-of-possible-outputs",
    "href": "posts/2017/dnn-04/2017-08-06-deep-neural-networks-notes-04.html#lecture-4e-ways-to-deal-with-the-large-number-of-possible-outputs",
    "title": "Notes for Lesson 4 of Deep Neural Networks",
    "section": "Lecture 4e: Ways to deal with the large number of possible outputs",
    "text": "Lecture 4e: Ways to deal with the large number of possible outputs\nWhen softmax is very big it becomes hard to train and store.\n\nWay 1: a serial architecture, based on trying candidate next words, using feature vectors (like in the family example). This means fewer parameters, but still a lot of work.\nWay 2: using a binary tree.\nWay 3: Collobert & Weston’s search for good feature vectors for words, without trying to predict the next word in a sentence.\n\nDisplaying the learned feature vectors in a 2-D map\nt-sne output We can get an idea of the quality of the learned feature vectors by displaying them in a 2-D map. — Display very similar vectors very close to each other. Use a multi-scale method called “t-sne” that also displays similar clusters near each other. The learned feature vectors capture lots of subtle semantic distinctions, just by looking at strings of words. — No extra supervision is required. — The information is all in the contexts that the word is used in. — Consider “She scrommed him with the frying pan.”\nather increasingly greatly briefly c tay already only just both either even yet then once y _o icia.lly *bortny A.xyely cu51Nt1y never immediately not even tuallyaga.in »rmery tably probably likely possibly perh ce ther but whe because whil before e here tod.atr\nDisplaying learned feature vectors. Pretty picture!"
  },
  {
    "objectID": "posts/2017/dnn-06/2017-08-06-deep-neural-networks-notes-06.html",
    "href": "posts/2017/dnn-06/2017-08-06-deep-neural-networks-notes-06.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Now we’re going to discuss numerical optimization: how best to adjust the weights and biases, using the gradient information from the backprop algorithm. This video elaborates on the most standard neural net optimization algorithm (mini-batch gradient descent), which we’ve seen before. We’re elaborating on some issues introduced in video 3e. ## Lecture 6b: A bag of tricks for mini-batch gradient descent initializing weights: we must not initialize units with equal weights as they can never become different. we cannot use zero as it will remain zero we want to avoid explosion and vanishing weights fan in - the number of inputs\nPart 1 is about transforming the data to make learning easier. At 1:10, there’s a comment about random weights and scaling. The “it” in that comment is the average size of the input to the unit. At 1:15, the “good principle”: what he means is INVERSELY proportional. At 4:38, Geoff says that the hyperbolic tangent is twice the logistic minus one. This is not true, but it’s almost true. As an exercise, find out’s missing in that equation. At 5:08, Geoffrey suggests that with a hyperbolic tangent unit, it’s more difficult to sweep things under the rug than with a logistic unit. I don’t understand his comment, so if you don’t either, don’t worry. This comment is not essential in this course: we’re never using hyperbolic tangents in this course. Part 2 is about changing the stochastic gradient descent algorithm in sophisticated ways. We’ll look into these four methods in more detail, later on in the course. Jargon: “stochastic gradient descent” is mini-batch or online gradient descent. The term emphasizes that it’s not full-batch gradient descent. “stochastic” means that it involves randomness. However, this algorithm typically does not involve randomness. However, it would be truly stochastic if we would randomly pick 100 training cases from the entire training set, every time we need the next mini-batch. We call traditional “stochastic gradient descent” stochastic because it is, in effect, very similar to that truly stochastic version. Jargon: a “running average” is a weighted average over the recent past, where the most recent past is weighted most heavily. ## Lecture 6c: The momentum method Drill down into momentum mentioned before.\nThe biggest challenge in this video is to think of the error surface as a mountain landscape. If you can do that, and you understand the analogy well, this video will be easy. You may have to go back to video 3b, which introduces the error surface. Important concepts in this analogy: “ravine”, “a low point on the surface”, “oscillations”, “reaching a low altitude”, “rolling ball”, “velocity”. All of those have meaning on the “mountain landscape” side of the analogy, as well as on the “neural network learning” side of the analogy. The meaning of “velocity” in the “neural network learning” side of the analogy is the main idea of the momentum method. Vocabulary: the word “momentum” can be used with three different meanings, so it’s easy to get confused. It can mean the momentum method for neural network learning, i.e. the idea that’s introduced in this video. This is the most appropriate meaning of the word. It can mean the viscosity constant (typically 0.9), sometimes called alpha, which is used to reduce the velocity. It can mean the velocity. This is not a common meaning of the word. Note that one may equivalently choose to include the learning rate in the calculation of the update from the velocity, instead of in the calculation of the velocity. ## Lecture 6d: Adaptive learning rates for each connection This is really “for each parameter”, i.e. biases as well as connection strengths. Vocabulary: a “gain” is a multiplier. This video introduces a basic idea (see the video title), with a simple implementation. In the next video, we’ll see a more sophisticated implementation. You might get the impression from this video that the details of how best to use such methods are not universally agreed on. That’s true. It’s research in progress. ## Lecture 6e: Rmsprop: Divide the gradient by a running average of its recent magnitude This is another method that treats every weight separately. rprop uses the method of video 6d, plus that it only looks at the sign of the gradient. Make sure to understand how momentum is like using a (weighted) average of past gradients. Synonyms: “moving average”, “running average”, “decaying average”. All of these describe the same method of getting a weighted average of past observations, where recent observations are weighted more heavily than older ones. That method is shown in video 6e at 5:04. (there, it’s a running average of the square of the gradient) “moving average” and “running average” are fairly generic. “running average” is the most commonly used phrase. “decaying average” emphasizes the method that’s used to compute it: there’s a decay factor in there, like the alpha in the momentum method."
  },
  {
    "objectID": "posts/2017/dnn-06/2017-08-06-deep-neural-networks-notes-06.html#lecture-6a-overview-of-mini-batch-gradient-descent",
    "href": "posts/2017/dnn-06/2017-08-06-deep-neural-networks-notes-06.html#lecture-6a-overview-of-mini-batch-gradient-descent",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Now we’re going to discuss numerical optimization: how best to adjust the weights and biases, using the gradient information from the backprop algorithm. This video elaborates on the most standard neural net optimization algorithm (mini-batch gradient descent), which we’ve seen before. We’re elaborating on some issues introduced in video 3e. ## Lecture 6b: A bag of tricks for mini-batch gradient descent initializing weights: we must not initialize units with equal weights as they can never become different. we cannot use zero as it will remain zero we want to avoid explosion and vanishing weights fan in - the number of inputs\nPart 1 is about transforming the data to make learning easier. At 1:10, there’s a comment about random weights and scaling. The “it” in that comment is the average size of the input to the unit. At 1:15, the “good principle”: what he means is INVERSELY proportional. At 4:38, Geoff says that the hyperbolic tangent is twice the logistic minus one. This is not true, but it’s almost true. As an exercise, find out’s missing in that equation. At 5:08, Geoffrey suggests that with a hyperbolic tangent unit, it’s more difficult to sweep things under the rug than with a logistic unit. I don’t understand his comment, so if you don’t either, don’t worry. This comment is not essential in this course: we’re never using hyperbolic tangents in this course. Part 2 is about changing the stochastic gradient descent algorithm in sophisticated ways. We’ll look into these four methods in more detail, later on in the course. Jargon: “stochastic gradient descent” is mini-batch or online gradient descent. The term emphasizes that it’s not full-batch gradient descent. “stochastic” means that it involves randomness. However, this algorithm typically does not involve randomness. However, it would be truly stochastic if we would randomly pick 100 training cases from the entire training set, every time we need the next mini-batch. We call traditional “stochastic gradient descent” stochastic because it is, in effect, very similar to that truly stochastic version. Jargon: a “running average” is a weighted average over the recent past, where the most recent past is weighted most heavily. ## Lecture 6c: The momentum method Drill down into momentum mentioned before.\nThe biggest challenge in this video is to think of the error surface as a mountain landscape. If you can do that, and you understand the analogy well, this video will be easy. You may have to go back to video 3b, which introduces the error surface. Important concepts in this analogy: “ravine”, “a low point on the surface”, “oscillations”, “reaching a low altitude”, “rolling ball”, “velocity”. All of those have meaning on the “mountain landscape” side of the analogy, as well as on the “neural network learning” side of the analogy. The meaning of “velocity” in the “neural network learning” side of the analogy is the main idea of the momentum method. Vocabulary: the word “momentum” can be used with three different meanings, so it’s easy to get confused. It can mean the momentum method for neural network learning, i.e. the idea that’s introduced in this video. This is the most appropriate meaning of the word. It can mean the viscosity constant (typically 0.9), sometimes called alpha, which is used to reduce the velocity. It can mean the velocity. This is not a common meaning of the word. Note that one may equivalently choose to include the learning rate in the calculation of the update from the velocity, instead of in the calculation of the velocity. ## Lecture 6d: Adaptive learning rates for each connection This is really “for each parameter”, i.e. biases as well as connection strengths. Vocabulary: a “gain” is a multiplier. This video introduces a basic idea (see the video title), with a simple implementation. In the next video, we’ll see a more sophisticated implementation. You might get the impression from this video that the details of how best to use such methods are not universally agreed on. That’s true. It’s research in progress. ## Lecture 6e: Rmsprop: Divide the gradient by a running average of its recent magnitude This is another method that treats every weight separately. rprop uses the method of video 6d, plus that it only looks at the sign of the gradient. Make sure to understand how momentum is like using a (weighted) average of past gradients. Synonyms: “moving average”, “running average”, “decaying average”. All of these describe the same method of getting a weighted average of past observations, where recent observations are weighted more heavily than older ones. That method is shown in video 6e at 5:04. (there, it’s a running average of the square of the gradient) “moving average” and “running average” are fairly generic. “running average” is the most commonly used phrase. “decaying average” emphasizes the method that’s used to compute it: there’s a decay factor in there, like the alpha in the momentum method."
  },
  {
    "objectID": "posts/2017/dnn-08/2017-08-06-deep-neural-networks-notes-08.html",
    "href": "posts/2017/dnn-08/2017-08-06-deep-neural-networks-notes-08.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course"
  },
  {
    "objectID": "posts/2017/dnn-08/2017-08-06-deep-neural-networks-notes-08.html#lecture-8a-a-brief-overview-of-hessian-free-optimization",
    "href": "posts/2017/dnn-08/2017-08-06-deep-neural-networks-notes-08.html#lecture-8a-a-brief-overview-of-hessian-free-optimization",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "Lecture 8a A brief overview of “Hessian-Free” optimization",
    "text": "Lecture 8a A brief overview of “Hessian-Free” optimization\nHow much can we reduce the error by moving in a given direction? - If we choose a direction to move in and we keep going in that direction, how much does the error decrease before it starts rising again? We assume the curvature is constant (i.e. it’s a quadratic error surface). - Assume the magnitude of the gradient decreases as we move down the gradient (i.e. the error surface is convex upward). - The maximum error reduction depends on the ratio of the gradient to the curvature. So a good direction to move in is one with a high ratio of gradient to curvature, even if the gradient itself is small. - How can we find directions like these?"
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "",
    "text": "Unable to display PDF file. Download instead.\nNotes from Hinton’s Coursera course"
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#reading-adaptive-mixtures-of-local-experts",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#reading-adaptive-mixtures-of-local-experts",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Reading: Adaptive Mixtures of Local Experts",
    "text": "Reading: Adaptive Mixtures of Local Experts\n(Nowlan and Hinton 1990)\n\nNowlan, Steven, and Geoffrey E Hinton. 1990. “Evaluation of Adaptive Mixtures of Competing Experts.” In Advances in Neural Information Processing Systems, edited by R. P. Lippmann, J. Moody, and D. Touretzky. Vol. 3. Morgan-Kaufmann. https://proceedings.neurips.cc/paper_files/paper/1990/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf.\n\nAbstract\n“We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.”\n\n\nMy Notes:\nThis paper is the first time I read about ensembles - and was a kind of introduction. As time goes by ensembles keep getting more of my attention. We put them to work in setting that provides higher capacity models for small data setting. Also, the gating network is like a meta model which may be adapted to quantify uncertainty for each expert at the training case level.\nThe architecture shown bellow uses expert networks trained on a vowel discrimination (classification) task alongside a gating network whose responsibility is to pick the best classifier for the input.\n\nI had been familiar with the idea that the gating network is responsible to convert the output of the experts to the actual experts. It turns out that the gating network also needs to learn which expert is better on a given type of input, and that it also controls the data expert get. This allocation can be hard (each training case goes to one expert) or soft (several experts are allocated). I also noted that some of the prior work was authored by Bastro, an authority on Reinforcement Learning. In prior work the gating network the learn to allocate training cases to one or a few expert - which allows them specialize (the weights are decoupled) also learns to The earlier idea is to utilize or learn to partition the training data so that one can train specialized models that are local experts on the problem space and then use some linear combination of the expert’s predictions to make predictions. But using such a linear combination requires that the expert cancel each other’s output.\n\n\nA Cooperative loss function\n\\[\nE^c= ||\\vec{d^c} -\\sum_i p_i^c \\vec o_i^c||^2\\;\\;\\;\\;(1)\n\\] where :\n\n\\(\\vec o_i^c\\) is the output vector of expert i on case c.\n\\(\\vec d_c\\) is the desired output for case c. \n\nThe authors say that the cooperative loss function in (1) foster an unwanted coupling between the experts, in the sense that a change in one expert’s weights will create a residual loss seen by the other experts in (1). This leads to cooperation but each expert has learn to neutralize the residual it sees from the others experts. So in both cases all models contribute to the inference, instead of just one or a few, which is counter to the idea of being an expert on a subset of the data.\n\n\nThe first competitive loss function\nJacobs, Jordan, and Barto, (1990) Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks used a hard selection mechanism by modifying the objective function to encourage competition and foster greater specialization by using only activate one expert at a time. This paper suggest that it is enough to modify the loss so that the experts compete. The idea being that “the selector acts as a multiple input, single output stochastic switch; the probability that the switch will select the output from expert j is \\(p_j\\)” governed by:\n\\[\nE^c = &lt;||\\vec d^c - \\vec o^c ||&gt; =\\sum_i\\ p_i^c||\\vec d^c- \\vec o_i||^2\\;\\;\\;\\;(2)\n\\]\nwhere :\n\\[\np_j = \\frac{e^{x_j}}{\\sum_i e^x_i}\n\\] soon we are shown a much better loss function:\n\n\nThe first competitive loss function\ndoes not encourage cooperation rather than specialization, which required using many experts in each prediction. Later work added penalty terms in the objective function to gate a single active exert in the prediction.Jacobs, Jordan, and Barton, 1990. The paper offers an alternative error function that encourages specialization.\nThe difference difference between the error functions.\n\n\nThe second competitive error function\n\\[\nE^c= -log\\sum_i\\ p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}\\;\\;\\;\\;(3)\n\\]\n\n\nWhy the second loss is more competitive?\nThe error defined in (2) is simply the negative log probability of generating the desired output vector under the mixture of gaussian’s model described at the end of the next section.\nTo see why this error function works better, it is helpful to compare the derivatives of the two error functions with respect to the output of an expert. From from equation (2) we get: \\[\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -2p_i^c(\\vec d^c-\\vec o_c^c)   \\;\\;\\;\\;  (4)\n\\] while the derivative from equation (3) gives us: \\[\n\\frac {\\partial E^c}{\\partial \\vec o_i^c} = -\\bigg[\\frac{p_i^c e^{\\frac{1}{2}||d^c- \\vec o_i||^2}}{\\sum_j p_j^c e^{\\frac{1}{2}||d^c- \\vec o_j||^2}}\\bigg](\\vec d^c-\\vec o_c^c)   \\;\\;\\;\\;  (5)\n\\] In equation (4) the term \\(\\vec p^c_i\\) is used to weigh the derivative for expert i, while in equation 5 the weighting term takes into account how well expert i does relative to other experts, which is a more useful measure of the relevance of expert i to training case c, especially early in the training. Suppose, that the gating network initially gives equal weights to all experts and \\(||d^c-\\vec o_j||&gt;1\\) for all the experts. Equation 4 will adapt the best-fitting expert the slowest, whereas equation 5 will adapt it the fastest.\n\n\nMaking the learning associative\nIf two loss function are not enough, the authors now suggest a third loss function. This loss looks at the distance from the average vector. \\[\nlogP^c= -log\\sum_i\\ p_i^c K e^{-\\frac{1}{2}||\\vec\\mu_i- \\vec o^c||^2}   \\;\\;\\;\\;  (6)\n\\]\n\n\nResults\nHowever I have not fully grasped the ideas behind this loss and it requires reading additional papers as it was not covered in the lectures. The results parts compares number of epochs needed for different models ensembles and neural networks to reach some level of accuracy on the validation set. The application is also rather complex, but the vowel clustering task itself seems rather simple."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#reading-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#reading-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Reading Improving neural networks by preventing co-adaptation of feature detectors",
    "text": "Reading Improving neural networks by preventing co-adaptation of feature detectors\n(Hinton et al. 2012)\n\nHinton, Geoffrey E., Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors.” https://doi.org/https://doi.org/10.48550/arXiv.1207.0580.\n\nAbstract\nWhen a large feed forward neural network is trained on a small training set, it typically performs poorly on held-out test data. This “overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorically large variety of internal contexts in which it must operate. Random “dropout” gives big improvements on many benchmark tasks and sets new records for speech and object recognition.\n\n\nMy notes\nThis is a paper about using dropout to as a regularization tool, to prevent nodes co-adaptation within parts of the neural network. As I see it if the network has sufficient capacity it will memorize all the training data and then will perform rather poorly on the holdout data and in real world inference. What happen during overfitting is that the network learn both the signal and the noise. In general the law of number works in our favour and the network and since the signal is stronger than the noise we do not initially overfit. However, as the remaining unlearned signal becomes more rare it becomes harder for the model to separate if from the noise. Rare signals will tend to appear less often than certain common noise patterns. Most regularization techniques try to boost the signal. In this case by effectively reducing the capacity and creating, and making the network overall less cohesive. Dropout effectively reduces the network’s capacity during training. It forces the network to create redundent components which relay less on other units. Another regularization is also used: instead of using L2 on the weights vector, L2 norm penalty is used on each weight. If the weight updates violates the constraints, they are normalized. This is motivated by a wish to start with a high learning rate which would otherwise lead to very large weights. This should intuitively allow the net to initially benefit from the stronger signal while reserving more opportunity for later epochs to leave their mark.\nAt trainng time the full network is used nut the Tha authors claim that dropout is equivilent to avareging many random networks. A point they fail to mention is that\n“Dropout is considerably simpler to implement than Bayesian model averaging which weights each model by its posterior probability given the training data. For complicated model classes, like feed forward neural networks, Bayesian methods typically use a Markov chain Monte Carlo method to sample models from the posterior distribution (14). By contrast, dropout with a probability of 0.5 assumes that all the models will eventually be given equal importance in the combination but the learning of the shared weights takes this into account.”\nMy thoughts are that we should be able to do better than this version of dropout.\n\nShortcoming:\nDropout on units can render the net very poor.\nDrop out slows training down - since we don’t update half the units and probably a large number of the weights.\nFor different networks (CNN, RNN, etc) drop out might work better on units that correspond to larger structures.\nWe should track dropout related stats to better understand the confidence of the model.\nA second idea is that the gated network of expert used a neural network to assign each network to its expert. If we want the network to make better use of its capacity, perahps we should introduce some correlation between the dropout nodes and the data. Could we develop a gated dropout?\n\n\nStart with some combinations \\(\\binom k n\\) of the weights. where \\(k = | {training\\; set}|*{minibatch\\_size}\\). We use the same dropout for each mini-batch, then switch.\nEach epoch we should try to switch our mini-batches. We may want to start with maximally heterogenous batches. We may want in subsequent epochs to pick more heterogenous batches. We should do this by shuffling the batches. We might want to shuffle by taking out a portion of the mini-batch inversely proportional to its error rate, shuffle and return. So that the worst mini-batches would get changed more often. We could ?\nWhen we switch we can shuffle different We score the errors per mini-batch dropout combo and try to reduce the error by shuffling between all mini-batches with similar error rates. The lower the error the smaller the shuffles. In each epoch we want to assign to each combination a net.\nIdeally we would like learn how to gate training cases to specific dropouts or to dropout that are within certain symmetry groups of some known dropouts. (i.e. related/between a large number of dropout-combos.). In the “full bayesian learning” we may want to learn a posterior distribution To build a correlation matrix between the training case and the dropout combo. If there was a structure like an orthogonal array for each we might be able to collect this kind of data in a minimal set of step.\nWe could use abstract algebra e.g. group theory to design a network/dropout/mini-batching symmetry mechanism.\nWe should construct a mini-batch shuffle group and a drop out group or a ring. We could also select an architecture that makes sense for the\n\nFurther c.f. Gal and Ghahramani (2016)\n\nGal, Yarin, and Zoubin Ghahramani. 2016. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In International Conference on Machine Learning, 1050–59. PMLR.\n\n\nMy wrap up\nGame theoretic framework have to formalize cooperative and competitive aspects of learning and how these might influence network architectures. c.f. David Balduzzi (2015) Semantics, Representations and Grammars for Deep Learning. There has been lots of progress in training single models for multiple tasks. c.f. Lukasz Kaiser et all. (2017) One Model To Learn Them All . - covered in this video: One Neural network learns EVERYTHING?! which uses mixture of expert layer which come from later work: Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean (2017) Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer in which mixture of experts is used within large neural networks"
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10a-why-it-helps-to-combine-models",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10a-why-it-helps-to-combine-models",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 10a: Why it helps to combine models",
    "text": "Lecture 10a: Why it helps to combine models\nThis lecture is about using a mixture of experts to reduce overfitting. The notion is to train lower capacity models specializing on subsets of the data and learn to predict which one would be the best predictor. Then use the best model for prediction. Alternatively we might average the results of the simpler models. The lecture is challenging as it skims the prior work failing to sufficiently motivate why the different error function arise (they depend on the way the learning scheme are set up) as the paper tries to bridge between competitive learning and a modular neural network.\nThere’s, again, a lot of math, although it’s less difficult than in videos 9d and 9e. Be sure to understand the formulas before moving on. We’re going to combine many models, by using the average of their predictions, at test time. 5:38: There’s a mistake in the explanation of why that term disappears. The mistake is that -2(t-ybar) is not a random variable, so it makes no sense to talk about its variance, mean, correlations, etc. The real reason why the term disappears is simply that the right half of the term, i.e. i, is zero, because ybar is the mean of the yi values."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10b-mixtures-of-experts",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10b-mixtures-of-experts",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 10b: Mixtures of Experts",
    "text": "Lecture 10b: Mixtures of Experts\nThis is a different way of combining multiple models. “Nearest neighbor” is a very simple regression method that’s not a neural network. 7:22: The formula is confusing. The idea is a weighted average of squared errors (weighted by those probabilities p_i). That can be written as an weighted expectation, with weights p_i, of (t-y_i)^2; or as a sum of p_i * (t-y_i)^2. The formula on the slide mixes those two notations. On the next slide it’s written correctly. 10:03: This formula is not trivial to find, but if you differentiate and simplify, you will find it."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10c-the-idea-of-full-bayesian-learning",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10c-the-idea-of-full-bayesian-learning",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 10c: The idea of full Bayesian learning",
    "text": "Lecture 10c: The idea of full Bayesian learning\nIn this video you learn what exactly we want to do with that difficult-to-compute posterior distribution. We learn about doing which is so time-consuming that we can never do it for normal-size neural networks. This is a theory video. We average the predictions from many weight vectors on test data, with averaging weights coming from the posterior over weight vectors given the training data. That sounds simple and is indeed, in a sense, what happens.\nHowever, there’s more to be said about what this “averaging” entails.\nThe Bayesian approach is all about probabilities, so the idea of producing a single number as output has no place in the Bayesian approach. Instead, the output is a distribution, indicating how likely the net considers every possible output value to be. In video 9e we introduced the idea that the scalar output from a network really is the mean of such a predictive distribution. We need that idea again here. That is what Geoffrey means at 6:37. “Adding noise to the output” is a way of saying that the output is simply the centre of a predictive distribution. What’s averaged is those distributions: the predictive distribution of the Bayesian approach is the weighted mean of all those Gaussian predictive distributions of the various weight vectors.\nBy the way, the result of this averaging of many such Gaussian distributions is not a Gaussian distribution. However, if we’re only interested in the mean of the predictive distribution (which would not be very Bayesian in spirit), then we can simply average the outputs of the networks to get that mean. You can mathematically verify this for yourself."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10d-making-full-bayesian-learning-practical",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10d-making-full-bayesian-learning-practical",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 10d: Making full Bayesian learning practical",
    "text": "Lecture 10d: Making full Bayesian learning practical\nMaximum Likelihood is the least Bayesian. Maximum A Posteriori (i.e. using weight decay) is slightly more Bayesian. This video introduces a feasible method that’s even closer to the Bayesian ideal. However, it’s necessarily still an approximation. 4:22: “save the weights” means recording the current weight vector as a sampled weight vector."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10e-dropout",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-10e-dropout",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 10e: Dropout",
    "text": "Lecture 10e: Dropout\nThis is not Bayesian. This is a specific way of adding noise (that idea was introduced in general in video 9c). It’s a recent discovery and it works very, very well. Dropout can be viewed in different ways: One way to view this method is that we add noise. Another more complicated way, which is introduced first in the video, is about weight sharing and different models. That second way to view it serves as the explanation of why adding noise works so well. The first slide in other words: a mixture of models involves taking the arithmetic mean (a.k.a. “the mean”) of the outputs, while a product of models involves taking the geometric mean of the outputs, which is a different kind of mean."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11a-hopfield-nets",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11a-hopfield-nets",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 11a: Hopfield Nets",
    "text": "Lecture 11a: Hopfield Nets\n(Hopfield 1982)\n\nHopfield, John J. 1982. “Neural Networks and Physical Systems with Emergent Collective Computational Abilities.” Proceedings of the National Academy of Sciences 79 (8): 2554–58.\nNow, we leave behind the feedforward deterministic networks that are trained with backpropagation gradients. We’re going to see quite a variety of different neural networks now. These networks do not have output units. These networks have units that can only be in states 0 and 1. These networks do not have units of which the state is simply a function of the state of other units. These networks are, instead, governed by an “energy function”. Best way to really understand Hopfield networks: Go through the example of the Hopfield network finding a low energy state, by yourself. Better yet, think of different weights, and do the exercise with those. Typically, we’ll use Hopfield networks where the units have state 0 or 1; not -1 or 1."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11b-dealing-with-spurious-minima",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11b-dealing-with-spurious-minima",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 11b: Dealing with spurious minima",
    "text": "Lecture 11b: Dealing with spurious minima\nThe last in-video question is not easy. Try to understand how the perceptron learning procedure is used in a Hopfield net; it’s not very thoroughly explained."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11c-hopfield-nets-with-hidden-units",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11c-hopfield-nets-with-hidden-units",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 11c: Hopfield nets with hidden units",
    "text": "Lecture 11c: Hopfield nets with hidden units\nThis video introduces some sophisticated concepts, and is not entirely easy. An “excitatory connection” is a connection of which the weight is positive. “inhibitory”, likewise, means a negative weight. We look for an energy minimum, “given the state of the visible units”. That means that we look for a low energy configuration, and we’ll consider only configurations in which the visible units are in the state that’s specified by the data. So we’re only going to consider flipping the states of the hidden units. Be sure to really understand the last two sentences that Geoffrey speaks in this video."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11d-using-stochastic-units-to-improve-search",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11d-using-stochastic-units-to-improve-search",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 11d: Using stochastic units to improve search",
    "text": "Lecture 11d: Using stochastic units to improve search\nWe’re still working with a mountain landscape analogy. This time, however, it’s not an analogy for parameter space, but for state space. A particle is, therefore, not a weight vector, but a configuration. What’s the same is that we’re, in a way, looking for low points in the landscape. We’re also using the physics analogy of systems that can be in different states, each with their own energy, and subject to a temperature. This analogy is introduced in slide 2. This is the analogy that originally inspired Hopfield networks. The idea is that at a high temperature, the system is more inclined to transition into configurations with high energy, even though it still prefers low energy. 3:25: “the amount of noise” means the extent to which the decisions are random. 4:20: If T really were 0, we’d have division by zero, which is not good. What we really mean here is “as T gets really, really small (but still positive)”. For mathematicians: it’s the limit as T goes to zero from above. Thermal equilibrium, and this whole random process of exploring states, is much like the exploration of weight vectors that we can use in Bayesian methods. It’s called a Markov Chain, in both cases."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11e-how-a-boltzmann-machine-models-data",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-11e-how-a-boltzmann-machine-models-data",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 11e: How a Boltzmann machine models data",
    "text": "Lecture 11e: How a Boltzmann machine models data\nNow, we’re making a generative model of binary vectors. In contrast, mixtures of Gaussians are a generative model of real-valued vectors. 4:38: Try to understand how a mixture of Gaussians is also a causal generative model. 4:58: A Boltzmann Machine is an energy-based generative model. 5:50: Notice how this is the same as the earlier definition of energy. What’s new is that it’s mentioning visible and hidden units separately, instead of treating all units the same way."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-12a-boltzmann-machine-learning",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-12a-boltzmann-machine-learning",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 12a: Boltzmann machine learning",
    "text": "Lecture 12a: Boltzmann machine learning\nClarification: The energy is linear in the weights, but quadratic in the states. What matters for this argument is just that it’s linear in the weights."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-12c-restricted-boltmann-machines",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-12c-restricted-boltmann-machines",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 12c: Restricted Boltmann Machines",
    "text": "Lecture 12c: Restricted Boltmann Machines\nHere, a “particle” is a configuration. These particles are moving around the configuration space, which, when considered with the energy function, is our mountain landscape.\nIt’s called a reconstruction because it’s based on the visible vector at t=0 (via the hidden vector at t=0). It will, typically, be quite similar to the visible vector at t=0.\nA “fantasy” configuration is one drawn from the model distribution by running a Markov Chain for a long time.\nThe word “fantasy” is chosen as part of the analogy of a Boltzmann Machine vs. a brain that learned several memories."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-12d-an-example-of-rbm-learning",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-12d-an-example-of-rbm-learning",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 12d: An example of RBM learning",
    "text": "Lecture 12d: An example of RBM learning\nThis is not an easy video. Prerequisite is a rather extensive understanding of what an RBM does. Be sure to understand video 12c quite well before proceeding with 12d.\nPrerequisite for this video is that you understand the “reconstruction” concept of the previous video.\nThe first slide is about an RBM, but uses much of the same phrases that we previously used to talk about deterministic feedforward networks.\nThe hidden units are described as feature detectors, or “features” for short.\nThe weights are shown as arrows, even though a Boltzmann Machine has undirected connections.\nThat’s because calculating the probability of the hidden units turning on, given the state of the visible units, is exactly like calculating the real-valued state of a logistic hidden unit, in a deterministic feedforward network.\nHowever, in a Boltzmann Machine, that number is then treated as a probability of turning on, and an actual state of 1 or 0 is chosen, randomly, based on that probability. We’ll make further use of that similarity next week.\n2:30. That procedure for changing energies, that was just explained, is a repeat (in different words) of the Contrastive Divergence story of the previous video. If you didn’t fully realize that, then review."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-13a-the-ups-and-downs-of-back-propagation",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-13a-the-ups-and-downs-of-back-propagation",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 13a: The ups and downs of back propagation",
    "text": "Lecture 13a: The ups and downs of back propagation\n6:15: Support Vector Machines are a popular method for regression: for learning a mapping from input to output, as we have been doing with neural networks during the first half of the course."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-13b-belief-nets",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-13b-belief-nets",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 13b: Belief Nets",
    "text": "Lecture 13b: Belief Nets\n7:43. For this slide, keep in mind Boltzmann Machines. There, too, we have hidden units and visible units, and it’s all probabilistic. BMs and SBNs have more in common than they have differences. 9:16. Nowadays, “Graphical Models” are sometimes considered as a special category of neural networks, but in the history that’s described here, they were considered to be very different types of systems."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-13c-learning-sigmoid-belief-nets",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-13c-learning-sigmoid-belief-nets",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 13c: Learning sigmoid belief nets",
    "text": "Lecture 13c: Learning sigmoid belief nets\nIt would be good to read the first part of “The math of Sigmoid Belief Nets” before watching this video. 4:39. The second part of “The math of Sigmoid Belief Nets” mathematically derives this formula. Read it after finishing this video. 7:04. Actually, those numbers aren’t quite correct, although they’re not very far off. The take-home message, however, is correct: p(0,1) and p(1,0) are large, while the other two are small. 7:33. Here’s “explaining away” rephrased in a few more ways: If the house jumps, everybody starts wondering what might have caused that. Was there an earthquake? Did a truck hit the house? We’re not at all sure. When the wind then carries, through the open window, the voice of an upset truck driver bemoaning his bad luck, we know that a truck hit the house. That finding “explains away” the possibility that there might have been an earthquake: all of a sudden, we no longer suspect that there might have been an earthquake, even though we haven’t consulted the seismological office. In other words: as soon as we learn something about one possible cause (truck hits house), we can make an inference about other possible causes (earthquake)."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-13d-the-wake-sleep-algorithm",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-13d-the-wake-sleep-algorithm",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 13d: The wake-sleep algorithm",
    "text": "Lecture 13d: The wake-sleep algorithm\n4:38. Another way to say this is that the multiple units behave independently: the probability of unit 2 turning on has nothing to do with whether or not unit 1 turned on. 5:30. The green weights are the weights of the Sigmoid Belief Net. An “unbiased sample” from some distribution is a sample that’s really drawn from that distribution. A “biased sample” is a sample that’s not quite from the intended distribution. We don’t really do maximum likelihood learning. We just use the maximum likelihood learning rule, while substituting “a sample from the posterior” by “a sample from the approximate posterior”. The only “maximum likelihood” part of it is that the formula for going from that sample to delta w is the same."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15a-from-pca-to-autoencoders",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15a-from-pca-to-autoencoders",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 15a: From PCA to autoencoders",
    "text": "Lecture 15a: From PCA to autoencoders\nRemember how, in assignment 4, we’re use unsupervised learning to obtain a different representation of each data case? PCA is another example of that, but for PCA, there’s even greater emphasis on obtaining that different representation. Chapter 15 is about unsupervised learning using deterministic feedforward networks. By contrast, the first part of the course was about supervised learning using deterministic feedforward networks, and the second part was about unsupervised learning using very different types of networks. 0:26. A linear manifold is a hyperplane. 1:25. A curved manifold is no longer a hyperplane. One might say it’s a bent hyperplane, but really, “hyperplane” means that it’s not bent. 1:37. “N-dimensional data” means that the data has N components and is therefore handled in a neural network by N input units. 1:58. Here, that “lower-dimensional subspace” is yet another synonym for “linear manifold” and “hyperplane”. 2:46 and 3:53. Geoffrey means the squared reconstruction error. 4:43. Here, for the first time, we have a deterministic feedforward network with lots of output units that are not a softmax group. An “autoencoder” is a neural network that learns to encode data in such a way that the original can be approximately reconstructed."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15b-deep-autoencoders",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15b-deep-autoencoders",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 15b: Deep autoencoders",
    "text": "Lecture 15b: Deep autoencoders\n2:51. “Gentle backprop” means training with a small learning rate for not too long, i.e. not changing the weights a lot."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15c-deep-autoencoders-for-document-retrieval",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15c-deep-autoencoders-for-document-retrieval",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 15c: Deep autoencoders for document retrieval",
    "text": "Lecture 15c: Deep autoencoders for document retrieval\n“Latent semantic analysis” and “Deep Learning” sound pretty good as phrases… there’s definitely a marketing component in choosing such names :) 1:14. The application for the method in this video is this: “given one document (called the query document), find other documents similar to it in this giant col## Lection of documents.” 2:04. Some of the text on this slide is still hidden, hence for example the count of 1 for “reduce”. 3:09. This slide is a bit of a technicality, not very central to the story. If you feel confused, postpone focusing on this one until you’ve understood the others well. 6:49. Remember t-SNE?"
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15d-semantic-hashing",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15d-semantic-hashing",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 15d: Semantic Hashing",
    "text": "Lecture 15d: Semantic Hashing\nWe’re continuing our attempts to find documents (or images), in some huge given pile, that are similar to a single given document (or image). Last time, we focused on making the search produce truly similar documents. This time, we focus on simply making the search fast (while still good). This video is one of the few times when machine learning goes hand in hand very well with intrinsically discrete computations (the use of bits, in this case). We’ll still use a deep autoencoder. This video is an example of using noise as a regularizer (see video 9c). Crucial in this story is the notion that units of the middle layer, the “bottleneck”, are trying to convey as much information as possible in their states to base the reconstruction on. Clearly, the more information their states contain, the better the reconstruction can potentially be."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15e-learning-binary-codes-for-image-retrieval",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15e-learning-binary-codes-for-image-retrieval",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 15e: Learning binary codes for image retrieval",
    "text": "Lecture 15e: Learning binary codes for image retrieval\nIt is essential that you understand video 15d before you try 15e. 7:13. Don’t worry if you don’t understand that last comment."
  },
  {
    "objectID": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15f-shallow-autoencoders-for-pre-training",
    "href": "posts/2017/dnn-10/2017-08-06-deep-neural-networks-notes-10.html#lecture-15f-shallow-autoencoders-for-pre-training",
    "title": "Notes for Lesson 10 of Deep Neural Networks",
    "section": "Lecture 15f: Shallow autoencoders for pre-training",
    "text": "Lecture 15f: Shallow autoencoders for pre-training\nThis video is quite separate from the others of chapter 15.\nCNN Architecture & hyper parameters\nConvolutional Neural Network example INPUT [F,F,3]\nCONV [F,F,K] - basis sensor RELU [F,F,K ] - elementwise activation POOL [F/2,F/2,S] - down sampling\nFC - convers volume to class probability Hyper parameters: K – depth is the number of filters/kernels to use say 12 F - the RECEPTIVE FIELD or spatial extent of the filters – pixels width and height a neuron sees say 32x32 S – the STRIDE = step size for the offset used for sliding the filters so that there is an overlap neurons – say 1 P the amount of PADDING= padding round input with zeros, used because output and input might otherwise have different sizes\nAs of 2015 per STRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET the recommendation is to Removing\nPooling Removing normalization also recommended\nINPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]M -&gt; [FC -&gt; RELU]K -&gt; FC\nSeems FC and CONV are functionally equivalent and can be interchanged. Some other techniques/layers types: 1x1 convolution Dilated convolutions (acting on spaced out pixels) Replacing Max Pooling with ROI region of interrest pooling Loss layer – represent the overall error Dropout layer - Regularization by droping a unit with probabpility p DropConnect - Regularization by dropping connections instead of units\nStochastic pooling\nWeight decay = 0.001 Image whitening and contrast normalization in preprocessing"
  },
  {
    "objectID": "posts/2017/dnn-12/2017-08-06-deep-neural-networks-notes-12.html",
    "href": "posts/2017/dnn-12/2017-08-06-deep-neural-networks-notes-12.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course\n\nNeural Networks for Machine Learning\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} {From} {Hinton’s}\n    {Course}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//posts/2017/dnn-12/2017-08-06-deep-neural-networks-notes-12.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes From Hinton’s\nCourse.” August 6, 2017. https://orenbochman.github.io/blog//posts/2017/dnn-12/2017-08-06-deep-neural-networks-notes-12.html."
  },
  {
    "objectID": "posts/2017/dnn-14/2017-08-06-deep-neural-networks-notes-14.html",
    "href": "posts/2017/dnn-14/2017-08-06-deep-neural-networks-notes-14.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course\n\nNeural Networks for Machine Learning\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} {From} {Hinton’s}\n    {Course}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//posts/2017/dnn-14/2017-08-06-deep-neural-networks-notes-14.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes From Hinton’s\nCourse.” August 6, 2017. https://orenbochman.github.io/blog//posts/2017/dnn-14/2017-08-06-deep-neural-networks-notes-14.html."
  },
  {
    "objectID": "posts/2017/dnn-16/2017-08-06-deep-neural-networks-notes-16.html",
    "href": "posts/2017/dnn-16/2017-08-06-deep-neural-networks-notes-16.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course\n\nNeural Networks for Machine Learning\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} {From} {Hinton’s}\n    {Course}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//posts/2017/dnn-16/2017-08-06-deep-neural-networks-notes-16.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes From Hinton’s\nCourse.” August 6, 2017. https://orenbochman.github.io/blog//posts/2017/dnn-16/2017-08-06-deep-neural-networks-notes-16.html."
  },
  {
    "objectID": "posts/2017/dnn-references/2017-09-09-deep-neural-networks-references.html",
    "href": "posts/2017/dnn-references/2017-09-09-deep-neural-networks-references.html",
    "title": "Deep Neural Networks - Notes From Hinton’s Course",
    "section": "",
    "text": "Notes from Hinton’s Coursera course\n\nNeural Networks for Machine Learning\n\n\n\n\nCitationBibTeX citation:@online{bochman2017,\n  author = {Bochman, Oren},\n  title = {Deep {Neural} {Networks} - {Notes} {From} {Hinton’s}\n    {Course}},\n  date = {2017-08-06},\n  url = {https://orenbochman.github.io/blog//posts/2017/dnn-references/2017-09-09-deep-neural-networks-references.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2017. “Deep Neural Networks - Notes From Hinton’s\nCourse.” August 6, 2017. https://orenbochman.github.io/blog//posts/2017/dnn-references/2017-09-09-deep-neural-networks-references.html."
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html",
    "href": "posts/2023/2023-12-20-autogluon/index.html",
    "title": "AutoGluon Cheetsheets",
    "section": "",
    "text": "AutoGluon is a powerful framework for auto-ML."
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "href": "posts/2023/2023-12-20-autogluon/index.html#tabular",
    "title": "AutoGluon Cheetsheets",
    "section": "Tabular",
    "text": "Tabular\n\n\n\nTabular"
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "href": "posts/2023/2023-12-20-autogluon/index.html#time-series",
    "title": "AutoGluon Cheetsheets",
    "section": "Time Series",
    "text": "Time Series\n\n\n\nTime Series"
  },
  {
    "objectID": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "href": "posts/2023/2023-12-20-autogluon/index.html#multimodal",
    "title": "AutoGluon Cheetsheets",
    "section": "Multimodal",
    "text": "Multimodal\n\n\n\nMultimodal\n\n\n\n\n\nautogluon\nTabular\nTime Series\nMultimodal"
  },
  {
    "objectID": "posts/2024/post-with-code/index.html",
    "href": "posts/2024/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n2\n\n\n\n\n\nCitationBibTeX citation:@online{bochman2024,\n  author = {Bochman, Oren},\n  title = {Post {With} {Code}},\n  date = {2024-01-28},\n  url = {https://orenbochman.github.io/blog//posts/2024/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBochman, Oren. 2024. “Post With Code.” January 28, 2024. https://orenbochman.github.io/blog//posts/2024/post-with-code."
  }
]